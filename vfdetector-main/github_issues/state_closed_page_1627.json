[{"number": 4099, "title": "Build Error Keeps Coming", "body": "vyraun@vyraun:~/tensorflow$ bazel build //tensorflow/examples/android:tensorflow_demo\nWARNING: Bazel Android NDK crosstools are based on Android NDK revision 11. The revision of the Android NDK given in android_ndk_repository rule 'androidndk' is '12.1.2977051'.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/debug:debug_graph_utils.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/debug:debug_graph_utils.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_patch_3d.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nWARNING: /home/vyraun/tensorflow/tensorflow/core/BUILD:646:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\nINFO: Found 1 target...\nERROR: missing input file '@androidsdk//:build-tools/23.0.1/aapt'.\nERROR: /home/vyraun/.cache/bazel/_bazel_vyraun/b219210c52a294104956e4ed70d84022/external/androidsdk/BUILD:5:1: @androidsdk//:aapt_binary: missing input file '@androidsdk//:build-tools/23.0.1/aapt'.\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nERROR: /home/vyraun/.cache/bazel/_bazel_vyraun/b219210c52a294104956e4ed70d84022/external/androidsdk/BUILD:5:1 1 input file(s) do not exist.\nINFO: Elapsed time: 2.821s, Critical Path: 0.03s\nvyraun@vyraun:~/tensorflow$ bazel clean\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\nvyraun@vyraun:~/tensorflow$ bazel build //tensorflow/examples/android:tensorflow_demo\nERROR: no such package '@androidndk//': Could not read RELEASE.TXT in Android NDK: /home/vyraun/.cache/bazel/_bazel_vyraun/b219210c52a294104956e4ed70d84022/external/androidndk/ndk/RELEASE.TXT (No such file or directory).\nERROR: no such package '@androidndk//': Could not read RELEASE.TXT in Android NDK: /home/vyraun/.cache/bazel/_bazel_vyraun/b219210c52a294104956e4ed70d84022/external/androidndk/ndk/RELEASE.TXT (No such file or directory).\nINFO: Elapsed time: 2.406s\n\nI tried changing the api levels in workspace. Didn't work. I do not have any folder for 23.0.1 in my build-tools directory. I guess there is some configuration issue. How could I resolve them? Thanks for your help.\n", "comments": ["Pete, help with Android build?\n", "Hi, thanks for the reply. I had to change the workspace SDK to my own\nversion 24.0.2 and then it worked.\n\nOn Sep 1, 2016 10:40 AM, \"Martin Wicke\" notifications@github.com wrote:\n\n> Pete, help with Android build?\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4099#issuecomment-243977366,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AQa2LKQYY7gloQFjYMOitjrYjsHaNtvKks5qll4_gaJpZM4Jv_mM\n> .\n"]}, {"number": 4098, "title": "breakpoints not working in tf_ios_makefile_example", "body": "### Environment info\n\nOperating System: OS X El Capitan\nXcode v7.3.1\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nI'm able to build and the run the tf_ios_makefile_example according to https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples but while debugging the app, the breakpoints I set in the app (RunModelViewController.mm even in AppDelegate.mm) doesn't work - NSLog statements next to the breakpoints work fine. \n\nAnyone has the problem? Any solutions or ideas? Thanks!\n### What other attempted solutions have you tried?\n\nI verified new Xcode project or other existing Xcode projects work fine with pause at set breakpoints. I also Googled \"xcode 7 breakpoints not working\" but didn't see helpful info. I noticed \nthe line CFLAGS=\"-DNDEBUG...\" in compile_ios_protobuf.sh but am not sure if it's the cause.\n", "comments": ["If you look at the \"Other Linker Flags\" section of your Xcode settings, you should see several settings. If you remove \n`\n-Xlinker\n-S\n-Xlinker\n-x\n-Xlinker\n-dead_strip\n`\nyou should see more symbols when you run your app, and so find it easier to set breakpoints.\n", "After I removed `-Xlinker -S -Xlinker -x -Xlinker -dead_strip` I got linker errors: `Undefined symbols for architecture x86_64: \"_deflate\", reference from: tensorflow::io::ZliboutputBuffer::Deflate(int) in libtensorflow-core.a`... Are those -Xlinker settings optional for building the app?\n", "@jeffxtang You can get rid of those errors by linking with libz. Add libz.1.2.5.tbd in `Link Binary with Libraries`.\n", "Thanks @jackflips! It works like a charm! Sorry for my late response - I just came back from a 2-week trip and tested it.\n"]}, {"number": 4097, "title": "tf.nn.log_poisson_loss missing from tf.nn", "body": "On Ubuntu 14.04: pip freeze is\n\n```\nfuncsigs==1.0.2\nmock==2.0.0\nnumpy==1.11.1\npbr==1.10.0\nprotobuf==3.0.0b2\nsix==1.10.0\ntensorflow==0.10.0rc0\n```\n\nTrying to import tf.nn.log_poisson_loss fails\n\n```\n>>> import tensorflow as tf\n>>> 'log_poisson_loss' in tf.nn.__dict__\nFalse\n>>> tf.nn.log_poisson_loss(tf.constant(1), tf.constant(1))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: 'module' object has no attribute 'log_poisson_loss'\n```\n\nLooking at the source, it looks fine to me, not sure why it's missing. \n", "comments": ["That makes no sense. In nn_test it is used exactly this way (via import of tensorflow). \n", "(I'm not saying you're wrong, I'm just confused)\n", "No I know what you mean! I replicated this bug like 6 times because I was sure I was crazy. \n", "I just installed the nightly pip in a fresh virtualenv, and:\n\n```\n$ python\nPython 2.7.6 (default, Jun 22 2015, 17:58:13) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:110] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:110] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:110] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:110] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:110] successfully opened CUDA library libcurand.so locally\n>>> tf.nn.log_poisson_loss\n<function log_poisson_loss at 0x7fa705e0fde8>\n```\n\nSo I don't know what's wrong with your install.  We've got 99+ issues but a pip ain't one.\n", "I installed `0.10.0rc0` is this not a stable release? Shouldn't this issue be tested against the release that I claimed it occurred in? \n\nFYI I just found another one. Fresh installation:\n\n```\n>>> 'OPTIMIZER_SUMMARIES' in tf.contrib.layers.optimizers.__dict__\nFalse\n```\n", "That one should probably be module private. No need to use that.\n@ilblackdragon what do you think?\n\nOn Thu, Sep 1, 2016 at 4:09 PM, Eli Bixby notifications@github.com wrote:\n\n> I installed 0.10.0rc0 is this not a stable release? Shouldn't this issue\n> be tested against the release that I claimed it occurred in?\n> \n> FYI I just found another one. Fresh installation:\n> \n> > > > 'OPTIMIZER_SUMMARIES' in tf.contrib.layers.optimizers.**dict**\n> > > > False\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4097#issuecomment-244239175,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_c_PPstZf2KjRolzdZTdZ7WU9WiRks5ql1tBgaJpZM4Jv1Aw\n> .\n", "Hm.  It looks like when we cherry-pick new commits into r0.10, we're not rebuilding the .whl's that go to storage.googleapis.  So yeah, this is basically fixed, but we haven't pushed out new wheels yet.  We'll work on this process in the future.\n\nI believe r0.10 will be out of RC very soon, so this will be fixed when we push out the official release.\n", "@martinwicke best practice to avoid magic strings. Much better to say `OPTIMIZER_SUMMARIES`, which says gives me all available summaries, especially if the list of available summaries expands in the future. Or `OPTIMIZER_SUMMARIES[0], OPTIMIZER_SUMMARIES[:2]` e.g.\n", "@vrv The RC wheels should not magically upate -- that would destroy the\nnotion of an RC. We could automatically make a new RCx release every time\nwe push (similar to nightlies), maybe.\n\n@elibixby I would welcome a PR.\n\nOn Thu, Sep 1, 2016 at 4:49 PM, Eli Bixby notifications@github.com wrote:\n\n> @martinwicke https://github.com/martinwicke best practice to avoid\n> magic strings. Much better to say OPTIMIZER_SUMMARIES, which says gives\n> me all available summaries, especially if the list of available summaries\n> expands in the future. Or OPTIMIZER_SUMMARIES[0], OPTIMIZER_SUMMARIES[:2]\n> e.g.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4097#issuecomment-244245811,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_af8dO2s0yRaahX3Ue7aK4LNVy7kks5ql2R2gaJpZM4Jv1Aw\n> .\n", "@martinwicke yeah, but we should be bumping up the rc number on every set of cherry-picks :).  I'm sure we'll get there\n", "We can do that, but only as a nightly process -- it involves a bit of extra\nsetup, but could actually be not too bad. Basically a release job which\npoints to a branch (or set of branches) and updates automatically on push.\n\nOn Thu, Sep 1, 2016 at 5:52 PM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> @martinwicke https://github.com/martinwicke yeah, but we should be\n> bumping up the rc number on every set of cherry-picks :). I'm sure we'll\n> get there\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4097#issuecomment-244254855,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_epsKbqQ78UfSL294ZPojjpTghxAks5ql3NigaJpZM4Jv1Aw\n> .\n"]}, {"number": 4096, "title": "Use canonical cuda include directory path in CROSSTOOL.", "body": "Fixes #3985\n", "comments": ["Is it known yet which bazel release will contain that change?\n", "The next Bazel release should contain bazelbuild/bazel#1685.\n\n+@damienmg to confirm.\n", "Definitely it will\n", "Any updates on this? has the bazel release gone out?\n", "Yes the release was done last Friday.\n", "@tensorflow-jenkins test this please\n", "Our CI needs to upgrade bazel, and then we need to bump the version of the check_version required bazel before we can accept this.  It would be nice if the check_version went in with this PR.\n", "A lot of the bazel upgrades can be incorporated into this change.\nDavid, could you update all our Dockerfiles that install bazel?\nWe will then also need to upgrade out MAC executors manually before we merge this.\nAlso this file:\ntensorflow/tools/ci_build/install/install_bazel.sh\n", "Thanks for the pointers. Will do.\n", "DO NOT MERGE\nthe update change to bazel 0.3.2 has been reverted due to failures in CI.\n", "The change to upgrade to Bazel 0.3.2 has been rolled-forward. I have rebased this PR on master, and all of the tests are now passing.\n\nPTAL.\n"]}, {"number": 4095, "title": "How to use pre-trained word embeddings in seq2seq?", "body": "I am building a seq2seq model using functions in seq2seq.py, where they have a function like this:\n\n```\nembedding_rnn_seq2seq(encoder_inputs, decoder_inputs, cell,\n                          num_encoder_symbols, num_decoder_symbols,\n                          embedding_size, output_projection=None,\n                          feed_previous=False, dtype=dtypes.float32,\n                          scope=None)\n```\n\nhowever, it seems that this function does not take pre-trained embeddings as input, are there any ways that I can take pre-trained word embeddings as input in this function?\n", "comments": ["What you want to do is load pretrained embedding variables and use the tf.nn.rnn_cell.EmbeddingWrapper (or some variant of this that accesses the exact correct variable) around your cell instance, instead of calling embedding_rnn_seq2seq.  the code you're referring will be deprecated in some near future because it's not very flexible (the example you are asking about is one which should be more easily supported) + @lukaszkaiser.  To see how to load pretrained embeddings, see @mrry's excellent answer to this StackOverflow post:\n\nhttp://stackoverflow.com/questions/35687678/using-a-pre-trained-word-embedding-word2vec-or-glove-in-tensorflow\n", "(Lukasz, if you'd like to say more, feel free to reopen & assign self)\n", "Hi @ebrevdo , quick question....\r\nWhat I don't understand is how to match the IDs in x with the IDs of your embedding.\r\n\r\nFor instance, if my sentence is \"I like to run with dogs\", and that is vectorized as\r\n[5, 9, 2, 25, 16, 4], how will Tensorflow know that id 5 in the pretrained embedding matrix should correspond to 'I' and 9 to 'like', 2 to 'to', 25 to 'run', and so on? \r\n\r\nso maybe we have something like \r\nx=tf.placeholder(...)   # [5, 9, 2, 25, 16, 4]\r\nW = tf.Variable... # What the attached stackoverflow answer says\r\n...\r\n\r\nSo when we do\r\ntf.nn.embedding_lookup(W, x), how will this know that the first element of x (5) and the 5th entry in W are mapping to the same underlying word?\r\n\r\nI hope my question made sense. Thanks in advance.", "It doesn't, you have to have a map and reorder your loaded embedding to\nyour vocabulary.  This can be done with the vocabulary lookup table,\ntf.range, and tf.nn.top_k but is not obvious.  Alternatively manually\nreshuffle the embedding matrix to match your vocabulary.  We hope to have\nan example of how to do this in tf in the upcoming seq2seq tutorial with\nglove embedding files.\n\nOn May 21, 2017 3:37 PM, \"andrewjylee\" <notifications@github.com> wrote:\n\n> Hi @ebrevdo <https://github.com/ebrevdo> , quick question....\n> What I don't understand is how to match the IDs in x with the IDs of your\n> embedding.\n>\n> For instance, if my sentence is \"I like to run with dogs\", and that is\n> vectorized as\n> [5, 9, 2, 25, 16, 4], how will Tensorflow know that id 5 in the pretrained\n> embedding matrix should correspond to 'I' and 9 to 'like', 2 to 'to', 25 to\n> 'run', and so on?\n>\n> so maybe we have something like\n> x=tf.placeholder(...) # [5, 9, 2, 25, 16, 4]\n> W = tf.Variable... # What the attached stackoverflow answer says\n> ...\n>\n> So when we do\n> tf.nn.embedding_lookup(W, x), how will this know that the first element of\n> x (5) and the 5th entry in W are mapping to the same underlying word?\n>\n> I hope my question made sense. Thanks in advance.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4095#issuecomment-302968113>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1BZ9csSOmnlnFO_RpIdKM2trfEZks5r8LydgaJpZM4JvsZA>\n> .\n>\n", "@ebrevdo Can u pls point me to the seq2seq tutorial which uses glove embedding files ? ", "Hi @andrewjylee, I have the same doubt with you. did you know how to align the word ids to ones in the pretrained embeddings? \r\nDoes what @ebrevdo said means we have to align word ids when loading the pretrained embedding?\r\nthanks.", "I was facing same issue so I wrote detailed tutorial on that , check it out this notebook how to Use Pre-trained word_embedding in Tensorflow\r\n\r\nhttps://github.com/monk1337/word_embedding-in-tensorflow/blob/master/Use%20Pre-trained%20word_embedding%20in%20Tensorflow.ipynb"]}, {"number": 4094, "title": "Queue operation in conditional execution context fails with \"operation has been marked as not fetchable\"", "body": "Hey TensorFlow Community,\na while ago I wrote some code for doing gradient descent in a distributed environment. I adapted code from `tensorflow/tensorflow/python/training/sync_replicas_optimizer.py` and wrote my own `apply_gradients` method. Crucially, I included a conditional op, that either performed (distributed, synchronised) gradient descent or returned the `apply_gradients` op from the original optimizer. \nThis code worked well when I used it with TensorFlow 0.8, but it broke with 0.9 and is still broken in my current installation from source (see below). The QueueRunner that is responsible for the synchronization op between the workers now fails with the error message \"operation has been marked as not fetchable\".\nA bit of searching in the execution stack led me to the lines \n`if self._control_flow_context is not None:`\n      `self._control_flow_context.AddOp(self)`\nin `tensorflow/tensorflow/python/framework/ops.py`\nfrom which I take that the error comes from the fact that the queue operations are placed inside the conditional execution context. \n\nOut of curiosity: What is the idea behind \"fetchable\" ops - and why should enqueuing and dequeueing be \"dangerous to fetch\"? \n\nAny help would be appreciated!\n\nMatthias\n### Environment info\n\nOperating System: \nDistributor ID: LinuxMint\nDescription:    Linux Mint 17.2 Rafaela\nRelease:    17.2\nCodename:   rafaela\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\ncuda/lib64/libcudadevrt.a\ncuda/lib64/libcudart.so -> libcudart.so.8.0\ncuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\ncuda/lib64/libcudart.so.8.0.27\ncuda/lib64/libcudart_static.a\ncuda/lib64/libcudnn.so\ncuda/lib64/libcudnn.so.5\ncuda/lib64/libcudnn.so.5.1.5\ncuda/lib64/libcudnn_static.a\nIf installed from binary pip package, provide:\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n   `57ff9d9be2f1faaba9598a3d99ef6c3af02342a4`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n### What other attempted solutions have you tried?\n\nRemoving the conditional execution clause removes the error\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["Any op created in a branch of a TensorFlow conditional or the body of a TensorFlow loop is marked as \"non-fetchable\", to prevent various programming errors. For example, there was once a bug where if you fetched (i.e. passed directly to `Session.run()`) the result of an op that was created in the _non-taken_ branch of a conditional, it was possible to cause a crash (or perhaps a deadlock - I'm not 100% sure). Similarly, if you fetched the result of an op in a loop, the runtime would become confused because that tensor could have multiple values (one per iteration) at execution time. I'm not sure what your old code looked like, but it's possible that it was working by chance, given the way control flow is implemetned :).\n\nAs I understand it, it's only possible to get into this situation if you create an op inside a branch/loop and store a reference to it in an outer context: either explicitly by assigning it to a non-local variable, or implicitly by adding it to a graph collection. The former case is easy to avoid; the latter is a bit more insidious. Recall that all functions passed to `tf.cond()` or `tf.while_loop()` must be pure functions, and so they must not modify their environment.\n\nIn general, it should be possible to workaround this constraint by returning the ops that you want to run from the true-branch and false-branch functions (adding a `tf.no_op()` to the other side if necessary). Let us know if that isn't the case though.\n", "Let me see if I understand correctly. In the True branch, I create a queue and define a QueueRunner object (which is stored and referenced later). This QueueRunner is then started by the tf.train.Supervisor() before starting the main training loop. Does this started QueueRunner then violate the assumption that the cond() branch is only a function? \n\nOn another note, If I manipulate the condition to always evaluate to the \"True\" branch (which involves the queueing operations), the error still arises. \n", "That's correct. Instead of creating the QueueRunner inside a branch of the cond, you should create it based on the result returned by the `tf.cond()` function. The fetchability of tensors is determined statically, before any optimizations take place, so the error will be raised even if the same branch is always taken.\n", "Thanks for the help! Putting the QueueRunner logic outside the tf.cond() context, based on the result of the tf.cond() fixed the issue. \n", "@mrry Let's say an op should be called conditionally using `tf.cond(predicate, lambda: op, lambda: tf.no_op())`, but also manually using `sess.run(op)`. The manual execution is prevented by the \"marked as not fetchable\" mechanism. Is this intended?", "@danijar No, that sounds like a bug. Feel free to open another issue about this (or send a pull request with a fix)!", "@matthiasreisser  hi, I have similar problem. I create a queue runner object and  add it to collection to the graph. It reports error enqueue_many  ops has been marked as not fetchable. So I change my code to  return the queuerunner object. It reports errors \"Expected binary or unicode string, got <tensorflow.python.training.queue_runner_impl.QueueRunner\". How did you solve the problem?", "@mrry  hi mrry. I have encountered similar problem.  I add queue runner to collection using tf.train.add_queue_runner function  inside the conditional execution context. According to your reply , I should return the operations from true-branch and false-branch function. So I try to return the enqueue operations and the equeue from true-branch and false-branch function. But when I return the queue, tensorflow reports errors: \"Expected binary or unicode string, got queue\"", "I ran into the same error \"marked as not fetchable\" at tf.cond(predicate, lambda: op, lambda: tf.no_op()). I got it around by setting the identity for the opp after  tf.cond()\r\n\r\n`tf.identity(self.object_inside_op, name=\"boxes\")`", "Hi, \r\n\r\nI have run into a problem which I think is relevant to this issue. I want to try to reference a pythonic list by evaluating a tensor inside a while loop like so:\r\n```\r\nimport tensorflow as tf\r\n\r\nclass A():\r\n    def __init__(self):\r\n        self.lst = [1, 2, 3]\r\n        self.sess = tf.Session()\r\n        self.total_length = tf.constant(len(self.lst))\r\n\r\n    def loop(self, i):\r\n        pr = tf.print(i)\r\n        current_value = self.lst[i.eval(session=self.sess)]\r\n        with tf.control_dependencies([pr]):\r\n            i = tf.add(i, 1)\r\n        return [i]\r\n\r\n    def cond(self, i):\r\n        return tf.less(i, self.total_length) \r\n\r\n    def run(self):\r\n        i = tf.constant(0)\r\n        while_op = tf.while_loop(self.cond, self.loop, [i])\r\n        final_i = self.sess.run(while_op)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    obj = A()\r\n    obj.run()\r\n```\r\n\r\nIt throws the following error:\r\n```\r\nValueError: Operation 'while/Identity_1' has been marked as not fetchable.\r\n```\r\nAm I doing something wrong? ", "@mrry excuse me, so if i really want to fetch op created in the tf,while_loop (eg: use tf.eval() to transform a tensor to numpy), what should i do? is that possible? really appreciate your kindness:)", "@nishantb21 That appears to be a bug. Can you please open a new issue with your example code and someone will take a look?\r\n\r\n@Jemmagu No, it is not possible to fetch the result of an op created inside the while loop's body, because the body might execute 0 or more times, based on the loop condition. To get a value out of the loop, you need to return it from the body function (as one of the loop variables), and its final value after all the iterations will be returned from `tf.while_loop()`.", "I have a usecase where I haven't been able to work around with tf.no_ops within cond branches.\r\n\r\nI run an eval pipeline where I run metric update ops per batch based on a condition on the inputs. Then I run a final metric op to retrieve the metrics.\r\n\r\nPseudocode:\r\niter = tf.data.Iterator on a dictionary ['input': float32, 'type' string]\r\n\r\ndef compute_metrics(x):\r\n    _, metric_op = tf.metrics.mean(x, metrics_collections=[tf.GraphKeys.METRIC_VARIABLES], name='my_mean_op')\r\n    return metric_op\r\n\r\nupdate_ops = tf.cond(condition on iter['type'], lambda: compute_metrics(iter['input']), lambda: tf.no_op())\r\nmetric_ops = Find the op by name from tf.get_collection(tf.GraphKeys.METRIC_VARIABLES)\r\n\r\nI then run sess.run(update_ops) until the iterator is exhausted and then finally do sess.run(metric_ops) to obtain the final metrics.\r\n\r\nThe issue here is that most tf.metric.xyz methods return both metric and update ops but in my case I want a conditional update but an always present value op which is initialized to 0 in tf.metric.xyz. Any suggestions on how to go about this?\r\n"]}, {"number": 4093, "title": "While running test cases of SVHN data using Tensor Flow I get Resource exhausted : OOM when allocating tensor with shapedim { size: 73257 } dim { size: 32 } dim { size: 32 } dim { size: 32 }", "body": "groove@groove-VirtualBox:~/Udacity-Nanodegree-Project5-Capstone$ python Capstone-project.py\n(32, 32, 3, 26032)\n(26032, 1)\n(32, 32, 3, 73257)\n(73257, 1)\n(26032, 32, 32, 3)\n(73257, 32, 32, 3)\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 4\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 4\nstarted at :  2016-08-29 19:06:23.180157\nException AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f4db351d690>> ignored\nstep 0, training accuracy 0.109375\nstep 250, training accuracy 0.328125\nstep 500, training accuracy 0.640625\nstep 750, training accuracy 0.6875\nstep 1000, training accuracy 0.6875\nstep 1250, training accuracy 0.8125\nstep 1500, training accuracy 0.8125\nstep 1750, training accuracy 0.828125\nW tensorflow/core/kernels/conv_ops.cc:162] Resource exhausted: OOM when allocating tensor with shapedim { size: 73257 } dim { size: 32 } dim { size: 32 } dim { size: 32 }\nW tensorflow/core/common_runtime/executor.cc:1027] 0x3399fd0 Compute status: Resource exhausted: OOM when allocating tensor with shapedim { size: 73257 } dim { size: 32 } dim { size: 32 } dim { size: 32 }\n     [[Node: Conv2D = Conv2D[T=DT_FLOAT, padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape, Variable)]]\nTraceback (most recent call last):\n  File \"Capstone-project.py\", line 272, in <module>\n    x: testDataX, y_:testDataY , keep_prob: 1.0}))\n  File \"/home/groove/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 405, in eval\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\n  File \"/home/groove/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2728, in _eval_using_default_session\n    return session.run(tensors, feed_dict)\n  File \"/home/groove/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 345, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/home/groove/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 419, in _do_run\n    e.code)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shapedim { size: 73257 } dim { size: 32 } dim { size: 32 } dim { size: 32 }\n     [[Node: Conv2D = Conv2D[T=DT_FLOAT, padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Reshape, Variable)]]\nCaused by op u'Conv2D', defined at:\n  File \"Capstone-project.py\", line 150, in <module>\n    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n  File \"Capstone-project.py\", line 117, in conv2d\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n  File \"/home/groove/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 207, in conv2d\n    use_cudnn_on_gpu=use_cudnn_on_gpu, name=name)\n  File \"/home/groove/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 633, in apply_op\n    op_def=op_def)\n  File \"/home/groove/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1710, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/groove/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 988, in __init__\n    self._traceback = _extract_stack()\n\ngroove@groove-VirtualBox:~/Udacity-Nanodegree-Project5-Capstone$ \n", "comments": ["You're trying to allocate a 4D tensor of 2400485376 elements.  Assuming these are float32, that's about 9GB which is quite large.  It looks like it's for a GPU Op.  How big is the memory on your GPU, and what other tensors are live at the same time?  Did you really manage to train 1750 steps with this tensor allocating at every step?\n", "@poxvoculi  Hi, I have alloted 11GB ram and I am using CPU-only tensorflow. I made it work by reducing dimension by a scale of 6 but the accuracy is around .83 which is not so good\n", "OK.  So it may be possible to reduce your max memory consumption a bit without down-scaling.  Unfortunately I don't know of any specific documentation on this.  We have some future improvements planned which will hopefully make it less necessary and/or easier to do.   \n\nThe fact that your program doesn't run out of memory in the first couple of steps, but runs for some thousands first, suggests that either the OOM condition is the result of an unlucky concurrent execution that exceeds memory limits, or you have variable size Op inputs somewhere, and eventually you hit a minibatch which raises the aggregate live tensor size above your limit.\n\nYou'll need to figure out exactly what your computation graph looks like, i.e. what are the types and sizes of every Op input and output, and what edges connect them together.  There may be a *.pbtxt file generated as a side effect of execution which describes this graph.  Your python program will determine which subgraphs execute by session.run() statements that specify a set of input and output nodes that induce a subgraph.  Wherever two Ops in that subgraph don't have a serial dependency, i.e. one of them does not take as input an edge which is descendant from the other, they are potentially concurrent and may overlap in execution.  When that happens, all of their inputs and outputs are allocated simultaneously.  Knowing the types and sizes, you should be able to count up fairly precisely the memory requirement to execute each Op.  Keep in mind that Vars are always live, and if you're training, some values from the forward computation phase need to live until they're used (again) in the backprop phase.  This should all be explicit in the graph.\n\nIf you have some potential parallelism in the graph that may be causing OOM with an unlucky execution overlap, you can eliminate it by introducing a control edge that prevents one of the potentially overlapping Ops from beginning execution until the other has finished.  See [control dependencies](https://www.tensorflow.org/versions/r0.10/api_docs/python/framework.html#control_dependencies).\n\nIf your OOM problem is due to variable sized inputs, maybe you can change your input minibatch selection somehow to stay within your limits.\n", "I am closing this issue, as it is more of a question appropriate for stackoverflow. If you see  behavior that is a bug, you would need to reduce it to something small that we can reproduce. It seems like this is a case of a model that is too big. Thank you.\n"]}, {"number": 4092, "title": "the learning_rate became nan in seq2seq model", "body": "I use the seq2seq provided by tensorflow, a very curious problem is that the learning_rate will become nan only when the length of the bucket is more than 40 with 4 layers.\nThe learning_rate should be decaied and more and more smaller, why it will became nan?\nThe init learning_rate is 0.01 and the learning_rate_decay_factor is 0.5\nThe training data is nist\nThe training is in a centos with four K40 together.\n", "comments": ["Closing due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 4091, "title": "Add TF_AllocateTensor to version 0.10", "body": "Recently there was [added](https://github.com/tensorflow/tensorflow/commit/ae7b1310c5b2bbb333191d0def7985202dee382a) a very useful function to the C API\u2014namely, `TF_AllocateTensor`. Currently the commit is not present on branch `r0.10`. I\u2019m wondering if it\u2019s possible to cherry-pick the commit to `r0.10` so that the function is available already in version 0.10. Thank you.\n\nRegards,\nIvan \n", "comments": ["@josh11b what do you think? Is this part of the C API completeness?\n", "Looks like that commit didn't make it into 0.10. Don't worry, 0.11 is going to contain it.\n"]}, {"number": 4090, "title": "Make CUDA library version numbers available from python", "body": "Hi!\n\nIs it possible to make the versions of the CUDA libraries available as a python variable? Currently, after importing, there is the following output:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5.1.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally\n```\n\nSo apparently the versioning information is loaded, I just couldn't find an API to access it.\n\nI would like to record the CUDA as meta-information along with my checkpoint and event files, to be able to 100% reproduce any runs.\n", "comments": ["@akors, this information is only known to stream-executor, not TensorFlow. It would take quite a bit of plumbing to set that information to TensorFlow. For your use case, would be sufficient to save the log file itself? \n", "@zheng-xq Well, is there a way for me to get it from stream-executor?\nI guess the log file would work, although it's not an ideal solution. \n", "The log file is better than nothing. Is there at least a way to somehow capture that specific portion of the log, preferably without any stdout/stderr redirections?\n", "maybe add the \"contributions welcome\" label ? seems reasonable to have more info about the stream_executor.  \n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\ndeduping against https://github.com/tensorflow/tensorflow/issues/10827"]}, {"number": 4089, "title": "Bake TF git commit hash into TensorFlow build", "body": "Hi, is it possible to record the Git commit hash, when building from source and making it available as a variable?\n\nCurrently, there is only \n\n`tensorflow.__version__`\n\nwhich is nice, but with a fast-moving codebase is not quite granular enough. Something like \n\n`tensorflow.__commit__` would make each build uniquely identifiable which is important for me to produce reproducible runs.\n", "comments": ["Andrew is working on this.\n", "Now currently available as \"__git_version__\": \n\nExample: \n\n```\n$ python\n>>> import tensorflow\n>>> tensorflow.__git_version__\n'v0.10.0-1576-g754048a-dirty'\n```\n", "Thanks a lot!\n"]}, {"number": 4088, "title": "tf.nn.conv2d_transpose doc fix", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@googlebot, your ability to express gratitude is philosophically disputable.\n", "@tensorflow-jenkins test this please\n", "Looks like this was reverted by https://github.com/tensorflow/tensorflow/commit/0d700eff9860757469b65c3b49b9829803d2a9b2. What happened?", "I think the .md files are automatically generated from:\r\nhttps://github.com/tensorflow/tensorflow/blob/9171cbec195f2f41c55f83d251f5e532a1910580/tensorflow/python/ops/nn_ops.py#L1022"]}, {"number": 4087, "title": "Clearer rank mismatch error message", "body": "`...logits rank (received %s) - 1.`\nThe `- 1` is confusing for readers, can be mistook for em-dash instead of a subtraction. Also made overall wording more reader-friendly\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n"]}, {"number": 4086, "title": "Exception when restore DNN model", "body": "I am using skflow of tensorflow(0.10.0rc0), I create a TensorFlowDNNRegressor model and saved to local, when I try to restore it gives me exception:\nnew_regressor = skflow.TensorFlowEstimator.restore('/Users/yichen.wei/Desktop/xiaoxiang_ml/model') File \"/Users/yichen.wei/project/vscienv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 326, in restore raise ValueError(\"Restore folder doesn't contain model definition.\").\n\nThen I add a file model.def to my model directory and restore again it gives me\nnew_regressor = skflow.TensorFlowEstimator.restore('/Users/yichen.wei/Desktop/xiaoxiang_ml/model') File \"/Users/yichen.wei/project/vscienv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 331, in restore model_def = json.loads(fmodel.read()) File \"/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/json/**init**.py\", line 319, in loads return _default_decoder.decode(s) File \"/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/json/decoder.py\", line 339, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File \"/usr/local/Cellar/python3/3.5.2/Frameworks/Python.framework/Versions/3.5/lib/python3.5/json/decoder.py\", line 357, in raw_decode raise JSONDecodeError(\"Expecting value\", s, err.value) from None json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nIt seems need json model definition in model.def file but it didn't create when saving.\nCould someone help with this issue? Thanks!\n\nPS: dir of saved model :\ncheckpoint\nevents.out.tfevents\ngraph.pbtxt\nmodel.ckpt-xxxx-xxxxx-of-xxxxx\nmodel.ckpt-xxxx.meta\n\nenv: \nOS X EI Capitan 10.11.6\nPython: 3.5.2\ntensorflow: 0.10.0rc0\n", "comments": ["@ispirmustafa can you take a look?\n", "@kinddevil You should use `DNNRegressor` instead of `TensorFlowDNNRegressor` (which is deprecated).\nRestoring is done via providing the same `model_dir` to the constructor.\n"]}, {"number": 4085, "title": "Building Issue: Is it possible to download everything before building from source?", "body": "I am trying to build Tensorflow from source on Ubuntu 16.04 with CUDA 8.0 and CUDNN 5.\n\nHowever, since I am blocked by the firewall, I encountered a bunch of network issues after running `./configure`, for example:\n\n`ERROR: /home/icstpie/lbf/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_form_element_behavior//': Error cloning repository: https://github.com/polymerelements/iron-form-element-behavior.git: cannot open git-upload-pack caused by https://github.com/polymerelements/iron-form-element-behavior.git: cannot open git-upload-pack caused by Connection refused github.com and referenced by '//tensorflow/tensorboard/bower:bower'.`\n`\nERROR: /home/icstpie/lbf/tensorflow/tensorflow/core/platform/default/build_config/BUILD:56:1: no such package '@gif_archive//': Error downloading from http://ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz to /home/icstpie/.cache/bazel/_bazel_root/2510eb67c1daed69b89d219166d8dadf/external/gif_archive: Error downloading http://ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz to /home/icstpie/.cache/bazel/_bazel_root/2510eb67c1daed69b89d219166d8dadf/external/gif_archive/giflib-5.1.4.tar.gz: Expected 721KB, got 294KB and referenced by '//tensorflow/core/platform/default/build_config:platformlib'.`\n`\nERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.`\n\nI think these errors are caused by the command `bazel fetch //...` in the configure file.\n\nSince bazel ignores the proxy environment LD_PRELOAD, I'm wondering that if I can manually download these things with my proxy and put them somewhere so that bazel can find them? I've tried putting the protobuf repository (the first thing bazel will download) in the root directory of tensorflow but bazel still tries to download it anyway.\n\nI'm not sure if there is any workaround to use proxy with bazel. However, since my GPU server has a slow network and I can use faster network elsewhere, it will be nice if I can download these things manually instead of running `bazel fetch //...` on my server.\n", "comments": ["I somehow managed to use a faster network on my server and the error of cloning github repositories just disappeared. I think this error is caused by the slow network.\n\nAnd I also find that shadowsocks+proxychains works fine with bazel when I operate on the server directly rather than login with SSH. I'm not sure if this is the real reason but the downloading part just finished smoothly after these two changes.\n\nI'll close this issue since I have successfully built tensorflow from source. Feel free to reopen if my experience cannot help you with this problem.\n", "OK I have an example of this problem, I have one machine I can run on the internet, and then I have another machine behind the firewall with no internet access where I can build\r\n\r\ncan I run \"bazel fetch **\" where everything possible gets pulled down and then go to the other machine and build\r\n\r\nstopping and pulling one more package is wasting me hours of time."]}, {"number": 4084, "title": "Process hanging when using TF_SessionRun with multiple times the same input", "body": "It seems that if the same input appears multiple times in the inputs argument of TF_SessionRun (from c_api.h) then the TF_SessionRun call never returns.\nThis issue can be reproduced by modifying c_api_test.cc and replacing the line:\n   csession.SetInputs({{feed, Int32Tensor(3)}});\nWith:\n   csession.SetInputs({{feed, Int32Tensor(3)}, {feed, Int32Tensor(3)}});\nAccording to gdb, the process is waiting for a mutex in the RunState destructor from DirectSession.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nnone\n### Environment info\n\nOperating System: Linux 4.4\n\nInstalled version of CUDA and cuDNN: none\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n   008bcaea38815f46804fc3f56492f4dd93837a56\n2. The output of `bazel version`\n   Build label: 0.3.1\n   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n   Build time: Fri Jul 29 09:09:52 2016 (1469783392)\n   Build timestamp: 1469783392\n   Build timestamp as int: 1469783392\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nSee above.\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["Thanks for the report.\n", "In general the C API is unforgiving when it comes to usage, but this should be an easy and cheap piece of validation code to add.\n", "Running tests now... it should appear after the next push.\n"]}, {"number": 4083, "title": "ar", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": []}, {"number": 4081, "title": "Error while running CIFAR-10 example from the site.", "body": "When I run the CIFAR-10 code from [https://www.tensorflow.org/versions/r0.10/tutorials/deep_cnn/index.html#convolutional-neural-networks](here) I get the error\n `argparse.ArgumentError: argument --batch_size: conflicting option string: --batch_size`\n\nI didn't modify anything in the code. Just ran the code with `python cifar10.py`  after cloning the tensorflow repo. Python version is 3.5 , and tensorflow version  0.10.0rc0\n", "comments": ["To execute the sample run `python cifar10_train.py` or `python cifar10_eval.py` instead as suggested on this [stackoverflow thread](http://stackoverflow.com/questions/34734054/tensorflow-argumenterror-running-cifar-10-example).\n"]}, {"number": 4080, "title": "Update deepdream.ipynb", "body": "It would be better if \"from **future** import print_function\" occurred at the beginning of the code.\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 4079, "title": "LeakyReLU uses up too much memory.", "body": "Right now I am implementing leaky relus like this tf.maximum(0.1 \\* x, x).  This works fine except when it comes to memory usage.  Networks which will fit on my GPU when using tf.nn.relu or tf.nn.elu fail when I am using my leaky relu implementation.  I think this is because it needs to store both the intermediate 0.1 \\* x and x values of the activations to compute the gradients which essentially does the memory usage.  However, I do not think this would be an issue if there were a dedicated tf.nn.leaky_relu.  Can someone consider adding this to a future tensorflow release.\n", "comments": ["Try this:\n\n```\ndef lrelu(x, leak=0.2, name=\"lrelu\"):\n     with tf.variable_scope(name):\n         f1 = 0.5 * (1 + leak)\n         f2 = 0.5 * (1 - leak)\n         return f1 * x + f2 * abs(x)\n```\n", "In the future we plan to generate faster, more memory efficient code by automatically fusing operations in some subgraphs, rather than adding lots more specialized Ops.\n"]}, {"number": 4078, "title": "Installation Issue:  Couldn't open CUDA library libcuda.so.1.", "body": "In the step where I have to run\n`bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu`\nI get the following messages:\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH:...\n...\n...\n...\nfailed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n...\n...\n...\nE tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE\n...\n...\n...\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:81] No GPU devices available on machine.\n### Environment info\n\nOperating System: Debian 8\n\nInstalled version of CUDA and cuDNN: CUDA 7.0, cuDNN 4.0.7\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n/usr/local/cuda/lib/libcudadevrt.a\n/usr/local/cuda/lib/libcudart.so -> libcudart.so.7.0\n/usr/local/cuda/lib/libcudart.so.7.0 -> libcudart.so.7.0.28\n/usr/local/cuda/lib/libcudart.so.7.0.28\n/usr/local/cuda/lib/libcudart_static.a\n1. The commit hash (`git rev-parse HEAD`) \n   `554ddd9ad2d4abad5a9a31f2d245f0b1012f0d10`\n2. The output of `bazel version`\n   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n   Build time: Thu Jan 01 00:00:00 1970 (0)\n   Build timestamp: Thu Jan 01 00:00:00 1970 (0)\n   Build timestamp as int: 0\n\nP.S. I have two GPUs installed on my server. One is used for GUI (AMD) and the other is used for computation (NVIDIA).\nShould I do anything to make sure the second GPU is being used?\n", "comments": ["Did you check on your system if libcuda.so.\\* does exist ? On my system it is in /usr/local/nvidia/lib64 and another of my system it is in /usr/lib/.\n\nIf it does exist, then set the LD_LIBRARY_PATH env variable to point to the correct path. If it doesn't you might've to install it on your system.\n\nCuda should not recognize the AMD device as a valid cuda target, so you should run on the right GPU.\n", "I have libcuda.so.7, but not libcuda.so.1\n\nShould I reinstall cuda drivers?\n", "You could just ln -s libcuda.so.7.\\* libcuda.so.1 and see if it works.\n", "I tried this solution. Now I get the following messages:\n\n`E tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUresult(-1)`\n\n...\n\n`I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program`\n\n`I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:356] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  340.96  Sun Nov  8 22:33:28 PST 2015\n`\n", "@MahdiNazemi  Do you solve this problem? I also face the same situation. Thanks!\n", "@chocolate9624 I still have this issue. Please let me know if you find a solution.\n", "@chocolate9624 Could you find any solutions to the problem? I'm still looking for a solution...\n", "Your driver (kernel module) is really old: 340.96. Can you upgrade to a more recent driver/cuda? \n", "I updated to the latest version on jessie-backports which is 352.79, but it didn't solve the issue.\n", "@MahdiNazemi  I solved this problem by comparing with other machines.  In /use/lib64 \n\nlibcuda.so -> /usr/lib64/libcuda.so.1\nlibcuda.so.1 -> /usr/lib64/nvidia/libcuda.so.1\n\nlibcuda.so.1 should be linked to   /usr/lib64/nvidia/libcuda.so.1\n\nGood luck!\n", "Thanks a lot @chocolate9624.\n\nFinally, I was able to solve the issue. It looks like none of the drivers that are available on Debian (jessie, jessie-backports, and stretch) can make the GPU work (not my Quadro nor my Tesla). I used the driver that is available on Nvidia's website and it solved the problem.\n", "That's good to know. Is it an issue with the driver version, or about where they install to?\n", "I installed 3 different versions, i.e. 340.96, 352.79, and 367.44, but none of them worked. I don't think the problem is with the driver version, but I'm not sure if it's about where they are installed or not.\n", "Hi, I have solved it by these steps:\n1.`$ sudo apt install nvidia-361-dev`\n2.`$ sudo find /usr/ -name 'libcuda.so.1'` (then you will know path of `libcuda.so.1`)\n3. just copy the `libcuda.so.1` to `/usr/local/cuda/lib/` \n_actually the `libcuda.so.1` you find is a link file, see:_\n![image](https://cloud.githubusercontent.com/assets/8605990/19565023/13815d58-9718-11e6-8179-74433464ee0a.png)\n", "@ShiKaiWi Which repo does the nvidia-361-dev come from? I am seeing following when I try to install it.\n\nE: Unable to locate package nvidia-361-dev\n", "@soichih Hi, my os is ubuntu 16.04 so this repo is provided by default. So I suggest you can try to install nvidia-3**-dev (I guess the  version is 352 if you use ubuntu 14.04) and then try to find the `libcuda.so.1`.\n", "Looks resolved. Thanks all.\n", "> Hi, I have solved it by these steps:\r\n> 1.`$ sudo apt install nvidia-361-dev`\r\n> 2.`$ sudo find /usr/ -name 'libcuda.so.1'` (then you will know path of `libcuda.so.1`)\r\n> 3. just copy the `libcuda.so.1` to `/usr/local/cuda/lib/`\r\n> _actually the `libcuda.so.1` you find is a link file, see:_\r\n> ![image](https://cloud.githubusercontent.com/assets/8605990/19565023/13815d58-9718-11e6-8179-74433464ee0a.png)\r\n\r\nit works for me", "hello sir i make mistake and accidentally delete the libcuda.so.1 like this :\r\n![image](https://user-images.githubusercontent.com/52781140/62705467-ac586880-ba17-11e9-861b-6fc33bc5a625.png)\r\n\r\nhow can i solve this problem ?\r\nor is that red icon mean deleted ? i'm not really sure", " Directory of C:\\Windows\\System32\\lxss\\lib\r\n\r\n09/18/2020  03:53 PM    <DIR>          .\r\n08/30/2020  09:51 AM           124,664 libcuda.so\r\n09/12/2020  08:44 AM           832,936 libd3d12.so\r\n09/12/2020  08:44 AM         5,073,944 libd3d12core.so\r\n09/12/2020  08:44 AM        25,069,816 libdirectml.so\r\n09/12/2020  08:44 AM           878,768 libdxcore.so\r\n08/30/2020  09:51 AM        40,496,936 libnvwgf2umx.so\r\n               6 File(s)     72,477,064 bytes\r\n               1 Dir(s)  643,723,309,056 bytes free\r\n\r\nC:\\Windows\\System32\\lxss\\lib>mklink libcuda.so.1 libcuda.so\r\nsymbolic link created for libcuda.so.1 <<===>> libcuda.so\r\n", "> hello sir i make mistake and accidentally delete the libcuda.so.1 like this :\r\n> ![image](https://user-images.githubusercontent.com/52781140/62705467-ac586880-ba17-11e9-861b-6fc33bc5a625.png)\r\n> \r\n> how can i solve this problem ?\r\n> or is that red icon mean deleted ? i'm not really s\r\n\r\n> hello sir i make mistake and accidentally delete the libcuda.so.1 like this :\r\n> ![image](https://user-images.githubusercontent.com/52781140/62705467-ac586880-ba17-11e9-861b-6fc33bc5a625.png)\r\n> \r\n> how can i solve this problem ?\r\n> or is that red icon mean deleted ? i'm not really sure\r\n\r\nCould you manage to solve that issue? Because my libcudart.so is also in red color .\r\nThanks in advance!", "where I can find those on Mac ? \r\nlibcuda.so -> ?\r\nlibcuda.so.1 ->?"]}, {"number": 4077, "title": "modify scatter_update to hack variable initialization check", "body": "Fixes #3373 \n", "comments": ["Can one of the admins verify this patch?\n", "no tests? Also, it looks very hacky in the way it forces initialization.\n", "I'll add test. I have tested on cpu, and it truly reduced memory cost. I find that if use `sess.run(data.initializer)`, memory cost doubles because init_variable will stay in memory.\n", "@suiyuan2009 Thanks a lot for the PR and sorry for the late response. There is lot of good work gone into this CL. However, a kernel initializing an input ref tensor has no precedent elsewhere in TensorFlow. Also, adding @alextp to comment as he might have more insight into this.\n", "@suiyuan2009 Thanks for writing this!\n\nIt sounds like the problem you're trying to fix here is that the memory usage for zero-initializing variables is bigger than what it takes to store the variable. Is that it?\n\nIf so, I wonder if there's a better way of doing it than hacking scatter_add.\n", "@alextp modifying tf.Variable is a choice.\n", "@suiyuan2009 another option is to make a ZeroInitialize op which allocates storage and zero-initializes it (the current assign op will allocate storage but it needs the zero-tensor to exist).\n\n@mrry @keveman  do you think it makes sense to support this?\n", "@alextp I think your suggestion of a ZeroInitializer op makes more sense than adding a flag to the individual assign ops. I'd be inclined to keep it in `tf.contrib` because it seems specific to the current implementation of variables, it's not fundamental, and we will probably want to deprecate it once there's no longer a need for it.\n\nLonger term, it would be great if we could identify points in the graph where a tensor buffer is expiring, so we could replace the copy in `tf.assign()` with a move, but in the short term this should work.\n", "@suiyuan2009 do you want to implement this ZeroInitialize op? I expect it\nwill be quite similar to what you have here in the end.\n\nOn Wed, Sep 7, 2016 at 9:42 AM, Derek Murray notifications@github.com\nwrote:\n\n> @alextp https://github.com/alextp I think your suggestion of a\n> ZeroInitializer op makes more sense than adding a flag to the individual\n> assign ops. I'd be inclined to keep it in tf.contrib because it seems\n> specific to the current implementation of variables, it's not fundamental,\n> and we will probably want to deprecate it once there's no longer a need for\n> it.\n> \n> Longer term, it would be great if we could identify points in the graph\n> where a tensor buffer is expiring, so we could replace the copy in\n> tf.assign() with a move, but in the short term this should work.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4077#issuecomment-245342032,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAATxUGcDQo6ifSM9-uT_CYwDn4eCo3dks5qnulcgaJpZM4JuwXP\n> .\n\n## \n- Alex\n", "@alextp I'll implement it.\n", "@alextp ZeroInitializerOp is added.\n", "@alextp I moved all codes to tf.contrib.framework, and modified some code as you advised. I don't find a way to track memory usage of tensorflow in python, maybe the allocator in C++ can do it.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Can you rebase this with master? I see two conflicts, on contrib/.../variables.py and contrib/.../variables_test.py\n", "@alextp conflict solved.\n", "Jenkins, please test this change.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "@gunan do you understand these test failures? Why is a deptest only failing on linux gpu? Is Jenkins building from a merge of master and this branch or from the head of this branch? I'd like to merge this PR but I don't know how to make it work.\n", "@keveman can you help with the build issue?\n", "ERROR: /workspace/tensorflow/contrib/framework/BUILD:38:1: in check_deps rule //tensorflow/contrib/framework:python/ops/_variable_ops.so_check_deps: \nTraceback (most recent call last):\n    File \"/workspace/tensorflow/contrib/framework/BUILD\", line 38\n        check_deps(name = 'python/ops/_variable_ops.so_check_deps')\n    File \"/workspace/tensorflow/tensorflow.bzl\", line 669, in _check_deps_impl\n        fail(_dep_label(input_dep) + \" cannot...))\ntensorflow/contrib/framework/kernels:zero_initializer_op cannot depend on tensorflow/core:framework.\n\nhere is the actual failure message.\nDoes this help about the issue any better?\n", "@gunan I did find this error message, but I don't understand why only the GPU build is doing this dependency check, and where was it added. Any clues?\n", "@alextp I find `def tf_custom_op_library` in `tensorflow/tensorflow.bzl` has this deps check, and `//tensorflow/core:framework` in its `disallowed_deps` list. I use `tf_custom_op_library` rule in `tensorflow/contrib/framework/BUILD`, but my `tensorflow/contrib/framework/kernels:zero_initializer_op` does not depend on `tensorflow/core:framework` but `tensorflow/core:framework_headers_lib`.\n\nI find `:framework_headers_lib`  depends on `:framework`...\n", "@alextp works now.\n", "Jenkins, test this please.\n", "@alextp sorted now.\n", "Jenkins test this please.\n\nOn Sep 22, 2016 12:15, \"Ziming Dong\" notifications@github.com wrote:\n\n> @alextp https://github.com/alextp sorted now.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4077#issuecomment-248873534,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAATxS0_Z0XR-UgE2reE0R9pDaReKEkfks5qsmNCgaJpZM4JuwXP\n> .\n", "Jenkins, test this please.\n", "@suiyuan2009 can you merge with master? It seem like this picked a version which fails some unrelated tests. I need the tests to pass before I can merge this PR.\n", "@alextp I did a `git rebase`, so this is supposed to be the latest.\n", "@alextp I configured python path to python3 and run bazel test the-target, everything is ok.\n", "Jenkins, test this please.\n", "Looking at the jenkins status I think tensorflow master might be broken. @m3bm3b as the buildcop, am I right or did I misread something?\n", "(and I suspect things are broken because https://ci.tensorflow.org/view/Tensorflow%20Jenkins%20Monitored%20builds/ has red in suspicious places)\n", "It certainly looks suspicious to me.\n\nI will look into it..\n\nMike\n\nOn 22 September 2016 at 10:43, Alexandre Passos notifications@github.com\nwrote:\n\n> (and I suspect things are broken because https://ci.tensorflow.org/\n> view/Tensorflow%20Jenkins%20Monitored%20builds/ has red in suspicious\n> places)\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4077#issuecomment-248974836,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AJsp2TFa3LuLGvtfIihowQIx2bU4HaHtks5qsr5XgaJpZM4JuwXP\n> .\n", "Opensource tests are triaged by another rotation.\nAdding @martinwicke FYI.\n", "Jenkins, test this please.\n", "@alextp can you see the build log, I used another machine to build and test, and it failed test, the log says `Error in '/usr/bin/python': double free or corruption`\n", "test passed on my master branch, but failed on this pr branch, error msg is\n\n```\n/tensorflow/bazel-out/local-py3-opt/testlogs/tensorflow/contrib/learn/random_forest_test/test.log\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n-----------------------------------------------------------------------------\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmprlp8hgwp\nWARNING:tensorflow:Using default config.\nWARNING:tensorflow:Given features: Tensor(\"input:0\", shape=(?, 4), dtype=float32), required signatures: TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(4)]), is_sparse=False).\nWARNING:tensorflow:Given targets: Tensor(\"output:0\", shape=(?,), dtype=float32), required signatures: TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None)]), is_sparse=False).\n/home/zido/.cache/bazel/_bazel_zido/374c60a9747170d275c203f96a981f83/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/learn/random_forest_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/graph_actions.py:778: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n  logging.warn('Input iterator is exhausted: %s.', e)\nWARNING:tensorflow:Input iterator is exhausted: .\n.WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp5qucovm4\nWARNING:tensorflow:Using default config.\nWARNING:tensorflow:Given features: Tensor(\"input:0\", shape=(?, 13), dtype=float32), required signatures: TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(13)]), is_sparse=False).\nWARNING:tensorflow:Given targets: Tensor(\"output:0\", shape=(?,), dtype=float32), required signatures: TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None)]), is_sparse=False).\nWARNING:tensorflow:Input iterator is exhausted: .\n..\n----------------------------------------------------------------------\nRan 3 tests in 17.033s\n\nOK\n*** Error in `/usr/bin/python3': double free or corruption (!prev): 0x000000000171c350 ***\n```\n", "I run `gdb --args python3 tensorflow/contrib/quantization/python/dequantize_op_test.py`, get\n\n```\n----------------------------------------------------------------------\nRan 3 tests in 0.070s\n\nOK\n\nProgram received signal SIGSEGV, Segmentation fault.\n0x00007ffff787601f in _int_free (av=0x7ffff7bb5760 <main_arena>, p=<optimized out>, have_lock=0) at malloc.c:3996\n3996    malloc.c: No such file or directory.\n```\n", "@alextp I think it's ok now. It seems `load_op_library` has some problem with `test_session`, but I can't figure it out.\n", "Jenkins, test this please.\n", "`Android Demo App` test has a download problem.\n", "Jenkins, test this please.\n", "I do not see any error, It simply fails when I configure for GPU, and run:\n$ bazel test tensorflow/contrib/framework:variables_test -c opt\n--config=cuda\n\nOn Sat, Oct 1, 2016 at 9:05 PM, Ziming Dong notifications@github.com\nwrote:\n\n> ## _@suiyuan2009_ commented on this pull request.\n> \n> In tensorflow/contrib/framework/python/ops/variables_test.py\n> https://github.com/tensorflow/tensorflow/pull/4077:\n> \n> > -    var = tf.Variable(initializer)\n> > -    var_zero = tf.contrib.framework.zero_initializer(var)\n> > -    with self.test_session() as sess:\n> > -      with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n> > -        var.eval()\n> > -      if use_init:\n> > -        sess.run(var.initializer)\n> > -        with self.assertRaisesOpError(\"input is already initialized\"):\n> > -          var_zero.eval()\n> > -        self.assertAllClose(np.ones(shape), var.eval())\n> > -      else:\n> > -        var_zero.eval()\n> > -        self.assertAllClose(np.zeros(shape), var.eval())\n> >   +\n> > -  def testZeroInitializer(self):\n> > -    for dtype in (tf.int32, tf.int64, tf.float32, tf.float64):\n> \n> What's the error?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4077, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCOdln7PFHFtuDvcKzuodHX5G9Ciwtks5qvy1ygaJpZM4JuwXP\n> .\n", "Any nightly GPU build which runs this test is also broken:\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac1-slave/79/console\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac1-slave/79/console\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/247/console\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/247/console\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/247/console\n\nit is not even flaky, it only runs on pip tests, and it fails on every\nsingle run on GPU.\nThis test is omitted from no-pip runs on jenkins. That is how we missed the\nbreakage before.\n\nOn Sat, Oct 1, 2016 at 9:30 PM, Gunhan Gulsoy gunan@google.com wrote:\n\n> I do not see any error, It simply fails when I configure for GPU, and run:\n> $ bazel test tensorflow/contrib/framework:variables_test -c opt\n> --config=cuda\n> \n> On Sat, Oct 1, 2016 at 9:05 PM, Ziming Dong notifications@github.com\n> wrote:\n> \n> > ## _@suiyuan2009_ commented on this pull request.\n> > \n> > In tensorflow/contrib/framework/python/ops/variables_test.py\n> > https://github.com/tensorflow/tensorflow/pull/4077:\n> > \n> > > -    var = tf.Variable(initializer)\n> > > -    var_zero = tf.contrib.framework.zero_initializer(var)\n> > > -    with self.test_session() as sess:\n> > > -      with self.assertRaisesOpError(\"Attempting to use uninitialized value\"):\n> > > -        var.eval()\n> > > -      if use_init:\n> > > -        sess.run(var.initializer)\n> > > -        with self.assertRaisesOpError(\"input is already initialized\"):\n> > > -          var_zero.eval()\n> > > -        self.assertAllClose(np.ones(shape), var.eval())\n> > > -      else:\n> > > -        var_zero.eval()\n> > > -        self.assertAllClose(np.zeros(shape), var.eval())\n> > >   +\n> > > -  def testZeroInitializer(self):\n> > > -    for dtype in (tf.int32, tf.int64, tf.float32, tf.float64):\n> > \n> > What's the error?\n> > \n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > https://github.com/tensorflow/tensorflow/pull/4077, or mute the thread\n> > https://github.com/notifications/unsubscribe-auth/AHlCOdln7PFHFtuDvcKzuodHX5G9Ciwtks5qvy1ygaJpZM4JuwXP\n> > .\n", "@gunan I'll test on my local machine to see what's wrong.\n", "Sent out a proposed fix for it internally (did it there because it's easier to test). Don't worry about it :)\n\nEssentially just passed the Device as a parameter and changed it to:\n`auto out_flat = out_tensor->flat<T>();`\n`out_flat.device(ctx->eigen_device<Device>()) = out_flat.constant(T(0));`\n", "@jhseu Thank you~\n"]}, {"number": 4076, "title": "Gradients error when using while_loop: \"ValueError: None values not supported\"", "body": "Working on Tensorflow version 0.10, I am getting the following error when using `tf.gradients`:\n\n```\n.../python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 478, in gradients\n...\n.../python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 346, in make_tensor_proto\n    raise ValueError(\"None values not supported.\")\n```\n\nThe error occurs due to the use of the function `tf.while_loop` . When I turn off its use, the gradients are calculated with no error.\n\nA similar issue was related to in https://github.com/tensorflow/tensorflow/issues/783, but I can't fix the error using the suggested solutions.\n", "comments": ["We'll need more information.  What are you doing?\n"]}, {"number": 4075, "title": "Error while using makefile", "body": "Hello. I  tried to us TF using the method described [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile)  on Linux. \n\nAfter installin protobuf ( as instructed),  I get these two errors after using the last command ( make -f tensorflow/contrib/makefile/Makefile) \n\n> tensorflow/core/util/example_proto_fast_parsing.cc:177:38: error: invalid conversion from \u2018tensorflow::uint64\\* {aka long long unsigned int_}\u2019 to \u2018google::protobuf::uint64_ {aka long unsigned int_}\u2019 [-fpermissive]\n>            if (!stream.ReadVarint64(&n)) return false;\n> and\n> tensorflow/core/util/example_proto_fast_parsing.cc:186:38: error: invalid conversion from \u2018tensorflow::uint64_ {aka long long unsigned int_}\u2019 to \u2018google::protobuf::uint64_ {aka long unsigned int*}\u2019 [-fpermissive]\n>            if (!stream.ReadVarint64(&n)) return false;\n\nBTW: the commit hash I am using is: f9ae4fe749c5d7dbf1851cffc94730086e588c35\nand CUDA isn't installed. \n", "comments": ["Sorry you're hitting problems! Could you give me some more details on the Linux version you're using?\n\nThe issue looks like it's a mismatch between the protobuf version TensorFlow's being compiled with, and the one you have installed on the system.\n", "I am currently on a ubuntu 16.04.  \n", "I've updated the makefile to include a build_all_linux.sh script that compiles the protobuf locally, rather than using the system-installed version. I believe this should fix this problem you're hitting, can you give it a try and let me know?\n", "Closing this out due to inactivity. @AlperenAydin : feel free to reopen if your issue hasn't been resolved with Pete's changes above.\n"]}, {"number": 4074, "title": "Gradients Not Being Computed Correctly While Using tf.contrib.distributions.Categorical", "body": "Hello, there appears to be an issue with how TensorFlow gradients are being computed while using Categorical in the graph.  In particular, let's say we were computing the gradient through a single logit of a vector, we would expect only the corresponding column of the weight matrix affecting that logit value to update with gradients. This is indeed the case when I keep this index fixed, say `tf.constant(0, dtype=tf.int32)`, however, this is not the case while using Categorical.  The code below should clarify further.\n\nThanks!\nLiam\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nI have not found this particular bug in a quick search.\n### Environment info\n\nOperating System:  Ubuntu 14.04.4 LTS\n\nInstalled version of CUDA and cuDNN:  \n-rw-r--r-- 1 root root 189170 Mar 17 17:29 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Mar 17 17:29 /usr/local/cuda/lib/libcudart_static.a\n\nTensorFlow from source.\n1.  Commit hash:  1b50845ff01200b3f6fc78a2780df49baea674ff\n2.  bazel version:\n\nBuild label: 0.2.2b\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Mon Apr 25 08:08:53 2016 (1461571733)\nBuild timestamp: 1461571733\nBuild timestamp as int: 1461571733\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n```\nimport tensorflow as tf\nfrom tensorflow.contrib.distributions import Categorical\ntf.reset_default_graph()\n\ninput_dim = 3\nhidden_dim = 5\noutput_dim = 3\nlr = 1e-1\nnum_iterations = 50\nprint_every = 1\n\nx = tf.fill([1, input_dim], 1.)\ny = tf.fill([1, output_dim], 1.)\n\nwith tf.name_scope('Model'):\n    W_gen = tf.Variable(tf.random_uniform([input_dim, hidden_dim]), name = 'W_gen')\n    logits = tf.matmul(x, W_gen)\n\n    sample_op = tf.stop_gradient(Categorical(logits).sample(n=1))\n    index = tf.squeeze(sample_op)\n    one_hot = tf.one_hot(index, hidden_dim, dtype = tf.float32)\n    logits = logits * one_hot\n\n    W_dis = tf.Variable(tf.random_uniform([hidden_dim, output_dim]), name = 'W_dis')\n    output = tf.matmul(logits, W_dis)\n\n\nwith tf.name_scope('Loss'):\n    loss_op = tf.reduce_mean(tf.squared_difference(output, y))\n\nwith tf.name_scope('Train'):\n    train_vars = [W_gen]\n    train_op = tf.train.AdamOptimizer(lr).minimize(loss_op, var_list = train_vars)\n\n\nwith tf.Session() as sess:\n    init_op = tf.initialize_all_variables()\n    sess.run(init_op)\n\n    for i in xrange(num_iterations):\n        if i % print_every == 0:\n            print('Loss at iteration %d: %f' % (i, sess.run(loss_op)))\n            print('Sample: [%d]' % sess.run(index))\n            print sess.run(W_gen)\n        sess.run(train_op)\n    print sess.run(output)\n```\n### Logs or other output that would be helpful\n\nHere I show you the Sample generated by Categorical and then the corresponding change to the weight matrix.  Notice, that the first training step operates as expected, only the '1th' column is affected. \n\nHowever, the second training step both column 1 and 2 change.  \n\n```\nSample: [1]\n[[ 0.81418228  0.8655349   0.16064     0.55864608  0.35103011]\n [ 0.26089203  0.69035411  0.64020491  0.02805829  0.99758911]\n [ 0.56620026  0.52124786  0.23499095  0.59907818  0.44014001]]\n\nSample: [1]\n[[ 0.81418228  0.76553494  0.16064     0.55864608  0.35103011]\n [ 0.26089203  0.59035414  0.64020491  0.02805829  0.99758911]\n [ 0.56620026  0.4212479   0.23499095  0.59907818  0.44014001]]\n\nSample: [0]\n[[ 0.81418228  0.69852936  0.23505335  0.55864608  0.35103011]\n [ 0.26089203  0.52334857  0.71461827  0.02805829  0.99758911]\n [ 0.56620026  0.35424232  0.30940431  0.59907818  0.44014001]]\n```\n", "comments": ["This contrib package is not strongly supported, so a community provided solution would be very welcome.  @langmore do you know any more?\n", "Since the Categorical probabilities must sum to 1, a change in any one logit effects the probability of all output classes.  Does that explain what you're seeing?\n", "Thanks for the respone! @langmore, I don't think that could explain this particular problem.  So I agree that the Categorical probabilities must all sum to 1.0, however, in this instance, I am only selecting one of those values to be used for backpropagation, so only that particular column should update.  The normalization of the distribution that the value was drawn should not affect this at all.  I'll see if I can come up with a fix and contribute it!\n", "A few explanations here, and I think everything is working as intended:\n1. Multiple uses of session.run - this means the Sample: [0] that you print is not necessarily the one that you are will use for training. That in itself doesn't explain why multiple columns would change for a given iteration.\n2. You are using the adam optimizer, which uses momentum (i.e. the update of this iteration is some fusion of previous updates plus the current gradient step). This explains why a single column changes on the first iteration, two change on the next, etc.\n\ntf.Print is your friend, as is tf.gradients. Modifying your code to work with 0.10, I can see that the gradients are single-column only (good). This is what gave me the hint about momentum.\n\n```\nimport tensorflow as tf\ntf.reset_default_graph()\n\ninput_dim = 3\nhidden_dim = 5\noutput_dim = 3\nlr = 1e-1\nnum_iterations = 50\nprint_every = 1\n\nx = tf.fill([1, input_dim], 1.)\ny = tf.fill([1, output_dim], 1.)\n\nwith tf.name_scope('Model'):\n    W_gen = tf.Variable(tf.random_uniform([input_dim, hidden_dim]), name = 'W_gen')\n    logits = tf.matmul(x, W_gen)\n    sample_op = tf.stop_gradient(tf.contrib.distributions.Categorical(logits).sample_n(1))\n    index = tf.squeeze(sample_op)\n    one_hot = tf.one_hot(index, hidden_dim, dtype = tf.float32)\n    logits = logits * one_hot\n    W_dis = tf.Variable(tf.random_uniform([hidden_dim, output_dim]), name = 'W_dis')\n    output = tf.matmul(logits, W_dis)\n\nwith tf.name_scope('Loss'):\n    loss_op = tf.reduce_mean(tf.squared_difference(output, y))\n\nwith tf.name_scope('Train'):\n    train_vars = [W_gen]\n    train_op = tf.train.GradientDescentOptimizer(lr).minimize(loss_op, var_list = train_vars)\n\n\nwith tf.Session() as sess:\n    init_op = tf.initialize_all_variables()\n    sess.run(init_op)\n    for i in xrange(num_iterations):\n        if i % print_every == 0:\n          loss, ix, Wg, grads, _ = sess.run([loss_op, index, W_gen, tf.gradients(loss_op, [W_gen]), train_op])\n          print('Loss at iteration %d: %f' % (i, loss))\n          print('Sample: [%d]' % ix)\n          print(\"grads: %s\" % grads)\n          print Wg\n        else:\n          sess.run(train_op)\n    print sess.run(output)\n```\n\nSwitch to GradientDescentOptimizer if you want to see only a single column updated.\n", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow.", "@liamb315 Any updates?"]}, {"number": 4073, "title": "Trouble running Tensorflow on Xcode 8 beta 5", "body": "I get the following message, I don't know if this will be fixed in an upcoming version or if I need to add more configuration (it's running fine on the latest Xcode 7 though):\n\n```\nUndefined symbols for architecture arm64:\n  \"_deflate\", referenced from:\n      tensorflow::io::ZlibOutputBuffer::Deflate(int) in libtensorflow-core.a(zlib_outputbuffer.o)\n  \"_inflate\", referenced from:\n      tensorflow::io::ZlibInputBuffer::Inflate() in libtensorflow-core.a(zlib_inputbuffer.o)\n  \"_deflateInit2_\", referenced from:\n      tensorflow::io::ZlibOutputBuffer::ZlibOutputBuffer(tensorflow::WritableFile*, int, int, tensorflow::io::ZlibCompressionOptions const&) in libtensorflow-core.a(zlib_outputbuffer.o)\n  \"_inflateEnd\", referenced from:\n      tensorflow::io::ZlibInputBuffer::~ZlibInputBuffer() in libtensorflow-core.a(zlib_inputbuffer.o)\n      tensorflow::io::ZlibInputBuffer::~ZlibInputBuffer() in libtensorflow-core.a(zlib_inputbuffer.o)\n  \"_deflateEnd\", referenced from:\n      tensorflow::io::ZlibOutputBuffer::Close() in libtensorflow-core.a(zlib_outputbuffer.o)\n  \"_inflateInit2_\", referenced from:\n      tensorflow::io::ZlibInputBuffer::ZlibInputBuffer(tensorflow::RandomAccessFile*, unsigned long, unsigned long, tensorflow::io::ZlibCompressionOptions const&) in libtensorflow-core.a(zlib_inputbuffer.o)\nld: symbol(s) not found for architecture arm64\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\n```\n", "comments": ["That looks like we might need to add 'zlib' to the list of libraries to link against. On Linux, you can just add -lz to the linker command line, but I'm not sure how to do that with Xcode.\n", "I believe I've got this fixed now in the latest code. Can you give this a try and let me know?\n", "Now I get the error \n\n`/Users/Kevin/Developer/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:10: 'unsupported/Eigen/CXX11/Tensor' file not found`\n\non\n\n`#include \"unsupported/Eigen/CXX11/Tensor\"`\n", "@KevinAyuque  \r\nDid you fix your first problem \r\n`ld:symbol(s) not found for architecture arm64`? \r\nI have the same error now.", "Is this still broken with the latest version?", "Closing due to inactivity. Feel free to re-open if you would like us to look again.", "I have the the same error with the latest code, any body have a fix?", "@liliang027 getting the same error, could you fix it? :D"]}, {"number": 4072, "title": "relatively slower performance on FCN", "body": "according to the paper http://arxiv.org/abs/1608.07249,\nTensorFlow is good at CNN and RNN, however relatively slower on FCN, is this because of using python loop to feed data each mini-batch? Can we solve this since FCN is always important in general use of Deep Learning like competitions in kaggle.\n\n![image](https://cloud.githubusercontent.com/assets/11470826/18024624/9230e594-6c41-11e6-989e-f5a7ba9e927f.png)\n\n![image](https://cloud.githubusercontent.com/assets/11470826/18024628/9e3ad430-6c41-11e6-91b9-89347dabc1ce.png)\n\n![image](https://cloud.githubusercontent.com/assets/11470826/18024630/b0d5abd8-6c41-11e6-8584-877b5fa45d88.png)\n", "comments": ["We are very interested in improving TF performance on the kinds of models users care about. \n\nFor the models in this paper it seems unlikely that python overhead for initiating each minibatch is a major factor.  It's more likely that the performance differences are primarily due to the efficiency of the kernels in use for the expensive operations (matmul, for FCN), and the degree to which operator fusion is applied to compile away the overhead of launching individual ops, and save on memory bandwidth.   I notice that in the chart you posted, TF's relative performance is best for CPUs with lots of cores, which corresponds to the kind of CPU platform used at Google.  I'm a little surprised at the cited relatively poor performance for FCNs on GPUs, given that the paper claims all systems are using cuDNN.  \n\nIn the future we intend to apply operator fusion more widely and continue to improve the kernel libraries used.\n", "@fayeshine you could try `tf.while_loop` and the latest XLA compiler. Closing for now since it's been a while and it's not clear how the new code still behaves. Feel free to reopen."]}, {"number": 4071, "title": "\"Default session has been garbage collected\"", "body": "The following three lines give an error: \"RuntimeError: Default session has been garbage collected.\" One line above the line that raises the RuntimeError, there's a comment saying \"This should never happen with the current session implementations.\" Clearly, it did happen. The situation doesn't bother me greatly, and I'm only mentioning this because clearly something unintended happened here. \n\nimport tensorflow as tf\nwith tf.Session().as_default():\n    tf.get_default_session()\n\nThe following four lines do not give the error:\n\nimport tensorflow as tf\nsess = tf.Session()\nwith sess.as_default():\n    tf.get_default_session()\n\nThis suggests that the Session.as_default() method doesn't store <self> in any way that keeps it from being garbage collected.\n\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nNone.\n### Environment info\n\nOperating System:\n\nMac OS X 10.11.6\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nNone.\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed: I don't recall the link; I installed following the standard installation instructions.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n0.10.0rc0\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nSee above.\n### What other attempted solutions have you tried?\n\nSee above.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["Looks like an easy fix. It's in review now and should appear in the next push.\n", "Sweet :)\n", "still unsolved ?", "@MakeWater Please open a new issue with a reproduction if you're still running into this issue. Thanks!"]}, {"number": 4070, "title": "How to install tensorflow on the NVIDIA Jetson TK1 developer kit?", "body": "We are trying to install tensorflow on the [NVIDIA Jetson TK1 developer kit](http://www.nvidia.com/object/jetson-tk1-embedded-dev-kit.html).\n\nThe platform has ..\n\na GPU\n[Tegra-K1](http://www.nvidia.com/object/tegra-k1-processor.html)\n\na 64-bit ARM CPU\n[NVIDIA 4-Plus-1\u2122 Quad-Core ARM\u00ae Cortex\u2122-A15 CPU](http://www.nvidia.com/docs/io/116757/nvidia_quad_a15_whitepaper_finalv2.pdf)\n\nubuntu 14\n(Note: It originally came installed with a 32-bit version of ubuntu 14, but this caused problems so we installed the 64-bit version.)\n64-bit version of ubuntu 14\n\n```\nubuntu@tegra-ubuntu:~/grpc-java3/grpc-java$ java -version\njava version \"1.8.0_101\"\nJava(TM) SE Runtime Environment (build 1.8.0_101-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode)\n\nubuntu@tegra-ubuntu:~/grpc-java3/grpc-java$ uname -a\nLinux tegra-ubuntu 3.10.96-tegra #1 SMP PREEMPT Tue May 17 16:29:05 PDT 2016 aarch64 aarch64 aarch64 GNU/Linux\n\nubuntu@tegra-ubuntu:~/grpc-java3/grpc-java$ lsb_release -d\nDescription:    Ubuntu 14.04.5 LTS\nubuntu@tegra-ubuntu:~/grpc-java3/grpc-java$ \n\nubuntu@tegra-ubuntu:~/grpc-java3/grpc-java$ ldconfig -p | grep cuda\n    libicudata.so.52 (libc6,AArch64) => /usr/lib/aarch64-linux-gnu/libicudata.so.52\n    libcuda.so.1 (libc6,AArch64) => /usr/lib/aarch64-linux-gnu/tegra/libcuda.so.1\n    libcuda.so (libc6,AArch64) => /usr/lib/aarch64-linux-gnu/libcuda.so\n    libcuda.so (libc6,AArch64) => /usr/lib/aarch64-linux-gnu/tegra/libcuda.so\n```\n\nBecause it is an ARM processor rather than an X86 processor, we have to build from source.\n\nThe problem we are having is \nthe tensorflow build requires the protoc plugin for gRPC Java\nbut we are having difficulties compiling the protoc plugin for gRPC Java on this platform.\nthe grpc-java team does not support arm64.\n[they suggested editing the gradle script ...](https://github.com/grpc/grpc-java/issues/2202)\n\n[We\u2019re having problems trying to do so ...](https://discuss.gradle.org/t/tool-chain-gcc-gnu-gcc-dont-know-how-to-build-for-platform-linux-aarch64/19232)\n\nIs there something we are missing?\n\nIt would be nice if it were a little easier to install tensorflow on this nvidia GPU device.\n", "comments": ["You might have better luck trying to use the Makefile approach in contrib/makefile, rather than bazel.  We know it works for arm devices, and it currently doesn't link in any grpc code.\n", "Closing since this is really a better question for StackOverflow -- it's a request for help from the community, not a bug or explicit feature request.\n", "could we make it a feature request to support arm 64?\n", "You can check out contrib/makefile first, and then file a concrete bug about what is not supported, if any.  I believe we do sort of support arm64 via the Makefile\n", "Makefile approach seems tremendously useful! @vrv is there any plan to include GPU support with the Makefile system? Motivated by TX1 usecase, where can't build gRPC but whole point of platform is the GPU acceleration. Thanks!\n", "No specific plan, if anybody who has a TX1 and is interested in getting it to work wants to send a patch, we'd be ecstatic!\n", "@vrv Hi I've tried with Makefile and successfully installed tensorflow on Jetson TK1 by following RPi build (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#raspberry-pi), but I'm using tensorflow as backend for Keras, so I need python wrapping of it along with GPU support, could you please tell me how should I do that. Thank you!", "Hi every body,\r\nIs there solve this problem ?\r\n\r\nI have the nvidia jetson TK1 kit. I can't add tensorflow-gpu to this card. I've been looking for a lot of forms, but I couldn't find anything for 32 bits. Can you help with this?\r\n"]}, {"number": 4069, "title": "close opened file descriptors properly", "body": "Calling `fclose()` will ensure the file descriptor is properly disposed of and output buffers flushed so the data written to the file will be present in the file on disk.\n\nFound by https://github.com/bryongloden/cppcheck\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n"]}]