[{"number": 8028, "title": "Fix local_test.sh to respect --num_workers and --num_parameter_servers", "body": "Previously, 2 workers and 2 parameters servers were always used,\r\nregardless of the value of these flags.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 8027, "title": "Unable to load \"_lstm_ops.so\" when using rnn.LSTMBlockCell on Windows 10", "body": "### Environment info\r\nOperating System:\r\nWindows 10, 64 bit\r\n\r\nInstalled version of CUDA and cuDNN: \r\nCUDA 8.0\r\ncuDNN 5.1\r\n\r\nIntalled TF by the following:\r\npip3 install --upgrade tensorflow-gpu\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import rnn\r\nseq_max_len = 6\r\nnum_hidden = 5\r\nsample_size = 4\r\n\r\n#print(seq_len_arr)\r\nx = tf.placeholder(tf.float32, [None, seq_max_len, sample_size], name='x')\r\n\r\nseq_len = tf.placeholder(tf.int32, [None])\r\n\r\nlstm_cell = rnn.LSTMBlockCell(num_hidden)\r\nlstmLayers = rnn.MultiRNNCell([lstm_cell]*1)\r\noutputs, states = tf.nn.dynamic_rnn(lstmLayers, x, sequence_length=seq_len, dtype=np.float32)\r\ninit_op = tf.global_variables_initializer()\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nTraceback (most recent call last):\r\n  File \"debug_lstm_zeros2.py\", line 33, in <module>\r\n    outputs, states = tf.nn.dynamic_rnn(lstmLayers, x, sequence_length=seq_len, dtype=np.float32)\r\n  File \"C:\\Users\\jun\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 545, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"C:\\Users\\jun\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 712, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n  File \"C:\\Users\\jun\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2626, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"C:\\Users\\jun\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2459, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"C:\\Users\\jun\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2409, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"C:\\Users\\jun\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 695, in _time_step\r\n    skip_conditionals=True)\r\n  File \"C:\\Users\\jun\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 177, in _rnn_step\r\n    new_output, new_state = call_cell()\r\n  File \"C:\\Users\\jun\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 683, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n  File \"C:\\Users\\jun\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\core_rnn_cell_impl.py\", line 655, in __call__\r\n    cur_inp, new_state = cell(cur_inp, cur_state)\r\n  File \"C:\\Users\\jun\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\lstm_ops.py\", line 402, in __call__\r\n    use_peephole=self._use_peephole)\r\n  File \"C:\\Users\\jun\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\lstm_ops.py\", line 122, in _lstm_block_cell\r\n    return _lstm_ops_so.lstm_block_cell(\r\nAttributeError: 'NoneType' object has no attribute 'lstm_block_cell'\r\n\r\nThanks a lot for your assistance!\r\n\r\nJun", "comments": ["This is a duplicate of  #7500 - there was a bug that's since been fixed.  The nightly builds have the fix."]}, {"number": 8026, "title": "(Withdrawn)", "body": "Withdrawn", "comments": ["Issue filed in error"]}, {"number": 8025, "title": "quantize_graph round and quantize modes are broken", "body": "Looking at code in master. /CC @petewarden \r\n\r\nA `KeyError` is always produced.\r\n\r\n[Consider the `round` mode](already_visited):\r\n```python\r\n    if self.mode == \"round\":\r\n      self.already_visited = {}\r\n      for output_node in output_nodes:\r\n        self.round_nodes_recursively(output_node)\r\n```\r\nwhich [will fails for all calls](https://github.com/tensorflow/tensorflow/blob/fa4ba830f437fdb9dc1085b4d68a3bab41a16e20/tensorflow/tools/quantization/quantize_graph.py#L402-L404):\r\n```\r\n  def round_nodes_recursively(self, current_node):\r\n    \"\"\"The entry point for simple rounding quantization.\"\"\"\r\n    if self.already_visited[current_node.name]:\r\n```\r\nwith a key error since the `already_visited` dict will be empty. ", "comments": ["Thanks for filing this issue @cancan101 !  I think you're completely right.\r\n\r\nI tried to track down where this bug was introduced.  Assigning this to @andrewharp since it looks like he touched this part of the code.  Andrew, feel free to re-assign as appropriate.  Thanks!", "@andrewharp Is this fixable?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@andrewharp: Is there a fix for this bug? "]}, {"number": 8024, "title": "Building with XLA throws 'error: non-constant-expression cannot be narrowed from type 'std::vector<se::DeviceMemoryBase>::size_type' (aka 'unsigned long') to 'long long' in initializer list'", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/5143\r\nhttps://github.com/bazelbuild/bazel/issues/596\r\nhttps://github.com/tensorflow/tensorflow/issues/4103\r\nhttp://stackoverflow.com/questions/39157631/tensorflow-build-error-with-bazel\r\nhttp://stackoverflow.com/questions/37313212/tensorflow-bazel-build-failing\r\nhttp://stackoverflow.com/questions/34941620/unable-to-build-tensorflow-from-source-with-bazel-22nd-january-2016\r\nhttp://stackoverflow.com/questions/38603017/tensorflow-install-bazel-build-error-failed-package-loads\r\n\r\nUnfortunately none of these posts report similar errors.\r\n\r\n\r\n### Environment info\r\nOperating System:\r\nMac OSX Sierra\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nNeither are installed on my system:\r\n```\r\n~/tensorflow master*\r\n\u276f ls -l /path/to/cuda/lib/libcud*\r\nls: /path/to/cuda/lib/libcud*: No such file or directory\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n```\r\n~/tensorflow master*\r\n\u276f git rev-parse HEAD\r\n8cac382a5425d64f3083cb5adec525baa163e18e\r\n```\r\n2. The output of `bazel version`\r\n```\r\n~/tensorflow master*\r\n\u276f bazel version\r\nWarning: ignoring LD_PRELOAD in environment.\r\n............\r\nBuild label: 0.4.4-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Feb 2 01:05:15 2017 (1485997515)\r\nBuild timestamp: 1485997515\r\nBuild timestamp as int: 1485997515\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI simply followed [these instructions](https://www.tensorflow.org/install/install_sources). This was my interaction with the `configure` script:\r\n```\r\n~/tensorflow master* 4m 13s\r\n\u276f ./configure\r\nPlease specify the location of python. [Default is /usr/local/bin/python]: \r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y\r\nXLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]\r\n\r\nUsing python library path: /usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] n\r\nNo CUDA support will be enabled for TensorFlow\r\nConfiguration finished\r\nWarning: ignoring LD_PRELOAD in environment.\r\nExtracting Bazel installation...\r\n..........................................................\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\nWarning: ignoring LD_PRELOAD in environment.\r\n..........................................................\r\nINFO: All external dependencies fetched successfully.\r\n```\r\n\r\nI then ran\r\n```\r\n~/tensorflow master* 1m 35s\r\n\u276f bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nand got the following error\r\n\r\n```\r\nERROR: /Users/ethan/tensorflow/tensorflow/compiler/xla/service/BUILD:406:1: C++ compilation of rule '//tensorflow/compiler/xla/service:allocation_tracker' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 105 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/compiler/xla/service/allocation_tracker.cc:178:54: error: non-constant-expression cannot be narrowed from type 'std::vector<se::DeviceMemoryBase>::size_type' (aka 'unsigned long') to 'long long' in initializer list [-Wc++11-narrowing]\r\n        ShapeUtil::GetSubshape(allocation->shape(), {i}),\r\n                                                     ^\r\ntensorflow/compiler/xla/service/allocation_tracker.cc:178:54: note: insert an explicit cast to silence this issue\r\n        ShapeUtil::GetSubshape(allocation->shape(), {i}),\r\n                                                     ^\r\n                                                     static_cast<long long>( )\r\n1 error generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nBecause XLA seemed to be causing the error, I tried building without it and the build was successful.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n- It does look like bazel is recording a log but I couldn't find it.\r\n\r\n- I posted the full output of `bazel build` in [this gist](https://gist.github.com/lobachevzky/3bcd214e747613d4d0aaaafda2f48f4c).\r\n\r\n", "comments": ["FYI I believe this is the result of a problematic PR #7752 .  Note that PR #8039 fixes some of the issues with that, but I don't think it'll fix your particular problem.\r\n\r\nMaybe in the short-term, sync to a commit hash before the commit for #7752 .  E.g. try bb11fdaac7295651195c3190b6aac72b18fa14e5", "Checking out the earlier commit worked, but their were some significant differences with the most recent version of Tensorflow, so I decided to live without XLA.", "have same issue, use 'bazel build --cxxopt=\"-DEIGEN_HAS_VARIADIC_TEMPLATES=0\"' as hot fix."]}, {"number": 8023, "title": "logger dublication", "body": "When basicConfig() from logging (python 2.7.11) is used, tensorflow duplicates output of logger (load logger twice).\r\n\r\nExample code and output:\r\n```\r\nfrom logging import basicConfig\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.learn import Estimator\r\n\r\nbasicConfig()\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ndef cnn_model_fn(features,labels,mode):\r\n    pass\r\n\r\nif __name__ == '__main__':\r\n    classifier = Estimator(\r\n        model_fn=cnn_model_fn)\r\n\r\n\r\n\r\n```\r\n\r\noutput: \r\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/qn/z_b3zdss1g730kn_mlrwcwnc0000gn/T/tmpEdygBY\r\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/qn/z_b3zdss1g730kn_mlrwcwnc0000gn/T/tmpEdygBY\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using default config.\r\n", "comments": ["I believe this is working-as-intended.\r\n\r\nHere's the python documentation for `logging.basicConfig`\r\nhttps://docs.python.org/2/library/logging.html#logging.basicConfig\r\n\r\n> Does basic configuration for the logging system by creating a StreamHandler with a default Formatter and adding it to the root logger.\r\n\r\nBy calling `logging.basicConfig`, you're adding a StreamHandler to the root logger in python.  Note that `tf.logging` adds its own StreamHandler to its logger.  So now each log line will be sent to two streams, causing the duplication.\r\n\r\nFor reference, here's the code for tf.logging\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/tf_logging.py\r\n\r\nI'm closing this out.  If you feel I'm mistaken, respond to this issue and I'll re-open."]}, {"number": 8022, "title": "ADD to CONTRIB: single_image_random_do_stereograms for data analysis", "body": "I have written a TensorFlow OP \"single_image_random_dot_stereograms\" which takes a 2 D array with Z values and outputs a Stereogram image for use in \"encode_png/jpg\".  \r\n\r\nit is based upon this paper:\r\nhttp://www.learningace.com/doc/4331582/b6ab058d1e206d68ab60e4e1ead2fe6e/sirds-paper\r\n\r\nI am sure this can be used to output data to TensorBoard as well but haven't tried that yet.\r\n\r\nI would like to offer to add this as a contrib for TensorFlow and would like some advice if there is interest on getting the process started.  Right now I compile it into the \"user_ops\" section, but think it needs to move for contrib.\r\n\r\nBasically, this helps display 3D data without hidden lines like a \"waterfall\" plot.\r\n\r\nHere is the basic code as a demo:\r\n\r\n    img=[[1,2,3,3,2,1],\r\n         [1,2,3,4,5,2],\r\n         [1,2,3,4,5,3],\r\n         [1,2,3,4,5,4],\r\n         [6,5,4,4,5,5]]\r\n\r\n    session = tf.InteractiveSession()\r\n\r\n    sirds = single_image_random_dot_stereograms(img,convergence_dots_size=8,number_colors=256,normalize=True)\r\n\r\n    out = sirds.eval()\r\n\r\n    png = tf.image.encode_png(out).eval()\r\n\r\n    with open('picture_out.png', 'wb') as f:\r\n        f.write(png)\r\n\r\nThe output image then looks like the attached image.  If you can view this image outside of github it will look better.  Try to look beyond the picture so the 2 dots at the bottom appear to be 3 dots.  The image should then converge and see the shape as described in \"img\" above.\r\n![picture_out](https://cloud.githubusercontent.com/assets/18412448/23521590/a2deb0fc-ff4d-11e6-9dfc-b00759f921c8.png)\r\n\r\nIf there is interest, I can breakup the code to better match one of the contrib subdirectories and start a pull request.\r\n\r\nHere is a different image, inverted cone, windowed in full color.\r\n![picture_out_color_cone](https://cloud.githubusercontent.com/assets/18412448/23529957/a4737112-ff6d-11e6-8d35-dbdd29049edf.png)\r\n\r\n", "comments": ["Very cool stuff @Mazecreator , thanks for sharing!\r\n\r\n@martinwicke Can you give some advice on whether this should go into contrib, or what other options there might be?  Thanks!", "contrib is the right place I'd say.\n", "Sounds good, how about adding it to the \"/contrib/image/\" section?  I can add it into this directory, there is a few ops there already.\r\n\r\nIf this is okay, I will work on getting a pull request together over the next few days so you have something to look at.", "That is the perfect location. Please send a PR.", "3rd times a charm.\r\n\r\nI have created a quick overview of the Op here:\r\nhttps://github.com/Mazecreator/TensorFlow-SIRDS\r\n", "Seems fixed."]}, {"number": 8021, "title": "pywrap_tensorflow.list_devices() allocates all available memory (on all GPU devices)", "body": "### Environment info\r\nOperating System:\r\nUbuntu 14.04.5 LTS\r\nPython 3.4.3\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nlibcudadevrt.a  libcudart.so.8.0     libcudart_static.a  libcudnn.so.5      libcudnn_static.a\r\nlibcudart.so    libcudart.so.8.0.61  libcudnn.so         libcudnn.so.5.1.5\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n`pip install tensorflow-gpu`\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.0\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nfrom tensorflow.python import pywrap_tensorflow\r\npywrap_tensorflow.list_devices()\r\n```\r\n\r\nI have tried this locally on a system with two Quadro K620 and another system with one TITAN X.", "comments": ["Doesn't happen if this runs first:\r\n```\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333, allow_growth=True)\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n```", "@garibarba I'm glad you discovered `tf.GPUOptions`, and thanks for filing an issue!\r\n\r\nHere's documentation on why TensorFlow allocates all GPU memory up-front, by default, and other GPU-related information:\r\nhttps://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\r\n\r\nSo basically this is working-as-intended.  Closing this out.", "PS, this is a sign of missing functionality in TensorFlow - there's no stand-alone way to configure some process-global settings, such as GPU Allocator and Eigen threadpool. Instead they get configured based on ConfigProto passed to the first tf.Session() call. This is a bit confusing -- first tf.Session() call and second tf.Session can get different values, and it seems right now the second set of values is ignored. @mrry ", "That's a good point @yaroslavvb.  Can you file a new issue for that?  Thanks!", "Filed https://github.com/tensorflow/tensorflow/issues/8136"]}, {"number": 8020, "title": "Make quantize_graph importable", "body": "Currently `quantize_graph` is a runnable script but does not seem to be importable.\r\nThis import fails:\r\n```\r\nfrom tensorflow.tools.quantization import quantize_graph\r\n```", "comments": ["I'm not sure whether we intend for the quantize_graph logic to be used directly.\r\n\r\n@petewarden would know.", "@petewarden?", "The recommended way to access this functionality is now through the graph transform tool's quantize_nodes rule, which can be called through Python:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/python/transform_graph_test.py"]}, {"number": 8019, "title": "[Windows - Bazel] ERROR: No toolchain found for cpu 'x64_windows'", "body": "It is more probable to be something related to Bazel but I am posting here as well just in case.  \r\n\r\nWindows 10 x64\r\nMsys2 v20160205\r\nBazel 0.4.3 (tested with 0.4.4 as well)\r\nVisual C++ 2015\r\n\r\nI get this error when building with Bazel after successfully configuring TensorFlow \r\n```\r\n$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nERROR: No toolchain found for cpu 'x64_windows'. Valid cpus are: [\r\n  k8,\r\n  piii,\r\n  arm,\r\n  darwin,\r\n  ppc,\r\n].\r\nINFO: Elapsed time: 24.952s\r\n```\r\n\r\n#### Steps to reproduce:\r\n- Compile Bazel or use a pre-built binary (both yielded same result)\r\n- Clone TensorFlow repository\r\n- Configure TensorFlow\r\n- Build it with bazel\r\n\r\n#### I tried so far as suggested:\r\n\r\n- Configuring again and build ([entire log](https://github.com/bazelbuild/bazel/issues/2594#issuecomment-283073027)) using:\r\n```\r\nexport BUILD_OPTS='--cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=/w --verbose_failures --experimental_ui'\r\nbazel build -c opt $BUILD_OPTS tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n- Using `-c opt` instead of `--config=cuda`.\r\n\r\n- Running without `--config=cuda`, which generates the error:\r\n `AssertionError: Could not find python binary: python3.exe` perhaps related to [Bazel issue 2457](https://github.com/bazelbuild/bazel/issues/2457).\r\n\r\n[Issue on Bazel](https://github.com/bazelbuild/bazel/issues/2594).\r\n", "comments": ["@Carmezim It looks like the right folks are involved on your Bazel issue.\r\n\r\nI'm closing this out for now; if you feel I've made a mistake, respond to this issue and I'll re-open.  Thanks!", "same issue"]}, {"number": 8018, "title": "Fixed broken link to Community", "body": "The original index.md file in the g3doc/resources is removed, and this would be the most appropriate one that found in docs_src/about.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "@calpa Thank you for the change. Could you please sign the CLA?", "@caisq I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 8017, "title": "AttributeError: module 'tensorflow' has no attribute 'streaming_accuracy'", "body": "```\r\n#Accuracy calculation\r\naccuracy = tf.streaming_accuracy(y_p, mnist.train.lables, weights=None,\r\n                       metrics_collections=None, updates_collections=None,\r\n                       name=\"Accuracy\")\r\nacc = sess.run(accuracy)\r\nprint (acc)\r\n```\r\nwhile calculating Accuracy in my model I encountered this error!\r\nDon't have any clue why?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there.\r\n\r\nAlso, I suggest on your StackOverflow question that you include the version of TensorFlow you're using.\r\n\r\nThanks!\r\n"]}, {"number": 8016, "title": "C++ demo failed with lib in the makefile project", "body": "I can not build and run the demo on the C++ API page:\r\n\r\nhttps://www.tensorflow.org/api_guides/cc/guide\r\n\r\nWhen link to the lib built in the /contrib/makefile project, does that mean that the cc/ops are totally not supported or I missed some steps or information?\r\n\r\nCurrent errors are as the following:\r\n\r\n```\r\n../../../../Git/tensorflow/bazel-out/local-fastbuild/genfiles/tensorflow/cc/ops/array_ops.h:24:57: error: no type named 'Input' in namespace 'tensorflow::ops'; did you mean simply 'Input'?\r\n  BatchMatrixBandPart(const ::tensorflow::Scope& scope, ::tensorflow::ops::Input\r\n                                                        ^~~~~~~~~~~~~~~~~~~~~~~~\r\n                                                        Input\r\n```", "comments": ["@chenleo What version of TensorFlow are you using?  Sounds like a version mismatch.", "I used the master rep version. despite from that, I did not found the correspond /cc folder locates in bazel-out/local-fastbuild/genfiles/tensorflow/cc, in the makefile project, (I think is somewhere like gen/host_obj/tensorflow/), which leads me to think that the ops namespace is not supported.", "Perhaps @josh11b might know.", "Assigning to @petewarden because makefiles.", "The generated C++ op interfaces are not built by the mobile build systems, either using Bazel or Makefiles. That means that the CC examples won't run unfortunately."]}, {"number": 8015, "title": "Cleanup iOS Camera Example", "body": "Fix some bugs, modernize the project, resolve warnings.", "comments": ["Can one of the admins verify this patch?", "Thanks for the review! (testing, can I do this?) @tensorflow-jenkins test this please ", "@tensorflow-jenkins test this please"]}, {"number": 8014, "title": "404 in contrib.layers documentation", "body": "In the official doc for version 1.0, [this link](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/linear) to **tf.contrib.layers.linear** in [here](https://www.tensorflow.org/api_guides/python/contrib.layers) leads to a 404.\r\n\r\n", "comments": ["That `tf.contrib.layers.linear` link should probably be removed.  There are no op links for the other activation function aliases (partial objects), `tf.contrib.layers.relu` or `tf.contrib.layers.relu6`, on the Python API Guides page.  I can remove the `@{tf.contrib.layers.linear}` line from the [`docs_src` file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/api_guides/python/contrib.layers.md) and submit a pull request, but I think that I might first check that all other `@{}` links work (at least on the `tf.contrib.layers` page.)", "Thanks for filing the issue @Mec-iS , and thanks for the analysis @ClarkZinzow !\r\n\r\nI'm closing this out, since #8029 covers this as well."]}, {"number": 8013, "title": "Fixes ios camera", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 8012, "title": "Not support i686?", "body": "Install from source.\r\n\r\n git rev-parse HEAD\r\n`8cac382a5425d64f3083cb5adec525baa163e18e`\r\n\r\n bazel version\r\n```\r\nBuild label: 0.4.4- (@non-git)\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 2 08:11:08 2017 (1488442268)\r\nBuild timestamp: 1488442268\r\nBuild timestamp as int: 1488442268\r\n```\r\n\r\n./configure\r\nPlease specify the location of python. [Default is /env/tensorflow/bin/python]:\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to use jemalloc as the malloc implementation? [Y/n]\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]\r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /env/tensorflow/lib/python3.5/site-packages\r\nPlease input the desired Python library path to use.  Default is [/env/tensorflow/lib/python3.5/site-packages]\r\n\r\nUsing python library path: /env/tensorflow/lib/python3.5/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N]\r\nNo CUDA support will be enabled for TensorFlow\r\nConfiguration finished\r\n\r\n\r\n.\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n..................................\r\nERROR: /prj/tmp/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:321:1: no such package '@grpc//': Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz, https://github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz] to /root/.cache/bazel/_bazel_root/3dd36c466b8b5ba9ee6025155ebdf4cb/external/grpc/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: Connection reset and referenced by '//tensorflow/core/distributed_runtime/rpc:grpc_testlib_server'.\r\nERROR: /prj/tmp/tensorflow/third_party/eigen3/BUILD:20:1: no such package '@eigen_archive//': Error downloading [http://bazel-mirror.storage.googleapis.com/bitbucket.org/eigen/eigen/get/290bfb42684a.tar.gz, https://bitbucket.org/eigen/eigen/get/290bfb42684a.tar.gz] to /root/.cache/bazel/_bazel_root/3dd36c466b8b5ba9ee6025155ebdf4cb/external/eigen_archive/290bfb42684a.tar.gz: Tried to reconnect at offset 498,992 but server didn't support it and referenced by '//third_party/eigen3:eigen3'.\r\nERROR: Evaluation of query \"deps(((//tensorflow/... - //tensorflow/contrib/nccl/...) - //tensorflow/examples/android/...))\" failed: errors were encountered while computing transitive closure.\r\n\r\n", "comments": ["bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nERROR: /prj/tmp/tensorflow/tensorflow/tools/pip_package/BUILD:77:1: no such package '@eigen_archive//': Error downloading [http://bazel-mirror.storage.googleapis.com/bitbucket.org/eigen/eigen/get/290bfb42684a.tar.gz, https://bitbucket.org/eigen/eigen/get/290bfb42684a.tar.gz] to /root/.cache/bazel/_bazel_root/3dd36c466b8b5ba9ee6025155ebdf4cb/external/eigen_archive/290bfb42684a.tar.gz: Tried to reconnect at offset 1,297,437 but server didn't support it and referenced by '//tensorflow/tools/pip_package:licenses'.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\nINFO: Elapsed time: 92.479s\r\n", "@bfbd888 I think this like indicates a network issue:\r\n\r\n```\r\nError downloading [http://bazel-mirror.storage.googleapis.com/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz, https://github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz] to /root/.cache/bazel/_bazel_root/3dd36c466b8b5ba9ee6025155ebdf4cb/external/grpc/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: Connection reset and referenced by '//tensorflow/core/distributed_runtime/rpc:grpc_testlib_server'.\r\n```\r\n\r\nMake sure you're connected to the internet?", "If it's network error, I think it is because I'm in china.\r\nthanks very much.", "OK, feel free to open a new issue if you have a new problem.  Thanks!"]}, {"number": 8011, "title": "TypeError: Fetch argument None has invalid type <class 'NoneType'>", "body": "Feature request for a better error description OR for better summary handling:\r\n\r\nThe following code works fine if some summaries where defined before:\r\n```\r\nops=[] \r\nops += [tf.summary.merge_all()]\r\nsession.run(ops)\r\n```\r\nHowever if there were no summaries we get:\r\nTypeError: Fetch argument None has invalid type <class 'NoneType'>\r\n\r\nWhich is really saying:  \"One of the session.run ops where empty, which is forbidden.\"\r\nAlternatively let merge_all return a NoOp if there are no summaries.", "comments": ["@pannous Thanks for the concise and clear issue!\r\n\r\n@dandelionmane the suggestion sounds reasonable, can you comment?", "It should just work and return a `tf.no_op`.", "On it.", "bump - this one just cost me 5 minutes of my life :P", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8010, "title": "tf.identity() not copying varying inputs", "body": "When using `tf.identity()` it seems like a proper implementation to pass through the input tensor without copying. However, when the input is the value of a variable it might change later, leading to surprising behavior:\r\n\r\n```python\r\nvar = tf.Variable(1)\r\nold = tf.identity(var.value())\r\nwith tf.control_dependencies([old]):\r\n  with tf.control_dependencies([var.assign_add(1)]):\r\n    new = tf.identity(var.value())\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nprint(sess.run([old, new]))  # Unexpected: [2, 2]\r\n```\r\n\r\nI would think this code should be equivalent to the following workaround that uses a second variable to remember the previous value:\r\n\r\n```python\r\nvar = tf.Variable(1)\r\nold = tf.Variable(var.initialized_value())\r\nwith tf.control_dependencies([old.assign(var.value())]):\r\n  with tf.control_dependencies([var.assign_add(1)]):\r\n    new = tf.identity(var.value())\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nprint(sess.run([old, new]))  # Expected: [1, 2]\r\n```\r\n\r\nCan we make `tf.identity()` aware of whether its input is static or varying, to always return the value from the time it's executed?", "comments": ["@danijar Thanks for filing this issue!\r\n\r\nI believe it's a duplicate of #4663 , so I'm closing this out.  If you feel I've made a mistake, respond to this issue and I'll re-open it."]}, {"number": 8009, "title": "Connection timed out when loading MNIST", "body": "Hi,\r\n\r\nI got **Connection timed out** error when I was trying to load MNIST data using tensorflow script.\r\n```\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n```\r\n\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"create_mnistm.py\", line 9, in <module>\r\n    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n  File \"/home/user/miniconda2/envs/domain_adapt/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\", line 211, in read_data_sets\r\n    SOURCE_URL + TRAIN_IMAGES)\r\n  File \"/home/user/miniconda2/envs/domain_adapt/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\", line 208, in maybe_download\r\n    temp_file_name, _ = urlretrieve_with_retry(source_url)\r\n  File \"/home/user/miniconda2/envs/domain_adapt/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\", line 165, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/user/miniconda2/envs/domain_adapt/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\", line 190, in urlretrieve_with_retry\r\n    return urllib.request.urlretrieve(url, filename)\r\n  File \"/home/user/miniconda2/envs/domain_adapt/lib/python2.7/urllib.py\", line 98, in urlretrieve\r\n    return opener.retrieve(url, filename, reporthook, data)\r\n  File \"/home/user/miniconda2/envs/domain_adapt/lib/python2.7/urllib.py\", line 245, in retrieve\r\n    fp = self.open(url, data)\r\n  File \"/home/user/miniconda2/envs/domain_adapt/lib/python2.7/urllib.py\", line 213, in open\r\n    return getattr(self, name)(url)\r\n  File \"/home/user/miniconda2/envs/domain_adapt/lib/python2.7/urllib.py\", line 350, in open_http\r\n    h.endheaders(data)\r\n  File \"/home/user/miniconda2/envs/domain_adapt/lib/python2.7/httplib.py\", line 1038, in endheaders\r\n    self._send_output(message_body)\r\n  File \"/home/user/miniconda2/envs/domain_adapt/lib/python2.7/httplib.py\", line 882, in _send_output\r\n    self.send(msg)\r\n  File \"/home/user/miniconda2/envs/domain_adapt/lib/python2.7/httplib.py\", line 844, in send\r\n    self.connect()\r\n  File \"/home/user/miniconda2/envs/domain_adapt/lib/python2.7/httplib.py\", line 821, in connect\r\n    self.timeout, self.source_address)\r\n  File \"/home/user/miniconda2/envs/domain_adapt/lib/python2.7/socket.py\", line 575, in create_connection\r\n    raise err\r\nIOError: [Errno socket error] [Errno 110] Connection timed out\r\n```\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n```\r\n-rw-r--r-- 1 root root   558720 Jan 19 11:54 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Jan 19 11:54 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Jan 19 11:54 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Jan 19 11:54 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Jan 19 11:54 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 79337624 Jan 19 12:13 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 79337624 Jan 19 12:13 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 79337624 Jan 19 12:13 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Jan 19 12:13 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\n\r\n\r\nThe output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`:\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\n0.12.1\r\n```\r\n", "comments": ["Oh it seems the server is down."]}, {"number": 8008, "title": "fix compiler issues with intrinsics under Windows for Build Tools for VS2017", "body": "Based on discussion in Issue #7966 \r\n\r\ntwo changes made:\r\n1) add `#include <intrin.h>` to `tensorflow\\core\\platform\\windows\\cpu_info.h ` for availability of `__cpuidex` function based on [this](https://msdn.microsoft.com/en-us/library/hskdteyh.aspx)\r\n\r\n2) do not define functions `_mm256_extract_epi32 `and `_mm256_insert_epi32 `in `tensorflow/core/platform/windows/intrinsics_port.h` because they are already declared in `immintrin.h` in VS2017 (as extern as they are intrinsics now)\r\n\r\nQuote from `immintrin.h`\r\n```\r\n// Insert integer into 256-bit packed integer array at element selected by index\r\nextern __m256i __cdecl _mm256_insert_epi8 (__m256i /* dst */, int /* src */, const int /* index */);\r\nextern __m256i __cdecl _mm256_insert_epi16(__m256i /* dst */, int /* src */, const int /* index */);\r\nextern __m256i __cdecl _mm256_insert_epi32(__m256i /* dst */, int /* src */, const int /* index */);\r\n#if defined(_M_X64)\r\nextern __m256i __cdecl _mm256_insert_epi64(__m256i /* dst */, __int64 /* src */, const int /* index */);\r\n#endif  // defined (_M_X64)\r\n\r\n// Extract integer element selected by index from 256-bit packed integer array\r\nextern int __cdecl _mm256_extract_epi8 (__m256i /* src */, const int /* index */);\r\nextern int __cdecl _mm256_extract_epi16(__m256i /* src */, const int /* index */);\r\nextern int __cdecl _mm256_extract_epi32(__m256i /* src */, const int /* index */);\r\n#if defined(_M_X64)\r\nextern __int64 __cdecl _mm256_extract_epi64(__m256i /* src */, const int /* index */);\r\n#endif  // defined (_M_X64)\r\n```", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "I think the test failure was a flake....\r\n\r\n@tensorflow-jenkins test this please.", "Since there is no CI for vs2017 yet I ran the python unit tests for CPU manually - looks good.\r\n  ctest -C RelWithDebInfo\r\n  100% tests passed, 0 tests failed out of 226\r\n  Total Test time (real) = 4348.83 sec\r\n\r\nTried a GPU build but there is no CUDA SDK for vs2017 yet.", "Thanks for confirming that, Guenther!"]}, {"number": 8007, "title": "fix the filter position cacluating for the VALID padding mode", "body": "There is a typo for the filter position calculation for the padding mode of VALID. \r\nHere we fixing it by using the same way of ~/tensorflow/core/kernels/conv_ops_using_gemm.cc.\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Perhaps pete@petewarden.com<mailto:pete@petewarden.com> can do this.\r\n\r\n\r\n-------------------------------------------------------------------------------------------------------------------------------------\r\nTHIS E-MAIL INCLUDING ANY ATTACHMENTS CONTAINS SPREADTRUM\u2019S PROPRIETARY CONFIDENTIAL INFORMATION THAT IS HIGHLY CONFIDENTIAL AND PRIVILEGED.\r\n\r\nFrom: Tensorflow Jenkins [mailto:notifications@github.com]\r\nSent: Thursday, March 02, 2017 6:13 PM\r\nTo: tensorflow/tensorflow\r\nCc: Ji Qiu (\u90b1\u5409); Author\r\nSubject: Re: [tensorflow/tensorflow] fix the filter position cacluating for the VALID padding mode (#8007)\r\n\r\n\r\nCan one of the admins verify this patch?\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/8007#issuecomment-283612162>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AXR6uqFs_VByjGbmxXzEz-5ANDNtv_m3ks5rhpYWgaJpZM4MQygj>.\r\n", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 8006, "title": "tensorflow-gpu fail for windows 7 T_T", "body": "My computer installed tensorflow-gpu for windows 7 (FX 2800M)\r\n\r\nbut GPU not understanding my counter...T_T\r\n\r\nalways tensorflow cpu understandig...\r\n\r\nCUDA  v8.0 and v6.5 Cudnn v5.1 and v 2.1\r\n\r\nFail log is \r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:94] Check failed: s.ok() could not find cuDevice\r\n\r\nplz.. help me!!\r\n", "comments": ["According to this [page](https://developer.nvidia.com/cuda-legacy-gpus), Quadro FX 2800M only has CUDA Compute Capability 1.1 and therefore is not supported. 3.0 is the minimum requirement.", "Thanks for filing the issue @jumemae , and thanks for the reply @microtony !\r\n\r\nFor future reference, this type of question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8005, "title": "tf.contrib.rnn module object is not callable. ", "body": "I am trying to run a tutorial (http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and have some issue with modules that have moved. \r\n\r\nI researched a bit and found that some classes have moved and fixed: \r\ntf.nn.rnn_cell.BasicRNNCell(state_size)\r\nto \r\ntf.contrib.rnn.BasicRNNCell(state_size)\r\n\r\nThis works now.\r\n\r\nHowever, I have this function I can't seem to fix: \r\n\r\nrnn_outputs, final_state = tf.nn.rnn(cell, rnn_inputs, initial_state=init_state)\r\n--> as expected, says: module 'tensorflow.python.ops.nn' has no attribute 'rnn'\r\n\r\nSo I tried: \r\nrnn_outputs, final_state = tf.contrib.rnn(cell, rnn_inputs, initial_state=init_state)\r\n\r\nBut that says: 'module' object is not callable\r\n\r\nI am using python3 (tried 2 as well), with a fresh installed TensorFlow 1.0.0 through pip3. \r\n\r\nI tried looking at the API, but it's not making much sense to me, as I am still going through the tutorial to try to understand what is happening. I would think maybe the arguments have changed, or maybe this is in a sub-function now? https://www.tensorflow.org/api_docs/python/tf/contrib/rnn\r\n", "comments": ["@dorienh Yes, the rnn logic moved from `tf.contrib.rnn` to `tf.nn` for TensorFlow 1.0\r\n\r\nI suggest you contact the authors of the tutorial, for them to make the appropriate updates.\r\n\r\nI know renaming and code-moving is painful, but note that now that we're on TensorFlow 1.0, we have API compatibility guarantees:\r\nhttps://www.tensorflow.org/programmers_guide/version_semantics\r\n\r\nIf you're looking for tutorials, here's one for RNNs, with other links on the left as well:\r\nhttps://www.tensorflow.org/tutorials/recurrent\r\n\r\nNote that this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8004, "title": "tf.contrib.slim API not support tensorflow newest API", "body": "I like using slim write cnn net structures, but there is something emergency to catch up tensorflow newest API, \r\nthis simple code got deprecate warning follow!\r\n```\r\npredictions = inference.lenet(images, num_classes=FLAGS.num_classes+1, activation_fn=None)\r\nslim.losses.softmax_cross_entropy(predictions, labels)\r\n```\r\n\r\n```\r\nsoftmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\r\nInstructions for updating:\r\nUse tf.losses.softmax_cross_entropy instead.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:394: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\r\nInstructions for updating:\r\nUse tf.losses.compute_weighted_loss instead.\r\n```\r\nI think this is essue about slim.losses, however I tried using `tf.losses.softmax_cross_entropy`instead, got this error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train_tiny5_tensorflow.py\", line 154, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"train_tiny5_tensorflow.py\", line 150, in main\r\n    run_training()\r\n  File \"train_tiny5_tensorflow.py\", line 128, in run_training\r\n    tf.losses.softmax_cross_entropy(predictions, labels)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py\", line 529, in softmax_cross_entropy\r\n    name=\"xentropy\")\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1617, in softmax_cross_entropy_with_logits\r\n    precise_logits, labels, name=name)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2265, in _softmax_cross_entropy_with_logits\r\n    features=features, labels=labels, name=name)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 585, in apply_op\r\n    param_name=input_name)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 61, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter 'features' has DataType int32 not in list of allowed values: float16, float32, float64\r\n```", "comments": ["I meet the same situation too, the newest api document doesn't have slim", "@pobingwanghai I trend to abandom slim and use plane tensorflow to write layers, it's more powerful and let's you controll everything.", "Closing this out, please follow up on #7993 "]}, {"number": 8003, "title": "A small problem about tf.case function and Tensor's shape", "body": "I write the following code on jupyter notebook:\r\na = tf.placeholder(shape=(5, ),  dtype=tf.float32)\r\nb = tf.placeholder(shape=(5, ),  dtype=tf.float32)\r\nx = tf.constant(0)\r\ny = tf.constant(0)\r\nf1 = lambda: a\r\nf2 = lambda: b\r\nres_tensor = tf.case([ (tf.not_equal(x, y), f1) ], default=f2)\r\n\r\nBut I found that the res_tensor's shape is unknown rather than (5, ).", "comments": ["I came here to report the exact same behavior, and saw your issue on the first page.\r\n\r\nThe same goes for vectors:\r\n\r\n```import tensorflow as tf\r\n\r\nvalue_1 = [tf.placeholder(tf.float32, shape=(2, 3, 4)), tf.placeholder(tf.float32, shape=(5, 6, 7)), tf.placeholder(tf.float32, shape=(8, 9, 10))]\r\nvalue_2 = [tf.placeholder(tf.float32, shape=(2, 3, 4)), tf.placeholder(tf.float32, shape=(5, 6, 7)), tf.placeholder(tf.float32, shape=(8, 9, 10))]\r\nswitcher = tf.placeholder(tf.bool)\r\n\r\ndef get_value_1():\r\n\treturn value_1\r\n\r\ndef get_value_2():\r\n\treturn value_2\r\n\r\nbatch_sample = tf.case([(switcher, get_value_1)], default=get_value_2, exclusive=True)\r\n\r\nprint(get_value_1())\r\nprint(get_value_2())\r\nprint(batch_sample)\r\n```\r\n\r\nA simple explanation is that tf.case can handle different output shapes for different cases. For example, if we change value_2 to have different shapes than value_1 (e.g. 2,3,3 instead of 2,3,4) the case will still work.\r\n\r\nHowever, I do believe it would be of best interest to have a defined output shape if all cases have the same shape. It would save us from forcing the shape with something like:\r\n\r\n`for k in range(len(batch_sample)): batch_sample[k].set_shape(get_value_1()[k].shape)`\r\n\r\nThis isn't critical, of course, as there is a rather clean workaround.", "Thanks for reporting the issue @fuhuamosi !\r\n\r\n@MicaelCarvalho is exactly right.  In general, it's impossible for `tf.case` to know the output shape in all scenarios.\r\n\r\nIt's true that in the original example, it looks \"easy\" to infer the output shape.  But the trade-off is the additional code complexity to implement this behavior, that only works for a handful of scenarios.  In typical real usage of `tf.case`, we either won't be able to infer the output shape, or the output shape isn't actually necessary statically.\r\n\r\nClosing this out since we won't be implementing this.  If you feel I've made a mistake, feel free to comment on this issue, and I'll re-open it.  Thanks!", "@MicaelCarvalho  hey, what if the output of tf.case is sent to conv2d which don't support for input with rank none. Does manually forcing the shape of output of tf.case work?", "@vanpersie32 calling `your_tensor.set_shape(...)` should do it, assuming you know the expected shape of the tensor. That's exactly what the last line of code in my previous message does. :-)", "I feel this should be implemented. Or at least an informative warning message. Or remove this API...\r\n\r\nAs the justification on the first place for such high level methods (tf.case, instead of nested tf.cond) is usability, to my opinion, it doesnt make sense to expect the programmer to browse through github issues. This is making TF as some kind of \"magic\", where replacing equivalent statements makes a difference :)\r\n\r\n"]}, {"number": 8002, "title": " load model package in a linux tensorflow project", "body": "\r\n![model](https://cloud.githubusercontent.com/assets/17719977/23498012/a391d9e4-ff60-11e6-895b-5aeb167d612d.png)\r\nHello,master:\r\n      How do I load models package in a linux tensorflow project that has already been installed?thanks\uff01", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "all right,thanks!But I still look forward to having a tutorial document somewhere."]}, {"number": 8001, "title": "small grammar/typo edits", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8000, "title": "Android Camera Detection with Yolo model not working", "body": "I trained and generated my yolo model accodring to the repository https://github.com/thtrieu/darkflow. While testing on few images locally, the model detects objects.\r\n\r\nOnce I build and test it on the phone (adding .pb to assets and setting flag to YOLO), TFDetect runs but I obtain no detections at all.\r\n\r\nWhat could be the cause of that and how could I actually start debugging?\r\n\r\n(trying to run the default graph-tiny-yolo-voc.pb specified in the example demo, succeeded)", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 7999, "title": "Broken link", "body": "The \"See Download and Setup\" link in the readme is broken.", "comments": ["Already an existing issue #7989 with a pull request that fixes the issue: #7990.  Just waiting for a CLA to be signed.", "Thanks for filing the bug @Bihaqo , and thanks for the redirect @ClarkZinzow !\r\n\r\nClosing this out."]}]