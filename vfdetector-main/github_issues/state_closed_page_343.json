[{"number": 43833, "title": "TF2.3 converter.convert ValueError: Input 0 of node StatefulPartitionedCall/functional_1/resnet50/conv1_bn/AssignNewValue was passed float from Func/StatefulPartitionedCall/input/_5:0 incompatible with expected resource.", "body": "**System information**\r\n- OS Platform and Distribution (Ubuntu 18.04, cuda 10.1):\r\n- TensorFlow installed from (binary):\r\n- TensorFlow version (TF 2.3):\r\n\r\n\r\nHere is the part of my code. \r\n```\r\n       export_path './test_model'\r\n        tf.saved_model.save(model, export_path)\r\n        converter = tf.lite.TFLiteConverter.from_saved_model(export_path)\r\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n        tflite_model = converter.convert()\r\n        open(\"./converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nHere is my model summary, which is a typical image classification model using pre_trained ResNet50 model for transfer-learning. \r\n\r\n```\r\nModel: \"functional_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_2 (InputLayer)         [(None, 160, 160, 3)]     0         \r\n_________________________________________________________________\r\nsequential (Sequential)      (None, 160, 160, 3)       0         \r\n_________________________________________________________________\r\ntf_op_layer_RealDiv (TensorF [(None, 160, 160, 3)]     0         \r\n_________________________________________________________________\r\ntf_op_layer_Sub (TensorFlowO [(None, 160, 160, 3)]     0         \r\n_________________________________________________________________\r\nresnet50 (Functional)        (None, 5, 5, 2048)        23587712  \r\n_________________________________________________________________\r\nglobal_average_pooling2d (Gl (None, 2048)              0         \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 2048)              0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 3)                 6147      \r\n=================================================================\r\nTotal params: 23,593,859\r\nTrainable params: 23,540,739\r\nNon-trainable params: 53,120\r\n_________________________________________________________________\r\n```\r\n\r\nHere is the failure errors. \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 497, in _import_graph_def_internal\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input 0 of node StatefulPartitionedCall/functional_1/resnet50/conv1_bn/AssignNewValue was passed float from Func/StatefulPartitionedCall/input/_5:0 incompatible with expected resource.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"main.py\", line 321, in <module>\r\n    main(train='lite')\r\n  File \"main.py\", line 210, in main\r\n    tflite_model = converter.convert()\r\n  File \"/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1076, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 878, in convert\r\n    self._funcs[0], lower_control_flow=False))\r\n  File \"/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 1109, in convert_variables_to_constants_v2_as_graph\r\n    converted_input_indices)\r\n  File \"/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 1001, in _construct_concrete_function\r\n    new_output_names)\r\n  File \"/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py\", line 650, in function_from_graph_def\r\n    wrapped_import = wrap_function(_imports_graph_def, [])\r\n  File \"/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py\", line 628, in wrap_function\r\n    collections={}),\r\n  File \"/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py\", line 87, in __call__\r\n    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n  File \"/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py\", line 93, in wrapped\r\n    return fn(*args, **kwargs)\r\n  File \"/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py\", line 648, in _imports_graph_def\r\n    importer.import_graph_def(graph_def, name=\"\")\r\n  File \"/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 405, in import_graph_def\r\n    producer_op_list=producer_op_list)\r\n  File \"/home/ktatc/anaconda3/envs/HHproject/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 501, in _import_graph_def_internal\r\n    raise ValueError(str(e))\r\n**ValueError: Input 0 of node StatefulPartitionedCall/functional_1/resnet50/conv1_bn/AssignNewValue was passed float from Func/StatefulPartitionedCall/input/_5:0 incompatible with expected resource.**\r\n\r\n```\r\n\r\nHow can i solve it? I don't know the reason why these kind of error happen. please let me know in detail.\r\nThanks in advance. ", "comments": ["@tolry418 \r\n\r\nPlease, share colab link or code with supporting files (`model file)` to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "Thank you for your reply. \r\nMy saved model is here.  \r\n[saved_model.zip](https://github.com/tensorflow/tensorflow/files/5338241/saved_model.zip)\r\nIf you want whole code, please let me know. \r\nThanks.", "I have tried in colab with TF version 2.3 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/86b909fb62d4b691f61bea5da3ec53af/untitled429.ipynb).Thanks!", "@tolry418 I am seeing different exception error as shown below. Can you please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/94b2fd060335f1177cfa4c830ba5277b/untitled429.ipynb).\r\n\r\nCan you please share model building code? Thanks!\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    211                                                  debug_info_str,\r\n--> 212                                                  enable_mlir_converter)\r\n    213       return model_str\r\n\r\n4 frames\r\nException: <unknown>:0: error: loc(\"Conv_1_bn/moving_mean\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    213       return model_str\r\n    214     except Exception as e:\r\n--> 215       raise ConverterError(str(e))\r\n    216 \r\n    217   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: <unknown>:0: error: loc(\"Conv_1_bn/moving_mean\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable\r\n```", "@jvishnuvardhan I used TF 2.3, not tf-nightly. And my python version is 3.7. \r\n\r\nHere is my code. \r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport cv2\r\nfrom datetime import datetime\r\n\r\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\r\nfrom tensorflow.keras.models import Model\r\n\r\ndef main(train=True):\r\n    now = datetime.now()\r\n    \r\n    PATH = './my data' # << here is my own data\r\n    train_dir = os.path.join(PATH, 'train')\r\n    validation_dir = os.path.join(PATH, 'validation')\r\n    \r\n    BATCH_SIZE = 32\r\n    IMG_SIZE = (160, 160)\r\n    \r\n    train_dataset = image_dataset_from_directory(train_dir,\r\n                                                 label_mode='int',\r\n                                                 shuffle=True,\r\n                                                 batch_size=BATCH_SIZE,\r\n                                                 image_size=IMG_SIZE)\r\n        \r\n\r\n    validation_dataset = image_dataset_from_directory(validation_dir,\r\n                                                      label_mode='int',\r\n                                                      shuffle=True,\r\n                                                      batch_size=BATCH_SIZE,\r\n                                                      image_size=IMG_SIZE)\r\n\r\n\r\n    val_batches = tf.data.experimental.cardinality(validation_dataset)\r\n    test_dataset = validation_dataset.take(val_batches // 5)\r\n    validation_dataset = validation_dataset.skip(val_batches // 5)\r\n    \r\n    AUTOTUNE = tf.data.experimental.AUTOTUNE #-1\r\n    train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\r\n    validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\r\n    test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\r\n\r\n    data_augmentation = tf.keras.Sequential([\r\n    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\r\n    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\r\n    ])\r\n    \r\n    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\r\n    rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\r\n    IMG_SHAPE = IMG_SIZE + (3,)\r\n\r\n    # base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\r\n    #                                            include_top=False,\r\n    #                                            weights='imagenet')\r\n    base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,\r\n                                           include_top=False,\r\n                                           weights='imagenet')\r\n\r\n    image_batch, label_batch = next(iter(train_dataset))\r\n    feature_batch = base_model(image_batch)\r\n\r\n    print (\"start!!\")\r\n    if train=='true':\r\n        base_model.trainable = True\r\n      \r\n        global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\r\n        feature_batch_average = global_average_layer(feature_batch)\r\n        \r\n        prediction_layer = tf.keras.layers.Dense(3)\r\n        prediction_batch = prediction_layer(feature_batch_average)\r\n\r\n        \r\n        inputs = tf.keras.Input(shape=(160, 160, 3))\r\n        x = data_augmentation(inputs)\r\n        x = preprocess_input(x)     \r\n        # outputs = rescale(x)\r\n        x = base_model(x, training=True)\r\n        x = global_average_layer(x)\r\n        x = tf.keras.layers.Dropout(0.2)(x)\r\n        outputs = prediction_layer(x)\r\n        model = tf.keras.Model(inputs, outputs)\r\n        \r\n        base_learning_rate = 0.0001\r\n        model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\r\n                  loss='sparse_categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n        print (model.summary())    \r\n        print (\"len(model.trainable_variables\", len(model.trainable_variables))\r\n        \r\n        initial_epochs = 1\r\n        \r\n        loss0, accuracy0 = model.evaluate(validation_dataset)\r\n        print(\"initial loss: {:.2f}\".format(loss0))\r\n        print(\"initial accuracy: {:.2f}\".format(accuracy0))\r\n        \r\n        history = model.fit(train_dataset,\r\n                            epochs=initial_epochs,\r\n                            validation_data=validation_dataset)\r\n        \r\n        dt_string = now.strftime(\"%d%m%Y_%H:%M:%S.hdf5\")\r\n        export_path = \"./Weight/{}\".format(dt_string)\r\n        # model.save(filepath = export_path, save_format='tf')\r\n        # tf.keras.models.save_model(model, export_path, save_format='h5')\r\n        tf.saved_model.save(model, export_path)    \r\n         \r\n    elif train=='lite':\r\n        load_path = './Weight/08102020_09:04:46/'\r\n        converter = tf.lite.TFLiteConverter.from_saved_model(load_path)\r\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n        tflite_model = converter.convert()\r\n        open(\"./converted_model.tflite\", \"wb\").write(tflite_model)    \r\n        \r\n        \r\nif __name__ == \"__main__\":\r\n    main(train='lite')\r\n```\r\n\r\nIf you done the training, change the path of 'load_path' in train=='lite' phase, then you can reproduce it. \r\nThanks you. \r\n\r\n", "@tolry418 Can you please share a simple dataset or any similar public data?\r\n\r\n> PATH = './my data' # << here is my own data", "@jvishnuvardhan \r\nYou can download datasets from the link .\r\n\ufeffCats and dogs filtered image datasets.\r\n\r\nhttps://www.kaggle.com/birajsth/cats-and-dogs-filtered\r\n\r\nThanks. ", "Hi All,\r\n\r\nI have the same issues when I convert saved model to onnx model by below command:\r\n> python3 -m tf2onnx.convert --saved-model . --output tf2-preview_inception_v3_classification_4.onnx --concrete_function 0\r\n\r\nError log:\r\n\r\n> ValueError: Input 0 of node StatefulPartitionedCall/StatefulPartitionedCall/train/InceptionV3/InceptionV3/Conv2d_1a_3x3/BatchNorm/AssignMovingAvg_1/AssignSubVariableOp was passed float from Func/StatefulPartitionedCall/StatefulPartitionedCall/input/_410:0 incompatible with expected resource.\r\n\r\nMy detail issue [#1152](https://github.com/onnx/tensorflow-onnx/issues/1152)\r\nif there are anyone know this issue, please help me.", "I met the same issue while converting a RNN model (GRU) with stateful enabled to tflite. \r\n", "I have the same issue when converting to tensorrt", "Sorry for encountering this issue in your side. Both TFRT and TFLite converter do not support mutable resource variable use cases yet. We are working on supporting the missing features in the MLIR converter.\r\n\r\nFor TFLite converter, please use the V2 saved model converter if you have an issue related to resource variables since it has the better support in dealing with immutable resource variables than other graph representations (e.g., keras model, concrete function, and frozen graphdef).", "so now I have to scrape off my project :/", "can someone please convert the model to tflite and send me it would be a great help this is the model file -https://drive.google.com/drive/folders/1-1jAHa_N2YoA28sLCa2a0xJFkEfJu4Td?usp=sharing", "@abattery Thanks for your hint. Any update on this issue?", "Please use the saved model converter from the recent TF version. We have improved the support for resources and variants both in the saved model converter. For mutable variable cases, you can enable the experimental feature, `converter._enable_tflite_resource_variables = True`.", "If you experience the similar issue, please open a new issue to make issues be focused.", "Thanks @abattery, however this is not solving it for me. I am having a similar issue with converting to tf-trt. Is there a similar flag for tf-trt?", "@ciklista If you experience the similar issue, please open a new issue to make issues be focused.\r\n\r\n", "@ciklista I meet a similar issue when using TF-TRT (the model uses Embedding layer). I solve it using \"TF to ONNX to TRT\" instead of \"TF to TRT\". Hope this method can help you.", "@abattery I know, sorry, I thought if this is a quick answer this might fit better here. Especially since others in this thread also had issues with tf-trt. As this seems not to be the case, I will go ahead and open a new issue. \r\n\r\n@Damcy yes, you are right, going to \"bare bones\" trt might be a solution but I specifically want to use TensorFlowTRT. ", "@abattery I also have the same issue with tf-trt and would like to know if/when there is a solution for it.\r\n\r\n@ciklista Please let me know if you open a new issue for this problem in the tf-trt context.\r\n\r\nThank you.", " Closing since the nightly version can support variables in the TFLite.\r\n\r\nPlease file another issue for tf-trt.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43833\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43833\">No</a>\n", "For the TFLite conversion, please use the saved model converter from the recent TF version. We have improved the support for resources and variants both in the saved model converter. For mutable variable cases, you can enable the experimental feature, `converter._enable_tflite_resource_variables = True`.", "The bug can be recreated with the following code.  It would be good to have it fixed.\r\n\r\nimport tensorflow as tf\r\nfrom keras.models import Model\r\nfrom keras.layers import BatchNormalization\r\nfrom keras.models import Input\r\n\r\nin_image = Input(shape=(256,256,3))  # Create minimalistic model that creates the error (full model is much bigger)\r\nd7 = BatchNormalization()(in_image, training=True)  # This is the line that creates the error.  Error goes away when this line is removed or if \"training=True\" is removed\r\nmodel = Model(in_image, d7)\r\nmodel.compile(loss='mse')\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)  # Convert model to tflite\r\ntflite_model = converter.convert()\r\nwith open('model.tflite', 'wb') as f:\r\n      f.write(tflite_model)", "> d7 = BatchNormalization()(in_image, training=True)\r\n\r\nWhy are you specifying that argument, though? Based on where and how your model is being used this layer can adapt its behavior. [This Keras guide](https://keras.io/guides/transfer_learning) sheds some more details on it. ", "That is how the code in a book on [GANs](https://machinelearningmastery.com/generative_adversarial_networks/) is.  I think that saying \"training=True\" should not trip up the conversion process to tflite.", "During conversion, the BatchNorm layer is supposed to be running in inference mode I think. Is there any particular reason why you'd want it to run in training mode?\r\n\r\n/cc: @abattery @karimnosseir ", "In the book it says:-\r\n\r\nIn Keras, layers like Dropout and BatchNormalization operate differently during training\r\nand in inference model. We can set the training argument when calling these layers to True\r\nto ensure that they always operate in training-model, even when used during inference. For\r\nexample, a Dropout layer that will drop out during inference as well as training can be added\r\nto the model as follows ...", "You'd want those models to actually run in inference mode really during inference. See the following notes:\r\n\r\n* https://cs231n.github.io/neural-networks-2/#reg\r\n* https://cs231n.github.io/neural-networks-2/#batchnorm", "If I remove \"training=True\" from all the lines that have BatchNormalization(), the GAN model doesn't train nearly as well.  Remember that with GAN training \"Inference\" is used as a part of the model training.  During training \"Inference\" is done and that image is passed to the Discriminator who gives it a Real/Fake score.  I'm not an expert, but it looks as though the code in the [book](https://machinelearningmastery.com/generative_adversarial_networks/) is correct.\r\n\r\nI can see that having \"training=True\" for TensorFlowLite is an issue because TensorFlowLite is designed to be for inference only.  Maybe TensorFlowLite  should ignore \"training=True\" for BatchNormalization() and Dropout() layers.\r\n\r\nIt would be good to have this issue fixed so that we can use TensorFlowLite for generating GAN images.", "Please consider uploading a new issue if there are leftover issues here. It is hard to track the closed issue when we have different contexts per user."]}, {"number": 43831, "title": "TFLu Apollo3 Maintenance", "body": "- Resolves benchmark performance for Apollo3 EVB\r\n- Updates Apollo3 support to the 2.5.1 AmbiqSuite SDK\r\n- Supports additional SFE Apollo3 boards for ```micro_speech```, ```magic_wand``` and ```person_detection``` examples", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43831) for more info**.\n\n<!-- need_author_cla -->", "@oclyke Can you please sign CLA. Thanks!", "@gbaned I seem to be having some issues with the CLA. I've previously signed the CLA with both my work and personal emails. I reviewed the commits and the only emails associated were:\r\n* 65086349+rambiqmicro@users.noreply.github.com\r\n* owen.lyke@sparkfun.com\r\n\r\nAFAIK both those emails have signed CLAs at this point. Is there any additional information you could provide for me about this issue?\r\n\r\nThank you much!", "@oclyke Can you please resolve conflicts? Thanks!", "@oclyke, we will not be able to accept this PR in its current for per our [updated contribution guidelines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md#new-target--platform--ide--examples).\r\n\r\nThe underlying issue in the case of this particular PR is that we want to control the amount of duplicated code involved in supporting examples across a range of very similar but slightly different target hardware and keep the scope of the pull requests to [doing one thing](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md#general-pull-request-guidelines).\r\n\r\nIf you are up for a rethink and refactoring of the exisitng apollo3/sparkfun_edge support towards this end then I'd be happy to give you pointers on the direction that I would like this to go in.\r\n\r\nIf you have the time and interest in pursuing this, a next step would be to make a github issue describing the problem (and tag me) and I can guide the refactoring as a series of smaller PRs.\r\n\r\nI'm going to close the current PR but would be happy to continue the conversation via a github issue to figure out a path forward."]}, {"number": 43830, "title": "Tensorflow cannot run inference together with TensorRT on the same GPU", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo, for TF model there is no custom code.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version (use command below):\r\n`1.15` / Command line: `unknown 1.15.2`\r\n- Python version: N/A\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2/7.6\r\n- GPU model and memory:  2080Ti, 12GB\r\n\r\n**Describe the current behavior**\r\n\r\nI'm running C++ inference of TF and TensorRT on the same GPU. However, what I found is TF inference would be broken if I initialized TRT engine in any way.\r\nBoth TF and TRT are doing inference in their separated CPU threads, and creating separated `CUcontext` for these 2 inference engine returns the same errors from TF side.\r\n\r\nThe error is like this:\r\n```\r\n[Switching to Thread 0x7ffc1de50700 (LWP 26137)]\r\nCuda API error detected: cudaPointerGetAttributes returned (0x1)\r\n(cuda-gdb) bt\r\n#0  0x00007ffca7b41130 in cudbgReportDriverApiError () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#1  0x00007ffca7b440ca in cudbgReportDriverInternalError () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#2  0x00007ffca7b475a3 in cudbgApiDetach () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#3  0x00007ffca7d8abe3 in cudbgMain () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#4  0x00007ffca7d9d5a3 in cudbgMain () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#5  0x00007fffcbc60d99 in cudaPointerGetAttributes () from /home/.cache/_solib_local/_U@tensorflow_S_S_Ctensorflow___Uexternal_Stensorflow_Slib/libtensorflow_cc.so.1\r\n#6  0x00007fffccbe2c24 in void stream_executor::gpu::(anonymous namespace)::CheckPointerIsValid<void const*>(void const*, absl::string_view) [clone .constprop.161] ()\r\n   from /home/.cache/_solib_local/_U@tensorflow_S_S_Ctensorflow___Uexternal_Stensorflow_Slib/libtensorflow_cc.so.1\r\n#7 stream_executor::gpu::GpuDriver::AsynchronousMemcpyH2D(stream_executor::gpu::GpuContext*, unsigned long long, void const*, unsigned long long, CUstream_st*) ()\r\n...\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect both inference engine could be ran without broken by the other.\r\n", "comments": ["Creating `CUcontext` or not doesn't seem to affect TF inference, but initialization of TRT inference engine does break TF inference.\r\nDoes TF inference assumes certain `CUcontext` to be available? Or other stateful variables from Nvidia device API?", "@golden0080 \r\nCould you please upgrade to tf 2.x and let us know fi the issue exist, as there is no official support for 1.x.", "Thanks @Saduf2019 , guess I'm closing this ticket and update when I upgraded to 2.0.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43830\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43830\">No</a>\n"]}, {"number": 43829, "title": "train.py: error: argument --silence_percentage: invalid float value: '{SILENT_PERCENTAGE}'", "body": "I got this issue when I was trying to train on my custom data", "comments": []}, {"number": 43828, "title": "Fix multiline", "body": "Try to fix https://github.com/tensorflow/tensorflow/issues/43813", "comments": ["Superseded by a direct internal commit: https://github.com/tensorflow/tensorflow/issues/43813#issuecomment-704569532"]}, {"number": 43827, "title": "BinaryCrossentropy and binary_crossentropy in the. same `tf.keras.losses` module", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/binary_crossentropy\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy\r\n\r\n## Description of issue (what needs changing):\r\nThere are 2 versions of Binary Cross Entropy, it would be less confusing to have just one.\r\n\r\nAlso, only `tf.keras.losses.binary_crossentropy` (or alternatively `\"binary_crossentropy\"`) works in the below code:\r\n```python\r\nmodel.compile(optimizer = RMSprop(lr=0.0001), \r\n              loss = tf.keras.losses.binary_crossentropy, \r\n              metrics = [\"accuracy\"])\r\n```\r\nI wonder why `\"BinaryCrossentropy\"` or `tf.keras.losses.BinaryCrossentropy` don't work.", "comments": ["Which TF version are you using?\r\nI tried following example on TF 2.3.0 and it works as expected:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\ny_true = [[0, 1], [0, 0]]\r\ny_pred = [[0.6, 0.4], [0.4, 0.6]]\r\n\r\nbce1 = tf.keras.losses.binary_crossentropy(y_true, y_pred)\r\nbce2 = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\r\n\r\nassert np.all([ bce1.numpy(), bce2(y_true, y_pred).numpy() ]) == True\r\n```", "Hi ranzer\r\n\r\nI believe I was confused by the difference between them (class vs function). Yes, if you instantiate `BinaryCrossentropy` first, then pass the data, it works.\r\n\r\nSo actually, `model.compile(optimizer=\"adam\", metrics=['accuracy'], loss=tf.keras.losses.SparseCategoricalCrossentropy())` works, notice the extra `()` needed when using **BinaryCrossentropy**."]}, {"number": 43826, "title": "[XLA] Better warning. ", "body": "With that we can just to replay_computation directly for further analysis.\r\n\r\n@sanjoy ", "comments": []}, {"number": 43825, "title": "why does tensorflow import_pb_to_tensorboard.py tool require the pb file to be a fixed name \"saved_model.pb\"?", "body": "I tried to use import_pb_to_tensorboard.py tool come with Tensorflow, but when I used the tool to process my pb file, it always failed, after check the source code of the tool, I found the name of pb file has to be \"saved_model.pb\". I am wondering why the tool requires a fixed .pb file instead of any name users preferred?", "comments": ["I think it process a model_dir not the pb file right? See https://github.com/tensorflow/tensorflow/issues/37814", "yes, you can give path of the dir or the saved_model.pb file as input, but my question is why the name of pb file has to be \"saved_model.pb\"? if you give a path of a dir, which doesn't have a pb file named \"saved_model.pb\", then the tool will pop up an error. Just wonder why the tool requires the name of pb file to be \"saved_model.pb\", and which is not mentioned in help of the tool.", "What I see Is that it support only model_dir args", "yes, but it implicitly requires there is pb file named \"saved_model.pb\" in the model_dir.", "It was generally the automatic name with save and load with model_dir for Savedmodel format.", "why not remove this limit so that the tool can use \"saved_model.pb\" as default name if users don't specify a pb file name, but if users specify a pb file, the tool should accept the pb file users specified instead of popping an error information? I think in this way, the tool is more robust and easier to use.", "@cuixiaom I think you can propose a PR so we could collect some feedbacks.", "sounds good! thanks!", "@cuixiaom \r\nPlease move the issue to closed status if resolved.", "will propose a PR for this issue."]}, {"number": 43824, "title": "Keras way of auto-naming layers triggers a naming issues when loading an existing model, and slicing its output", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nWhen adding a strided_slice layer using \"numpy-style\" syntax (eg [: , :....]), Keras auto-names this layer \"tf_op_layer_strided_slice_{i}\", incrementing i every time such layer is created.\r\nHowever, when opening an already-existing model, without creating any layer in the session, i restarts at 0. Hence, when adding at strided_slice operation at the end of the model, an error is triggered as the model now has 2 layers with the same name \"tf_op_layer_strided_slice\"\r\n\r\nPS: i cannot use the explicit tf.strided_slice layer with naming, as it does not have the same \"shape-inference\" capabilities as numpy-style slicing\r\n\r\n**Describe the expected behavior**\r\nauto-naming shall carry on incrementing from all known layer in the session, whether they have been created or loaded from a model file.\r\n\r\n**Standalone code to reproduce the issue**\r\nRun this code first with initial_builing = True, then False\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ninitial_builing = False\r\nmodel_loading = True\r\n\r\nif initial_builing:\r\n\r\n    layer1 = tf.keras.Input((1,),)\r\n    layer2 = tf.keras.layers.Dense(1)\r\n\r\n    model_output = layer2(layer1)[:,:-1]\r\n\r\n    model = tf.keras.Model(layer1, model_output)\r\n    model.summary()\r\n    model.save('testmodel')\r\n\r\nif model_loading:\r\n    model = tf.keras.models.load_model('testmodel')\r\n    model = tf.keras.Model(model.inputs, model.layers[-1].output[:,:-1])\r\n```\r\n\r\n\r\n**Other info / logs** \r\n```\r\nException has occurred: ValueError\r\nThe name \"tf_op_layer_strided_slice\" is used 2 times in the model. All layer names should be unique.\r\n  File \"C:\\tests_v2.py\", line 19, in <module>\r\n    model = tf.keras.Model(model.inputs, model.layers[-1].output[:,:-1])\r\n```\r\n", "comments": ["I  cannot reproduce this on Colab.", "you have to first run with (to build the model)\r\ninitial_builing = True\r\n\r\nthen run a second time (to only load the model)\r\ninitial_builing = False", "> you have to first run with (to build the model)\n> initial_builing = True\n> \n> then run a second time (to only load the model)\n> initial_builing = False\n\nYes", "@scd75 \r\n\r\nI have tried in colab with TF version 2.3.1 and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/204cabefa50b64d006a6fb90acb94c14/untitled428.ipynb).Please, correct me if i am doing something wrong.Thanks!", "I think that since you are running on jupyter, the variables stay in the cache between the 2 cells. Can you try relaunching the Google colab environment between the 2 executions ? to make sure the model is loaded purely from scratch ?\r\n\r\nHere is what I get on your gist when doing so (eg. running the first cell, relaunching the environment, then running the 2nd cell)\r\n\r\n```\r\nWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-570ad4959fa8> in <module>()\r\n     17 if model_loading:\r\n     18     model = tf.keras.models.load_model('testmodel')\r\n---> 19     model = tf.keras.Model(model.inputs, model.layers[-1].output[:,:-1])\r\n\r\n5 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py in _map_graph_network(inputs, outputs)\r\n    940     if all_names.count(name) != 1:\r\n    941       raise ValueError('The name \"' + name + '\" is used ' +\r\n--> 942                        str(all_names.count(name)) + ' times in the model. '\r\n    943                        'All layer names should be unique.')\r\n    944   return network_nodes, nodes_by_depth, layers, layers_by_depth\r\n\r\nValueError: The name \"tf_op_layer_strided_slice\" is used 2 times in the model. All layer names should be unique.\r\n```\r\n", "Ok I've tried with https://github.com/tensorflow/tensorflow/pull/43841 but I don't know if it will have side effects. Let's wait for a review.", "@tomerk you can reproduce on Colab with:\r\n```\r\n!pip install tf-nightly-cpu\r\nimport tensorflow as tf\r\nimport gc \r\n\r\ninitial_builing = True\r\nmodel_loading = True\r\n\r\nif initial_builing:\r\n\r\n    layer1 = tf.keras.Input((1,),)\r\n    layer2 = tf.keras.layers.Dense(1)\r\n\r\n    model_output = layer2(layer1)[:,:-1]\r\n\r\n    model = tf.keras.Model(layer1, model_output)\r\n    model.summary()\r\n    model.save('testmodel')\r\ndel model, model_output\r\ntf.keras.backend.clear_session()\r\ngc.collect()\r\nif model_loading:\r\n    model = tf.keras.models.load_model('testmodel')\r\n    model = tf.keras.Model(model.inputs, model.layers[-1].output[:,:-1])\r\n```", "Hmm okay that's surprising to me that it still happens in the nightlies, because the name-gen for op layers should follow similar logic as for other layers. I'll take a look.", "@tomerk Is name in  that land in ` name=_TF_OP_LAYER_NAME_PREFIX + name` already autoincrement/unique?", "So, I've confirmed that this is a general saving/loading issue with the auto-naming layers *whenever adding any layer after loading the model* not just when slicing the output. (I repro'd it w/ e.g. another dense layer at the end instead of slicing.)\r\n\r\nI'll go ahead and escalate this because this seems like a pretty serious issue.", "@tomerk So can I close https://github.com/tensorflow/tensorflow/pull/43841 or do you have any hints for change that PR?", "yeah that pr won't fix this problem. We'll have to fix either the save/load code or the backend unique_object_name code or both.", "@tomerk Let me know If you have any pinpoint I could try to modify that PR if you are out of bandwidth.", "I have a tentative fix ready internally. Once it's submitted it should get synced out to github pretty quickly.", "Okay, this should now be working in the nightlies.\r\n\r\nThe fix in commit 62d414b works by adding any names seen during layer creation to an avoid-list that is used when generating names for auto-gen'd op layers. Note that we *don't* do this for all layers (even though auto-naming can run into name collisions for other layers as well), because:\r\n1. it would have the practical affect of changing actual layer names when people do transfer learning w/ a loaded model as a layer in their outer model. This would break a lot of tf1-style name-based checkpoint loading and model manipulation code.\r\n2. for layers other than tf op layers, users can explicitly pass a `name` if needed to get around accidental name collisions, so it's not as big of a problem as for auto-gen'd op layers.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43824\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43824\">No</a>\n", "@tomerk I've a question. `OBSERVED_NAMES` in not as `PER_GRAPH_OBJECT_NAME_UIDS` with `Graph` key (As PER_GRAPH in the name). Is it ok that it is global? ", "@tomerk Can you clarify?", "The per-graph stuff is primarily legacy-compatibility. The new op layers and the OBSERVED_NAMES set are only used in an eager-first tf2 setting, so it doesn't need special graph-specific logic. A lot of the graph-specific logic in backend has been prone to be fairly buggy and prone to error, which is why I try to avoid it when working on things that aren't needed in tf v1 graph compatibility.", "@tomerk Thank you for exposing some history"]}, {"number": 43823, "title": "fix grappler/costs:op_performance_data dependency", "body": "1. when tensorflow is used as third party, there is a dependency\r\n   problem when building on macOS. E.g.,\r\n   https://github.com/mlperf/mobile_app/issues/90\r\n\r\n2. op_performance_data.proto doesn't really need all the\r\n   tf_additional_all_protos()", "comments": []}, {"number": 43822, "title": "tensorflow.vectorized_map cannot recive loop variable", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): ananconda\r\n- TensorFlow version (use command below): 2.0.2\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.2\r\n- GPU model and memory: GTX1080ti RTX2080ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nThe tensorflow.vectorized_map function cannot receive the positional parameters in the loop variable.\r\n\r\n**Describe the expected behavior**\r\nThe tensorflow.vectorized_map function can receive the positional parameters in the loop variable like its predecessor map_fn function.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\narangelist = tf.cast(tf.range(start=0, limit=1000 - 100 + 1, delta=1), dtype=tf.float32)\r\ndef errfunc(Index):\r\n    \"\"\"\r\n    generate input for tf.vectorized_map\r\n    :param Index:\r\n    :return:\r\n    \"\"\"\r\n    Index = tf.cast(Index, dtype=tf.int32)\r\n    return Index\r\nc = tf.vectorized_map(errfunc, arangelist)\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Can you format your code block?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43822\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43822\">No</a>\n", "`\r\nimport tensorflow as tf\r\narangelist = tf.cast(tf.range(start=0, limit=1000 - 100 + 1, delta=1), dtype=tf.float32)\r\nTemp_Input_Tensor = tf.ones((128, 4000))\r\n\r\ndef errfunc(Index):\r\n    Index = tf.cast(Index, dtype=tf.int32)\r\n    Sequence_Fragment_Tensor = Temp_Input_Tensor[:,Index * 4:(Index * 4 + 10 * 4)]  \r\n    return Sequence_Fragment_Tensor\r\nc = tf.vectorized_map(errfunc, arangelist)\r\n`\r\n\r\n\r\n\r\nif I run the code above, I will get the \"ValueError: Input \"input\" of op \"StridedSlice\" expected to be not loop invariant\"", "```python\r\nimport tensorflow as tf\r\narangelist = tf.cast(tf.range(start=0, limit=1000 - 100 + 1, delta=1), dtype=tf.float32)\r\nTemp_Input_Tensor = tf.ones((128, 4000))\r\n\r\ndef errfunc(Index):\r\n  Index = tf.cast(Index, dtype=tf.int32)\r\n  Sequence_Fragment_Tensor = Temp_Input_Tensor[:,Index * 4:(Index * 4 + 10 * 4)]\r\n  return Sequence_Fragment_Tensor\r\nc = tf.vectorized_map(errfunc, arangelist)\r\n```", "With TF 2.3.1 I just get a Warning:\r\n`WARNING:tensorflow:Using a while_loop for converting StridedSlice`", "@sybwjdnr,\r\nI was able to reproduce the error with [TF v2.0.2](https://colab.research.google.com/gist/amahendrakar/430f6301b99595e38f0ad3e5bdc888ed/43822-2-0.ipynb), however the issues seems to be fixed with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/72e8a4d02125bc75e144cb133796a612/43822.ipynb). Please find the attached gist.\r\n\r\nCould you please update TensorFlow to v2.3 and check if it works. Thanks!", "thank you!", "Closing the issue, as it is resolved. Please feel free to re-open if necessary. Thanks!"]}, {"number": 43821, "title": "Serial printing breaks after updating Mbed OS", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Linux Ubuntu 20.04\r\n- TensorFlow installed from source\r\n- Target platform Arm Mbed OS for the NXP FRDM K66F board\r\n\r\n**Describe the problem**\r\nThe version of Mbed OS used for this [example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech#deploy-to-nxp-frdm-k66f) is old and unsupported. After running the update command to get the latest version there is a problem in the mbed/DebugLog.cc file since it uses a deprecated API (Serial) that is no longer available on the latest version of Mbed OS (v6.3.0). \r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nThe steps to build the working application for the K66F micro speech example directly from the Tensor Flow repo are the following:\r\n1. git clone https://github.com/tensorflow/tensorflow.git \r\n2. cd tensorflow \r\n3. make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"nxp_k66f\" generate_micro_speech_mbed_project \r\n4. cd tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed \r\n5. mbed config root . \r\n6. mbed deploy \r\n7. mbed compile -m K66F -t GCC_ARM --flash \r\n8. sudo screen /dev/ttyACM0 9600\r\n\r\nThis builds and executes correctly.\r\n \r\nTo update to the Mbed 6.3 version after step 6 we have done the following: \r\n\r\n\r\n7. cd mbed-os/\r\n8. mbed update mbed-os-6.3.0\r\n9. cd ..\r\n10. In /tensorflow/lite/micro/mbed/debug_log.cc I have changed the function to\r\n```\r\nextern \"C\" void DebugLog(const char* s) to:\r\n{\r\n  static BufferedSerial serial(USBTX,USBRX);\r\n        printf(\"%s\", s);\r\n}\r\n```\r\n11. make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"nxp_k66f\" generate_micro_speech_mbed_project \r\n12. mbed compile -m K66F -t GCC_ARM --flash\r\n13. sudo screen /dev/ttyACM0 9600 \r\n \r\nThis compiles correctly but does not correctly display the serial output for the \"yes/no\" voice commands. Instead, we get a memory error which says \"8 bytes lost due to alignment. To avoid this loss, please make sure the tensor_arena is 16 bytes aligned.\"\r\n \r\n\r\n\r\n", "comments": ["Figured out how to fix this. I'll submit a PR later this week/early next week. Just need to ensure how to do it in such a way that it doesn't break the other examples.", "@spartacoos Could you please let us know if this is still an issue in TF v2.6.0 ,please refer to [build from source](https://www.tensorflow.org/install/source#tested_build_configurations) ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43821\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43821\">No</a>\n"]}, {"number": 43820, "title": "Fix missing raise", "body": "Try to fix  https://github.com/tensorflow/tensorflow/issues/43818", "comments": []}, {"number": 43819, "title": "Fix operator check", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/43817", "comments": []}, {"number": 43818, "title": "Exceptions not raised because the \"raise\" keyword is missing in a few places", "body": "\r\nHello,\r\n\r\nWhile analyzing Tensorflow on SonarCloud I saw what looks like two errors in [`tensorflow/python/tpu/tpu_embedding.py`](https://github.com/tensorflow/tensorflow/blob/c6c6d90b29d4f9ccf501520ae691ccd58f0bbd85/tensorflow/python/tpu/tpu_embedding.py#L1639) and [`tensorflow/python/keras/losses.py`](https://github.com/tensorflow/tensorflow/blob/090f260aab3dab00bcdf0232962e753bb9fab696/tensorflow/python/keras/losses.py#L183):\r\n\r\n<img width=\"1846\" alt=\"Screenshot 2020-10-06 at 14 44 31\" src=\"https://user-images.githubusercontent.com/40498978/95203471-18cd2400-07e3-11eb-98f2-288b909ffe28.png\">\r\n\r\n\r\n<img width=\"1834\" alt=\"Screenshot 2020-10-06 at 14 44 48\" src=\"https://user-images.githubusercontent.com/40498978/95203495-1ff43200-07e3-11eb-9d15-b7830b794c7f.png\">\r\n\r\n\r\nYou can see both issues in SonarCloud [here](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60QzlBMD9OHnI8rBe&open=AXT60QzlBMD9OHnI8rBe) and [here](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60O-aBMD9OHnI8qI7&open=AXT60O-aBMD9OHnI8qI7).\r\n\r\nThe problem is pretty simple: exceptions are created but not raised because the `raise` keyword is missing. This is a pretty common mistake in python ;)\r\n\r\nIn case you have any question, suggestion or if you see a False Positive on SonarCloud you can reach out on [SonarSource community forum](https://community.sonarsource.com/).\r\n\r\nA few notes in case you want to use SonarCloud:\r\n* I am currently testing the python analyzer so [the project on SonarCloud](https://sonarcloud.io/dashboard?id=nicolas-harraudeau-sonarsource_tensorflow) will only show python issues, but SonarCloud can also analyze C/C++ code and other languages.\r\n* SonarCloud can also import pylint issues in case you want to use a rule SonarCloud does not already provide. Note however that pylint rules and SonarCloud rules are implemented differently. You might see new issues with SonarCloud, or less issues in some cases (we try to avoid False Positives as much as possible).\r\n* It is free for open-source projects.", "comments": ["That's a good find. Please tag me in all similar issues from SonarSource.", "Hi @mihaimaruseac and thank you.\r\nDo you mean any issue found by SonarCloud? I recently reported #43817, #43813 and #43816.\r\nI didn't report every issue our analyzer categorized as a bug because I didn't have time. You can take a look at the list of bugs [here](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&languages=py&open=AXT60OurBMD9OHnI8qEG&resolved=false&types=BUG). It's quite short.\r\n\r\nYou might also be interested in some code smell issues such as:\r\n* [this one](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60RJRBMD9OHnI8rFL&open=AXT60RJRBMD9OHnI8rFL), it looks to me like a classic [modification of a default argument](https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments) which could lead to a bug one day.\r\n* [or this one](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60Q4FBMD9OHnI8rCo&open=AXT60Q4FBMD9OHnI8rCo). I think that this is actually a bug. `len` cannot be called on an integer, so `len(len(...))` will fail.\r\n\r\nWe design our code analyzers to be quite conservative and avoid categorizing as bug things which might be code smells. That way developers are not disturbed with False Positives and trust our tools. The disadvantage is that some bugs are categorized as code smells.\r\n\r\nAt SonarSource we validate our static code analyzers using popular open source projects, such as Tensorflow, to make sure that we raise valuable issues and have no False Positives. This is how I saw these bugs.\r\n\r\nAs mentioned above, if you see any False Positives or if you have a question or feedback don't hesitate to share it [on the community forum](https://community.sonarsource.com/).", "@mihaimaruseac Let me know. I can pick some of these.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43818\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43818\">No</a>\n", "Thank you @nicolas-harraudeau-sonarsource \r\n\r\n@bhack sure, thank you. I have so many things to do and team is severly downsized, so any help is great"]}, {"number": 43817, "title": "Duplicate condition in \"is_square\"", "body": "Hello,\r\n\r\nWhile analyzing Tensorflow on SonarCloud I saw what looks like an error in [`tensorflow/python/ops/linalg/registrations_util.py`](https://github.com/tensorflow/tensorflow/blob/9648531c0bf5163de26e8688c017d58b3eb80405/tensorflow/python/ops/linalg/registrations_util.py#L59):\r\n\r\n<img width=\"1863\" alt=\"Screenshot 2020-10-06 at 14 23 29\" src=\"https://user-images.githubusercontent.com/40498978/95201004-9a22b780-07df-11eb-919e-180242560a57.png\">\r\n\r\nYou can see the issue in SonarCloud [here](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60PaoBMD9OHnI8qUK&open=AXT60PaoBMD9OHnI8qUK).\r\n\r\nThe condition `operator_a.is_square is not None and operator_a.is_square is not None` doesn't make sense as it checks twice the same thing. I guess what the developer intended was `operator_a.is_square is not None and operator_b.is_square is not None` but I can't be sure as I don't know this code base.\r\n\r\nIn case you have any question, suggestion or if you see a False Positive on SonarCloud you can reach out on [SonarSource community forum](https://community.sonarsource.com/).\r\n\r\nA few notes in case you want to use SonarCloud:\r\n* I am currently testing the python analyzer so [the project on SonarCloud](https://sonarcloud.io/dashboard?id=nicolas-harraudeau-sonarsource_tensorflow) will only show python issues, but SonarCloud can also analyze C/C++ code and other languages.\r\n* SonarCloud can also import pylint issues in case you want to use a rule SonarCloud does not already provide. Note however that pylint rules and SonarCloud rules are implemented differently. You might see new issues with SonarCloud, or less issues in some cases (we try to avoid False Positives as much as possible).\r\n* It is free for open-source projects.", "comments": ["Thanks but I think that it is hard to auto identify the \"real\" issue :wink: https://github.com/tensorflow/tensorflow/pull/43819", "Thanks for your rapid answer @bhack.\r\nI'm not sure to understand. Do you mean that the message provided by SonarCloud was not describing the problem properly? Or that I shouldn't have suggested a fix? I hesitated before doing it as I didn't want to introduce a bug with my suggestion.", "@nicolas-harraudeau-sonarsource No I just meant that it is a little bit more hard to automatically check that the required operation was on the 2nd arg (as hint). ", "@bhack Yes you are totally right. That's why the message says \"Correct **one of** the identical sub-expressions\". Sadly static code analyzers cannot provide fixes in many cases, developers will not be out of jobs soon ;)\r\n\r\nDid you find the highlight on the second value confusing? We could highlight the whole `and` expression but it would create some confusions in cases such as `(a and b) and (a and b)`. Developers would not understand right away which `and` operand we are talking about. I'm open to any suggestion.", "I think it is ok, we will need advanced NLP models for this :wink: At some point Tensorflow models could generate this type of hints on its own code.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43817\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43817\">No</a>\n"]}, {"number": 43816, "title": "TypeError in \"Bidirectional.compute_output_shape\"", "body": "Hello,\r\n\r\nWhile analyzing Tensorflow on SonarCloud I saw what looks like an error in [`tensorflow/python/keras/layers/wrappers.py`](https://github.com/tensorflow/tensorflow/blob/d6426459bca2971b611ca9773858e55f6cc1a9af/tensorflow/python/keras/layers/wrappers.py#L522):\r\n\r\n<img width=\"1899\" alt=\"Screenshot 2020-10-06 at 14 11 27\" src=\"https://user-images.githubusercontent.com/40498978/95199955-eb31ac00-07dd-11eb-8bee-2a0c006ed40f.png\">\r\n\r\n\r\nYou can see the issue in SonarCloud [here](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60Og4BMD9OHnI8p_E&open=AXT60Og4BMD9OHnI8p_E).\r\n\r\nAfter reviewing the code myself I can indeed see that `state_shape` will have the type `tuple`, and using the operator `+` on a `list` and a `tuple` will fail with the exception:\r\n> TypeError: can only concatenate list (not \"tuple\") to list\r\n\r\nHere is the flow which leads to this issue:\r\n```\r\noutput_shape = tuple(output_shape.as_list())  # output_shape is a tuple\r\n...\r\nif self.return_state:\r\n      state_shape = output_shape[1:]  # state_shape is a tuple\r\n...\r\nreturn [output_shape] + state_shape + copy.copy(state_shape)  # the expression \"[output_shape] + state_shape\" will fail\r\n```\r\n\r\nIn case you have any question, suggestion or if you see a False Positive on SonarCloud you can reach out on [SonarSource community forum](https://community.sonarsource.com/).\r\n\r\nA few notes in case you want to use SonarCloud:\r\n* I am currently testing the python analyzer so [the project on SonarCloud](https://sonarcloud.io/dashboard?id=nicolas-harraudeau-sonarsource_tensorflow) will only show python issues, but SonarCloud can also analyze C/C++ code and other languages.\r\n* SonarCloud can also import pylint issues in case you want to use a rule SonarCloud does not already provide. Note however that pylint rules and SonarCloud rules are implemented differently. You might see new issues with SonarCloud, or less issues in some cases (we try to avoid False Positives as much as possible).\r\n* It is free for open-source projects.", "comments": ["@nicholasbutlin \r\nPlease share your tf version as you have not filled in the issue template.", "> @nicholasbutlin\r\n> Please share your tf version as you have not filled in the issue template.\r\n\r\nHi - Not me this time! Think this is for someone else...", "/cc @mihaimaruseac ", "Hi @Saduf2019. The bug I am describing here is directly in the master branch of tensorflow. I didn't check if it was released in a specific version of tensorflow.\r\n\r\nI read the template but it doesn't seem to apply in this case. I am not describing a runtime issue but rather a bug visible in tensorflow's code. I apologize if there is a specific template for this case and I missed it.", "I think this might be a leftover from Py2. Let's try to convert both sides of `+` to list?", "A list of tuples?", "@nicolas-harraudeau-sonarsource \r\nIs this still an issue.", "Hi @Saduf2019,\r\nAs far as I can see the code remained the same, so [the bug is still there](https://github.com/tensorflow/tensorflow/blob/574463afb8581ca65988aee0ce1c956b4614e061/tensorflow/python/keras/layers/wrappers.py#L525). The same pattern I described earlier is visible:\r\n\r\n```\r\noutput_shape = tuple(output_shape.as_list())  # output_shape is a tuple\r\n...\r\nif self.return_state:\r\n      state_shape = output_shape[1:]  # state_shape is a tuple\r\n...\r\nreturn [output_shape] + state_shape + copy.copy(state_shape)  # the expression \"[output_shape] + state_shape\" will fail\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43816\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43816\">No</a>\n"]}, {"number": 43815, "title": "Failed to load the native TensorFlow runtime", "body": "Im on Windows10\r\nPython 3.7.7\r\nTensorflow 2.2\r\nKeras 2.4.3\r\n\r\nwhen i run\r\n`from tensorflow.keras.datasets import mnist`\r\n\r\ni get \r\n\r\n![image](https://user-images.githubusercontent.com/28235664/95198114-98062c00-07d2-11eb-887b-4a787dafaf19.png)\r\n![image](https://user-images.githubusercontent.com/28235664/95198139-a3f1ee00-07d2-11eb-9ab7-578ab22b99dc.png)\r\n", "comments": ["@Marouaneghoulami \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Thanks!", "@ravikyram \r\nMy processor is : \r\n![image](https://user-images.githubusercontent.com/28235664/95199982-8ffbbb80-07d5-11eb-894d-d1696daad76a.png)\r\n\r\nand i have x64 windows10\r\npython x64\r\n\r\nI will see and let you know.\r\nThanks.", "@ravikyram \r\n\r\nHi,\r\ni installed :\r\n**Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019**\r\n\r\nand downgraded Numpy\r\n\r\n> tensorflow 2.3.1 requires numpy<1.19.0,>=1.16.0\r\n\r\nNow its all good, thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43815\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43815\">No</a>\n"]}, {"number": 43814, "title": "Add GPU kernel for ApplyProximalAdagrad", "body": "First PR from splitting https://github.com/tensorflow/tensorflow/pull/43299 up into multiple PRs.\r\n\r\nI'm not sure if the relevant tests need to be told to run on the GPU.\r\n\r\ncc @nluehr @sanjoy ", "comments": ["> I'm not sure if the relevant tests need to be told to run on the GPU.\r\n\r\nI suspect not, but you can verify by intentionally introducing a bug and re-running.  :)"]}, {"number": 43813, "title": "Buggy string concatenation in \"experimental_tpu_predict_loop\" and \"experimental_tpu_test_loop\"", "body": "Hello,\r\n\r\nWhile analyzing Tensorflow on SonarCloud I saw what looks like two errors in `tensorflow/python/keras/engine/training_distributed_v1.py` [here](https://github.com/tensorflow/tensorflow/blob/d6426459bca2971b611ca9773858e55f6cc1a9af/tensorflow/python/keras/engine/training_distributed_v1.py#L536-L537) and [here](https://github.com/tensorflow/tensorflow/blob/d6426459bca2971b611ca9773858e55f6cc1a9af/tensorflow/python/keras/engine/training_distributed_v1.py#L389-L390):\r\n\r\n<img width=\"1833\" alt=\"Screenshot 2020-10-06 at 11 45 32\" src=\"https://user-images.githubusercontent.com/40498978/95185842-8c623780-07c9-11eb-96b4-a128cb3fb09f.png\">\r\nI put only one screenshot as the code is identical for both issues.\r\n\r\nYou can see the issues in SonarCloud [here](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60OurBMD9OHnI8qEF&open=AXT60OurBMD9OHnI8qEF) and [here](https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60OurBMD9OHnI8qEI&open=AXT60OurBMD9OHnI8qEI).\r\n\r\nThe problem is quite simple: the developer expected both strings to concatenate automatically. However as there are no parentheses nor a backslash at the end of the first line, these are two separate statements: one assignment and one string formatting which is then discarded.\r\n\r\nIn case you have any question, suggestion or if you see a False Positive on SonarCloud you can reach out on [SonarSource community forum](https://community.sonarsource.com/).\r\n\r\nA few notes in case you want to use SonarCloud:\r\n* I am currently testing the python analyzer so [the project on SonarCloud](https://sonarcloud.io/dashboard?id=nicolas-harraudeau-sonarsource_tensorflow) will only show python issues, but SonarCloud can also analyze C/C++ code and other languages.\r\n* SonarCloud can also import pylint issues in case you want to use a rule SonarCloud does not already provide. Note however that pylint rules and SonarCloud rules are implemented differently. You might see new issues with SonarCloud, or less issues in some cases (we try to avoid False Positives as much as possible).\r\n* It is free for open-source projects.", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43813\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43813\">No</a>\n", "/cc @frankchn Please check PR connected to ticket on the right bar next time (https://github.com/tensorflow/tensorflow/pull/43828)"]}, {"number": 43812, "title": "replace PFLR DeviceGetContext hardcode with Device::IsRemoteCallAllowed", "body": "This delegates decision of whether a particular device type\r\ncan host a remote call to the device implementation itself.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43812) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!\r\n________________________________\r\nFrom: googlebot <notifications@github.com>\r\nSent: Tuesday, October 6, 2020 10:50 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Pawel Piskorski <ppiskorski@habana.ai>; Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] replace PFLR DeviceGetContext hardcode with Device::IsRemoteCallAllowed (#43812)\r\n\r\n\r\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n\r\n\ud83d\udcdd Please visit https://cla.developers.google.com/ to sign.\r\n\r\nOnce you've signed (or fixed any issues), please reply here with @googlebot I signed it! and we'll verify it.\r\n\r\n________________________________\r\nWhat to do if you already signed the CLA\r\nIndividual signers\r\n\r\n  *   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data<https://cla.developers.google.com/clas> and verify that your email is set on your git commits<https://help.github.com/articles/setting-your-email-in-git/>.\r\n\r\nCorporate signers\r\n\r\n  *   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot<http://go/cla#troubleshoot> (Public version<https://opensource.google/docs/cla/#troubleshoot>).\r\n  *   The email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data<https://cla.developers.google.com/clas> and verify that your email is set on your git commits<https://help.github.com/articles/setting-your-email-in-git/>.\r\n  *   The email used to register you as an authorized contributor must also be attached to your GitHub account<https://github.com/settings/emails>.\r\n\r\n\u2139\ufe0f Googlers: Go here<https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43812> for more info.\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/43812#issuecomment-704126947>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AQ2PARS3M5VJOQICZKWOW7DSJLK5PANCNFSM4SFXGN7Q>.\r\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43812) for more info**.\n\n<!-- ok -->", "Can you give more context about why you make this change? Does it cause any bugs/issues in your use case?\r\n\r\nI think some device types (e.g. composite device) are not supposed to be used by PFLR. Looks like it indicates some bugs in the caller site as well.\r\n\r\nThanks.", "Hi qqfish\r\nThanks for comment!\r\n> \r\n> \r\n> Can you give more context about why you make this change? Does it cause any bugs/issues in your use case?\r\n> \r\n> I think some device types (e.g. composite device) are not supposed to be used by PFLR. Looks like it indicates some bugs in the caller site as well.\r\n> \r\n> Thanks.\r\n\r\nThis PR is in reference to a [discussion I started on tf.developers](https://groups.google.com/a/tensorflow.org/g/developers/c/q8C3CkMkKew). In short, we are adding support for a hardware accelerator device that has significant speedup when prefetch_to_device is used in the data pipeline. That modifier creates an iterator on the accelerator device that executes the whole pipeline on CPU using RemoteCall which is only allowed for a limted set of devices that's currently hardcoded in ProcessFunctionLibraryRuntime::GetDeviceContext.\r\n\r\nMy naive proposal with this PR is to extend device interface so each device class can testify if it can be a remote caller or not. \r\n", "Thanks for the context. It sgtm.", "@ppiskorski Can you please check @qqfish's comments and keep us posted ? Thanks!", "@ppiskorski  Can you please resolve conflicts? Thanks!"]}, {"number": 43811, "title": "Unable to build tflite for aarch64", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version: master branch (latest)\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): Android NDK toolchain 21.1.6352462 and GCC aarch64 toolchain 8.3 (2019.3)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nbuild_aarch64_lib.sh failed because it tries to build x86 cpuid source files\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./build_aarch64_lib.sh\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/81ddb09077702debafd61e8ef4312f5806348266\r\n\r\nAfter the above patch is merged, aarch64 build failed.\r\nif you need any further information please let me know.", "comments": ["I have the same issue", "Confirmed. Preparing a fix.", "I confirmed that the build is successfully done.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43811\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43811\">No</a>\n"]}, {"number": 43810, "title": "Support tf.keras.metrics.MeanIoU on TPU", "body": "Please add support for tf.keras.metrics.MeanIoU on TPU", "comments": ["Please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/f4a2ee991f57f67f240e881034d26870/github_54125.ipynb) with a working example of `tf.keras.metrics.MeanIoU` on colab TPU. Thanks!\r\n", "> \r\n\r\nlol, I totally forgot this issue, since I solved this issue immediately (by motified some TensorFlow codes) after I reported it.\r\n\r\nClose this issue, and thx for your reply :)  @sachinprasadhs \r\n"]}, {"number": 43809, "title": "Exploding loss when converting tf1 to tf2", "body": "I want to convert this notebook written in tf1 to tf2, more specifically the softmax reccommender (Section VI. Softmax model)  model in there:\r\nhttps://colab.research.google.com/github/google/eng-edu/blob/master/ml/recommendation-systems/recommendation-systems.ipynb\r\n\r\nThis is from the ML Course, previous steps describe the logic: https://developers.google.com/machine-learning/recommendation/labs/movie-rec-softmax-programming-exercise\r\n\r\nI have tensorflow version '2.3.1' on Mac OS X\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom zipfile import ZipFile\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom pathlib import Path\r\nimport matplotlib.pyplot as plt\r\ntf.config.run_functions_eagerly(True)\r\n# tf.random.set_seed(42)\r\n# np.random.seed(42)\r\n\r\nfrom urllib.request import urlretrieve\r\nimport zipfile\r\nurlretrieve(\"http://files.grouplens.org/datasets/movielens/ml-100k.zip\", \"movielens.zip\")\r\nzip_ref = zipfile.ZipFile('movielens.zip', \"r\")\r\nzip_ref.extractall()\r\nprint(\"Done. Dataset contains:\")\r\nprint(zip_ref.read('ml-100k/u.info'))\r\n\r\n# Load each data set (users, movies, and ratings).\r\nusers_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\r\nusers = pd.read_csv(\r\n    'ml-100k/u.user', sep='|', names=users_cols, encoding='latin-1')\r\n\r\nratings_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\r\nratings = pd.read_csv(\r\n    'ml-100k/u.data', sep='\\t', names=ratings_cols, encoding='latin-1')\r\n\r\n# The movies file contains a binary feature for each genre.\r\ngenre_cols = [\r\n    \"genre_unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\r\n    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\r\n    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\r\n]\r\nmovies_cols = [\r\n    'movie_id', 'title', 'release_date', \"video_release_date\", \"imdb_url\"\r\n] + genre_cols\r\nmovies = pd.read_csv(\r\n    'ml-100k/u.item', sep='|', names=movies_cols, encoding='latin-1')\r\n\r\n# Since the ids start at 1, we shift them to start at 0.\r\nusers[\"user_id\"] = users[\"user_id\"].apply(lambda x: str(x-1))\r\nmovies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: str(x-1))\r\nmovies[\"year\"] = movies['release_date'].apply(lambda x: str(x).split('-')[-1])\r\nratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: str(x-1))\r\nratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: str(x-1))\r\nratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))\r\n\r\n# Compute the number of movies to which a genre is assigned.\r\ngenre_occurences = movies[genre_cols].sum().to_dict()\r\n\r\n# Since some movies can belong to more than one genre, we create different\r\n# 'genre' columns as follows:\r\n# - all_genres: all the active genres of the movie.\r\n# - genre: randomly sampled from the active genres.\r\ndef mark_genres(movies, genres):\r\n  def get_random_genre(gs):\r\n    active = [genre for genre, g in zip(genres, gs) if g==1]\r\n    if len(active) == 0:\r\n      return 'Other'\r\n    return np.random.choice(active)\r\n  def get_all_genres(gs):\r\n    active = [genre for genre, g in zip(genres, gs) if g==1]\r\n    if len(active) == 0:\r\n      return 'Other'\r\n    return '-'.join(active)\r\n  movies['genre'] = [\r\n      get_random_genre(gs) for gs in zip(*[movies[genre] for genre in genres])]\r\n  movies['all_genres'] = [\r\n      get_all_genres(gs) for gs in zip(*[movies[genre] for genre in genres])]\r\n\r\nmark_genres(movies, genre_cols)\r\n\r\n# Create one merged DataFrame containing all the movielens data.\r\nmovielens = ratings.merge(movies, on='movie_id').merge(users, on='user_id')\r\n\r\n# Utility to split the data into training and test sets.\r\ndef split_dataframe(df, holdout_fraction=0.1):\r\n  \"\"\"Splits a DataFrame into training and test sets.\r\n  Args:\r\n    df: a dataframe.\r\n    holdout_fraction: fraction of dataframe rows to use in the test set.\r\n  Returns:\r\n    train: dataframe for training\r\n    test: dataframe for testing\r\n  \"\"\"\r\n  test = df.sample(frac=holdout_fraction, replace=False, random_state=42)\r\n  train = df[~df.index.isin(test.index)]\r\n  return train, test\r\n\r\nrated_movies = (ratings[[\"user_id\", \"movie_id\"]]\r\n                .groupby(\"user_id\", as_index=False)\r\n                .aggregate(lambda x: list(x)))\r\n\r\n\r\nBATCH_SIZE = 32\r\nEMBEDDING_SIZE = 35\r\n\r\nclass SoftmaxRecommender(keras.Model):\r\n    def __init__(self, feature_columns, **kwargs):\r\n        super(SoftmaxRecommender, self).__init__(**kwargs)\r\n        self.movie_embedding = layers.DenseFeatures(feature_columns['movie'])\r\n        self.other_features = layers.DenseFeatures(feature_columns['other'])\r\n        self.hidden_layer = layers.Dense(EMBEDDING_SIZE,\r\n            kernel_initializer=keras.initializers.TruncatedNormal(\r\n            stddev=1. / np.sqrt(EMBEDDING_SIZE) / 10.))\r\n        # self.linear = layers.Dense(num_movies, use_bias=True, activation='softmax')\r\n        # self.softmax_activation = keras.layers.Activation('softmax')\r\n\r\n\r\n    def call(self, inputs, train=True, **kwargs):\r\n        V = self.movie_embedding({'movie_id': inputs['movie_id']})\r\n        inputs.pop('movie_id')\r\n        other_fts = self.other_features(inputs)\r\n        U = tf.concat([V,other_fts], axis=1)\r\n        U = self.hidden_layer(U)\r\n        logits = tf.matmul(U, self.movie_embedding.get_weights()[0], transpose_b=True)\r\n        # logits = self.linear(logits)\r\n        # logits = self.softmax_activation(logits)\r\n        return logits\r\n\r\n\r\ndef custom_loss(labels, logits):\r\n    # labels = tf.reshape(labels, [-1])\r\n    # labels_org = labels.copy()\r\n    labels = select_random(labels)\r\n    # labels = tf.cast(labels, 'float32')\r\n    labels = tf.reshape(labels, [-1, 1])\r\n    # cce = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n    # preds = tf.argmax(logits, axis=1)\r\n    # preds = tf.cast(preds, 'float32')\r\n    # loss = cce(y_true=labels, y_pred=logits)\r\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\r\n        logits=logits, labels=labels))\r\n    return loss\r\n\r\n\r\n\r\n\r\nyears_dict = {\r\n    movie: year for movie, year in zip(movies[\"movie_id\"], movies[\"year\"])\r\n}\r\ngenres_dict = {\r\n    movie: genres.split('-')\r\n    for movie, genres in zip(movies[\"movie_id\"], movies[\"all_genres\"])\r\n}\r\n\r\ndef select_random(x):\r\n  \"\"\"Selectes a random elements from each row of x.\"\"\"\r\n  def to_float(x):\r\n    return tf.cast(x, tf.float32)\r\n  def to_int(x):\r\n    return tf.cast(x, tf.int64)\r\n  batch_size = tf.shape(x)[0]\r\n  rn = tf.range(batch_size)\r\n  nnz = to_float(tf.math.count_nonzero(x >= 0, axis=1))\r\n  rnd = tf.random.uniform([batch_size])\r\n  ids = tf.stack([to_int(rn), to_int(nnz * rnd)], axis=1)\r\n  return to_int(tf.gather_nd(x, ids))\r\n\r\ndef make_embedding_col(key, embedding_dim):\r\n  categorical_col = tf.feature_column.categorical_column_with_vocabulary_list(\r\n      key=key, vocabulary_list=list(set(list(movies[key].values))), num_oov_buckets=0)\r\n  return tf.feature_column.embedding_column(\r\n      categorical_column=categorical_col, dimension=embedding_dim,\r\n      initializer=keras.initializers.TruncatedNormal(\r\n                                             stddev=1. / np.sqrt(embedding_dim) / 10.),\r\n      # default initializer: trancated normal with stddev=1/sqrt(dimension)\r\n      combiner='mean')\r\n\r\ndef make_dataset(ratings):\r\n  \"\"\"Creates a batch of examples.\r\n  Args:\r\n    ratings: A DataFrame of ratings such that examples[\"movie_id\"] is a list of\r\n      movies rated by a user.\r\n    batch_size: The batch size.\r\n  \"\"\"\r\n  def pad(x, fill):\r\n    return pd.DataFrame.from_dict(x).fillna(fill).values\r\n\r\n  movie = []\r\n  year = []\r\n  genre = []\r\n  label = []\r\n  for movie_ids in ratings[\"movie_id\"].values:\r\n    movie.append(movie_ids)\r\n    genre.append([x for movie_id in movie_ids for x in genres_dict[movie_id]])\r\n    year.append([years_dict[movie_id] for movie_id in movie_ids])\r\n    label.append([int(movie_id) for movie_id in movie_ids])\r\n\r\n  features = {\r\n      \"movie_id\": pad(movie, \"\"),\r\n      \"year\": pad(year, \"\"),\r\n      \"genre\": pad(genre, \"\"),\r\n  }\r\n\r\n  y = pad(label, -1)\r\n  # y = select_random(y)\r\n  # print('y.nunique:', len(np.unique(y.numpy())))\r\n\r\n  return features, y\r\n\r\n\r\ntrain_rated_movies, test_rated_movies = split_dataframe(rated_movies)\r\nx_train, y_train = make_dataset(train_rated_movies)\r\nx_val, y_val = make_dataset(test_rated_movies)\r\n\r\n\r\n# create two separate DenseFeatures in model to be able to access movie_embeddings\r\nfeature_cols = {\r\n    'movie': [make_embedding_col(\"movie_id\", 35)],\r\n    'other': [make_embedding_col(\"genre\", 3), make_embedding_col(\"year\", 2)]\r\n}\r\n\r\nmodel = SoftmaxRecommender(feature_columns=feature_cols)\r\nmodel.compile(\r\n    loss=custom_loss, optimizer=keras.optimizers.SGD(.1, clipnorm=1)\r\n    # loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=keras.optimizers.SGD(.1, clipnorm=1)\r\n)\r\n\r\n# batch_x1 = {\r\n#     'movie_id':x_train['movie_id'][:32],\r\n#     'year':x_train['year'][:32],\r\n#     'genre':x_train['genre'][:32]\r\n#            }\r\n#\r\n# batch_y1 = y_train[:32]\r\n#\r\n# logits = model(batch_x1)\r\n# loss = custom_loss(batch_y1, logits)\r\n#\r\n# batch_x2 = {\r\n#     'movie_id':x_train['movie_id'][32:64],\r\n#     'year':x_train['year'][32:64],\r\n#     'genre':x_train['genre'][32:64]\r\n#            }\r\n#\r\n# batch_y2 = y_train[32:64]\r\n#\r\n# logits = model(batch_x2)\r\n# loss = custom_loss(batch_y2, logits)\r\n\r\nhistory = model.fit(\r\n    x=x_train,\r\n    y=y_train,\r\n    batch_size=32,\r\n    epochs=10,\r\n    verbose=2,\r\n    validation_data=(x_val, y_val),\r\n)\r\n```\r\n\r\nThe above model with custom_loss that uses tf.nn.softmax_cross_entropy_with_logits leads to exploding gradients.\r\n\r\nI also tried adding softmax activation in the model class and using keras.losses.SparseCategoricalCrossentropy(from_logits=True) while doing select_random for labels in make_dataset function once and not in each batch. This approach lead to weights staying stable. If I use keras.losses.SparseCategoricalCrossentropy without adding softmax activation to the logits, loss explodes again.\r\n\r\nMy understanding of keras.Model and its call() method might be wrong, and/or my DenseFeatures layer application. I understand that with exponentially increasing loss it might be that there is something wrong with the dataset, but data-preparation steps are directly copied from the original code.\r\n\r\nI highly appreciate any help!", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/44d9f4df61bee0612fd10291ff881c7a/43809.ipynb). Thanks!", "I think Stack Overflow can be a good platform to raise this issue since it is not a bug or feature request.\r\nHowever you may want to check out new [TensorFlow Recommenders](https://github.com/tensorflow/recommenders) package using TF 2 workflow for building recommender systems.\r\n"]}, {"number": 43808, "title": "Converting a LSTM model from Pytorch to Tensorflow using ONNX", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nI am trying to convert a very simple LSTM model from Pytorch to Tensorflow using ONNX. The model conversion from Pytorch to ONNX is happening, but I am unable to convert that ONNX model to Tensorflow.\r\n\r\n**Describe the expected behavior**\r\nSuccessfully convert LSTM from .onnx to .pb.\r\n\r\n**Standalone code to reproduce the issue**\r\nimport onnx\r\nfrom onnx_tf.backend import prepare\r\n\r\nonnx_model = onnx.load(\"./SimpleLSTM.onnx\")\r\ntf_rep = prepare(onnx_model)\r\ntf_rep.export_graph('./SimpleLSTM.pb')\r\n\r\n\r\n**Other info / logs** \r\nTraceback (most recent call last):\r\n  File \"testLSTM.py\", line 14, in <module>\r\n    tf_rep.export_graph('./SimpleLSTM.pb')\r\n  File \"/workspace/ctg_nitya/onnx-tensorflow/onnx_tf/backend_rep.py\", line 93, in export_graph\r\n    tf.saved_model.save(self.tf_module, path, signatures=self.tf_module.__call__.get_concrete_function(**self.signatures))\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 1167, in get_concrete_function\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 1074, in _get_concrete_function_garbage_collected\r\n    self._initialize_uninitialized_variables(initializers)\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 965, in _initialize_uninitialized_variables\r\n    return initialize_variables.get_concrete_function()()\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2939, in get_concrete_function\r\n    *args, **kwargs)\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2906, in _get_concrete_function_garbage_collected\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 3213, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 3075, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 957, in initialize_variables\r\n    inits, ops.get_default_graph(), op_map=op_map)\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/eager/lift_to_graph.py\", line 260, in lift_to_graph\r\n    add_sources=add_sources))\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_2.3_gpu/lib/python3.6/site-packages/tensorflow/python/ops/op_selector.py\", line 413, in map_subgraph\r\n    % (repr(init_tensor), repr(op), _path_from(op, init_tensor, sources)))\r\n**tensorflow.python.ops.op_selector.UnliftableError: Unable to lift tensor <tf.Tensor 'concat_2:0' shape=(6, 12) dtype=float32> because it depends transitively on placeholder <tf.Operation 'Squeeze_1/60' type=Placeholder> via at least one path, e.g.: concat_2 (ConcatV2) <- transpose_1 (Transpose) <- concat_1 (ConcatV2) <- split_1 (Split) <- Squeeze_1 (Squeeze) <- Squeeze_1/60 (Placeholder)**\r\n\r\n", "comments": ["@Nitya05 \r\nplease share stand alone code to replicate the issue.", "Following is the Pytorch code for creating an ONNX model for a LSTM:\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\n\r\nimport torch.onnx\r\nimport torchvision\r\n\r\ntorch.manual_seed(1)\r\n\r\nlstm = nn.LSTM(3, 3, num_layers=1)\r\nlstm.eval()\r\n\r\nwith torch.no_grad():\r\ninputs = [torch.randn(1,3) for _ in range(5)] #make a sequence of length 5\r\ninputs = torch.cat(inputs).view(len(inputs), 1, -1)\r\nh0 = torch.randn(1, 1, 3)\r\nc0 = torch.randn(1, 1, 3)\r\nout, (hn, cn) = lstm(inputs, (h0, c0))\r\n\r\ninput_names = [\u201cinput\u201d, \u201ch0\u201d, \u201cc0\u201d]\r\noutput_names = [\u201coutput\u201d, \u201chn\u201d, \u201ccn\u201d]\r\n\r\ntorch.onnx.export(lstm, (inputs, (h0, c0)), \u201cSimpleLSTM.onnx\u201d, input_names=input_names, output_names=output_names)\r\n\r\n\r\nAnd, I am using the below code for converting the obtained ONNX model to Tensorflow:\r\n\r\nimport onnx\r\nfrom onnx_tf.backend import prepare\r\n\r\nonnx_model = onnx.load(\"./SimpleLSTM.onnx\")\r\ntf_rep = prepare(onnx_model)\r\ntf_rep.export_graph('./SimpleLSTM.pb')", "@Nitya05 I am not an expert in pytorch. Can you please share a standalone code in google colab or jupyter notebook that can be run. I tried your code in google colab. Please check [gist here](https://colab.research.google.com/gist/jvishnuvardhan/3a79da9f92ccc137c873cfa564bc8c94/untitled.ipynb). Thanks!\r\n\r\n", "Hi @jvishnuvardhan \r\nI have made required installations in [here](https://colab.research.google.com/gist/jvishnuvardhan/3a79da9f92ccc137c873cfa564bc8c94/untitled.ipynb#scrollTo=83AWhRhpgK40). But, I am getting a different tensorflow error now :\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'ceil'\r\n", "Hi..\r\n\r\n Just came across this issue - I am facing the same problem trying to convert a LSTM model in ONNX format to Tensor flow.  To make sure, I also tried the simple example provided in the gist and can confirm I get the same error.\r\n", "> @Nitya05 I am not an expert in pytorch. Can you please share a standalone code in google colab or jupyter notebook that can be run. I tried your code in google colab. Please check [gist here](https://colab.research.google.com/gist/jvishnuvardhan/3a79da9f92ccc137c873cfa564bc8c94/untitled.ipynb). Thanks!\r\n\r\nHi @jvishnuvardhan @Nitya05 ,\r\nI changed your notebook to use tensorflow 1.15 and the error seems to go away and you do get you model exported into tensorflow format.\r\nhttps://colab.research.google.com/gist/dhruvsakalley/4ea0b1b4b1ea8c1e755eae731709d968/untitled.ipynb#scrollTo=83AWhRhpgK40", "Additionally this issue seems to be related, let me know if you get it working with tf2 https://github.com/onnx/onnx-tensorflow/issues/454", "@Nitya05 I think this is more related to onnx and i think @dhruvsakalley solution will help you. When I tried with TF2.x, the following part of the code was throwing an error. I think `onnx` was updated recent for `TF2.x`. hence, `from onnx_tf.backend import prepare` line throws an error.\r\n\r\n```\r\nimport onnx\r\nfrom onnx_tf.backend import prepare\r\nonnx_model = onnx.load(\"./SimpleLSTM.onnx\")\r\ntf_rep = prepare(onnx_model)\r\ntf_rep.export_graph('./SimpleLSTM.pb')\r\n```\r\nPlease close the issue if this was resolved for you. Thanks!", "Hi @dhruvsakalley,\r\nI tried to follow the solution mentioned in onnx/onnx-tensorflow#454, but it did not help in resolving my problem. I am still getting the following error:\r\n\r\n**tensorflow.python.ops.op_selector.UnliftableError: Unable to lift tensor <tf.Tensor 'concat_2:0' shape=(6, 12) dtype=float32> because it depends transitively on placeholder <tf.Operation 'Squeeze_1/60' type=Placeholder> via at least one path, e.g.: concat_2 (ConcatV2) <- transpose_1 (Transpose) <- concat_1 (ConcatV2) <- split_1 (Split) <- Squeeze_1 (Squeeze) <- Squeeze_1/60 (Placeholder)**\r\n\r\n\r\nAlso, I tried to use tensorflow 1.15 to run my code, but the import of onnx_tf is giving the following warning and error:\r\n\r\n**UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \r\n The versions of TensorFlow you are currently using is 1.15.0 and is not supported. \r\nSome things might work, some things might not.\r\nIf you were to encounter a bug, do not file an issue.\r\nIf you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \r\nYou can find the compatibility matrix in TensorFlow Addon's readme:\r\nhttps://github.com/tensorflow/addons\r\n  UserWarning,\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_1.15_gpu/lib/python3.6/site-packages/onnx_tf/__init__.py\", line 1, in <module>\r\n    from . import backend\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_1.15_gpu/lib/python3.6/site-packages/onnx_tf/backend.py\", line 27, in <module>\r\n    from onnx_tf.common.handler_helper import get_all_backend_handlers\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_1.15_gpu/lib/python3.6/site-packages/onnx_tf/common/handler_helper.py\", line 3, in <module>\r\n    from onnx_tf.handlers.backend import *  # noqa\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_1.15_gpu/lib/python3.6/site-packages/onnx_tf/handlers/backend/hardmax.py\", line 3, in <module>\r\n    import tensorflow_addons as tfa\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_1.15_gpu/lib/python3.6/site-packages/tensorflow_addons/__init__.py\", line 21, in <module>\r\n    from tensorflow_addons import activations\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_1.15_gpu/lib/python3.6/site-packages/tensorflow_addons/activations/__init__.py\", line 17, in <module>\r\n    from tensorflow_addons.activations.gelu import gelu\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_1.15_gpu/lib/python3.6/site-packages/tensorflow_addons/activations/gelu.py\", line 27, in <module>\r\n    @tf.keras.utils.register_keras_serializable(package=\"Addons\")\r\n  File \"/workspace/ctg_nitya/python3_venv_tf_1.15_gpu/lib/python3.6/site-packages/tensorflow_core/python/util/module_wrapper.py\", line 193, in __getattr__\r\n    attr = getattr(self._tfmw_wrapped_module, name)\r\nAttributeError: module 'tensorflow.python.keras.api._v1.keras.utils' has no attribute 'register_keras_serializable'**\r\n\r\n\r\n@jvishnuvardhan I am able to successfully import prepare. The problem happens while doing **tf_rep.export_graph('./SimpleLSTM.pb')**. Is there anything else that can be done to rectify this issue?", "I forgot to mention that I am able to successfully run the onnx model using onnxruntime. The problem happens only when I try to export the .onnx model to .pb.", "@Nitya05 \r\n\r\n I would recommend setting up a fresh conda/venv environment and install the following in order:\r\n\r\n!pip install tensorflow==1.15\r\n!pip install onnx\r\n!pip install onnx-tf\r\n\r\n It was the only way I got onnx-tf working.", "Thanks @khandeas. I did try a fresh install earlier, but I was using python3 then. I just tried to use python2 with tensorflow-1.15, and it worked. Thanks.", "I am closing this issue as it was resolved. Thanks!"]}, {"number": 43807, "title": "is it possible tensorflow-gpu  on RK3399 mali t-860?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): LUbuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: RK3399 \r\n- TensorFlow installed from (source or binary): pip3\r\n- TensorFlow version: tensorflow 1.14(cpu)\r\n- Python version: python3.5\r\n- Installed using virtualenv? pip? conda?: pip3\r\n- Bazel version (if compiling from source): none\r\n- GCC/Compiler version (if compiling from source): gcc 5.5\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: mali t-860\r\n\r\nhi.\r\n\r\nMy problem is running time is too long tensorflow(cpu version)\r\nso, i try to install tensorflow-gpu on RK3399  GPU- mali t-860 (ARM)\r\n\r\nbut i can't install yet. \r\nis it possible? \r\n\r\nif possible how to install it?\r\n\r\nThank you\r\n\r\n\r\n\r\n\r\n", "comments": [" https://www.tensorflow.org/install/source#tested_build_configurations these versions are for x86_64 devices.\r\nFor ARM build, you might need to build it by yourself.\r\n\r\nIf you want to try TFLite, check followings.\r\n- https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package#alternative-build-with-bazel-experimental\r\n- https://www.tensorflow.org/lite/guide/build_arm64", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43807\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43807\">No</a>\n"]}, {"number": 43806, "title": "Model containing EfficientNet cannot be restored", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.6 / Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below):v2.3.0-54-gfcc4b966f1 2.3.1 (Local),  v2.3.0-54-gfcc4b966f1 2.3.1 (Colab)\r\n- Python version: 3.7.3 (Local), 3.6.9 (Colab)\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n1. Trained a model containing pretrained EfficientNetB0 with GradientTape without compiling a model\r\n2. Saved a model by ```model.save('path')```\r\n3. Restoreing a model shows warning that gradient cannot be requested\r\n4. Finetuning fails\r\n\r\n**Describe the expected behavior**\r\nSuccessfully save, restore and finetune.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n[Colab](https://colab.research.google.com/drive/1gzOwSWJ1Kvwzo01SEpjqGq6Lb-OsI-ob?usp=sharing)\r\n[StackOverflow](https://stackoverflow.com/questions/64218503/finetune-savedmodel-failure-due-to-no-gradient-loaded)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nI followed this tutorial. How can I save and restore to train again with different learning rate for finetuning?\r\nhttps://www.tensorflow.org/tutorials/quickstart/advanced\r\n\r\nAt first I posted on StackOverflow and then tried to reproduce on Colab to give more information, but error messages are different. It might be because of data loading difference, but I'm not sure.\r\nThank you for reading.", "comments": ["This happens when I use EfficientnetB0, not happens when I replace it with ResNet50", "I found this third party repository is a workaround\r\nhttps://github.com/qubvel/efficientnet", "Can you unlock colab access?", "@bhack I unlocked. Thank you for checking.", "Check https://github.com/tensorflow/tensorflow/issues/43576", "@bhack \r\nThank you. It is a related issue. It seems no solution for now. So, If this is not a bug, is this expected to happen? Thank you!", "Yes probably you can close this and subscribe to that one or https://github.com/tensorflow/tensorflow/issues/40166"]}, {"number": 43805, "title": "Correct way to cast a RaggeredTensor to list of strings?", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Titan V\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to incorporate a customized function into tensorflow via `tf.py_function`, which is something like the following\r\n\r\n```\r\n    def run(x):\r\n        return tf.py_function(my_func, x, tf.float32, name=None)\r\n    out = tf.keras.layers.Lambda(run)(raggedtensor)\r\n```\r\n\r\nThe desired input of `my_func` is a list of strings, while the `raggedtensor` is `tf.RaggedTensor(values=Tensor(\"StringSplit/StringSplitV2:1\", shape=(None,), dtype=string), row_splits=Tensor(\"StringSplit/RaggedFromValueRowIds/concat:0\", shape=(None,), dtype=int64)).`\r\n\r\nThe current code will throw an error of `TypeError: Expected list for 'input' argument to 'EagerPyFunc' Op, not tf.RaggedTensor(values=Tensor...`, while if I changed it to the following\r\n\r\n```\r\n    def run(x):\r\n        return tf.py_function(my_func, x.to_list(), tf.float32, name=None)\r\n    out = tf.keras.layers.Lambda(run)(raggedtensor)\r\n```\r\n\r\nThe error will change to `ValueError: RaggedTensor.to_list() is only supported in eager mode; in graph mode, evaluate the RaggedTensor first and then use RaggedTensorValue.to_list().`\r\n\r\nIs there a way I could cast the tensor to my desired input?", "comments": ["@mralexisw \r\nI ran the code on tf nightly and do not face any errors, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/c45ab30f3e1631c8e9b58f29abced508/untitled418.ipynb).\r\nPlease verify on nightly and let us know.", "@Saduf2019 The gist only runs the definition of the function, which will definitely return no error. Could you see what will happen if you pass a tensor into it? Thanks!", "@mralexisw \r\nPlease provide with complete code to replicate or if possible share a colab gist with the error reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43805\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43805\">No</a>\n"]}, {"number": 43804, "title": "[INTEL MKL] MKL DNN 0.x code cleanup - Matmul op", "body": "DNN 0.x cleanup of MklMatmul op:\r\n\r\n(1) Remove all DNN 0.x related code \r\n\r\n(2) Replace all DNN 1.x macro usages", "comments": ["@gzmkl  Can you please resolve conflicts? Thanks!", "@gbaned I have addressed the merge conflict. Thanks!"]}, {"number": 43803, "title": "Set the \"nightly\" branch to master HEAD every day", "body": "This GitHub Actions workflow runs every night at 8pm PDT / 9pm PST, and\nsets the \"nightly\" branch to whatever master HEAD is at that time. The\nTensorFlow team uses this branch for our suite of nightly tests, NOT for\nthe \"tf-nightly\" releases.", "comments": ["Note that the `nightly` branch must exist (it already does) for this to work.", "Is 8pm PDT / 9pm PST ok? \r\nThis is the PST commit grid:\r\n![commit_punch](https://user-images.githubusercontent.com/1710528/95214666-6b610d00-07f0-11eb-84b8-ec1ac3cc0918.png)\r\n", "Is 23/24 lower traffic?", "It is, but it's also unlikely that there will be someone to shepherd build in case of infra changes. 9 pm is a better compromise in this light"]}]