[{"number": 43594, "title": "Split env library so that there is only one instance of `Env::Default()`", "body": "While working on modular file systems, noticed that tensorflow's pip package\r\nincludes two copies of `Env::Default()` in two shared objects libraries.\r\nOne is in `libtensorflow_framework.so`, another is in `_pywrap_tensorflow_internal.so`.\r\n\r\nBecause of that, `Env::Default()` could return different instance depending on when\r\na modular file system is called. The Lookup vs. Register discrepancy may confuse the file\r\nsystem to believe the scheme (e.g., `s3`/`gcs`) is not registered (actually registered\r\nby another `Env::Default()`).\r\n\r\nThe duplication of `Env::Default()` is caused by the fact that `tensorflow/core/platform:env`\r\ndependency is included in both `libtensorflow_framework.so` and `_pywrap_tensorflow_internal.so`.\r\nin bazel indirectly.\r\n\r\nThere are two many dpendency graph component in between `tensorflow/core/platform:env`\r\nand `libtensorflow_framework.so`/`_pywrap_tensorflow_internal.so` in bazel.\r\nAs a result this PR tries to address the issue by split out the `Env::Default()` implementation\r\nout and only link it with `libtensorflow_framework.so` to guarantee `libtensorflow_framework.so`\r\nis the only place to include `Env::Default()`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["This one needs a lot of changes internally since we also have a Google-only `Env` with separate implementations. I'll be working on importing this manually, but will probably take a little bit longer.", "There is actually an issue with splitting things out like here. External packages using TF Bazel (not from pip package) are supposed to be able to get to `Env:Default` by just depending on `//tensorflow/core:lib` but with this change they would fail.", "Thanks @mihaimaruseac for the help. I have made additional changes to the PR and `//tensorflow/core:lib` should include `Env:Default`. Can you give it a try?"]}, {"number": 43593, "title": "Possible GPU memory leak at tf.vectorized_map", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 18.04\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (use command below): 2.4.0-dev20200821\r\n- Python version:3.7\r\n- GPU model and memory:v100 / titan rtx\r\n\r\nWhen using tf.vectorized_map, GPU memory is continually increased, and finaly caused OOM after many training iterations. tf.map_fn works fine.\r\n", "comments": ["Looking at the code and documentation, looks like `vectorized_map` stacking the output for each iteration, which is causing increase in GPU memory after each iteration. The [documentation](https://www.tensorflow.org/api_docs/python/tf/vectorized_map) also says that  the function is `optimized to run much faster, possibly with a much larger memory footprint` as compared to `map_fn`, also the function `vectorized_map` is experimental. ", "> \r\n> \r\n> Looking at the code and documentation, looks like `vectorized_map` stacking the output for each iteration, which is causing increase in GPU memory after each iteration. The [documentation](https://www.tensorflow.org/api_docs/python/tf/vectorized_map) also says that the function is `optimized to run much faster, possibly with a much larger memory footprint` as compared to `map_fn`, also the function `vectorized_map` is experimental.\r\n\r\nI think a much larger memory footprint is acceptable. However, increasing in GPU memory after each iteration should be a bug? What do you think?", "@edwardyehuang \r\n\r\nI tried in colab with nightly version(2.4.0-dev20200927) and i am not seeing any OOM error.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/e670809edde31664fabb04f4e9be6092/untitled396.ipynb).Please, share the reproducible code to localize the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I found this issue is not caused by tf.vectorized_map. Close this issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43593\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43593\">No</a>\n", "I don't know why, but some commits in recent days solve my GPU memory leak issue"]}, {"number": 43591, "title": "RaggedTensor raises error with Keras model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip installed\r\n- TensorFlow version (use command below):2.3.0\r\n- Python version:3.7.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:RTX 2060 6G/16G RAM\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI builded a lstm model for one to one word classification:\r\n```\r\nmodel = Sequential()\r\nmodel.add(layers.Input(shape=(None, 512), ragged=True))\r\nmodel.add(layers.LSTM(32, return_sequences=True, dropout=0.4))\r\nmodel.add(layers.TimeDistributed(layers.Dense(13, activation='softmax')))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\r\nmodel.summary()\r\n```\r\nI then generated my x_train as a tf.RaggedTensor, with shape [10000, None, 512], dtype='float32'.\r\ny_train as a tf.RaggedTensor, with shape [10000, None, 13], dtype='float32'.\r\n\r\nNow if run it with:\r\n`history = model.fit(x, y, epochs=10, verbose=1)`\r\n\r\nI'd get:\r\n> TypeError: Failed to convert object of type <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. Contents: tf.RaggedTensor(values=Tensor(\"sequential_42/time_distributed_46/dense_54/Softmax:0\", shape=(None, 13), dtype=float32), row_splits=Tensor(\"sequential_42/time_distributed_46/RaggedFromRowLengths/control_dependency:0\", shape=(None,), dtype=int64)). Consider casting elements to a supported type.\r\n**Describe the expected behavior**\r\nTimeDistributed layer should support RaggedTensor if the underlying model can process slice of RaggedTensor and output dimension is matched.\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers, Model, Sequential\r\nimport numpy as np\r\n\r\nx = tf.RaggedTensor.from_row_splits(np.ones((100, 512)), [0, 4, 20, 100])\r\ny = tf.RaggedTensor.from_row_splits(np.ones((100, 13)), [0, 4, 20, 100])\r\nprint(x.shape)\r\nprint(y.shape)\r\nmodel = Sequential()\r\nmodel.add(layers.Input(shape=(None, 512), ragged=True))\r\nmodel.add(layers.LSTM(32, return_sequences=True, dropout=0.4))\r\nmodel.add(layers.TimeDistributed(layers.Dense(13, activation='softmax')))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\r\nmodel.summary()\r\nhistory = model.fit(x=x, y=y, epochs=10, verbose=1)\r\n```", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c3822d0a920d389d8a5ca6312bc1e424/43591.ipynb#scrollTo=DDbwlfhWR0vu). Thanks!", "@kuan0808 I guess the root-cause is the mismatch in input arguments (x and y) to `model.fit`. Please [check](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) what is supported by `model.fit`.\r\n\r\nThe error clearly says \r\n\r\n> TypeError: Failed to convert object of type <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. Contents: tf.RaggedTensor(values=Tensor(\"sequential/time_distributed/dense/Softmax:0\", shape=(None, 13), dtype=float32), row_splits=Tensor(\"sequential/time_distributed/RaggedFromRowLengths/control_dependency:0\", shape=(None,), dtype=int64)). Consider casting elements to a supported type.\r\n\r\nCan you please use supported types and let us know how it progresses. Thanks!", "Do you mean the dimension mismatch?\r\nI have check the dimension, and they should be matched. And both x, y are ragged tensors, which is supposed to supported by model.fit I think\ud83e\udd14", "Note that supporting RaggedTensors as targets (e.g., in `model.fit`) is a frequent request, see #44988, #44112, #43591, #43093, #42320, #41810.\r\n\r\nSome partial progress is in pull requests #45060 and #45015, which will allow using custom losses in Keras model (but existing losses like MSE or (S)CE will still not work).", "Note that I created a new feature request #45403 to support RaggedTensors in standard Keras loss functions.", "@kuan0808,\r\nIssue has been resolved in **`TF Nightly`** (`2.5.0-dev20210302`). Please find the [Gist of the working code](https://colab.research.google.com/gist/rmothukuru/9d24fde1d43daf19d376723196d97425/43591.ipynb#scrollTo=c9xLppX3TOis). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43590, "title": "[INTEL MKL] Fix a Conv2D unit test failure", "body": "Simple change of MklConvOp:  \r\n     Return a correct error message, to fix a conv test failure (Conv2DTest.**testOpEdgeCases** in\r\n     conv_ops_test.py). ", "comments": []}, {"number": 43589, "title": "some questions for coreml_executor.mm ", "body": "I have a few questions for tensorflow/lite/experimental/delegates/coreml/coreml_executor.mm:\r\n1.  line 143: id<MLFeatureProvider> outputFeature = [_model predictionFromFeatures:inputFeature options:options error:&error];\r\nHow does Coreml recognize custom constructs TensorData?\r\n\r\n2.line 80: if ([featureName cStringUsingEncoding:NSUTF8StringEncoding] == input.name) {}\r\nfeatureName  is output name of network and input.name is input name of network, the network output node name must not be equal to the input node name, this code doesn't make sense?\r\n", "comments": ["Hi @ToBigboss, thanks for asking!\r\n\r\n1. The `inputFeature` has type of `MultiArrayFeatureProvider`, inherited from [`MLFeatureProvider`](https://developer.apple.com/documentation/coreml/mlfeatureprovider) class. It's initializer handles `TensorData` struct, and the interface `FeatureValueForName` is actually called from Core ML side. That being said, `MultiArrayFeatureProvider` parses `TensorData` struct and returns appropriate `MLFeatureValue` for a name provided. (The logic that you mentioned in question 2.)\r\n\r\n2. The `MultiArrayFeatureProvider` is only used for inputs, so that the internal vector of `TensorData` struct is also named `_inputs`. Outputs are handled from line 150, and note that the output's `MLFeatureProvider` is the base class one, not the `MultiArrayFeatureProvider`, thus nothing to do with the code inside `MultiArrayFeatureProvider` :) \r\n\r\nPlease let me know if I missed something, or you need further clarifications. Thanks!", "> Hi @ToBigboss, thanks for asking!\r\n> \r\n> 1. The `inputFeature` has type of `MultiArrayFeatureProvider`, inherited from [`MLFeatureProvider`](https://developer.apple.com/documentation/coreml/mlfeatureprovider) class. It's initializer handles `TensorData` struct, and the interface `FeatureValueForName` is actually called from Core ML side. That being said, `MultiArrayFeatureProvider` parses `TensorData` struct and returns appropriate `MLFeatureValue` for a name provided. (The logic that you mentioned in question 2.)\r\n> 2. The `MultiArrayFeatureProvider` is only used for inputs, so that the internal vector of `TensorData` struct is also named `_inputs`. Outputs are handled from line 150, and note that the output's `MLFeatureProvider` is the base class one, not the `MultiArrayFeatureProvider`, thus nothing to do with the code inside `MultiArrayFeatureProvider` :)\r\n> \r\n> Please let me know if I missed something, or you need further clarifications. Thanks!\r\n\r\nget it\uff0cThanks very much"]}, {"number": 43588, "title": "Failed to build on Cuda-11.1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: related to servers\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: commit: a0b68d1ecc46f9bbc8fad4f18c68f25c6bd5ae48\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: none\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0-3ubuntu1\r\n- CUDA/cuDNN version: CUDA-11.1, CuDNN 8.0.3 for CUDA-11.0\r\n- GPU model and memory: RTX 3090, 24GB, RTX Titan, 24GB, RTX 2080Ti, 11GB (total three GPUs)\r\n\r\n\r\n\r\n**Describe the problem**\r\nHello,\r\nfailed to compile Tensorflow.\r\nPlease check the log messages then give me an advice.\r\nThanks!\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n(Including logging messages)\r\nbazel build --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\ntensorflow$ deactivate\r\nsephiroce@bike:~/open_source/tensorflow$ bazel build --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: Ignoring JAVA_HOME, because it must point to a JDK, not a JRE.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=228\r\nINFO: Reading rc options for 'build' from /home/sephiroce/open_source/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/sephiroce/open_source/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/sephiroce/open_source/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/sephiroce/virtualenv/py3-tf2-gpu/bin/python3 --action_env PYTHON_LIB_PATH=/home/sephiroce/virtualenv/py3-tf2-gpu/lib/python3.6/site-packages --python_path=/home/sephiroce/virtualenv/py3-tf2-gpu/bin/python3 --config=xla --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.6 --action_env LD_LIBRARY_PATH=/home/sephiroce/open_source/rdkit/build/lib:/usr/local/cuda/lib64:/usr/local/lib:/usr/local/lib/openmpi:/home/sephiroce/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib: --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/sephiroce/open_source/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/sephiroce/open_source/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/sephiroce/open_source/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file /home/sephiroce/open_source/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/sephiroce/open_source/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:mkl in file /home/sephiroce/open_source/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 -c opt\r\nINFO: Found applicable config definition build:linux in file /home/sephiroce/open_source/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/sephiroce/open_source/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Repository local_config_cuda instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule cuda_configure defined at:\r\n  /home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl:1407:18: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1377\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1054, in _create_local_cuda_repository\r\n                _find_libs(repository_ctx, <2 more arguments>)\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 599, in _find_libs\r\n                _check_cuda_libs(repository_ctx, <2 more arguments>)\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 501, in _check_cuda_libs\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nNo library found under: /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1377\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1054, in _create_local_cuda_repository\r\n                _find_libs(repository_ctx, <2 more arguments>)\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 599, in _find_libs\r\n                _check_cuda_libs(repository_ctx, <2 more arguments>)\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 501, in _check_cuda_libs\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nNo library found under: /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1377\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1054, in _create_local_cuda_repository\r\n                _find_libs(repository_ctx, <2 more arguments>)\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 599, in _find_libs\r\n                _check_cuda_libs(repository_ctx, <2 more arguments>)\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 501, in _check_cuda_libs\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"/home/sephiroce/open_source/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nNo library found under: /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1\r\nINFO: Elapsed time: 1.183s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```", "comments": ["My graphics card: 3080\r\nCuda 11.1, ubuntu 20.04, cudnn 8.0.3 (the very recent one), python 3.8\r\n\r\nI installed cuda 11.1 using the .deb file (to create all the symlinks automatically)\r\nI think the problem is that CUDA11.1 installs symlink libcudart 11.0 but it does not create symlink libcudart11.1. \r\nHowever, tensorflow tries to find libcudart 11.1, because cuda version is 11.1\r\nI tried to make a symlink libcudart11.1 -> libcudart11.1.74 as suggested in here : https://github.com/tensorflow/tensorflow/issues/26150#issuecomment-469058265\r\nHowever, then the error showed that SONAME did not match.\r\nNext, I tried this method : https://github.com/tensorflow/tensorflow/issues/26289#issuecomment-477848947\r\nI opened third_party/gpus/cuda_configure.bzl and searched for 'cudart'\r\nIn the dictionary check_cuda_libs_params, I changed cuda_config.cuda_version of \"cudart\" and \"cudart_static\" to \"11.0\"\r\nFor me, changing cuda_config.cuda_version of only cudart and cudart_static worked and I am currently building tensorflow.\r\nMy CPU is too slow (never expected to build tensorflow myself), so it will take a long time to see if it successfully builds, but until now, it is building without any errors.\r\nI will update if anything happens.", "Thanks!\r\nIt worked \ud83d\udc4d  (I'm not sure the RTX 3090 is fully utilized or not though.)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43588\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43588\">No</a>\n", "> It worked \ud83d\udc4d (I'm not sure the RTX 3090 is fully utilized or not though.)\r\n\r\nsm_86 doesnt seem to be implemented in nvcc yet, at least not in the copy i have (same cuda/cudnn configuration) so there's probably still room for performance improvement", "> > It worked \ud83d\udc4d (I'm not sure the RTX 3090 is fully utilized or not though.)\r\n> \r\n> sm_86 doesnt seem to be implemented in nvcc yet, at least not in the copy i have (same cuda/cudnn configuration) so there's probably still room for performance improvement\r\n\r\nI ran a simple test training, and Conv2D operation was about 2~3 times faster than 2070super. The tensorboard profiling showed that 3080 did not use tensorcores, and the speed was actually a bit faster with fp32 than mixed precision. Considering some issues saying that using tensorcores does not speed up much, I think the performance is within the range of expectation. \r\nOh, and by the way, profiling fails in CUDA11.1. It worked with CUDA11.0, but after I upgraded to CUDA11.1, segmentation fault happens with profile_batch turned on.\r\n", "> I ran a simple test training, and Conv2D operation was about 2~3 times faster than 2070super. The tensorboard profiling showed that 3080 did not use tensorcores, and the speed was actually a bit faster with fp32 than mixed precision. Considering some issues saying that using tensorcores does not speed up much, I think the performance is within the range of expectation.\r\n> Oh, and by the way, profiling fails in CUDA11.1. It worked with CUDA11.0, but after I upgraded to CUDA11.1, segmentation fault happens with profile_batch turned on.\r\n\r\nPlease tell me what versions of CUDA and cuDNN did you use, and the parameters at which the build was successful? And also a listing of a test training, if not difficult.\r\n\r\nMy platform:\r\n`OS: Windows 10 x64, Graphics Card: Palit RTX 3080`\r\n\r\nMy REQUIRED_PACKAGES were working for me:\r\n`python 3.6\r\nabsl-py==0.7.0\r\nastunparse==1.6.3\r\ngast==0.3.3\r\ngoogle_pasta==0.1.8\r\nh5py==2.10.0\r\nkeras_preprocessing==1.1.1\r\nopt_einsum==2.3.2\r\nprotobuf==3.9.2\r\ntensorboard==2.3.0\r\ntensorflow_estimator==2.3.0\r\ntermcolor==1.1.0\r\nwrapt==1.11.1\r\nwheel==0.26\r\nsix==1.12.0\r\nnumpy==1.18.5`\r\n\r\nMy build options that where working:\r\n`bazel build --config=opt --config=avx2_win --config=short_logs --define=no_tensorflow_py_deps=true --copt=-DTHRUST_IGNORE_CUB_VERSION_CHECK --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nI used cuDNN v8.0.3 (August 26th, 2020), for CUDA 11.0 and Latest Release CUDA Toolkit 11.1 (Sept 2020), but this required renaming cudart64_110.dll to cudart64_111.dll and cufftw64_10.dll to cufftw64_11.dll for the test vgg16 model to work\r\n\r\nWith CUDA Toolkit 11.0 Update1 (Aug 2020) and cuDNN v8.0.3 (August 26th, 2020), for CUDA 11.0 i got:\r\n`nvcc fatal   : Unsupported gpu architecture 'compute_86'`\r\n\r\n", "> \r\n> Please tell me what versions of CUDA and cuDNN did you use, and the parameters at which the build was successful? And also a listing of a test training, if not difficult.\r\n> \r\n> My platform:\r\n> `OS: Windows 10 x64, Graphics Card: Palit RTX 3080`\r\n> \r\n> My REQUIRED_PACKAGES were working for me:\r\n> `python 3.6 absl-py==0.7.0 astunparse==1.6.3 gast==0.3.3 google_pasta==0.1.8 h5py==2.10.0 keras_preprocessing==1.1.1 opt_einsum==2.3.2 protobuf==3.9.2 tensorboard==2.3.0 tensorflow_estimator==2.3.0 termcolor==1.1.0 wrapt==1.11.1 wheel==0.26 six==1.12.0 numpy==1.18.5`\r\n> \r\n> My build options that where working:\r\n> `bazel build --config=opt --config=avx2_win --config=short_logs --define=no_tensorflow_py_deps=true --copt=-DTHRUST_IGNORE_CUB_VERSION_CHECK --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package`\r\n> \r\n> I used cuDNN v8.0.3 (August 26th, 2020), for CUDA 11.0 and Latest Release CUDA Toolkit 11.1 (Sept 2020), but this required renaming cudart64_110.dll to cudart64_111.dll and cufftw64_10.dll to cufftw64_11.dll for the test vgg16 model to work\r\n> \r\n> With CUDA Toolkit 11.0 Update1 (Aug 2020) and cuDNN v8.0.3 (August 26th, 2020), for CUDA 11.0 i got:\r\n> `nvcc fatal : Unsupported gpu architecture 'compute_86'`\r\n\r\nOK, here's all I have tried since installing 3080\r\nThese are the same for all cases:\r\n`Ubuntu 20.04`\r\n`cuDNN v8.0.3`\r\n`bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n(Following www.tensorflow.org/install/source)\r\n\r\n1. `CUDA 11.0 Update1 `\r\n\r\nAs you mentioned, building from the source with compute compatibility 8.6 failed.\r\nI think it's because CUDA 11.0 does not support 30 series geforce graphics card.\r\nThey added 30 series in CUDA 11.1\r\nHowever, installing with `pip tf-nightly` works.\r\nThe problem with pypi installation is that it does not recognize sm_86, and spits out\r\n`ptxas fatal : Value 'sm_86' is not defined for option 'gpu-name'` and compiles ptxas every single time it launches.\r\nTensorboard profiling worked in this case.\r\n\r\n2. `CUDA 11.1`\r\n\r\n  - `pip tf-nightly`\r\nIt still works.\r\nTensorflow does fail to find libcudart.so.11.1 (as CUDA 11.1 does not make libcudart.so.11.1 nor libcudart.so.11 symlink), but because Tensorflow then searches for libcudart.so file, it managed to load libcudart.so anyway.\r\nHowever, it still does not recognize sm_86 and compiles ptxas everytime it starts.\r\nMoreover, Tensorboard profiling does not work. (segmentation fault)\r\n\r\n  - Build from source\r\nThis is the case I mentioned above.\r\nIn my case, making symlink for libcudart.so.11.1 did not work. \r\nI had to edit third_party/gpus/cuda_configure.bzl file.\r\nWith this build, it now recognizes sm_86 and does not compile ptxas anymore.\r\nHowever, tensorboard profiling error consists.\r\n\r\n3. In both `CUDA 11.0 Update1` and `CUDA 11.1`\r\n\r\nThey both have to set `set_memory_growth=True`, or they spit out errors when predict and/or fit a model with Conv layers.\r\nThis problem does not care how big the model is. Even with a single Conv layer, it just fails.\r\n\r\nAbout testing, I used customized HRNet. It has batchnorm, Conv2D, Conv2D Transpose, Global pooling average and Dense layers.\r\nIt is the only log I have left with profiling turned on using 2070super, so I only compared with this model.\r\nI think I can get my profiling function back if I roll back to CUDA 11.0, but I really do not want to change anything anymore, until new versions come out.", "Why is this closed ?", "> Why is this closed ?\r\n\r\nBecause the chief complaint of this issue is failing to build on CUDA-11.1, and at least building tensorflow is now successful.", "How to change the value of cuda_config.cuda_version to 11.0\r\ncudart\": _check_cuda_lib_params(\r\n            \"cudart\",\r\n            cpu_value,\r\n            cuda_config.config[\"cuda_library_dir\"],\r\n            cuda_config.cuda_version,    <-------------------- here\r\n            static = False,\r\n        ),\r\n        \"cudart_static\": _check_cuda_lib_params(\r\n            \"cudart_static\",\r\n            cpu_value,\r\n            cuda_config.config[\"cuda_library_dir\"],\r\n            cuda_config.cuda_version,               <--------------------------- and here\r\n            static = True,\r\n        ),\r\nPlease, I am not too good at python... ", "> How to change the value of cuda_config.cuda_version to 11.0\r\n> cudart\": _check_cuda_lib_params(\r\n> \"cudart\",\r\n> cpu_value,\r\n> cuda_config.config[\"cuda_library_dir\"],\r\n> cuda_config.cuda_version, <-------------------- here\r\n> static = False,\r\n> ),\r\n> \"cudart_static\": _check_cuda_lib_params(\r\n> \"cudart_static\",\r\n> cpu_value,\r\n> cuda_config.config[\"cuda_library_dir\"],\r\n> cuda_config.cuda_version, <--------------------------- and here\r\n> static = True,\r\n> ),\r\n> Please, I am not too good at python...\r\n\r\nI meant literally changing it to \"11.0\"\r\n\r\n- Before :\r\ncuda_config.config[\"cuda_library_dir\"],\r\ncuda_config.cuda_version,\r\nstatic = False,\r\n\r\n- After:\r\ncuda_config.config[\"cuda_library_dir\"],\r\n\"11.0\",\r\nstatic = False,", "Thank you very much for the so swift help. I am not being able to build tensorflow with bazel. Somebody wrote that the soultion is to change the version to  11.0, since I use cuda11.1 and cudnn8... I changed the version as you kindly suggested, but I still can not build it. After changing I still get these errors:\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\r\n\r\nI feel so sorry but some help would be very much appreciated.\r\n\r\n\r\n\r\n", "> Thank you very much for the so swift help. I am not being able to build tensorflow with bazel. Somebody wrote that the soultion is to change the version to 11.0, since I use cuda11.1 and cudnn8... I changed the version as you kindly suggested, but I still can not build it. After changing I still get these errors:\r\n> ERROR: An error occurred during the fetch of repository 'local_config_cuda':ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n> WARNING: Target pattern parsing failed.\r\n> ERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n> \r\n> I feel so sorry but some help would be very much appreciated.\r\n\r\nWhat specific CUDA version/CUDNN version/ OS are you using?\r\n", "Thank you very much for your answer.\r\nI am using  cuda-11.1 and libcudnn8-8.0.0.180-1, and I my system is linux_suse_x86-64.", "Ok. things to try:\r\n\r\n1. Install cudnn 8.0.4 or 8.0.3\r\n8.0.4 is the cudnn which supports cuda 11.1.\r\nI had no problem building with cudnn 8.0.3, and am currently building with cudnn 8.0.4 without any errors right now.\r\n(My cpu is not so good. It takes ages to complete building)\r\nHonestly, I do not think this is the problem, but give it a try.\r\n\r\n2. Check your cuda path.\r\nWhen executing configure file before building with bazel, does that properly recognize your cuda and cudnn?\r\nIf not so, check LD_LIBRARY_PATH and PATH.", "Thousand thanks for your reply. I will try. Yeah, something strange happened. Actually, I have installed cuda10.2 in my system, and when running the .configure of tensor flow, it asks for the cuda version and so on, and the program would go to limbo, it would ask for the cudnn version, and also for the RT support and so on. Then I thought that the problem would be not having \"cupy\" installed, because I want to use GPU's, and the card I got is a GeForce GTX 1060 6GB.  It was also kind of cumbersome to install cupy, but finally I did. Suddenly .configure would ask all these questions again, but now the default cuda, was the cuda11.1, and the defalut cudnn would be cudnn7.1. But the cudnn installed was the cudnn8.0.0. So I input that, and then .configure finished, indicating the options to run bazel. I thought I succeeded, but now I am facing this problem for more than a week. My LD_LIBRARY_PATH and PATH are pointing to my cuda-10.2 installation, and it seems that .configure doesn't care. It automatically goes to thecuda11.1 installation. ", "If you are using gtx 1060, I really suggest using cuda 10.1 or  10.2, not 11.x.\r\nFor me, I have 3080 which does not work with cuda 10 so I have to use cuda 11.\r\nHowever, right now, there is absolutely no benefit for using cuda 11.\r\n\r\nWhether or not you still want to use cuda 11, I recommend uninstalling CUDA completely and reinstalling CUDA. \r\nSomething seems to be messed up.", "I have no words to thank you for your time. You got almost the best GPU card in the market. Does it maintain a low temperature? Well, comparatively I mean. Let me then re-install cuda-10.2 and the libcuddnn corresponding is \r\nThen I will have to uninstall cuda11.1 that suddenly appeared when I installed \"cupy\", and then re-install cuda. In fact I got cuda and cuda10.2 which are the same. My CUDA_HOME points to /usr/local/cuda. And I will proceed with cudnn-8.0.4", "Now, in Linux SUSE-15.1  cuda 10.2 and libcudnn8.0.0.0.180.1 come pre installed. \r\n", "I looked up cupy installation document briefly. It seems like it does not require CUDA 11.1.\r\nIf you do not need 11.1 for any other reason, I recommand you to go for cuda 10.1 and cudnn 7.6.5\r\n(Compatibility matrix : https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html)\r\nI haven't used SUSE, so I am not sure whether installing cuda 10.1 will be successful, but if possible, it is the most stable configure for tensorflow.\r\nAs you said, remove all cuda related things first, and then install cuda 10.1 (or 10.2) with corresponding cudnn (recommend 7.6.5).\r\nIf CUDA11.1 was installed when installing cupy, try installing cupy-cuda101 (or cupy-cuda102 if you want to use 10.2)", "OK, I will follow your advice. Thank you very much. Hope this time around I can install tensorflow with the GPU's. Because I installed one, indeed, but pycharm keeps warning that even though it detects the GPU, it can't use it... So I need this bazel installation.", "\r\nHello, how are you? I am still fighting with the installation of TF, and when I thought I did, the monitor suddenly went off, and after 10 or 12 minutes it came back and this is the state. Do you think the installation is going well? Because for some time it doesn\u00b4t continue.\r\n  Thank you for your comments.\r\n________________________________\r\n\u5dee\u51fa\u4eba: jaentrouble <notifications@github.com>\r\n\u9001\u4fe1\u65e5\u6642: 2020\u5e7410\u67087\u65e5 5:24\r\n\u5b9b\u5148: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCC: minsky-1 <delcmca@hotmail.com>; Comment <comment@noreply.github.com>\r\n\u4ef6\u540d: Re: [tensorflow/tensorflow] Failed to build on Cuda-11.1 (#43588)\r\n\r\n\r\nI looked up cupy installation document briefly. It seems like it does not require CUDA 11.1.\r\nIf you do not need 11.1 for any other reason, I recommand you to go for cuda 10.1 and cudnn 7.6.5\r\n(Compatibility matrix : https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html)\r\nI haven't used SUSE, so I am not sure whether installing cuda 10.1 will be successful, but if possible, it is the most stable configure for tensorflow.\r\nAs you said, remove all cuda related things first, and then install cuda 10.1 with corresponding cudnn (recommend 7.6.5).\r\nIf CUDA11.1 was installed when installing cupy, try installing cupy-cuda101 (or cupy-cuda102 if you want to use 10.2)\r\n\r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43588#issuecomment-704699886>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ARE5YSNJB7MEZEPCV2EQ7V3SJP3RBANCNFSM4R3AYDJQ>.\r\n", "Hello,\r\n  I am still struggling with the TF installation, and thought everything was going OK, when suddenly the computer has stopped. This happened last time, after I changed as you advised me. But the computer was back after 3 hours or so. But this time, its more than 12 hours and it doesn\u00b4t come back.\r\nDo you think I should force shutdown and retry? It has stopped as I show in the photo. How far am I from installing it?  Thank you for your time.\r\n\r\n\r\n________________________________\r\n\u5dee\u51fa\u4eba: jaentrouble <notifications@github.com>\r\n\u9001\u4fe1\u65e5\u6642: 2020\u5e7410\u67086\u65e5 8:02\r\n\u5b9b\u5148: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCC: minsky-1 <delcmca@hotmail.com>; Comment <comment@noreply.github.com>\r\n\u4ef6\u540d: Re: [tensorflow/tensorflow] Failed to build on Cuda-11.1 (#43588)\r\n\r\n\r\nOk. things to try:\r\n\r\n  1.  Install cudnn 8.0.4 or 8.0.3\r\n8.0.4 is the cudnn which supports cuda 11.1.\r\nI had no problem building with cudnn 8.0.3, and am currently building with cudnn 8.0.4 without any errors right now.\r\n(My cpu is not so good. It takes ages to complete building)\r\nHonestly, I do not think this is the problem, but give it a try.\r\n\r\n  2.  Check your cuda path.\r\nWhen executing configure file before building with bazel, does that properly recognize your cuda and cudnn?\r\nIf not so, check LD_LIBRARY_PATH and PATH.\r\n\r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43588#issuecomment-704102087>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ARE5YSJBLEDAXEAU2TJKSVLSJLFJHANCNFSM4R3AYDJQ>.\r\n", "Hi,\r\n\r\nI'm having issues with building TF 2.3.\r\n\r\nOS: Ubuntu 18.04\r\nGPU: RTX2080\r\nCUDA: 11.1\r\ncudNN: 8.0.4\r\nTensorRT: 7.2\r\n\r\nI already tried :\r\nchanging it to \"11.0\"\r\n\r\nBefore :\r\ncuda_config.config[\"cuda_library_dir\"],\r\ncuda_config.cuda_version,\r\nstatic = False,\r\n\r\nAfter:\r\ncuda_config.config[\"cuda_library_dir\"],\r\n\"11.0\",\r\nstatic = False,\r\n\r\nPlease see the following to see what happened and let me know what else I should do.\r\n\r\nroot:~/tensorflow# ./configure\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: /usr/bin/python3\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n/usr/lib/python3/dist-packages\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: y\r\nROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nTraceback (most recent call last):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 649, in <module>\r\n    main()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 641, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 579, in find_cuda_config\r\n    result.update(_find_cuda_config(cuda_paths, cuda_version))\r\n  File \"third_party/gpus/find_cuda_config.py\", line 255, in _find_cuda_config\r\n    get_header_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 241, in _find_header\r\n    required_version, get_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 230, in _find_versioned_file\r\n    actual_version = get_version(file)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 247, in get_header_version\r\n    version = int(_get_header_version(path, \"CUDA_VERSION\"))\r\nValueError: invalid literal for int() with base 10: ''\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 11.1\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 8.0.4\r\n\r\n\r\nPlease specify the TensorRT version you want to use. [Leave empty to default to TensorRT 6]: 7.2\r\n\r\n\r\nPlease specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]:\r\n\r\n\r\nPlease specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /lib,/lib/x86_64-linux-gnu,/usr,/usr/lib/x86_64-linux gnu/libfakeroot,/usr/local/cuda,/usr/local/cuda-11.1,/usr/local/cuda-11.1/targets/x86_64-linux/lib,/TensorRT-7.2.0.14\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 649, in <module>\r\n    main()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 641, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 579, in find_cuda_config\r\n    result.update(_find_cuda_config(cuda_paths, cuda_version))\r\n  File \"third_party/gpus/find_cuda_config.py\", line 255, in _find_cuda_config\r\n    get_header_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 241, in _find_header\r\n    required_version, get_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 230, in _find_versioned_file\r\n    actual_version = get_version(file)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 247, in get_header_version\r\n    version = int(_get_header_version(path, \"CUDA_VERSION\"))\r\nValueError: invalid literal for int() with base 10: ''\r\nAsking for detailed CUDA configuration...\r\n", "Hello my friend.\r\nI succeeded with the installation, it recognizes the card, and all the cublas libraries...!!!\r\nAnd it is working in tandem with pycharm...\r\n Thank you very much for your replies. It took me half the month to do so.\r\nI installed python3.7, then I had to reinstall python3.6, because cuda was running with python3.6.\r\nI have neve had a problem like this. But it is done. Thank you for your help...\r\n\r\n________________________________\r\n\u5dee\u51fa\u4eba: jaentrouble <notifications@github.com>\r\n\u9001\u4fe1\u65e5\u6642: 2020\u5e7410\u67087\u65e5 5:24\r\n\u5b9b\u5148: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCC: minsky-1 <delcmca@hotmail.com>; Comment <comment@noreply.github.com>\r\n\u4ef6\u540d: Re: [tensorflow/tensorflow] Failed to build on Cuda-11.1 (#43588)\r\n\r\n\r\nI looked up cupy installation document briefly. It seems like it does not require CUDA 11.1.\r\nIf you do not need 11.1 for any other reason, I recommand you to go for cuda 10.1 and cudnn 7.6.5\r\n(Compatibility matrix : https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html)\r\nI haven't used SUSE, so I am not sure whether installing cuda 10.1 will be successful, but if possible, it is the most stable configure for tensorflow.\r\nAs you said, remove all cuda related things first, and then install cuda 10.1 with corresponding cudnn (recommend 7.6.5).\r\nIf CUDA11.1 was installed when installing cupy, try installing cupy-cuda101 (or cupy-cuda102 if you want to use 10.2)\r\n\r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43588#issuecomment-704699886>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ARE5YSNJB7MEZEPCV2EQ7V3SJP3RBANCNFSM4R3AYDJQ>.\r\n", "Another RTX 3080 user here, trying to make CUDA 11.1 with cuDNN and tensorflow works (using conda to make my environment) but fail to do so, I get that I have to use latest cuda as it supports my card, but there seems to be so many conflicts upon trying to make the environment, if someone got a solution would really help~", "Can verify that I was able to start compiling from [this](https://github.com/tensorflow/tensorflow/issues/43588#issuecomment-699592351) answer above!\r\n- Ubuntu 20.04\r\n- Cudnn8.0.4.40\r\n- libnvinfer 7.2.1\r\n- tf2.3.1\r\n- cuda 11.1\r\n\r\nMy build finished successfully and technically works, but if I try to run something on the 3070, TF hangs while accessing the GPU for 4 or so minutes. I suspect it is still JIT compiling something, but I am confident that my cudart is 11.1, which supports compute capability 8.6.\r\n\r\nIf I wait 4 minutes, the commands continue execution just fine, and the JIT compiled caching works on the next run.\r\n\r\nI have verified that the tf-nightly build doesn't hang, so I'm wondering if I have some libs that are 11.0 or something..?\r\n\r\n\r\nEDIT: Easy solution - I had forgotten to configure the build for compute capability 8.6 :facepalm:. Doing this produced a working build for 3xxx GPUs.", "Another resolution is to modify soname in libcudart.so.\r\nsudo patchelf --set-soname libcudart.so.11.1 /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1.74\r\nBut the system will show the file was truncated."]}, {"number": 43587, "title": "enlarge the format data width", "body": "When run this _//tensorflow/core/kernels/image:non_max_suppression_op_benchmark_test_, since this op is time consumption, and the origin format only keep 1 digits after the decimal point. the output items_processed per second is always zero. Enlarge the digits number to run the benchmark.", "comments": ["Could you provide the exact command you ran and the sample output you saw? ", "Hi @rohan100jain  The running command I use is:\r\n```\r\nbazel --output_user_root=$build_dir run --copt=-O3 //tensorflow/core/kernels/image:non_max_suppression_op_benchmark_test -- --benchmarks=../\r\n```\r\nBut there is no output with the latest master.\r\nIt seems someone is developing the new benchmark feature. Do you know who should I connect with this issue?\r\n", "Refer to this commit, for the details of change: https://github.com/tensorflow/tensorflow/commit/29bb0deb26db7179eefc87e750fd8755b2c9def3\r\n", "Report a issue here: https://github.com/tensorflow/tensorflow/issues/45698", "https://github.com/tensorflow/tensorflow/commit/bba12d0401800fbf873ea35f34517a8c47a54272 should have fixed the issue and you should now see results. "]}, {"number": 43586, "title": "[INTEL MKL] Changes to remove MKL binary blob.", "body": "This PR includes the following changes \r\n1) Removes the dependence on the mkl binary blob \r\n2) Replaces the binary openmp with the opensource version from LLVM,\r\n3) Modifies LLVM build files so they can be used by the openmp build and updated expand_cmake_vars so it can now expand variables of type @Vars@ \r\nChanges affect only the Linux build, I will follow up with another PR for Windows.", "comments": ["@penpornk I addressed all the comments. Thanks for reviewing the PR.", "@penpornk thanks. I ran into some issue previously using cc_library, I will try it again.", "@penpornk I made the requested changes. Please review again. Thanks.", "@agramesh1  Can you please check @penpornk's comments and keep us posted ? Thanks!", "> @agramesh1 Can you please check @penpornk's comments and keep us posted ? Thanks!\r\n\r\nHi @gbaned I synced off line with @penpornk on it. Thanks."]}, {"number": 43585, "title": "Slow training with tfRecords and tf.functions", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): unknown\r\n- GCC/Compiler version (if compiling from source): unknown\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: 2 Nvidia RTX 2080 Ti graphics cards\r\n\r\n**Describe the current behavior**\r\nI have been trying to train a 3D CNN with tfRecords and also do augmentation on the fly when reading the records using @tf.function decorator and tf.numpy_function but the training is horribly slow.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThis is the function for reading the tfRecords and make the augmentation on the fly\r\n```\r\ndef input_fn(filenames, subset, batch_size, buffer_size=512, data_augmentation=True):\r\n    # Args:\r\n    # filenames:   Filenames for the TFRecords files.\r\n    # subset:      Subset to make either train, valid, test.\r\n    # batch_size:  Return batches of this size.\r\n    # buffer_size: Read buffers of this size. The random shuffling\r\n    #              is done on the buffer, so it must be big enough.\r\n\r\n    # Create a TensorFlow Dataset-object which has functionality\r\n    # for reading and shuffling data from TFRecords files.\r\n    dataset = tf.data.TFRecordDataset(filenames=filenames)\r\n\r\n    # Parse the serialized data in the TFRecords files.\r\n    # This returns TensorFlow tensors for the image and labels.\r\n    dataset = dataset.map(parse_example, num_parallel_calls=8)\r\n\r\n    if subset == 'train' or subset =='valid':\r\n        # Allow infinite reading of the data.\r\n        dataset = dataset.repeat()\r\n    else :\r\n        dataset = dataset.repeat(1)\r\n\r\n    if subset == 'train':\r\n        dataset = dataset.shuffle(buffer_size=buffer_size)\r\n\r\n    ''' DATA AUGMENTATION '''\r\n    if (subset != 'test' and data_augmentation == True):\r\n        dataset = dataset.map(elastic3D)\r\n        dataset = dataset.map(flip3D)\r\n        dataset = dataset.map(rotation3D)\r\n        dataset = dataset.map(blur3D)\r\n\r\n\r\n    # Get a batch of data with the given size.\r\n    dataset = dataset.batch(batch_size)\r\n\r\n    if subset == 'train':\r\n        dataset = dataset.prefetch(16)\r\n\r\n    return dataset\r\n\r\n```\r\n\r\n\r\nThis is one function that I use in the map for augmentation\r\n```\r\n@tf.function\r\ndef flip3D(volume, label):\r\n\r\n    def flip(volume):\r\n        choice = np.random.randint(3)\r\n        if choice == 0: # flip on x\r\n            volume_flip = volume[::-1, :, :, :]\r\n        if choice == 1: # flip on y\r\n            volume_flip = volume[:, ::-1, :, :]\r\n        if choice == 2: # flip on z\r\n            volume_flip = volume[:, :, ::-1, :]\r\n\r\n        return volume_flip\r\n\r\n    augmented_volume = tf.numpy_function(flip, [volume], tf.float32)\r\n\r\n    return augmented_volume, label\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n", "comments": ["@quartermaine,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using. \r\n\r\nAlso, could you please update TensorFlow to v2.3 and check if you are facing the same issue. Thanks!", "@amahendrakar \r\nI have attach the code that I use to train the CNN but where should I upload the records because they are quite big in size?\r\n\r\n[scripts.zip](https://github.com/tensorflow/tensorflow/files/5294868/scripts.zip)\r\n\r\n", "@quartermaine,\r\nCould you please upload it to your Google Drive and share the link of the folder with us. Thanks!", "@amahendrakar ,\r\n\r\nI am trying to upload because its quite big file.", "@amahendrakar \r\nthis is the link to the [data](https://drive.google.com/file/d/1FVg2Ku5U3eS5MpiELYxMeEKM6AVwiCbw/view?usp=sharing)", "@amahendrakar ,\r\n\r\nI am trying to upload because its quite big file.", "Accidentally closed it", "@quartermaine,\r\nThank you for the update. I am unable to view the files from the given link. Could you please provide the required permissions to access the files as well? Thanks!", "@amahendrakar,\r\n\r\nCan you try this [link](https://drive.google.com/file/d/1FVg2Ku5U3eS5MpiELYxMeEKM6AVwiCbw/view?usp=sharingl).", "@quartermaine,\r\nOn running the code, I am facing a different error stating `Invalid argument:  Input to reshape is a tensor with 5975926 values, but the requested shape has 2987963`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f8d38dfd91bc67fe466bb5f8c9566ff8/43585.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43584, "title": "tf.summary.create_file_writer does not write any event file and tensorboard show nothing", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.8.3\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: RTX 2070super 8GB\r\n\r\n**Describe the current behavior**\r\nI have tried the example of https://www.tensorflow.org/api_docs/python/tf/summary\r\n```python\r\nimport tensorflow as tf\r\n\r\nwriter = tf.summary.create_file_writer(\"/tmp/mylogs\")\r\nwith writer.as_default():\r\n  for step in range(100):\r\n    # other model code would go here\r\n    tf.summary.scalar(\"my_metric\", 0.5, step=step)\r\n    writer.flush()\r\n```\r\nand it works fine. However, when I migrate some codes from tf 1.X to tf 2.2 using tf_upgrade_v2 script, and add some codes to store some data in files, the summary_writer doesn't create any folder or event files.\r\n``` python\r\n#!/usr/bin/env python\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        # Currently, memory growth needs to be the same across GPUs\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n    except RuntimeError as e:\r\n        # Memory growth must be set before GPUs have been initialized\r\n        print(e)\r\n\r\ndef start_experiment():\r\n    file_writer = tf.summary.create_file_writer(\"/tmp/{}_{}\".format('none', 0))\r\n\r\n    trainer = Trainer(file_writer=file_writer)\r\n    trainer.train()\r\n    file_writer.close()\r\n\r\n\r\nclass Trainer(object):\r\n    def __init__(self, file_writer):\r\n        self.file_writer = file_writer\r\n\r\n    def train(self):\r\n        info = {'tcount': 0, 'int_rew': 1, 'eprew':1, 'opt_tot':1}\r\n        while True:\r\n            # record data to tensorboard\r\n            with self.file_writer.as_default():\r\n                x = int(info['tcount'])\r\n                tf.summary.scalar(\"int_reward\", info['int_rew'], step=x)\r\n                tf.summary.scalar(\"ext_reward\", info['eprew'], step=x)\r\n                tf.summary.scalar(\"total_loss\", info['opt_tot'], step=x)\r\n                self.file_writer.flush()\r\n            info['tcount'] += 1\r\n            if info['tcount'] > 100:\r\n                break\r\n\r\n\r\nif __name__ == '__main__':\r\n    start_experiment()\r\n```\r\nI removed some detail codes to make it more clear. The training part of codes works fine, and it just doesn't record anything. \r\n**Describe the expected behavior**\r\nIt should create a folder in /tmp/none_0 and write some event file, but it did nothing.", "comments": ["Can you minimize the code/imports? \r\nIt will make easy for us to copy, paste and run your code example to reproduce your issue.", "\r\n\r\n\r\n\r\n> Can you minimize the code/imports?\r\n> It will make easy for us to copy, paste and run your code example to reproduce your issue.\r\n\r\nI updated my code to remove all the unnecessary codes and imports as above. It still records nothing. The folder \"/tmp/none_0\" is not created. And there are no event files. If you would like to run my code and help me find the problem, that would be really helpful. Thanks.", "@JosepLeder I think that there is a note in the documentation for running in the graph:\r\n\r\n> Default writers do not (yet) propagate across the @tf.function execution boundary - they are only detected when the function is traced - so best practice is to call writer.as_default() within the function body, and to ensure that the writer object continues to exist as long as the @tf.function is being used\r\n\r\nAs you see in the graph function execution case is still inside `tf.function` perimeter. So please be aware about `tf.compat.v1.disable_eager_execution()`:\r\n\r\n```\r\nwriter = tf.summary.create_file_writer(\"/tmp/mylogs/tf_function\")\r\n\r\n@tf.function\r\ndef my_func(step):\r\n  with writer.as_default():\r\n    # other model code would go here\r\n    tf.summary.scalar(\"my_metric\", 0.5, step=step)\r\n\r\nfor step in tf.range(100, dtype=tf.int64):\r\n  my_func(step)\r\n  writer.flush()\r\n```", "@Joseph-Rance \r\nPlease update as per above comment, if possible could you try on tf-nightly and let us know if the issue exist, please share a colab gist with the issue reported.", "@Saduf2019 Not sure you meant to @ me in that last comment?", "@JosepLeder \r\nIs this still an issue, can you follow bhack comment and let us know.", "> @JosepLeder I think that there is a note in the documentation for running in the graph:\r\n> \r\n> > Default writers do not (yet) propagate across the @tf.function execution boundary - they are only detected when the function is traced - so best practice is to call writer.as_default() within the function body, and to ensure that the writer object continues to exist as long as the @tf.function is being used\r\n> \r\n> As you see in the graph function execution case is still inside `tf.function` perimeter. So please be aware about `tf.compat.v1.disable_eager_execution()`:\r\n> \r\n> ```\r\n> writer = tf.summary.create_file_writer(\"/tmp/mylogs/tf_function\")\r\n> \r\n> @tf.function\r\n> def my_func(step):\r\n>   with writer.as_default():\r\n>     # other model code would go here\r\n>     tf.summary.scalar(\"my_metric\", 0.5, step=step)\r\n> \r\n> for step in tf.range(100, dtype=tf.int64):\r\n>   my_func(step)\r\n>   writer.flush()\r\n> ```\r\n\r\nI add `@tf.function` to my functions as follows:\r\n``` python\r\n#!/usr/bin/env python\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        # Currently, memory growth needs to be the same across GPUs\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n    except RuntimeError as e:\r\n        # Memory growth must be set before GPUs have been initialized\r\n        print(e)\r\n\r\n@tf.function\r\ndef start_experiment():\r\n    trainer = Trainer()\r\n    trainer.train()\r\n\r\n\r\nclass Trainer(object):\r\n\r\n    def __init__(self):\r\n        pass\r\n\r\n    @tf.function\r\n    def train(self):\r\n        file_writer = tf.summary.create_file_writer(\"/tmp/{}_{}\".format('none', 0))\r\n        info = {'tcount': 0, 'int_rew': 1, 'eprew':1, 'opt_tot':1}\r\n        while True:\r\n            # record data to tensorboard\r\n            with file_writer.as_default():\r\n                x = int(info['tcount'])\r\n                tf.summary.scalar(\"int_reward\", info['int_rew'], step=x)\r\n                tf.summary.scalar(\"ext_reward\", info['eprew'], step=x)\r\n                tf.summary.scalar(\"total_loss\", info['opt_tot'], step=x)\r\n                file_writer.flush()\r\n            info['tcount'] += 1\r\n            if info['tcount'] > 100:\r\n                break\r\n        file_writer.close()\r\n\r\n\r\nif __name__ == '__main__':\r\n    start_experiment()\r\n```\r\nBut it still doesn't work. If I remove `tf.compat.v1.disable_eager_execution()`, it works as the document said. However, I have to change lots of code using placeholder after using tf_upgrade_v2 if I remove `tf.compat.v1.disable_eager_execution()`. I am not sure if there is a way  I can use the writer without remove that code.", "You can try to take a look at https://www.tensorflow.org/tensorboard/migrate", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43584\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43584\">No</a>\n"]}, {"number": 43582, "title": "getting an error as 'tuple' object has no attribute 'apply_gradients'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): kaggle environment\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6+ \r\n\r\nI'm a beginner in the field of deep learning and getting hands on working with bert implementation for the classification problem using tensorflow. However I have encountered this error as mentioned in the title.  This is [link ](https://www.kaggle.com/dv1453/notebook3d0337bd04?scriptVersionId=43629422) to my kaggle notebook.  I referred some other notebooks to create train function as below:\r\n`\r\ndef train_step(model, token_ids, masks, labels):\r\n    \r\n    labels = tf.dtypes.cast(labels, tf.float32)\r\n    \r\n    with tf.GradientTape() as tape:\r\n        \r\n        predictions = model(token_ids, attention_mask=masks)\r\n        \r\n        loss = loss_object(labels, predictions)\r\n    \r\n    \r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n    \r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables), name = 'gradients')\r\n    \r\n    train_loss(loss)\r\n\r\n    for i, auc in enumerate(train_auc_metrics):\r\n        \r\n        auc.update_state(labels[:,i], predictions[:,i])\r\n        \r\ndef validation_step(model, token_ids, masks, labels):\r\n    \r\n    labels = tf.dtypes.cast(labels, tf.float32)\r\n\r\n    predictions = model(token_ids, attention_mask=masks, training=False)\r\n    \r\n    v_loss = loss_object(labels, predictions)\r\n\r\n    validation_loss(v_loss)\r\n    \r\n    for i, auc in enumerate(validation_auc_metrics):\r\n        \r\n        auc.update_state(labels[:,i], predictions[:,i])\r\n\r\nseeds = [0]\r\n\r\nfor seed in range(len(seeds)):\r\n    \r\n    print('=' * 50, f\"CV {seed+1}\", '=' * 50)\r\n    \r\n    model = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))\r\n    \r\n    labels =  train_df[label_cols].values\r\n    \r\n    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=seed, test_size = 0.2)\r\n\r\n    train_masks, validation_masks = train_test_split(attention_masks, random_state=seed, test_size=0.2)\r\n\r\n    train_size = len(train_inputs)\r\n\r\n    validation_size = len(validation_inputs)\r\n\r\n\r\n    train_dataset = create_dataset((train_inputs, train_masks, train_labels), batch_size=BATCH_SIZE,train=True)\r\n\r\n    validation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), batch_size=BATCH_SIZE,train=False)\r\n    \r\n    \r\n    steps_per_epoch = train_size // (BATCH_SIZE)\r\n\r\n    #  Loss Function\r\n    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\r\n\r\n    train_loss = tf.keras.metrics.Mean(name='train_loss')\r\n\r\n    validation_loss = tf.keras.metrics.Mean(name='val_loss')\r\n\r\n    #  Optimizer (with 1-cycle-policy)\r\n    warmup_steps = steps_per_epoch // 3\r\n\r\n    total_steps = steps_per_epoch * NR_EPOCHS - warmup_steps\r\n\r\n    optimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\r\n\r\n    # Gradients\r\n    \r\n    gradients = 0\r\n    \r\n    #  Metrics\r\n    train_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\r\n\r\n    validation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\r\n\r\n\r\n    for epoch in range(NR_EPOCHS):\r\n\r\n        print('=' * 50, f\"EPOCH {epoch+1}\", '=' * 50)\r\n\r\n        start = time.time()\r\n\r\n\r\n        for batch_no, (token_ids, masks, labels) in enumerate(tqdm(train_dataset)):\r\n\r\n            train_step(model, token_ids, masks, labels)\r\n\r\n            if batch_no % 100 == 0:\r\n\r\n                    print(f'\\nTrain Step: {batch_no}, Loss: {train_loss.result()}')\r\n\r\n                    for i, label_name in enumerate(label_cols):\r\n\r\n                        print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\r\n\r\n                        train_auc_metrics[i].reset_states()\r\n\r\n        for batch_no, (token_ids, masks, labels) in enumerate(tqdm(validation_dataset)):\r\n\r\n            validation_step(model, token_ids, masks, labels)\r\n\r\n        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\r\n\r\n        for i, label_name in enumerate(label_cols):\r\n\r\n            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\r\n\r\n            validation_auc_metrics[i].reset_states()\r\n`\r\nhowever I got the error as fallowing:\r\n\r\n`---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-16-11f3f3661ec3> in <module>\r\n     60         for batch_no, (token_ids, masks, labels) in enumerate(tqdm(train_dataset)):\r\n     61 \r\n---> 62             train_step(model, token_ids, masks, labels)\r\n     63 \r\n     64             if batch_no % 100 == 0:\r\n\r\n<ipython-input-14-d39f9869214e> in train_step(model, token_ids, masks, labels)\r\n     12     gradients = tape.gradient(loss, model.trainable_variables)\r\n     13 \r\n---> 14     optimizer.apply_gradients(zip(gradients, model.trainable_variables), name = 'gradients')\r\n     15 \r\n     16     train_loss(loss)\r\n\r\nAttributeError: 'tuple' object has no attribute 'apply_gradients'`\r\n\r\n\r\n", "comments": ["https://stackoverflow.com/questions/64074872/tensorflow-2-attributeerror-tuple-object-has-no-attribute-apply-gradients", "The notebook link It is not working.", "Hi , I have updated the link. It should be working now.", "@ThatGuyDV,\r\nOn running the code, I am facing an error stating `FileNotFoundError: [Errno 2] No such file or directory: '../input/eda-and-baseline/processed_train.csv'`\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "My apologies, earlier I added that from one of private kaggle notebook. I have updated the dataset and tested from other account to generate same error. I request you to please take a look once again. Thank you. ", "The Kaggle notebook still doesn't run. \r\nBut your problem is that in the third_party Transformer library when you call [`create_optimizer`](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#transformers.create_optimizer)   \r\n`return optimizer, lr_schedule` and instead `apply_gradients` is an optimizer method.\r\n\r\nPlease ask for Transformers library support at https://discuss.huggingface.co/", "Yep the problem was with third party optimizer. When I used keras' optimizer, then my training is working properly. Thanks a lot for the advice. I guess Hugging Faces' create_optimizer does not support apply gradient method for now. I will add this issue to their forum. Thanks a lot once again.", "> Yep the problem was with third party optimizer. When I used keras' optimizer, then my training is working properly.\r\n\r\n@ThatGuyDV,\r\nThank you for the update. Marking this issue as closed, please feel free to re-open the issue if necessary.", "Could someone leave more details about how to use the keras's optimizer instead of transformer's create_optimizer(). Forgive me for asking such a stupid question, I am just a beginner .", "**The workaround** (not quite fixing it)\r\n@jackfromeast , to your questions, what they mean is that create_optimizer() returns a tuple with two elements: the optimizer itself and an LR scheduler. This causes the error if you use apply_gradients(...) on it. A simple solution is to change optimizer = create_optimizer(...) for optimizer, lr = create_optimizer(...)\r\n\r\nHowever, if you do that you may get this new error later: ValueError: name for name_scope must be a string.\r\n:O\r\n\r\n**The actual problem**\r\nThis notebook requires an older version of transformers (2.3.0), as shown in the output of the first cell:\r\n  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\r\n\r\n**The solution (that worked for me)**\r\nInstall the expected version with this line:\r\n!pip install transformers==2.3.0\r\n\r\nGood luck!", "I have also faced the same problem, but I'm using TF v2.7 does that mean I should use another version of transformers?"]}, {"number": 43581, "title": "tf-nightly fails to initialize TPUs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  2.4.0-dev20200925\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nEverything was fine 1.5 days ago,\r\n until I created a TPU node half a day ago with TPU software version \"nightly\":\r\n```\r\ngcloud compute tpus create node-123 \\\r\n\t--zone=us-central1-f \\\r\n\t--network=default \\\r\n    --accelerator-type=v2-8 \\\r\n    --range=10.123.0.0 \\\r\n\t--preemptible \\\r\n    --version=nightly\r\n```\r\nand tried to initialize it:\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nTPU_NAME='node-123'\r\nZONE='us-central1-f'\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_NAME, zone=ZONE)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\n```\r\nThe output was\r\n```\r\nINFO:tensorflow:Initializing the TPU system: node-123\r\n```\r\nand I was stuck with the initialization.\r\n\r\n\r\nI believe this is a new bug in `2.4.0-dev20200925`.\r\nI also suggest that there should be more tf-nightly versions to choose from when creating TPUs.", "comments": ["A temporary fix I found is to change the software version of the TPU back to `2.4.0.dev20200924` before initializaiton:\r\n```\r\nfrom cloud_tpu_client import Client\r\nc = Client(tpu=TPU_NAME, zone=ZONE)\r\nc.configure_tpu_version('2.4.0.dev20200924', restart_type='ifNeeded')\r\n```", "I am also facing this issue. The temporary fix worked.", "@tsc2017 \r\nPlease update as per above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I believe this bug has been fixed in the latest `tf-nightly`. Closing Issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43581\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43581\">No</a>\n"]}, {"number": 43580, "title": "Training loop from scratch for multi-out model - how to calculate loss and update it.", "body": "\r\nI have a ML models which have 3 output. Each of them have passed into a Soft-Max player. I'm trying to train the model from scratch with training loop in tf.keras. My generator output are something like this:\r\n\r\n```\r\nx,y = (batch_size, h,w,c), [(batch_size, 2), (batch_size, 8), (batch_size, 10)]\r\n```\r\n\r\nAs you notice, each of the 3 output has different numbers of labels. First output layer has 2 labels output, second one has 8 and last one has 10. The problem is not about the data generator; It's fine. **However, I don't understand how will I calculate loss function and update it in scratch training.**\r\n\r\n---\r\n\r\nI've started with [document guidelines](https://keras.io/guides/writing_a_training_loop_from_scratch/). Here is the code snippet,\r\n\r\n```python\r\n\r\nepochs = 1\r\noptimizer   = tf.keras.optimizers.Adam(learning_rate=0.05)\r\nloss_fn     = tf.keras.losses.CategoricalCrossentropy()\r\n\r\ntrain_acc_metric = tf.keras.metrics.Accuracy()\r\nval_acc_metric   = tf.keras.metrics.Accuracy()\r\n\r\n@tf.function\r\ndef train_step(x, y):\r\n    with tf.GradientTape() as tape:\r\n        logits = model(x, training=True)  # Logits for this minibatch\r\n\r\n        # Compute the loss value for this minibatch.\r\n        train_loss_value = loss_fn(y, logits)\r\n\r\n    grads = tape.gradient(train_loss_value, model.trainable_weights)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n    train_acc_metric.update_state(y, logits)\r\n    return train_loss_value\r\n```\r\n\r\nAnd, \r\n\r\n```python\r\n\r\nfor epoch in range(epochs):\r\n    # Iterate over the batches of the dataset.\r\n    for step, (x_batch_train, y_batch_train) in enumerate(train_generator):\r\n        train_loss_value = train_step(x_batch_train, y_batch_train)\r\n\r\n    # Reset metrics at the end of each epoch\r\n    train_acc_metric.reset_states()\r\n```\r\n\r\n\r\nI suspect, I am not handling properly the computation of loss in here: `loss_fn(y, logits)`. `y` has 3 output, `logits` has also 3 output. \r\n\r\nUsing `model.compile` is simply straight-forward, just passing the loss function in the loss argument. But what to do in scratch training. The 3 output (though are co-related) needs to trained separately. \r\n\r\nAsked [SO](https://stackoverflow.com/q/64065473/9215780), nobody knows. \r\n ", "comments": ["The `logits = model(x, training=True)` has 3 output. So to calculate the loss function separately (I think) we need to unpack it. `loss_one, loss_two, loss_thr = model(x)`. And next, maybe like this:\r\n\r\n```\r\ntrain_loss_value_one = loss_fn(y[0], loss_one)\r\ntrain_loss_value_two = loss_fn(y[1], loss_two)\r\ntrain_loss_value_thr = loss_fn(y[2], loss_thr)\r\n```\r\nI'm feeling I'm definitively missing something. ", "@ravikyram is it possible in custom training in tf.keras?", "@Suzan009 Yes, it is possible in custom training in `tf.keras`. Are you facing any error? Can you please share a standalone code to reproduce the error. Thanks!", "Mainly I need to solve above scenario. I know we can do custom training in tf.keras. But how to define loss and metrics for multi-label multi class classification problem.\r\n\r\nPlease, consider the following cases:\r\n\r\n```\r\nx,y = (batch_size, h,w,c), [(batch_size, 2), (batch_size, 8), (batch_size, 10)]\r\n```\r\n\r\nHere, the model has 3 output. But each of the single output layer doesn't give single out put itself. See, first output has 2 different classes, second one has 8 different classes and third one has 10 different classes. Now each of these three output are mutually related. \r\n\r\nIn such problem, how will I define the loss and metrics in custom training in tf.keras. \r\n\r\nI can do it in old keras. fairly simple. Such as:\r\n\r\n```python\r\noputput1 = Dense(2,  activation='softmax') (curr_output)\r\noputput2 = Dense(8,  activation='softmax') (curr_output)\r\noputput3 = Dense(10,  activation='softmax') (curr_output)\r\noutput_tensor = [oputput1, oputput2, oputput3]\r\n\r\nmodel = Model(input_tensor, output_tensor)\r\n```\r\nAnd compile like this, \r\n\r\n```python\r\n\r\nmodel.compile(\r\noptimizer = , \r\nloss = 'categorical_crossentropy',\r\nmetrics='accuracy'\r\n)\r\n```\r\n", "Hey,\r\nAnswering wrt TF2.4\r\nYou will need a separate metric for each output task. While you can calculate the total epoch loss by adding the three losses, the accuracy for each will need to be calculated separately only.\r\n\r\n```\r\ntrain_acc_metric_label1 = tf.keras.metrics.Accuracy()\r\ntrain_acc_metric_label2 = tf.keras.metrics.Accuracy()\r\ntrain_acc_metric_label3 = tf.keras.metrics.Accuracy()\r\n```\r\n\r\nYou have already separated the losses. So, the only thing missing is updating them as following\r\n```\r\nlogits = model(x, training=True)\r\ntrain_acc_metric_label1.update_state(y[0], logits[0])\r\ntrain_acc_metric_label2.update_state(y[1], logits[1])\r\ntrain_acc_metric_label3.update_state(y[2], logits[2])\r\n```\r\nFor total loss value that you are trying to return from `train_step()`\r\n`\r\ntrain_loss_total = loss_fn(y[0], logits[0]) + loss_fn(y[1], logits[1]) + loss_fn(y[2], logits[2])\r\n`"]}, {"number": 43579, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.", "body": "Getting this Error, please help me to resolve this issue.\r\n(tensor) C:\\Users\\Bkmaurya>python\r\nPython 3.6.12 |Anaconda, Inc.| (default, Sep  9 2020, 00:29:25) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Bkmaurya\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\n**ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.**\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Bkmaurya\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Bkmaurya\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\Bkmaurya\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\Bkmaurya\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Bkmaurya\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Bkmaurya\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nThanks!", "comments": ["Trying uninstalling and re-installing TensorFlow 1.15. Make sure that you use conda install instead of pip. ", "@Strange1-7,\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.\r\n\r\nCould you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues) and fill in the template, so that the issue can be tracked there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43579\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43579\">No</a>\n"]}, {"number": 43578, "title": "Compiled TF2.3 with MKL on OSX does not progress beyond first epoch", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.15.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): clang 10.0.1\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\nWhen executing code:\r\n- vae.fit(minst_digits, epochs = 30, batch_size = 128)\r\nit does not progress beyond the epoch 1 and seems to hang even after waiting several hours to see if the training progresses beyond epoch 1. I notice that the CPU load is showing 2 threads executing at 100%\r\n\r\n**Describe the expected behavior**\r\nNeeds to be able to progress beyond 1st epoch and start training.\r\n\r\n**Standalone code to reproduce the issue**\r\nPlease find the code to run the test here [vae 0.1.py](https://drive.google.com/file/d/14ntp7WeSGVHOy_Fx_5PS1nIneRS1DjOB/view?usp=sharing)\r\n\r\n**Other info / logs** \r\n\r\nBasel build info:\r\nbazel build --action_env CC=/usr/local/opt/llvm/bin/clang  --config=noaws --config=nogcp --config=mkl --config=opt -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nConsole output after export MKLDNN_VERBOSE=1\r\n```\r\n\r\n2020-09-26 08:52:10.680539: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ffc4db8dd80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-26 08:52:10.680577: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-26 08:52:10.680685: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nModel: \"encoder\"\r\n\r\nLayer (type)                    Output Shape         Param #    Connected to                     \r\ninput_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \r\nconv2d (Conv2D)                 (None, 14, 14, 32)   320         input_1[0][0]                    \r\nconv2d_1 (Conv2D)               (None, 7, 7, 64)     18496       conv2d[0][0]                     \r\nflatten (Flatten)               (None, 3136)         0           conv2d_1[0][0]                   \r\ndense (Dense)                   (None, 16)           50192       flatten[0][0]                    \r\nz_mean (Dense)                  (None, 2)            34          dense[0][0]                      \r\nz_log_var (Dense)               (None, 2)            34          dense[0][0]                      \r\nsampling (Sampling)             (None, 2)            0           z_mean[0][0]                     \r\n                                                                 z_log_var[0][0]                  \r\n\r\nTotal params: 69,076\r\nTrainable params: 69,076\r\nNon-trainable params: 0\r\n\r\nModel: \"decoder\"\r\nLayer (type)                 Output Shape              Param #   \r\ninput_2 (InputLayer)         [(None, 2)]               0         \r\ndense_1 (Dense)              (None, 3136)              9408      \r\nreshape (Reshape)            (None, 7, 7, 64)          0         \r\nconv2d_transpose (Conv2DTran (None, 14, 14, 64)        36928     \r\nconv2d_transpose_1 (Conv2DTr (None, 28, 28, 32)        18464     \r\nconv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         289       \r\n\r\nTotal params: 65,089\r\nTrainable params: 65,089\r\nNon-trainable params: 0\r\n\r\nEpoch 1/30\r\ndnnl_verbose,info,oneDNN v1.4.0 (commit N/A)\r\ndnnl_verbose,info,cpu,runtime:OpenMP\r\ndnnl_verbose,info,cpu,isa:Intel AVX2\r\ndnnl_verbose,info,gpu,runtime:none\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:Acdb8a:f0,,,32x1x3x3,0.00488281\r\ndnnl_verbose,exec,cpu,convolution,jit:avx2,forward_training,src_f32::blocked:abcd:f0 wei_f32::blocked:Acdb8a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:aBcd8b:f0,post_ops:'eltwise_relu;';,alg:convolution_direct,mb128_ic1oc32_ih28oh14kh3sh2dh0ph0_iw28ow14kw3sw2dw0pw0,1.90991\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:ABcd8b8a:f0,,,64x32x3x3,0.0571289\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,128x32x14x14,1.68311\r\ndnnl_verbose,exec,cpu,convolution,jit:avx2,forward_training,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8b8a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:aBcd8b:f0,post_ops:'eltwise_relu;';,alg:convolution_direct,mb128_ic32oc64_ih14oh7kh3sh2dh0ph0_iw14ow7kw3sw2dw0pw0,3.87109\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,128x64x7x7,0.25\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,128x64x7x7,0.225098\r\ndnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ba:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,post_ops:'eltwise_relu;';,,mb128ic3136oc16,15.105\r\ndnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ba:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb128ic16oc2,0.0161133\r\ndnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ba:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb128ic16oc2,0.00512695\r\ndnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ba:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,post_ops:'eltwise_relu;';,,mb128ic2oc3136,2.96802\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:ABcd8a8b:f0,,,64x64x3x3,0.0251465\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x64x7x7,0.787109\r\ndnnl_verbose,exec,cpu,convolution,jit:avx2,backward_data,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8a8b:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic64oc64_ih14oh7kh3sh2dh0ph0_iw14ow7kw3sw2dw0pw0,9.69995\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,128x64x14x14,2.89893\r\ndnnl_verbose,exec,cpu,eltwise,jit:avx2,forward_training,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,128x14x14x64,0.614014\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:ABcd8a8b:f0,,,64x32x3x3,0.0151367\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x64x14x14,3.52783\r\ndnnl_verbose,exec,cpu,convolution,jit:avx2,backward_data,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8a8b:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic32oc64_ih28oh14kh3sh2dh0ph0_iw28ow14kw3sw2dw0pw0,12.1011\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,128x32x28x28,2.14697\r\ndnnl_verbose,exec,cpu,eltwise,jit:avx2,forward_training,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,128x28x28x32,1.00903\r\ndnnl_verbose,exec,cpu,reorder,simple:any,undef,src_f32::blocked:cdba:f0 dst_f32:p:blocked:ABcd8a8b:f0,,,32x1x3x3,0.00488281\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x32x28x28,6.56982\r\ndnnl_verbose,exec,cpu,convolution,jit:avx2,backward_data,src_f32:p:blocked:aBcd8b:f0 wei_f32:p:blocked:ABcd8a8b:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic1oc32_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,6.87988\r\ndnnl_verbose,exec,cpu,reorder,simple:any,undef,src_f32:p:blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,128x1x28x28,0.666992\r\ndnnl_verbose,exec,cpu,sum,simple:any,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,,128x28x28x1,0.0817871\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:Acdb8a:f0,,,32x1x3x3,0.00292969\r\ndnnl_verbose,exec,cpu,convolution,jit:avx2,forward_training,src_f32::blocked:abcd:f0 wei_f32::blocked:Acdb8a:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic1oc32_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,1.34204\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x32x28x28,3.01611\r\ndnnl_verbose,exec,cpu,reorder,simple:any,undef,src_f32::blocked:acdb:f0 dst_f32:p:blocked:aBcd8b:f0,,,128x1x28x28,0.464111\r\ndnnl_verbose,exec,cpu,eltwise,jit:avx2,backward_data,data_f32::blocked:aBcd8b:f0 diff_f32::blocked:aBcd8b:f0,,alg:eltwise_relu alpha:0 beta:0,128x32x28x28,2.26904\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x64x14x14,1.04614\r\ndnnl_verbose,exec,cpu,convolution,jit:avx2,backward_weights,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8b8a:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic32oc64_ih28oh14kh3sh2dh0ph0_iw28ow14kw3sw2dw0pw0,2.33911\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:ABcd8b8a:f0 dst_f32::blocked:cdba:f0,,,64x32x3x3,0.0158691\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:ABcd8b8a:f0,,,64x32x3x3,0.013916\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x32x28x28,7.34912\r\ndnnl_verbose,exec,cpu,convolution,jit:avx2,forward_training,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8b8a:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic32oc64_ih28oh14kh3sh2dh0ph0_iw28ow14kw3sw2dw0pw0,11.5352\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x64x14x14,0.933105\r\ndnnl_verbose,exec,cpu,eltwise,jit:avx2,backward_data,data_f32::blocked:aBcd8b:f0 diff_f32::blocked:aBcd8b:f0,,alg:eltwise_relu alpha:0 beta:0,128x64x14x14,0.945068\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x64x7x7,0.780029\r\ndnnl_verbose,exec,cpu,convolution,jit:avx2,backward_weights,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8b8a:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic64oc64_ih14oh7kh3sh2dh0ph0_iw14ow7kw3sw2dw0pw0,1.15698\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:ABcd8b8a:f0 dst_f32::blocked:cdba:f0,,,64x64x3x3,0.0258789\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:cdba:f0 dst_f32::blocked:ABcd8b8a:f0,,,64x64x3x3,0.0200195\r\ndnnl_verbose,exec,cpu,convolution,jit:avx2,forward_training,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8b8a:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic64oc64_ih14oh7kh3sh2dh0ph0_iw14ow7kw3sw2dw0pw0,5.6731\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:aBcd8b:f0 dst_f32::blocked:acdb:f0,,,128x64x7x7,0.27002\r\ndnnl_verbose,exec,cpu,eltwise,jit:avx2,backward_data,data_f32::blocked:ab:f0 diff_f32::blocked:ab:f0,,alg:eltwise_relu alpha:0 beta:0,128x3136,0.307129\r\ndnnl_verbose,exec,cpu,sum,simple:any,undef,src_f32::blocked:ab:f0 src_f32::blocked:ab:f0 src_f32::blocked:ab:f0 dst_f32::blocked:ab:f0,,,128x2,0.00195312\r\ndnnl_verbose,exec,cpu,sum,simple:any,undef,src_f32::blocked:ab:f0 src_f32::blocked:ab:f0 dst_f32::blocked:ab:f0,,,128x2,0.000976562\r\ndnnl_verbose,exec,cpu,sum,simple:any,undef,src_f32::blocked:ab:f0 src_f32::blocked:ab:f0 dst_f32::blocked:ab:f0,,,128x16,0.000976562\r\ndnnl_verbose,exec,cpu,eltwise,jit:avx2,backward_data,data_f32::blocked:ab:f0 diff_f32::blocked:ab:f0,,alg:eltwise_relu alpha:0 beta:0,128x16,0.00390625\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:acdb:f0 dst_f32::blocked:aBcd8b:f0,,,128x64x7x7,0.231201\r\ndnnl_verbose,exec,cpu,eltwise,jit:avx2,backward_data,data_f32::blocked:aBcd8b:f0 diff_f32::blocked:aBcd8b:f0,,alg:eltwise_relu alpha:0 beta:0,128x64x7x7,0.28418\r\ndnnl_verbose,exec,cpu,convolution,jit:avx2,backward_weights,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8b8a:f0 bia_f32::blocked:a:f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic32oc64_ih14oh7kh3sh2dh0ph0_iw14ow7kw3sw2dw0pw0,0.559082\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:ABcd8b8a:f0 dst_f32::blocked:cdba:f0,,,64x32x3x3,0.0100098\r\ndnnl_verbose,exec,cpu,reorder,jit:uni,undef,src_f32::blocked:ABcd8b8a:f0 dst_f32::blocked:ABcd8a8b:f0,,,64x32x3x3,0.012207\r\ndnnl_verbose,exec,cpu,convolution,jit:avx2,backward_data,src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8a8b:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,alg:convolution_direct,mb128_ic32oc64_ih14oh7kh3sh2dh0ph0_iw14ow7kw3sw2dw0pw0,3.57495\r\ndnnl_verbose,exec,cpu,eltwise,jit:avx2,backward_data,data_f32::blocked:aBcd8b:f0 diff_f32::blocked:aBcd8b:f0,,alg:eltwise_relu alpha:0 beta:0,128x32x14x14,0.571045\r\ndnnl_verbose,exec,cpu,reorder,simple:any,undef,src_f32::blocked:acdb:f0 dst_f32:p:blocked:aBcd8b:f0,,,128x1x28x28,0.368164\r\n\r\n```", "comments": ["I decide to log under bugs because I was able to build from source TF 2.3.0 successfully, install and run the \"Hello World\" example. I experienced this issue only with the attached code.\r\n\r\nPlease find the environment file as attached here [tf_env.txt](https://drive.google.com/file/d/1jXRGynFl_ne_dv9WU8azWA8w8yUYpjKR/view?usp=sharing)", "@ianlokh \r\n\r\nIt's not officially support TF+MKL with OSX even though oneDNN supports.  It could be that oneDNN is not compiled with openMP so oneDNN code is not threaded.\r\n\r\nIn the latest master and also TF2.4, there are some changes to use an opensource  openMP, it has been tested on windows and Linux but not Mac.  \r\n\r\nCould you try these branches?", "@ianlokh \r\nIs there any feedback?", "@ianlokh \r\n\r\nIs it possible to close the issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43578\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43578\">No</a>\n", "Thanks Neo for your pointers. I will try it again ", "Thank you support too!"]}, {"number": 43577, "title": "[INTEL MKL] Bug-fix to in-place computation with tensor forwarding.", "body": "This PR fixes a bug in a fused operator (`conv2d` + `bias_add` + `add` + `relu`), wherein one of the inputs to `add` is used as in-place buffer for output result. Previous code was forcefully forwarding the input to the output, although the input could have more than one fan-outs.", "comments": ["@penpornk Thanks a lot for the valuable comments. I have addressed them. Please check."]}, {"number": 43576, "title": "Problem loading model trained with EfficientNetB0 ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: GeForce GTX 1660 Ti with Max-Q Design 6 GB\r\n\r\n**Describe the current behavior**\r\nError using tensorflow.keras.applications EfficientNetB0.\r\n\r\nWhen I load my trained model, I have the following warnings (I only copied part of them here, you can check the rest in the colab link that is below:\r\n\r\n```\r\nWARNING:tensorflow:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_344566) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_344566) with ops with custom gradients. Will likely fail if a gradient is requested.\r\n```\r\nI basically save the model and load it again in the colab link below and I get this message.\r\n\r\nBecause of this I cannot continue training my model if I wish to do so. Am I missing something here?\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nLink to COLAB:\r\nhttps://colab.research.google.com/drive/1Vjb65Y7E_FDbVZm_ZDs6ZP3yjjAN-rr5?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nI am basically using the same code from here: https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\r\nfor the colab example. Am I doing something wrong?", "comments": ["I switched from activation=\"swish\" (default) to activation=\"relu\" and the problem was gone. Now I don't know why this happens while using swish.", "I was able to reproduce the issue in colab. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/e060bc6ed459d44c31121d0a1684e962/untitled398.ipynb).Thanks!", "This might be related to this issue #40166 Please take a look at it and let me know if it helps. Thanks!", "> This might be related to this issue #40166 Please take a look at it and let me know if it helps. Thanks!\r\n\r\nYes, I think it is related to that. \r\nSo there's no solution right now for this problem? \r\n\r\nThank you!", "@mesquita You can close this issue as this is a duplicate of this [issue](https://github.com/tensorflow/tensorflow/issues/40166) and we can track it there. Thanks!", "Sure", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43576\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43576\">No</a>\n"]}, {"number": 43575, "title": "tensorflow.keras.wrappers.scikit_learn KerasClassifier using 1% of GPU", "body": "Hi,\r\nHaving tf 2.3.0 and keras 2.4.3\r\nGridSearchCV not using full GPU\r\n```\r\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\r\nfrom sklearn.model_selection import GridSearchCV\r\nBatchSize = [100,200,300,400,500,600,700,800,900,1000]\r\nEpochs = [100,200,300,400,500,600,700,800,900,1000]\r\n\r\n    def create_model():\r\n    \tmodel = Sequential()\r\n    \tmodel.add(Dense(int(num_cols/2), input_dim=num_cols, activation='relu'))\r\n    \tmodel.add(Dense(1, activation='sigmoid'))\r\n    \tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n    \treturn model\r\n    model = KerasClassifier(build_fn=create_model, verbose=0)\r\n    param_grid = dict(batch_size=BatchSize , epochs=Epochs )\r\n    grid = GridSearchCV(model, cv = 3, param_grid=param_grid,n_jobs=-1)\r\n    grid_result = grid.fit(X_train, y_train)         \r\n    best_hyperparameters = grid_result.best_params_\r\n```\r\ni have set\r\n```\r\nfrom tensorflow.python.client import device_lib\r\nassert 'GPU' in str(device_lib.list_local_devices())\r\n```\r\n![image](https://user-images.githubusercontent.com/30790120/94322155-0712a200-ffd5-11ea-83bf-c057628c71cf.png)\r\n\r\nIs this correct?", "comments": ["@hanzigs,\r\nCould you please run the below code and share the output with us\r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\n\r\nBy default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible to the process. Please take a look at [this](https://www.tensorflow.org/guide/profiler#gpu_kernel_stats) memory profile tool and let us know if you are facing the same issue. Thanks! ", "![image](https://user-images.githubusercontent.com/30790120/94655245-9efbed00-0341-11eb-9916-7b1f8d376ea2.png)\r\n\r\nThanks Got it\r\n\r\n![image](https://user-images.githubusercontent.com/30790120/94746300-cb564e80-03bf-11eb-95bc-d52a217dd5ee.png)\r\n\r\n\r\n", "In Spyder, i get following error\r\n\r\n![image](https://user-images.githubusercontent.com/30790120/94745881-ef656000-03be-11eb-8c33-897b4c986a07.png)\r\n\r\ni'm using\r\n\r\n![image](https://user-images.githubusercontent.com/30790120/94745889-f3917d80-03be-11eb-8cac-97f4c3eac234.png)\r\n\r\n![image](https://user-images.githubusercontent.com/30790120/94745913-0015d600-03bf-11eb-8ce6-15824708f4ac.png)\r\n\r\nAs per here\r\nhttps://www.tensorflow.org/install/source_windows#tested_build_configurations\r\n\r\nDo i have to change to cuDNN 7.4?\r\n", "Thanks, Above error happened first time, in second run it was not there. Not sure why?"]}, {"number": 43574, "title": "Fix for unit test: check for different error message in INTEL MKL enabled Tensorflow", "body": "In INTEL MKL enabled flow, the error shown is different from XLA/Eigen flow. This causes one of the unit tests to fail in INTEL MKL enabled flow. The fix addresses this case. \r\nFailing test :/tensorflow/python/kernel_tests/conv_ops_test.py (testOpEdgeCases)", "comments": ["Please close this PR, as it is replaced by a more recent one\r\n    https://github.com/tensorflow/tensorflow/pull/43590\r\n", "@jojivk73 Can you please check the above comment from @gzmkl and give confirmation to close this PR?  Thanks!", "@gbaned  Please close this PR, as it is replaced by a more recent one\r\n#43590", "@jojivk73 Can you please check the above comment from @gzmkl and give confirmation to close this PR? Thanks!", "No response from @jojivk73, hence closing the PR. Thanks!"]}, {"number": 43573, "title": "Fix for concat v2 unit test failure in [INTEL MKL] enabled Tensorflow", "body": "Fix for unit test failure in [INTEL MKL] Tensorflow\r\n    //tensorflow/python/ops/parallel_for:array_test (concat_v2) \r\nThe unit test has axis type : INT64 i not supported by MKLDNN.\r\nThe fix disallows the mapping to MKLDNN ops when any type other than INT32 is used for axis.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43573) for more info**.\n\n<!-- need_author_consent -->", "@penpornk  Update has been done per code review suggestions.", "> All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\r\n> \r\n> We need to confirm that all authors are ok with their commits being contributed to this project. Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\r\n> \r\n> _Note to project maintainer:_ There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent. In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43573) for more info**.\r\n\r\n@googlebot I consent \r\n\r\nHi, the author ( @jojivk73, Jojimon) is my teammate at Intel. He is on vacation and thus cannot confirm before he is back to work next Monday (October 12 2020). So please do special process for this case as we are approaching TF 2.4 cutting date. \r\nThanks!", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43573) for more info**.\n\n<!-- cla_yes -->", "Manually setting CLA to yes per @gzmkl's [comment](https://github.com/tensorflow/tensorflow/pull/43573#issuecomment-703853824)."]}, {"number": 43571, "title": "Added option to use MicroMutableOpResolver with the benchmarks.", "body": "This allows us to have the benchmarks additionally be representative of a final application w.r.t. binary size.\r\n\r\nModified the keyword_benchmark to use a `MicroMutableOpResolver` but leaving all other benchmarks with `AllOpsResolver`.\r\n\r\nNote that this change has the `MicroBenchmarkRunner` class take a pointer to a MicroOpResolver as a param, instead of the pattern of passing a const ref that the `MicroInterpreter` uses. This is because a const ref as a param is problematic if we store a reference (https://abseil.io/tips/116).\r\n\r\nThe biggest difference as a result of this change is a decrease in the text section (as a result of only having symbols for the kernels registered via the `MicroMutableOpResolver` as opposed to including all the kernels via the `AllOpsResover`.\r\n\r\nWe also get some bss / data reductions as well from having fewer globals and statics.\r\n\r\nManually tested the following commands:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 person_detection_experimental_benchmark person_detection_benchmark keyword_benchmark TARGET=sparkfun_edge\r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 person_detection_experimental_benchmark person_detection_benchmark keyword_benchmark TARGET=bluepill\r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 test_person_detection_experimental_benchmark TAGS=posix\r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 test_person_detection_benchmark TAGS=posix\r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 test_keyword_benchmark TAGS=posix\r\n```\r\n\r\nSize profiling (AllOpsResolver vs MicroMutableOpResolver):\r\n\r\nSparkfun Edge with reference kernels:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=sparkfun_edge keyword_benchmark\r\nsize tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/keyword_benchmark\r\n```\r\n\r\n|              |     text     |   data |      bss    |    dec     |\r\n|-----        |   --------   | -------  |  ------      | -----        |\r\n| Before  |  284040  |    124 |  108908  |  393072 |\r\n| After     |    92884  |    124 |  104396  |  197404 |\r\n\r\nWith `TARGET=xtensa_hifimini TAGS=xtensa_hifimini`:\r\n|              |   text      |   data   |   bss      |    dec    |\r\n|-----        | --------    | -------    |  ------     | -----       |\r\n| Before  |  255544 |  56888 |  29944  |  342376 |\r\n| After     |    54160 |  48824 |  25064  |  128048 |\r\n \r\n\r\n", "comments": ["@njeffrie this PR is now ready for review. The PR description describes the tests since we don't have much CI coverage yet (which is the subject of #43509)"]}, {"number": 43570, "title": "Solve IndexError: list index out of range #43561", "body": "Just as I said in #43561 , when I was trying to load the sequential model [here](https://github.com/vogon101/skincancer/blob/master/models/model_6_combined_2_DA.h5) using `tf.keras.models.load_model` in **TF 2.3.1**, an error is thrown at the following location:\r\n\r\n```bash\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in _should_skip_first_node(layer)\r\n   1031   return (isinstance(layer, Functional) and\r\n   1032           # Filter out Sequential models without an input shape.\r\n-> 1033           isinstance(layer._layers[0], input_layer_module.InputLayer))\r\n   1034 \r\n   1035 \r\nIndexError: list index out of range\r\n```\r\n\r\nThe model is believed to be trained using keras and under TF1.9, and the structure of the model can be found [here](https://github.com/vogon101/skincancer/blob/master/src/sandbox/ml_lib/combined_model_v2.py), and here's [the code for training](https://github.com/vogon101/skincancer/blob/master/src/sandbox/v6_combined_model_v2.py).\r\n\r\nHere you can find the full stack trace and running code under TF 2.3.1: https://colab.research.google.com/drive/1Lfo0O7D0cM8EtR0h6noqCoWqoqf8bzAD?usp=sharing\r\n\r\nI am able to fix this issue when replace [the code](https://github.com/tensorflow/tensorflow/blob/5d5534edf7d0b73cb23f7069135d674e1d27250b/tensorflow/python/keras/engine/functional.py#L1078-L1085) into following:\r\n```python3\r\ndef _should_skip_first_node(layer):\r\n  \"\"\"Returns True if the first layer node should not be saved or loaded.\"\"\"\r\n  # Networks that are constructed with an Input layer/shape start with a\r\n  # pre-existing node linking their input to output. This node is excluded from\r\n  # the network config.\r\n  if layer._layers:\r\n    return (isinstance(layer, Functional) and\r\n          # Filter out Sequential models without an input shape.\r\n          isinstance(layer._layers[0], input_layer_module.InputLayer))\r\n  else:\r\n    return isinstance(layer, Functional)\r\n```\r\n\r\nIf there's no side effects with this workaround, please merge this PR. Thanks!\r\n\r\nSigned-off-by: Hollow Man <hollowman@hollowman.ml>", "comments": ["/cc @k-w-w", "@k-w-w  Any updates about the reviewing?", "I have renamed `layer._layers` to `layer._self_tracked_trackables` to solve renaming conflict introduced in https://github.com/tensorflow/tensorflow/commit/d2664949539e69aadd5050329104b4a81a217ee3 . \r\n\r\nBy the way, again any updates about the reviewing? @k-w-w"]}, {"number": 43569, "title": "RuntimeError: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.Failed to apply the default TensorFlow Lite delegate indexed at 0.", "body": "Hi,\r\n\r\nI tried to use build XNNPack with respect of accelerating tflite inference on CPU. I built tensorflow2.4 from source code with flag \"--define tflite_with_xnnpack=true\".When I run inference with my .tflite model with BiLSTM layer, ther is an error:\r\nRuntimeError: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.Failed to apply the default TensorFlow Lite delegate indexed at 0.\r\n\r\nBut when I run a ResNet50 tflite model downloaded on TensorFlow Hub, no error occurs. What's more, I tried to run my tflite model with tf-nightly 2.4.0-dev20200917 and no error occurs.\r\n\r\nWhat's probably going wrong? Thank you.\r\n", "comments": ["@le8888e \r\nPlease provide with simple stand alone code to replicate issue faced along with error logs or if possible share a colab gist with issue reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43569\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43569\">No</a>\n", "I have the same problem. I converted an encoder-decoder model for machine translation using two separate tflite models respectively for the encoder and the decoder.\r\n\r\nThe encoder uses BiLSTM and generates the error in the title. The same error doesn't occur if I use `unroll=True`, but this is unfeasible for me as I need input sequences of length 100. In this case, I don't get the error but then the usage of RAM explodes.\r\n\r\nThe decoder, on the other hand, uses input of length 1 and LSTMCell instead of LSTM, and it doesn't produce the error.\r\n\r\nIs there a way to make the operation static-length without using unroll?\r\n", "> I have the same problem. I converted an encoder-decoder model for machine translation using two separate tflite models respectively for the encoder and the decoder.\r\n> \r\n> The encoder uses BiLSTM and generates the error in the title. The same error doesn't occur if I use `unroll=True`, but this is unfeasible for me as I need input sequences of length 100. In this case, I don't get the error but then the usage of RAM explodes.\r\n> \r\n> The decoder, on the other hand, uses input of length 1 and LSTMCell instead of LSTM, and it doesn't produce the error.\r\n> \r\n> Is there a way to make the operation static-length without using unroll?\r\n\r\nSame issue"]}, {"number": 43567, "title": "Test code for tensorflow source code", "body": "Sometimes, it is hard to debug the code without testing, or it is needed to write test for code.\r\nIn this case, if we create tests manually, then it will be needed to delete the test code directory or create a new branch without tests to send pull request.\r\nIs it possible to take into account?", "comments": ["`git add` (with the `-p` flag too) allows you to control which files/patches to add to a commit. You can push only a few commits to a branch for pull requests.\r\n\r\nIn general, we recommend each PR to also contain tests. There are multiple benefits to having tests in the same PR: we can see that the test fails without the fix and passes with it, we can see usage pattern, we can prevent future regression when a new change might break similar functionality.", "@mihaimaruseac \r\nThank you... Now if I send a pull request, I will write tests."]}, {"number": 43566, "title": "TFLu: Port TFL detection postprocess operator", "body": "This is fixing issue: https://github.com/tensorflow/tensorflow/issues/43565\r\n\r\n", "comments": ["@mansnils  Can you please resolve conflicts? Thanks!", "@mansnils Any update on this PR? Please. Thanks!", "Removing support for unknown output dimensions. Because it does not work with TFLiteEvalTensor API and it would require major changes in TFLu.\r\nMost models seen with this operator have unknown output dimensions. So to run those they have to be modified first so that the out dimensions are known beforehand.", "@mansnils can you please check sanity build failures ?", "A few of the CI failures:\r\n * the x86 build is failing for the TfLite Micro CI build. I think you need the arrays to be unsigned char instead of char.\r\n * the other two (MacOS CPU Python3 and Ubuntu Sanity) are related to the bazel build and BUILD file formatting.\r\n\r\nI think you can recreate with:\r\n```\r\nbazel test tensorflow/lite/micro:micro_interpreter_test\r\n```\r\n\r\nAnd buildifier can be installed using instructions from [here](https://github.com/bazelbuild/buildtools/tree/master/buildifier).\r\n\r\nI can help tomorrow if you have trouble reproducing these errors locally.", "> \r\n> ```\r\n> bazel test tensorflow/lite/micro:micro_interpreter_test\r\n> ```\r\n> \r\n\r\nThis one likely needs the arena size to be updated.", "@advaitjain Thanks for the tip about: bazel test tensorflow/lite/micro:micro_interpreter_test. It turns out that it TestIncompleteInitializationAllocationsWithSmallArena that fails in this line:\r\n   TF_LITE_MICRO_EXPECT_EQ(\r\n      static_cast<size_t>(128),\r\n       allocator->GetSimpleMemoryAllocator()->GetHeadUsedBytes()); And it turns out this is caused by the change of kMaxScratchBuffersPerOp  in micro_allocator.cc. It seems allocator->GetSimpleMemoryAllocator()->GetHeadUsedBytes() will change depending on kMaxScratchBuffersPerOp in this test.\r\n", "I am pushing the fix for the micro_interpreter_test to see what tests it is fixing", "@mansnils, I have pushed a commit fixing some remaining issues with https://github.com/tensorflow/tensorflow/pull/43566/commits/5491648d7f710d445ffe873afbec0b9c7834be47.\r\n\r\nThis PR should be ready to go now.", "Unfortunately, the TFLM build is still failing. I think we're going to have to take a second look at the kernel runner and see how we can fix this.\r\n\r\nI'll take a look tomorrow.", "There is error: narrowing conversion of '-108' from 'int' to 'char' inside { } [-Wnarrowing] because of\r\nconst char g_gen_data_none_regular_nms[] = { .. -108\r\nNormally that would be wrong but in this case it is actually intentional so I figure it is ok to use\r\n#pragma GCC diagnostic ignored \"-Wnarrowing\"\r\nin the generated code.", "@advaitjain  To get consistent results between TFL and TFLM I am considering changing this in both kernels.\r\nFrom:\r\n\r\n    float ycenter = box_centersize.y / scale_values.y * anchor.h + anchor.y;\r\n    float xcenter = box_centersize.x / scale_values.x * anchor.w + anchor.x;\r\n    float half_h =\r\n        0.5f * static_cast<float>(std::exp(box_centersize.h / scale_values.h)) *\r\n        anchor.h;\r\n    float half_w =\r\n        0.5f * static_cast<float>(std::exp(box_centersize.w / scale_values.w)) *  anchor.w;\r\n\r\nTo:\r\n\r\n    float ycenter = static_cast<float>(\r\n        static_cast<double>(box_centersize.y) /\r\n        static_cast<double>(scale_values.y) *\r\n        static_cast<double>(anchor.h) + static_cast<double>(anchor.y));\r\n    float xcenter =  static_cast<float>(\r\n        static_cast<double>(box_centersize.x) /\r\n        static_cast<double>(scale_values.x) *\r\n        static_cast<double>(anchor.w) + static_cast<double>(anchor.x));\r\n    float half_h = static_cast<float>(\r\n        0.5 * (std::exp(static_cast<double>(box_centersize.h) /\r\n                        static_cast<double>(scale_values.h))) *\r\n        static_cast<double>(anchor.h));\r\n    float half_w =  static_cast<float>(\r\n        0.5 * (std::exp(static_cast<double>(box_centersize.w) /\r\n                        static_cast<double>(scale_values.w))) *\r\n                        static_cast<double>(anchor.w));\r\n\r\nQuestion: Would it it be ok to change this in the TFL kernel or could it break some test?", "Sorry, forget the comment about: #pragma GCC diagnostic ignored \"-Wnarrowing\"", "@advaitjain  To get rid of -Wnarrowing we should generate const int8_t* and cast back to const char* in detection_postprocess_test.cc. I was about to push this (after downloading your last commit) but have some problem:\r\n\r\nerror: failed to push some refs to 'git@github.com:mansnils/tensorflow.git'\r\nhint: Updates were rejected because the remote contains work that you do\r\nhint: not have locally. This is usually caused by another repository pushing\r\nhint: to the same ref. You may want to first integrate the remote changes\r\nhint: (e.g., 'git pull ...') before pushing again.\r\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\r\n\r\nHow do I pull your changes?", "> @advaitjain To get rid of -Wnarrowing we should generate const int8_t* and cast back to const char* in detection_postprocess_test.cc. I was about to push this (after downloading your last commit) but have some problem:\r\n> \r\n> error: failed to push some refs to '[git@github.com](mailto:git@github.com):mansnils/tensorflow.git'\r\n> hint: Updates were rejected because the remote contains work that you do\r\n> hint: not have locally. This is usually caused by another repository pushing\r\n> hint: to the same ref. You may want to first integrate the remote changes\r\n> hint: (e.g., 'git pull ...') before pushing again.\r\n> hint: See the 'Note about fast-forwards' in 'git push --help' for details.\r\n> \r\n> How do I pull your changes?\r\n\r\nI think if you git pull that would do the trick. I had pushed to your remote branch so you local branch was a couple commits behind.\r\n\r\nI have made the fix that you suggested with https://github.com/tensorflow/tensorflow/pull/43566/commits/aad242613d71dbc38bc7a8a0633e31fb192ebb85 so this issue is resolved.\r\n", "> @advaitjain To get consistent results between TFL and TFLM I am considering changing this in both kernels.\r\n> From:\r\n> \r\n> ```\r\n> float ycenter = box_centersize.y / scale_values.y * anchor.h + anchor.y;\r\n> float xcenter = box_centersize.x / scale_values.x * anchor.w + anchor.x;\r\n> float half_h =\r\n>     0.5f * static_cast<float>(std::exp(box_centersize.h / scale_values.h)) *\r\n>     anchor.h;\r\n> float half_w =\r\n>     0.5f * static_cast<float>(std::exp(box_centersize.w / scale_values.w)) *  anchor.w;\r\n> ```\r\n> \r\n> To:\r\n> \r\n> ```\r\n> float ycenter = static_cast<float>(\r\n>     static_cast<double>(box_centersize.y) /\r\n>     static_cast<double>(scale_values.y) *\r\n>     static_cast<double>(anchor.h) + static_cast<double>(anchor.y));\r\n> float xcenter =  static_cast<float>(\r\n>     static_cast<double>(box_centersize.x) /\r\n>     static_cast<double>(scale_values.x) *\r\n>     static_cast<double>(anchor.w) + static_cast<double>(anchor.x));\r\n> float half_h = static_cast<float>(\r\n>     0.5 * (std::exp(static_cast<double>(box_centersize.h) /\r\n>                     static_cast<double>(scale_values.h))) *\r\n>     static_cast<double>(anchor.h));\r\n> float half_w =  static_cast<float>(\r\n>     0.5 * (std::exp(static_cast<double>(box_centersize.w) /\r\n>                     static_cast<double>(scale_values.w))) *\r\n>                     static_cast<double>(anchor.w));\r\n> ```\r\n> \r\n> Question: Would it it be ok to change this in the TFL kernel or could it break some test?\r\n\r\nBeing explicit about float vs double math seems very reasonable to me. It would be nice if we could make this less verbose though. I'd be surprised if it breaks something.\r\n\r\nLet's keep this change for a follow-on PR.", "Summarizing where we are with this PR for @mansnils @njeffrie and me:\r\n\r\n * the flex buffer generation and char vs unsinged char is resolved and all tests pass with both bazel and make (on x86)\r\n * build files etc are all formatted\r\n\r\nWe are still back to a different version of the original problem on bluepill:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill kernel_detection_postprocess_test\r\n```\r\n\r\ngives the following error:\r\n```\r\n`__lock___malloc_recursive_mutex' referenced in section `.text.__malloc_lock' of /home/advaitjain/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/lib/thumb/v7-m/libc.a(lib_a-mlock.o): defined in discarded section `COMMON' of /home/advaitjain/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/lib/thumb/v7-m/libc.a(lib_a-lock.o)\r\n`__lock___malloc_recursive_mutex' referenced in section `.text.__malloc_unlock' of /home/advaitjain/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/lib/thumb/v7-m/libc.a(lib_a-mlock.o): defined in discarded section `COMMON' of /home/advaitjain/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/lib/thumb/v7-m/libc.a(lib_a-lock.o)\r\n`errno' referenced in section `.text._sbrk_r' of /home/advaitjain/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/lib/thumb/v7-m/libc.a(lib_a-sbrkr.o): defined in discarded section `COMMON' of /home/advaitjain/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/lib/thumb/v7-m/libc.a(lib_a-reent.o)\r\n/home/advaitjain/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/lib/thumb/v7-m/libc.a(lib_a-sbrkr.o): In function `_sbrk_r':\r\nsbrkr.c:(.text._sbrk_r+0xc): undefined reference to `_sbrk'\r\n```\r\n\r\nThe error message is puzzling to me since it is saying something about being in the discarded section which makes me wonder why its an issue.\r\n\r\n@mansnils: you should be able to `git pull` and have my changes appear on your local branch as well. If you can debug this _sbrk issue, that would be great.", "It turns out the _sbrk issue is caused by the call to Flexbuffers AsFloat() in PPD. That call will reference  flatbuffers::StringToNumber(), which is the culprit. flatbuffers::StringToNumber() is not called directly so it can be removed with the unit test for PPD still passing. I am not sure how to resolve this issue in the best way. However I am proposing a temporary workaround to patch flexbuffers.h for Bluepill.", "Agreed that we should patch flatbuffers as a workaround for now.\r\n\r\nI'd prefer having the patching be centralized instead of in the bluepill makefile.\r\n\r\nI'm going to do this in two steps:\r\n 1. https://github.com/tensorflow/tensorflow/pull/44860 pulls the flatbuffer downloads into its own script.\r\n 2. I'll add a follow-on PR that does the patching. I have some prototyping commands here and will put them into a PR on Monday:\r\n```bash\r\nstring_to_num_line=`awk '/StringToNumber/{ print NR; }' flexbuffers.h`\r\ncase_string_line=$((${string_to_num_line} - 2))\r\n\r\nhead -n ${case_string_line} flexbuffers.h > new_flexbuffers.h\r\n\r\necho \"#if 1\" >> new_flexbuffers.h\r\necho \"          // Introduce a segfault for an unsupported code path for TFLM.\" >> new_flexbuffers.h\r\necho \"          return *(static_cast<double*>(nullptr));\" >> new_flexbuffers.h\r\necho \"#else\" >> new_flexbuffers.h\r\necho \"          // This is the original code\" >> new_flexbuffers.h\r\nsed -n -e $((${string_to_num_line} -  1)),$((${string_to_num_line} + 1))p flexbuffers.h >> new_flexbuffers.h\r\necho \"#endif\" >> new_flexbuffers.h\r\n\r\ntotal_num_lines=`wc -l flexbuffers.h | awk '{print $1}'`\r\nsed -n -e $((${string_to_num_line} + 2)),${total_num_lines}p flexbuffers.h >> new_flexbuffers.h\r\n```\r\n", "Created #44915 to patch flatbuffers to avoid pulling in strtod. Once that is merged, we should be able to revert 18ec320.\r\n\r\n@mansnils, I'd suggest removing 18ec320 and then force-push.", "@advaitjain Ok, will do. Will just correct the name of the readme file.", "@mansnils  Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned Ubuntu Sanity error fixed", "Pushed a commit to fix internal errors. Let's see how the merging goes this time.", "@advaitjain After pulling your changes, ci_sanity.sh no longer passed so I pushed https://github.com/tensorflow/tensorflow/pull/43566/commits/0bc6594e93d9f1326d1e359f274ab531b2da4730. At least now Ubunty Sanity passed but Windows and Mac builds still failed. Is it stability issues or what has changed?", "> @advaitjain After pulling your changes, ci_sanity.sh no longer passed so I pushed [0bc6594](https://github.com/tensorflow/tensorflow/commit/0bc6594e93d9f1326d1e359f274ab531b2da4730). At least now Ubunty Sanity passed but Windows and Mac builds still failed. Is it stability issues or what has changed?\r\n\r\nThanks Mans. The Win and Mac errors look like existing failures. Approved this PR and will see what the internal checks have to say.", "***(sigh)*** Internal checks failed but the external CI for Micro passed due to discrepancy between internal and external.\r\n\r\nThis PR makes the external CI closer to the internal checks: https://github.com/tensorflow/tensorflow/pull/45018\r\n", "@advaitjain It seems the iOS error is an existing error, right?", "Thanks for the fix. Yes, the iOS error is existing and unrelated.", "Still having issues unfortunately :-/\r\n\r\nThe CI is failing with:\r\n```\r\ntensorflow/lite/micro/kernels/detection_postprocess.cc: In function \u2018TfLiteStatus tflite::{anonymous}::Eval(TfLiteContext*, TfLiteNode*)\u2019:\r\ntensorflow/lite/micro/kernels/detection_postprocess.cc:591:58: error: potential null pointer dereference [-Werror=null-dereference]\r\n   tflite::micro::GetTensorData<float>(num_detections)[0] =\r\n   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\r\n       size_of_sorted_indices;\r\n       ~~~~~~~~~~~~~~~~~~~~~~                              \r\ntensorflow/lite/micro/kernels/detection_postprocess.cc:687:58: error: potential null pointer dereference [-Werror=null-dereference]\r\n   tflite::micro::GetTensorData<float>(num_detections)[0] = output_box_index;\r\n```\r\n\r\nbut it is not reproducible locally.\r\n\r\nMy current guess is that the gcc version used as part of CI is old (gcc-6.3.0). I have a change under review that updates https://github.com/tensorflow/tensorflow/blob/3d3c4aaaedaf117b3b8cc9c30a594889094db6f1/tensorflow/tools/ci_build/Dockerfile.micro#L4\r\n\r\nto\r\n```\r\npython:3.9.0-buster\r\n```\r\n\r\nwhich should mean that we use gcc 8.3.0 as part of CI. Let's see what happens when that change get merged.", "@advaitjain It seems like the updated docker didn't help, or do we need to rebase?", "> Still having issues unfortunately :-/\r\n> \r\n> The CI is failing with:\r\n> \r\n> ```\r\n> tensorflow/lite/micro/kernels/detection_postprocess.cc: In function \u2018TfLiteStatus tflite::{anonymous}::Eval(TfLiteContext*, TfLiteNode*)\u2019:\r\n> tensorflow/lite/micro/kernels/detection_postprocess.cc:591:58: error: potential null pointer dereference [-Werror=null-dereference]\r\n>    tflite::micro::GetTensorData<float>(num_detections)[0] =\r\n>    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\r\n>        size_of_sorted_indices;\r\n>        ~~~~~~~~~~~~~~~~~~~~~~                              \r\n> tensorflow/lite/micro/kernels/detection_postprocess.cc:687:58: error: potential null pointer dereference [-Werror=null-dereference]\r\n>    tflite::micro::GetTensorData<float>(num_detections)[0] = output_box_index;\r\n> ```\r\n> \r\n> but it is not reproducible locally.\r\n> \r\n> My current guess is that the gcc version used as part of CI is old (gcc-6.3.0). I have a change under review that updates\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/3d3c4aaaedaf117b3b8cc9c30a594889094db6f1/tensorflow/tools/ci_build/Dockerfile.micro#L4\r\n> \r\n> to\r\n> \r\n> ```\r\n> python:3.9.0-buster\r\n> ```\r\n> \r\n> which should mean that we use gcc 8.3.0 as part of CI. Let's see what happens when that change get merged.\r\n\r\n\r\nI wasn't able to reproduce locally because I hadn't done a `make clean_downloads`.\r\n\r\nCreated https://github.com/tensorflow/tensorflow/pull/45286 to address this issue. [This comment](https://github.com/tensorflow/tensorflow/issues/44971#issuecomment-736130061) has some additional context."]}, {"number": 43565, "title": "Detection postprocess operator missing", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source): 9aff666a8db689168e2d3aaef17b8a252791ada7\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\nDetection postprocess operator is missing from TFLu. It is needed for e.g. SSD Mobilenet V2.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["Detection post process op is finally merged. However it still differs between TFL and TFLM because of floating point imprecision on different platforms. It can be fixed by e.g. this change in both kernels:\r\n\r\n#if 1\r\n\r\n    float ycenter = static_cast<float>(\r\n        static_cast<double>(box_centersize.y) /\r\n        static_cast<double>(scale_values.y) *\r\n        static_cast<double>(anchor.h) + static_cast<double>(anchor.y));\r\n    float xcenter = static_cast<float>(\r\n        static_cast<double>(box_centersize.x) /\r\n        static_cast<double>(scale_values.x) *\r\n        static_cast<double>(anchor.w) + static_cast<double>(anchor.x));\r\n    float half_h = static_cast<float>(\r\n        0.5 * (std::exp(static_cast<double>(box_centersize.h) /\r\n                        static_cast<double>(scale_values.h))) *\r\n        static_cast<double>(anchor.h));\r\n    float half_w = static_cast<float>(\r\n        0.5 * (std::exp(static_cast<double>(box_centersize.w) /\r\n                        static_cast<double>(scale_values.w))) *\r\n        static_cast<double>(anchor.w));\r\n#else\r\n\r\n    float ycenter = box_centersize.y / scale_values.y * anchor.h + anchor.y;\r\n    float xcenter = box_centersize.x / scale_values.x * anchor.w + anchor.x;\r\n    float half_h =\r\n        0.5f * static_cast<float>(std::exp(box_centersize.h / scale_values.h)) *\r\n        anchor.h;\r\n    float half_w =\r\n        0.5f * static_cast<float>(std::exp(box_centersize.w / scale_values.w)) *\r\n        anchor.w;\r\n\r\n#endif", "I will create a new issue for this. So that this one can be closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43565\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43565\">No</a>\n"]}, {"number": 43564, "title": "Missing 1.15.4 release on Docker Hub", "body": "I'm sorry if this is not the proper place to file that kind of issue, but I could not find a better place to report that issue. If such a place exist, please advise me of the details, I'll happily forward my request.\r\n\r\nSo, 1.15.4 has been released yesterday, containing a fix for CuDNN usage. While working on upgrading our codebase to refer that new version, I stumbled upon the fact that the TensorFlow Docker Hub does not offer any image after 1.15.2: as you can see on that link https://hub.docker.com/r/tensorflow/tensorflow/tags?page=1&name=1.15 there is no 1.15.3 nor 1.15.4.\r\n\r\nI obviously don't know the details of your release process, but I don't think this is expected, and the lack of 1.15.3 makes me thinking this is not just a delay in building / uploading.\r\n\r\nSo, it would really be nice if the 1.15.4 images could be built and pushed to Docker Hub :).", "comments": ["/cc @mihaimaruseac ", "It seems we were missing this from the release process. 1.15.3 docker images have just been updated (we were testing if things work) and 1.15.4 and the other tags for the recent patch releases are currently building.", "@mihaimaruseac Thanks for the quick handling, it looks like it is working as expected, this looks fixed to me :)", "@lissyx Thanks, please close the ticket if it is ok for you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43564\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43564\">No</a>\n"]}, {"number": 43563, "title": "Avoid using TF file system before TF is initialized with plugins (avoid circular dependency)", "body": "While playing with modular file systems I noticed that\r\nif modular file systems is enabled through bazel with linkage inclusion,\r\ntensorflow will not be able to be imported:\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py\", line 434, in <module>\r\n    _ll.load_library(_main_dir)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/load_library.py\", line 143, in load_library\r\n    if file_io.file_exists(library_location):\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py\", line 250, in file_exists\r\n    return file_exists_v2(filename)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py\", line 268, in file_exists_v2\r\n    _pywrap_file_io.FileExists(compat.path_to_bytes(path))\r\ntensorflow.python.framework.errors_impl.UnimplementedError: File system scheme '[local]' not implemented (file: '/usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels')\r\n```\r\n\r\nThe reason is that, at the very beginning of tensorflow, tf will try to load libraries in .so file (see `tensorflow/__init__.py`).\r\nHowever, at this time the plugins has not been initialized yet, so TF's own `file_io.file_exists` is not working yet. But as `tf.load_library` used `file_io.file_exists`, this causes a circular dependency.\r\n\r\nThis PR replaces `file_io.file_exists` with `os.path.exists` in or before `tf.load_library`.\r\n\r\nThis will not impact exisitng behavior: since `tf.load_library` relies on `dlopen` (and `LoadLibrary` on Windows), a non-native file system (e.g., s3/gcs/etc) will not work with `tf.load_library` anyway. As such there is no need to use gcs/s3 compatible `file_io.file_exists`.\r\n\r\nThis PR is part of the effort to enable modular file system from external repos (e.g., tensorflow-io).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["I think plan here was to always link the local filesystem to the pip package. But let's take this one for now, to not have you blocked while we work on implementing Windows and doing the full switch.", "Thanks @mihaimaruseac a lot for the help! \ud83d\udc4d \u2764\ufe0f "]}, {"number": 43562, "title": "issue numpy at Bazel build", "body": "In continue #29845:\r\n>@navi63 this issue has no relationship with your question. Please open new issues if they >are issues with TensorFlow library code or please ask other questions on StackOverflow.\r\n>\r\n>Locking conversation here as it has been solved already\r\n>\r\n>_Originally posted by @mihaimaruseac in >https://github.com/tensorflow/tensorflow/issues/29845#issuecomment-613544203_\r\n\r\n@mihaimaruseac New contributors have a problem with build with Bazel. What is a solution you refer once you locked not resolved issue?!\r\nIs any valid build instruction?", "comments": ["I did investigation. There are a lot off open/close issues at numpy repo about error message we are getting during build with Bazel. Such closed issues was closed without code change. So problem exist. Best solution describe build process with Bazel at CI/CD here. ", "@spirinvladimir,\r\nIn order to expedite the trouble-shooting process, could you please provide the below details \r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nand the complete error log with the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "@amahendrakar I do not have an issue. There is a problem at numpy and tensorflow by luck of using numpy.\r\nPlease use all details from not resolved issue #29845. I create this one cause not resolution was provided to locked issue  #29845.", "I as part of open-source community glad to know exactly resolution:\r\n>Locking conversation here as it has been solved already", "Hi, @spirinvladimir \r\n\r\nFirst, issue you link to has been resolved: https://github.com/tensorflow/tensorflow/issues/29845#issuecomment-612574610\r\n\r\nNext comment on the issue is an unrelated problem (building a protobuf file vs building TF): https://github.com/tensorflow/tensorflow/issues/29845#issuecomment-613265619 That's why in my comment, which you quoted, I say the issue has been resolved; https://github.com/tensorflow/tensorflow/issues/29845#issuecomment-613544203\r\n\r\nThe install from source page mentions that users have to install numpy before building. It's the second code block on the page, before any `blaze` command. https://www.tensorflow.org/install/source\r\n\r\nError also suggest installing numpy in the environment of the Python interpreter that is used during compilation:\r\n```\r\nModuleNotFoundError: No module named 'numpy'\r\nIs numpy installed?\r\n```\r\n\r\nFinally, same solution (make sure numpy is installed for the python you are using) is also listed in the issue you quoted: https://github.com/tensorflow/tensorflow/issues/29845#issuecomment-613303239\r\n\r\nSo, to reiterate the solution: make sure your environment that you use to build TF has numpy installed. Best is to create a virtual environment, install numpy there and use the same python for `./configure.py`.\r\n\r\nIf this still does not work, feel free to post actual compile logs and fill in issue template.", "Hi, @mihaimaruseac \r\n\r\nI do understand logic and relation of links to text you write.\r\nI think I got a point. \ud83d\udca1 \ud83d\ude04 \r\n@google I tried to help tensorflow. \ud83d\udc4d ", "Can we close the issue now? Or are there still things left unsolved?", "Up to you.\n\nOn Mon, 28 Sep 2020, 8:21 PM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> Can we close the issue now? Or are there still things left unsolved?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43562#issuecomment-700171577>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAVGZ4MA6Q6LA2A7NHGGLD3SIDAYRANCNFSM4RZNSXCQ>\n> .\n>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43562\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43562\">No</a>\n"]}, {"number": 43561, "title": "TF1 Keras Model Errors on Loading using TF2 - IndexError: list index out of range", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu **18.04**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.3.1**\r\n- Python version: **3.7.3**\r\n\r\n**Describe the current behavior**\r\n\r\nWhen When trying to load the sequential model [here](https://github.com/vogon101/skincancer/blob/master/models/model_6_combined_2_DA.h5) using `tf.keras.models.load_model` in **TF 2.3.1**, an error is thrown at the following location:\r\n\r\n```bash\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in _should_skip_first_node(layer)\r\n   1031   return (isinstance(layer, Functional) and\r\n   1032           # Filter out Sequential models without an input shape.\r\n-> 1033           isinstance(layer._layers[0], input_layer_module.InputLayer))\r\n   1034 \r\n   1035 \r\nIndexError: list index out of range\r\n```\r\n\r\nThe model is believed to be trained using keras and under TF1.9, and the structure of the model can be found [here](https://github.com/vogon101/skincancer/blob/master/src/sandbox/ml_lib/combined_model_v2.py), and here's [the code for training](https://github.com/vogon101/skincancer/blob/master/src/sandbox/v6_combined_model_v2.py).\r\n\r\nHere you can find the full stack trace and running code under TF 2.3.1: https://colab.research.google.com/drive/1Lfo0O7D0cM8EtR0h6noqCoWqoqf8bzAD?usp=sharing\r\n\r\nThen I downgraded to TF 2.2 and 2.1 with the same code above, it threw the error just as #35934 .\r\n\r\nThen I downgraded to TF 2.0, the code was executing indefinitely. Finally I had to manually stop it:\r\n\r\n```bash\r\n/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py in IsMapping(o)\r\n   2569 \r\n   2570     \"\"\"\r\n-> 2571     return _pywrap_tensorflow_internal.IsMapping(o)\r\n   2572 \r\n   2573 def IsMappingView(o):\r\nKeyboardInterrupt: \r\n```\r\n\r\nHere you can find the full stack trace when I stopped the code under TF 2.0: https://colab.research.google.com/drive/1fCR-ci05NuYhQ8M9O2lRVG0F0YzI9Ggo?usp=sharing\r\n\r\nThen I had no choice but to use TF 1.15.4 and Keras 2.3.1, and finally it worked out fine, inputs, outputs, summary etc. are all parsed correctly, as well as being able to run data through the model: https://colab.research.google.com/drive/1XaRMeiT1SefS6Q10wsa0y9rEercyFlCR?usp=sharing\r\n\r\n**Describe the expected behavior**\r\n\r\nI hope the Bug can be resolved so that TF2 can enhance the support for old models.\r\n", "comments": ["I don't see your model definition but please double check our compatibility checklist:\r\nhttps://www.tensorflow.org/guide/versions#compatibility_of_savedmodels_graphs_and_checkpoints", "> I don't see your model definition but please double check our compatibility checklist:\r\n> https://www.tensorflow.org/guide/versions#compatibility_of_savedmodels_graphs_and_checkpoints\r\n\r\nThanks for your suggestion. It's a Keras Model, and I don't believe the compatibility checklist includes my issue. The model definition is [here](https://github.com/vogon101/skincancer/blob/master/src/sandbox/ml_lib/combined_model_v2.py), and here's [the code for training](https://github.com/vogon101/skincancer/blob/master/src/sandbox/v6_combined_model_v2.py).", "I think you can try to upgrade your code:\r\nhttps://www.tensorflow.org/guide/upgrade\r\n\r\nAlso in the model definition you was using `from keras import` but keras in Tensorflow is now `from tensorflow.keras import`", "> I think you can try to upgrade your code:\r\n> https://www.tensorflow.org/guide/upgrade\r\n> \r\n> Also in the model definition you was using `from keras import` but keras in Tensorflow is now `from tensorflow.keras import`\r\n\r\nThank you for your quick reply. I know the code needs to upgrade since it was the code for TF1, but what I'm asking is that can I just reuse in TF2 the model [model_6_combined_2_DA.h5](https://github.com/vogon101/skincancer/blob/master/models/model_6_combined_2_DA.h5) trained in TF1? When I was trying to load the model in TF2.3.1 with the following code:\r\n```python3 \r\nfrom tensorflow.keras.models import load_model\r\nmodel = load_model(\"model_6_combined_2_DA.h5\")\r\nprint(model.summary())\r\n```\r\nAn error was thrown:\r\n\r\n```bash\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in _should_skip_first_node(layer)\r\n   1031   return (isinstance(layer, Functional) and\r\n   1032           # Filter out Sequential models without an input shape.\r\n-> 1033           isinstance(layer._layers[0], input_layer_module.InputLayer))\r\n   1034 \r\n   1035 \r\nIndexError: list index out of range\r\n```\r\n\r\nOther details can be found in the description part above.\r\n\r\n", "You can try to use Keras instead of `tf.keras` for model loading:\r\n`from keras.models import load_model` but with a TF version that is compatible with Keras 2.3.1", "> You can try to use Keras instead of `tf.keras` for model loading:\r\n> `from keras.models import load_model` but with a TF version that is compatible with Keras 2.3.1\r\n\r\nWell, I have tried to use `keras` instead of `tf.keras` with TF 2.3.1 and Keras 2.3.1, first I encountered an error that can be solved in this way: https://github.com/tensorflow/tensorflow/issues/38589#issuecomment-665930503 . Then another error occurs:\r\n\r\n```bash\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in function(inputs, outputs, updates, name, **kwargs)\r\n   3931     if updates:\r\n   3932       raise ValueError('`updates` argument is not supported during '\r\n-> 3933                        'eager execution. You passed: %s' % (updates,))\r\n   3934     from tensorflow.python.keras import models  # pylint: disable=g-import-not-at-top\r\n   3935     from tensorflow.python.keras.utils import tf_utils  # pylint: disable=g-import-not-at-top\r\n\r\nValueError: `updates` argument is not supported during eager execution. You passed: [<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=0>, <tf.Variable 'UnreadVariable' shape=(3, 3, 3, 32) dtype=float32, numpy=\r\narray([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n          0., 0.],\r\n......\r\n```\r\n\r\nThe gist is here: https://colab.research.google.com/drive/1OovMHVrMBsIwcwn2PUcgbEXHUfPLMdyM?usp=sharing\r\n\r\nAny suggestions?", "> \r\n> ```python\r\n> from tensorflow.keras.models import load_model\r\n> model = load_model(\"model_6_combined_2_DA.h5\")\r\n> print(model.summary())\r\n> ```\r\n> \r\n> An error was thrown:\r\n> \r\n> ```shell\r\n> ~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in _should_skip_first_node(layer)\r\n>    1031   return (isinstance(layer, Functional) and\r\n>    1032           # Filter out Sequential models without an input shape.\r\n> -> 1033           isinstance(layer._layers[0], input_layer_module.InputLayer))\r\n>    1034 \r\n>    1035 \r\n> IndexError: list index out of range\r\n> ```\r\n> \r\n> Other details can be found in the description part above.\r\n\r\n\r\nI am able to fix this issue when replace [the code](https://github.com/tensorflow/tensorflow/blob/5d5534edf7d0b73cb23f7069135d674e1d27250b/tensorflow/python/keras/engine/functional.py#L1078-L1085) into following:\r\n```python3\r\ndef _should_skip_first_node(layer):\r\n  \"\"\"Returns True if the first layer node should not be saved or loaded.\"\"\"\r\n  # Networks that are constructed with an Input layer/shape start with a\r\n  # pre-existing node linking their input to output. This node is excluded from\r\n  # the network config.\r\n  if layer._layers:\r\n    return (isinstance(layer, Functional) and\r\n          # Filter out Sequential models without an input shape.\r\n          isinstance(layer._layers[0], input_layer_module.InputLayer))\r\n  else:\r\n    return isinstance(layer, Functional)\r\n```\r\n\r\nCan you suggest if there's some side effects? if no, I will make a pull request.\r\n\r\nHere is the gist:https://colab.research.google.com/drive/1jmp77GoIdZ8wy-hplmvq4MqgMRkSePsK?usp=sharing", "As it is not a bug I suggest you to close this and post you example in https://stackoverflow.com/questions/tagged/tensorflow", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43561\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43561\">No</a>\n"]}]