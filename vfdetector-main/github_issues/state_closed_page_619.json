[{"number": 35078, "title": "[INTEL MKL] Fix quantize accuracy loss", "body": "Allocate quantize min first mode tmp buffer every time, to avoid the\r\naccuracy loss  with batches run in parallel", "comments": []}, {"number": 35077, "title": "Update README.md to include Feature prioritization survey", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35077) for more info**.\n\n<!-- need_sender_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35077) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 35076, "title": "how to convert NonMaxSuppressionV4 to older versions", "body": "Hi, \r\nI am training a tensorflow object detection model and am converting it to tensorflow js. Although there is no NonMaxSuppressionV4 in the config.proto file (but has batch_non_max_suppression), I am getting the error \r\nValueError: Unsupported Ops in the model before optimization\r\nNonMaxSuppressionV4\r\n\r\nwhen trying to use tensorflowjs_converter to convert the trained model into tensorflow js. Is there a way I can convert my checkpoints into saved_model that doesn't convert nonmaxsuppression to v4 (I think v3 is supported in tensorflowjs)? I am using tensorflow 1.15.0, but back tracking to tensorflow 1.13.0 has not helped either. \r\n\r\nI would appreciate it if someone explains what is happening, or how I can convert the model into supported tensorflow js operators (after creating tensorflowjs with --skip_op_check or before it at the time of model training or exporting it into saved or frozen model)?\r\n\r\nThank you in advance\r\n", "comments": ["@shnamin ,\r\nThis issue is related to Tensorflow.js repo, can you please open issue using this [link](https://github.com/tensorflow/tfjs/issues/new)? Also there are already issue's [2540](\r\nhttps://github.com/tensorflow/tfjs/issues/2540)& [2450](https://github.com/tensorflow/tfjs/issues/2450) being reported for the same.Thanks!"]}, {"number": 35075, "title": "ValueError: Unable to save the object ListWrapper([ListWrapper([None])]) tensorflow 2.0", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 10/ cudnn 7.6.5\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\nWhen saving a model, in my case, HRNet, i met the following error: ValueError: Unable to save the object ListWrapper([ListWrapper([None])])\r\nThe error message suggest that \"If you don't need this list checkpointed, wrap it in a tf.contrib.checkpoint.NoDependency object; it will be automatically un-wrapped and subsequently ignored.\" However, tf 2.0 has no module named contri.\r\n\r\n**Describe the expected behavior**\r\nSave the model correctly.\r\n**Code to reproduce the issue**\r\nYou can run the following code to reproduce this issue\r\nhttps://github.com/zheLim/auto-face-parsing/blob/master/lib/model/seg_hrnet.py\r\n", "comments": ["@zheLim, Could provide the minimal standalone code to replicate the reported issue. Thanks!", "Hi, I have try to provide minimal code, but the problem only happens on the code i proviode. I think it is better to give it up.\r\n", "@zheLim, It would be easy if you share the minimal reproducible code to analyze the reported issue and move faster. Thanks!", "@zheLim, Any update on minimal code snippet.", "> @zheLim, Any update on minimal code snippet.\r\n\r\nThanks for your attention. I did not spend more time on this issue. ", "@zheLim, Will close the issue. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35074, "title": "InvalidArgumentError: indices[6] = 3712 is not in [0, 3592)", "body": "\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n\r\nInvalidArgumentError: indices[6] = 3712 is not in [0, 3592)", "comments": ["this all messages:\r\n\r\nrunfile('C:/Users/Allen/7-15  train.py', wdir='C:/Users/Allen')\r\nReloaded modules: 7-14  MKR, 7-16  data_loader\r\nreading rating file ...\r\nsplitting dataset ...\r\nreading KG file ...\r\ndata loaded.\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-22-55c5f394f6c2>\", line 1, in <module>\r\n    runfile('C:/Users/Allen/7-15  train.py', wdir='C:/Users/Allen')\r\n\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 827, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/Allen/7-15  train.py\", line 149, in <module>\r\n    train(args, data, show_loss, show_topk) #\u8fdb\u884c\u8bad\u7ec3\u53ca\u8bc4\u4f30\r\n\r\n  File \"C:/Users/Allen/7-15  train.py\", line 33, in train\r\n    _, loss = model.train_rs(sess, get_feed_dict_for_rs(model, train_data, start, start + args.batch_size))\r\n\r\n  File \"C:\\Users\\Allen\\Desktop\\TensorFlow_Engineering_Implementation-master\\code\\7-14  MKR.py\", line 154, in train_rs\r\n    return sess.run([self.optimizer_rs, self.loss_rs], feed_dict)\r\n\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 956, in run\r\n    run_metadata_ptr)\r\n\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1180, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1359, in _do_run\r\n    run_metadata)\r\n\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n\r\nInvalidArgumentError: indices[6] = 3712 is not in [0, 3592)\r\n\t [[node embedding_lookup_1 (defined at C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\r\n\r\nOriginal stack trace for 'embedding_lookup_1':\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\spyder_kernels\\console\\__main__.py\", line 11, in <module>\r\n    start.main()\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\spyder_kernels\\console\\start.py\", line 318, in main\r\n    kernel.start()\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\r\n    self.io_loop.start()\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\asyncio\\base_events.py\", line 539, in run_forever\r\n    self._run_once()\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1775, in _run_once\r\n    handle._run()\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\r\n    self._context.run(self._callback, *self._args)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\r\n    lambda f: self._run_callback(functools.partial(callback, future))\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\r\n    ret = callback()\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\r\n    self.run()\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\r\n    yielded = self.gen.send(value)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2854, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2880, in _run_cell\r\n    return runner(coro)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3057, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3248, in run_ast_nodes\r\n    if (await self.run_code(code, result,  async_=asy)):\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-22-55c5f394f6c2>\", line 1, in <module>\r\n    runfile('C:/Users/Allen/7-15  train.py', wdir='C:/Users/Allen')\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 827, in runfile\r\n    execfile(filename, namespace)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n  File \"C:/Users/Allen/7-15  train.py\", line 149, in <module>\r\n    train(args, data, show_loss, show_topk) #\u8fdb\u884c\u8bad\u7ec3\u53ca\u8bc4\u4f30\r\n  File \"C:/Users/Allen/7-15  train.py\", line 14, in train\r\n    model = MKR(args, n_user, n_item, n_entity, n_relation)\r\n  File \"C:\\Users\\Allen\\Desktop\\TensorFlow_Engineering_Implementation-master\\code\\7-14  MKR.py\", line 39, in __init__\r\n    self._build_low_layers(args)\r\n  File \"C:\\Users\\Allen\\Desktop\\TensorFlow_Engineering_Implementation-master\\code\\7-14  MKR.py\", line 74, in _build_low_layers\r\n    self.item_embeddings = tf.nn.embedding_lookup(self.item_emb_matrix, self.item_indices)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\embedding_ops.py\", line 364, in embedding_lookup_v2\r\n    return embedding_lookup(params, ids, \"div\", name, max_norm=max_norm)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\embedding_ops.py\", line 317, in embedding_lookup\r\n    transform_fn=None)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\embedding_ops.py\", line 135, in _embedding_lookup_and_transform\r\n    array_ops.gather(params[0], ids, name=name), ids, max_norm)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 3967, in gather\r\n    return gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\", line 4081, in gather_v2\r\n    batch_dims=batch_dims, name=name)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 793, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3360, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3429, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Allen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1751, in __init__\r\n    self._traceback = tf_stack.extract_stack()", "Please fill in issue template. Please use markdown formatting around the error/code blocks so that they are readable. Please provide a simple code that reproduces the error", "@allen-DM \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35073, "title": "`tf.vectorized_map` works incorrectly for functions with `tf.stack`", "body": "Description of the bug is pretty much in the title, here is the little snippet:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\ndef log_prob(x):\r\n    x = 0 # we don't care about the input tensor\r\n    final_log_probs = [tf.ones(1)]\r\n    concat_log_probs = tf.stack(final_log_probs, 0)\r\n    return concat_log_probs\r\n\r\nprint(log_prob(tf.ones(1)).shape) # (1,1)\r\n\r\n# vectorized function\r\nlog_prob = tf.vectorized_map(log_prob, tf.ones(1))\r\nprint(log_prob.shape) # (1,1,1)\r\n```\r\nThe returning shapes are different for two types of calls. Maybe the behaviour is documented (limit on `tf.stack`) but it seems to be odd for me.\r\n\r\n```\r\ntf: 2.0.0\r\ntfp: 0.8.0\r\n```", "comments": ["I guess the problem is with `is_stacked` flag and the appropriate logic in `*/pfor.py`?", "This looks like the expected behavior. FWIW, replacing vectorized_map with map_fn also gives the same shape.", "Still it is not documented so it makes it unexpected. I believe that this issue is causing some problems in tfp. @agarwal-ashish can you please give more constructive answer why this behaviour should be expected?", "As per documentation, the function returns \" A tensor or (possibly nested) sequence of tensors. Each tensor packs the\r\n    results of applying fn to tensors unpacked from elems along the first\r\n    dimension, from first to last.\"\r\n\r\nThe idea is that log_prob is applied to each row of your input and the results are stacked together. Since log_prob returns a tensor of rank 2, stacking these results should give rank 3. This has nothing to do with the usage of tf.stack inside your log_prob function or the fact that log_prob's output is independent of the inputs.\r\n\r\nDoes that help ? \r\n", "@agarwal-ashish thanks, yeah, it is much more clear now."]}, {"number": 35072, "title": "Explain how int8 input and output quantization conversion works in TensorFlow Lite", "body": "We've had feedback from multiple developers that it's hard to figure out how to calculate the right  int8 values for quantized inputs, and understand what int8 values mean as outputs.\r\n\r\nFor example, when feeding an image to uint8 quantized inputs, the values can be left as in their source 0 to 255 range. For int8 inputs, the developer will typically need to subtract 128 from each value, but this knowledge (and how the offset value is calculated) is not documented. In the same way, users will need to map the -128 to 127 output values to the actual real number range of their outputs, but this process is unclear.\r\n\r\nTagging the @tensorflow/micro team.", "comments": ["Made clarification. Created Pull Request #36271 ", "@petewarden \r\nPlease let us know if we can move this to closed status as the pull request is in merged status.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Moving this to close status as the pr is merged."]}, {"number": 35071, "title": "TensorFlow Lite Micro MaxPool kernel needs int8 support", "body": "TensorFlow Lite for Microcontrollers has a MAXPOOL operation, but it only supports float and uint8 execution, not int8.", "comments": ["Tagging @tensorflow/micro ", "Hi @petewarden I will start working on it, is that ok for you guys?", "Issue resolved. It should be safe to close this :)", "Hi @petewarden ! I think MaxPool Kernel from TFLite Micro does support int8 now. Can we move this issue to closed status?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 35070, "title": "Calculate arena size for TensorFlow Lite Micro models", "body": "\r\n\r\nTensorFlow Lite for Microcontrollers doesn't depend on dynamic memory allocation, so it requires users to supply a memory arena when an interpreter is created, as described in this documentation:\r\nhttps://www.tensorflow.org/lite/microcontrollers/get_started\r\nWe need a better way to decide 'tensor_arena_size'. Currently, the above page says 'The size required will depend on the model you are using, and may need to be determined by experimentation.'\r\n\r\nThe simplest solution might be an offline script that returns the size needed to hold a model's activation buffers, but this won't include miscellaneous or platform-dependent allocations in its total. Another approach might be to return the size needed as an integer along with the error report string from the interpreter creation.", "comments": ["Tagging @tensorflow/micro ", "> Tagging @tensorflow/micro\r\n\r\nthis link is not available ", "For anyone else looking for a solution: hacked up https://github.com/edgeimpulse/tflite-find-arena-size which can be called from Node.js (or native) and calculates the arena size. The WebAssembly version seems to approximate the required memory on Cortex-M quite well.", "This is a real pain, I have an Acute Lymphoblastic Leukemia model loaded on a Nano 33 BLE and get the following, \"Arena size is too small for all buffers. Needed 2497920 but only 209248 was available.\" unfortunately incrementing the arena size any further leads to the board becoming unresponsive, it uses 97% of dynamic memory and if I push arena size any further have to double tap reset to bring the board back to life. I saw a notebook that showed additional steps to help shrink model size (my model is 702632, more than double of person detector) but all Tensorflow notebooks and documentation links are dead within the space of the day. ", "They have moved here by the way: https://github.com/tensorflow/tflite-micro but none of the old notebooks are available any more. All links from Tensorflow website are broken, not good for people who may be considering using this tech lol.", "FYI TFLite micro has an API to show the exact arena size after initialization, just in case it's helpful for you.\r\n\r\nhttps://github.com/tensorflow/tflite-micro/blob/2791785562d660f328c54055bb4ab2df56a37f4a/tensorflow/lite/micro/micro_interpreter.h#L127", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 35069, "title": "[r2.1 Cherrypick] Edit release notes", "body": "", "comments": []}, {"number": 35068, "title": "Unable to use optimisation when modifying gradients returned by tf.gradienttape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux AMI 2018.03\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla K80\r\n\r\nMy network has multiple outputs and I wanted to be able to use separate loss functions on each output branch.\r\n\r\nInitially, I had it setup like this:\r\n\r\n```\r\n@tf.function\r\ndef train_step(inputs, labels):\r\n    with tf.GradientTape(persistent=True) as tape:\r\n        predictions = model([X, F], training=True)\r\n        losses = [l_f(tf.expand_dims(labels[:,i], axis=-1), predictions[i]) for i, l_f in enumerate(loss_functions)]\r\n    gradients = [tape.gradient(l, model.trainable_variables) for l in losses]\r\n    for g in gradients:\r\n        grads = [gg if gg is not None else tf.zeros_like(model.trainable_variables[i], dtype=tf.float32) for i, gg in enumerate(g)]\r\n        optimizer.apply_gradients(zip(grads, model.trainable_variables)\r\n    del tape\r\n    return losses\r\n\r\n\r\ndef weighted_loss(weights):\r\n    @tf.function\r\n    def loss_func(labels, predictions):\r\n        min_class_filter = tfk.backend.greater(labels, 0.5)\r\n\r\n        y_min = tf.boolean_mask(labels, min_class_filter)\r\n        y_max = tf.boolean_mask(labels, tf.math.logical_not(min_class_filter))\r\n        y_pred_min = tf.boolean_mask(predictions, min_class_filter)\r\n        y_pred_max = tf.boolean_mask(predictions, tf.math.logical_not(min_class_filter))\r\n\r\n        loss_min_class = tfk.backend.mean(tfk.backend.binary_crossentropy(y_min, y_pred_min))\r\n        loss_max_class = tfk.backend.mean(tfk.backend.binary_crossentropy(y_max, y_pred_max))\r\n        loss_all = tfk.backend.mean(tfk.backend.binary_crossentropy(labels, predictions))\r\n        return weights[0]*loss_min_class + weights[1]*loss_max_class + weights[2]*loss_all\r\n    return loss_func\r\n\r\nloss_functions = [weighted_loss(w) for w in target_weights]\r\n```\r\n\r\nThe loss functions were giving None gradients for the branch of the prediction that was not included in that particular loss, so I replaced those None gradients with zeros_like.\r\n\r\nWhen I run this as written, it takes an extremely long time (10min+) to run a single training step, and I see the following message in the logs:\r\n\r\n\r\n`E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_operator failed: Invalid argument: Input 0 of node model/LSTM_forward_0/zeros_like was passed int32 from model/LSTM_forward_0/StatefulPartitioned Call:9 incompatible with expected variant.\r\n`\r\n\r\nWhen I remove the @tf.function decorator, it runs in about 10% of the time, and I do not see this log warning, but GPU utilisation is still 0%, and it still runs more slowly than I would expect (i.e. much slower than (number of output branches)*(time for updating just one branch))\r\n\r\nThe way that I had to resolve this, was to stop overwriting the None gradients, which I was able to do like this:\r\n\r\n```\r\n@tf.function\r\ndef train_step(inputs, labels):\r\n    with tf.GradientTape(persistent=True) as tape:\r\n    predictions = model([X, F], training=True)\r\n    losses = [l_f(labels, predictions, i) for i, l_f in enumerate(loss_functions)]\r\n    gradients = [tape.gradient(l, model.trainable_variables) for l in losses]\r\n    for g in gradients:\r\n        optimizer.apply_gradients(zip(g, model.trainable_variables)\r\n    del tape\r\n    return losses\r\n\r\n\r\ndef weighted_loss(weights):\r\n    @tf.function\r\n    def loss_func(labs, preds, i):\r\n        labels = tf.expand_dims(labs[:,i], axis=-1)\r\n        predictions = preds[i]\r\n        min_class_filter = tfk.backend.greater(labels, 0.5)\r\n\r\n        y_min = tf.boolean_mask(labels, min_class_filter)\r\n        y_max = tf.boolean_mask(labels, tf.math.logical_not(min_class_filter))\r\n        y_pred_min = tf.boolean_mask(predictions, min_class_filter)\r\n        y_pred_max = tf.boolean_mask(predictions, tf.math.logical_not(min_class_filter))\r\n\r\n        loss_min_class = tfk.backend.mean(tfk.backend.binary_crossentropy(y_min, y_pred_min))\r\n        loss_max_class = tfk.backend.mean(tfk.backend.binary_crossentropy(y_max, y_pred_max))\r\n        loss_all = tfk.backend.mean(tfk.backend.binary_crossentropy(labels, predictions))\r\n        return weights[0]*loss_min_class + weights[1]*loss_max_class + weights[2]*loss_all\r\n    return loss_func\r\n\r\nloss_functions = [weighted_loss(w) for w in target_weights]\r\n```\r\nThis is fine, and now training at the expected rate, however I can't see why one shouldn't be able to modify the gradients in this way? The zero gradients that I was passing had the correct dtype, which isn't reflected in the warning message that says it was receiving ints, and that was the reason it was failing to optimise.\r\n", "comments": ["@gkennos Please post this question in stack overflow as this question is not related to bug/performance, build/install, feature request or docs related issue. Thanks!"]}, {"number": 35067, "title": "RaggedTensor Hierarchical Model Issue", "body": "I have the following model:\r\n\r\n```\r\n   import numpy as np\r\n\r\n   def example():\r\n\r\n        # Encode each timestep\r\n        in_sentence = Input(shape=(None,),  dtype='int64', name=\"Input1\")\r\n        embedded_sentence = Embedding(1000, 300, trainable=False)(in_sentence)\r\n        lstm_sentence = LSTM(300)(embedded_sentence)\r\n        encoded_model = Model(in_sentence, lstm_sentence)\r\n\r\n        section_input = Input(shape=(None, None), dtype='int64', name=\"Input2\")\r\n        section_encoded = TimeDistributed(encoded_model)(section_input)\r\n        section_encoded = LSTM(300)(section_encoded)\r\n        section_model = Model(section_input, section_encoded)\r\n\r\n        document_input = Input(shape=(None, None, None), dtype='int64', name=\"Input3\")\r\n        document_encoded = TimeDistributed(section_model)(document_input)\r\n        document_encoded = LSTM(300, return_sequences=True)(document_encoded)\r\n        document_encoded = Dense(1)(document_encoded)\r\n        document_model = Model(document_input, document_encoded)\r\n        document_model.compile(loss='categorical_crossentropy',\r\n                      optimizer='rmsprop',\r\n                      metrics=['accuracy'])\r\n\r\n        print(section_model.summary())\r\n        print(document_model.summary())\r\n        return document_model\r\n```\r\nAnd I call it:\r\n\r\n```\r\n\r\ny = []\r\nfor doc in documents:\r\n    tocs = []\r\n    for token in doc:\r\n        tocs.append(np.random.random_integers(0, high=1))\r\n        y.append(tocs)\r\n\r\noutput = tf.ragged.constant(y)\r\n\r\ninput = tf.ragged.constant([[[[4, 5, 2, 6]], [[10, 12, 9], [26, 20, 21, 22], [35, 34, 31]]]])\r\n\r\nexample().fit(input, output, use_multiprocessing=True, epochs=10, batch_size=1)\r\n```\r\nI get the following error:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 318, in assert_same_structure\r\n    expand_composites)\r\nValueError: The two structures don't have the same nested structure.\r\n\r\nFirst structure: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([5, None, None, None]), tf.int32, 3, tf.int64)\r\n\r\nSecond structure: type=Tensor str=Tensor(\"Input3:0\", shape=(None, None, None, None), dtype=int64)\r\n\r\nMore specifically: Substructure \"type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([5, None, None, None]), tf.int32, 3, tf.int64)\" is a sequence, while substructure \"type=Tensor str=Tensor(\"Input3:0\", shape=(None, None, None, None), dtype=int64)\" is not\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:/Development/competitions/Kaggle/Tensorflow 2.0 - Q_a_A/tensorflow_2.0/QuestionAnswerContextAttention.py\", line 392, in <module>\r\n    qa_model.fit(input=documents, output=output)\r\n  File \"D:/Development/competitions/Kaggle/Tensorflow 2.0 - Q_a_A/tensorflow_2.0/QuestionAnswerContextAttention.py\", line 163, in fit\r\n    self.example().fit(input, output, use_multiprocessing=True, epochs=10, batch_size=1)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 224, in fit\r\n    distribution_strategy=strategy)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 547, in _process_training_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 594, in _process_inputs\r\n    steps=steps)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2497, in _standardize_user_data\r\n    nest.assert_same_structure(a, b, expand_composites=True)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\", line 325, in assert_same_structure\r\n    % (str(e), str1, str2))\r\nValueError: The two structures don't have the same nested structure.\r\n\r\nFirst structure: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([5, None, None, None]), tf.int32, 3, tf.int64)\r\n\r\nSecond structure: type=Tensor str=Tensor(\"Input3:0\", shape=(None, None, None, None), dtype=int64)\r\n\r\nMore specifically: Substructure \"type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([5, None, None, None]), tf.int32, 3, tf.int64)\" is a sequence, while substructure \"type=Tensor str=Tensor(\"Input3:0\", shape=(None, None, None, None), dtype=int64)\" is not\r\nEntire first structure:\r\n.\r\nEntire second structure:\r\n.\r\n\r\nProcess finished with exit code 1\r\n\r\n\r\n<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@nectario, Please provide the Tensorflow version and complete standalone code to reproduce the reported issue. Thanks!", "Hi @gadagashwini , \r\n\r\nI have attached a zip file that contains the code and data that recreates the problem. Just unzip the file and type:\r\n\r\npython RaggedProblem.py\r\n\r\n[ragged_problem.zip](https://github.com/tensorflow/tensorflow/files/3969504/ragged_problem.zip)\r\n\r\nThanks!\r\n\r\nNektarios", "I could replicate the issue with Tf 1.15.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/3bcb5544395d4609e3ce596ecb473d8e/untitled317.ipynb). Thanks!", "Thank you! For me this happened with TensorFlow 2.0 also.", "@nectario Can you please take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/27170) and let me know if it helps. Thanks!", "I tried specifing ragged = True on all Input layers, but now I get a different issue:\r\n\r\nTypeError: Failed to convert object of type <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. Contents: tf.RaggedTensor(values=Tensor(\"Input1/flat_values:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"Input1/row_splits_0:0\", shape=(None,), dtype=int64)). Consider casting elements to a supported type.\r\n", "@nectario This is very similar to the issue #27170. Lets track the issue there and close it here. Please let me know if you think otherwise. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35067\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35067\">No</a>\n", "@nectario did you manage to resolve the issue after all? \r\nI am having exactly the same problem...\r\nRagged tensor support still seems to be lacking... \r\nI do not want to have to clip and pad my sentences/words to represent documents. ", "> @nectario did you manage to resolve the issue after all?\r\n> I am having exactly the same problem...\r\n> Ragged tensor support still seems to be lacking...\r\n> I do not want to have to clip and pad my sentences/words to represent documents.\r\n@Jordy-VL Never resolved this issued. I actually gave up on ragged tensors and padded my inputs. I am hoping for better support for ragged tensors in Keras.\r\n", "Thanks for the heads-up @nectario \r\nQuite sad to still wait for this support as the concept behind ragged tensors is beautiful. \r\nIt deserves to be embedded deeper in the Tensorflow toolbox.\r\n\r\nFor anyone wanting to pad and clip ragged tensors I have a handy helper function:\r\n\r\n```\r\ndef hierarchical_tensorize(variable_length_sequences, max_sentences=None, max_document_len=None):\r\n    '''\r\n    since support for ragged tensors is still incipient; we need to convert ragged to full tensors \r\n    Args:\r\n        variable_length_sequences (list): 2D variable length \"sentences x words\" elements\r\n        max_sentences (int, optional): \r\n        max_document_len (int, optional): IS abused for max words per sentence\r\n\r\n        -> .to_tensor()[:,:max_sentences,:max_document_len]\r\n    if using ragged tensor -> can only use batch without padding\r\n\r\n    Returns:\r\n        3D tensor\r\n    '''\r\n    docs = tf.ragged.constant(variable_length_sequences)\r\n    docs = docs.to_tensor()\r\n    if max_sentences:\r\n        docs = docs[:, :max_sentences, :]\r\n    if max_document_len:\r\n        docs = docs[:, :, :max_document_len]\r\n    return docs\r\n```\r\n\r\n"]}, {"number": 35066, "title": "[r2.1 Cherrypick] Unify V1/2 layer naming in internal imports", "body": "This fixes V2 behavior issues in places where we import `keras.layers` internally (e.g. in Keras Applications).", "comments": []}, {"number": 35065, "title": "Bug in documentation of tf.while_loop.  parallel_iterations doesn't seem to affect performance.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes and no.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or binary): I used `pip install tensorflow-gpu==2.0.0`\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 1080 TI and 11170 MiB\r\n\r\n**Describe the current behavior**\r\nFirst as discussed in this [issue](https://github.com/tensorflow/tensorflow/issues/18257). There is a bug in the first example of the documentation of `tf.while_loop`. \r\n\r\nThen, the parallel_iterations argument doesn't seem to parallelize the loop. There is no difference between the run time with `parallel_iterations = 1` or `parallel_iterations = 10`. \r\nI have a question opened on [Stackoverflow](https://stackoverflow.com/questions/59299060/tf-2-0-while-loop-and-parallel-iterations) .\r\n\r\n**Describe the expected behavior**\r\nIf the function in iteration n doesn't depend on previous iterations, then I expect that by setting `parallel_iterations = 10,` the loop should finish about 10 times faster than setting `parallel_iterations = 1`\r\n**Code to reproduce the issue**\r\nThe code is posted on the [Stackoverflow](https://stackoverflow.com/questions/59299060/tf-2-0-while-loop-and-parallel-iterations). \r\n", "comments": ["@charles19920528 \r\n\r\nI have tried in colab with TF 2.0. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/d7958779064096e958bbcf734e8df670/untitled486.ipynb). Is this the expected behavior?", "@ravikyram Not really. I was hoping that by just setting `parallel_iterations=10`, the `print_fun` will run in parallel. I don't think this [code](https://colab.sandbox.google.com/gist/ravikyram/d7958779064096e958bbcf734e8df670/untitled486.ipynb) is running in parallel. \r\n\r\nI sort of got my answer on the [StackOverflow](https://stackoverflow.com/questions/59299060/tf-2-0-while-loop-and-parallel-iterations). As Srihari Humbarwadi pointed out, the `parallel_iterations` argument in `tf.while_loop` doesn't mean anything in the eager mode. So in order to run the loop in parallel, I need to use the graph mode with `@tf.function`. The documentation of `tf.while_loop` , as far as I can tell, doesn't explicitly mention this important point . So perhaps someone can improve the documentation.", "@charles19920528 \r\nIs this still an issue", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Side note: a bug in tf.while_loop itself caused tf.while_loop to run sequentially, even in graph mode. Eager tf.while_loop is still identical to running a Python while."]}, {"number": 35064, "title": "tensorflow 1.14 not picking up GPU: CUBLAS_STATUS_INTERNAL_ERROR", "body": "I am having problems running tensorflow on my GPU.\r\n\r\nMy environment is as follows:\r\n\r\n* OS: Linux Mint\r\n* GPU: GeForce RTX 2070 Super\r\n* Nvidia Driver Version: 435.21\r\n* CUDA Version: 10.1\r\n* gcc --version: gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n* tensorflow-gpu 1.14\r\n* keras 2.3.1\r\n\r\nBoth the keras and tensorflow packages are located in a conda environment with no other tensorflow or keras versions to avoid package conflict. First, I get conflicting GPU devices when I run the following quick test:\r\n\r\n```\r\n# picks up the GPU it seems\r\nfrom tensorflow.python.client import device_lib\r\nprint(device_lib.list_local_devices())\r\n>>> [name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 4346857393168915334\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 15716071101553989809\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n, name: \"/device:XLA_GPU:0\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 6257014534476475142\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n]\r\n\r\n# but then Keras doesn't pick up the GPU?\r\nfrom keras import backend as K\r\nK.tensorflow_backend._get_available_gpus()\r\n>>> []\r\n```\r\nThis doesn't make much sense as I thought keras uses tensorflow as the backend.\r\n\r\nThen I tried running the below program (I got some hints from these two SO posts [[1]](https://datascience.stackexchange.com/a/41958/41929)[[2]](https://stackoverflow.com/a/52132342/4139143))\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom datetime import datetime\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n\r\n# Choose which device you want to test on: either 'cpu' or 'gpu'\r\ndevices = ['/device:CPU:0', '/device:XLA_GPU:0']\r\n\r\n# Choose size of the matrix to be used.\r\n# Make it bigger to see bigger benefits of parallel computation\r\nshapes = [(50, 50), (100, 100), (500, 500), (1000, 1000)]\r\n\r\n\r\ndef compute_operations(device, shape):\r\n    \"\"\"Run a simple set of operations on a matrix of given shape on given device\r\n\r\n    Parameters\r\n    ----------\r\n    device : the type of device to use, either 'cpu' or 'gpu'\r\n    shape : a tuple for the shape of a 2d tensor, e.g. (10, 10)\r\n\r\n    Returns\r\n    -------\r\n    out : results of the operations as the time taken\r\n    \"\"\"\r\n\r\n    # Define operations to be computed on selected device\r\n    with tf.device(device):\r\n        random_matrix = tf.random.uniform(shape=shape, minval=0, maxval=1)\r\n        dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))\r\n        sum_operation = tf.reduce_sum(dot_operation)\r\n\r\n    # Time the actual runtime of the operations\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    start_time = datetime.now()\r\n    with tf.compat.v1.Session(config=config) as session:\r\n            result = session.run(sum_operation)\r\n    elapsed_time = datetime.now() - start_time\r\n\r\n    return result, elapsed_time\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    # Run the computations and print summary of each run\r\n    for device in devices:\r\n        print(\"--\" * 20)\r\n\r\n        for shape in shapes:\r\n            _, time_taken = compute_operations(device, shape)\r\n\r\n            # Print the result and also the time taken on the selected device\r\n            print(\"Input shape:\", shape, \"using Device:\", device, \"took: {:.2f}\".format(time_taken.seconds + time_taken.microseconds/1e6))\r\n            #print(\"Computation on shape:\", shape, \"using Device:\", device, \"took:\")\r\n```\r\n\r\nBut got multiple errors\r\n```\r\n2019-12-12 18:35:05.957991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.8\r\npciBusID: 0000:42:00.0\r\n...\r\n...\r\n2019-12-12 18:35:06.655452: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n2019-12-12 18:35:06.655474: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR\r\n2019-12-12 18:35:06.655483: F tensorflow/compiler/xla/service/gpu/gemm_thunk.cc:176] Check failed: stream->parent()->GetBlasGemmAlgorithms(&algorithms) \r\nAborted (core dumped)\r\n```\r\n\r\nAm I doing something incorrect here? TF was certainly working with my GPU (the program above worked) just a few weeks ago. It seems every time a new TF or keras version is released, I install it and get GPU issues. Any help / advice is much appreciated. Thank you\r\n\r\n", "comments": ["TF 1.14 pre built binary supports cuda 10.0 where as your configuration shows cuda 10.1\r\n` Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory`\r\nPlease roll back to cuda 10.0 \r\nSee https://www.tensorflow.org/install/gpu#software_requirements", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35064\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35064\">No</a>\n", "@ymodak apologies for the delay in replying. Thank you for clarifying, that has solved my issue"]}, {"number": 35062, "title": " [1.15] Build from source fails on Raspberry Pi", "body": "**System information**\r\n- OS Platform and Distribution: Archlinux\r\n- Mobile device: Raspberry Pi\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.15\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: the docker image you provide.\r\n- Bazel version (if compiling from source): the one you put in that image.\r\n- GCC/Compiler version (if compiling from source): the one you put in the image\r\n- CUDA/cuDNN version: No cuda\r\n- GPU model and memory: The computer on which Docker runs has 16gb of ram.\r\n\r\nBuilds fails while compiling. Build log has been attached. Instructions actually mention that one should checkout a known working version but it does not specify where I could discover which version actually builds on raspberry.\r\n\r\nThe one listed at this [documentation page](https://www.tensorflow.org/install/source_rpi):\r\n\r\n    CI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4\" \\\r\n    tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 \\\r\n    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n\r\n\r\n**Any other info / logs**\r\n[tensorflow-1.15-docker-raspberry-armhf-build.log](https://github.com/tensorflow/tensorflow/files/3957401/tensorflow-1.15-docker-raspberry-armhf-build.log).\r\n\r\nActually it just failed on another point, too.\r\nLast part of the latter log:\r\n\r\n    peStorage; UniquerT = mlir::detail::TypeUniquer]'\r\n    external/local_config_mlir/lib/IR/Types.cpp:66:66:   required from here\r\n    external/local_config_mlir/include/mlir/Support/StorageUniquer.h:232:34: error: 'getKey' is not a member of 'mlir::detail::OpaqueTypeStorage'\r\n         return ImplTy::getKey(args...);\r\n                                      ^\r\n    external/local_config_mlir/include/mlir/Support/StorageUniquer.h: In instantiation of 'static typename std::enable_if<typename mlir::detail::detector<void, mlir::detail::has_impltype_hash_t, ImplTy, DerivedKey>::value_t:: value, llvm::hash_code>::type mlir::StorageUniquer::getHash(unsigned int, const DerivedKey&) [with ImplTy = mlir::detail::OpaqueTypeStorage; DerivedKey = std::pair<mlir::Identifier, llvm::StringRef>; typename std::enable_if<typename mlir::detail::detector<void, mlir::detail::has_impltype_hash_t, ImplTy, DerivedKey>::value_t:: value, llvm::hash_code>::type = llvm::hash_code]':\r\n    external/local_config_mlir/include/mlir/Support/StorageUniquer.h:141:59:   required from 'Storage* mlir::StorageUniquer::get(std::function<void(Storage*)>, unsigned int, Arg&&, Args&& ...) [with Storage = mlir::detail::OpaqueTypeStorage; Arg = mlir::Identifier&; Args = {llvm::StringRef&}]'\r\n    external/local_config_mlir/include/mlir/IR/TypeSupport.h:104:42:   required from 'static T mlir::detail::TypeUniquer::get(mlir::MLIRContext*, unsigned int, Args&& ...) [with T = mlir::OpaqueType; Args = {mlir::Identifier&, llvm::StringRef&}]'\r\n    external/local_config_mlir/include/mlir/IR/StorageUniquerSupport.h:67:64:   required from 'static ConcreteT mlir::detail::StorageUserBase<ConcreteT, BaseT, StorageT, UniquerT>::get(mlir::MLIRContext*, unsigned int, Args ...) [with Args = {mlir::Identifier, llvm::StringRef}; ConcreteT = mlir::OpaqueType; BaseT = mlir::Type; StorageT = mlir::detail::OpaqueTypeStorage; UniquerT = mlir::detail::TypeUniquer]'\r\n    external/local_config_mlir/lib/IR/Types.cpp:66:66:   required from here\r\n    external/local_config_mlir/include/mlir/Support/StorageUniquer.h:255:63: error: 'hashKey' is not a member of 'mlir::detail::OpaqueTypeStorage'\r\n         return llvm::hash_combine(kind, ImplTy::hashKey(derivedKey));\r\n\r\n**EDIT 19/12/17:** As a temporary workaround I installed [PINTO0309](https://github.com/PINTO0309/Tensorflow-bin)'s binaries.", "comments": ["Just hit this same issue building PI-PYTHON3, PI_ONE on a Ubuntu 18.04 host build machine.", "I believe this issue has to do with the older cross compiler in the 1.15 Pi build (v4.9) which doesn't support all C++14 extensions.  I've worked around the issue by cherry-picking [commit 20dfc83d](https://github.com/tensorflow/tensorflow/commit/20dfc83d59d591339ba4be06230a7350543cfe40#diff-00a21b168974d2d842d84a57987129f4) and then building:\r\n```bash\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow/\r\ngit checkout v1.15.0\r\ngit cherry-pick 20dfc83d\r\n```", "Thanks, @dougyfresh42 \r\nI've created PR for the cherry-pick.\r\nhttps://github.com/tensorflow/tensorflow/pull/39441", "https://github.com/tensorflow/tensorflow/pull/39441 was merged into r1.15 branch.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35062\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35062\">No</a>\n"]}, {"number": 35061, "title": "1.15.0 disappeared from pypi", "body": "Hi All,\r\n\r\nrelated ticket :https://github.com/RasaHQ/rasa/issues/4965\r\n\r\nI just noticed that this doesn't work anymore:\r\n\r\n```\r\n# pip3 install tensorflow~=1.15.0   \r\n\r\nCollecting tensorflow~=1.15.0\r\n  Could not find a version that satisfies the requirement tensorflow~=1.15.0 (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc0, 1.1.0rc1, 1.1.0rc2, 1.1.0, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0, 1.4.1, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.8.0rc0, 1.8.0rc1, 1.8.0, 1.9.0rc0, 1.9.0rc1, 1.9.0rc2, 1.9.0, 1.10.0rc0, 1.10.0rc1, 1.10.0, 1.10.1, 1.11.0rc0, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.12.0rc0, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.2, 1.12.3, 1.13.0rc0, 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\r\nNo matching distribution found for tensorflow~=1.15.0\r\n```\r\n\r\nbut when you browse pypi: https://pypi.org/project/tensorflow/#history , 1.15.0 versions are there, could you see what's going on pls ? ", "comments": ["You need to have an updated pip.", "Yes, @sokoow Please try upgrading your pip using this command\r\n`sudo -H pip3 install --upgrade pip`\r\n\r\nas the error is caused due to using the older version of pip. Thanks!", "that was it, thanks"]}, {"number": 35060, "title": "Increasing predict time every iteration of loop", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 12.04\r\n- TensorFlow version (use command below): conda install tensorflow-gpu=1.14.0 \r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla k80\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nmodel predict time increases for every tensor. \r\n\r\nUsing a resnet 32 layer model. No modifications except for input shape.\r\n\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\n--- 2.172 seconds ---\r\n1\r\n--- 0.3469 seconds ---\r\n1\r\n--- 0.3441 seconds ---\r\n1\r\n--- 0.3465 seconds ---\r\n1\r\n--- 0.3543 seconds ---\r\n1\r\n--- 0.3583 seconds ---\r\n1\r\n--- 0.3638 seconds ---\r\n1\r\n--- 0.3676 seconds ---\r\n1\r\n--- 0.3727 seconds ---\r\n1\r\n--- 0.3779 seconds ---\r\n1\r\n--- 0.3838 seconds ---\r\n1\r\n--- 0.3918 seconds ---\r\n0\r\n--- 0.3958 seconds ---\r\n0\r\n--- 0.4069 seconds ---\r\n0\r\n--- 0.4084 seconds ---\r\n0\r\n--- 0.4144 seconds ---\r\n0\r\n--- 0.4143 seconds ---\r\n0\r\n--- 0.4173 seconds ---\r\n0\r\n--- 0.4180 seconds ---\r\n0\r\n--- 0.4244 seconds ---\r\n0\r\n--- 0.4355 seconds ---\r\n0\r\n--- 0.4313 seconds ---\r\n0\r\n--- 0.4429 seconds ---\r\n0\r\n--- 0.4446 seconds ---\r\n0\r\n--- 0.4435 seconds ---\r\n0\r\n--- 0.4547 seconds ---\r\n0\r\n--- 0.4513 seconds ---\r\n0\r\n--- 0.4641 seconds ---\r\n0\r\n--- 0.4697 seconds ---\r\n0\r\n--- 0.4717 seconds ---\r\n0\r\n--- 0.4737 seconds ---\r\n0\r\n--- 0.4794 seconds ---\r\n0\r\n--- 0.4816 seconds ---\r\n0\r\n--- 0.4845 seconds ---\r\n0\r\n--- 0.4906 seconds ---\r\n0\r\n--- 0.4923 seconds ---\r\n0\r\n--- 0.5027 seconds ---\r\n0\r\n--- 0.5028 seconds ---\r\n0\r\n--- 0.5088 seconds ---\r\n0\r\n--- 0.5185 seconds ---\r\n0\r\n--- 0.5173 seconds ---\r\n0\r\n--- 0.5223 seconds ---\r\n0\r\n--- 0.5292 seconds ---\r\n0\r\n--- 0.5280 seconds ---\r\n0\r\n--- 0.5391 seconds ---\r\n0\r\n--- 0.5429 seconds ---\r\n0\r\n--- 0.5413 seconds ---\r\n0\r\n--- 0.5503 seconds ---\r\n0\r\n--- 0.5559 seconds ---\r\n1\r\n--- 0.5666 seconds ---\r\n1\r\n--- 0.5622 seconds ---\r\n1\r\n--- 0.5678 seconds ---\r\n0\r\n--- 0.5780 seconds ---\r\n0\r\n--- 0.5866 seconds ---\r\n1\r\n--- 0.5863 seconds ---\r\n\r\n\r\n**Describe the expected behavior**\r\nconstant predict times\r\n\r\n** code **\r\n```\r\nclass PredictPerFeat(object):\r\n   def __init__(self, model, params):\r\n\r\n        self.model = tf.keras.models.load_model(model, compile=False)\r\n\r\n   def predict(feats):\r\n        start_time = time.time()\r\n        out = self.model.predict_on_batch(feats)\r\n        \r\n        print(\"--- %s seconds ---\" % (time.time() - start_time))\r\n        out = int(out > 0.5) # outputs label\r\n\r\n        return out\r\n\r\ndef main(args):\r\n      predperfeat = PredictPerFeat(args.model_path)\r\n      feats = loadfeats(args.feat_path)   \r\n\r\n      for i in range(len(feats)):\r\n         # each feature of shape 1, 25, 40, 1\r\n         # also tested for random input \r\n         # f = tf.convert_to_tensor(np.random.rand(1,25,40,1))\r\n         f = feats[i]\r\n         pred = predperfeat.predict(feats[i])\r\n         print(prediction)\r\n```\r\n    \r\n     \r\n\r\n\r\n", "comments": ["Could you please provide a complete standalone code. Looks like model_path and feat_path are not defined. \r\n\r\nTried to reproduce this code but didn't get any output. Please find the [Gist](https://colab.sandbox.google.com/gist/amahendrakar/a0969c270e6d2d0fe6eeccc5c0a8695f/35060.ipynb) here. Thanks!", "Any updates regarding this issue? Thanks!", "Apologies,\n\nWill get you a reproducible code by tomorrow / day after.\nI found a temporary work around.\nWill update soon.\n\n\nOn Sun, Dec 22, 2019 at 10:41 PM amahendrakar <notifications@github.com>\nwrote:\n\n> Any updates regarding this issue? Thanks!\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35060?email_source=notifications&email_token=ADSFDZAZMQTC5U3UQKGNNLDQ2BMPVA5CNFSM4J2BWT52YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHQLLVY#issuecomment-568374743>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ADSFDZGVX7UUH2ZQ6Q3Z6Y3Q2BMPVANCNFSM4J2BWT5Q>\n> .\n>\n", "@Dabuk,\r\nAny updates regarding the reproducible code? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35059, "title": "No matching distribution found for tensorflow", "body": "", "comments": []}, {"number": 35058, "title": "4D transpose convolutional layers", "body": "I've been looking for an implementation of a 4D conv layer in tensorflow and found a solution here:\r\nhttps://github.com/funkey/conv4d\r\nI was wondering if anyone could help me with the transpose version of this 4D conv layer. How would that look like in the implementation? It seems that in this case, frame_results is the output and is the result of summing the convolution of the current input frame with its previous kernel frame. The reverse would be the result of summing the 3D transpose layers? or there is more to it? any comments will be very appreciated, thanks.", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 35057, "title": "TF2.1 Unable to load model in h5-Format", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.11\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0-rc1 CPU\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI create, compile, fit and then save a tf.keras.Model the following way:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as layers\r\nimport tensorflow.keras.backend as K\r\n\r\n\r\ndef density_model(input_shape, n_clusters):\r\n\r\n    input_layer = layers.Input(shape=input_shape)\r\n\r\n    y = input_layer\r\n\r\n    y = layers.Conv1D(32, 10, 2, padding='same', kernel_initializer='he_uniform')(y)\r\n    y = layers.BatchNormalization()(y)\r\n    y = layers.Activation('relu')(y)\r\n\r\n    y = layers.Conv1D(32, 5, 2, padding='same', kernel_initializer='he_uniform')(y)\r\n    y = layers.BatchNormalization()(y)\r\n    y = layers.Activation('relu')(y)\r\n\r\n    y = layers.Conv1D(16, 3, 2, padding='same', kernel_initializer='he_uniform')(y)\r\n    y = layers.BatchNormalization()(y)\r\n    y = layers.Activation('relu')(y)\r\n\r\n    y = layers.Flatten()(y)\r\n\r\n    # mixture coefficients\r\n    alpha = layers.Dense(n_clusters, activation=None)(y)\r\n    alpha = layers.Softmax(name='coeff')(alpha)\r\n\r\n    # mean\r\n    mu = layers.Dense(n_clusters, activation=None, name='mu')(y)\r\n\r\n    # stddev\r\n    sigma = layers.Dense(n_clusters, activation=None)(y)\r\n    sigma = layers.Lambda(tf.keras.backend.exp, name='sigma')(sigma)\r\n\r\n    out = layers.Concatenate(axis=1)([alpha, mu, sigma])\r\n\r\n    model = tf.keras.models.Model(input_layer, out)\r\n\r\n    return model\r\n\r\n\r\nclass LogLikelihood(tf.keras.losses.Loss):\r\n    def __init__(self, n_clusters):\r\n        super(LogLikelihood, self).__init__()\r\n        self.n_clusters = n_clusters\r\n\r\n    def call(self, y_true, y_pred):\r\n\r\n        eps = 0.00001\r\n\r\n        alpha = K.expand_dims(y_pred[:, 0:self.n_clusters], axis=1)\r\n        mu = K.expand_dims(y_pred[:, self.n_clusters:2*self.n_clusters], axis=1)\r\n        sigma = K.expand_dims(y_pred[:, 2*self.n_clusters:3*self.n_clusters], axis=1)\r\n\r\n        y = K.expand_dims(y_true, axis=-1)\r\n\r\n        # 1/(2*pi)**0.5 = 0.398...\r\n        coeff = 0.4*alpha/sigma\r\n        exponent = -0.5*K.square((y-mu)/sigma)\r\n\r\n        density = K.sum(coeff*K.exp(exponent), axis=-1)\r\n        log_density = -K.log(density+eps)\r\n        log_likelihood = K.sum(log_density, axis=-1)\r\n\r\n        return K.mean(log_likelihood)\r\n\r\nif __name__ == '__main__':\r\n\r\n    train_data = np.random.normal(loc=0.0, scale=1.0, size=1000*100).reshape((1000, 100, 1))\r\n    train_target = np.random.normal(loc=1.0, scale=2.0, size=1000*100).reshape((1000, 100))\r\n\r\n    model = density_model(input_shape=(100, 1), n_clusters=2)\r\n\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n        loss=LogLikelihood(n_clusters=2)\r\n    )\r\n\r\n    model.fit(x=train_data, y=train_target, batch_size=32, epochs=1, verbose=0)\r\n\r\n    model.save('./density_h5', save_format='h5', include_optimizer=False)\r\n```\r\n\r\nAll works fine until here. Since I am using a custom Loss and I don't want to deal with the trouble of loading it, I am specifying 'include_optimizer=False' when saving. My model is pretty straightforward, apart from using a Lambda-Layer.\r\n\r\nHowever I cannot load the model. \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.load_model('./density_h5', custom_objects={'tf': tf}, compile=False)\r\n```\r\n\r\ngives the following error:\r\n\r\nTraceback (most recent call last):\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 842, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/core.py\", line 593, in call\r\n    outputs.set_shape(self.compute_output_shape(inputs.shape))\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/core.py\", line 601, in compute_output_shape\r\n    if all(input_shape[1:]):\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py\", line 765, in __bool__\r\n    self._disallow_bool_casting()\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py\", line 534, in _disallow_bool_casting\r\n    self._disallow_in_graph_mode(\"using a `tf.Tensor` as a Python `bool`\")\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py\", line 523, in _disallow_in_graph_mode\r\n    \" this function with @tf.function.\".format(task))\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/fischeru/.PyCharmCE2017.2/config/scratches/scratch_9.py\", line 3, in <module>\r\n    model = tf.keras.models.load_model('./density_h5', custom_objects={'tf': tf}, compile=False)\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/saving/save.py\", line 146, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 168, in load_model_from_hdf5\r\n    custom_objects=custom_objects)\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/saving/model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/serialization.py\", line 102, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\", line 191, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/network.py\", line 906, in from_config\r\n    config, custom_objects)\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1852, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1799, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"/data1/fischeru/MY_PYTHON_PROJECTS/quar/env/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 854, in __call__\r\n    str(e) + '\\n\"\"\"')\r\nTypeError: You are attempting to use Python control flow in a layer that was not declared to be dynamic. Pass `dynamic=True` to the class constructor.\r\nEncountered error:\r\n\"\"\"\r\nusing a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n\"\"\"\r\n\r\n**Describe the expected behavior**\r\nI expect to be able to load the model. As far as I know the \"custom_objects={'tf': tf}\" should take care of the Lambda-Layer. And since I explicitely did not include the optimizer, my custom loss should also not make any trouble. \r\n\r\n**Code to reproduce the issue**\r\nsee above\r\n\r\n**Other info / logs**\r\nsee anove\r\n", "comments": ["Sorry, found the problem. Seems that one must use tf.math.exp instead of tf.keras.backend.exp.\r\nNo clue why, but it solves the issue."]}, {"number": 35056, "title": "ValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.", "body": "Hello,\r\n\r\nI'm trying to implement a Unet segmentation model using Keras with Tensorflow backend. I created a Conda virual environement and installed Tensorflow 2.0.0 package via Conda package manager.\r\n![image](https://user-images.githubusercontent.com/20855725/70704814-9542c380-1cf4-11ea-8b80-df3bb9d48d38.png)\r\n \r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: - \r\n- TensorFlow installed from (source or binary): source, I guess\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.1 / 7.6.5.32\r\n- GPU model and memory: 1080Ti / 16 Gb\r\n\r\n**Describe the current behavior**\r\nLearning process crashes with a following error:\r\n`ValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.`\r\n**Describe the expected behavior**\r\nLearning process goes fine =).\r\n**Code to reproduce the issue**\r\nHere's a complete code:\r\n```\r\nimport numpy as np\r\nimport os\r\nimport cv2\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\r\nfrom tensorflow.python.keras.layers.core import Lambda, RepeatVector, Reshape\r\nfrom tensorflow.python.keras.layers.convolutional import Conv2D, Conv2DTranspose\r\nfrom tensorflow.python.keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\r\nfrom tensorflow.python.keras.layers.merge import concatenate, add\r\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\nfrom tensorflow.python.keras.optimizers import Adam\r\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\r\n\r\ndef data_gen(templates_folder, masks_folder, im_width, batch_size):\r\n    c = 0\r\n    n = os.listdir(templates_folder)  # List of training images\r\n    m = os.listdir(masks_folder)\r\n    # random.shuffle(n)\r\n\r\n    while (True):\r\n        img = np.zeros((batch_size, im_width, im_width, 1)).astype('float')\r\n        mask = np.zeros((batch_size, im_width, im_width, 1)).astype('float')\r\n\r\n        for i in range(c, c + batch_size):  # initially from 0 to 16, c = 0.\r\n\r\n            train_img = cv2.imread(templates_folder + '/' + n[i], cv2.IMREAD_GRAYSCALE) / 255.\r\n            train_img = train_img.reshape(im_width, im_width, 1)\r\n\r\n            img[i - c] = train_img  # add to array - img[0], img[1], and so on.\r\n            train_mask = cv2.imread(masks_folder + '/' + m[i], cv2.IMREAD_GRAYSCALE) / 255.\r\n            train_mask = train_mask.reshape(im_width, im_width, 1)  # Add extra dimension for parity with train_img size [512 * 512 * 3]\r\n\r\n            mask[i - c] = train_mask\r\n\r\n        c += batch_size\r\n        if (c + batch_size >= len(os.listdir(templates_folder))):\r\n            c = 0\r\n            # random.shuffle(n)\r\n        yield img, mask\r\n\r\ndef conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True):\r\n    # first layer\r\n    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\", padding=\"same\")(input_tensor)\r\n    if batchnorm:\r\n        x = BatchNormalization()(x)\r\n    x = Activation(\"relu\")(x)\r\n    # second layer\r\n    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\", padding=\"same\")(x)\r\n    if batchnorm:\r\n        x = BatchNormalization()(x)\r\n    x = Activation(\"relu\")(x)\r\n    return x\r\n\r\ndef get_unet(input_img, n_filters=16, dropout=0.5, batchnorm=True):\r\n    # contracting path\r\n    c1 = conv2d_block(input_img, n_filters=n_filters * 1, kernel_size=3, batchnorm=batchnorm)\r\n    p1 = MaxPooling2D((2, 2))(c1)\r\n    p1 = Dropout(dropout * 0.5)(p1)\r\n\r\n    c2 = conv2d_block(p1, n_filters=n_filters * 2, kernel_size=3, batchnorm=batchnorm)\r\n    p2 = MaxPooling2D((2, 2))(c2)\r\n    p2 = Dropout(dropout)(p2)\r\n\r\n    c3 = conv2d_block(p2, n_filters=n_filters * 4, kernel_size=3, batchnorm=batchnorm)\r\n    p3 = MaxPooling2D((2, 2))(c3)\r\n    p3 = Dropout(dropout)(p3)\r\n\r\n    c4 = conv2d_block(p3, n_filters=n_filters * 8, kernel_size=3, batchnorm=batchnorm)\r\n    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\r\n    p4 = Dropout(dropout)(p4)\r\n\r\n    c5 = conv2d_block(p4, n_filters=n_filters * 16, kernel_size=3, batchnorm=batchnorm)\r\n\r\n    # expansive path\r\n    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides=(2, 2), padding='same')(c5)\r\n    u6 = concatenate([u6, c4])\r\n    u6 = Dropout(dropout)(u6)\r\n    c6 = conv2d_block(u6, n_filters=n_filters * 8, kernel_size=3, batchnorm=batchnorm)\r\n\r\n    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides=(2, 2), padding='same')(c6)\r\n    u7 = concatenate([u7, c3])\r\n    u7 = Dropout(dropout)(u7)\r\n    c7 = conv2d_block(u7, n_filters=n_filters * 4, kernel_size=3, batchnorm=batchnorm)\r\n\r\n    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides=(2, 2), padding='same')(c7)\r\n    u8 = concatenate([u8, c2])\r\n    u8 = Dropout(dropout)(u8)\r\n    c8 = conv2d_block(u8, n_filters=n_filters * 2, kernel_size=3, batchnorm=batchnorm)\r\n\r\n    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides=(2, 2), padding='same')(c8)\r\n    u9 = concatenate([u9, c1], axis=3)\r\n    u9 = Dropout(dropout)(u9)\r\n    c9 = conv2d_block(u9, n_filters=n_filters * 1, kernel_size=3, batchnorm=batchnorm)\r\n\r\n    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\r\n    model = Model(inputs=[input_img], outputs=[outputs])\r\n    return model\r\n\r\ncallbacks = [\r\n    EarlyStopping(patience=10, verbose=1),\r\n    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\r\n    ModelCheckpoint(\"model-prototype.h5\", verbose=1, save_best_only=True, save_weights_only=True)\r\n]\r\n\r\nim_width = 1536\r\nim_height = 1536\r\n\r\ninput_img = Input((im_height, im_width, 1), name='img')\r\nmodel = get_unet(input_img, n_filters=16, dropout=0.05, batchnorm=True)\r\n\r\nmodel.compile(optimizer=Adam(lr=0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\r\n\r\ntrain_templates_path = \"E:/train/templates\"\r\ntrain_masks_path = \"E:/train/masks\"\r\nvalid_templates_path = \"E:/valid/templates\"\r\nvalid_masks_path = \"E:/valid/masks\"\r\n\r\ntrain_generator = data_gen(train_templates_path, train_masks_path, im_width, batch_size = 4)\r\nval_generator = data_gen(valid_templates_path, valid_masks_path, im_width, batch_size = 4)\r\n\r\nresults = model.fit_generator(train_generator, epochs=5, steps_per_epoch=10, validation_data=val_generator, validation_steps=1, callbacks=callbacks)\r\n```\r\n\r\n**Other info / logs**\r\nHere's a complete error log:\r\n```\r\n2019-12-12 15:19:36.384413: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX AVX2\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2019-12-12 15:19:36.386318: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 16. Tune using inter_op_parallelism_threads for best performance.\r\nEpoch 1/5\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 527, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 286, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 265, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\", line 437, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 541, in _apply_op_helper\r\n    values, as_ref=input_arg.is_ref).dtype.name\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 286, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 265, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\", line 437, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"E:/Explorium/python/networks_learning.py\", line 143, in <module>\r\n    results = model.fit_generator(train_generator, epochs=5, steps_per_epoch=10, validation_data=val_generator, validation_steps=1, callbacks=callbacks)\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1297, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\", line 265, in model_iteration\r\n    batch_outs = batch_function(*batch_data)\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1017, in train_on_batch\r\n    self._make_train_function()\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2116, in _make_train_function\r\n    params=self._collected_trainable_weights, loss=self.total_loss)\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizers.py\", line 476, in get_updates\r\n    grads = self.get_gradients(loss, params)\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizers.py\", line 92, in get_gradients\r\n    if None in grads:\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\", line 1336, in tensor_equals\r\n    return gen_math_ops.equal(self, other)\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\", line 3626, in equal\r\n    name=name)\r\n  File \"C:\\Users\\E-soft\\Anaconda3\\envs\\Explorium\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 545, in _apply_op_helper\r\n    (input_name, err))\r\nValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.\r\n```\r\n**Note!** Changing imports from \r\n\"from tensorflow.python.keras.models import Model\"\r\nto\r\n\"from tensorflow.keras.models import Model\"\r\ndoesn't work. I get another error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"E:/Explorium/python/networks_learning.py\", line 6, in <module>\r\n    from tensorflow.keras.layers.core import Lambda, RepeatVector, Reshape\r\nModuleNotFoundError: No module named 'tensorflow.keras.layers.core'\r\n```\r\n", "comments": ["@EtagiBI, Could you provide the supporting files to replicate the reported issue. Thanks!", "> @EtagiBI, Could you provide the supporting files to replicate the reported issue. Thanks!\r\n\r\n@gadagashwini Unfortunately, I'm not allowed to share the whole dataset (not to mention it's huge), but I can provide some extamples:\r\n[Dataset.zip](https://github.com/tensorflow/tensorflow/files/3960188/Dataset.zip)\r\n\r\n", "I was able to reproduce the issue with Tf 2.0.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/45d85483511e7784bbf7e91d3ca65f2f/untitled312.ipynb#scrollTo=iYpBkcu82gGS). Thanks! ", "@EtagiBI Please change all the imports as given below.\r\n\r\n```\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\r\nfrom tensorflow.keras.layers.core import Lambda, RepeatVector, Reshape\r\nfrom tensorflow.keras.layers.convolutional import Conv2D, Conv2DTranspose\r\nfrom tensorflow.keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\r\nfrom tensorflow.keras.layers.merge import concatenate, add\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\r\n```\r\nwhich should help you in solving the issue.\r\n\r\nFor more information, take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/32646#issuecomment-533250690). Thanks!", "@EtagiBI Did the above comment solve your issue. Can I close this issue? ", "\r\n\r\n\r\n> @EtagiBI Please change all the imports as given below.\r\n> \r\n> ```\r\n> from tensorflow.keras.models import Model\r\n> from tensorflow.keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\r\n> from tensorflow.keras.layers.core import Lambda, RepeatVector, Reshape\r\n> from tensorflow.keras.layers.convolutional import Conv2D, Conv2DTranspose\r\n> from tensorflow.keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\r\n> from tensorflow.keras.layers.merge import concatenate, add\r\n> from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\n> from tensorflow.keras.optimizers import Adam\r\n> from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\r\n> ```\r\n> \r\n> which should help you in solving the issue.\r\n> \r\n> For more information, take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/32646#issuecomment-533250690). Thanks!\r\n\r\n@gowthamkpr thank you. Unfortunately, it doesn't work, because there's no subfolder named \"keras\" in \"tensorflow-core\" folder. \r\n```\r\nTraceback (most recent call last):\r\n  File \"E:/Explorium/python/networks_learning.py\", line 7, in <module>\r\n    from tensorflow.keras.layers.core import Lambda, RepeatVector, Reshape\r\nModuleNotFoundError: No module named 'tensorflow.keras.layers.core'\r\n```\r\n\r\nHere's my \"tensorflow-core\" folder:\r\n![image](https://user-images.githubusercontent.com/20855725/72731422-8550b600-3bb5-11ea-9fc1-46f2f6b6943c.png)\r\nAs you can see, there's a \"python\" subfolder that contains following files and directories including \"keras\":\r\n![image](https://user-images.githubusercontent.com/20855725/72731763-28a1cb00-3bb6-11ea-9827-0c4c47144a9e.png)\r\n\r\nSo, that's why I was using \"tensorflow.python.keras\" in my imports.\r\n\r\n", "I am sorry for the above comment but try following this [comment](https://github.com/tensorflow/tensorflow/issues/32646#issuecomment-554609113). just replace\r\n`from tensorflow.keras.optimizers import Adam`\r\nin place of\r\n`from tensorflow.python.keras.optimizers import Adam`\r\n\r\nI tried running this and my session is crashing due to high usage of RAM", "@gowthamkpr, the following imports work indeed:\r\n```\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\r\nfrom tensorflow.python.keras.layers.core import Lambda, RepeatVector, Reshape\r\nfrom tensorflow.python.keras.layers.convolutional import Conv2D, Conv2DTranspose\r\nfrom tensorflow.python.keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\r\nfrom tensorflow.python.keras.layers.merge import concatenate, add\r\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\r\n```\r\nAnd yes, I get a memory allocation error as well =(....\r\nAnyway, thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35056\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35056\">No</a>\n"]}, {"number": 35055, "title": "Post-training quantization leaves quantization nodes", "body": "As reported [here](https://towardsdatascience.com/hacking-google-coral-edge-tpu-motion-blur-and-lanczos-resize-9b60ebfaa552), post-training quantization with representative dataset leaves quantization and dequantization nodes in the graph. \r\nIn the post, `edgetpu_compiler` works but the model expects `float32`, which is suboptimal. \r\nAlso all the tensors have `dtype` `int8` instead of `uint8`.\r\nIs this the expected behavior? Or a known issue?\r\n\r\n Is there a robust workaround? \r\n\r\nI can work on an example with DNNs if needed but I just wanted to ask first.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include minimum standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes but I didn't take time to make a minimal working example so I'll close it, I'll reopen it maybe later. "]}, {"number": 35054, "title": "Failed to load delegate from libedgetpu.so.1 on PCIe EdgeTPU [SOLVED]", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n++ Linux Ubuntu 18.04.3\r\n- TensorFlow installed from (source or binary):\r\n++ pip install tflite-runtime...whl\r\n- TensorFlow version:\r\n++ tflite-runtime-1.14.0-cp37...\r\n- Python version:\r\n++ 3.7.5\r\n- Installed using virtualenv? pip? conda?:\r\n++ conda\r\n- CUDA/cuDNN version:\r\n++ CPU Only with EdgeTPU\r\n- GPU model and memory:\r\n++ PCIe EdgeTPU\r\n\r\n```\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              8\r\nOn-line CPU(s) list: 0-7\r\nThread(s) per core:  2\r\nCore(s) per socket:  4\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               60\r\nModel name:          Intel(R) Core(TM) i7-4700EQ CPU @ 2.40GHz\r\nStepping:            3\r\nCPU MHz:             1097.578\r\nCPU max MHz:         3400.0000\r\nCPU min MHz:         800.0000\r\nBogoMIPS:            4789.32\r\nVirtualisation:      VT-x\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            256K\r\nL3 cache:            6144K\r\nNUMA node0 CPU(s):   0-7\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer xsave avx f16c rdrand lahf_lm abm cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts md_clear flush_l1d\r\n```\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen running `classify_image.py`, I receive the following error\r\n```\r\n(Coral) v4@v4-TPU:~/coral/tflite/python/examples/classification$ python3 classify_image.py --model models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite --labels models/inat_bird_labels.txt --input images/parrot.jpg\r\nTraceback (most recent call last):\r\n  File \"/home/v4/anaconda3/envs/Coral/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 165, in load_delegate\r\n    delegate = Delegate(library, options)\r\n  File \"/home/v4/anaconda3/envs/Coral/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 119, in __init__\r\n    raise ValueError(capture.message)\r\nValueError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"classify_image.py\", line 118, in <module>\r\n    main()\r\n  File \"classify_image.py\", line 95, in main\r\n    interpreter = make_interpreter(args.model)\r\n  File \"classify_image.py\", line 69, in make_interpreter\r\n    {'device': device[0]} if device else {})\r\n  File \"/home/v4/anaconda3/envs/Coral/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 168, in load_delegate\r\n    library, str(e)))\r\nValueError: Failed to load delegate from libedgetpu.so.1\r\n```\r\n\r\n**Any other info / logs**\r\nI have seen various solutions, suggesting I add the user to plugdev group (This was on a solution regarding the USB Accelerator, but i tried it anyway)\r\n\r\nAnother point was that it wasnt plugged in (again for the usb accelerator), which it is:\r\n```\r\nv4@v4-TPU:~$ ls /dev/apex_0\r\n/dev/apex_0\r\n```\r\n```\r\nv4@v4-TPU:~$ lspci -x | grep 089a\r\n04:00.0 Non-VGA unclassified device: Device 1ac1:089a\r\n```\r\nAnd dmesg: \r\n```\r\nv4@v4-TPU:~$ dmesg | grep apex\r\n[    7.830468] apex 0000:04:00.0: Apex performance not throttled due to temperature\r\n```\r\n```\r\nv4@v4-TPU:~$ dmesg | grep 089a\r\n[    0.274318] pci 0000:04:00.0: [1ac1:089a] type 00 class 0x0000ff\r\n```\r\nso the device is connected properly.\r\n\r\nThe Libraries are appropriately installed:\r\n```\r\n(Coral) v4@v4-TPU:~/coral/tflite/python/examples/classification$ ls /usr/lib/x86_64-linux-gnu/libedgetpu.so.1*\r\n/usr/lib/x86_64-linux-gnu/libedgetpu.so.1  /usr/lib/x86_64-linux-gnu/libedgetpu.so.1.0\r\n```\r\nand these are the permissions of the libraries:\r\n```\r\n(Coral) v4@v4-TPU:~/coral/tflite/python/examples/classification$ ll /usr/lib/x86_64-linux-gnu/libedgetpu.so.1*\r\nlrwxrwxrwx 1 root root     17 Sep 16 21:27 /usr/lib/x86_64-linux-gnu/libedgetpu.so.1 -> libedgetpu.so.1.0\r\n-rwxr-xr-x 1 root root 956392 Sep 16 21:27 /usr/lib/x86_64-linux-gnu/libedgetpu.so.1.0\r\n```\r\nNOTE: I did have to add the 2nd and 3rd execute permissions on `libedgetpu.so.1.0`\r\n", "comments": ["Having gone through this with Coral technical, it was narrowed down to not properly adding the user to the apex group, causing the error in accessing the libs.\r\n\r\nMarked Closed and Solved", "I am also facing a similar issue.\r\nThe demo API gives error at tflite.load_delegate.\r\n\r\n```\r\npi@bpi-iot-ros-ai:~/coral/tflite/python/examples/classification$ python3 classify_image.py --model models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite --labels models/inat_bird_labels.txt --input images/parrot.jpg\r\nE :248] HIB Error. hib_error_status = 0000000000000001, hib_first_error_status = 0000000000000001\r\nE :248] HIB Error. hib_error_status = 0000000000000001, hib_first_error_status = 0000000000000001\r\nINFO: Initialized TensorFlow Lite runtime.\r\n----INFERENCE TIME----\r\nNote: The first inference on Edge TPU is slow because it includes loading the model into Edge TPU memory.\r\n\r\n\r\n\r\npi@bpi-iot-ros-ai:~$ uname -a\r\nLinux bpi-iot-ros-ai 5.4.0-bpi-r64 #1 SMP PREEMPT Mon Dec 16 16:00:08 IST 2019 aarch64 aarch64 aarch64 GNU/Linux\r\npi@bpi-iot-ros-ai:~$ lscpu\r\nArchitecture:          aarch64\r\nByte Order:            Little Endian\r\nCPU(s):                2\r\nOn-line CPU(s) list:   0,1\r\nThread(s) per core:    1\r\nCore(s) per socket:    2\r\nSocket(s):             1\r\nCPU max MHz:           1350.0000\r\nCPU min MHz:           30.0000\r\npi@bpi-iot-ros-ai:~$ ls -l /usr/lib/aarch64-linux-gnu/libedge*\r\nlrwxrwxrwx 1 root root     17 Sep 17 04:27 /usr/lib/aarch64-linux-gnu/libedgetpu.so.1 -> libedgetpu.so.1.0\r\n-rwxrwxrwx 1 root root 792376 Sep 17 04:27 /usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\r\npi@bpi-iot-ros-ai:~$ lspci\r\n00:00.0 PCI bridge: MEDIATEK Corp. Device 3258\r\n01:00.0 System peripheral: Device 1ac1:089a\r\npi@bpi-iot-ros-ai:~$ ls /dev/apex_0 \r\n/dev/apex_0\r\npi@bpi-iot-ros-ai:~$ sudo sh -c \"echo 'SUBSYSTEM==\\\"apex\\\", MODE=\\\"0660\\\", GROUP=\\\"apex\\\"' >> /etc/udev/rules.d/65-apex.rules\"\r\npi@bpi-iot-ros-ai:~$ sudo groupadd apex\r\ngroupadd: group 'apex' already exists\r\npi@bpi-iot-ros-ai:~$ sudo adduser $USER apex\r\nThe user `pi' is already a member of `apex'.\r\n```\r\n", "Coral Tech Support had indicated from the dmesg logs that the memory was not assigned for BAR 0\r\n\r\n```\r\n[ 1.604440] pci 0000:01:00.0: [1ac1:089a] type 00 class 0x0000ff \r\n[ 1.610743] pci 0000:01:00.0: reg 0x10: [mem 0x00000000-0x00003fff 64bit pref] \r\n[ 1.618078] pci 0000:01:00.0: reg 0x18: [mem 0x00000000-0x000fffff 64bit pref] \r\n[ 1.626285] pci 0000:01:00.0: 2.000 Gb/s available PCIe bandwidth, limited by 2.5 GT/s x1 link at 0000:00:00.0 (capable of 4.000 Gb/s with 5 GT/s x1 link) \r\n[ 1.641429] pci_bus 0000:01: fixups for bus \r\n[ 1.645617] pci_bus 0000:01: bus scan returning with max=01 \r\n[ 1.651195] pci_bus 0000:01: busn_res: [bus 01-ff] end is updated to 01 \r\n[ 1.657816] pci_bus 0000:00: bus scan returning with max=01 \r\n[ 1.663407] pci 0000:00:00.0: BAR 0: no space for [mem size 0x200000000 64bit pref] \r\n[ 1.671071] pci 0000:00:00.0: BAR 0: failed to assign [mem size 0x200000000 64bit pref] \r\n[ 1.679079] pci 0000:00:00.0: BAR 8: assigned [mem 0x20000000-0x201fffff] \r\n[ 1.685872] pci 0000:00:00.0: PCI bridge to [bus 01] \r\n[ 1.690846] pci 0000:00:00.0: bridge window [mem 0x20000000-0x201fffff] \r\n```\r\n\r\nThe BAR0 for pcie bridge didn't get memory range assigned, is is an issue with the driver in the SoC. This \"BAR 0 failed to assign\" is preventing our device from accessing the host's memory. This is exactly why you get this issue right during load delegate. Unfortunately, this is a little outside of our hand. On another note, the swiotlb=512 from your kernel command line is very low, I suggest increasing to swiotlb=65536 for possible unrelated issues.\r\n\r\nI have now update the swiotlb=65536 and gasket.dma_bit_mask=32 in the kernel command line and also managed to resolve the BAR0 memory assignment issue.\r\nFollowing is the log.\r\n\r\n```\r\npi@bpi-iot-ros-ai:~$ lsmod\r\nModule                  Size  Used by\r\nmt7622                 45056  -2\r\nmt76                   45056  -2\r\nmac80211              364544  -2\r\nlibarc4                16384  -2\r\napex                   24576  -2\r\ngasket                 98304  -2\r\ncfg80211              258048  -2\r\nbtmtkuart              24576  -2\r\npi@bpi-iot-ros-ai:~$ dmesg | grep apex\r\n[    8.381572] apex 0000:01:00.0: assign IRQ: got 140\r\n[    8.381703]  apex_pci_probe+0x38/0x468 [apex]\r\n[    8.381762]  apex_init+0x44/0x1000 [apex]\r\n[    8.381906]  apex_pci_probe+0x38/0x468 [apex]\r\n[    8.381960]  apex_init+0x44/0x1000 [apex]\r\n[    8.382002] apex 0000:01:00.0: Assigned....BAR 0 [mem 0x20100000-0x20103fff 64bit pref]........\r\n[    8.382008] apex 0000:01:00.0: Assigned and claimed....BAR 0 [mem 0x20100000-0x20103fff 64bit pref]........\r\n[    8.382014] apex 0000:01:00.0: Assigned....BAR 2 [mem 0x20000000-0x200fffff 64bit pref]........\r\n[    8.382020] apex 0000:01:00.0: Assigned and claimed....BAR 2 [mem 0x20000000-0x200fffff 64bit pref]........\r\n[    8.382025] apex 0000:01:00.0: enabling device (0000 -> 0002)\r\n[    8.382078] apex 0000:01:00.0: enabling bus mastering\r\n[   13.538841] apex 0000:01:00.0: Apex performance not throttled due to temperature\r\npi@bpi-iot-ros-ai:~$ dmesg | grep pci\r\n[    1.494119] mtk-pcie 1a143000.pcie: host bridge /pcie@1a143000 ranges:\r\n[    1.505642] mtk-pcie 1a143000.pcie: Parsing ranges property...\r\n[    1.515992] mtk-pcie 1a143000.pcie:   MEM 0x20000000..0x2fffffff -> 0x20000000\r\n[    1.556439] mtk-pcie 1a143000.pcie: PCI host bridge to bus 0000:00\r\n[    1.568448] pci_bus 0000:00: root bus resource [bus 00-ff]\r\n[    1.573946] pci_bus 0000:00: root bus resource [mem 0x20000000-0x2fffffff]\r\n[    1.580841] pci_bus 0000:00: scanning bus\r\n[    1.584897] pci 0000:00:00.0: [14c3:3258] type 01 class 0x060400\r\n[    1.590957] pci 0000:00:00.0: reg 0x10: [mem 0x00000000-0x1ffffffff 64bit pref]\r\n[    1.599829] pci_bus 0000:00: fixups for bus\r\n[    1.604022] pci 0000:00:00.0: scanning [bus 00-00] behind bridge, pass 0\r\n[    1.610729] pci 0000:00:00.0: bridge configuration invalid ([bus 00-00]), reconfiguring\r\n[    1.618747] pci 0000:00:00.0: scanning [bus 00-00] behind bridge, pass 1\r\n[    1.625563] pci_bus 0000:01: scanning bus\r\n[    1.629683] pci 0000:01:00.0: [1ac1:089a] type 00 class 0x0000ff\r\n[    1.635984] pci 0000:01:00.0: reg 0x10: [mem 0x00000000-0x00003fff 64bit pref]\r\n[    1.643318] pci 0000:01:00.0: reg 0x18: [mem 0x00000000-0x000fffff 64bit pref]\r\n[    1.651524] pci 0000:01:00.0: 2.000 Gb/s available PCIe bandwidth, limited by 2.5 GT/s x1 link at 0000:00:00.0 (capable of 4.000 Gb/s with 5 GT/s x1 link)\r\n[    1.666709] pci_bus 0000:01: fixups for bus\r\n[    1.670896] pci_bus 0000:01: bus scan returning with max=01\r\n[    1.676472] pci_bus 0000:01: busn_res: [bus 01-ff] end is updated to 01\r\n[    1.683093] pci_bus 0000:00: bus scan returning with max=01\r\n[    1.688684] pci 0000:00:00.0: BAR 0: no space for [mem size 0x200000000 64bit pref]\r\n[    1.696347] pci 0000:00:00.0: BAR 0: failed to assign [mem size 0x200000000 64bit pref]\r\n[    1.704355] pci 0000:00:00.0: BAR 8: assigned [mem 0x20000000-0x201fffff]\r\n[    1.711151] pci 0000:01:00.0: BAR 2: assigned [mem 0x20000000-0x200fffff 64bit pref]\r\n[    1.718983] pci 0000:01:00.0: BAR 0: assigned [mem 0x20100000-0x20103fff 64bit pref]\r\n[    1.726814] pci 0000:00:00.0: PCI bridge to [bus 01]\r\n[    1.731787] pci 0000:00:00.0:   bridge window [mem 0x20000000-0x201fffff]\r\n[    1.738888] mtk-pcie 1a145000.pcie: host bridge /pcie@1a145000 ranges:\r\n[    1.745425] mtk-pcie 1a145000.pcie: Parsing ranges property...\r\n[    1.751269] mtk-pcie 1a145000.pcie:   MEM 0x28000000..0x2fffffff -> 0x28000000\r\n[    1.758508] mtk-pcie 1a145000.pcie: resource collision: [mem 0x28000000-0x2fffffff] conflicts with pcie@1a143000 [mem 0x20000000-0x2fffffff]\r\n[    1.771141] mtk-pcie: probe of 1a145000.pcie failed with error -16\r\n[    8.381659]  pci_enable_resources+0x68/0x18c\r\n[    8.381665]  pcibios_enable_device+0xc/0x14\r\n[    8.381670]  do_pci_enable_device+0x50/0xd8\r\n[    8.381675]  pci_enable_device_flags+0x100/0x15c\r\n[    8.381680]  pci_enable_device+0x10/0x18\r\n[    8.381684]  pci_enable_bridge+0x50/0x78\r\n[    8.381689]  pci_enable_device_flags+0x98/0x15c\r\n[    8.381694]  pci_enable_device+0x10/0x18\r\n[    8.381703]  apex_pci_probe+0x38/0x468 [apex]\r\n[    8.381709]  pci_device_probe+0xa0/0x144\r\n[    8.381755]  __pci_register_driver+0x40/0x48\r\n[    8.381811] pci 0000:00:00.0: Assigned....BAR 8 [mem 0x20000000-0x201fffff]........\r\n[    8.381817] pci 0000:00:00.0: Assigned and claimed....BAR 8 [mem 0x20000000-0x201fffff]........\r\n[    8.381823] pci 0000:00:00.0: enabling device (0000 -> 0002)\r\n[    8.381837] pci 0000:00:00.0: enabling bus mastering\r\n[    8.381880]  pci_enable_resources+0x68/0x18c\r\n[    8.381884]  pcibios_enable_device+0xc/0x14\r\n[    8.381889]  do_pci_enable_device+0x50/0xd8\r\n[    8.381893]  pci_enable_device_flags+0x100/0x15c\r\n[    8.381898]  pci_enable_device+0x10/0x18\r\n[    8.381906]  apex_pci_probe+0x38/0x468 [apex]\r\n[    8.381911]  pci_device_probe+0xa0/0x144\r\n[    8.381953]  __pci_register_driver+0x40/0x48\r\npi@bpi-iot-ros-ai:~$ \r\npi@bpi-iot-ros-ai:~$ dmesg | grep gasket\r\n[    0.000000] Kernel command line: board=bpi-r64 console=ttyS0,115200n1 earlyprintk root=/dev/mmcblk1p2 rootfstype=ext4 rootwait service=linux debug=7 initcall_debug=0 androidboot.hardware=mt7622 swiotlb=65536 gasket.dma_bit_mask=32\r\n[    8.360957] gasket: loading out-of-tree module taints kernel.\r\npi@bpi-iot-ros-ai:~$ cd coral/tflite/python/examples/classification/\r\npi@bpi-iot-ros-ai:~/coral/tflite/python/examples/classification$ python3 classify_image.py --model models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite --labels models/inat_bird_labels.txt --input images/parrot.jpg\r\nE :237] HIB Error. hib_error_status = 0000000000000001, hib_first_error_status = 0000000000000001\r\nE :237] HIB Error. hib_error_status = 0000000000000001, hib_first_error_status = 0000000000000001\r\n----INFERENCE TIME----\r\nNote: The first inference on Edge TPU is slow because it includes loading the model into Edge TPU memory.\r\n\r\n```\r\n\r\nInspite of this I am still getting this error\r\n\r\nAny advice to resolve this", "As a note for anybody who finds this issue from Google (like me) after installing the mini PCIe accelerator and following the Coral getting started guide (from coral.ai) and you hit this error.\r\n\r\nCheck the permissions on your /dev/apex_0 device.\r\n\r\nMine was owned by user root, group root with permissions 600.\r\n\r\nAs root, change the owner  of /dev/apex_0 to be user root, group apex, and permissions to 660:\r\n`chown root:apex /dev/apex_0 && chmod 660 /dev/apex_0`"]}, {"number": 35053, "title": " AndroidRuntime: Node number 1 (SPLIT) failed to prepare.", "body": "Hi, when i use my own trained rnn model to inference on my andorid app\r\nthe tflite convert is successully with python api \r\n\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\nbut when i run this model in andorid app ,\r\ntflite.run(input, output);\r\n it run into the issuses as the log shows:\r\ncould anyone take a look at this?\r\n\r\n\r\n**Any other info / logs**\r\n[16-14-29.810] 01-01 08:49:45.232 12405 12427 E AndroidRuntime: FATAL EXCEPTION: inference\r\n\ufeff\ufeff[16-14-29.810] 01-01 08:49:45.232 12405 12427 E AndroidRuntime: Process: com.hse.wifimotiondetection, PID: 12405\r\n\ufeff\ufeff[16-14-29.810] 01-01 08:49:45.232 12405 12427 E AndroidRuntime: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/split.cc Not an even split\r\n\ufeff\ufeff[16-14-29.810] 01-01 08:49:45.232 12405 12427 E AndroidRuntime: Node number 1 (SPLIT) failed to prepare.\r\n\ufeff\ufeff[16-14-29.854] 01-01 08:49:45.232 12405 12427 E AndroidRuntime: \r\n\ufeff\ufeff[16-14-29.854] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)\r\n\ufeff\ufeff[16-14-29.854] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:143)\r\n\ufeff\ufeff[16-14-29.854] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:296)\r\n\ufeff\ufeff[16-14-29.898] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.run(Interpreter.java:259)\r\n\ufeff\ufeff[16-14-29.898] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at com.hse.wifimotiondetection.tflite.Classifier.recognizeWiFiData(Classifier.java:282)\r\n\ufeff\ufeff[16-14-29.898] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at com.hse.wifimotiondetection.MainActivity$1.run(MainActivity.java:101)\r\n\ufeff\ufeff[16-14-29.899] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at android.os.Handler.handleCallback(Handler.java:789)\r\n\ufeff\ufeff[16-14-29.942] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at android.os.Handler.dispatchMessage(Handler.java:98)\r\n\ufeff\ufeff[16-14-29.942] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at android.os.Looper.loop(Looper.java:164)\r\n\ufeff\ufeff[16-14-29.943] 01-01 08:49:45.232 12405 12427 E AndroidRuntime:        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n\ufeff\ufeff[16-14-29.943] 01-01 08:54:34.738 13372 13372 I AndroidRuntime: VM exiting with result code 0.\r\n", "comments": ["@glevenc Can you please try \r\n1. the example I provided in this [example](https://colab.sandbox.google.com/gist/jvishnuvardhan/66d5a457486c4a3189950d91a0b8c159/untitled632.ipynb) and see whether the tensorflow model and the tf_lite model are working similar? \r\n2. Can you please share a standalone code to reproduce the issue. Thanks!", "> \r\n> \r\n> @glevenc Can you please try\r\n> \r\n>     1. the example I provided in this [example](https://colab.sandbox.google.com/gist/jvishnuvardhan/66d5a457486c4a3189950d91a0b8c159/untitled632.ipynb) and see whether the tensorflow model and the tf_lite model are working similar?\r\n> \r\n>     2. Can you please share a standalone code to reproduce the issue. Thanks!\r\nyou can go ahead to ues the same standalone code for my first issues\r\nhttps://colab.research.google.com/gist/glevenc/20ecb878b520653257f4d290dd79fec0/test.ipynb\r\nThx\r\n\r\n@jvishnuvardhan \r\nHi, sorry , i cannot try the example during the \r\ntf.compat.v1.disable_eager_execution() this mode  in my code  when i convert the model to tflite format\r\nas the log error shows\r\n\r\n\r\n>>> tflite_model = converter.convert()\r\nWARNING:tensorflow:Issue encountered when serializing variables.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\nTensor.name is meaningless when eager execution is enabled.\r\n2019-12-14 10:58:26.734639: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-12-14 10:58:26.741202: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-12-14 10:58:26.755469: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-12-14 10:58:26.760993: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-12-14 10:58:26.764765: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\d\\.conda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\lite\\python\\lite.py\", line 405, in convert\r\n    self._funcs[0], lower_control_flow=False)\r\n  File \"C:\\Users\\d\\.conda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\python\\framework\\convert_to_constants.py\", line 411, in convert_variables_to_constants_v2\r\n    tensor_data = _get_tensor_data(func)\r\n  File \"C:\\Users\\d\\.conda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\python\\framework\\convert_to_constants.py\", line 193, in _get_tensor_data\r\n    data = map_index_to_variable[idx].numpy()\r\n  File \"C:\\Users\\d\\.conda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 579, in numpy\r\n    \"numpy() is only available when eager execution is enabled.\")\r\nNotImplementedError: numpy() is only available when eager execution is enabled.", "hi, does this issue still exist?", "@glevenc \r\nIt looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35053\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35053\">No</a>\n"]}, {"number": 35052, "title": "model.run_eagerly=False is much slower than model.run_eagerly=True", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: Python 3.7.5\r\n\r\n**Describe the current behavior**\r\nI try to implemente a simple fm algorithm in tensorflow 2.0. I found use keras.fit is very slow in default params.\r\nIf I change the model.run_eagerly to True the performance will be better.\r\nThen I tried turn off eager_execution  by tf.compat.v1.disable_eager_execution(), the performance is the same as tf1.14 with estimator.\r\n\r\n1. default\r\n```\r\n2019-12-12 15:37:04.152742: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2019-12-12 15:37:04.174603: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz\r\n2019-12-12 15:37:04.187437: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556f783dc5b0 executing computations on platform Host. Devices:\r\n2019-12-12 15:37:04.187474: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\nEpoch 1/1000\r\n    121/Unknown - 31s 256ms/step - loss: 0.6940 - AUC: 0.4324   \r\n```\r\n2. model.run_eagerly=True\r\n```\r\n2019-12-12 15:38:36.014767: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2019-12-12 15:38:36.038416: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz\r\n2019-12-12 15:38:36.051835: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b826158d40 executing computations on platform Host. Devices:\r\n2019-12-12 15:38:36.051874: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\nEpoch 1/1000\r\n     96/Unknown - 7s 72ms/step - loss: 0.6902 - AUC: 0.3739    \r\n```\r\n\r\n3. tf.compat.v1.disable_eager_execution()\r\n```\r\nWARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\r\n2019-12-12 15:39:18.986171: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2019-12-12 15:39:19.008642: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz\r\n2019-12-12 15:39:19.020877: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561448d34bb0 executing computations on platform Host. Devices:\r\n2019-12-12 15:39:19.020914: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:From /home/luoxinchen/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nTrain on 1000 steps\r\nEpoch 1/1000\r\n1000/1000 [==============================] - 1s 1ms/step - loss: 0.6942 - AUC: 0.4701  \r\nEpoch 2/1000\r\n1000/1000 [==============================] - 1s 633us/step - loss: 0.6939 - AUC: 0.4797\r\nEpoch 3/1000\r\n1000/1000 [==============================] - 1s 640us/step - loss: 0.6936 - AUC: 0.4908\r\nEpoch 4/1000\r\n```\r\n\r\n**Describe the expected behavior**\r\nI think keras.fit with model.run_eagerly=False will use tf.function to wrap the training loop, and it's performance should be close to the disable eager execution. But it's perform awfully, it even slower than model.run_eagerly=True.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport os\r\nimport sys\r\nimport timeit\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n# tf.compat.v1.disable_eager_execution()\r\ntf.config.threading.set_inter_op_parallelism_threads(8)\r\nos.environ['OMP_NUM_THREADS'] = '1'\r\n\r\nbucket = int(1e7)\r\n\r\nclass MyModel(keras.Model):\r\n\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n\r\n  def build(self, input_shape):\r\n    self.user_emb = self.add_weight(\r\n        shape=(bucket + 1, 32),\r\n        dtype=tf.float32,\r\n        initializer=tf.keras.initializers.TruncatedNormal(),\r\n        name=\"user_emb\")\r\n    self.item_emb = self.add_weight(\r\n        shape=(bucket + 1, 32),\r\n        dtype=tf.float32,\r\n        initializer=tf.keras.initializers.TruncatedNormal(),\r\n        name=\"item_emb\")\r\n    self.bias = tf.Variable(0.0)\r\n\r\n  def call(self, inputs):\r\n    user_id, item_id = inputs\r\n    user_id = tf.reshape(user_id, [-1])\r\n    item_id = tf.reshape(item_id, [-1])\r\n    out = tf.gather(self.user_emb, user_id) * tf.gather(self.item_emb, item_id)\r\n    out = tf.reduce_sum(out, axis=1, keepdims=True) + self.bias\r\n    out = tf.sigmoid(out)\r\n    return out\r\n\r\n\r\ndef main():\r\n\r\n  def py_func(feats):\r\n    label = feats['labels']\r\n    return (feats['user_id'], feats['item_id']), label\r\n\r\n  model = MyModel()\r\n\r\n  dataset = tf.data.Dataset.from_tensor_slices({\r\n      \"user_id\": np.random.randint(bucket, size=[1000, 1]),\r\n      \"item_id\": np.random.randint(bucket, size=[1000, 1]),\r\n      \"labels\": np.random.randint(2, size=[1000, 1])\r\n  }).map(py_func)\r\n\r\n  model.compile(\r\n      keras.optimizers.SGD(0.01), 'binary_crossentropy', metrics=['AUC'])\r\n\r\n  # model.run_eagerly = True\r\n  model.fit(\r\n      dataset,\r\n      shuffle=False,\r\n      workers=1,\r\n      epochs=1000)\r\n\r\nif __name__ == '__main__':\r\n  main()\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Issue is replicating with Tf 2.0.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/e0cc91b7b2153c3ae4fac2a586734f0a/untitled302.ipynb). Thanks!", "@doldre I was able to reproduce the issue with `TF2.0`. However, I could not reproduce the issue with `tf-nightly`. There were couple of important updates that are related to keras model performance. I think those updates resolved the issue you are noticing with `TF2.0`. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/847af5f6433d42d043e1e40d440f3f04/untitled716.ipynb) is the gist with default settings with `tf-nightly`.  [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/3aec5ef78ccd5e047944ec19c4c9f600/untitled717.ipynb) is the gist with setting `model.run_eagerly=True` and `tf-nightly`.\r\n\r\n\r\n1.default\r\n```\r\nTrain for 1000 steps\r\nEpoch 1/1000\r\n1000/1000 [==============================] - 2s 2ms/step - loss: 0.6939 - AUC: 0.4710\r\nEpoch 2/1000\r\n1000/1000 [==============================] - 1s 1ms/step - loss: 0.6935 - AUC: 0.4834\r\n```\r\n2. model.run_eagerly=True\r\n\r\n```\r\nTrain for 1000 steps\r\nEpoch 1/1000\r\n1000/1000 [==============================] - 11s 11ms/step - loss: 0.6941 - AUC: 0.4770\r\nEpoch 2/1000\r\n1000/1000 [==============================] - 11s 11ms/step - loss: 0.6938 - AUC: 0.4881\r\n```\r\n\r\nPlease close this issue if it was resolved by `tf-nightly`. Thanks!", "@jvishnuvardhan  thanks for your help. I reproduced your result with tf-nightly(2.1.0-dev20191125). It looks like the problem was fixed.", "I found this problem is still existed when using RMSprop optimizer.\r\n[Here ](https://colab.research.google.com/gist/doldre/3d55ce61030fccdc2db1a7bc2d02adaa/untitled717.ipynb)is the gist with RMSprop.", "This issue has been fixed, @doldre you can verify it through tf-nightly. If this is not do-able, you can also wait for 2.2."]}, {"number": 35051, "title": "Use bazel to build tensorflow has some error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution :Linux Ubuntu 18.04 \r\n- TensorFlow installed from (source or binary):source\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):bazel-1.0.0 and soft lint to bazel\r\n- GCC/Compiler version (if compiling from source): 7.4.0 \r\n- CUDA/cuDNN version: no \r\n\r\n\r\n\r\n**Describe the problem**\r\nuse bazel to build tensorflow (newest clone from github)\r\n\r\n```\r\nbazel build --local_ram_resources=2048 --config=opt //tensorflow/tools/pip_package:build_pip_package \r\n``` \r\nError message : \r\n```\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/test/tensorflow/tensorflow/python/tools/BUILD:280:1 C++ compilation of rule '//tensorflow/core/kernels:conv_ops' failed (Exit 4)\r\nINFO: Elapsed time: 13914.473s, Critical Path: 2099.24s\r\nINFO: 3244 processes: 3244 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["I am also experiencing the same error. Were you able to fix it?\r\n", "Can you try to reduce amount of resources used by the compiler?", "Solved mine by using `--local_cpu_resources 1` instead of `--local_ram_resources=2048`", "In this case, can we close as fixed?", "@kscorpio,\r\nIs this still an issue?\r\n\r\n> Solved mine by using `--local_cpu_resources 1` instead of `--local_ram_resources=2048`\r\n\r\nPlease take a look at @mapavia's comment and check if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35051\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35051\">No</a>\n"]}, {"number": 35050, "title": "tf throw error and not may get correct gradient when eager_mode is disable in tf=2.0.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 in Docker\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 10.0 / 7\r\n- GPU model and memory: GTX 1080Ti / 11175MiB\r\n\r\n**Describe the current behavior**\r\n\r\nHi authors and developers,\r\n\r\nI am developing our project in tf=2.0.0 and eager_mode is disable.\r\n\r\nThe main reason is tf=1.x will not be maintained but third party libraries have not been ready for tf=2.0 yet.\r\n\r\nHowever, I met some strange bugs when I was training custom model.\r\n\r\nThis bug can be reproduced by the following minimal testcase:\r\n\r\n```python\r\n#%%\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n#tf.compat.v1.disable_v2_behavior()\r\n\r\nimport numpy as np\r\n\r\nbatch_size = 100\r\n\r\ndef download_data():\r\n\r\n    # get raw data\r\n    (trainX, trainY), (testX, testY) = tf.keras.datasets.cifar10.load_data()\r\n    trainX = trainX.astype(np.float32)\r\n    testX  = testX.astype(np.float32)\r\n\r\n    # ont-hot\r\n    trainY = tf.keras.utils.to_categorical(trainY, 10)\r\n    testY  = tf.keras.utils.to_categorical(testY , 10)\r\n\r\n    # get validation sets\r\n    training_size = 45000\r\n    validX = trainX[training_size:,:]\r\n    validY = trainY[training_size:,:]\r\n\r\n    trainX = trainX[:training_size,:]\r\n    trainY = trainY[:training_size,:]\r\n\r\n    return trainX, trainY, validX, validY, testX, testY\r\n\r\ndef data_pipeline(dataX, dataY):\r\n\r\n        dataset = tf.data.Dataset.from_tensor_slices( (dataX, dataY) )\r\n        dataset = dataset.shuffle(batch_size * 8)\r\n        dataset = dataset.repeat()\r\n        dataset = dataset.batch(batch_size)\r\n        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n        return dataset\r\n\r\nclass custom_model():\r\n    def __init__(self):\r\n\r\n        def Acc():\r\n            acc = tf.keras.metrics.categorical_accuracy(label_ref, clf_out)\r\n            return tf.math.reduce_mean(acc)\r\n\r\n        def c_loss():\r\n            loss = tf.keras.losses.categorical_crossentropy(label_ref, clf_out)\r\n            loss = tf.math.reduce_mean(loss)\r\n            return loss\r\n\r\n        # create model\r\n        clf_input = tf.keras.layers.Input(shape=(32,32,3), name=\"model/input\")\r\n        model = tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, weights=None, input_tensor=clf_input, pooling='max', classes=10)\r\n        #model = tf.keras.applications.vgg16.VGG16(include_top=True, weights=None, input_tensor=clf_input, pooling='max', classes=10)\r\n        model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\r\n\r\n        label_ref = tf.keras.layers.Input(shape=(10,) , name='label_ref')\r\n        clf_out = model(clf_input)\r\n\r\n        # using tf.keras.optimizers.Nadam would get error\r\n        optimizer = tf.keras.optimizers.Nadam(lr=0.0005)\r\n        #optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\r\n        self.train_op = optimizer.minimize(c_loss, var_list=[model.trainable_variables])\r\n\r\n        self.clf_model = model\r\n        self.clf_input = clf_input\r\n        self.label_ref = label_ref\r\n        self.op_acc = Acc()\r\n        self.c_loss = c_loss()\r\n\r\nif __name__ == '__main__':\r\n\r\n    # set GPU\r\n    import os\r\n    if os.environ.get(\"CUDA_VISIBLE_DEVICES\") is None:\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\n    # reset tf session\r\n    tf.compat.v1.keras.backend.clear_session()\r\n    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\r\n    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\r\n    tf.compat.v1.keras.backend.set_session(sess) \r\n\r\n    # prepare data\r\n    trainX, trainY, validX, validY, testX, testY = download_data()\r\n    train_gen = data_pipeline(trainX, trainY)\r\n    valid_gen = data_pipeline(validX, validY)\r\n    test_gen = data_pipeline(testX, testY)\r\n\r\n    # build targeted model\r\n    model = tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, weights=None, input_shape=(32,32,3), pooling='max', classes=10)\r\n    #model = tf.keras.applications.vgg16.VGG16(include_top=True, weights=None, input_shape=(32,32,3), pooling=None, classes=10)\r\n    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\r\n\r\n    # fit and evalutate\r\n    model.fit(train_gen,\r\n            steps_per_epoch = trainY.shape[0] // batch_size,\r\n            validation_data = valid_gen,\r\n            validation_steps= validY.shape[0] // batch_size,\r\n            epochs=5,\r\n            verbose=2)\r\n    model.evaluate(testX, testY, verbose=2, batch_size=batch_size)\r\n\r\n    # create a new model\r\n    print('Make sure that we create a new model.')\r\n    model = custom_model()\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    model.clf_model.evaluate(testX, testY, verbose=2, batch_size=batch_size)\r\n\r\n    # train model\r\n    num_epoch = 5\r\n    tf_iter = tf.compat.v1.data.make_initializable_iterator(train_gen)\r\n    tf_next = tf_iter.get_next()\r\n    sess.run(tf_iter.initializer)\r\n    for epoch in range(num_epoch):\r\n        c_loss, acc = 0.0, 0.0\r\n        for ii in range(trainY.shape[0] // batch_size):\r\n            X, Y = sess.run(tf_next)\r\n            [b_c_loss, b_acc, _] = sess.run([model.c_loss, model.op_acc, model.train_op],\r\n                                                feed_dict={ model.clf_input: X,\r\n                                                            model.label_ref: Y,\r\n                                                            tf.keras.backend.learning_phase(): 1})\r\n            c_loss = c_loss + b_c_loss\r\n            acc = acc + b_acc\r\n        \r\n        c_loss = c_loss / (trainY.shape[0] // batch_size)\r\n        acc = acc / (trainY.shape[0] // batch_size)\r\n        print('[Training]Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss, acc) )\r\n\r\n    # evaluate\r\n    num_epoch = 1\r\n    tf_iter = tf.compat.v1.data.make_initializable_iterator(valid_gen)\r\n    tf_next = tf_iter.get_next()\r\n    sess.run(tf_iter.initializer)\r\n    for epoch in range(num_epoch):\r\n        c_loss, acc = 0.0, 0.0\r\n        for ii in range(validY.shape[0] // batch_size):\r\n            X, Y = sess.run(tf_next)\r\n            [b_c_loss, b_acc, _] = sess.run([model.c_loss, model.op_acc, model.train_op],\r\n                                                feed_dict={ model.clf_input: X,\r\n                                                            model.label_ref: Y,\r\n                                                            tf.keras.backend.learning_phase(): 0})\r\n            c_loss = c_loss + b_c_loss\r\n            acc = acc + b_acc\r\n        \r\n        c_loss = c_loss / (validY.shape[0] // batch_size)\r\n        acc = acc / (validY.shape[0] // batch_size)\r\n        print('[Validation]Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss, acc) )\r\n\r\n    # evaluate\r\n    num_epoch = 1\r\n    tf_iter = tf.compat.v1.data.make_initializable_iterator(test_gen)\r\n    tf_next = tf_iter.get_next()\r\n    sess.run(tf_iter.initializer)\r\n    for epoch in range(num_epoch):\r\n        c_loss, acc = 0.0, 0.0\r\n        for ii in range(testY.shape[0] // batch_size):\r\n            X, Y = sess.run(tf_next)\r\n            [b_c_loss, b_acc, _] = sess.run([model.c_loss, model.op_acc, model.train_op],\r\n                                                feed_dict={ model.clf_input: X,\r\n                                                            model.label_ref: Y,\r\n                                                            tf.keras.backend.learning_phase(): 0})\r\n            c_loss = c_loss + b_c_loss\r\n            acc = acc + b_acc\r\n        \r\n        c_loss = c_loss / (testY.shape[0] // batch_size)\r\n        acc = acc / (testY.shape[0] // batch_size)\r\n        print('[Testing]Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss, acc) )\r\n```\r\n\r\nThe first part of testing case is training model with high leval API and the result is as expected.\r\n```\r\n450/450 - 36s - loss: 1.9549 - accuracy: 0.2993 - val_loss: 1.7695 - val_accuracy: 0.3776\r\nEpoch 2/5\r\n450/450 - 29s - loss: 1.5775 - accuracy: 0.4314 - val_loss: 1.5351 - val_accuracy: 0.4478\r\nEpoch 3/5\r\n450/450 - 29s - loss: 1.3974 - accuracy: 0.4954 - val_loss: 1.4687 - val_accuracy: 0.4846\r\nEpoch 4/5\r\n450/450 - 30s - loss: 1.2743 - accuracy: 0.5430 - val_loss: 1.3919 - val_accuracy: 0.5096\r\nEpoch 5/5\r\n450/450 - 29s - loss: 1.1646 - accuracy: 0.5820 - val_loss: 1.3872 - val_accuracy: 0.5110\r\n10000/10000 - 3s - loss: 1.4111 - accuracy: 0.5104\r\n```\r\n\r\nI met a bug in our custom model which complainted that `No gradients provided for any variable` and reported the following message:\r\n```\r\nMake sure that we create a new model.\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 107, in <module>\r\n    model = custom_model()\r\n  File \"bug.py\", line 64, in __init__\r\n    self.train_op = optimizer.minimize(c_loss, var_list=[model.trainable_variables])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 319, in minimize\r\n    return self.apply_gradients(grads_and_vars, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 427, in apply_gradients\r\n    grads_and_vars = _filter_grads(grads_and_vars)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 1025, in _filter_grads\r\n    ([v.name for _, v in grads_and_vars],))\r\nValueError: No gradients provided for any variable: ['conv1_conv_1/kernel:0', 'conv1_conv_1/bias:0', ... skip ...,  'post_bn_1/gamma:0', 'post_bn_1/beta:0', 'probs_1/kernel:0', 'probs_1/bias:0'].\r\n```\r\n\r\nFrom this error message, I guess that tensorflow may not get the graph for computing gradient correctly.\r\n\r\nSo I've trid to modify the file `/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py` near line 316:\r\n\r\n```diff\r\n-    grads_and_vars = self._compute_gradients(\r\n-        loss, var_list=var_list, grad_loss=grad_loss)\r\n+    var_list = nest.flatten(var_list)\r\n+    grads = self.get_gradients(loss, var_list)\r\n+    grads_and_vars = list(zip(grads, var_list))\r\n    return self.apply_gradients(grads_and_vars, name=name)\r\n```\r\n\r\nAnd I've also modified my testing case:\r\n\r\n```diff\r\n-        self.train_op = optimizer.minimize(c_loss, var_list=[model.trainable_variables])\r\n+        self.train_op = optimizer.minimize(c_loss(), var_list=[model.trainable_variables])\r\n\r\n```\r\n\r\nFinally, tensorflow can compute gradient. But I got a strange result. \r\n\r\nTraining loss and accuracy look as the same as the normal case.\r\n\r\nBut with comparing the the baseline model, validation and testing loss are not  reasonbale.\r\n\r\n```\r\nMake sure that we create a new model.\r\n10000/10000 - 4s - loss: 10.8579 - accuracy: 0.0995\r\n[Training]Epoch: 1/5 - loss: 1.697 - acc: 0.396\r\n[Training]Epoch: 2/5 - loss: 1.310 - acc: 0.528\r\n[Training]Epoch: 3/5 - loss: 1.105 - acc: 0.609\r\n[Training]Epoch: 4/5 - loss: 0.955 - acc: 0.664\r\n[Training]Epoch: 5/5 - loss: 0.827 - acc: 0.709\r\n[Validation]Epoch: 1/1 - loss: 6545.217 - acc: 0.152\r\n[Testing]Epoch: 1/1 - loss: 2.103 - acc: 0.219\r\n\r\n```\r\n\r\nSo I doubt this modification may not a proper patch.\r\n\r\nNext, I reverted the modification and tried to replace keras.optimizer with `tf.compat.v1.train.AdamOptimizer`:\r\n\r\n```diff\r\n        # using tf.keras.optimizers.Nadam would get error\r\n-        optimizer = tf.keras.optimizers.Nadam(lr=0.0005)\r\n-        #optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\r\n-        self.train_op = optimizer.minimize(c_loss, var_list=[model.trainable_variables])\r\n+        #optimizer = tf.keras.optimizers.Nadam(lr=0.0005)\r\n+        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\r\n+        self.train_op = optimizer.minimize(c_loss(), var_list=[model.trainable_variables])\r\n```\r\n\r\nI still got a strange result. Training loss and accuracy look as the same as the normal case.\r\n\r\nBut validation and testing loss are `nan` and accuracy are `0.1` which means model is not trained.\r\n\r\n It can be shown as the following.\r\n\r\n```\r\nMake sure that we create a new model.\r\n10000/10000 - 3s - loss: 8.4923 - accuracy: 0.1174\r\n[Training]Epoch: 1/5 - loss: 2.546 - acc: 0.198\r\n[Training]Epoch: 2/5 - loss: 1.737 - acc: 0.365\r\n[Training]Epoch: 3/5 - loss: 1.592 - acc: 0.417\r\n[Training]Epoch: 4/5 - loss: 1.385 - acc: 0.496\r\n[Training]Epoch: 5/5 - loss: 1.247 - acc: 0.552\r\n[Validation]Epoch: 1/1 - loss: nan - acc: 0.099\r\n[Testing]Epoch: 1/1 - loss: nan - acc: 0.100\r\n```\r\n\r\nIn conclusion, I think that when eager_mode is disable in tf=2.0.0,\r\n\r\n1. tensorflow throws errors if we use standard optimizer\r\n\r\n2. tensorflow may not get the right gradient if we use `tf.compat.v1.train.AdamOptimizer`\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should work properly.\r\n\r\n**Code to reproduce the issue**\r\n\r\nPlease see the section of **Describe the current behavior**\r\n\r\n**Other info / logs**\r\n\r\nThe following message is the result generated by `tf_env_collect.sh`\r\n```\r\n== check python ===================================================\r\npython version: 3.5.2\r\npython branch:\r\npython build version: ('default', 'Oct  8 2019 13:06:37')\r\npython compiler version: GCC 5.4.0 20160609\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019\r\nos release version: 5.0.0-37-generic\r\nos platform: Linux-5.0.0-37-generic-x86_64-with-Ubuntu-16.04-xenial\r\nlinux distribution: ('Ubuntu', '16.04', 'xenial')\r\nlinux os distribution: ('Ubuntu', '16.04', 'xenial')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='f7f509f1dacf', release='5.0.0-37-generic', version='#40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019', machine='x86_64', processor='x86_64')\r\narchitecture: ('64bit', 'ELF')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy                  1.17.4\r\nprotobuf               3.11.1\r\ntensorflow-estimator   2.0.1\r\ntensorflow-gpu         2.0.0\r\ntensorflow-probability 0.8.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.0.0\r\ntf.version.GIT_VERSION = v2.0.0-rc2-26-g64c3d38\r\ntf.version.COMPILER_VERSION = 7.3.1 20180303\r\nSanity check: array([1], dtype=int32)\r\n       443:     find library=libpthread.so.0 [0]; searching\r\n       443:      search path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64          (LD_LIBRARY_PATH)\r\n       443:       trying file=/usr/local/nvidia/lib/tls/x86_64/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib/tls/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib/x86_64/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib64/tls/x86_64/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib64/tls/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib64/x86_64/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib64/libpthread.so.0\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libpthread.so.0\r\n       443:\r\n       443:     find library=libc.so.6 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libc.so.6\r\n       443:\r\n       443:     find library=libdl.so.2 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libdl.so.2\r\n       443:\r\n       443:     find library=libutil.so.1 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libutil.so.1\r\n       443:\r\n       443:     find library=libexpat.so.1 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libexpat.so.1\r\n       443:\r\n       443:     find library=libz.so.1 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libz.so.1\r\n       443:\r\n       443:     find library=libm.so.6 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libm.so.6\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libpthread.so.0\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libc.so.6\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libm.so.6\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libz.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libexpat.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libutil.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libdl.so.2\r\n       443:\r\n       443:\r\n       443:     initialize program: /usr/local/bin/python\r\n       443:\r\n       443:\r\n       443:     transferring control: /usr/local/bin/python\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_opcode.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libopenblasp-r0-34a18dc3.3.7.so [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/x86_64:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/x86_64/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/x86_64/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:\r\n       443:     find library=libgfortran-ed201abd.so.3.0.0 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs         (RPATH from file /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_tests.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libbz2.so.1.0 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libbz2.so.1.0\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libbz2.so.1.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_bz2.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=liblzma.so.5 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/liblzma.so.5\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/liblzma.so.5\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_lzma.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libmpdec.so.2 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/libmpdec.so.2\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/x86_64-linux-gnu/libmpdec.so.2\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_decimal.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/fft/_pocketfft_internal.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/common.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/bounded_integers.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/mt19937.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/bit_generator.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libcrypto.so.1.0.0 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libcrypto.so.1.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libcrypto.so.1.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_hashlib.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/philox.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/pcg64.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/sfc64.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/generator.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libtensorflow_framework.so.2 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..            (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n       443:\r\n       443:     find library=librt.so.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/librt.so.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../librt.so.1\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/librt.so.1\r\n       443:\r\n       443:     find library=libstdc++.so.6 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libstdc++.so.6\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libstdc++.so.6\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n       443:\r\n       443:     find library=libgcc_s.so.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libgcc_s.so.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libgcc_s.so.1\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libgcc_s.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libgcc_s.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/librt.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n       443:\r\n       443:     find library=libhdfs.so [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..           (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libhdfs.so\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libhdfs.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libhdfs.so\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:      search path=/lib/x86_64-linux-gnu/tls/x86_64:/lib/x86_64-linux-gnu/tls:/lib/x86_64-linux-gnu/x86_64:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/tls/x86_64:/usr/lib/x86_64-linux-gnu/tls:/usr/lib/x86_64-linux-gnu/x86_64:/usr/lib/x86_64-linux-gnu:/lib/tls/x86_64:/lib/tls:/lib/x86_64:/lib:/usr/lib/tls/x86_64:/usr/lib/tls:/usr/lib/x86_64:/usr/lib              (system search path)\r\n       443:       trying file=/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so\r\n       443:       trying file=/lib/x86_64-linux-gnu/tls/libhdfs.so\r\n       443:       trying file=/lib/x86_64-linux-gnu/x86_64/libhdfs.so\r\n       443:       trying file=/lib/x86_64-linux-gnu/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/tls/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/x86_64/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/libhdfs.so\r\n       443:       trying file=/lib/tls/x86_64/libhdfs.so\r\n       443:       trying file=/lib/tls/libhdfs.so\r\n       443:       trying file=/lib/x86_64/libhdfs.so\r\n       443:       trying file=/lib/libhdfs.so\r\n       443:       trying file=/usr/lib/tls/x86_64/libhdfs.so\r\n       443:       trying file=/usr/lib/tls/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64/libhdfs.so\r\n       443:       trying file=/usr/lib/libhdfs.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/fast_tensor_util.so\r\n       443:\r\n       443:     find library=libuuid.so.1 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libuuid.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libuuid.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/wrapt/_wrappers.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_json.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libssl.so.1.0.0 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libssl.so.1.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libssl.so.1.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_ssl.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libhdf5-49599f4e.so.103.0.0 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls:/usr/local/lib/python3.5/dist-packages/h5py/.libs/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/x86_64/libhdf5-49599f4e.so.103.0.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/libhdf5-49599f4e.so.103.0.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/x86_64/libhdf5-49599f4e.so.103.0.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0\r\n       443:\r\n       443:     find library=libhdf5_hl-db841637.so.100.1.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs          (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1\r\n       443:\r\n       443:     find library=libsz-1c7dd0cf.so.2.0.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls:/usr/local/lib/python3.5/dist-packages/h5py/.libs/./x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/x86_64/libsz-1c7dd0cf.so.2.0.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/libsz-1c7dd0cf.so.2.0.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./x86_64/libsz-1c7dd0cf.so.2.0.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1\r\n       443:\r\n       443:     find library=libaec-2147abcd.so.0.0.4 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4\r\n       443:\r\n       443:     find library=libz-a147dcb0.so.1.2.3 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       452:     find library=libc.so.6 [0]; searching\r\n       452:      search path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64          (LD_LIBRARY_PATH)\r\n       452:       trying file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib/tls/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib/x86_64/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib64/tls/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib64/x86_64/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib64/libc.so.6\r\n       452:      search cache=/etc/ld.so.cache\r\n       452:       trying file=/lib/x86_64-linux-gnu/libc.so.6\r\n       452:\r\n       452:\r\n       452:     calling init: /lib/x86_64-linux-gnu/libc.so.6\r\n       452:\r\n       452:\r\n       452:     initialize program: /bin/sh\r\n       452:\r\n       452:\r\n       452:     transferring control: /bin/sh\r\n       452:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5z.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5a.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5s.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5p.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5ac.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_proxy.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5d.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5ds.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5f.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5g.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5i.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5fd.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5pl.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5o.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5l.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/_lib/_ccallback_c.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_sparsetools.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_csparsetools.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_tools.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_traversal.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_reordering.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libjpeg-3b10b538.so.9.3.0 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls:/usr/local/lib/python3.5/dist-packages/PIL/.libs/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/x86_64/libjpeg-3b10b538.so.9.3.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/libjpeg-3b10b538.so.9.3.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/x86_64/libjpeg-3b10b538.so.9.3.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0\r\n       443:\r\n       443:     find library=libopenjp2-b3d7668a.so.2.3.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs           (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1\r\n       443:\r\n       443:     find library=libtiff-8267adfe.so.5.4.0 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs           (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0\r\n       443:\r\n       443:     find library=liblzma-6cd627ed.so.5.2.4 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls:/usr/local/lib/python3.5/dist-packages/PIL/.libs/./x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/.            (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/x86_64/liblzma-6cd627ed.so.5.2.4\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/liblzma-6cd627ed.so.5.2.4\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./x86_64/liblzma-6cd627ed.so.5.2.4\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libopenblasp-r0-2ecf47d5.3.7.dev.so [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/x86_64:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/x86_64/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/x86_64/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flinalg.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/bin/python [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libexpat.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libz.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_opcode.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_tests.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_bz2.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libbz2.so.1.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_lzma.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/liblzma.so.5 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_decimal.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/x86_64-linux-gnu/libmpdec.so.2 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/fft/_pocketfft_internal.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/common.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/bounded_integers.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/mt19937.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/bit_generator.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_hashlib.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/philox.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/pcg64.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/sfc64.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/generator.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/fast_tensor_util.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libuuid.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/wrapt/_wrappers.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_json.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_ssl.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libssl.so.1.0.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libcrypto.so.1.0.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5z.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5a.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5s.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5p.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5ac.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_proxy.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5d.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5ds.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5f.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5g.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5i.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5fd.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5pl.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5o.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5l.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/_lib/_ccallback_c.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_sparsetools.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_csparsetools.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_tools.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_traversal.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_reordering.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flinalg.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/x86_64-linux-gnu/libstdc++.so.6 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libgcc_s.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libm.so.6 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]\r\n       443:\r\n\r\n```", "comments": ["I have tried on colab with TF version 2.0 ,2.1.0-rc1 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/24cfd9e1268da2369ec3a75b76dbac4a/untitled472.ipynb). Thanks!", "Hi @ravikyram,\r\n\r\nThanks you for help me on reproducing this issues in gist.\r\n\r\nI read your gist and noticed that the second example which uses `tf.compat.v1.train.AdamOptimizer` throws error instead of unreasonable loss.\r\n\r\nThe main reason is standard optimizers in tf=2.0.0 requires `callable` object. [doc](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Nadam)\r\n\r\n```python\r\nself.train_op = optimizer.minimize(c_loss, var_list=[model.trainable_variables])\r\n```\r\n\r\nBut optimizers in tf=1.x requires `tensor` object. [doc](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdamOptimizer)\r\n\r\n\r\n```python\r\nself.train_op = optimizer.minimize(c_loss(), var_list=[model.trainable_variables])\r\n```\r\n\r\nThis pitfall has been addressed in different issue.\r\n\r\nSo, bracket `()` is missing in the second example in your gist.\r\n\r\nAfter adding `()`, `c_loss()` become the `tensor` object and tensorflow would not throw error.\r\n\r\nFinally, you will see the strange loss in the testcase.", "From the previous discussion, I am afraid that the testcast for showing strange loss may not clear enough.\r\n\r\nSo, I rewrote the testcase. Surprisely, the root cause may be the `tf.keras.backend.learning_phase(): 0`\r\n\r\nThe following is the testcase for showing strange loss which uses `tf.compat.v1.train.AdamOptimizer`:\r\n\r\n```python\r\n#%%\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n#tf.compat.v1.disable_v2_behavior()\r\n\r\nimport numpy as np\r\n\r\nbatch_size = 100\r\n\r\ndef download_data():\r\n\r\n    # get raw data\r\n    (trainX, trainY), (testX, testY) = tf.keras.datasets.cifar10.load_data()\r\n    trainX = trainX.astype(np.float32)\r\n    testX  = testX.astype(np.float32)\r\n\r\n    # ont-hot\r\n    trainY = tf.keras.utils.to_categorical(trainY, 10)\r\n    testY  = tf.keras.utils.to_categorical(testY , 10)\r\n\r\n    # get validation sets\r\n    training_size = 45000\r\n    validX = trainX[training_size:,:]\r\n    validY = trainY[training_size:,:]\r\n\r\n    trainX = trainX[:training_size,:]\r\n    trainY = trainY[:training_size,:]\r\n\r\n    return trainX, trainY, validX, validY, testX, testY\r\n\r\ndef data_pipeline(dataX, dataY):\r\n\r\n        dataset = tf.data.Dataset.from_tensor_slices( (dataX, dataY) )\r\n        dataset = dataset.shuffle(batch_size * 8)\r\n        dataset = dataset.repeat()\r\n        dataset = dataset.batch(batch_size)\r\n        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n        return dataset\r\n\r\nclass custom_model():\r\n    def __init__(self):\r\n\r\n        def Acc():\r\n            acc = tf.keras.metrics.categorical_accuracy(label_ref, clf_out)\r\n            return tf.math.reduce_mean(acc)\r\n\r\n        def c_loss():\r\n            loss = tf.keras.losses.categorical_crossentropy(label_ref, clf_out)\r\n            loss = tf.math.reduce_mean(loss)\r\n            return loss\r\n\r\n        # create model\r\n        clf_input = tf.keras.layers.Input(shape=(32,32,3), name=\"model/input\")\r\n        model = tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, weights=None, input_tensor=clf_input, pooling='max', classes=10)\r\n        #model = tf.keras.applications.vgg16.VGG16(include_top=True, weights=None, input_tensor=clf_input, pooling='max', classes=10)\r\n        model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\r\n\r\n        label_ref = tf.keras.layers.Input(shape=(10,) , name='label_ref')\r\n        clf_out = model(clf_input)\r\n\r\n        # using tf.keras.optimizers.Nadam would get error\r\n        #optimizer = tf.keras.optimizers.Nadam(lr=0.0005)\r\n        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\r\n        self.train_op = optimizer.minimize(c_loss(), var_list=[model.trainable_variables])\r\n\r\n        self.clf_model = model\r\n        self.clf_input = clf_input\r\n        self.label_ref = label_ref\r\n        self.op_acc = Acc()\r\n        self.c_loss = c_loss()\r\n\r\nif __name__ == '__main__':\r\n\r\n    # set GPU\r\n    import os\r\n    if os.environ.get(\"CUDA_VISIBLE_DEVICES\") is None:\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\n    # reset tf session\r\n    tf.compat.v1.keras.backend.clear_session()\r\n    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\r\n    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\r\n    tf.compat.v1.keras.backend.set_session(sess) \r\n\r\n    # prepare data\r\n    trainX, trainY, validX, validY, testX, testY = download_data()\r\n    train_gen = data_pipeline(trainX, trainY)\r\n    valid_gen = data_pipeline(validX, validY)\r\n    test_gen = data_pipeline(testX, testY)\r\n\r\n    # build targeted model\r\n    model = tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, weights=None, input_shape=(32,32,3), pooling='max', classes=10)\r\n    #model = tf.keras.applications.vgg16.VGG16(include_top=True, weights=None, input_shape=(32,32,3), pooling=None, classes=10)\r\n    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\r\n\r\n    # fit and evalutate\r\n    model.fit(train_gen,\r\n            steps_per_epoch = trainY.shape[0] // batch_size,\r\n            validation_data = valid_gen,\r\n            validation_steps= validY.shape[0] // batch_size,\r\n            epochs=5,\r\n            verbose=2)\r\n    model.evaluate(testX, testY, verbose=2, batch_size=batch_size)\r\n\r\n    # create a new model\r\n    print('Make sure that we create a new model.')\r\n    model = custom_model()\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    model.clf_model.evaluate(testX, testY, verbose=2, batch_size=batch_size)\r\n\r\n    # train model\r\n    num_epoch = 5\r\n    total_len = trainY.shape[0] // batch_size\r\n    tf_iter = tf.compat.v1.data.make_initializable_iterator(train_gen)\r\n    tf_next = tf_iter.get_next()\r\n    sess.run(tf_iter.initializer)\r\n    for epoch in range(num_epoch):\r\n        c_loss, acc = 0.0, 0.0\r\n        for ii in range(total_len):\r\n            X, Y = sess.run(tf_next)\r\n            [b_c_loss, b_acc, _] = sess.run([model.c_loss, model.op_acc, model.train_op],\r\n                                                feed_dict={ model.clf_input: X,\r\n                                                            model.label_ref: Y,\r\n                                                            tf.keras.backend.learning_phase(): 1})\r\n            c_loss = c_loss + b_c_loss\r\n            acc = acc + b_acc\r\n        \r\n        c_loss = c_loss / total_len\r\n        acc = acc / total_len\r\n        print('[Training]Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss, acc) )\r\n\r\n    print('Show loss and accuracy with keras API')\r\n    model.clf_model.evaluate(trainX, trainY, verbose=2, batch_size=batch_size)\r\n    model.clf_model.evaluate(validX, validY, verbose=2, batch_size=batch_size)\r\n    model.clf_model.evaluate(testX, testY, verbose=2, batch_size=batch_size)\r\n\r\n    print('Show loss and accuracy with low level API')\r\n    # evaluate\r\n    num_epoch = 1\r\n    total_len = validY.shape[0] // batch_size\r\n    tf_iter = tf.compat.v1.data.make_initializable_iterator(valid_gen)\r\n    tf_next = tf_iter.get_next()\r\n    sess.run(tf_iter.initializer)\r\n    for epoch in range(num_epoch):\r\n        c_loss_t, acc_t, c_loss_f, acc_f = 0.0, 0.0, 0.0, 0.0\r\n        for ii in range(total_len):\r\n            X, Y = sess.run(tf_next)\r\n            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],\r\n                                        feed_dict={ model.clf_input: X,\r\n                                                    model.label_ref: Y,\r\n                                                    tf.keras.backend.learning_phase(): 1})\r\n            c_loss_t = c_loss_t + b_c_loss\r\n            acc_t = acc_t + b_acc\r\n\r\n            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],\r\n                                        feed_dict={ model.clf_input: X,\r\n                                                    model.label_ref: Y,\r\n                                                    tf.keras.backend.learning_phase(): 0})\r\n            c_loss_f = c_loss_f + b_c_loss\r\n            acc_f = acc_f + b_acc\r\n\r\n        c_loss_t = c_loss_t / total_len\r\n        c_loss_f = c_loss_f / total_len\r\n        acc_t = acc_t / total_len\r\n        acc_f = acc_f / total_len\r\n        print('[Validation][learning_phase=1] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_t, acc_t) )\r\n        print('[Validation][learning_phase=0] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_f, acc_f) )\r\n\r\n    # evaluate\r\n    num_epoch = 1\r\n    total_len = testY.shape[0] // batch_size\r\n    tf_iter = tf.compat.v1.data.make_initializable_iterator(test_gen)\r\n    tf_next = tf_iter.get_next()\r\n    sess.run(tf_iter.initializer)\r\n    for epoch in range(num_epoch):\r\n        c_loss_t, acc_t, c_loss_f, acc_f = 0.0, 0.0, 0.0, 0.0\r\n        for ii in range(total_len):\r\n            X, Y = sess.run(tf_next)\r\n            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],\r\n                                        feed_dict={ model.clf_input: X,\r\n                                                    model.label_ref: Y,\r\n                                                    tf.keras.backend.learning_phase(): 1})\r\n            c_loss_t = c_loss_t + b_c_loss\r\n            acc_t = acc_t + b_acc\r\n\r\n            [b_c_loss, b_acc] = sess.run([model.c_loss, model.op_acc],\r\n                                        feed_dict={ model.clf_input: X,\r\n                                                    model.label_ref: Y,\r\n                                                    tf.keras.backend.learning_phase(): 0})\r\n            c_loss_f = c_loss_f + b_c_loss\r\n            acc_f = acc_f + b_acc\r\n\r\n        c_loss_t = c_loss_t / total_len\r\n        c_loss_f = c_loss_f / total_len\r\n        acc_t = acc_t / total_len\r\n        acc_f = acc_f / total_len\r\n        print('[Testing][learning_phase=1] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_t, acc_t) )\r\n        print('[Testing][learning_phase=0] Epoch: {:d}/{:d} - loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_f, acc_f) )\r\n\r\n```\r\n\r\nThe following is the result of this testing case:\r\n\r\n```\r\n450/450 - 39s - loss: 1.9658 - accuracy: 0.2993 - val_loss: 1.7215 - val_accuracy: 0.3738\r\nEpoch 2/5\r\n450/450 - 28s - loss: 1.5722 - accuracy: 0.4334 - val_loss: 1.5897 - val_accuracy: 0.4152\r\nEpoch 3/5\r\n450/450 - 27s - loss: 1.3876 - accuracy: 0.4993 - val_loss: 1.4867 - val_accuracy: 0.4770\r\nEpoch 4/5\r\n450/450 - 28s - loss: 1.2564 - accuracy: 0.5477 - val_loss: 1.3498 - val_accuracy: 0.5060\r\nEpoch 5/5\r\n450/450 - 27s - loss: 1.1488 - accuracy: 0.5888 - val_loss: 1.3380 - val_accuracy: 0.5232\r\n10000/10000 - 3s - loss: 1.3523 - accuracy: 0.5289\r\nMake sure that we create a new model.\r\n10000/10000 - 3s - loss: 10.2004 - accuracy: 0.1048\r\n[Training]Epoch: 1/5 - loss: 2.288 - acc: 0.268\r\n[Training]Epoch: 2/5 - loss: 1.513 - acc: 0.448\r\n[Training]Epoch: 3/5 - loss: 1.285 - acc: 0.537\r\n[Training]Epoch: 4/5 - loss: 1.426 - acc: 0.487\r\n[Training]Epoch: 5/5 - loss: 1.306 - acc: 0.535\r\nShow loss and accuracy with keras API\r\n45000/45000 - 9s - loss: nan - accuracy: 0.1002\r\n5000/5000 - 1s - loss: nan - accuracy: 0.0986\r\n10000/10000 - 2s - loss: nan - accuracy: 0.1000\r\nShow loss and accuracy with low level API\r\n[Validation][learning_phase=1] Epoch: 1/1 - loss: 1.163 - acc: 0.585\r\n[Validation][learning_phase=0] Epoch: 1/1 - loss: nan - acc: 0.099\r\n[Testing][learning_phase=1] Epoch: 1/1 - loss: 1.179 - acc: 0.587\r\n[Testing][learning_phase=0] Epoch: 1/1 - loss: nan - acc: 0.100\r\n```\r\n\r\nObviously, after training custom model with low level API, the result would be wrong when setting `tf.keras.backend.learning_phase(): 0`\r\n\r\nAlso, the result from keras API is wrong too.\r\n\r\n`tf.keras.backend.learning_phase(): 0` may affect the behavior of `tf.keras.layers.BatchNormalization()` but I'm not sure whether this is root cause.\r\n\r\nI will test another custom model without `tf.keras.layers.BatchNormalization()`.\r\n\r\nIn conclusion, I think that `No gradients provided for any variable` issue and strange loss issue are two separate issues.\r\n\r\nSo, should I close this issue and re-summit two issues separately?", "@CNOCycle Agree. I think it is better to create two separate issues so that users will follow them clearly. Please also mention why you want to disable eager. You could use @tf.function to run specific functions in graph mode. Thanks!", "@jvishnuvardhan thank your reply.\r\n\r\nIn the beginning, our projection was running in tf=1.13.1. \r\n\r\nWe got a uneasy fixed bug by ourselve. According to the disscustion from community, this bug has been fixed in \"the latest\" version.\r\n\r\nAfter we upgraded tf vsrion to 1.14, 1.15 and 2.0(eager_mode=disable), the original bug has been fixed but we still got different bugs unfortunately.\r\n\r\nFrom the official announcement, tf=1.x will not be maintained in the future so we plan a scheduler for upgrading tf=1.x to tf=2.x(eager_mode=enable).\r\n\r\nHowever, our model and training procedure is highly custom so we can't upgrade to tf=2.x(eager_mode=enable) immediately.\r\n\r\nFrom the [offical suggestion](https://www.tensorflow.org/guide/upgrade), user should do **test with v1.disable_v2_behavior in TsnsorFlow 2.0**\r\n\r\nSo that is why we want to run our code in tf=2.x(eager_mode=disenable). If eager_mode is enable, our code would be broken.\r\n\r\nCurrently, our testing matrix is [(eager_mode=disenable, v2_behavior=disenable), (eager_mode=disenable, v2_behavior=enable)]\r\n\r\nFor this issues, we can reproduce this bug in both cases v2_behavior=disenable or v2_behavior=enable.", "I'm assuming this has been resolved by the same answer as for \r\nhttps://github.com/tensorflow/tensorflow/issues/35107\r\n\r\nWhere: If you disable eager execution you should be importing `tensorflow.compat.v1 as tf` instead of `tensorflow as tf`, to make sure the APIs & behaviors match?\r\n\r\n(Feel free to re-open if that's not the case!)", "After carefully checking, this issues may not be solved correctly if BatchNorm layer is included in the model.\r\n\r\nPlease read the thread in https://github.com/tensorflow/tensorflow/issues/35107#issuecomment-599207167", "@CNOCycle Is this still an issue for you? Can you please follow @tomerk suggestion and close the issue if this was resolved for you. Also, try with most recent `tf-version` and stable version. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35050\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35050\">No</a>\n"]}, {"number": 35049, "title": "tf.dataset may out of memory", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 in Docker\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 10.0 / 7\r\n- GPU model and memory: GTX 1080Ti / 11175MiB\r\n\r\n**Describe the current behavior**\r\n\r\nHi authors and developers,\r\n\r\nI am developing our project in tf=2.0.0 and eager_mode is disable.\r\n\r\nThe main reason is tf=1.x will not be maintained but third party libraries have not been ready for tf=2.0 yet.\r\n\r\nFor some resaons, we have to re-generate `trainX` at the end of each epoch in our custom model.\r\n\r\nIn tf=1.x version, tensorflow provides `placeholder` API so we can feed new `trainX` to `tf.data` and it works very well.\r\n\r\nHowever, `placeholder` API is deprecated in tf=2.0 or above.\r\n\r\nI have to re-generate `tf.data` again and again at the end of each epoch.\r\n\r\nFinally, our program will be killed eventually because it is out of memory.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should work properly.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n#%%\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n#tf.compat.v1.disable_v2_behavior()\r\n\r\nimport numpy as np\r\n\r\nbatch_size = 100\r\n\r\ndef download_data():\r\n\r\n    # get raw data\r\n    (trainX, trainY), (testX, testY) = tf.keras.datasets.cifar10.load_data()\r\n    trainX = trainX.astype(np.float32)\r\n    testX  = testX.astype(np.float32)\r\n\r\n    # ont-hot\r\n    trainY = tf.keras.utils.to_categorical(trainY, 10)\r\n    testY  = tf.keras.utils.to_categorical(testY , 10)\r\n\r\n    # get validation sets\r\n    training_size = 45000\r\n    validX = trainX[training_size:,:]\r\n    validY = trainY[training_size:,:]\r\n\r\n    trainX = trainX[:training_size,:]\r\n    trainY = trainY[:training_size,:]\r\n\r\n    return trainX, trainY, validX, validY, testX, testY\r\n\r\ndef data_pipeline(dataX, dataY):\r\n\r\n    # create dataset API\r\n    def preprocess_fn(dataX, dataY):\r\n        \r\n        dataX = tf.image.random_flip_left_right(dataX)\r\n        return dataX, dataY\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices( (dataX, dataY) )\r\n    dataset = dataset.shuffle(batch_size * 8)\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.batch(batch_size)\r\n    dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n    return dataset\r\n\r\nif __name__ == '__main__':\r\n\r\n    # set GPU\r\n    import os\r\n    if os.environ.get(\"CUDA_VISIBLE_DEVICES\") is None:\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\n    # reset tf session\r\n    tf.compat.v1.keras.backend.clear_session()\r\n    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\r\n    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\r\n    tf.compat.v1.keras.backend.set_session(sess) \r\n\r\n    # prepare data\r\n    trainX, trainY, validX, validY, testX, testY = download_data()\r\n    train_gen = data_pipeline(trainX, trainY)\r\n    valid_gen = data_pipeline(validX, validY)\r\n    test_gen = data_pipeline(testX, testY)\r\n\r\n    # build targeted model\r\n    model = tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, weights=None, input_shape=(32,32,3), pooling='max', classes=10)\r\n    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\r\n\r\n    # fit and evalutate\r\n    num_epoch = 20\r\n    for ii in range(num_epoch):\r\n        model.fit(train_gen,\r\n                steps_per_epoch = trainY.shape[0] // batch_size,\r\n                validation_data = valid_gen,\r\n                validation_steps= validY.shape[0] // batch_size,\r\n                epochs=1,\r\n                verbose=2)\r\n        model.evaluate(testX, testY, verbose=2, batch_size=batch_size)\r\n\r\n        # update trainX and re-generate train_gen\r\n        trainX = trainX + 0\r\n        train_gen = data_pipeline(trainX, trainY)\r\n```\r\n\r\nThe following is the output:\r\n```\r\n450/450 - 37s - loss: 1.9472 - accuracy: 0.3077 - val_loss: 1.7661 - val_accuracy: 0.3764\r\n10000/10000 - 3s - loss: 1.7696 - accuracy: 0.3729\r\nTrain on 450 steps, validate on 50 steps\r\n450/450 - 37s - loss: 1.5704 - accuracy: 0.4347 - val_loss: 1.6101 - val_accuracy: 0.4224\r\n10000/10000 - 3s - loss: 1.6036 - accuracy: 0.4274\r\nTrain on 450 steps, validate on 50 steps\r\n450/450 - 37s - loss: 1.4119 - accuracy: 0.4903 - val_loss: 1.4621 - val_accuracy: 0.4728\r\n10000/10000 - 3s - loss: 1.4667 - accuracy: 0.4759\r\nTrain on 450 steps, validate on 50 steps\r\n450/450 - 38s - loss: 1.3042 - accuracy: 0.5313 - val_loss: 1.3688 - val_accuracy: 0.5060\r\n10000/10000 - 3s - loss: 1.3773 - accuracy: 0.5024\r\nTrain on 450 steps, validate on 50 steps\r\n450/450 - 36s - loss: 1.2168 - accuracy: 0.5671 - val_loss: 1.3069 - val_accuracy: 0.5330\r\n10000/10000 - 3s - loss: 1.3197 - accuracy: 0.5284\r\nTrain on 450 steps, validate on 50 steps\r\n450/450 - 36s - loss: 1.1384 - accuracy: 0.5935 - val_loss: 1.2692 - val_accuracy: 0.5462\r\n10000/10000 - 3s - loss: 1.2831 - accuracy: 0.5437\r\nTrain on 450 steps, validate on 50 steps\r\n450/450 - 36s - loss: 1.0762 - accuracy: 0.6156 - val_loss: 1.3297 - val_accuracy: 0.5320\r\n10000/10000 - 3s - loss: 1.3435 - accuracy: 0.5324\r\nTrain on 450 steps, validate on 50 steps\r\n450/450 - 38s - loss: 1.0080 - accuracy: 0.6396 - val_loss: 1.3039 - val_accuracy: 0.5404\r\n10000/10000 - 3s - loss: 1.3260 - accuracy: 0.5351\r\nTrain on 450 steps, validate on 50 steps\r\n450/450 - 37s - loss: 0.9562 - accuracy: 0.6609 - val_loss: 1.1603 - val_accuracy: 0.5926\r\n10000/10000 - 3s - loss: 1.1833 - accuracy: 0.5848\r\nTrain on 450 steps, validate on 50 steps\r\n450/450 - 38s - loss: 0.8957 - accuracy: 0.6823 - val_loss: 1.2314 - val_accuracy: 0.5728\r\n10000/10000 - 3s - loss: 1.2559 - accuracy: 0.5720\r\nKilled\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe following message is the result generated by `tf_env_collect.sh`\r\n```\r\n== check python ===================================================\r\npython version: 3.5.2\r\npython branch:\r\npython build version: ('default', 'Oct  8 2019 13:06:37')\r\npython compiler version: GCC 5.4.0 20160609\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019\r\nos release version: 5.0.0-37-generic\r\nos platform: Linux-5.0.0-37-generic-x86_64-with-Ubuntu-16.04-xenial\r\nlinux distribution: ('Ubuntu', '16.04', 'xenial')\r\nlinux os distribution: ('Ubuntu', '16.04', 'xenial')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='f7f509f1dacf', release='5.0.0-37-generic', version='#40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019', machine='x86_64', processor='x86_64')\r\narchitecture: ('64bit', 'ELF')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy                  1.17.4\r\nprotobuf               3.11.1\r\ntensorflow-estimator   2.0.1\r\ntensorflow-gpu         2.0.0\r\ntensorflow-probability 0.8.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.0.0\r\ntf.version.GIT_VERSION = v2.0.0-rc2-26-g64c3d38\r\ntf.version.COMPILER_VERSION = 7.3.1 20180303\r\nSanity check: array([1], dtype=int32)\r\n       443:     find library=libpthread.so.0 [0]; searching\r\n       443:      search path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64          (LD_LIBRARY_PATH)\r\n       443:       trying file=/usr/local/nvidia/lib/tls/x86_64/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib/tls/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib/x86_64/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib64/tls/x86_64/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib64/tls/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib64/x86_64/libpthread.so.0\r\n       443:       trying file=/usr/local/nvidia/lib64/libpthread.so.0\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libpthread.so.0\r\n       443:\r\n       443:     find library=libc.so.6 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libc.so.6\r\n       443:\r\n       443:     find library=libdl.so.2 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libdl.so.2\r\n       443:\r\n       443:     find library=libutil.so.1 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libutil.so.1\r\n       443:\r\n       443:     find library=libexpat.so.1 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libexpat.so.1\r\n       443:\r\n       443:     find library=libz.so.1 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libz.so.1\r\n       443:\r\n       443:     find library=libm.so.6 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libm.so.6\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libpthread.so.0\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libc.so.6\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libm.so.6\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libz.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libexpat.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libutil.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libdl.so.2\r\n       443:\r\n       443:\r\n       443:     initialize program: /usr/local/bin/python\r\n       443:\r\n       443:\r\n       443:     transferring control: /usr/local/bin/python\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_opcode.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libopenblasp-r0-34a18dc3.3.7.so [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/x86_64:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/x86_64/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/x86_64/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:\r\n       443:     find library=libgfortran-ed201abd.so.3.0.0 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs         (RPATH from file /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_tests.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libbz2.so.1.0 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libbz2.so.1.0\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libbz2.so.1.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_bz2.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=liblzma.so.5 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/liblzma.so.5\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/liblzma.so.5\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_lzma.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libmpdec.so.2 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/libmpdec.so.2\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/x86_64-linux-gnu/libmpdec.so.2\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_decimal.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/fft/_pocketfft_internal.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/common.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/bounded_integers.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/mt19937.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/bit_generator.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libcrypto.so.1.0.0 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libcrypto.so.1.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libcrypto.so.1.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_hashlib.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/philox.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/pcg64.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/sfc64.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/numpy/random/generator.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libtensorflow_framework.so.2 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..            (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../x86_64/libtensorflow_framework.so.2\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n       443:\r\n       443:     find library=librt.so.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/librt.so.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../librt.so.1\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/librt.so.1\r\n       443:\r\n       443:     find library=libstdc++.so.6 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libstdc++.so.6\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libstdc++.so.6\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n       443:\r\n       443:     find library=libgcc_s.so.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libgcc_s.so.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libgcc_s.so.1\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libgcc_s.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libgcc_s.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/librt.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n       443:\r\n       443:     find library=libhdfs.so [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..           (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libhdfs.so\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             (RPATH from file /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libhdfs.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libhdfs.so\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:      search path=/lib/x86_64-linux-gnu/tls/x86_64:/lib/x86_64-linux-gnu/tls:/lib/x86_64-linux-gnu/x86_64:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/tls/x86_64:/usr/lib/x86_64-linux-gnu/tls:/usr/lib/x86_64-linux-gnu/x86_64:/usr/lib/x86_64-linux-gnu:/lib/tls/x86_64:/lib/tls:/lib/x86_64:/lib:/usr/lib/tls/x86_64:/usr/lib/tls:/usr/lib/x86_64:/usr/lib              (system search path)\r\n       443:       trying file=/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so\r\n       443:       trying file=/lib/x86_64-linux-gnu/tls/libhdfs.so\r\n       443:       trying file=/lib/x86_64-linux-gnu/x86_64/libhdfs.so\r\n       443:       trying file=/lib/x86_64-linux-gnu/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/tls/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/x86_64/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64-linux-gnu/libhdfs.so\r\n       443:       trying file=/lib/tls/x86_64/libhdfs.so\r\n       443:       trying file=/lib/tls/libhdfs.so\r\n       443:       trying file=/lib/x86_64/libhdfs.so\r\n       443:       trying file=/lib/libhdfs.so\r\n       443:       trying file=/usr/lib/tls/x86_64/libhdfs.so\r\n       443:       trying file=/usr/lib/tls/libhdfs.so\r\n       443:       trying file=/usr/lib/x86_64/libhdfs.so\r\n       443:       trying file=/usr/lib/libhdfs.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/fast_tensor_util.so\r\n       443:\r\n       443:     find library=libuuid.so.1 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libuuid.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libuuid.so.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/wrapt/_wrappers.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_json.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libssl.so.1.0.0 [0]; searching\r\n       443:      search path=           (LD_LIBRARY_PATH)\r\n       443:      search cache=/etc/ld.so.cache\r\n       443:       trying file=/lib/x86_64-linux-gnu/libssl.so.1.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /lib/x86_64-linux-gnu/libssl.so.1.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/lib/python3.5/lib-dynload/_ssl.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libhdf5-49599f4e.so.103.0.0 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls:/usr/local/lib/python3.5/dist-packages/h5py/.libs/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/x86_64/libhdf5-49599f4e.so.103.0.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/libhdf5-49599f4e.so.103.0.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/x86_64/libhdf5-49599f4e.so.103.0.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0\r\n       443:\r\n       443:     find library=libhdf5_hl-db841637.so.100.1.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs          (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1\r\n       443:\r\n       443:     find library=libsz-1c7dd0cf.so.2.0.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls:/usr/local/lib/python3.5/dist-packages/h5py/.libs/./x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/x86_64/libsz-1c7dd0cf.so.2.0.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/libsz-1c7dd0cf.so.2.0.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./x86_64/libsz-1c7dd0cf.so.2.0.1\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1\r\n       443:\r\n       443:     find library=libaec-2147abcd.so.0.0.4 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4\r\n       443:\r\n       443:     find library=libz-a147dcb0.so.1.2.3 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                (RPATH from file /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       452:     find library=libc.so.6 [0]; searching\r\n       452:      search path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64          (LD_LIBRARY_PATH)\r\n       452:       trying file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib/tls/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib/x86_64/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib64/tls/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib64/x86_64/libc.so.6\r\n       452:       trying file=/usr/local/nvidia/lib64/libc.so.6\r\n       452:      search cache=/etc/ld.so.cache\r\n       452:       trying file=/lib/x86_64-linux-gnu/libc.so.6\r\n       452:\r\n       452:\r\n       452:     calling init: /lib/x86_64-linux-gnu/libc.so.6\r\n       452:\r\n       452:\r\n       452:     initialize program: /bin/sh\r\n       452:\r\n       452:\r\n       452:     transferring control: /bin/sh\r\n       452:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5z.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5a.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5s.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5p.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5ac.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/_proxy.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5d.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5ds.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5f.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5g.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5i.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5fd.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5pl.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5o.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/h5py/h5l.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/_lib/_ccallback_c.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_sparsetools.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_csparsetools.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_tools.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_traversal.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_reordering.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libjpeg-3b10b538.so.9.3.0 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls:/usr/local/lib/python3.5/dist-packages/PIL/.libs/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/x86_64/libjpeg-3b10b538.so.9.3.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/libjpeg-3b10b538.so.9.3.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/x86_64/libjpeg-3b10b538.so.9.3.0\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0\r\n       443:\r\n       443:     find library=libopenjp2-b3d7668a.so.2.3.1 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs           (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1\r\n       443:\r\n       443:     find library=libtiff-8267adfe.so.5.4.0 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs           (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0\r\n       443:\r\n       443:     find library=liblzma-6cd627ed.so.5.2.4 [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls:/usr/local/lib/python3.5/dist-packages/PIL/.libs/./x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/.            (RPATH from file /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/x86_64/liblzma-6cd627ed.so.5.2.4\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/liblzma-6cd627ed.so.5.2.4\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./x86_64/liblzma-6cd627ed.so.5.2.4\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:     find library=libopenblasp-r0-2ecf47d5.3.7.dev.so [0]; searching\r\n       443:      search path=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/x86_64:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs            (RPATH from file /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so)\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/x86_64/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/x86_64/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:       trying file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flinalg.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so\r\n       443:\r\n       443:\r\n       443:     calling init: /usr/local/lib/python3.5/dist-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/bin/python [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libexpat.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libz.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_opcode.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_tests.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_bz2.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libbz2.so.1.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_lzma.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/liblzma.so.5 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_decimal.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/x86_64-linux-gnu/libmpdec.so.2 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/fft/_pocketfft_internal.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/common.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/bounded_integers.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/mt19937.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/bit_generator.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_hashlib.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/philox.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/pcg64.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/sfc64.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/random/generator.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/fast_tensor_util.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libuuid.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/wrapt/_wrappers.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_json.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/python3.5/lib-dynload/_ssl.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libssl.so.1.0.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libcrypto.so.1.0.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5z.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5a.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5s.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5p.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5ac.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/_proxy.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5d.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5ds.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5f.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5g.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5i.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5fd.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5pl.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5o.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/h5l.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/_lib/_ccallback_c.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_sparsetools.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/_csparsetools.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_tools.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_traversal.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_reordering.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_flinalg.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /usr/lib/x86_64-linux-gnu/libstdc++.so.6 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libgcc_s.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libm.so.6 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]\r\n       443:\r\n       443:\r\n       443:     calling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]\r\n       443:\r\n\r\n```", "comments": ["Issue replicating for TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/374c28e456d3f4163f45a57c55c6ea0c/35049.ipynb).Thanks!", "You are (likely) running out of memory because every call to `data_pipeline` will embed the `dataX` and `dataY` inputs into the graph (as constant inputs to `from_tensor_slices`).\r\n\r\nWhat does your \"update\" to `trainX` do? (In your repro, you are mocking this out using `+ 0`). Can you provide an example that uses a placeholder that does not have the memory issue?\r\n\r\nTaking a step back, you seem to try to avoid reliance on the placeholder API because it is deprecated in TF 2 but your repro program still uses `Session` which is also deprecated.\r\n\r\nMy recommendation would be to write your program in idiomatic TF 2 style using eager mode and use `tf.function` when efficient graph execution is needed? For instance, creating the dataset in eager mode will result in proper garbage collection of the objects once they are not references (as opposed to them being accumulated in the default graph which is what's going on in your program).", "@jsimsa thank you for your reply.\r\n\r\n> What does your \"update\" to trainX do?\r\n\r\nThe update means that `trainX` at each epoch would slightly different. Imagine that this update could be simplified as `trainX = trainX + random_noise` because the detail of the update is not the point in this issue.\r\n\r\n> Can you provide an example that uses a placeholder that does not have the memory issue?\r\n\r\nI will provide example code soon.\r\n\r\n> My recommendation would be to write your program in idiomatic TF 2 style using eager mode and use tf.function when efficient graph execution is needed\r\n\r\nThank you for your suggestion. Unfortunately, a lot of third party libraries have not been ready for tf=2.0 yet. So many papers' author released their code using TF1. Currently, if eager_mode is enabled, our program would broken totally. That is why we want to remove deprecated API step by step.\r\n\r\n---\r\nI want to know whether we can forcibly do garbage collection and this issues would be fixed in the future or not.\r\n\r\nThanks\r\n", "> Can you provide an example that uses a placeholder that does not have the memory issue?\r\n\r\nThe following is the minimal test case:\r\n\r\n```python\r\n#%%\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n#tf.compat.v1.disable_v2_behavior()\r\n\r\nimport numpy as np\r\n\r\nbatch_size = 100\r\n\r\ndef download_data():\r\n\r\n    # get raw data\r\n    (trainX, trainY), (testX, testY) = tf.keras.datasets.cifar10.load_data()\r\n    trainX = trainX.astype(np.float32)\r\n    testX  = testX.astype(np.float32)\r\n\r\n    # ont-hot\r\n    trainY = tf.keras.utils.to_categorical(trainY, 10)\r\n    testY  = tf.keras.utils.to_categorical(testY , 10)\r\n\r\n    # get validation sets\r\n    training_size = 45000\r\n    validX = trainX[training_size:,:]\r\n    validY = trainY[training_size:,:]\r\n\r\n    trainX = trainX[:training_size,:]\r\n    trainY = trainY[:training_size,:]\r\n\r\n    return trainX, trainY, validX, validY, testX, testY\r\n\r\nclass DataGenerator():\r\n\r\n    def __init__(self, sess, dataX, dataY, batch_size):\r\n\r\n        self.batch_size = batch_size\r\n        self.sess = sess\r\n        self.rawX = dataX\r\n        self.rawY = dataY\r\n\r\n        # create dataset API\r\n        def preprocess_fn(dataX, dataY):\r\n\r\n            dataX = tf.image.random_flip_left_right(dataX)\r\n            return dataX, dataY\r\n\r\n        tf_dataX = tf.compat.v1.placeholder(tf.float32, shape=[None, 32, 32, 3])\r\n        tf_dataY = tf.compat.v1.placeholder(tf.float32, shape=[None, 10])\r\n\r\n        dataset = tf.data.Dataset.from_tensor_slices( (tf_dataX, tf_dataY) )\r\n        dataset = dataset.shuffle(self.batch_size * 8)\r\n        dataset = dataset.repeat()\r\n        dataset = dataset.batch(self.batch_size)\r\n        dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\n        tf_iter = tf.compat.v1.data.make_initializable_iterator(dataset)\r\n        tf_next = tf_iter.get_next()\r\n\r\n        self.tf_dataX = tf_dataX\r\n        self.tf_dataY = tf_dataY\r\n        self.dataset   = dataset\r\n        self.tf_iter   = tf_iter\r\n        self.tf_next   = tf_next\r\n\r\n        self.on_epoch_end()\r\n\r\n    def  __len__(self):\r\n\r\n        return self.rand_idx.shape[0] // self.batch_size\r\n\r\n    def __getitem__(self, index):\r\n\r\n        return self.sess.run(self.tf_next)\r\n\r\n    def on_epoch_end(self):\r\n\r\n        dataX  = self.rawX.copy()\r\n        dataY  = self.rawY.copy()\r\n\r\n        # run permutation\r\n        total_len = dataY.shape[0]\r\n        self.rand_idx = np.random.permutation(total_len)\r\n        dataX = dataX[self.rand_idx,:]\r\n        dataY = dataY[self.rand_idx,:]\r\n\r\n        self.sess.run(self.tf_iter.initializer,\r\n                    feed_dict={ self.tf_dataX: dataX,\r\n                                self.tf_dataY: dataY})\r\n\r\ndef data_pipeline(dataX, dataY):\r\n\r\n    # create dataset API\r\n    def preprocess_fn(dataX, dataY):\r\n        \r\n        dataX = tf.image.random_flip_left_right(dataX)\r\n        return dataX, dataY\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices( (dataX, dataY) )\r\n    dataset = dataset.shuffle(batch_size * 8)\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.batch(batch_size)\r\n    dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n    return dataset\r\n\r\nif __name__ == '__main__':\r\n\r\n    # set GPU\r\n    import os\r\n    if os.environ.get(\"CUDA_VISIBLE_DEVICES\") is None:\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\n    # reset tf session\r\n    tf.compat.v1.keras.backend.clear_session()\r\n    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\r\n    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\r\n    tf.compat.v1.keras.backend.set_session(sess) \r\n\r\n    # prepare data\r\n    trainX, trainY, validX, validY, testX, testY = download_data()\r\n    train_gen = DataGenerator(sess, trainX, trainY, batch_size)\r\n    valid_gen = DataGenerator(sess, validX, validY, batch_size)\r\n    test_gen = data_pipeline(testX, testY)\r\n\r\n    # build targeted model\r\n    model = tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, weights=None, input_shape=(32,32,3), pooling='max', classes=10)\r\n    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\r\n\r\n    # fit and evalutate\r\n    num_epoch = 20\r\n    for ii in range(num_epoch):\r\n        model.fit(train_gen.tf_iter,\r\n                steps_per_epoch = trainY.shape[0] // batch_size,\r\n                validation_data = valid_gen.tf_iter,\r\n                validation_steps= validY.shape[0] // batch_size,\r\n                epochs=1,\r\n                verbose=2)\r\n        model.evaluate(testX, testY, verbose=2, batch_size=batch_size)\r\n\r\n        # update trainX and re-generate train_gen\r\n        train_gen.on_epoch_end()\r\n```\r\n\r\nFirst, you will see the custom class `DataGenerator` and the graph is build in function `__init__`. \r\n\r\nI also implemented a function `on_epoch_end`.\r\n\r\nBut I skipped the detail of `update`, I only permute data instead.\r\n\r\n`on_epoch_end` will be called at the end of each epoch(line 141).\r\n\r\n```python\r\n        # update trainX and re-generate train_gen\r\n        train_gen.on_epoch_end()\r\n```\r\n\r\nNext, I ran the test case in the docker. Following is the reproduction step:\r\n\r\n```bash\r\n# pull docker image and run container\r\ndocker pull tensorflow/tensorflow:2.0.0-gpu-py3 & \\\r\ndocker run --runtime=nvidia -it tensorflow/tensorflow:2.0.0-gpu-py3\r\n\r\n# install vim\r\napt update & apt install -y vim\r\n\r\n# paste test code and run\r\nvim test.py\r\npython test.py\r\n```\r\n\r\nIf you run this testing case native, you will get the following error message:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 137, in <module>\r\n    verbose=2)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 708, in fit\r\n    func = self._select_training_loop(x)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 487, in _select_training_loop\r\n    raise ValueError('For performance reasons Keras `fit`, `evaluate` and'\r\nValueError: For performance reasons Keras `fit`, `evaluate` and`predict` accept tf.data `Datasets` as input but not iterators that have been manually generated from Datasets by users. Please directly pass in the original `Dataset` object instead of passing in `iter(dataset)`.\r\n```\r\n\r\nBecause of some reason, tf=1.15.0 or above does not accept `iter` as `model.fit`'s input.\r\n\r\nYou have to comment out this guard in `/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py:487`:\r\n\r\n```diff\r\n  def _select_training_loop(self, inputs):\r\n    \"\"\"Select training loop for fit/eval/predict based on the inputs.\"\"\"\r\n    # TODO(kaftan) or TODO(scottzhu): This check should eventually be nicely\r\n    #  integrated into the data adapters in the v2 loop. We can't do this yet\r\n    #  because we currently have to fall back for unhandled data types.\r\n-    if isinstance(inputs, (iterator_ops.Iterator,\r\n-                           iterator_ops.IteratorV2)):\r\n-      raise ValueError('For performance reasons Keras `fit`, `evaluate` and'\r\n-                       '`predict` accept tf.data `Datasets` as input but not '\r\n-                       'iterators that have been manually generated from '\r\n-                       'Datasets by users. Please directly pass in the '\r\n-                       'original `Dataset` object instead of passing in '\r\n-                       '`iter(dataset)`.')\r\n```\r\n\r\nNow, run the testing case again.\r\n\r\nThe memory stat by `docker stat` is shown in the following:\r\n```\r\n# first epoch\r\nCONTAINER ID        NAME                  CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS\r\n4db26b9b1056        trusting_archimedes   254.10%             4.543GiB / 31.33GiB   14.50%              183MB / 880kB       4.1kB / 20.9MB      44\r\n\r\n# after 19 epochs \r\nCONTAINER ID        NAME                  CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS\r\n4db26b9b1056        trusting_archimedes   161.62%             5.033GiB / 31.33GiB   16.06%              184MB / 1.16MB      8.19kB / 15MB       81\r\n```", "As far as I know, TensorFlow does not provide APIs for garbage collecting parts of a graph.\r\n\r\nIf the `random_noise` could be represented by a variable, what you could do instead of recreating the dataset from scratch every epoch is to add `dataset = dataset.map(apply_noise)` transformation to your input pipeline which accesses the random noise state (e.g. represented as a TF resource variable).\r\n\r\nLast but not least, the `on_epoch_end` method in your example does not seem to be applying random noise. Instead, it seems to be permuting all elements of the training set. The equivalent can be achieved by adding `dataset = dataset.shuffle(num_elements)` transformation in your input pipeline, avoiding the need to creating a copy of the dataset.", "@jsimsa thank you for your reply.\r\n\r\nThe custom `update` would be the important point in this issue actually because we shouldn't re-create resource on the graph. Otherwise, it would run out of memory, this thing couldn't be avoided in graph mode.\r\n\r\nSo, the `update` is a variation of adversarial training. The following is the **pseudo code**:\r\n```python\r\n    def on_epoch_end(self):\r\n\r\n        dataX  = self.rawX.copy()\r\n        dataY  = self.rawY.copy()\r\n\r\n        # generate adversarial data\r\n        eps = 8\r\n        grads = model.get_gradients(loss, dataX)\r\n        AdvX = dataX + eps * np.sign(grads)\r\n\r\n        # combine data\r\n        dataX = np.vstack([dataX, AdvX])\r\n        dataY = np.vstack([dataY, dataY])\r\n\r\n        # run permutation\r\n        total_len = dataY.shape[0]\r\n        self.rand_idx = np.random.permutation(total_len)\r\n        dataX = dataX[self.rand_idx,:]\r\n        dataY = dataY[self.rand_idx,:]\r\n\r\n        self.sess.run(self.tf_iter.initializer,\r\n                    feed_dict={ self.tf_dataX: dataX,\r\n                                self.tf_dataY: dataY})\r\n\r\n```\r\n\r\nThe adversarial data is the original data plus sing of gradient which will be applied in dataX.\r\n```python\r\n        # generate adversarial data\r\n        eps = 8\r\n        grads = model.get_gradients(loss, dataX)\r\n        AdvX = dataX + eps * np.sign(grads)\r\n```\r\n\r\nWe want to generate adversarial data at the end of each epoch and put it into next epoch's training set.\r\n```python\r\n        # combine data\r\n        dataX = np.vstack([dataX, AdvX])\r\n        dataY = np.vstack([dataY, dataY])\r\n```\r\n\r\nCurrently, I don't how to use model and compute gradient in the `dataset` pipeline. My solution is generating adversarial data at the end of  each epoch. This solution finally would run out of memory in graph mode if we do not use `placeholder` API.", "You would still compute gradients at the end of the epoch and store the result in a [tensor variable](https://www.tensorflow.org/api_docs/python/tf/Variable?version=stable).\r\n\r\nHere is a simple snippet that illustrates how to augment an input pipeline with adversarial example computed based on a resource variable that can change between epochs:\r\n\r\n```\r\nimport tensorflow.compat.v2 as tf\r\n\r\ntf.enable_v2_behavior()\r\n\r\ngradients = tf.Variable([0, 0, 0, 0, 0])\r\n\r\nds = tf.data.Dataset.range(5)\r\n\r\ndef map_fn(i):\r\n  return tf.stack([i, i + tf.cast(gradients[i], tf.int64)])\r\n\r\nds = ds.map(map_fn).unbatch()\r\n\r\nfor elem in ds:\r\n  print(elem.numpy())\r\n\r\ngradients.assign([-1, 1, -1, 1, -1])\r\n\r\nfor elem in ds:\r\n  print(elem.numpy())\r\n```\r\n\r\nWhen executed, the above program produces:\r\n\r\n```\r\n0\r\n0\r\n1\r\n1\r\n2\r\n2\r\n3\r\n3\r\n4\r\n4\r\n0\r\n-1\r\n1\r\n2\r\n2\r\n1\r\n3\r\n4\r\n4\r\n3\r\n```", "@jsimsa thank you for your reply.\r\n\r\nTanks for all your replies and suggestions.\r\n\r\nFor TF2, the test case would not cause out of memory issue. We can also use `tf.function` with eager mode very efficiently.\r\n\r\nFor TF1, we have developed an alternative solution before migrating to TF2.\r\n\r\nAs you said, there is not any garbage collection API for graph model in TF1.\r\n\r\nSo, I thought this issue could be closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35049\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35049\">No</a>\n"]}, {"number": 35048, "title": "Multi thread check correction", "body": "Reopened #28149", "comments": ["@ANSHUMAN87 Can you please check reviewer comments and keep us posted. Thanks!", "@multiverse-tf : Thanks for your efforts in reviewing. I have handled one of your comments. For other comments i have replied in line, please check and revert back. Thanks!", "@multiverse-tf : Thanks for all your valuable comments! It has indeed improvised the PR. I have handled all your comments now. Please check and let me know in case any other, TIA!", "Almost there, and many thanks for making this PR!", "@multiverse-tf : All your comments are handled now! Thanks!"]}]