[{"number": 47933, "title": "model.save() outputs unexpected files in SavedModel directory", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): '2.5.0-dev20210318'\r\n- Python version: python 3.8\r\n- CUDA/cuDNN version: cuda11.2; cudnn 8.x\r\n\r\n**Describe the current behavior**\r\n\r\nmodel.save() outputs an extra \"keras_metadata.pb\" file.\r\n\r\n**Standalone code to reproduce the issue**\r\nI wrote some code to load my maskRCNN model, and save it to a SavedModel.\r\n\r\nthis [link](https://drive.google.com/drive/folders/1qf3LaEhu16o6s7fu_X-6qf6M-Dg8Zq3W?usp=sharing) includes: \r\n\r\n1. Model weights: \"mask_rcnn_nucleus_0040_thick_Pf.h5\"\r\n2. My code to load and save the model: \"load_SavedModel.py\"\r\n3. Source code needed to load the maskRCNN model: Folder \"mrcnn\"\r\n4. The SavedModel folder, which is the output of my code: \"thick_Pf_model\"\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47933\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47933\">No</a>\n", "@abattery Can you help me with this one? Sorry to @ you directly, but I don't know why this issue is not assigned.", "Sorry. I am looking at the TFLite product issues only.", "Can you look at this tensorflow lite [issue](https://github.com/tensorflow/tensorflow/issues/48025#issue-839067869)? @abattery ", "@k-w-w Hello, are you investigating this issue?", "@Yu-Hang Yes, `keras_metadata.pb` is saved and is intended behavior. [Here](https://colab.research.google.com/gist/jvishnuvardhan/adacb23fc2ec9ba647a3e74089d1864e/untitled.ipynb) is a gist for your reference.\r\n\r\nThis is a recent addition to the functionality of `model.save`. If you try to save a model load a model without `keras_metadata.pb`, then it will throw a warning as shown below \r\n\r\n#### loading saved model without `keras_metadata.pb`\r\n`loaded_model = tf.keras.models.load_model('MyModel')``\r\n\r\nA warning will be thrown as shown below\r\n\r\n> `WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.`\r\n\r\nSo, `keras_metadata.pb` is intended and not an unexpected output. Thanks!\r\n", "@jvishnuvardhan I see. Thanks. Quick follow up questions:\r\n\r\n1. Does the nightly version still require generating an intermediate SavedModel before converting to a .tflite model?\r\n2. Are support for EfficientDet models included in the nightly version? ", "@Yu-Hang I am not sure whether I understand your question completely.\r\n\r\nAns \r\n\r\n(1) You don't need to save (`model.save`) and load (`load_model`) the model for tflite conversion. You can train a keras model and pass that model to `tf-lite-converter`. If you ran a  large model in cluster or high end GPU, then you can save the model, load the model and use `tf.lite.TFLiteConverter.from_keras_model()` to convert the model.\r\n\r\n(2) I think \"Yes\" but I am not an expert  about this model. \r\n\r\nPlease close the issue. For further questions, please open a new issue with a standalone code to reproduce the issue. Thanks! \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47933\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47933\">No</a>\n"]}, {"number": 47932, "title": "TensorflowLiteObj duplicate symbol", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac Catalina v.10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: TensorFlowLiteObjC\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?: cocoapods\r\n- Bazel version (if compiling from source): Not used\r\n\r\n\r\n**Describe the problem**\r\n\r\nI've tried to follow this [tutorial ](https://www.tensorflow.org/lite/guide/ios)to use tensorflowLite with Objective-C on my mac\r\n\r\nI've inserted the line in my podfile like this:\r\n\r\n` pod 'TensorFlowLiteObjC'`\r\n\r\nThen I run pod install and got the following line:\r\n\r\n```\r\nAnalyzing dependencies\r\nDownloading dependencies\r\nInstalling TensorFlowLiteC (2.4.0)\r\nInstalling TensorFlowLiteObjC (2.4.0)\r\n```\r\n\r\nI build my code with xCode but I have the following error:\r\n\r\n```\r\nduplicate symbol '_TfLiteDelegateCreate' in:\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteIntArrayCopy' in:\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteIntArrayCreate' in:\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteIntArrayEqual' in:\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteIntArrayEqualsArray' in:\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteIntArrayFree' in:\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteIntArrayGetSizeInBytes' in:\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteTensorDataFree' in:\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteTensorFree' in:\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteTensorRealloc' in:\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteTensorReset' in:\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteTypeGetName' in:\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLite/Frameworks/tensorflow_lite.framework/tensorflow_lite(c_api_internal.o)\r\n    /Users/admin/ios/foodgardian/foodgardian_app/ios/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nld: 12 duplicate symbols for architecture x86_64\r\n```\r\n\r\nI've remove the installation line in my podfile and `pod deintegrate`, I try to build and it's working.\r\nI've try to reinstall the lib but same problem again...\r\n\r\n\r\n\r\n", "comments": ["@terryheo could you take a look?", "Hi, there is an update. I've seen on the bottom of the [page](https://www.tensorflow.org/lite/guide/ios) : \r\n\r\n`Note: For CocoaPods developers who want to import the Objective-C TensorFlow Lite module, you must also include use_frameworks! in your Podfile.`\r\n\r\nSo I modified my podfile like this:\r\n\r\n```\r\npost_install do |pi|\r\n  pi.pods_project.targets.each do |t|\r\n    t.build_configurations.each do |config|\r\n      config.build_settings['IPHONEOS_DEPLOYMENT_TARGET'] = '9.0'\r\n    end\r\n  end\r\nend\r\n\r\nplatform :ios, '10.0'\r\n\r\nrequire_relative '../node_modules/react-native-unimodules/cocoapods'\r\nrequire_relative '../node_modules/@react-native-community/cli-platform-ios/native_modules'\r\n\r\nuse_frameworks!\r\n\r\ntarget 'foodguardian' do\r\n  rnPrefix = \"../node_modules/react-native\"\r\n\r\n  # React Native and its dependencies\r\n  pod 'FBLazyVector', :path => \"#{rnPrefix}/Libraries/FBLazyVector\"\r\n  pod 'FBReactNativeSpec', :path => \"#{rnPrefix}/Libraries/FBReactNativeSpec\"\r\n  pod 'RCTRequired', :path => \"#{rnPrefix}/Libraries/RCTRequired\"\r\n  pod 'RCTTypeSafety', :path => \"#{rnPrefix}/Libraries/TypeSafety\"\r\n  pod 'React', :path => \"#{rnPrefix}/\"\r\n  pod 'React-Core', :path => \"#{rnPrefix}/\"\r\n  pod 'React-CoreModules', :path => \"#{rnPrefix}/React/CoreModules\"\r\n  pod 'React-RCTActionSheet', :path => \"#{rnPrefix}/Libraries/ActionSheetIOS\"\r\n  pod 'React-RCTAnimation', :path => \"#{rnPrefix}/Libraries/NativeAnimation\"\r\n  pod 'React-RCTBlob', :path => \"#{rnPrefix}/Libraries/Blob\"\r\n  pod 'React-RCTImage', :path => \"#{rnPrefix}/Libraries/Image\"\r\n  pod 'React-RCTLinking', :path => \"#{rnPrefix}/Libraries/LinkingIOS\"\r\n  pod 'React-RCTNetwork', :path => \"#{rnPrefix}/Libraries/Network\"\r\n  pod 'React-RCTSettings', :path => \"#{rnPrefix}/Libraries/Settings\"\r\n  pod 'React-RCTText', :path => \"#{rnPrefix}/Libraries/Text\"\r\n  pod 'React-RCTVibration', :path => \"#{rnPrefix}/Libraries/Vibration\"\r\n  pod 'React-Core/RCTWebSocket', :path => \"#{rnPrefix}/\"\r\n  pod 'React-Core/DevSupport', :path => \"#{rnPrefix}/\"\r\n  pod 'React-cxxreact', :path => \"#{rnPrefix}/ReactCommon/cxxreact\"\r\n  pod 'React-jsi', :path => \"#{rnPrefix}/ReactCommon/jsi\"\r\n  pod 'React-jsiexecutor', :path => \"#{rnPrefix}/ReactCommon/jsiexecutor\"\r\n  pod 'React-jsinspector', :path => \"#{rnPrefix}/ReactCommon/jsinspector\"\r\n  pod 'React-callinvoker', :path => \"../node_modules/react-native/ReactCommon/callinvoker\"\r\n  pod 'ReactCommon/turbomodule/core', :path => \"#{rnPrefix}/ReactCommon\"\r\n  pod 'Yoga', :path => \"#{rnPrefix}/ReactCommon/yoga\"\r\n  pod 'DoubleConversion', :podspec => \"#{rnPrefix}/third-party-podspecs/DoubleConversion.podspec\"\r\n  pod 'glog', :podspec => \"#{rnPrefix}/third-party-podspecs/glog.podspec\"\r\n  pod 'Folly', :podspec => \"#{rnPrefix}/third-party-podspecs/Folly.podspec\"\r\n  pod 'TensorFlowLiteObjC'\r\n  # Other native modules\r\n\r\n  # Automatically detect installed unimodules\r\n  use_unimodules!\r\n\r\n  # react-native-cli autolinking\r\n  use_native_modules!\r\n\r\nend\r\n\r\n```\r\n\r\nBut by doing this, I've an error at the begining of the build, `UMCore/UMModuleRegistry.h not found`\r\n\r\nI'm a bit lost here", "Hello,\r\n\r\nI was able to find a solution, I had this [module ](https://www.npmjs.com/package/tflite-react-native)installed , I uninstalled it and it worked.\r\n\r\nI think we can close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47932\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47932\">No</a>\n"]}, {"number": 47931, "title": "Failed to convert SparseTensor to Tensor", "body": "My code raises this error message:\r\n\r\n`TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"DeserializeSparse:0\", shape=(None, 2), dtype=int64), values=Tensor(\"DeserializeSparse:1\", shape=(None,), dtype=float32), dense_shape=Tensor(\"stack:0\", shape=(2,), dtype=int64)). Consider casting elements to a supported type.`\r\n\r\nIt is based almost 1:1 on this [Google guide on Text Classification](https://developers.google.com/machine-learning/guides/text-classification) for an ngram-model.\r\n\r\nIt works if I add `.todense()` like this:\r\n```\r\nx_train = vectorizer.fit_transform(train_texts).todense()\r\nx_val = vectorizer.transform(val_texts).todense()\r\n```\r\n\r\nIt seems to be very slow, though. That's why I reduced the number of texts and labels in the example.\r\n\r\nI'm running it in a pipenv environment with Python 3.8.6, TensorFlow 2.4.1, MacOS Big Sur 11.2.3.\r\n\r\n_Originally posted by @DanielMH in https://github.com/tensorflow/tensorflow/issues/42916#issuecomment-802169046_", "comments": ["This is my code:\r\n\r\n```[obsolete]```", "Can you minimize the code snippet to reproduce this? I see third_party lib dependency  and also some data files dependencies.\r\n\r\nIt would ideal that you can reproduce this issue with minimal code surface and dummy input data.", "The following code uses dummy input data now. I have simplified it a bit and can't easily simplify it further. Anyway, if the issue then would disappear, it would seem pointless to me.\r\n\r\n```\r\nimport numpy as np\r\n\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\nfrom sklearn.feature_selection import SelectKBest\r\nfrom sklearn.feature_selection import f_classif\r\n\r\nfrom tensorflow.python.keras import models\r\nfrom tensorflow.python.keras.layers import Dense\r\nfrom tensorflow.python.keras.layers import Dropout\r\n\r\nimport tensorflow as tf\r\n\r\nNGRAM_RANGE = (1, 2)\r\nTOP_K = 20000\r\nTOKEN_MODE = 'word'\r\nMIN_DOCUMENT_FREQUENCY = 2\r\n\r\ndef load_dataset():\r\n    train_texts = [\r\n        \"the movie was great\",\r\n        \"I loved the movie\",\r\n        \"very entertaining\",\r\n        \"best movie ever\",\r\n        \"5 stars\",\r\n        \"awful movie\",\r\n        \"hated it\",\r\n        \"boring\",\r\n        \"stupid story\",\r\n        \"waste of time\"\r\n    ]\r\n    train_labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\r\n    test_texts, test_labels = train_texts, train_labels \r\n\r\n    return ((train_texts, np.array(train_labels)),\r\n            (test_texts, np.array(test_labels)))\r\n\r\ndef ngram_vectorize(train_texts, train_labels, val_texts):\r\n    \"\"\"Vectorizes texts as n-gram vectors.\r\n\r\n    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\r\n\r\n    # Arguments\r\n        train_texts: list, training text strings.\r\n        train_labels: np.ndarray, training labels.\r\n        val_texts: list, validation text strings.\r\n\r\n    # Returns\r\n        x_train, x_val: vectorized training and validation texts\r\n    \"\"\"\r\n    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\r\n    kwargs = {\r\n            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\r\n            'dtype': 'int32',\r\n            'strip_accents': 'unicode',\r\n            'decode_error': 'replace',\r\n            'analyzer': TOKEN_MODE,  # Split text into word tokens.\r\n            'min_df': MIN_DOCUMENT_FREQUENCY,\r\n    }\r\n    vectorizer = TfidfVectorizer(**kwargs)\r\n\r\n    # Learn vocabulary from training texts and vectorize training texts.\r\n    x_train = vectorizer.fit_transform(train_texts) # .todense()\r\n\r\n    # Vectorize validation texts.\r\n    x_val = vectorizer.transform(val_texts) # .todense()\r\n\r\n    # Select top 'k' of the vectorized features.\r\n    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\r\n    selector.fit(x_train, train_labels)\r\n    x_train = selector.transform(x_train).astype('float32')\r\n    x_val = selector.transform(x_val).astype('float32')\r\n    return x_train, x_val\r\n\r\ndef train_ngram_model(data,\r\n                      learning_rate=1e-3,\r\n                      epochs=1000,\r\n                      batch_size=128,\r\n                      layers=2,\r\n                      units=64,\r\n                      dropout_rate=0.2):\r\n    \"\"\"Trains n-gram model on the given dataset.\r\n\r\n    # Arguments\r\n        data: tuples of training and test texts and labels.\r\n        learning_rate: float, learning rate for training model.\r\n        epochs: int, number of epochs.\r\n        batch_size: int, number of samples per batch.\r\n        layers: int, number of `Dense` layers in the model.\r\n        units: int, output dimension of Dense layers in the model.\r\n        dropout_rate: float: percentage of input to drop at Dropout layers.\r\n\r\n    # Raises\r\n        ValueError: If validation data has label values which were not seen\r\n            in the training data.\r\n    \"\"\"\r\n    # Get the data.\r\n    (train_texts, train_labels), (val_texts, val_labels) = data\r\n\r\n    # Vectorize texts.\r\n    x_train, x_val = ngram_vectorize(\r\n        train_texts, train_labels, val_texts)\r\n\r\n    # Create model instance.\r\n    op_units, op_activation = 1, 'sigmoid'\r\n    model = models.Sequential()\r\n    model.add(Dropout(rate=dropout_rate, input_shape=x_train.shape[1:]))\r\n\r\n    for _ in range(2):\r\n        model.add(Dense(units=units, activation='relu'))\r\n        model.add(Dropout(rate=dropout_rate))\r\n\r\n    model.add(Dense(units=op_units, activation=op_activation))\r\n\r\n    # Compile model with learning parameters.\r\n    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\r\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\r\n\r\n    # Train and validate model.\r\n    history = model.fit(\r\n            x_train,\r\n            train_labels,\r\n            epochs=epochs,\r\n            validation_data=(x_val, val_labels),\r\n            verbose=2,  # Logs once per epoch.\r\n            batch_size=batch_size)\r\n\r\nif __name__ == \"__main__\":\r\n    data = load_dataset()\r\n    train_ngram_model(data,\r\n                      learning_rate=1e-3,\r\n                      epochs=10,\r\n                      batch_size=4,\r\n                      layers=2,\r\n                      units=64,\r\n                      dropout_rate=0.2)\r\n```", "Can you try to comment your two dropout lines?", "I commented them out. Then it works without raising an error!\r\n\r\nActually, only this line seems to cause the error:\r\n\r\n`model.add(Dropout(rate=dropout_rate, input_shape=x_train.shape[1:]))`", "As mentioned before, I took the code from the Google guide on text classification using the mlp model. About 1, 2 years ago I think it still worked, later (with a newer TensorFlow version, I guess) I got the error message mentioned above which I could get rid of by adding `todense()`.", "Is this related to your sequential model definition as in https://github.com/tensorflow/tensorflow/issues/37721?", "Not sure what you mean. I'm by no means a TensorFlow expert, just found it strange that when implementing this code (from the tutorial), it doesn't work.\r\n\r\nAs I understand it, when I would correctly define a keras.InputLayer in the model, it might work without the need to convert the sparse tensor to a dense tensor. I'm not sure, how to do that.", "There is a feedback button in the tutorial website. Click there and please mention this issue URL.", "Done.", "@DanielMH \r\nPlease confirm if this issue is resolved.", "I can't really confirm it, the code only works if I remove one line or if I add `.todense()` as indicated above. I guess the original code (from the Google Guide) should work without changes.\r\n\r\nMy code works by adding `.todense()`, but in other cases it seems to get very slow then.", "I am able to replicate the issue on tf2.3,2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/37b969984beb099ace9afd32946e02b6/untitled576.ipynb).", "Was able to replicate the issue in TF 2.6.0-dev20210531,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/34b3011ce282f83da9850ed73e97e1b4/untitled169.ipynb#scrollTo=sC9HKKOmxg0S)..Thanks !", "@DanielMH \r\nI ran the code shared on tf nightly and face a different error,please find the [gist here](https://colab.research.google.com/gist/Saduf2019/17858068afab0029ebb072144fde42c6/untitled642.ipynb), as the error reported is addressed, kindly move this to closed status, and for any further queries please pen an issue as per below info:\r\n\r\n\r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues).\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47931\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47931\">No</a>\n"]}, {"number": 47930, "title": "Share variable across devices in Tensorflow 2 cluster", "body": "Hello,\r\nI would like to use Tensorflow 2.4 in order to share a variable across devices on a cluster. In my use case, I want to share a queue stored on a parameter server that can be filled by all the workers. I know this was possible with tensorflow 1 but I do not understand how to do this in tf2 with the strategy API and the documentation does not seem to mention it. \r\nHere is my code to do this with a simple variable:\r\n\r\n```python\r\n# run.py\r\nimport tensorflow as tf\r\nimport os\r\nimport json\r\nimport argparse\r\nimport time\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--type', type=str, default='ps', required=False)\r\nparser.add_argument('--index', type=int, default=0, required=False)\r\nargs = parser.parse_args()\r\n\r\ncluster_dict = {\r\n    \"cluster\": {\r\n        \"worker\": ['localhost:50848'],\r\n        \"ps\": ['localhost:50849'],\r\n    },\r\n    \"task\": {\"type\": args.type, \"index\": args.index}\r\n}\r\n\r\n\r\nos.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\r\nos.environ[\"TF_CONFIG\"] = json.dumps(cluster_dict)\r\ncluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\r\n\r\nserver = tf.distribute.Server(\r\n    cluster_resolver.cluster_spec(), job_name=cluster_resolver.task_type,\r\n    task_index=cluster_resolver.task_id, protocol=\"grpc\"\r\n)\r\n# Initializes tf server internally.\r\nstrategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)\r\n\r\n\r\nwith tf.device('/job:ps/task:0'):\r\n    v = tf.Variable(\r\n        1, name='test')\r\n\r\nif cluster_resolver.task_type == \"worker\":\r\n    while True:\r\n        with tf.device('/job:ps/task:0'):\r\n            # <tf.Variable 'test:0' shape=() dtype=int32, numpy=>: ok\r\n            print(v.device)\r\n            # Increments variable on the worker\r\n            v.assign_add(1)\r\n            # v is incremented: ok\r\n            print('v:', v)\r\n\r\n            time.sleep(1)\r\nelif cluster_resolver.task_type == \"ps\":\r\n    with tf.device('/job:ps/task:0'):\r\n        while True:\r\n            # <tf.Variable 'test:0' shape=() dtype=int32, numpy=>: ok\r\n            print(v.device)\r\n            # v should be incremented by the worker\r\n            print(v)\r\n            # v=1: not working\r\n            time.sleep(1)\r\n\r\nelse:\r\n    pass\r\n```\r\n\r\nI then launch the worker and the parameter server using:\r\n```\r\npython run.py --type ps\r\n```\r\nand\r\n```\r\npython run.py --type worker\r\n```\r\n\r\nHowever the code in its current form does not work: the variable stored in the parameter server is updated by the worker but not visible by the parameter server. Note that I use the PS strategy but I have also tried the Mirrored Strategy. \r\n\r\nIs this behavior (i.e. sharing a custom variable over different workers of the cluster) supported in tf2? If so, could someone help me with the synthax, I would be very grateful! \r\n\r\nThanks! \r\n\r\n\r\n", "comments": ["@Fabien-Couthouis \r\n\r\nKindly open a [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) issue for this as it is not a bug or feature request, Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions.\r\nThanks!", "Hi @Saduf2019,\r\n\r\nThanks for the answer, a [stackoverflow issue](https://stackoverflow.com/questions/66669910/share-variable-across-devices-in-tensorflow-2-cluster) has already be opened (without any answer unfortunately). I understand you have a lot of work with tensorflow and you cannot provide the support but I was just wondering if my use case was doable in tensorflow 2, and if not, this would be a nice feature to add in the future.\r\n\r\n", "@Fabien-Couthouis Please follow the guide on distribution strategy [here](https://www.tensorflow.org/guide/distributed_training).  In the tutorial section of TF website, there are lot of tutorials on [distribution strategies](https://www.tensorflow.org/tutorials/distribute/keras). \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks! \r\n\r\nPlease verify those guides and tutorials on distribution strategies and close this issue. thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47930\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47930\">No</a>\n"]}, {"number": 47928, "title": "Failing saved model multi output model unittest", "body": "This PR pushes a failing test case (093f927aa9c217bfbecf7350c3c3f651964587af) showing the bug described in #47927.\r\n\r\nThe failure can be reproduced by running the unittests with `bazelisk test tensorflow/lite/python:lite_v2_test --test_output=all`.", "comments": ["Thanks for providing your case as a PR. Will discuss it in #47929."]}, {"number": 47926, "title": "Remove uint8 support from Fully Connected", "body": "Progress towards #44912\r\n\r\nRelated CMSIS issue: https://github.com/ARM-software/CMSIS_5/issues/1151", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47925, "title": "[determinism] Add softmax/cross-entropy op exceptions for GPU determinism", "body": "## High-Level Summary\r\n\r\nThis current PR adds and tests the following functionality:\r\n\r\nWhen the environment variable `TF_DETERMINISTIC_OPS` is set to `\"true\"` or `\"1\"`, an attempt to run the following ops on a GPU will throw `tf.errors.UnimplementedError` (with an understandable message).\r\n\r\n`tf.nn.softmax_cross_entropy_with_logits`\r\n`tf.nn.sparse_softmax_cross_entropy_with_logits`\r\n\r\nPlease see _RFC: Enhancing determinism in TF_ (being added via tensorflow/community PR [346](https://github.com/tensorflow/community/pull/346)).\r\n\r\n## Additional Notes\r\n\r\n### Data Types\r\n\r\nThe exceptions will be thrown for all currently GPU-supported data types for the `logits` input: `tf.float16` and `tf.float32` for both ops, and, additionally, `tf.float64` for `tf.nn.softmax_cross_entropy_with_logits`.\r\n\r\nException-throwing for all combinations of relevant data types for `logits` and `labels` (`tf.int32` and `tf.int64`) are tested in both eager and graph mode when the op is used in the forward direction.\r\n\r\n### Forward vs Backward\r\n\r\nIt is currently suspected that the introduction of random noise into the gradients passed backwards from this op actually originate in the forward path algorithm, but the backward path algorithm might add additional noise. However, the backprop path for this op is not, and cannot, be used without the forward path algorithm also being used (due to this being a loss function). Therefore, the presence of exception-throwing on the backward path specifically is not necessary and is not implemented or tested by this current PR.\r\n\r\nWhen these ops have a fully deterministic mode of operation, the bit-exact reproducibility of the outputs of both the forward and backward paths of the ops should be verified.\r\n\r\n### XLA\r\n\r\nThe tests will not be run with XLA auto-jit enabled because any XLA implementation of these ops will not throw these exceptions.\r\n\r\nWhen a fully deterministic mode for these ops is implemented, the bit-exact reproducibility of the outputs of both the forward and backward paths of the ops should be verified both with and without XLA auto-jit enabled.", "comments": ["@sanjoy: It would be so cool for this one to get into TensorFlow version 2.5 as well. :-)", "@sanjoy, please will it be possible to get this merged before the version 2.5 branch is cut, which I believe will be on March 25 (tomorrow)?", "> @sanjoy, please will it be possible to get this merged before the version 2.5 branch is cut, which I believe will be on March 25 (tomorrow)?\r\n\r\nI just approved it, but I can't guarantee that it will make it through the merge process before the branch cut.", "My spam filter was acting up and I missed your comment, @sanjoy, sorry. I also missed the internal checks failure. This PR didn't make it into 2.5. Please will you let me know what the `feedback/copybara` problem is? (I can't see into it).", "> My spam filter was acting up and I missed your comment, @sanjoy, sorry. I also missed the internal checks failure. This PR didn't make it into 2.5. Please will you let me know what the `feedback/copybara` problem is? (I can't see into it).\r\n\r\nLooks like some internal tooling failure, I'll merge it manually.  Unfortunately this won't make 2.5, as you noted."]}, {"number": 47922, "title": "Fix Keras Callbacks logs / numpy_logs sync", "body": "50480faea75f56def464b84f251b4aee388dfce9 introduced a bug where callbacks with `_supports_tf_logs=True` would access a different `logs` dictionary compared to callbacks with `_supports_tf_logs=False`.\r\nThis lead to problems when users would mutate the dictionary in callbacks which is a common pattern that is also used in some built in callbacks.\r\n\r\nThis PR fixes this by converting the logs dictionary to numpy in cases where not all callbacks support TF logs. This makes sure that all callbacks will access the same dictionary. The changes make the assumption that callbacks with `_supports_tf_logs=True` also support numpy logs. For all builtin callbacks this is already the case and in fact the logs dictionary frequently includes a mix of TensorFlow and Python scalars in the current implementation as well, so I don't think this causes a problem.\r\n\r\nThe PR also includes a small performance optimization in c38818b65da0f1284dcce4fb7aaf0c92ee98c1c1 which removes the need for converting logs during batch hooks in cases where all callbacks implementing batch hooks support TF logs.\r\n\r\nFixes #41851\r\nFixes #45895\r\n\r\n@fchollet @rmothukuru @reedwm @omalleyt12 would you be able to take a look at this PR? It would be great if this fix could still make it into the TF 2.5 release since it currently breaks many custom callbacks in user space (e.g. https://github.com/horovod/horovod/pull/2549).\r\nNote: For easier review, I'd recommend looking at the two commits one by one.", "comments": ["@omalleyt12 Thanks for approving. It looks like I missed a pylint error. I fixed it in f99f687974a51207c6dde7faedb98fd5d982e363\r\n\r\n>  LGTM, although I think it's dangerous in general to have a Callback mutate the logs, since then Callback order becomes important\r\n\r\nI fully agree. But unfortunately this has been possible for a long time and even TensorFlow core relies on this behaviour in some places:\r\nhttps://github.com/tensorflow/tensorflow/blob/8a322f312a9a77256650cbfc3b2154911f31e887/tensorflow/python/keras/callbacks.py#L1956 https://github.com/tensorflow/tensorflow/blob/8a322f312a9a77256650cbfc3b2154911f31e887/tensorflow/python/keras/callbacks.py#L2667 https://github.com/tensorflow/tensorflow/blob/8a322f312a9a77256650cbfc3b2154911f31e887/tensorflow/python/keras/callbacks.py#L935-L939\r\nso unfortunately I think this either needs to be kept supported or properly deprecated with an alternative way of extending the `logs`. Probably overwriting `train_step` would work (although that might be a too involved for some users) or making every callback explicitly return the `logs` dict so that it doesn't need to be mutated anymore.", "I resolved the merge conflicts introduced in d592b6e634413d1c288569c79ddb94e4da121c56. @omalleyt12 can you approve this PR again, so it can be merged?"]}, {"number": 47921, "title": "[Pluggable Device] Add custom device mem allocator for Pluggable device.", "body": "The commit provides a simple alternative to BFCAllocator for Pluggable devices\r\nto do their own device memory management.\r\n\r\n@penpornk , @annarev , @jzhoulon ", "comments": ["> Thank you very much for the PR! I think you forgot to add `pluggable_device_simple_allocator.cc` and `pluggable_device_simple_allocator.h`?\r\n\r\nAh, thanks!. Added it.", "@kulinseth can you please check sanity build failures ?", "@rthadur They are minor formatting errors. In the interest of time (since the branch cut is next week), I'll just manually import the PR and fix them. :) I'm just waiting for the `oneDNN` and `Windows Bazel GPU` CI builds to finish.", "Thanks a lot @penpornk. Please let me know if there is needed form my side."]}, {"number": 47920, "title": "Replacing cuda_py_tests with cuda_py_test where applicable", "body": "This commit replaces the use of `cuda_py_tests` (plural) with `cuda_py_test`, in cases where the unit-test has only one source file (and hence the singular version will suffice).\r\n\r\nAmongst other things, test targets defined via the plural version do not seem to get the `_gpu` suffix :(\r\n\r\nIdeally `cuda_py_tests` should just iterate over the `srcs` list, and call `cuda_py_test` for each source file. But that change probably has much wider implications, and hence not attempting it here.\r\n\r\n-----------------------------\r\n\r\n/cc @chsigg @cheshire ", "comments": ["@chsigg @cheshire gentle ping", "@chsigg @cheshire gentle ping", "rebase PR to resolve merge conflict.\r\n\r\n@chsigg please re-approve", "@chsigg gentle ping", "@chsigg do you know the wider context here? Why were they plural in the first place? What do we get by making them singular?", "I don't know. Avoid boilerplate repetition? Maybe for sharing?\r\n\r\nBut I agree with the PR description, ideally the plural would just loop over the sources and create one test each. Although one could easily do that with just `[cuda_py_test(...) for src in [...]]` and you wouldn't need the plural macro at all. "]}, {"number": 47919, "title": "[ROCm] Fix for ROCm CSB breakages - 210319", "body": "## 1\r\n\r\nThe following commit breaks the following unit-tests on ROCm\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/92e03156976ee8d045e872b3194c8f21576dfc03\r\n\r\n```\r\n//tensorflow/compiler/xla/service/gpu/tests:gpu_infeed_test              FAILED in 3 out of 3 in 1.8s\r\n//tensorflow/compiler/xla/tests:local_client_execute_test_gpu            FAILED in 3 out of 3 in 13.7s\r\n//tensorflow/compiler/xla/tests:outfeed_in_nested_computation_test_gpu   FAILED in 3 out of 3 in 2.0s\r\n//tensorflow/compiler/xla/tests:while_test_gpu                           FAILED in 3 out of 3 in 9.9s\r\n```\r\n\r\nThe cause is that part of the changes were not enabled for ROCm, and this commit fixes that.\r\n\r\n## 2\r\n\r\nThe following commit breaks the following unit-test on ROCm\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/47772\r\n\r\n```\r\n//tensorflow/python/kernel_tests:segment_reduction_ops_deterministic_test_gpu FAILED in 3 out of 3 in 5.3s\r\n```\r\n\r\nROCm platform does not yet support complex datatype for the segment reduction ops, which is what leads to the failure.\r\n\r\nThis commit modifies the testcase to skip testing the complex datatype on ROCm\r\n\r\n\r\n---------------------------------------------\r\n\r\n\r\n/cc @cheshire @chsigg ", "comments": []}, {"number": 47918, "title": "Installation error: Tensorflow having some conflict with rocm-3.9.1", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.10 with kernel 5.10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.2 with tensorflow-rocm2.3.4\r\n- Python version: 3.8.6\r\n- GPU model and memory: AMD Radeon RX580 4g\r\n\r\n**Describe the problem**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: libamdhip64.so.4: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/eager/context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: libamdhip64.so.4: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Any other info / logs**\r\n-\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47918\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47918\">No</a>\n"]}, {"number": 47917, "title": "Tensorflow ROCM getting wrong version", "body": "Didn't find this error on errors list: https://www.tensorflow.org/install/errors\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.10 with kernel 5.10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.2 with tensorflow-rocm2.3.4\r\n- Python version: 3.8.6\r\n- GPU model and memory: AMD Radeon RX580\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: libamdhip64.so.4: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/eager/context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/static/pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: libamdhip64.so.4: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n**Describe the current behavior** I have rocm 3.9.1 (I downgraded from 4), but it is still trying to get libamdhip64.so.4 instead of libamdhip64.so.3.\r\n\r\n**Describe the expected behavior** look for correct version.\r\n\r\n**Standalone code to reproduce the issue**\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n", "comments": ["Have you checked TF and ROCM versions compatibility at https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/rocm_docs/tensorflow-rocm-release.md ?\r\n\r\n@lamberta As this is quite hidden probably we could expose this somewhere in https://www.tensorflow.org/install/gpu#software_requirements", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47917\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47917\">No</a>\n", "We say \"CUDA\u00ae-enabled cards\" in the install section and [gpu](https://www.tensorflow.org/install/gpu) page (it's highlighted at the top of that page).\r\nWhat did you you want to add? ROCm is not officially supported so we won't add that\u2014but might fit into [SIG Build](https://github.com/tensorflow/build).", "@bhack sorry I didn't answer but you were precise in your answer, I didn't knew about this compatibility table. Thanks.", "@lamberta I meant as we list [Community builds](https://github.com/tensorflow/tensorflow#community-supported-builds) in the landing Readme  here we could find a place just to tell that there are community builds available and to ask support in their relative repositories (or SIG Build if they will support these kind of builds).", "Yeah, I think that might be ok.\r\nWe currently link out to \"SIG Build\" in the leftnav of the /install section, and really the extent of what we want to support on tensorflow.org. But if there's a good place to link out, I'm ok with that.\r\nI'm not sure who owns that community build table in the readme. cc: @angerson \r\n", "I don't know who is the owner but this is the last PR that we merged in that section https://github.com/tensorflow/tensorflow/pull/41928", "Thanks, bhack. cc: @mihaimaruseac \r\nIt does seem like a useful table and buried, though I'd argue it should be more prominent in [SIG Build](https://github.com/tensorflow/build). Ideally that would be the one URL we could give folks for community supported builds.\r\n", "Yes specially to clearly route support requests/tickets if they are not supported here."]}, {"number": 47916, "title": "Tensorflow Training crashes when using tf dataset on large amount of data", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes, using tf dataset pipelines\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**: \r\n-   **TensorFlow version (use command below)**: 2.2.0\r\n-   **Python version**: 3.6\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**: \r\n-   **CUDA/cuDNN version**: 11.0\r\n-   **GPU model and memory**: \r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nGet Error \" IVER_API, cbid)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2021-03-08 08:25:35.572927: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2247026860 exceeds 10% of free system memory.\r\n[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:523] CHECK failed: (value.size()) <= (kint32max):\r\n[1]    4943 segmentation fault  sudo python3 msynth-train-script.py\"\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["|  Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nWe really need to have a very minimum standalone code snippet  (or Colab) to reproduce this.\r\n\r\nIn the meantime can you try to reduce the batch size?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hey!\r\nSo, I am training with huge amount of data (in millions). When I try training with lets say approx 25M images, I get \"[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:523] CHECK failed: (value.size()) <= (kint32max): \r\nSegmentation fault (core dumped)\" when model.fit is called and the TensorFlow training crashes. I have used TensorFlow dataset API and created datasets using a generator.\r\nBut on the other hand, if I try reducing the dataset to say 23M images, it works well and does not crash.\r\n\r\n\r\nAlso, I have tried reducing the batch size as well, same issue prevails.\r\n\r\n\r\n\r\n\r\n", "@Guneetkaur03,\r\nCould you please provide a minimal code snippet with a dummy dataset, so that we can reproduce the issue on our end. \r\n\r\nAlternatively, you can also run the code on [Google Colab](https://colab.research.google.com/) and share the notebook with us. Thanks!", "@Guneetkaur03 @amahendrakar I have the same problem\uff01I\u2018m doing a semantic segmentation task\uff0ci have thousands of pictures and labels. The kernel will died and restart when i use anaconda to train pictures that exceed 1600, but the kernel will not died when i train 1000 pictures.But the result is so bad and i want to train more than 10000 pictures.It hint the information when i use the Pycharm:\r\n\r\n2021-04-09 11:06:08.187912: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cupti64_110.dll'; dlerror: cupti64_110.dll not found\r\n2021-04-09 11:06:08.190073: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found\r\n2021-04-09 11:06:08.191626: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2021-04-09 11:06:08.192304: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\r\n2021-04-09 11:06:08.192482: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1496] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\r\nProcess finished with exit code -1073741819 (0xC0000005)\r\n\r\n\r\nI tried to solve this problem but failed.\r\nMy system information are:\r\n**Python version: 3.8\r\nCUDA/cuDNN version: 10.1\u30017.6\r\nTensorFlow version : 2.4.1\r\nWindows10**\r\n\r\nFallowing is my code of loading the datasets\uff1a\r\n\r\nimport cv2\r\nfrom PIL import Image\r\nimport numpy as np\r\nimport os\r\n\r\npath = 'D:\\CDS\\cropped\\label\\_label_1_6.png'\r\ndef create_one_hot_labels(path):\r\n    NCLASSES = 6\r\n    img = cv2.imread(path)\r\n    labels = np.zeros((img.shape[0], img.shape[1], NCLASSES))\r\n    for i in range(img.shape[0]):\r\n        for j in range(img.shape[1]):\r\n            if (img[i, j, 0] == 0.) and (img[i, j, 1] == 0.) and (img[i, j, 2] == 0.):  #black \r\n                labels[i, j, 0] = 1  #to create one-hot-label making the third dimension have shape of six\r\n            elif (img[i, j, 0] == 0.) and (img[i, j, 1] == 0.) and (img[i, j, 2] == 255.): #red\r\n                labels[i, j, 1] = 1 \r\n            elif (img[i, j, 0] == 0.) and (img[i, j, 1] == 255.) and (img[i, j, 2] == 0.): #green\r\n                labels[i, j, 2] = 1\r\n            elif (img[i, j, 0] == 255.) and (img[i, j, 1] == 0.) and (img[i, j, 2] == 0.): #blue\r\n                labels[i, j, 3] = 1\r\n            elif (img[i, j, 0] == 255.) and (img[i, j, 1] == 255.) and (img[i, j, 2] == 0.): #cyan\r\n                labels[i, j, 4] = 1\r\n            else:\r\n                labels[i, j, 5] = 1\r\n    labels = np.reshape(labels, (-1, NCLASSES))\r\n    return labels\r\n\r\nfor i in range(1200):\r\n    one_hot_label = create_one_hot_labels(paths_label[i*16])\r\n    one_hot_label = np.array(one_hot_label).astype(np.float32)\r\n    labels.append(one_hot_label)      #train labels\r\n\r\nX_train = []\r\nfor j in range(1200):\r\n    img = cv2.imread(paths_image[j*16])\r\n    img = (np.array(img) / 255).astype(np.float32)\r\n    X_train.append(img)           #train data\r\n\r\nfor i in range(700):\r\n    one_hot_label = create_one_hot_labels(paths_label[i*37])\r\n    one_hot_label = np.array(one_hot_label).astype(np.float32)\r\n    val_labels.append(one_hot_label)\r\n\r\nX_val = []\r\nfor j in range(700):\r\n    img = cv2.imread(paths_image[j*37])\r\n    img = (np.array(img) / 255.).astype(np.float32)\r\n    X_val.append(img)        #validation data\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nAUTOTUNE = tf.data.experimental.AUTOTUNE\r\nBATCH_SIZE = 20\r\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train, labels))\r\n# val_ds = tf.data.Dataset.from_tensor_slices((X_val, val_labels))\r\nval_ds = tf.data.Dataset.from_tensor_slices((X_val, val_labels))\r\ntrain_ds = train_ds.cache()\r\ntrain_ds = train_ds.apply(\r\n  tf.data.experimental.shuffle_and_repeat(buffer_size=200))\r\ntrain_ds = train_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\r\nval_ds = val_ds.batch(BATCH_SIZE)\r\n\r\n# model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\r\nmiou = tf.keras.metrics.MeanIoU(num_classes=6)\r\nmodel.compile(optimizer=keras.optimizers.Adam(3e-3),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=6)])\r\nimport tensorflow as tf\r\n\r\nEPOCHS = 500\r\nVAL_SUBSPLITS = 5\r\nVALIDATION_STEPS = 700//BATCH_SIZE//VAL_SUBSPLITS\r\nSTEPS_PER_EPOCH = 1200//BATCH_SIZE\r\n\r\nmodel_history = model.fit(train_ds, epochs=EPOCHS,\r\n                          steps_per_epoch=STEPS_PER_EPOCH,\r\n                          validation_steps=VALIDATION_STEPS,\r\n                          validation_data=val_ds,\r\n#                           callbacks=[DisplayCallback()])\r\n                          callbacks=[early_stopping, tensorboard_callback, csv_log])\r\n\r\n**So my problem is if the way of loading the dataset is wrong and how can i load more than 10000 pictures to train. Or if i have the problem in the version of the configuration.**\r\n\r\nMany thanks\r\n", "@Cuids,\r\nOn running the code, I am facing an error stating `NameError: name 'paths_label' is not defined`. \r\n\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47916\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47916\">No</a>\n"]}, {"number": 47915, "title": "Build fails on Arch Linux", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux, LSB version 1.4, Linux 5.10.23-1-lts, x86-64 \r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: virtualenv, pip\r\n- Bazel version (if compiling from source): 4.0.0\r\n- GCC/Compiler version (if compiling from source): 10.2.0\r\n- CUDA/cuDNN version: CUDA 11.2.0-3, cuDNN 8.0.5.39-1\r\n- GPU model and memory: Nvidia GeForce RTX 280 Ti 11016MiB\r\n\r\n**Describe the problem**\r\nI have two problems. These problems are related to two different types of situations. However, I have opened just one issue because, even if the problems could not be related themselves, they are probably related to building/installation problems.\r\n\r\n1. Manual compilation of TensorFlow (2.4.1 version) using the instruction related to Arch Linux ([see the PKGBUILD file here](https://github.com/archlinux/svntogit-community/blob/packages/tensorflow/trunk/PKGBUILD)) fails. The error is the following (I'll log the entire output, error at the end). Note: The compilation has been started outside any virtualenv.\r\n```bash\r\npatching file tensorflow/tools/ci_build/install/install_centos_pip_packages.sh\r\nReversed (or previously applied) patch detected!  Skipping patch.\r\n1 out of 1 hunk ignored -- saving rejects to file tensorflow/tools/ci_build/install/install_centos_pip_packages.sh.rej\r\npatching file tensorflow/tools/ci_build/install/install_pip_packages.sh\r\nReversed (or previously applied) patch detected!  Skipping patch.\r\n1 out of 1 hunk ignored -- saving rejects to file tensorflow/tools/ci_build/install/install_pip_packages.sh.rej\r\npatching file tensorflow/tools/ci_build/install/install_python3.5_pip_packages.sh\r\nReversed (or previously applied) patch detected!  Skipping patch.\r\n1 out of 1 hunk ignored -- saving rejects to file tensorflow/tools/ci_build/install/install_python3.5_pip_packages.sh.rej\r\npatching file tensorflow/tools/ci_build/install/install_python3.6_pip_packages.sh\r\nReversed (or previously applied) patch detected!  Skipping patch.\r\n1 out of 1 hunk ignored -- saving rejects to file tensorflow/tools/ci_build/install/install_python3.6_pip_packages.sh.rej\r\npatching file tensorflow/tools/ci_build/release/common_win.bat\r\nReversed (or previously applied) patch detected!  Skipping patch.\r\n2 out of 2 hunks ignored -- saving rejects to file tensorflow/tools/ci_build/release/common_win.bat.rej\r\npatching file tensorflow/tools/pip_package/setup.py\r\nReversed (or previously applied) patch detected!  Skipping patch.\r\n1 out of 1 hunk ignored -- saving rejects to file tensorflow/tools/pip_package/setup.py.rej\r\npatching file tensorflow/python/keras/saving/hdf5_format.py\r\nReversed (or previously applied) patch detected!  Skipping patch.\r\n1 out of 1 hunk ignored -- saving rejects to file tensorflow/python/keras/saving/hdf5_format.py.rej\r\npatching file tensorflow/python/keras/saving/hdf5_format.py\r\nReversed (or previously applied) patch detected!  Skipping patch.\r\n3 out of 3 hunks ignored -- saving rejects to file tensorflow/python/keras/saving/hdf5_format.py.rej\r\npatching file tensorflow/python/keras/saving/hdf5_format.py\r\nReversed (or previously applied) patch detected!  Skipping patch.\r\n2 out of 2 hunks ignored -- saving rejects to file tensorflow/python/keras/saving/hdf5_format.py.rej\r\npatching file tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common.py\r\nHunk #2 FAILED at 80.\r\n1 out of 2 hunks FAILED -- saving rejects to file tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common.py.rej\r\npatching file tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common.py\r\nHunk #2 FAILED at 79.\r\n1 out of 2 hunks FAILED -- saving rejects to file tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common.py.rej\r\npatching file tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common.py\r\npatching file tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common.py\r\npatching file tensorflow/python/pywrap_mlir.py\r\nHunk #1 succeeded at 38 (offset 1 line).\r\npatching file tensorflow/python/pywrap_mlir.py\r\nHunk #1 succeeded at 38 (offset 1 line).\r\nYou have bazel 4.0.0 installed.\r\nFound CUDA 11.2 in:\r\n    /opt/cuda/targets/x86_64-linux/lib\r\n    /opt/cuda/targets/x86_64-linux/include\r\nFound cuDNN 8 in:\r\n    /usr/lib\r\n    /usr/include\r\nFound NCCL 2 in:\r\n    /usr/lib\r\n    /usr/include\r\n\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=198\r\nINFO: Reading rc options for 'build' from /run/media/federico/XData/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /run/media/federico/XData/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /run/media/federico/XData/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/usr/lib/python3.9/site-packages --python_path=/usr/bin/python --config=xla --action_env TF_CUDA_VERSION=11.2 --action_env TF_CUDNN_VERSION=8 --action_env TF_NCCL_VERSION=2.8 --action_env TF_CUDA_PATHS=/opt/cuda,/usr/lib,/usr,/usr/include,/usr/local/lib --action_env CUDA_TOOLKIT_PATH=/opt/cuda --action_env NCCL_INSTALL_PATH=/usr --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.5 --action_env LD_LIBRARY_PATH=/opt/cuda/extras/CUPTI/lib64: --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc --config=cuda --action_env TF_SYSTEM_LIBS=boringssl,curl,cython,gif,icu,libjpeg_turbo,lmdb,nasm,pcre,png,pybind11,zlib --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /run/media/federico/XData/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /run/media/federico/XData/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /run/media/federico/XData/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file /run/media/federico/XData/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /run/media/federico/XData/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:avx2_linux in file /run/media/federico/XData/tensorflow/.bazelrc: --copt=-mavx2\r\nINFO: Found applicable config definition build:mkl in file /run/media/federico/XData/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt\r\nINFO: Found applicable config definition build:linux in file /run/media/federico/XData/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /run/media/federico/XData/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\nDEBUG: Repository io_bazel_rules_go instantiated at:\r\n  /run/media/federico/XData/tensorflow/WORKSPACE:37:30: in <toplevel>\r\n  /home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/external/bazel_toolchains/repositories/repositories.bzl:55:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nDEBUG: /home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /run/media/federico/XData/tensorflow/WORKSPACE:37:30: in <toplevel>\r\n  /home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nINFO: Analyzed 4 targets (418 packages loaded, 31513 targets configured).\r\nINFO: Found 4 targets...\r\nINFO: Deleting stale sandbox base /home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/sandbox\r\nERROR: /run/media/federico/XData/tensorflow/tensorflow/python/keras/api/BUILD:111:19: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\n2021-03-18 20:25:35.715885: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nTraceback (most recent call last):\r\n  File \"/home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 26, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 48, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py\", line 27, in <module>\r\n    from tensorflow.python.keras import models\r\n  File \"/home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/keras/models.py\", line 26, in <module>\r\n    from tensorflow.python.keras.engine import functional\r\n  File \"/home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/keras/engine/functional.py\", line 38, in <module>\r\n    from tensorflow.python.keras.engine import training as training_lib\r\n  File \"/home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/keras/engine/training.py\", line 52, in <module>\r\n    from tensorflow.python.keras.engine import data_adapter\r\n  File \"/home/federico/.cache/bazel/_bazel_federico/9ebe032d906e33a6e519d86fafa71920/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen.runfiles/org_tensorflow/tensorflow/python/keras/engine/data_adapter.py\", line 65, in <module>\r\n    import pandas as pd  # pylint: disable=g-import-not-at-top\r\n  File \"/usr/lib/python3.9/site-packages/pandas/__init__.py\", line 29, in <module>\r\n    from pandas._libs import hashtable as _hashtable, lib as _lib, tslib as _tslib\r\n  File \"/usr/lib/python3.9/site-packages/pandas/_libs/__init__.py\", line 13, in <module>\r\n    from pandas._libs.interval import Interval\r\n  File \"pandas/_libs/interval.pyx\", line 1, in init pandas._libs.interval\r\nValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject\r\nERROR: /run/media/federico/XData/tensorflow/tensorflow/python/tools/BUILD:282:10 Middleman _middlemen/tensorflow_Spython_Stools_Sprint_Uselective_Uregistration_Uheader-runfiles failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\nINFO: Elapsed time: 7946.935s, Critical Path: 567.22s\r\nINFO: 28075 processes: 5830 internal, 22245 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n2. Manual compilation of TensorFlow (git cloned and checked out at r2.4/v2.4.1) following the instruction [here](https://www.tensorflow.org/install/source) seems to work. The `pip install` of the wheel produced correctly works too. However, trying to create a new CustoOps, in particular a basic one `zero_out.cc` as proposed [here](https://www.tensorflow.org/guide/create_op#define_the_op_interface), after having compiled with bazel, when I try to test it as described [here](https://www.tensorflow.org/guide/create_op#use_the_op_in_python) I receive this error (demangled):\r\n`tensorflow::OpKernel::TraceString(tensorflow::OpKernelContext const&, bool) const`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Regarding the first problem, I have tried to compile TensorFlow following a subset of the instruction in this [PKGBUILD](https://github.com/archlinux/svntogit-community/blob/packages/tensorflow/trunk/PKGBUILD) file (see below for the exact subset of commands I have used). In particular:\r\n\r\n* I've used the `tensorflow-2.4.1-cuda-opt` extracted from the source indicated in the `PKGBUILD` (`source=(\"$pkgname-$pkgver.tar.gz::https://github.com/tensorflow/tensorflow/archive/v${_pkgver}.tar.gz\"`)\r\n* Renamed the folder into `tensorflow`\r\n* Put the `fix-h5py3.0.patch` into the above `tensorflow` folder\r\n* From the mentioned `PKGBUILD` file, I've used this subset of instructions:\r\n```bash\r\n#!/bin/bash          \r\n\r\n# Set the bazel version\r\necho \"4.0.0\" > .bazelversion\r\n\r\n# Patch the h5py\r\npatch -Np1 -d . -i fix-h5py3.0.patch\r\n\r\n# Get rid of hardcoded versions. Not like we ever cared about what upstream\r\n# thinks about which versions should be used anyway. ;) (FS#68772) \r\nsed -i -E \"s/'([0-9a-z_-]+) .= [0-9].+[0-9]'/'\\1'/\" tensorflow/tools/pip_package/setup.py\r\n\r\n# Set the variables\r\nexport PYTHON_BIN_PATH=/usr/bin/python\r\nexport USE_DEFAULT_PYTHON_LIB_PATH=1\r\nexport TF_NEED_JEMALLOC=1\r\nexport TF_NEED_KAFKA=1\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport TF_NEED_AWS=1\r\nexport TF_NEED_GCP=1\r\nexport TF_NEED_HDFS=1\r\nexport TF_NEED_S3=1\r\nexport TF_ENABLE_XLA=1\r\nexport TF_NEED_GDR=0\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_MPI=0\r\nexport TF_NEED_TENSORRT=0\r\nexport TF_NEED_NGRAPH=0\r\nexport TF_NEED_IGNITE=0\r\nexport TF_NEED_ROCM=0\r\n# See https://github.com/tensorflow/tensorflow/blob/master/third_party/systemlibs/syslibs_configure.bzl\r\nexport TF_SYSTEM_LIBS=\"boringssl,curl,cython,gif,icu,libjpeg_turbo,lmdb,nasm,pcre,png,pybind11,zlib\"\r\nexport TF_SET_ANDROID_WORKSPACE=0\r\nexport TF_DOWNLOAD_CLANG=0\r\nexport TF_NCCL_VERSION=2.8\r\nexport TF_IGNORE_MAX_BAZEL_VERSION=1\r\nexport TF_MKL_ROOT=/opt/intel/mkl\r\nexport NCCL_INSTALL_PATH=/usr\r\nexport GCC_HOST_COMPILER_PATH=/usr/bin/gcc\r\nexport HOST_C_COMPILER=/usr/bin/gcc\r\nexport HOST_CXX_COMPILER=/usr/bin/g++\r\nexport TF_CUDA_CLANG=0  # Clang currently disabled because it's not compatible at the moment.\r\nexport CLANG_CUDA_COMPILER_PATH=/usr/bin/clang\r\nexport TF_CUDA_PATHS=/opt/cuda,/usr/lib,/usr,/usr/include,/usr/local/lib\r\nexport TF_CUDA_VERSION=$(/opt/cuda/bin/nvcc --version | sed -n 's/^.*release \\(.*\\),.*/\\1/p')\r\nexport TF_CUDNN_VERSION=$(sed -n 's/^#define CUDNN_MAJOR\\s*\\(.*\\).*/\\1/p' /usr/include/cudnn_version.h)\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.5\r\nexport CC=gcc\r\nexport CXX=g++\r\nexport BAZEL_ARGS=\"--config=mkl -c opt --copt=-I/usr/include/openssl-1.0 --host_copt=-I/usr/include/openssl-1.0 --linkopt=-l:libssl.so.1.0.0 --linkopt=-l:libcrypto.so.1.0.0 --host_linkopt=-l:libssl.so.1.0.0 --host_linkopt=-l:libcrypto.so.1.0.0\"\r\nexport BAZEL_ARGS=\"$BAZEL_ARGS --host_copt=-Wno-stringop-truncation\"\r\n\r\n\r\necho \"Building with cuda and with non-x86-64 optimizations\" \r\nexport CC_OPT_FLAGS=\"-march=haswell -O3\"\r\nexport TF_NEED_CUDA=1\r\n./configure\r\nbazel build --config=avx2_linux ${BAZEL_ARGS[@]} //tensorflow:libtensorflow.so //tensorflow:libtensorflow_cc.so //tensorflow:install_headers //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n2. For the second problem, related to the successful build and installation, I reproduced the steps indicated in the official build guide [here](https://www.tensorflow.org/install/source), and then I have installed the wheel inside a virtualenv with python 3.9. The only thing I had to do to succeed in the installation process has been to downgrade the  `hdf5` library installed on my system from the `1.12.0-2` to the `1.10.5-1`.\r\n\r\nAny hint will be appreciated. Thank you.", "comments": ["@iLeW \r\nWe see that you have cuda 11.2 which is compatible with tf nightly 2.5, can you downgrade to cuda 11.0 and let us know.\r\nyou may refer to [this link](https://github.com/tensorflow/tensorflow/issues/44159#issuecomment-735542190) and let us know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47915\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47915\">No</a>\n", "> @iLeW We see that you have cuda 11.2 which is compatible with tf nightly 2.5, can you downgrade to cuda 11.0 and let us know. you may refer to [this link](https://github.com/tensorflow/tensorflow/issues/44159#issuecomment-735542190) and let us know.\r\n\r\nOk, some time has passed since the last reply. I'm replying just for completeness. Finally, we moved to another version and into a new solution. The problem was probably tied to the CUDA version. We can keep this closed."]}, {"number": 47914, "title": "[TFLM] Add support for optimized SVDF kernel for CEVA-DSP BX1 & SP500", "body": "Adds support for optimized int8 and float32 kernels.\r\nComes after the refactoring done in https://github.com/tensorflow/tensorflow/pull/47911\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@yair-ehrenwald  Any update on this PR? Please. Thanks!", "With TFLM moving to its own GitHub repository, we are not going to be merging any TFLM specific pull requests in the TensorFlow repository starting today.\r\n\r\nI am closing the current PR but please feel free to open a new PR in https://github.com/tensorflow/tflite-micro/.\r\n\r\nhttps://groups.google.com/a/tensorflow.org/g/micro/c/W4DACgjPmOE\r\n"]}, {"number": 47913, "title": "Tenorflow Traceback Errors ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\nThis is what I do \r\n\r\n\r\n!git clone https://github.com/tensorflow/models.git\r\n\r\n# Commented out IPython magic to ensure Python compatibility.\r\n# %cd /content/models/research/\r\n!protoc object_detection/protos/*.proto --python_out=.\r\n# Install TensorFlow Object Detection API.\r\n!cp object_detection/packages/tf2/setup.py .\r\n!python -m pip install .\r\n\r\n!python /content/models/research/object_detection/builders/model_builder_tf2_test.py\r\n\r\n!pip install -q kaggle\r\n!pip install -q kaggle-cli\r\n!pip install lxml\r\n\r\nimport os\r\nos.environ['KAGGLE_USERNAME'] = 'callumfawcett'\r\nos.environ['KAGGLE_KEY'] = '4e2c04f4c1e23da1796a889a29104c55'\r\n\r\n# Commented out IPython magic to ensure Python compatibility.\r\n# %%bash\r\n# mkdir /content/dataset\r\n# cd /content/dataset\r\n# kaggle datasets download -d callumfawcett/graphs --unzip\r\n\r\n# Commented out IPython magic to ensure Python compatibility.\r\n# %cd /content/\r\n\r\n!python dataset/generate_tf_records.py -l /content/labelmap.pbtxt -o dataset/train.record -i dataset/images -csv dataset/train_labels.csv\r\n!python dataset/generate_tf_records.py -l /content/labelmap.pbtxt -o dataset/test.record -i dataset/images -csv dataset/test_labels.csv\r\n\r\n# Commented out IPython magic to ensure Python compatibility.\r\n# %cd /content\r\n!wget http://download.tensorflow.org/models/object_detection/classification/tf2/20200710/mobilenet_v2.tar.gz\r\n!tar -xvf mobilenet_v2.tar.gz\r\n!rm mobilenet_v2.tar.gz\r\n\r\n!wget https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_mobilenet_v2_320x320_coco17_tpu-8.config\r\n!mv ssd_mobilenet_v2_320x320_coco17_tpu-8.config mobilenet_v2.config\r\n\r\nnum_classes = 5\r\nbatch_size = 96\r\nnum_steps = 7500\r\nnum_eval_steps = 1000\r\n\r\ntrain_record_path = '/content/dataset/train.record'\r\ntest_record_path = '/content/dataset/test.record'\r\nmodel_dir = '/content/training/'\r\nlabelmap_path = '/content/labelmap.pbtxt'\r\n\r\npipeline_config_path = 'mobilenet_v2.config'\r\nfine_tune_checkpoint = '/content/mobilenet_v2/mobilenet_v2.ckpt-1'\r\n\r\nimport re\r\n\r\nwith open(pipeline_config_path) as f:\r\n    config = f.read()\r\n\r\nwith open(pipeline_config_path, 'w') as f:\r\n\r\n  # Set labelmap path\r\n  config = re.sub('label_map_path: \".*?\"', \r\n             'label_map_path: \"{}\"'.format(labelmap_path), config)\r\n  \r\n  # Set fine_tune_checkpoint path\r\n  config = re.sub('fine_tune_checkpoint: \".*?\"',\r\n                  'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), config)\r\n  \r\n  # Set train tf-record file path\r\n  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', \r\n                  'input_path: \"{}\"'.format(train_record_path), config)\r\n  \r\n  # Set test tf-record file path\r\n  config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', \r\n                  'input_path: \"{}\"'.format(test_record_path), config)\r\n  \r\n  # Set number of classes.\r\n  config = re.sub('num_classes: [0-9]+',\r\n                  'num_classes: {}'.format(num_classes), config)\r\n  \r\n  # Set batch size\r\n  config = re.sub('batch_size: [0-9]+',\r\n                  'batch_size: {}'.format(batch_size), config)\r\n  \r\n  # Set training steps\r\n  config = re.sub('num_steps: [0-9]+',\r\n                  'num_steps: {}'.format(num_steps), config)\r\n  \r\n  f.write(config)\r\n\r\n!python /content/models/research/object_detection/model_main_tf2.py \\\r\n    --pipeline_config_path={pipeline_config_path} \\\r\n    --model_dir={model_dir} \\\r\n    --alsologtostderr \\\r\n    --num_train_steps={num_steps} \\\r\n    --sample_1_of_n_eval_examples=1 \\\r\n    --num_eval_steps={num_eval_steps}\r\n error given\r\n2021-03-19 13:10:29.033663: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-19 13:10:31.900190: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-19 13:10:31.901266: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-03-19 13:10:31.923864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:10:31.924658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\r\ncoreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\r\n2021-03-19 13:10:31.924704: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-19 13:10:31.927664: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-03-19 13:10:31.927752: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-03-19 13:10:31.929670: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-19 13:10:31.930074: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-19 13:10:31.932218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-19 13:10:31.933137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-03-19 13:10:31.933384: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-19 13:10:31.933500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:10:31.934304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:10:31.935034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-19 13:10:31.935512: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-19 13:10:31.935645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:10:31.936414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\r\ncoreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\r\n2021-03-19 13:10:31.936454: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-19 13:10:31.936504: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-03-19 13:10:31.936555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-03-19 13:10:31.936605: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-19 13:10:31.936647: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-19 13:10:31.936693: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-19 13:10:31.936739: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-03-19 13:10:31.936785: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-19 13:10:31.936893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:10:31.937699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:10:31.938434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-19 13:10:31.938494: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-19 13:10:32.376021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-19 13:10:32.376092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-03-19 13:10:32.376119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-03-19 13:10:32.376392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:10:32.377213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:10:32.378024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:10:32.378728: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n2021-03-19 13:10:32.378806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10637 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\r\nI0319 13:10:32.380789 140196829169536 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\r\nINFO:tensorflow:Maybe overwriting train_steps: 7500\r\nI0319 13:10:32.386101 140196829169536 config_util.py:552] Maybe overwriting train_steps: 7500\r\nINFO:tensorflow:Maybe overwriting use_bfloat16: False\r\nI0319 13:10:32.386274 140196829169536 config_util.py:552] Maybe overwriting use_bfloat16: False\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI0319 13:10:32.565161 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI0319 13:10:32.566583 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI0319 13:10:32.568816 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI0319 13:10:32.569891 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI0319 13:10:32.624391 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI0319 13:10:32.628402 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI0319 13:10:32.655039 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI0319 13:10:32.656113 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI0319 13:10:32.657880 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI0319 13:10:32.658880 140196829169536 cross_device_ops.py:565] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n2021-03-19 13:10:34.452403: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-19 13:10:35.371642: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-03-19 13:10:35.587620: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0319 13:10:36.040649 140196829169536 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0319 13:10:36.041203 140196829169536 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0319 13:10:36.041422 140196829169536 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0319 13:10:36.041633 140196829169536 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0319 13:10:36.041833 140196829169536 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0319 13:10:36.042120 140196829169536 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\r\nWARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:540: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nW0319 13:10:36.096746 140196829169536 deprecation.py:339] From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:540: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nTraceback (most recent call last):\r\n  File \"/content/models/research/object_detection/model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/content/models/research/object_detection/model_main_tf2.py\", line 110, in main\r\n    record_summaries=FLAGS.record_summaries)\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 540, in train_loop\r\n    train_dataset_fn)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py\", line 340, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 1143, in experimental_distribute_datasets_from_function\r\n    return self.distribute_datasets_from_function(dataset_fn, options)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 1135, in distribute_datasets_from_function\r\n    dataset_fn, options)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 547, in _distribute_datasets_from_function\r\n    options)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 162, in get_distributed_datasets_from_function\r\n    input_contexts, strategy, options)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 1273, in __init__\r\n    self._input_contexts, self._input_workers, dataset_fn))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 1936, in _create_datasets_from_function_with_input_context\r\n    dataset = dataset_fn(ctx)\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 535, in train_dataset_fn\r\n    input_context=input_context)\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/inputs.py\", line 898, in train_input\r\n    reduce_to_frame_fn=reduce_to_frame_fn)\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py\", line 210, in build\r\n    decoder = decoder_builder.build(input_reader_config)\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/builders/decoder_builder.py\", line 64, in build\r\n    load_keypoint_depth_features=input_reader_config\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/data_decoders/tf_example_decoder.py\", line 416, in __init__\r\n    default_value=''),\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/data_decoders/tf_example_decoder.py\", line 103, in __init__\r\n    default_value=-1)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\", line 314, in __init__\r\n    super(StaticHashTable, self).__init__(default_value, initializer)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\", line 179, in __init__\r\n    self._resource_handle = self._create_resource()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\", line 322, in _create_resource\r\n    name=self._name)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_lookup_ops.py\", line 121, in hash_table_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node HashTableV2}} = HashTableV2[container=\"\", key_dtype=DT_FLOAT, shared_name=\"8716\", use_node_name_sharing=false, value_dtype=DT_INT64]\r\nAll kernels registered for op HashTableV2:\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_STRING]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_INT64]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_INT32]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_FLOAT]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_DOUBLE]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_BOOL]\r\n  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_STRING]\r\n  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_INT64]\r\n  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_INT32]\r\n  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_FLOAT]\r\n  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_DOUBLE]\r\n  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_STRING]\r\n  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_INT32]\r\n  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_FLOAT]\r\n  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_DOUBLE]\r\n [Op:HashTableV2] name: hash_table\r\n\r\n!python /content/models/research/object_detection/model_main_tf2.py \\\r\n    --pipeline_config_path={pipeline_config_path} \\\r\n    --model_dir={model_dir} \\\r\n    --checkpoint_dir={model_dir}\r\n2021-03-19 13:11:15.148998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nWARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\r\nW0319 13:11:18.025036 139725362648960 model_lib_v2.py:1051] Forced number of epochs for all eval validations to be 1.\r\nINFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: None\r\nI0319 13:11:18.025284 139725362648960 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: None\r\nINFO:tensorflow:Maybe overwriting use_bfloat16: False\r\nI0319 13:11:18.025420 139725362648960 config_util.py:552] Maybe overwriting use_bfloat16: False\r\nINFO:tensorflow:Maybe overwriting eval_num_epochs: 1\r\nI0319 13:11:18.025554 139725362648960 config_util.py:552] Maybe overwriting eval_num_epochs: 1\r\nWARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\r\nW0319 13:11:18.025730 139725362648960 model_lib_v2.py:1072] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\r\n2021-03-19 13:11:18.033286: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-19 13:11:18.034218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-03-19 13:11:18.058184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:11:18.059025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\r\ncoreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\r\n2021-03-19 13:11:18.059071: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-19 13:11:18.062061: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-03-19 13:11:18.062148: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-03-19 13:11:18.063991: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-19 13:11:18.064394: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-19 13:11:18.066400: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-19 13:11:18.067184: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-03-19 13:11:18.067424: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-19 13:11:18.067565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:11:18.068417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:11:18.069163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-19 13:11:18.069685: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-19 13:11:18.069832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:11:18.070598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\r\ncoreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\r\n2021-03-19 13:11:18.070640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-19 13:11:18.070686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-03-19 13:11:18.070729: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-03-19 13:11:18.070776: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-19 13:11:18.070818: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-19 13:11:18.070879: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-19 13:11:18.070957: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-03-19 13:11:18.071001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-19 13:11:18.071108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:11:18.071985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:11:18.072687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-19 13:11:18.072745: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-19 13:11:18.508368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-19 13:11:18.508446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-03-19 13:11:18.508469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-03-19 13:11:18.508680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:11:18.509505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:11:18.510275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-19 13:11:18.511030: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n2021-03-19 13:11:18.511089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10637 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\n2021-03-19 13:11:19.738225: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-19 13:11:20.644943: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-03-19 13:11:20.845673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0319 13:11:21.081701 139725362648960 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0319 13:11:21.082200 139725362648960 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0319 13:11:21.082447 139725362648960 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0319 13:11:21.082655 139725362648960 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0319 13:11:21.082916 139725362648960 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nI0319 13:11:21.083143 139725362648960 convolutional_keras_box_predictor.py:154] depth of additional conv before box predictor: 0\r\nTraceback (most recent call last):\r\n  File \"/content/models/research/object_detection/model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/content/models/research/object_detection/model_main_tf2.py\", line 88, in main\r\n    wait_interval=300, timeout=FLAGS.eval_timeout)\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 1096, in eval_continuously\r\n    model=detection_model))\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/inputs.py\", line 1056, in eval_input\r\n    reduce_to_frame_fn=reduce_to_frame_fn)\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py\", line 210, in build\r\n    decoder = decoder_builder.build(input_reader_config)\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/builders/decoder_builder.py\", line 64, in build\r\n    load_keypoint_depth_features=input_reader_config\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/data_decoders/tf_example_decoder.py\", line 416, in __init__\r\n    default_value=''),\r\n  File \"/usr/local/lib/python3.7/dist-packages/object_detection/data_decoders/tf_example_decoder.py\", line 103, in __init__\r\n    default_value=-1)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\", line 314, in __init__\r\n    super(StaticHashTable, self).__init__(default_value, initializer)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\", line 179, in __init__\r\n    self._resource_handle = self._create_resource()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\", line 322, in _create_resource\r\n    name=self._name)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_lookup_ops.py\", line 121, in hash_table_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node HashTableV2}} = HashTableV2[container=\"\", key_dtype=DT_FLOAT, shared_name=\"5060\", use_node_name_sharing=false, value_dtype=DT_INT64]\r\nAll kernels registered for op HashTableV2:\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_STRING]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_INT64]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_INT32]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_FLOAT]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_DOUBLE]\r\n  device='CPU'; key_dtype in [DT_STRING]; value_dtype in [DT_BOOL]\r\n  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_STRING]\r\n  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_INT64]\r\n  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_INT32]\r\n  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_FLOAT]\r\n  device='CPU'; key_dtype in [DT_INT64]; value_dtype in [DT_DOUBLE]\r\n  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_STRING]\r\n  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_INT32]\r\n  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_FLOAT]\r\n  device='CPU'; key_dtype in [DT_INT32]; value_dtype in [DT_DOUBLE]\r\n [Op:HashTableV2] name: hash_table\r\n\r\n\r\n# Commented out IPython magic to ensure Python compatibility.\r\n# %reload_ext tensorboard\r\n# %tensorboard --logdir '/content/training/'", "comments": ["@Fawcett-cpu \r\nCould you try to call\r\n\"tf.config.experimental_connect_to_cluster(cluster_resolver)\" and let us know.\r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\nPlease upgrade your tensorflow version to 2.4.1 and let us know if you still face the issue.\r\n\r\nPlease provide with indented code such that we can replicate the issue reported or share a colab gist with the code and error.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47913\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47913\">No</a>\n"]}, {"number": 47912, "title": "Weights in tf.estimator.BoostedTreesClassifier()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): PIP\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: NVIDIA Quadro T2000 with Max Q-design, 4GB. 64GB RAM\r\n\r\n**Describe the current behavior**\r\nError when setting the weight_column in ```tf.estimator.BoostedTreesClassifier()```\r\nEncoded as a feature using tf.feature_column.numeric_column() and appended to list feature_columns.\r\n\r\nIt works perftectly with ```tf.estimator.DNNClassifier()```\r\n\r\n**Describe the expected behavior**\r\nNo error :)\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\ntf.estimator.BoostedTreesClassifier(feature_columns,\r\n                                    n_batches_per_layer = 1,\r\n                                    weight_column = 'weeks_to_complete_weights',\r\n                                    n_trees = 100)\r\n```\r\n**Other info / logs**\r\nTypeError: Expected float32 passed to parameter 'y' of op 'Mul', got 'weeks_to_complete_weights' of type 'str' instead. Error: Expected float32, got 'weeks_to_complete_weights' of type 'str' instead.", "comments": ["@mattiasthalen,\r\nOn running the code, I am facing an error stating `NameError: name 'feature_columns' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/dc2d218f7b1686832cda85f47bc9761f/47912.ipynb). \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47912\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47912\">No</a>\n"]}, {"number": 47911, "title": "[TFLM] SVDF kernel refactoring", "body": "In preparation for a PR for CEVA-DSP optimized SVDF kernel - moved the Prepare function into svdf_common as we did for previous kernels + since we have an optimized implementation for float32 as well, moved the reference code into svdf_common too so it can be used a fallback.\r\n \r\n(please ignore the depthwise commit in there, was removed)", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47910, "title": "Tracking python lists (or ListWrapper) in keras custom preprocessing layers", "body": "### System information\r\n\r\n-   **Custom code**\r\n-   **OS Platform and Distribution**:\r\n\r\n> - PRETTY_NAME=\"Debian GNU/Linux 9 (stretch)\"\r\n> - NAME=\"Debian GNU/Linux\"\r\n> - VERSION_ID=\"9\"\r\n> - VERSION=\"9 (stretch)\"\r\n> - ID=debian\r\n\r\n-   **TensorFlow installed from binaries**:\r\n-   **TensorFlow version**: 2.4.0\r\n-   **Python version**: 3.6.8\r\n\r\n### Problem\r\nHi,\r\nMy problem is the following: I try to create a custom keras preprocessing layer in which I absolutely need an attribute which is a regular python list computed via adapt method. When I include my layer in model and I save it (via SavedModel), my list attribute becomes empty at load time. Is there anyway to make this list (or any other python object) trackable so that it is properly loaded?\r\n\r\nSome additional points:\r\n- It can be made trackable if I use a variable instead of a list but in my case a list is necessary.\r\n- I still can use a variable and transform it to a list at call time, but this will work only in eager mode and I need it to work at graph mode also\r\n\r\n### Source code / logs\r\nHere a simple example to reproduce my issue:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.ops import math_ops\r\n\r\n\r\n@tf.keras.utils.register_keras_serializable()\r\nclass BucketizeLayer(tf.keras.layers.experimental.preprocessing.PreprocessingLayer):\r\n    \r\n    def __init__(self, quantiles, **kwargs):\r\n        self.quantiles = quantiles\r\n        super(BucketizeLayer, self).__init__(**kwargs)\r\n        self._boundaries = None\r\n        \r\n    def adapt(self, data):\r\n        if isinstance(data, tf.Tensor):\r\n            data = data.numpy()\r\n            \r\n        self._boundaries = np.nanquantile(data, self.quantiles).tolist()\r\n        \r\n    def call(self, data):\r\n        return math_ops.bucketize(data, self._boundaries)\r\n    \r\n    def get_config(self):\r\n        config = {'quantiles': self.quantiles}\r\n        base_config = super(BucketizeLayer, self).get_config()\r\n        return dict(**config, **base_config)\r\n\r\n\r\ndata = 3 * np.random.randn(100) + 5\r\nbucketize = BucketizeLayer(quantiles=[0.1, 0.5, 0.9])\r\nbucketize.adapt(data)\r\nprint(bucketize(data))\r\n\r\n\r\ninp = tf.keras.Input(shape=(1,), dtype=tf.float32)\r\nmodel = tf.keras.models.Model(inp, bucketize(inp))\r\nmodel.save('bucketize/')\r\ndel model\r\nmodel = tf.keras.models.load_model('bucketize/')\r\nmodel.layers[1]._boundaries\r\n# output: ListWrapper([])\r\n```\r\n", "comments": ["@ianqouda \r\ni ran the code shared and do not see an empty list, please refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/ac2b96a3d295f7744050b242d95fffb4/untitled565.ipynb) and share a colab gist with the error reported.", "@Saduf2019 thank you for your reponse, here is a [gist](https://colab.research.google.com/gist/ianqouda/a770f4cce6ef7d598cf5d4d7330a34d2/untitled565.ipynb) with more detailed code. The problem is that the attribute '_boubdaries' becomes empty after loading the model.  ", "I am able to replicate the issue reported on tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d0a9b04b87978907c7c112f7fad16104/untitled580.ipynb)", "@ianqouda I updated your code (mainly adding `get_config` method and `self._boundaries = boundaries`). With those modifications, everything works as expected. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/23725bbc9fb3d13952172dc1fb77a229/untitled580.ipynb).\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "@jvishnuvardhan Thank you very much! This works fine for this example I showed here but in more complexe context (which is my case) this solution alters a little bit the behaviour of my layer. More precisely if I have this boundary as an argument which I can set manually and force the adapt method not to override it then this solution would not work any more.\r\n\r\nIn the mean time I found another way to do it by creating a trackable list through inhereting from this object from tensorflow.train.experimental.PythonState and it works fine!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47910\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47910\">No</a>\n"]}, {"number": 47909, "title": "TF.keras in TF2.4", "body": "What are the differences in tf.keras in TF2.4 with respect to TF1.15 version? ", "comments": ["@Alok-Ranjan23 \r\nThere is no support for tf 1.x and there is support for tf 2.x, tf 2.4 is the latest stable version.\r\nFor more differences refer to this [link](https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/) and [this](https://stackoverflow.com/questions/59112527/primer-on-tensorflow-and-keras-the-past-tf1-the-present-tf2).\r\n\r\nAlso please move this to closed status as it is not a bug or a feature request, for any more clarity kindly open a [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) issue for this as it is not a bug or feature request and here is a big community to support and learn from your questions.\r\nThanks!"]}, {"number": 47908, "title": "TF2 + GH", "body": "![image](https://user-images.githubusercontent.com/24195074/111758231-24de0700-8894-11eb-9778-1716c8fdb9dd.png)\r\n\r\nI am getting list of error as I am trying to change TF2 BERT from using tfhub bert to hugging face bert.\r\n\r\nThe accuracy and results are not similar to using tfhub bert besides I am getting all the error above.\r\n\r\nI am using this:\r\nfrom transformers import BertConfig, AutoTokenizer, TFAutoModel\r\nmodel_name = \"bert-base-uncased\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\nconfig = BertConfig.from_pretrained(model_name, num_labels=2, return_dict=True,output_hidden_states=True,use_cache=True,output_attentions=True)\r\nbert = TFAutoModel.from_pretrained(model_name, config=config)\r\n\r\n\r\n\r\n", "comments": ["@Rababalkhalifa,\r\nCould you please let us know the TensorFlow version you are using? \r\n\r\nI did not face any warnings while running the code with TF v2.4.1. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/dd598bcdb663e7373ce48b960beb90f4/47908.ipynb). Thanks!", "Also, you can suppress the warning messages by changing the log level at the start of the program \r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\nimport tensorflow as tf\r\n```\r\n\r\nor you can set the [autograph verbosity](https://www.tensorflow.org/api_docs/python/tf/autograph/set_verbosity\r\n) level using\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nos.environ['AUTOGRAPH_VERBOSITY'] = 1\r\n```\r\nThanks!", "I am using tf 2.4.1 in colab.\r\nThe code was fine before changing to HF layers. All what I did is changing the tokenizer and bert parts.\r\nusing batch encoder and replacing this:\r\n  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='encoder')\r\nwith:\r\n  net = bert(inputs['input_word_ids'], attention_mask=inputs['input_mask'])[0]", "> I am using tf 2.4.1 in colab.\r\n\r\n@Rababalkhalifa,\r\nCould you please share the Colab notebook with us, so that we can look into this? Thanks!", "similar to this: https://colab.research.google.com/drive/1N-rSuvqD8hoM3_wh7HLQQfdB52jvuv_E#scrollTo=GzRZfBxlPHvf\r\n\r\nbut if you want my code email me at r.alkhalifah @ hot", "@ymodak,\r\nI was able to reproduce the issue with TF v2.4.1, TF v2.5.0-rc0 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/734fce9c108fedd0ec88613986a9750e/47908.ipynb). Thanks!", "I think [huggingface/transformers ](https://github.com/huggingface/transformers)repository is right place for this issue since the you have imports from transformers module. Please try posting this issue on transformers repository. Thanks!\r\n> from transformers import BertConfig, AutoTokenizer, TFAutoModel\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47908\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47908\">No</a>\n"]}, {"number": 47907, "title": "[TFLM] For CEVA-DSP Makefiles: Added ceva_dsp_lib to linker, added ceva_common.h to headers list", "body": "CEVA-DSP Makefiles: Added ceva_dsp_lib to linker, added ceva_common.h to headers list\r\nFor pulling in the header file added in this PR: https://github.com/tensorflow/tensorflow/pull/47783\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47906, "title": "Using custom callback to set trainable=true doesn't work when use model.fit()", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.2.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tensorflow==2.3.0\r\n- Python version: 3.6.10\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n\r\n\r\nHi, I want to train the model with transfer learning. First, I load the pretrain model, then I freeze some of layer at front of model. After several epoch, I want to unfreeze the weight for training. I use callback to write some  snippet to test it. But I found that when I set the trainable to True, the model still not training...\r\n\r\nCode:\r\n\r\n[Colab link](https://colab.research.google.com/drive/1suX_4qMZ0yyBGaIqmDGFLa6KN0hMK5Jo?usp=sharing)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport os\r\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint\r\n\r\nclass UnFreezeWeight(tf.keras.callbacks.Callback):\r\n    def __init__(self, freeze_before_epoch):\r\n        super().__init__()\r\n        self.freeze_before_epoch = freeze_before_epoch\r\n\r\n    def on_epoch_begin(self, epoch, logs=None):\r\n        if self.freeze_before_epoch != epoch:\r\n            return\r\n\r\n        # Unfreeze all weight.\r\n        print('set trainable to True.')\r\n        for layer in self.model.layers:\r\n            layer.trainable = True\r\n\r\n\r\ndef _normalize_img(img, label):\r\n    img = tf.cast(img, tf.float32) / 255.\r\n    return img, label\r\n\r\n\r\ndef main():\r\n    train_ds = tfds.load('mnist', split='train', as_supervised=True)\r\n    train_ds = train_ds.batch(32)\r\n    train_ds = train_ds.map(_normalize_img)\r\n\r\n    valid_ds = tfds.load('mnist', split='test', as_supervised=True)\r\n    valid_ds = valid_ds.batch(32)\r\n    valid_ds = valid_ds.map(_normalize_img)\r\n\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n        tf.keras.layers.Dense(128, activation='relu'),\r\n        tf.keras.layers.Dropout(0.2),\r\n        tf.keras.layers.Dense(10)\r\n    ])\r\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n\r\n    # Freeze all weight.\r\n    for layer in model.layers:\r\n        layer.trainable = False\r\n        \r\n    # If I unfreeze at epoch index 0, the model will learning.\r\n    # But if I un freeze after epoch index 1, the model won't training.\r\n    freeze = UnFreezeWeight(1)\r\n\r\n    model.compile(optimizer='adam',\r\n                  loss=loss_fn,\r\n                  metrics=['accuracy'])\r\n\r\n    model.fit(train_ds,\r\n              validation_data=valid_ds,\r\n              callbacks=[freeze],\r\n              epochs=5)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nOutput:\r\nWhen unfreeze weight at epoch 2\r\n```\r\nEpoch 1/5\r\n1875/1875 [==============================] - 9s 5ms/step - loss: 2.3907 - accuracy: 0.1362 - val_loss: 2.3740 - val_accuracy: 0.1488\r\nEpoch 2/5\r\nset trainable to True.\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 2.3922 - accuracy: 0.1349 - val_loss: 2.3740 - val_accuracy: 0.1488\r\nEpoch 3/5\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 2.3910 - accuracy: 0.1361 - val_loss: 2.3740 - val_accuracy: 0.1488\r\nEpoch 4/5\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 2.3914 - accuracy: 0.1378 - val_loss: 2.3740 - val_accuracy: 0.1488\r\nEpoch 5/5\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 2.3926 - accuracy: 0.1346 - val_loss: 2.3740 - val_accuracy: 0.1488\r\n```\r\n\r\nWhen unfreeze weight at epoch 1\r\n```\r\nEpoch 1/5\r\nset trainable to True.\r\n1875/1875 [==============================] - 12s 6ms/step - loss: 0.4833 - accuracy: 0.8568 - val_loss: 0.1364 - val_accuracy: 0.9607\r\nEpoch 2/5\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.1502 - accuracy: 0.9567 - val_loss: 0.0991 - val_accuracy: 0.9711\r\nEpoch 3/5\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.1100 - accuracy: 0.9668 - val_loss: 0.0870 - val_accuracy: 0.9751\r\nEpoch 4/5\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0910 - accuracy: 0.9734 - val_loss: 0.0776 - val_accuracy: 0.9744\r\nEpoch 5/5\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0768 - accuracy: 0.9759 - val_loss: 0.0790 - val_accuracy: 0.9751\r\n```\r\n", "comments": ["@jason9075 \r\nI ran the code shared please refer to [this gist ]( https://colab.research.google.com/gist/Saduf2019/3fd5ae49f624f7ce0d0372602b805101/untitled565.ipynb)and let us know the expected output.", "Hi Saduf2019\r\n\r\nI have some problem at line:\r\n```\r\nfreeze = UnFreezeWeight(10)\r\n```\r\nIf I unfreeze at 0, everything runs as expected.\r\nBut if I unfreeze at other epoch index, the val accuracy won't raise.\r\n\r\n[colab gist](https://colab.research.google.com/gist/jason9075/a97443f466a9028f7b2f3ffa57e9586c/untitled565.ipynb) (fixed)\r\n\r\n```\r\nEpoch 1/10\r\n1875/1875 [==============================] - 10s 5ms/step - loss: 2.4291 - accuracy: 0.1063 - val_loss: 2.4094 - val_accuracy: 0.1150\r\nEpoch 2/10\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 2.4317 - accuracy: 0.1056 - val_loss: 2.4094 - val_accuracy: 0.1150\r\nEpoch 3/10\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 2.4287 - accuracy: 0.1070 - val_loss: 2.4094 - val_accuracy: 0.1150\r\nEpoch 4/10\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 2.4289 - accuracy: 0.1064 - val_loss: 2.4094 - val_accuracy: 0.1150\r\nEpoch 5/10\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 2.4298 - accuracy: 0.1053 - val_loss: 2.4094 - val_accuracy: 0.1150\r\nEpoch 6/10\r\nset trainable to True.\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 2.4275 - accuracy: 0.1068 - val_loss: 2.4094 - val_accuracy: 0.1150\r\nEpoch 7/10\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 2.4283 - accuracy: 0.1075 - val_loss: 2.4094 - val_accuracy: 0.1150\r\nEpoch 8/10\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 2.4290 - accuracy: 0.1058 - val_loss: 2.4094 - val_accuracy: 0.1150\r\nEpoch 9/10\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 2.4284 - accuracy: 0.1062 - val_loss: 2.4094 - val_accuracy: 0.1150\r\nEpoch 10/10\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 2.4281 - accuracy: 0.1057 - val_loss: 2.4094 - val_accuracy: 0.1150\r\n```", "@jason9075 \r\n Incorrect gist shared in the above comment, please share the colab for which the output is posted above.", "@Saduf2019 \r\n\r\nSorry, wrong url...\r\n[this one](https://colab.research.google.com/gist/jason9075/a97443f466a9028f7b2f3ffa57e9586c/untitled565.ipynb)", "@jason9075\r\nI ran the gist shared on tf 2.3, tf 2.4 and nightly, please refer to [this gist](https://colab.research.google.com/gist/Saduf2019/4d8ef8dc33eb6dac577aed7edbca1032/untitled573.ipynb) and confirm.", "Hi, @Saduf2019 \r\n\r\nIn your gist, the tf2.4 the val_loss remain at 2.4462 and val_accuracy remain at 0.0984 after epoch 6 just show that the model is not learning.\r\nAnd tf 2.3 and nightly is also remain at same val_loss and val_accuracy.\r\nI expect my model start learning after epoch 6 but is not.\r\n", "Was able to replicate the issue in TF 2.6.0-dev20210531,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/a18f116207bdeb344488e90c5ae51ea1/untitled166.ipynb)..Thanks !", "@jason9075 This got resolved in `TF2.7`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/9fc435f9586f4141814ad5e580fef69c/untitled.ipynb).\r\n\r\nI updated your code by adding one line to the callback.\r\n```\r\n# Unfreeze all weight.\r\nself.model.make_train_function(force=True)\r\n```\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "@jvishnuvardhan It works!\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47906\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47906\">No</a>\n"]}, {"number": 47904, "title": "ASL Gesture Recognition SSD Model not showing labels upon running", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): version 2, ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\nThe image below shows how the label map was created for the ssd model.\r\n![ssd_model_label_code](https://user-images.githubusercontent.com/80737339/111729623-d86bda80-8845-11eb-8799-b2595f4be735.jpeg)\r\n\r\n\r\n### 3. Failure after conversion\r\nThe code is for ASL gesture recognition and translates ASL to text. The conversion is successful, however, when running on android studio, the labels of the ASL signs do not show but the accuracy does. What shows up on the android app is a '}' followed by the accuracy. We are assuming it is because of how the labels are defined in the label map file. We used the same object detection Android Studio project as the one in the link below. We just used our ssd tflite model instead of theirs. The labels show for their example and tflite model, but it does not show the labels for our model. \r\n\r\nExample Android Studio project used: https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android\r\n\r\nAny suggestions on how to get the labels to show when running on Android Studio will be appreciated.", "comments": ["@srjoglekar246 could you take a look?", "@saw999 To confirm, how big is your converted model? I assume you converted with the instructions [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md)?\r\n\r\n@lu-wang-g for help with setting up the Android app.", "@saw999 did you use `TF2.0` or recent stable version. It is recommended to use recent TF version. Thanks!", "> @saw999 To confirm, how big is your converted model? I assume you converted with the instructions [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md)?\r\n> \r\n> @lu-wang-g for help with setting up the Android app.\r\n\r\nThe ssd model is 11MB and we followed the same instructions to convert the model.\r\n", "> @saw999 did you use `TF2.0` or recent stable version. It is recommended to use recent TF version. Thanks!\r\n\r\nYes, we used TF2.0.", "The label file format is not compatible with what the Android example expects. See there for [an example](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/src/main/assets/labelmap.txt) of the label file.", "> The label file format is not compatible with what the Android example expects. See there for [an example](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/src/main/assets/labelmap.txt) of the label file.\r\n\r\nWe imported the label file into Android Studio in the format shown below:\r\n![labeltext](https://user-images.githubusercontent.com/80737339/111830138-f24c0280-88c3-11eb-917b-559c96469340.png)\r\n", "The above file looks good. Did you pack the associated file into the model? If not, you can do so by following the answer [here](https://stackoverflow.com/a/64493506/11031225).", "@saw999,\r\n\r\nCan you take a look at the above comment by @lu-wang-g and let us know if it helps in resolving your issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47904\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47904\">No</a>\n"]}, {"number": 47903, "title": "Train.py error Blas GEMM launch failed", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I used nmt code https://github.com/daniel-kukiela/nmt-chatbot\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from binary\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3.6.5\r\n- CUDA/cuDNN version: Cuda 10, cuDNN v7.6.5\r\n- GPU model and memory: Nvidia Geforce 3070 8gb\r\n\r\n\r\n\r\n**Im getting a large error**\r\n\r\n``\r\nTraceback (most recent call last):                                                                                                                File \"train.py\", line 17, in <module>                                                                                                             tf.app.run(main=nmt.main, argv=[os.getcwd() + '\\nmt\\nmt\\nmt.py'] + unparsed)                                                                  File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run          _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)                                                                          File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\absl\\app.py\", line 303, in run                                    _run_main(main, args)                                                                                                                         File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main                              sys.exit(main(argv))                                                                                                                          File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\nmt.py\", line 539, in main                                                                run_main(FLAGS, default_hparams, train_fn, inference_fn)                                                                                      File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\nmt.py\", line 532, in run_main                                                            train_fn(hparams, target_session=target_session)                                                                                              File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\train.py\", line 266, in train                                                             sample_tgt_data)                                                                                                                              File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\train.py\", line 140, in run_full_eval                                                     eval_model, eval_sess, model_dir, hparams, summary_writer)                                                                                    File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\train.py\", line 71, in run_internal_eval                                                  summary_writer, \"dev\")                                                                                                                        File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\train.py\", line 420, in _internal_eval                                                    ppl = model_helper.compute_perplexity(model, sess, label)                                                                                     File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\model_helper.py\", line 453, in compute_perplexity                                         loss, predict_count, batch_size = model.eval(sess)                                                                                            File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\model.py\", line 251, in eval                                                              self.batch_size])                                                                                                                             File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 956, in run       run_metadata_ptr)                                                                                                                             File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1180, in _run     feed_dict_tensor, options, run_metadata)                                                                                                      File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1359, in _do_run                                                                                                                                                  run_metadata)                                                                                                                                 File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1384, in _do_call                                                                                                                                                 raise type(e)(node_def, op, message)                                                                                                        tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.                                                                     (0) Internal: Blas GEMM launch failed : a.shape=(135, 512), b.shape=(512, 15003), m=135, n=15003, k=512                                                [[node dynamic_seq2seq/decoder/output_projection/Tensordot/MatMul (defined at C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]                                                                                        [[dynamic_seq2seq/truediv/_195]]                                                                                                         (1) Internal: Blas GEMM launch failed : a.shape=(135, 512), b.shape=(512, 15003), m=135, n=15003, k=512                                                [[node dynamic_seq2seq/decoder/output_projection/Tensordot/MatMul (defined at C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]                                                                               0 successful operations.                                                                                                                        0 derived errors ignored.                                                                                                                                                                                                                                                                       Original stack trace for 'dynamic_seq2seq/decoder/output_projection/Tensordot/MatMul':                                                            File \"train.py\", line 17, in <module>                                                                                                             tf.app.run(main=nmt.main, argv=[os.getcwd() + '\\nmt\\nmt\\nmt.py'] + unparsed)                                                                  File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run          _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)                                                                          File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\absl\\app.py\", line 303, in run                                    _run_main(main, args)                                                                                                                         File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main                              sys.exit(main(argv))                                                                                                                          File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\nmt.py\", line 539, in main                                                                run_main(FLAGS, default_hparams, train_fn, inference_fn)                                                                                      File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\nmt.py\", line 532, in run_main                                                            train_fn(hparams, target_session=target_session)                                                                                              File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\train.py\", line 223, in train                                                             eval_model = model_helper.create_eval_model(model_creator, hparams, scope)                                                                    File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\model_helper.py\", line 157, in create_eval_model                                          extra_args=extra_args)                                                                                                                        File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\attention_model.py\", line 61, in __init__                                                 extra_args=extra_args)                                                                                                                        File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\model.py\", line 97, in __init__                                                           res = self.build_graph(hparams, scope=scope)                                                                                                  File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\model.py\", line 284, in build_graph                                                       encoder_outputs, encoder_state, hparams)                                                                                                      File \"C:\\Users\\Harjyot\\Desktop\\code\\nmt-chatbot/nmt\\nmt\\model.py\", line 407, in _build_decoder                                                    logits = self.output_layer(outputs.rnn_output)                                                                                                File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\layers\\base.py\", line 548, in __call__     outputs = super(Layer, self).__call__(inputs, *args, **kwargs)                                                                                File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 854, in __call__                                                                                                                                         outputs = call_fn(cast_inputs, *args, **kwargs)                                                                                               File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\", line 234, in wrapper                                                                                                                                               return converted_call(f, options, args, kwargs)                                                                                               File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\", line 439, in converted_call                                                                                                                                        return _call_unconverted(f, args, kwargs, options)                                                                                            File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\", line 330, in _call_unconverted                                                                                                                                     return f(*args, **kwargs)                                                                                                                     File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\core.py\", line 1039, in call                                                                                                                                                  outputs = standard_ops.tensordot(inputs, self.kernel, [[rank - 1], [0]])                                                                      File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\", line 4071, in tensordot                                                                                                                                                  ab_matmul = matmul(a_reshape, b_reshape)                                                                                                      File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\", line 180, in wrapper                                                                                                                                                    return target(*args, **kwargs)                                                                                                                File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\", line 2754, in matmul     a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)                                                                            File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\", line 6136, in mat_mul                                                                                                                                                name=name)                                                                                                                                    File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper                                                                                                                                op_def=op_def)                                                                                                                                File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func                                                                                                                                                return func(*args, **kwargs)                                                                                                                  File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op                                                                                                                                                 attrs, op_def, compute_device)                                                                                                                File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal                                                                                                                                       op_def=op_def)                                                                                                                                File \"C:\\Users\\Harjyot\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__                                                                                                                                                  self._traceback = tf_stack.extract_stack()\r\n``\r\n\r\nthis error is when i run train.py\r\n", "comments": ["@hsahni01 \r\nWe see that you are using tf 1.x and there is no support for 1.x, please upgrade to 2.x and let u know.\r\n\r\nYou may refer to closed similar issues: #37233, #11812, #25403, [link](https://github.com/tensorflow/tensorflow/issues/36781#issuecomment-593611624)\r\n", "Wait but this code is in tf 1.15 tho", "@hsahni01 \r\nPlease upgrade to 2.0 and let us know if you face any issues.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47903\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47903\">No</a>\n"]}, {"number": 47902, "title": "Unable to link c++ project due to missing header files", "body": "**System information**\r\n- Linux Ubuntu 18.04\r\n- Tensorflow 2.4.1 (from source)\r\n- Bazel 4.0.0\r\n- g++ 7.5.0\r\n\r\n**Describe the problem**\r\nI am trying to run some c++ source examples that I've found around, but they require the following header files:\r\n```\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n```\r\nI ran into a compile issue:\r\n```\r\n/home/cargospectre/tensor/package/tensorflow-2.4.1/tensorflow/core/public/session.h:22:10: fatal error: tensorflow/core/framework/device_attributes.pb.h: No such file or directory\r\n```\r\nI found this file `/tensorflow-2.4.1/bazel-out/host/bin/tensorflow/core/framework/device_attributes.pb.h` and moved it to `tensorflow/core/framework/`, same with `graph.pb.h`. That seemed to work. However, now I'm running into this issue:\r\n\r\n```\r\n/home/cargospectre/tensor/package/tensorflow-2.4.1/tensorflow/core/framework/device_attributes.pb.h:10:10: fatal error: google/protobuf/port_def.inc: No such file or directory\r\n #include <google/protobuf/port_def.inc>\r\n```\r\nI think I found the protobuf that was downloaded for tensorflow at `tensorflow-2.4.1/bazel-bin/external/com_google_protobuf`, but it doesn't contain this file. I think the version is different? I tried independently downloading protobuf but this file still didn't exist. Here's my protobuf version:\r\n```\r\nprotoc --version\r\nlibprotoc 3.0.0\r\n```\r\n\r\nSo I'm kind of stuck at this point. Am I doing this right? Is there an easier way to access the c++ api? Is there a different example I need to be following?\r\n\r\nEDIT: I tried cloning protobuf and building from source, which seems to be version 3.15.6, but now I'm missing yet another file\r\n```\r\nprotoc --version\r\nlibprotoc 3.15.6\r\n\r\n/home/cargospectre/tensor/package/tensorflow-2.4.1/tensorflow/core/framework/device_attributes.pb.h:28:10: fatal error: google/protobuf/inlined_string_field.h: No such file or directory\r\n #include <google/protobuf/inlined_string_field.h>\r\n```", "comments": ["Hi, since this is a usage question / trouble shooting, I think it better belongs to StackOverflow, etc,...\r\n\r\n(Though this could help: https://www.tensorflow.org/guide/create_op , see `tf.sysconfig.get_compile_flags()` and `tf.sysconfig.get_link_flags()`)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47902\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47902\">No</a>\n"]}, {"number": 47900, "title": "[PluggableDevice] New device type support in python front-end", "body": "Enhance python front-end valid device type check , making dynamically registered device type be a valid device type.", "comments": ["@penpornk @allenlavoie This PR is to enhance python front-end valid device type check for those dynamically registered device types, please help to review. Thanks very much."]}, {"number": 47899, "title": "Add documentation for numeric_column api.", "body": "Added documentation for number_column api that will work self sufficiently.\r\n\r\nPull request for the work item - https://github.com/tensorflow/tensorflow/issues/47286", "comments": ["Thank you @MarkDaoust for taking time out for the review. I have implemented all the review comments. Feedback is welcome.", "@MarkDaoust , one check is failing, but I am unable to see the reason why. Could you kindly direct me? Thank you once again.", "> @MarkDaoust , one check is failing, but I am unable to see the reason why. Could you kindly direct me? Thank you once again.\r\n\r\nI'm fixing this internally, it should be merged soon."]}, {"number": 47898, "title": "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f817468a4c0>> and will run it as-is", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Docker image nvidia/cuda:10.2-devel-ubuntu18.04 (running Kebernetes on GCP)\r\n- TensorFlow installed from (source or binary): 2.4.0\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: Python 3.8.0\r\n- CUDA/cuDNN version: cuda:10.2\r\n- GPU model and memory:  NVIDIA Tesla T4 16 GB GDDR6\r\n\r\nCame across this message while debugging a problem in our processing pipeline, with the request to file a bug report with export AUTOGRAPH_VERBOSITY=10\r\n...\r\nTypeError: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\r\nError transforming entity <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f817468a4c0>>\r\nWARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f817468a4c0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f817468a4c0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n...\r\n\r\nFull output is rather large:\r\nhttps://pastebin.com/raw/G9jA8fXm\r\n", "comments": ["@coert \r\nPlease share simple stand aloe code such that we can replicate the issue reported or if possible share a colab gist.\r\n Mean while please refer to existing issues with same error: [link](https://github.com/tensorflow/tensorflow/issues/46177#issuecomment-799619015), #37144, [link1](https://stackoverflow.com/questions/61551592/tensorflow-fails-to-create-autograph-out-a-working-piece-of-code)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47898\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47898\">No</a>\n"]}]