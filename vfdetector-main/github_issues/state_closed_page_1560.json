[{"number": 6115, "title": "[Windows] How can we call builtin ops function in Windows like _set_ops.dense_to_dense_set_operation?", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttp://stackoverflow.com/questions/40947669/attributeerror-nonetype-object-has-no-attribute-dense-to-dense-set-operation\r\n\r\nOperating System: Windows\r\n\r\nInstalled version of CUDA and cuDNN: \r\n ```\r\nDirectory of C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\\x64\r\n\r\n12/02/2016  02:50 PM    <DIR>          .\r\n12/02/2016  02:50 PM    <DIR>          ..\r\n09/11/2016  10:26 AM            93,804 cublas.lib\r\n09/11/2016  10:19 AM        62,239,924 cublas_device.lib\r\n09/05/2016  02:51 PM            87,834 cuda.lib\r\n09/05/2016  02:51 PM           681,064 cudadevrt.lib\r\n09/05/2016  02:51 PM            64,550 cudart.lib\r\n09/05/2016  02:51 PM         2,318,456 cudart_static.lib\r\n07/26/2016  11:31 PM            37,452 cudnn.lib\r\n09/05/2016  02:51 PM            17,042 cufft.lib\r\n09/05/2016  02:51 PM            15,838 cufftw.lib\r\n09/05/2016  02:51 PM             8,534 curand.lib\r\n09/05/2016  02:51 PM           115,226 cusolver.lib\r\n09/05/2016  02:51 PM           175,020 cusparse.lib\r\n09/05/2016  02:51 PM             4,746 nppc.lib\r\n09/05/2016  02:51 PM         1,312,172 nppi.lib\r\n09/05/2016  02:51 PM           202,098 nppial.lib\r\n09/05/2016  02:51 PM           102,322 nppicc.lib\r\n09/05/2016  02:51 PM             9,324 nppicom.lib\r\n09/05/2016  02:51 PM           176,272 nppidei.lib\r\n09/05/2016  02:51 PM           244,310 nppif.lib\r\n09/05/2016  02:51 PM            73,428 nppig.lib\r\n09/05/2016  02:51 PM            25,078 nppim.lib\r\n09/05/2016  02:51 PM           441,650 nppist.lib\r\n09/05/2016  02:51 PM             8,416 nppisu.lib\r\n09/05/2016  02:51 PM            55,026 nppitc.lib\r\n09/05/2016  02:51 PM           214,486 npps.lib\r\n09/05/2016  02:51 PM            11,250 nvblas.lib\r\n09/05/2016  02:51 PM             6,814 nvcuvid.lib\r\n09/05/2016  02:51 PM             8,720 nvgraph.lib\r\n09/05/2016  02:51 PM            40,096 nvml.lib\r\n09/05/2016  02:51 PM             3,954 nvrtc.lib\r\n09/05/2016  02:51 PM            23,076 OpenCL.lib\r\n              31 File(s)     68,817,982 bytes\r\n               2 Dir(s)  206,249,398,272 bytes free\r\n```\r\n\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nhttps://github.com/tensorflow/tensorflow.git\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n```\r\n(tensorflow) C:\\Users\\e-budur>python -c \"import tensorflow; print(tensorflow.__v\r\nersion__)\"\r\nI c:\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opene\r\nd CUDA library cublas64_80.dll locally\r\nI c:\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opene\r\nd CUDA library cudnn64_5.dll locally\r\nI c:\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opene\r\nd CUDA library cufft64_80.dll locally\r\nI c:\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opene\r\nd CUDA library nvcuda.dll locally\r\nI c:\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opene\r\nd CUDA library curand64_80.dll locally\r\n0.12.head\r\n```\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\nC:\\tensorflow>git rev-parse HEAD\r\n778539ce3cb4a0effabe46e7674c22602cb08dc0\r\n\r\n2. The output of `bazel version`\r\nI've used MSBuild.\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n`true_positives = set_ops.set_intersection(predictions_idx, labels)`\r\n\r\nI've run udc_train.py by following the preliminary steps give in the repository page https://github.com/dennybritz/chatbot-retrieval.  As a result of the execution I got the following exception when the application calls the function tf.contrib.metrics.streaming_sparse_recall_at_k in udc_metrics.py\r\n\r\n### What other attempted solutions have you tried?\r\nI've rebuilded the source code by following the instruction in the following link hoping that the ops library will be compiled and embedded into the main excutable of tensorflow like '_pywrap_tensorflow.pyd' but no luck.  I kept receiving the same error after successful compilation and deployment of the latest source code (778539ce3cb4a0effabe46e7674c22602cb08dc0).\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md\r\n\r\nI've posted the question on Stackoverflow but I got no answer.\r\nhttp://stackoverflow.com/questions/40947669/attributeerror-nonetype-object-has-no-attribute-dense-to-dense-set-operation\r\n\r\n\r\n### Logs or other output that would be helpful\r\nI've downloaded the sample project Chatbot Retrieval from the following repository\r\nhttps://github.com/dennybritz/chatbot-retrieval\r\n\r\nI've run udc_train.py by following the preliminary steps give in the repository page above.  As a result of the execution I got the following exception when the application calls the function tf.contrib.metrics.streaming_sparse_recall_at_k\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:/Users/e-budur/PycharmProjects/chatbot-retrieval/udc_train.py\", line 72, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"C:/Users/e-budur/PycharmProjects/chatbot-retrieval/udc_train.py\", line 68, in main\r\n    estimator.fit(input_fn=input_fn_train, steps=None, monitors=[eval_monitor])\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 247, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 364, in fit\r\n    max_steps=max_steps)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 741, in _train_model\r\n    max_steps=max_steps)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\graph_actions.py\", line 301, in _monitored_train\r\n    None)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 473, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 628, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 595, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 737, in run\r\n    run_metadata=run_metadata))\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\monitors.py\", line 1210, in after_run\r\n    induce_stop = m.step_end(self._last_step, result)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\monitors.py\", line 409, in step_end\r\n    return self.every_n_step_end(step, output)\r\n  File \"C:/Users/e-budur/PycharmProjects/chatbot-retrieval/udc_train.py\", line 65, in every_n_step_end\r\n    steps=None)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 247, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 436, in evaluate\r\n    name=name)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 800, in _evaluate_model\r\n    eval_ops = self._get_eval_ops(features, labels, metrics)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1111, in _get_eval_ops\r\n    metrics, features, labels, model_fn_ops.predictions))\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 280, in _make_metrics_ops\r\n    result[name] = metric(predictions, labels_tensor_or_dict)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\metrics\\python\\ops\\metric_ops.py\", line 1422, in streaming_sparse_recall_at_k\r\n    weights=weights)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\metrics\\python\\ops\\metric_ops.py\", line 2102, in _streaming_sparse_true_positive_at_k\r\n    weights=weights)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\metrics\\python\\ops\\metric_ops.py\", line 2054, in _sparse_true_positive_at_k\r\n    tp = set_ops.set_size(set_ops.set_intersection(predictions_idx, labels))\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\metrics\\python\\ops\\set_ops.py\", line 138, in set_intersection\r\n    return _set_operation(a, b, \"intersection\", validate_indices)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\metrics\\python\\ops\\set_ops.py\", line 113, in _set_operation\r\n    indices, values, shape = _set_ops.dense_to_dense_set_operation(\r\nAttributeError: 'NoneType' object has no attribute 'dense_to_dense_set_operation'\r\n```\r\n\r\nThe main problem turns out to be loader.load_op_library fails to load .so files in NT system.\r\n\r\n```\r\n_set_ops = loader.load_op_library(resource_loader.get_path_to_datafile(\"_set_ops.so\"))\r\n\r\nindices, values, shape = _set_ops.dense_to_dense_set_operation(\r\n        a, b, set_operation, validate_indices) #_set_ops turns out to be None\r\n```\r\n\r\n```\r\ndef load_op_library(path):\r\n  \"\"\"Loads a contrib op library from the given path.\r\n\r\n  NOTE(mrry): On Windows, we currently assume that contrib op\r\n  libraries are statically linked into the main TensorFlow Python\r\n  extension DLL.\r\n\r\n  Args:\r\n    path: An absolute path to a shared object file.\r\n\r\n  Returns:\r\n    A Python module containing the Python wrappers for Ops defined in the\r\n    plugin.\r\n  \"\"\"\r\n  if os.name != 'nt':\r\n    path = resource_loader.get_path_to_datafile(path)\r\n    ret = load_library.load_op_library(path)\r\n    assert ret, 'Could not load %s' % path\r\n    return ret\r\n  else:\r\n    # NOTE(mrry):\r\n    return None\r\n```\r\nThe comments belongs to @mrry \r\n\r\nHence the question is : How can we call builtin ops function in Windows like _set_ops.dense_to_dense_set_operation?\r\n\r\nThanks", "comments": ["No, I don't mean custom op lib. I want to use the existing builtin op libs that are currently provided under contrib folder of tensorflow git repository. For example, ops in contrib/metrics folder.\n\n> On 6 Dec 2016, at 17:41, Adriano Carmezim <notifications@github.com> wrote:\n> \n> Do you mean load a custom op lib?\n> It is currently not supported.\n> release notes.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "Looks like this set of op registrations is missing from the Windows package. I'll implement the workaround.", "Thank you very much @mrry for your fast action.", "No problem! We'll merge this soon and get it into the nightly builds (and the upcoming 0.12rc1 release)."]}, {"number": 6114, "title": "R0.12", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 6113, "title": "[Windows/CMake] Add optimization flags", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "Jenkins, test this please.", "@googlebot I am okay with this.", "@mrry it turns out when cherrypicking changes googlebot will never be happy (known issue in cla bot)\r\nAs long as we manually verify CLAs, we are OK.", "@googlebot I just wanted to make you happy :(."]}, {"number": 6112, "title": "R0.12", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 6111, "title": "Flawed memory management: allow_growth=True consumes more memory, causing out-of-memory", "body": "To prevent tensorflow (TF) from allocating the totality of graphic memory, I always use the following options when creating sessions:\r\n\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n```\r\n\r\nHowever, doing so causes some experiments to run out of memory while not doing so will not cause memory overflow. For example, when running experiments involving RNN, such as translate.py or ptb_word_lm.py in the sample code, if I specify allow_growth=True, I always encounter the following:\r\n\r\n```\r\nTraining Epoch 0 ; learning_rate= 0.002 :                                                                                                \r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2230 evicted_count=1000 eviction_rate=0.44843 and unsatisfied allocation rate=0.570506                                                                              \r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110                                     \r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 2631 get requests, put_count=2546 evicted_count=1000 eviction_rate=0.392773 and unsatisfied allocation rate=0.421133                                                                             \r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 256 to 281                                     \r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 9384 get requests, put_count=9389 evicted_count=1000 eviction_rate=0.106508 and unsatisfied allocation rate=0.112319                                                                             \r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 655 to 720                                     \r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.62G (1739461632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.46G (1565515520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.31G (1408964096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 1.18G (1268067840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY  \r\n```\r\nHowever, without specifying allow_growth=True, I can run it successfully. Moreover, the OOM occurs only after going through some epoches in the training data, not right from the beginning.\r\n\r\nIn principle, for an ideal memory manager, whether OOM will occur should not depends on whether memory is pre-allocated in one go or allocated step-by-step dynamically. Thus, Tensorflow's low-level memory management code must be flawed in one way or another.\r\n\r\n\r\nBelow are my system info:\r\n```\r\nOperating System:\r\nUbuntu 14.04.5 LTS\r\n\r\nInstalled version of CUDA and cuDNN:\r\n/usr/local/cuda-8.0\r\ncudnn-8.0-linux-x64-v5.1.tgz\r\n\r\nIt is installed from binary pip package\r\n\r\nA link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\n\r\nThe output from python -c \"import tensorflow; print(tensorflow.version)\"\r\nxuancong@wxc-i2r:~/projects/tf-rnnlm$ python -c \"import tensorflow; print(tensorflow.version)\"\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n0.11.0\r\n```\r\n", "comments": ["Could you post the entire log somewhere? We need to find out how much memory were requested when OOM happens? Also we would need a small model to reproduce the problem. \r\n\r\nallow_growth is expected to introduce some level of fragmentation. So if your model is running very close to GPU memory limit, it is possible that allow_growth can push you over the limit.\r\n\r\nThe current allocator design is a trade-off between performance and efficiency, and sure it is not perfect. That's why the option exists. It will get better as we keep improving it. If the system still has a lot of free memory, and the OOM still happens, then it is a more serious problem than if the model is already running close to the limit, where we would recommend not to use allow_growth. \r\n", "In principle, memory fragmentation should only affect the speed rather than the total available memory.\r\n\r\nYou can reproduce the problem by running **translate.py** or **ptb_word_lm.py** in the sample code, by setting a large enough vocabulary size for OOM to occur.", ">> In principle, memory fragmentation should only affect the speed rather\nthan the total available memory.\n\nI am not sure about that.\n\nhttps://en.wikipedia.org/wiki/Fragmentation_(computing)\n\n\"In computer storage, fragmentation is a phenomenon in which storage space\nis used inefficiently, reducing capacity or performance and often both.\"\n\n\n\nOn Fri, Dec 16, 2016 at 12:07 AM, xuancong84 <notifications@github.com>\nwrote:\n\n> In principle, memory fragmentation should only affect the speed rather\n> than the total available memory.\n>\n> You can reproduce the problem by running *translate.py* or\n> *ptb_word_lm.py* in the sample code, by setting a large enough vocabulary\n> size for OOM to occur.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6111#issuecomment-267539412>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APAgTvY8fOvc7dxm-mh7mW0VYhMYdcMYks5rIkbCgaJpZM4LE_5n>\n> .\n>\n", "Hi,\r\n\r\nI am testing the TF Slim package, fine-tuning the flowers dataset using the instructions in https://github.com/tensorflow/models/tree/master/slim\r\n\r\nTo allow for memory growth, I use the following code:\r\n\r\n`    session_config.gpu_options.allow_growth = True\r\n    session_config.gpu_options.allocator_type = 'BFC'\r\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.4\r\n    # gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\r\n    # Run the training:\r\n    final_loss = slim.learning.train(\r\n        train_op,\r\n        logdir=train_dir,\r\n        init_fn=get_init_fn(),\r\n        number_of_steps=2000,\r\n        session_config=session_config)`\r\n\r\nThere are mainly two issues.\r\n\r\n1. As the GPU memory is not big enough, I want to try to allow for GPU growth, so that the process does not crash. The code I am using is the one below, however, it doesn't make any difference at all. Is allow_growth and/or per_process_gpu_memory_fraction really supported for TF slim or not? I suppose this will be anyways an issue, given the original message on the thread. So, I guess all left to ask Is whether there any development on the issue (to begin with, is it really an issue)?\r\n\r\n2. Is all the GPU needed for the allocation of the intermediate activation variables? Because the inception model is about 100 Mb, my total GPU memory (after killing also the X server) is 1.8 Gb, so it feels somewhat weird that the model needs in the end 18X more GPU RAM than the inception model size. When using Caffe with Alexnet (~350 Mb), there was no problem at all, no matter the batch size. I guess the way the gradients are computed is by allocating memory like: #MODEL_PARAMETERS x BATCH_SIZE, since for smaller batch sizes the code works. Given that batch gradients are cumultative over the batch samples, isn't this -although arguably faster- somehow redundant (?).\r\n\r\nI will try to get the RAM metadata log file, if that is of any help.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 6110, "title": "generate signature for SparseTensor failed", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI can not find any posts about how to export a sparse model in tensorflow. I searched for hours.\r\n\r\n### Environment info\r\nOperating System:\r\ncentos7 without gpu\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nexport function:\r\n```\r\n# Read TFRecords files for training\r\nfilename_queue = tf.train.string_input_producer(\r\n    tf.train.match_filenames_once(FLAGS.train),\r\n    num_epochs=epoch_number)\r\nserialized_example = read_and_decode(filename_queue)\r\nbatch_serialized_example = tf.train.shuffle_batch(\r\n    [serialized_example],\r\n    batch_size=batch_size,\r\n    num_threads=thread_number,\r\n    capacity=capacity,\r\n    min_after_dequeue=min_after_dequeue)\r\nfeatures = tf.parse_example(\r\n    batch_serialized_example,\r\n    features={\r\n        \"label\": tf.FixedLenFeature([], tf.float32),\r\n        \"ids\": tf.VarLenFeature(tf.int64),\r\n        \"values\": tf.VarLenFeature(tf.float32),\r\n    })\r\nbatch_labels = features[\"label\"]\r\nbatch_ids = features[\"ids\"]\r\nbatch_values = features[\"values\"]\r\n\r\n        print(\"Start to run export model\")\r\n        start_time = datetime.datetime.now()\r\n\r\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\r\n        if ckpt and ckpt.model_checkpoint_path:\r\n            print(\"Use the model {}\".format(ckpt.model_checkpoint_path))\r\n            saver.restore(sess, ckpt.model_checkpoint_path)\r\n            export_version = FLAGS.export_version\r\n            export_path = \"./export/\"+job_name\r\n            print('Exporting trained model to %s' % export_path)\r\n            model_exporter = exporter.Exporter(saver)\r\n            model_exporter.init(\r\n                sess.graph.as_graph_def(),\r\n                named_graph_signatures = {\r\n                  'inputs': exporter.generic_signature({'ids': features['ids'], 'values':features['values']}),\r\n                  'outputs': exporter.generic_signature({'labels': features['labels']})},\r\n                init_op=init_op)\r\n            model_exporter.export(export_path, tf.constant(export_version), sess)\r\n            print('Done exporting!')\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 343, in <module>\r\n    'inputs': exporter.generic_signature({'ids': features['ids'], 'values':features['values']}),\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/session_bundle/exporter.py\", line 121, in generic_signature\r\n    signature.generic_signature.map[name].tensor_name = tensor.name\r\nAttributeError: 'SparseTensor' object has no attribute 'name'\r\n```", "comments": ["@ericyue Could you find a solution?", "Also meet this problem. Any suggestion? @ericyue ", "I'm getting the same error when calling `tf.python.saved_model.utils.build_tensor_info()` on a `sparse_placeholder` (TF v1.0). Anyone have a solution yet?", "can we use like this?\r\n`inputs = {'source': utils.build_tensor_info(pred_data[0]),\r\n              'target_sparse': (utils.build_tensor_info(pred_data[1].indices), utils.build_tensor_info(pred_data[1].values), utils.build_tensor_info(pred_data[1].dense_shape)),\r\n              'source_len': utils.build_tensor_info(pred_data[2])\r\n              }`", "Why this issue was closed? I am experiencing the same problem.", "I think the details of the original issue are out of date, but it still doesnt seem possible to use sparse tensors as part of a signature.  Is this intended behavior?\r\n\r\nFrom this doc:\r\n\r\nhttps://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/signature_defs.md\r\n\r\n\"Note that TensorInfo itself requires specification of name, dtype and tensor shape. While tensor information is already present in the graph, it is useful to explicitly have the TensorInfo defined as part of the SignatureDef since tools can then perform signature validation, etc. without having to read the graph definition.\"\r\n\r\n(linked from here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/saved_model)\r\n\r\nThis seems consistent with the code -- looking here (in  `def _validate_signature_def_map(self, signature_def_map)`):\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/builder_impl.py#L202\r\n\r\nThe definition of TensorInfo seems to show that sparse will not have a name (there is a name in each sub-field instead):\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/meta_graph.proto\r\n\r\nSo basically, when using a VarLenFeature, export fails:\r\n```\r\nAssertionError: All TensorInfo protos used in the SignatureDefs must have the name field set: dtype: DT_STRING\r\n```\r\n\r\nThis fix from a few days ago in `saved_model.utils.py` seems to imply that using a SparseTensor should be possible, but maybe I'm misinterpreting:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/eb27277809c038d93edcfb7221295608654589d7\r\n\r\n", "same here, and still looking forward to the solutions.", "I am having the same problem. Is there really no way of generating SignatureDefs from sparse tensors?", "I have the same issue, I am working with sparse_placeholder and I get this error:\r\nAssertionError: All TensorInfo protos used in the SignatureDefs must have the name field set: dtype: DT_FLOAT\r\ntensor_shape {\r\n  unknown_rank: true\r\n}\r\ncoo_sparse {\r\n  values_tensor_name: \"input/values:0\"\r\n  indices_tensor_name: \"input/indices:0\"\r\n  dense_shape_tensor_name: \"input/shape:0\"\r\n}", "@MatteoGlarey I \"solved\" the problem by building tensor infos from the 3 individual Tensors that make up a SparseTensor (*/indices, */values, */shape) and then save the model using these tensor infos. The downside is that when the model is being deployed using Tensorflow Serving, the value to be scored has to be supplied in that same format, meaning as incides/values/shape individually.", "Thank you for the fast reply, I will try it as soon as possible!\r\n@ksbg I think I solved the problem of the format using tf.split and moving the pre-processing to the server-side:\r\n```\r\n input_ph = tf.placeholder(tf.string, [None, 3], \"input_placeholder\")\r\n    user, prod, qty = tf.split(input_ph, 3, 1)\r\n    with tf.name_scope('transactions_weights'):\r\n        indices = indices_add_ph(user, prod, user_array, prod_array)\r\n        qty_f = tf.string_to_number(qty, tf.float64)\r\n        weights_p = tf.SparseTensor(indices=tf.cast(indices, tf.int64),\r\n                                    values=tf.cast(tf.reshape(qty_f, [-1]), tf.float64),\r\n                                    dense_shape=[len(user_array), len(prod_array)])\r\n```\r\nIt seems to work this way and I did not have to change the client: \r\n```\r\nrequest.inputs['transactions'].CopyFrom(\r\n            tf.contrib.util.make_tensor_proto([rating[1], rating[2], rating[0]], shape=[1, 3]))\r\n```\r\nSo I send the user and the product name as strings (the third field is only to have compatibility for next steps)."]}, {"number": 6109, "title": "Regenerate docs for 0.12 rc1.", "body": "", "comments": ["@tensorflow-jenkins test this please.", "Jenkins, test this please."]}, {"number": 6108, "title": "[TensorBoard] Charts in TensorBoard in Safari don't display correctly", "body": "macOS Sierra, latest Safari (Version 10.0.1). Works good in Chrome.\r\n\r\nSteps to repro:\r\n1. Run https://github.com/openai/universe-starter-agent\r\n\r\n2. Observe the following graphs:\r\n<img width=\"423\" alt=\"screen shot 2016-12-05 at 6 32 12 pm\" src=\"https://cloud.githubusercontent.com/assets/823890/20910888/30ec783a-bb19-11e6-8514-5c1027ca068a.png\">\r\n\r\n3. Console contains these errors:\r\n<img width=\"505\" alt=\"screen shot 2016-12-05 at 6 32 58 pm\" src=\"https://cloud.githubusercontent.com/assets/823890/20910904/4936486c-bb19-11e6-8032-417677a78d2e.png\">", "comments": ["Have you tried Firefox or Chrome in Mac? We first need to nail down whether it is the browser's problem or Tensorboard's web-script problem?\r\n\r\nMacOS is known to cause problems because Apple developers are typically very ignorant of the conventional standards in HTML/CSS/Javascript/PHP/TCPIP and etc.. Last time, I hosted some web service to do online text translation, it turns out that only for MacOS, if your URL query ends with a unicode character, then MacOS or Safari will append a NULL character, then that extraneous NULL character causes my program to function incorrectly. It is a shame that for such a big company, their engineers have actually made such a silly mistake! -:(", "@xuancong84 yeah, as per note above, works good in Chrome.", "@html5cat Then how about Firefox? Firefox is the best explorer in the current state-of-the-art.", "Don't have it, sorry. Safari has big enough audience - mobile and default on all Macs. \n\n> On Dec 5, 2016, at 8:17 PM, xuancong84 <notifications@github.com> wrote:\n> \n> @html5cat Then how about Firefox? Firefox is the best explorer in the current state-of-the-art.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "We're currently committed to keeping it working on Chrome and Firefox. Open source contributors are welcome to fix it for safari.", "I'm going to close this for now. If you have a fix, please feel free to submit it to our new repository at https://github.com/tensorflow/tensorboard/pulls."]}, {"number": 6107, "title": "[Windows/CMake] Add optimization flags", "body": "", "comments": ["@yifeif another cherrypick candidate before we kick off builds for the release."]}, {"number": 6106, "title": "Update version string for 0.12 rc1.", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 6105, "title": "Revert \"Removed unnecessary classproperty decorator\"", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->"]}, {"number": 6104, "title": "Generating NaN when computing gradient", "body": "### Environment info\r\n**Operating System:**\r\nRed Hat Enterprise Linux Server release 6.6\r\n**Tensorflow version:** \r\n0.10.0rc0\r\n\r\n**Installed version of CUDA and cuDNN:** \r\n/usr/local/cuda/lib64/libcudart.so.7.5.23\r\n\r\nI'm running a model with temporal attention strategy (https://arxiv.org/abs/1608.02927). I use `tf.nn.seq2seq.sequence_loss_by_example()` to compute the loss, and use adam gradient (lr:0.001) to minimize loss. The loss is not NaN, but gradients of all weigths became NaN values. If I use plain attention strategy, it won't have this NaN problem.\r\n\r\nI even print out all hyperparameters, their values land in a sensible range until their gradients become NaN.\r\n\r\nHope someone can help me fix this issue. Thanks in advance.\r\n", "comments": ["This question is better asked on (stackoverflow)[http://stackoverflow.com/questions/tagged/tensorflow] since it is a usage question not a bug report."]}, {"number": 6103, "title": "Branch 141100982", "body": "Pushing internal commits.", "comments": ["@tensorflow-jenkins Test this, please.", "Is it possible same problems (windows cmake failures) as #6051 are happening here?\r\nMaybe we can cherrypick fixes from that?"]}, {"number": 6102, "title": "add support for gcc byteoder defines to windows platform", "body": "needed to unblock https://github.com/tensorflow/tensorflow/pull/5450\r\n", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please.", "CPU failures known issues."]}, {"number": 6101, "title": "Feature request - Android example app for Windows tensorflow.", "body": "There is now a support of TensorFlow for Windows. I installed it using the instructions provided as part of Anaconda, and it works. However, I can't seem to find information about developing an application with TensorFlow for Android in Windows, by using Android Studio.\r\n\r\nI found the path of TensorFlow installation in Windows,  C:\\Program Files\\Miniconda2\\envs\\py35\\Lib\\site-packages\\tensorflow\\contrib\\android\\java, but it is completely empty.\r\n\r\nIs there a chance of getting an example app for Windows Android Studio?\r\n\r\nI've also asked it on stackeoverflow, but didn't get any answer yet. \r\nI am asking it here because it looks like the example has not been developed yet.\r\n(http://stackoverflow.com/questions/40978859/creating-simple-android-app-using-android-studio-and-tensorflow/40979627#40979627)", "comments": ["There is an example Android application in tensorflow/examples/android. I don't know much about Anacondo, but tensorflow/contrib/android should not be empty (this provides the native inference interface used by the example app). We don't use or require python for the Android side of things, so this may be a clue (you have py35 in your path).\r\n\r\nWe also provide tensorflow/examples/android/build.gradle file that wraps around the TF Bazel build, which may now work for Android Studio on Windows (I haven't tested it). More support for cmake/maven etc is planned for the near future, which should address any remaining issues involved in building for Windows.", "Hi Andrew and thanks for your reply.\r\n\r\nI have installed the official WIN package located at https://pypi.python.org/pypi/tensorflow-gpu/0.12.0rc0, following the instructions at https://developers.googleblog.com/2016/11/tensorflow-0-12-adds-support-for-windows.html.\r\n\r\nJust to make sure I did not miss anything, I downloaded the .whl file manually, renamed it to .zip, opened and listed all the files and directories inside this package. There is nothing related to android. I am attaching a text file with a list of the files inside the .whl. \r\nI've also checked the Linux package and it also does not contain the Android example.\r\n\r\nI guess that these .whl Python distributions do not contain anything other than Python support? If so, I guess that I need another source code/distribution. \r\nFollowing your advice, I will try the experimental TF Bazel build for Windows and update.\r\n\r\n[files.txt](https://github.com/tensorflow/tensorflow/files/633199/files.txt)\r\n", "@AndreyRub Yes, the whl file you reference seems to only be the TF Python API which wraps around a precompiled native library for Windows.\r\n\r\nThe process for Android is a bit different; please refer to  https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android which contains basic instructions for running the demo with Bazel. If it doesn't work on Windows you could probably build it in a VM, or wait for the cmake support that will be coming soon.", "Closing; please reopen if issues persist."]}, {"number": 6100, "title": "Error on compiling model using Keras", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI don't know where to begin looking for this problem. It seems like an internal bug but I am not entirely sure if it is something I did wrong. I am fairly new to tensorflow, please help!\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN:\r\nCUDA 8.0, cuDNN 5.1\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.0rc0-cp27-none-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.12.0-rc0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Activation, Dropout\r\nfrom keras.layers.convolutional import Convolution1D\r\nfrom keras.layers.pooling import MaxPooling1D\r\nfrom keras.layers.core import Flatten\r\nfrom keras.engine.topology import Merge\r\nfrom keras import backend as K\r\nfrom keras.optimizers import SGD\r\nfrom keras.objectives import *\r\nfrom keras.utils.layer_utils import layer_from_config\r\nfrom keras.regularizers import l1, l2\r\nfrom keras.callbacks import *\r\nfrom keras.metrics import *\r\n\r\nsgd = SGD(lr=10 ** (-lr), momentum=0.9, decay=0, nesterov=True)\r\n\r\n# model1: ConvNet\r\nmodel = Sequential()\r\nmodel.add(Convolution1D(128, 6, border_mode='same', input_shape=(256,2)))\r\nmodel.add(MaxPooling1D())\r\nmodel.add(Convolution1D(64, 6, border_mode='same'))\r\nmodel.add(MaxPooling1D())\r\nmodel.add(Convolution1D(32, 6, border_mode='same'))\r\nmodel.add(MaxPooling1D())\r\nmodel.add(Convolution1D(16, 6, border_mode='same'))\r\nmodel.add(MaxPooling1D())\r\nmodel.add(Flatten())\r\n\r\n# merge model:\r\nmodel.add(Dense(128, name='d1'))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dense(128, name='d2'))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dense(128, name='d3'))\r\nmodel.add(Activation('tanh'))\r\nmodel.add(Dense(3))\r\nmodel.add(Activation('softmax'))\r\n\r\n# My y contains three columns, which can be understood as '-1', '0' and '+1'. I want to maximize the correct labeling of '+1' and '-1', don't care how many 0's that I label, and minimize the number of mis-labeling of '+1' or '-1'. \r\ndef func_loss(y_true, y_pred):\r\n\treturn -K.mean(K.prod(K.cast(K.argmax(y_pred, axis=1), K.floatx()) - 1.0), (K.cast(K.argmax(y_true, axis=1), K.floatx()) - 1.0))\r\n\r\nmodel.compile(loss=func_loss, optimizer='sgd', metrics=[categorical_accuracy])\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nI tried numerous tweaking on the loss function but they all end up with some problems. \r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 547, in compile\r\n>     **kwargs)\r\n>   File \"/usr/local/lib/python2.7/dist-packages/keras/engine/training.py\", line 622, in compile\r\n>     sample_weight, mask)\r\n>   File \"/usr/local/lib/python2.7/dist-packages/keras/engine/training.py\", line 324, in weighted\r\n>     score_array = fn(y_true, y_pred)\r\n>   File \"<stdin>\", line 2, in func_loss\r\n>   File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 490, in mean\r\n>     axis = _normalize_axis(axis, ndim(x))\r\n>   File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 435, in _normalize_axis\r\n>     if axis is not None and axis < 0:\r\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 547, in __nonzero__\r\n>     raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\n> TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n> ", "comments": ["Currently, in your loss function you are passing `(K.cast(K.argmax(y_true, axis=1), K.floatx()) - 1.0)` as the `axis` argument to `K.mean`. At best, I'm sure Keras is assuming the `axis` to be an integer (i.e. not `floatx`), and from my reading of the source, I'm not sure if it supports an integer `Tensor` there at all. ", "I'm very sorry about the typo. The loss function should look like this: \r\n```\r\ndef func_loss(y_true, y_pred):\r\n\treturn -K.mean(K.prod(K.cast(K.argmax(y_pred, axis=1), K.floatx()) - 1.0, K.cast(K.argmax(y_true, axis=1), K.floatx()) - 1.0))\r\n```\r\nin which both `K.cast`s are parameters of `K.prod`.\r\nThe error message looks the same: \r\n\r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 547, in compile\r\n>     **kwargs)\r\n>   File \"/usr/local/lib/python2.7/dist-packages/keras/engine/training.py\", line 622, in compile\r\n>     sample_weight, mask)\r\n>   File \"/usr/local/lib/python2.7/dist-packages/keras/engine/training.py\", line 324, in weighted\r\n>     score_array = fn(y_true, y_pred)\r\n>   File \"<stdin>\", line 2, in func_loss\r\n>   File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 464, in prod\r\n>     axis = _normalize_axis(axis, ndim(x))\r\n>   File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 435, in _normalize_axis\r\n>     if axis is not None and axis < 0:\r\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 547, in __nonzero__\r\n>     raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\n> TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n> \r\n", "This type of question is better suited to (stackoverflow)[http://stackoverflow.com/questions/tagged/tensorflow] since it is a usage question not a bug report.", "Thank you @michaelisard, I wasn't sure because it looks like a bug. I will post it on stackoverflow. "]}, {"number": 6099, "title": "Cherrypicks into r0.12 release", "body": "This is wave 1, with some merge conflicts. @mrry PTAL", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->", "all cherry-picked commits have cla, can ignore. Tests all passed, merging. "]}, {"number": 6098, "title": "Automated rollback of change 137917462", "body": "Change: 141085507", "comments": []}, {"number": 6097, "title": "Using classifiers with scikit-learn ensemble methods", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?: http://stackoverflow.com/questions/35464652/how-to-create-ensemble-in-tensorflow\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04.1 LTS\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): Using CPU only version of TensorFlow\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`: 0.12.0-rc0\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nIt would be useful if the TensorFlow Learn classifiers had a `get_params` method so that they could be used with the `VotingClassifier` in `scikit-learn`. Note that XGBoost does currently work well with `VotingClassifier`\r\n\r\n```python\r\nfrom sklearn.svm import SVC\r\nfrom sklearn import tree\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn import neighbors\r\nfrom sklearn.ensemble import VotingClassifier\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import learn\r\n\r\nclf_svm = SVC(C=1000, gamma=1)\r\nclf_dt = tree.DecisionTreeClassifier(max_depth=30)\r\nclf_knn = neighbors.KNeighborsClassifier(7, weights=\"distance\")\r\nfeature_columns = learn.infer_real_valued_columns_from_input(X_train)\r\nclf_dnn = learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[200, 400, 200], n_classes=10)\r\nclf_rf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\r\n\r\neclf = VotingClassifier(estimators=[('SVM', clf_svm), \r\n                                    ('DecisionTree', clf_dt), \r\n                                    ('KNN', clf_knn), \r\n                                    ('DNN', clf_dnn), \r\n                                    ('RandomForest', clf_rf),], \r\n                        voting='hard')\r\n\r\neclf.fit(X_train, y_train)\r\npredicted_labels = eclf.predict(X_test)\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-117-461ecf12b1de> in <module>()\r\n     17                         voting='hard')\r\n     18 \r\n---> 19 eclf.fit(X_train, y_train)\r\n     20 predicted_labels = eclf.predict(X_test)\r\n     21 \r\n\r\n/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/voting_classifier.py in fit(self, X, y, sample_weight)\r\n    163                 delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\r\n    164                     sample_weight)\r\n--> 165                     for _, clf in self.estimators)\r\n    166 \r\n    167         return self\r\n\r\n/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self, iterable)\r\n    756             # was dispatched. In particular this covers the edge\r\n    757             # case of Parallel used with an exhausted iterator.\r\n--> 758             while self.dispatch_one_batch(iterator):\r\n    759                 self._iterating = True\r\n    760             else:\r\n\r\n/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self, iterator)\r\n    601 \r\n    602         with self._lock:\r\n--> 603             tasks = BatchedCalls(itertools.islice(iterator, batch_size))\r\n    604             if len(tasks) == 0:\r\n    605                 # No more tasks available in the iterator: tell caller to stop.\r\n\r\n/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py in __init__(self, iterator_slice)\r\n    125 \r\n    126     def __init__(self, iterator_slice):\r\n--> 127         self.items = list(iterator_slice)\r\n    128         self._size = len(self.items)\r\n    129 \r\n\r\n/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/voting_classifier.py in <genexpr>(.0)\r\n    163                 delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\r\n    164                     sample_weight)\r\n--> 165                     for _, clf in self.estimators)\r\n    166 \r\n    167         return self\r\n\r\n/usr/local/lib/python3.5/dist-packages/sklearn/base.py in clone(estimator, safe)\r\n     63                             \"it does not seem to be a scikit-learn estimator \"\r\n     64                             \"as it does not implement a 'get_params' methods.\"\r\n---> 65                             % (repr(estimator), type(estimator)))\r\n     66     klass = estimator.__class__\r\n     67     new_object_params = estimator.get_params(deep=False)\r\n\r\nTypeError: Cannot clone object '<tensorflow.contrib.learn.python.learn.estimators.dnn.DNNClassifier object at 0x7f1d773f80f0>' (type <class 'tensorflow.contrib.learn.python.learn.estimators.dnn.DNNClassifier'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.\r\n```\r\n", "comments": ["We used to have get_params, but we never supported (and will not support) set_params, so we took it out to avoid confusion.\r\n\r\nThis is something that SKCompat can (and probably should) support. A PR would be welcome.\r\n", "@Anjum48 Hi! Is this still a feature you're interested in?", "Hi @itsmeolivia I think I managed to work around it at the time, so at this time I don't think it is super important to get this feature added. I guess we can close this for now and reopen if other people are interested", "Hi, I have been looking for something like this for a couple days now. I want to be able to use sklearn's ClassifierChain() with Tensorflow's DNNClassifier or SVM in order to make them multilabel. ", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing as per @Anjum48 comment. All, feel free to contribute a PR as per @martinwicke comment."]}, {"number": 6096, "title": "error download files during run ./configure", "body": "when i run ./configure, there are 2 files can't be downloaded.\r\n\r\nhttp://cdimage.debian.org/mirror/xbmc.org/build-deps/sources/swig-3.0.8.tar.gz\r\n\r\nhttp://cdimage.debian.org/mirror/xbmc.org/build-deps/sources/giflib-5.1.4.tar.gz\r\n\r\n\r\n============\r\n\r\n\r\nERROR: /home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/contrib/tfprof/python/tools/tfprof/BUILD:49:1: every rule of type _py_wrap_cc implicitly depends upon the target '@swig//:templates', but this target could not be found because of: no such package '@swig//': Error downloading from **http://cdimage.debian.org/mirror/xbmc.org/build-deps/sources/swig-3.0.8.tar.gz** to /home/scopeserver/.cache/bazel/_bazel_root/4bbf7fc8d0645aa96f0d41866ce2885c/external/swig: Error downloading http://cdimage.debian.org/mirror/xbmc.org/build-deps/sources/swig-3.0.8.tar.gz to /home/scopeserver/.cache/bazel/_bazel_root/4bbf7fc8d0645aa96f0d41866ce2885c/external/swig/swig-3.0.8.tar.gz: Timed out connecting to http://cdimage.debian.org/mirror/xbmc.org/build-deps/sources/swig-3.0.8.tar.gz : connect timed out.\r\n\r\nERROR: /home/scopeserver/RaidDisk/DeepLearning/mwang/tensorflow/tensorflow/core/platform/default/build_config/BUILD:96:1: no such package '@gif_archive//': Error downloading from **http://cdimage.debian.org/mirror/xbmc.org/build-deps/sources/giflib-5.1.4.tar.gz** to /home/scopeserver/.cache/bazel/_bazel_root/4bbf7fc8d0645aa96f0d41866ce2885c/external/gif_archive: Error downloading http://cdimage.debian.org/mirror/xbmc.org/build-deps/sources/giflib-5.1.4.tar.gz to /home/scopeserver/.cache/bazel/_bazel_root/4bbf7fc8d0645aa96f0d41866ce2885c/external/gif_archive/giflib-5.1.4.tar.gz: Timed out connecting to http://cdimage.debian.org/mirror/xbmc.org/build-deps/sources/giflib-5.1.4.tar.gz : connect timed out and referenced by '//tensorflow/core/platform/default/build_config:gif'.\r\n", "comments": ["This seems to be causing widespread trouble for our tests as well. It looks like #6098 should fix this.", "cdimage.debian.org was down, and as @mrry suggested it is resolved with #6098 \r\nSorry for the inconvenience."]}, {"number": 6095, "title": "Add atan2 trigonometry function", "body": "Tensorflow has a number of built-in trigonometry functions like `atan` but it doesn't have `atan2` yet.\r\n\r\nThis can be achieved with some custom code as pointed in this comment: https://github.com/tensorflow/tensorflow/issues/3624#issuecomment-242085609.\r\n\r\nThere's actually a small mistake on the first line, where `np.pi` should not be added. The corrected code is presented below:\r\n\r\n```python\r\ndef atan2(y, x):\r\n    angle = tf.select(tf.greater(x,0.0), tf.atan(y/x), tf.zeros_like(x))\r\n    angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)\r\n    angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)\r\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)\r\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)\r\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), np.nan * tf.zeros_like(x), angle)\r\n    return angle\r\n```\r\n\r\nWhile this custom code performs reasonably well, it'd be both faster and more convenient if Tensorflow had it built-in like the other existing functions.", "comments": ["Thanks, please feel free to submit a PR and reference this issue.", "Is there any way to convert the range to between 0 and 2\\*pi? I'm trying to mimic opencv's cartToPolar method this way, however, I'm having trouble finding a way to modify elements in a tensor (on a placeholder, so I can't use scatter update) based off of a condition i.e. where angle < 0, add 2*pi.", "For what it's worth, I managed to implement two versions of atan2. One that follows the opencv style (returns degrees), and another that uses tensorflow's atan function (returns rads).\r\n\r\n```python\r\ndef atan2(y, x):\r\n    angle = tf.select(tf.greater(x, 0.0), tf.atan(y / x), tf.zeros_like(x))\r\n    angle = tf.select(tf.greater(y, 0.0), 0.5 * np.pi - tf.atan(x / y), angle)\r\n    angle = tf.select(tf.less(y, 0.0), -0.5 * np.pi - tf.atan(x / y), angle)\r\n    angle = tf.select(tf.less(x, 0.0), tf.atan(y / x) + np.pi, angle)\r\n    angle = tf.select(tf.logical_and(tf.equal(x, 0.0), tf.equal(y, 0.0)),\r\n                      np.nan * tf.zeros_like(x), angle)\r\n\r\n    indices = tf.where(tf.less(angle, 0.0))\r\n    updated_values = tf.gather_nd(angle, indices) + (2 * np.pi)\r\n    update = tf.SparseTensor(indices, updated_values, angle.get_shape())\r\n    update_dense = tf.sparse_tensor_to_dense(update)\r\n\r\n    return angle + update_dense\r\n\r\ndef atan2_ocv(y, x):\r\n    # constants\r\n    DBL_EPSILON = 2.2204460492503131e-16\r\n    atan2_p1 = 0.9997878412794807 * (180 / np.pi)\r\n    atan2_p3 = -0.3258083974640975 * (180 / np.pi)\r\n    atan2_p5 = 0.1555786518463281 * (180 / np.pi)\r\n    atan2_p7 = -0.04432655554792128 * (180 / np.pi)\r\n\r\n    ax, ay = tf.abs(x), tf.abs(y)\r\n    c = tf.select(tf.greater_equal(ax, ay), tf.div(ay, ax + DBL_EPSILON),\r\n                  tf.div(ax, ay + DBL_EPSILON))\r\n    c2 = tf.square(c)\r\n    angle = (((atan2_p7 * c2 + atan2_p5) * c2 + atan2_p3) * c2 + atan2_p1) * c\r\n    angle = tf.select(tf.greater_equal(ax, ay), angle, 90.0 - angle)\r\n    angle = tf.select(tf.less(x, 0.0), 180.0 - angle, angle)\r\n    angle = tf.select(tf.less(y, 0.0), 360.0 - angle, angle)\r\n    return angle\r\n```\r\n\r\nI'm not sure if I should add or subtract np.pi when x < 0.0 for my atan2 function, since it seems to be a platform dependent thing, so I left it at adding np.pi. I validated the output of my atan2_ocv by using it for visualizing optical flow, and comparing that with opencv's flow visualization implementation which uses cv2.cartToPolar (which in turn uses its own implementation of atan2).\r\n\r\nopencv's flow visualization function:\r\nhttps://github.com/opencv/opencv/blob/master/samples/python/opt_flow.py#L37\r\n\r\nopencv's cartToPolar: https://github.com/opencv/opencv/blob/master/modules/core/src/mathfuncs.cpp#L266\r\n\r\nopencv's atan2:\r\nhttps://github.com/opencv/opencv/blob/cb1d4e692bd13ff413273baf6d3b22fabdde6e74/modules/core/src/mathfuncs_core.cpp#L159", "Oh, and if anyone's coming here to look for an atan2 function for their own flow visualization function, here's what I made (I know this isn't the point of this issue!):\r\n\r\n```python\r\ndef normalize(tensor, a=0, b=1):\r\n    return tf.div(tf.mul(tf.sub(tensor, tf.reduce_min(tensor)), b - a),\r\n                  tf.sub(tf.reduce_max(tensor), tf.reduce_min(tensor)))\r\n\r\n\r\ndef cart_to_polar_ocv(x, y, angle_in_degrees=False):\r\n    v = tf.sqrt(tf.add(tf.square(x), tf.square(y)))\r\n    ang = atan2_ocv(y, x)\r\n    scale = 1 if angle_in_degrees else np.pi / 180\r\n    return v, tf.mul(ang, scale)\r\n\r\n\r\ndef cart_to_polar(x, y, angle_in_degrees=False):\r\n    v = tf.sqrt(tf.add(tf.square(x), tf.square(y)))\r\n    ang = atan2(y, x)\r\n    scale = 180 / np.pi if angle_in_degrees else 1\r\n    return v, tf.mul(ang, scale)\r\n\r\n\r\ndef draw_hsv(flow):\r\n    fx, fy = flow[:, :, :, 0], flow[:, :, :, 1]\r\n    v, ang = cart_to_polar_ocv(fx, fy)\r\n\r\n    h = normalize(tf.mul(ang, 180 / np.pi))\r\n    s = tf.ones_like(h)\r\n    v = normalize(v)\r\n\r\n    hsv = tf.pack([h, s, v], 3)\r\n    rgb = tf.image.hsv_to_rgb(hsv) * 255\r\n\r\n    return tf.cast(rgb, tf.uint8)\r\n```", "As far as I can tell, the implementation mentioned in the first post will suffer from division-by-zero if x or y contain any zeros, which leads to NaN gradients. This fixes the issue for me:\r\n\r\n```python\r\ndef atan2(y, x, epsilon=1.0e-12):\r\n    # Add a small number to all zeros, to avoid division by zero:\r\n    x = tf.select(tf.equal(x, 0.0), x+epsilon, x)\r\n    y = tf.select(tf.equal(y, 0.0), y+epsilon, y)\r\n\r\n    angle = tf.select(tf.greater(x,0.0), tf.atan(y/x), tf.zeros_like(x))\r\n    angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)\r\n    angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)\r\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)\r\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)\r\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), tf.zeros_like(x), angle)\r\n    return angle\r\n``` ", "Any updates on adding builtin atan2 support?", "I think that select is deprecated in version 1.0, so this is now:\r\n\r\n```\r\ndef atan2(x, y, epsilon=1.0e-12):\r\n        \"\"\"\r\n        A hack until the tf developers implement a function that can find the angle from an x and y co-\r\n        ordinate.\r\n        :param x: \r\n        :param epsilon: \r\n        :return: \r\n        \"\"\"\r\n        # Add a small number to all zeros, to avoid division by zero:\r\n        x = tf.where(tf.equal(x, 0.0), x+epsilon, x)\r\n        y = tf.where(tf.equal(y, 0.0), y+epsilon, y)\r\n    \r\n        angle = tf.where(tf.greater(x,0.0), tf.atan(y/x), tf.zeros_like(x))\r\n        angle = tf.where(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)\r\n        angle = tf.where(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)\r\n        angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)\r\n        angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)\r\n        angle = tf.where(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), tf.zeros_like(x), angle)\r\n        return angle\r\n```", "Fixed by https://github.com/tensorflow/tensorflow/pull/9276."]}, {"number": 6094, "title": "build fail: ERROR: no such target '@local_config_cuda//crosstool:toolchain'", "body": "I just sync the latest code and meet build error. in fact i already assign to use GPU support during run **./configure**. but it can't find.\r\n\r\nI am using Ubuntu 16.04, cuda 8.0 and bazel 0.32\r\n\r\ngit rev-parse HEAD\r\n\r\ndca48e8b5adbb328c09a43b7d19300b52680d7ac\r\n\r\n\r\n======\r\n\r\nERROR: /home/scopeserver/.cache/bazel/_bazel_scopeserver/b562266fb6ddaaff7ca2ee0b31316144/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\r\n\tFile \"/home/scopeserver/.cache/bazel/_bazel_scopeserver/b562266fb6ddaaff7ca2ee0b31316144/external/local_config_cuda/crosstool/BUILD\", line 4\r\n\t\terror_gpu_disabled()\r\n\tFile \"/home/scopeserver/.cache/bazel/_bazel_scopeserver/b562266fb6ddaaff7ca2ee0b31316144/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\r\n\t\tfail(\"ERROR: Building with --config=c...\")\r\nERROR: **Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.**\r\nERROR: **no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/scopeserver/.cache/bazel/_bazel_scopeserver/b562266fb6ddaaff7ca2ee0b31316144/external/local_config_cuda/crosstool/BUILD.**\r\nINFO: Elapsed time: 0.352s\r\n\r\n\r\n", "comments": ["i just run  **bazel version** and it returns:\r\n**Build label: 0.3.2**\r\n_Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\r\nBuild timestamp: 1475861110\r\nBuild timestamp as int: 1475861110_\r\n\r\n\r\nHowever, when i run **sudo apt-get upgrade bazel**, it returns: \r\n\r\n**bazel is already the newest version (0.4.1).**\r\n\r\nwhy these 2 versions are different, one is 0.3.2 and another is 0.4.1 ?\r\n", "Could you run `which bazel`\r\nIt looks like you have two different bazel installations. \r\n\r\nYour bazel log says\r\n```\r\nBuilding with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\r\n```\r\nCould you retry running `./configure` and see if it helps.", "I just download bazel 0.41 install file and install it from local, now the bazel is 0.41. \r\n\r\nwhen i run ./configure, i already assign use GPU to to Y. \r\nhowever, when i run build again. it still report the same error.\r\n\r\n===========\r\n\r\nscopeserver@scopephotos:~/RaidDisk/DeepLearning/mwang/tensorflow$ sudo ./configure \r\n[sudo] password for scopeserver: \r\n~/RaidDisk/DeepLearning/mwang/tensorflow ~/RaidDisk/DeepLearning/mwang/tensorflow\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\r\nNo OpenCL support will be enabled for TensorFlow\r\n**Do you wish to build TensorFlow with CUDA support? [y/N] y**\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n**Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0**\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: \r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n........\r\nINFO: All external dependencies fetched successfully.\r\nConfiguration finished\r\nscopeserver@scopephotos:~/RaidDisk/DeepLearning/mwang/tensorflow$ **bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package**\r\nERROR: /home/scopeserver/.cache/bazel/_bazel_scopeserver/4bbf7fc8d0645aa96f0d41866ce2885c/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\r\n\tFile \"/home/scopeserver/.cache/bazel/_bazel_scopeserver/4bbf7fc8d0645aa96f0d41866ce2885c/external/local_config_cuda/crosstool/BUILD\", line 4\r\n\t\terror_gpu_disabled()\r\n\tFile \"/home/scopeserver/.cache/bazel/_bazel_scopeserver/4bbf7fc8d0645aa96f0d41866ce2885c/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\r\n\t\tfail(\"ERROR: Building with --config=c...\")\r\nERROR: **Building with --config=cuda but TensorFlow is not configured to build with GPU support.** Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\r\nERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/scopeserver/.cache/bazel/_bazel_scopeserver/4bbf7fc8d0645aa96f0d41866ce2885c/external/local_config_cuda/crosstool/BUILD.\r\nINFO: Elapsed time: 0.129s\r\n", "Wait, why are you running `sudo ./configure` ?\r\nYou are running `./configure` and `bazel build ...` as different users.\r\n\r\nPlease check out the tensorflow repository to a folder your user can write to, and then run ./configure without sudo.", "I reproduced your setup with ubuntu, bazel 0.3.2 and bazel 0.4.1, and cuda 8.0\r\nTried to run the commands (without `sudo`) and for me everything ran successfully.\r\n\r\nPlease try running `./configure` without sudo. In your current local repository it will fail, because there are now some files owned by the root, so I would suggest removing your folder with the code in it and checking the code out again.\r\n\r\nAs I tried with a few settings and could not reproduce the problem, I will now close the issue. If you still run into this issue please reopen.", "oh, yeah, i change the permission for the python folder and build pass.", "I have the same issue during build on MAC\r\n\r\n`ERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /private/var/tmp/_bazel_admin/f2f2e0ebb105daeebb988f12401380fc/external/local_config_cuda/crosstool/BUILD.`\r\n\r\n`./configure` was run with these options \r\n```\r\nadmins-MacBook-Pro:tensorflow admin$ ./configure\r\nPlease specify the location of python. [Default is /Users/admin/miniconda3/bin/python]: \r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /Users/admin/miniconda3/lib/python3.5/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Users/admin/miniconda3/lib/python3.5/site-packages]\r\n\r\nUsing python library path: /Users/admin/miniconda3/lib/python3.5/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 8.0\r\nPlease specify the location where cuDNN 8.0 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nInvalid path to cuDNN  toolkit. Neither of the following two files can be found:\r\n/usr/local/cuda/lib/libcudnn.8.0.dylib\r\n/usr/local/cuda/libcudnn.8.0.dylib\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 3.0\t\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n.........\r\nINFO: All external dependencies fetched successfully.\r\n```\r\n\r\nI noticed your guide for mac+GPU expects different output from configure script. \r\nthese parts are missing \r\n```\r\nSetting up Cuda include\r\nSetting up Cuda lib\r\nSetting up Cuda bin\r\nSetting up Cuda nvvm\r\nSetting up CUPTI include\r\nSetting up CUPTI lib64\r\n```\r\n\r\nThe repo's head is currently on \r\n```\r\ncommit 12254370ad9248170ca8acc58061f475ef8d6d62\r\nAuthor: Kristina Chodorow <kchodorow@google.com>\r\nDate:   Tue Jan 31 15:06:00 2017 -0500\r\n```\r\n\r\nPlease let me know if I should open a new issue.\r\n"]}, {"number": 6093, "title": "R0.12", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 6092, "title": "[Windows/CMake] Support compute capability 3.0 by default", "body": "Fixes #6083.", "comments": ["Running an additional Windows/GPU/CMake presubmit here: http://ci.tensorflow.org/job/tf-pr-win-cmake-gpu/4/console\r\n\r\n@gunan Since this is causing issues (#6001, #6083), can we cherry-pick this one into r0.12 if it passes?", "Thank you so much \ud83d\ude04 ", "I have no clue how to build using CMake where can I download the .whl file that has this fix ?", "This change will be built into tonights nightly wheel file build.\r\nOr this week we will have another release candidate ready which will have this fix."]}, {"number": 6091, "title": "Use model after trained in tensorflow (save/load graph)", "body": "**What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?**\r\nhttp://stackoverflow.com/questions/33759623/tensorflow-how-to-restore-a-previously-saved-model-python\r\n\r\n **Environment info\r\nOperating System:** Ubuntu 14.04\r\n\r\n**Installed version of tensorflow**: '0.11.0'\r\n\r\nI already asked my question on stackoverflow: http://stackoverflow.com/questions/40956040/how-to-use-model-after-trained-in-tensorflow-save-load-graph\r\nBut I can't find any helps so I have to ask here.\r\n\r\nI want to save a graph after training or save something else which tensorflow can load it. \r\nI found there are two ways : \r\n\r\n- using MetaGraph : https://www.tensorflow.org/versions/r0.11/how_tos/meta_graph/index.html\r\n- save Graph like the example **image_retraining** : https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/image_retraining\r\n\r\n**## I/ Using Exporting and Importing a MetaGraph**\r\n\r\nMy [Save.py](https://github.com/shaolinkhoa/tensorflow/blob/master/05_convolutional_net.py) file:\r\n```\r\nX = tf.placeholder(\"float\", [None, 28, 28, 1], name='X')\r\nY = tf.placeholder(\"float\", [None, 10], name='Y')\r\n\r\ntf.train.Saver()\r\nwith tf.Session() as sess:\r\n     ...run something ...\r\n     final_tensor = tf.nn.softmax(py_x, name='final_result')\r\n     tf.add_to_collection(\"final_tensor\", final_tensor)\r\n\r\n     predict_op = tf.argmax(py_x, 1)\r\n     tf.add_to_collection(\"predict_op\", predict_op)\r\n\r\nsaver.save(sess, 'my_project') \r\n```\r\n\r\nThen I run [load.py](https://github.com/shaolinkhoa/tensorflow/blob/master/load_05_convolution.py):\r\n```\r\nwith tf.Session() as sess:\r\n   new_saver = tf.train.import_meta_graph('my_project.meta')\r\n   new_saver.restore(sess, 'my_project')\r\n   predict_op = tf.get_collection(\"predict_op\")[0]\r\n   for i in range(2):\r\n        test_indices = np.arange(len(teX)) # Get A Test Batch\r\n        np.random.shuffle(test_indices)\r\n        test_indices = test_indices[0:test_size]\r\n\r\n        print(i, np.mean(np.argmax(teY[test_indices], axis=1) ==\r\n                         sess.run(predict_op, feed_dict={\"X:0\": teX[test_indices],\r\n                                                         \"p_keep_conv:0\": 1.0,\r\n                                                         \"p_keep_hidden:0\": 1.0})))\r\n\r\n\r\n```\r\n\r\nbut it return error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"load_05_convolution.py\", line 62, in <module>\r\n    \"p_keep_hidden:0\": 1.0})))\r\n  File \"/home/khoa/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 717, in run\r\n    run_metadata_ptr)\r\n  File \"/home/khoa/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 894, in _run\r\n    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\r\nValueError: Cannot feed value of shape (256, 784) for Tensor u'X:0', which has shape '(?, 28, 28, 1)'\r\n\r\n```\r\nI really don't know why?\r\n\r\n\r\nIf I add `final_tensor = tf.get_collection(\"final_result\")[0]`\r\n\r\nIt return another error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"load_05_convolution.py\", line 46, in <module>\r\n    final_tensor = tf.get_collection(\"final_result\")[0]\r\nIndexError: list index out of range\r\n```\r\n\r\nIs it because tf.add_to_collection only contains only one tensor ?\r\n\r\n## **II/ using tf.train.write_graph**\r\n\r\nI add this line to the end of the save.py :  `tf.train.write_graph(graph, 'folder', 'train.pb')`\r\n\r\nIt created file 'train.pb' successfully.\r\n\r\nMy [load.py](https://github.com/shaolinkhoa/tensorflow/blob/master/load_05_convolution.py):\r\n```\r\nwith tf.gfile.FastGFile('folder/train.pb', 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    _ = tf.import_graph_def(graph_def, name='')\r\n\r\nwith tf.Session() as sess:\r\n  predict_op = sess.graph.get_tensor_by_name('predict_op:0')\r\n  for i in range(2):\r\n        test_indices = np.arange(len(teX)) # Get A Test Batch\r\n        np.random.shuffle(test_indices)\r\n        test_indices = test_indices[0:test_size]\r\n\r\n        print(i, np.mean(np.argmax(teY[test_indices], axis=1) ==\r\n                         sess.run(predict_op, feed_dict={\"X:0\": teX[test_indices],\r\n                                                         \"p_keep_conv:0\": 1.0,\r\n                                                         \"p_keep_hidden:0\": 1.0})))\r\n\r\n```\r\nThen it return error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"load_05_convolution.py\", line 22, in <module>\r\n    graph_def.ParseFromString(f.read())\r\n  File \"/home/khoa/tensorflow/lib/python2.7/site-packages/google/protobuf/message.py\", line 185, in ParseFromString\r\n    self.MergeFromString(serialized)\r\n  File \"/home/khoa/tensorflow/lib/python2.7/site-packages/google/protobuf/internal/python_message.py\", line 1085, in MergeFromString\r\n    raise message_mod.DecodeError('Unexpected end-group tag.')\r\ngoogle.protobuf.message.DecodeError: Unexpected end-group tag.\r\n```\r\n\r\nwould you mind sharing the standard way, code or tutorial to save/load model ? I'm really confused.", "comments": ["There are some examples here that use Saver:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/how_tos/reading_data\r\n\r\nI'm sorry that you did not get a response on stackoverflow, but that really is the best place for this question. Please consider posting there again.", "this may help..\r\nhttp://stackoverflow.com/questions/38939081/saving-model-in-tensorflow", "Plese check out Tensorflow Estimator Local prediction sample at https://github.com/AshutoshDongare/Tensorflow-Wide-Deep-Local-Prediction"]}, {"number": 6090, "title": "Fix Python 3 dict object to list, described in issue #5488", "body": "It is also compatible with Python 2, converting a list to a list is a list.", "comments": ["Can one of the admins verify this patch?", "@nathansilberman can you take a look at this change?\r\nIs it expected that eval_ops are always a list/tuple, or can it be a dict()?", "@nathansilberman do you have cycles to take a look?", "@zuoxingdong in the meanwhile, please resolve conflicts and push again.", "@drpngx There were 2 conflicts with `saver=saver`, now they are resolved.", "Jenkins, test this please.", "Jenkins, test this please.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "It looks like there are still failures -- can you fix the failures and validate that the relevant tests are passing?"]}, {"number": 6089, "title": "Add missing parenthesis", "body": "", "comments": []}, {"number": 6088, "title": "Strided slice op CHECK failure", "body": "Two reports of a `CHECK` failure in the strided slice op have surfaced on Stack Overflow:\r\n\r\n* [Tensorflow print value of a tensor](http://stackoverflow.com/q/40979001/3574081)\r\n* [Tensorflow evaluate: Aborted (core dumped)](http://stackoverflow.com/q/40963017/3574081)\r\n\r\nIn both cases, the error message is:\r\n\r\n```\r\nF tensorflow/core/kernels/strided_slice_op.cc:316] Check failed: tmp.CopyFrom(input.Slice(begin[0], end[0]), final_shape) \r\nAborted (core dumped)\r\n```\r\n\r\n...which appears to match the `CHECK` [here](https://github.com/tensorflow/tensorflow/blob/d9c173fc7adf9443d58310dd08a84b9eb89d4c8f/tensorflow/core/kernels/strided_slice_op.cc#L90).\r\n\r\nIt looks like both questions might be using the same model code, so I'll ask the questioner to post additional details if possible.", "comments": ["I believe this bug has already been fixed (at least a month and a half ago). Did they specify what verison of TensorFlow they are using and if they tried it with the latest RC?", "Hi, I have encountered this issue on TF 0.10 release on my macbook pro (all of a sudden, I wonder what has changed, it has been fine all along.). It's MacOS Sierra 10.12.1. \r\n\r\nand TF release 0.10 is encountering this issue, and changing it to 0.11 has already fixed the issue. ", "Sounds like this has been fixed. Please reopen if this is still an issue in the latest release."]}, {"number": 6087, "title": "control_dependencies throw Frame error when used in while_loop", "body": "Hello,\r\n\r\n### Problem description\r\nI need to put InceptionV3 in a while loop to save both GPU and CPU memory usage since I am handling videos, each contains hundreds of images. The problem is, InceptionV3 uses `control_dependencies` for BatchNorm and TensorFlow throws Frame Error if   `control_dependencies` function is in the `while_loop`. It can run without errors if `control_dependencies` is removed though. \r\n\r\nBelow is a minimal snippet that reproduces the error:\r\n\r\n```python\r\nsess = tf.Session()\r\n\r\nwith tf.variable_scope('state'):\r\n    x = tf.get_variable('x', shape=(), \r\n                             initializer=tf.constant_initializer(1), \r\n                             dtype=tf.float32)\r\n    update_x = tf.assign(x, x+1)\r\n\r\ndef iter_fun(i, y):\r\n    # comment the line below, the program will run without any error\r\n    # but I need control_dependencies, or at least some way to replace it...\r\n    with tf.control_dependencies([update_x]): \r\n        y = y + x\r\n    return (i+1, y)\r\n\r\nwith tf.variable_scope('iteration'):\r\n    num_iterations = 5   \r\n    initial_i = tf.constant(0, dtype=tf.int32)\r\n    initial_y = tf.constant(0, dtype=tf.float32)\r\n    _, result = tf.while_loop(\r\n        cond=lambda i, *_: i < num_iterations,\r\n        body=iter_fun,\r\n        loop_vars=(initial_i, initial_y))\r\n\r\ninit_op = tf.global_variables_initializer()\r\nsess.run(init_op)\r\nsess.run(result)  \r\n``` \r\n\r\nThe stack trace of the error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"demo.py\", line 28, in <module>\r\n    sess.run(result)\r\n  File \"/workspace/bily/anoaconda2/envs/tensorflow0.12/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/workspace/bily/anoaconda2/envs/tensorflow0.12/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/workspace/bily/anoaconda2/envs/tensorflow0.12/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/workspace/bily/anoaconda2/envs/tensorflow0.12/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'iteration/while/add' has inputs from different frames. The input 'iteration/while/add/Enter' is in frame 'iteration/while/iteration/while/'. The input 'state/Assign' is in frame ''.\r\n```\r\n### Environments\r\n- CentOS Linux release 7.2.1511\r\n- TensorFlow 0.12 built from source\r\n- Python 2.7.12\r\n- CUDA 7.5 and CUDNN v5.1\r\n\r\n### Related issues\r\n1. [This issue in tflearn](https://github.com/tflearn/tflearn/issues/427) seems to be related to my problem but removing `control_denpendencies` isn't a solution for me. \r\n2. #4478 and #3114 are issues about frame errors but these errors are caused by variables instead of `control_dependencies`.\r\n\r\nAny help will be appreciated : )", "comments": ["Thanks for the report. I have a fix that should show up here in a few days.", "@yuanbyu Looking forward to it :smiley: ", "@yuanbyu  how is it going now ? ", "I run into a similar error too, involving a while loop\r\n`InvalidArgumentError: The node 'map_15/while/map/while/Variable/Assign' has inputs from different frames. The input 'map_15/while/map/while/Const' is in frame 'map_15/while/map/while/map_15/while/map/while/'. The input 'map_15/while/map/while/Variable' is in frame ''.`\r\n\r\nWhat is the state of this issue?\r\n", "I would also like to know the state of this issue. Getting `tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'RNN/DizzyRNNCellOptHacky/Linear/Matrix/Assign' has inputs from different frames. The input 'RNN/while/DizzyRNNCellOptHacky/Linear/random_normal' is in frame 'RNN/while/RNN/while/'. The input 'RNN/DizzyRNNCellOptHacky/Linear/Matrix' is in frame ''.`", "@yuanbyu , it seems slim.batch_norm still cannot be used inside while_loop. Here is the snippet to reproduce the error: `tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'control_dependency' has inputs from different frames. The input 'update_barrier' is in frame 'iteration/while/iteration/while/'. The input 'Mean' is in frame ''.`, I am using commit\u00a0`2255bc2c1ada8a300adfc8a9a1e68d59c5df4296`.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import control_flow_ops\r\n\r\nslim = tf.contrib.slim\r\n\r\nsess = tf.Session()\r\n\r\nwith tf.variable_scope('state'):\r\n    x = tf.get_variable('x', shape=(),\r\n                             initializer=tf.constant_initializer(1),\r\n                             dtype=tf.float32)\r\n    update_x = tf.assign(x, x+1)\r\n\r\ndef iter_fun(i, y):\r\n    # comment the line below, the program will run without any error\r\n    # but I need control_dependencies, or at least some way to replace it...\r\n    # with tf.control_dependencies([update_x]):\r\n        # y = y + x\r\n    y = slim.batch_norm(y)\r\n    return (i+1, y)\r\n\r\nwith tf.variable_scope('iteration'):\r\n    num_iterations = 5\r\n    initial_i = tf.constant(0, dtype=tf.int32)\r\n    initial_y = tf.constant((1, 2, 3, 4), dtype=tf.float32)\r\n    _, result = tf.while_loop(\r\n        cond=lambda i, *_: i < num_iterations,\r\n        body=iter_fun,\r\n        loop_vars=(initial_i, initial_y))\r\n\r\ntotal_loss = tf.reduce_mean(result)\r\nbatchnorm_updates = set(tf.get_collection(ops.GraphKeys.UPDATE_OPS))\r\nif batchnorm_updates:\r\n    with tf.control_dependencies(batchnorm_updates):\r\n        barrier = tf.no_op(name='update_barrier')\r\n    total_loss = control_flow_ops.with_dependencies([barrier], total_loss)\r\n\r\nopt = tf.train.AdamOptimizer()\r\ngradients = opt.compute_gradients(total_loss)\r\napply_gradient_op = opt.apply_gradients(gradients)\r\n\r\ninit_op = tf.global_variables_initializer()\r\nsess.run(init_op)\r\nprint(sess.run(apply_gradient_op))\r\nprint(sess.run(total_loss))\r\n```", "@yuanbyu Is the fix done?", "@gongbudaizhe, I think I have long submitted a fix to your problem. Let me know if you still see a problem. ", "@ffmpbgrnn, the problem with your program is that slim.batch_norm() is created in the loop body so the update_ops of batch_norm can't be used outside of the loop. (We don't allow a control edge from an op inside the loop body to an op outside the loop.)\r\n\r\nThose update ops are completed by the time that `result` is completed. So all the control dependency logic for them is probably not needed.", "@yuanbyu  Sorry, I was working on something else after coming across this bug. I just checked the snippet reproducing the bug and now it works smoothly in TensorFlow 1.0.\r\n\r\nThank you! ", "@yuanbyu @ffmpbgrnn I'm having a similar problem and am getting `tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'update_barrier' has inputs from different frames. The input 'while/BatchNorm/AssignMovingAvg_1' is in frame 'while/while/'. The input 'fc1/BatchNorm/AssignMovingAvg_1' is in frame '`'. Is there a way to use batch-norm inside `tf.while_loop`?", "Setting updates_collections=None in BatchNorm seems to resolve the issue since this ensures that updates are done within a frame.", "@NaveenAri facing a similar problem while using a nested map_fn with batch normalization written separately :\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'gradients/map/while/map/while/Conv2D_grad/Conv2DBackpropFilter/Enter' has inputs from different frames. The input 'gradients/map/while/map/while/Conv2D_grad/Conv2DBackpropFilter/StackPop' is in frame 'gradients/map/while/map/while/'. The input 'BatchNorm_1/AssignMovingAvg_1' is in frame ''.`\r\n\r\nDoes your solution ensure that moving_mean and moving_variance also get updated accordingly, similar to normally using `tf.control_dependencies(update_ops)`?", "@epiception I met the same error. I just put the snippet ` with tf.control_dependencies()... ` within `while_loop` code section and the error disappeared.\r\nI not quite understand the concept of `frame` and why\r\n>the problem with your program is that slim.batch_norm() is created in the loop body so the update_ops of batch_norm can't be used outside of the loop. (We don't allow a control edge from an op inside the loop body to an op outside the loop.)\r\n\r\n@yuanbyu Could you explain it more? thanks."]}, {"number": 6086, "title": "fix broken link in tools_developers docs", "body": "Fixes #5917", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}]