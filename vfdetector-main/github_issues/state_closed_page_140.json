[{"number": 50709, "title": "Failed precondition: Cannot compute input sources for dataset of type PaddedBatchDatasetV2", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0 rc0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2 8.1\r\n- GPU model and memory:\r\n\r\nWhen I use bucket_by_sequence_length function, program appear the following error message: \r\n\r\n2021-07-10 03:46:46.503910: E tensorflow/core/framework/dataset.cc:552] Failed precondition: Cannot compute input sources for dataset of type PaddedBatchDatasetV2, because sources could not be computed for input dataset of type Window\r\n\r\n", "comments": ["Also reported in https://github.com/tensorflow/tensorflow/issues/50693", "@LinJM \r\nCan you please try with [tested configurations](https://www.tensorflow.org/install/source#gpu), and let us know if the issue persist.", "@Saduf2019 I directly use the binary release of Tensorflow 2.6.0 rc0.", "@LinJM \r\nThe issue for 2.6 is already reported, please try to use the tested build configurations to avoid this issue which will be resolved in issue mentioned above [50693]", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50709\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50709\">No</a>\n"]}, {"number": 50708, "title": "r2.6 cherry-pick request: Upgrade oneDNN version to v2.3", "body": "TensorFlow currently uses oneDNN v2.3-rc2. v2.3 release has [dozens more fixes](https://github.com/oneapi-src/oneDNN/compare/v2.3-rc2...v2.3) since rc2.\r\n\r\nOriginal PR (https://github.com/tensorflow/tensorflow/pull/50657) was merged on Thursday 7/8 in the morning (got into Thursday's nightly).", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50708) for more info**.\n\n<!-- need_author_consent -->", "Manually setting CLA to yes because the content is from PR https://github.com/tensorflow/tensorflow/pull/50657 that is already merged into master."]}, {"number": 50707, "title": "Fix the shape inference function for FusedBatchNormGradEx", "body": "Related to https://github.com/tensorflow/tensorflow/pull/49277\r\n\r\nThis PR changes FusedBatchNormGradEx to use a different shape inference function to set output 5.\r\n\r\n\r\ncc. @nluehr ", "comments": ["@kaixih this test seems to be failing:\r\n\r\n//tensorflow/core/kernels:fused_batch_norm_ex_op_test_gpu                FAILED in 3 out of 3 in 11.3s\r\n\r\n```\r\nC++ exception with description \"vector::_M_range_check: __n (which is 5) >= this->size() (which is 5)\" thrown in the test body.\r\n```\r\n\r\nmaybe try to run with asan?\r\n", "Thanks for the reminder. The root cause is that there will be only 5 outputs when num_side_inputs == 0 for the FusedBatchNormGradEx. So, we need to skip setting the 5th output when no side input tensor is detected. It is fixed with the new commit. PTAL. @cheshire "]}, {"number": 50706, "title": "Saved model loading : KeyError: '__inference_depthwise_conv2d_layer_call_fn_126'", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.5\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory: 2080 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nPlease download the scripts to reproduce from : https://drive.google.com/drive/folders/15cajAZ9sAZ2Uyix8sDVSYku6QCqDCec7?usp=sharing\r\n\r\nCommand to run : `python sample.py`.\r\n\r\nI have a simple model with input layer and a depthwise conv2d layer. I quantize this model by adding quantize_and_dequantize nodes at the input of depthwiseconv2d layer (commented in the code). When I save the model and load it back, I see the following \r\n\r\n```\r\n  File \"/home/dperi/Downloads/py3/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 544, in <lambda>\r\n    \"function\": lambda: self._recreate_function(proto.function),\r\n  File \"/home/dperi/Downloads/py3/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 586, in _recreate_function\r\n    proto, self._concrete_functions), setattr\r\n  File \"/home/dperi/Downloads/py3/lib/python3.6/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 295, in recreate_function\r\n    concrete_function_objects.append(concrete_functions[concrete_function_name])\r\nKeyError: '__inference_depthwise_conv2d_layer_call_and_return_conditional_losses_117'\r\n```\r\n\r\nIf I change the depthwise conv2d layer to a regular conv2d layer, the saving and loading quantized model works fine. This is weird and I'm not sure why that's happening. \r\nCan anyone help me resolve the issue ? \r\n\r\nRelated issues  that I checked : There are similar issues filed but the comments were not helpful for me. \r\nhttps://github.com/tensorflow/tensorflow/issues/42004\r\nhttps://github.com/tensorflow/tensorflow/issues/45945\r\n\r\n**Describe the expected behavior**\r\nSaved model loading works fine. \r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes):\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nPlease download the scripts to reproduce from : https://drive.google.com/drive/folders/15cajAZ9sAZ2Uyix8sDVSYku6QCqDCec7?usp=sharing\r\n\r\n", "comments": ["@peri044 \r\n\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50706\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50706\">No</a>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50706\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50706\">No</a>\n"]}, {"number": 50704, "title": "Retrieving the Unreduced Losses", "body": "If we set the reduction in our binary cross entropy loss to tf.losses.Reduction.NONE when we compile our model (code below) is there a way to retrieve the unreduced losses when we evaluate on a sample?\r\n\r\n```\r\nmodel.compile(loss= tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=False),\r\n              optimizer=Adam(learning_rate=1e-4))\r\n```\r\n\r\nI'm running into an issue where the loss in my model (with zeroed-out weights and bias - code below on how i'm doing this) is not what I'm expecting.\r\n\r\n```\r\nfor ix, layer in enumerate(model.layers):\r\n      weights = layer.get_weights()\r\n      for arr in weights:\r\n        arr[(arr > 0) | (arr < 0)] = 0\r\n      layer.set_weights(weights)\r\n```\r\n\r\nSo I'd like to diagnose how my model is calculating the loss by looking at output from the 3 stages of reduction: NONE, SUM, and SUM_OVER_BATCH_SIZE\r\n", "comments": ["@chethanjjj  It looks like your Issue relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. As ([previously announced](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999/)), all future development of Keras is expected to happen in the [github.com/keras-team/keras](github.com/keras-team/keras) repository. If your issue lies with the TF-Core area please comment back with your explanation and we can look into it further. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50703, "title": "Update version numbers for TensorFlow 2.6.0-rc1", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 6 -> 6\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.6.0-rc0\" found in source directory \n\"tensorflow/\". Good.\nWARNING: Below are potentially instances of lingering old version string \n\"2.6.0rc0\" in source directory \"tensorflow/\" that are not updated by this \nscript. Please check them manually!\ntensorflow/tools/pip_package/setup.py:105:2.6.0rc0\n```", "comments": []}, {"number": 50702, "title": "AttributeError: 'ReLU' object has no attribute '_saved_model_inputs_spec'", "body": "Got following error `AttributeError: 'ReLU' object has no attribute '_saved_model_inputs_spec'` when trying to export a model (`tf.keras.models.save_model`) with an `input_signature`.\r\n\r\n-> I could solve it by switching `tensorflow.keras.layers.ReLU()(x)` to `tf.nn.relu(x)`\r\n\r\n<br>\r\n\r\nTested in tf=2.4.0\r\nStack trace:\r\n```\r\n/Scribosermo/extras/exporting/model.py:164 call  *\r\n    x = self.model(x)\r\n/Scribosermo/training/scode/nets/quartznet.py:221 call  *\r\n    x = self.model(x, training=False)\r\n/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:786 __call__  **\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/functional.py:424 call\r\n    return self._run_internal_graph(\r\n/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/functional.py:560 _run_internal_graph\r\n    outputs = node.layer(*args, **kwargs)\r\n/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py:1018 __call__\r\n    if self._saved_model_inputs_spec is None:\r\n\r\nAttributeError: 'ReLU' object has no attribute '_saved_model_inputs_spec'\r\n```", "comments": ["@DanBmh \r\n\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Abug&template=00-bug-issue.md) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced]. Thanks\r\n", "I did use Nvidia's tensorflow container: nvcr.io/nvidia/tensorflow:21.03-tf2-py3\r\nPython version is 3.8.5\r\n\r\nI've been trying to export a model with weights loaded from .index and .data-xxx files (exported in tf=2.4). As far as I remember it did not happen with a model loaded directly from saved_model.pb + variables/ (exported in tf=2.3)\r\n\r\nThe model+exporting code can be found at: https://gitlab.com/Jaco-Assistant/Scribosermo/-/tree/6c5b988e991a878cc3f25c1abd78c03b253dc52e/extras/exporting", "@DanBmh Can you please share a simple standalone code to reproduce the issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50702\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50702\">No</a>\n"]}, {"number": 50701, "title": "Check if layer has _metrics_lock attribute", "body": "Solving error `AttributeError: 'InputLayer' object has no attribute '_metrics_lock'` when trying to export a keras model.\r\n\r\nThis occurred when exporting a model with weights loaded from `.index` and `.data-xxx` files (exported in tf=2.4), but not with a model loaded directly from `saved_model.pb` + `variables/` (exported in tf=2.3)\r\n\r\nFull stack trace:\r\n\r\n```\r\n  File \"/Scribosermo/extras/exporting/export.py\", line 90, in <module>\r\n    main()\r\n  File \"/Scribosermo/extras/exporting/export.py\", line 71, in main\r\n    tf.keras.models.save_model(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/save.py\", line 133, in save_model\r\n    saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/save.py\", line 80, in save\r\n    save_lib.save(model, filepath, signatures, options)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\", line 975, in save\r\n    _, exported_graph, object_saver, asset_info = _build_meta_graph(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\", line 1046, in _build_meta_graph\r\n    signatures = signature_serialization.find_function_to_export(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/signature_serialization.py\", line 75, in find_function_to_export\r\n    functions = saveable_view.list_functions(saveable_view.root)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\", line 144, in list_functions\r\n    obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\", line 2589, in _list_functions_for_serialization\r\n    functions = super(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py\", line 2380, in _list_functions_for_serialization\r\n    return (self._trackable_saved_model_saver\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py\", line 87, in list_functions_for_serialization\r\n    fns = self.functions_to_serialize(serialization_cache)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 78, in functions_to_serialize\r\n    return (self._get_serialized_attributes(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 94, in _get_serialized_attributes\r\n    object_dict, function_dict = self._get_serialized_attributes_internal(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py\", line 56, in _get_serialized_attributes_internal\r\n    super(ModelSavedModelSaver, self)._get_serialized_attributes_internal(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 103, in _get_serialized_attributes_internal\r\n    objects = save_impl.wrap_layer_objects(self.obj, serialization_cache)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 125, in wrap_layer_objects\r\n    metrics=data_structures.ListWrapper(layer.metrics),\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training_v1.py\", line 502, in metrics\r\n    metrics.extend(_get_metrics_from_layers(self._layers))\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training_v1.py\", line 3198, in _get_metrics_from_layers\r\n    metrics.extend(_get_metrics_from_layers(layer.layers))\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training_v1.py\", line 3198, in _get_metrics_from_layers\r\n    metrics.extend(_get_metrics_from_layers(layer.layers))\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training_v1.py\", line 3200, in _get_metrics_from_layers\r\n    metrics.extend(layer.metrics)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1588, in metrics\r\n    with layer._metrics_lock:\r\nAttributeError: 'InputLayer' object has no attribute '_metrics_lock'\r\n```\r\n\r\nModel code can be found at: https://gitlab.com/Jaco-Assistant/Scribosermo/-/tree/master/extras/exporting\r\n\r\nTested with tf=2.3.3 and tf=2.4.0", "comments": ["It looks like your PR relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. Thank you,"]}, {"number": 50700, "title": "[CherryPick 2.6]Disable GPU kernel for SparseSegmentMeanGrad", "body": "It fails with \"Internal:  Failed to launch gpuprim::DeviceRadixSort::SortPairs, temp_storage_bytes: 3327status: invalid configuration argument\"\n\nDisable the kernel while we're triaging the issue.\n\nPiperOrigin-RevId: 383492772\nChange-Id: I6cf99f6d5cc8b39b081c48893a6db601ead6817c", "comments": []}, {"number": 50699, "title": "cudaMallocAsync: If OOM, sync the stream and retry allocation.", "body": "The sync allow the driver more option to find memory. So sometimes it can find memory available after a sync.\r\n\r\n@sanjoy ", "comments": ["> Can this theoretically introduce a deadlock? E.g. imagine that the stream has a NCCL all-reduce operation on it, and the matching all-reduce (on the other device) was going to be launched after this allocation returned. Synchronizing on the stream would deadlock in this case.\r\n\r\nI checked with the NCCL team and it would be safe for some version only.\r\n\r\n> WDYT about synchronizing with a timeout?\r\n\r\nI'll do that to have a version that is save for all NCCL version and maybe other potential deadlock.", "Maybe just a clarification. It can deadlock only because in corner case, we can have only 1 threads **EDIT for all devices** that does the dispatch. If it was sure that we always have 1 dispatch thread per device, there wouldn't be any risk of deadlock as all the nccl call would be done.\r\n\r\nI specify this as if TFRT could make this guarantee, it would bypass the current problem.", "Sadly just doing a way doesn't trigger what we want in the driver. So I'll close this PR.\r\nThanks for the review."]}, {"number": 50698, "title": "cudaMallocAsync: Check return value", "body": "Add verification of the return value.\r\n@sanjoy ", "comments": []}, {"number": 50697, "title": "TfLite static MT build ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.5\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): Bazel 2.7.2\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nHi, \r\nI'm trying to use TfLite static library in a visualProjet build in MT.\r\nMy issue is that TfLite is build in MD and even if I tried to force the MT build ( by adding \"set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /MT\")\" in TfLite cmake and every dependances ), I still have error like :\r\n\r\nError LNK2001\tUnresolved external symbol  \"void __cdecl ruy::KernelFloatAvx(struct ruy::KernelParamsFloat<8,8> const &)\" (?KernelFloatAvx@ruy@@YAXAEBU?$KernelParamsFloat@$07$07@1@@Z)peerconnection_client\t.......\\tensorflow-lite.lib(fully_connected.obj)\r\n\r\nDoes someone has already build TfLite as a static MT lib ? \r\n\r\nThanks", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50697\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50697\">No</a>\n"]}, {"number": 50696, "title": "Snapshot not saved in tf.data.experimental.save", "body": "Unable to load tf dataset at a later stage as the snapshot is not saved while running  - \r\n```\r\n# tf.data.experimental.save(\r\n#     ds_train, path = train_ds_path, compression=None, shard_func=None\r\n# )\r\n```\r\n\r\nWhat is the solution for this?\r\n\r\nI have to save a tf dataset and reload it later.\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50696\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50696\">No</a>\n"]}, {"number": 50694, "title": "Error when applying tf.map_fn on symbolic tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): tensorflow-gpu 2.3.0 (from conda install - he13fc11_0)\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: cudatoolkit 10.1.243 (from conda install), cudnn 7.6.5 (from conda install)\r\n- GPU model and memory: NVIDIA GeForce GT 730M (1GB)\r\n\r\n**Current behavior**\r\nError when applying tf.map_fn on symbolic tensors\r\n\r\n**Describe the expected behavior**\r\nNo error when applying tf.map_fn on symbolic tensors\r\n\r\n**Standalone code to reproduce the issue**\r\nThis is a toy example, of course I would like to perform a specific per-row function:\r\n\r\n```\r\nimport tensorflow as tf\r\ninput = tf.keras.layers.Input(shape=[1,1,3], \r\n                              batch_size=4,\r\n                              dtype=tf.double)\r\nresult = tf.map_fn(lambda x: x,\r\n                   input)\r\n```\r\n\r\nI would like the function to be applied on each of the 4 elems of the batch dimension.\r\n\r\n**Traceback:**\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-119-118c58833a26>\", line 4, in <module>\r\n    result = tf.map_fn(lambda x: x,\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py3tf2_3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 574, in new_func\r\n    return func(*args, **kwargs)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py3tf2_3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py3tf2_3\\lib\\site-packages\\tensorflow\\python\\ops\\map_fn.py\", line 633, in map_fn_v2\r\n    return map_fn(\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py3tf2_3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py3tf2_3\\lib\\site-packages\\tensorflow\\python\\ops\\map_fn.py\", line 440, in map_fn\r\n    elems_batchable_ta = [\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py3tf2_3\\lib\\site-packages\\tensorflow\\python\\ops\\map_fn.py\", line 441, in <listcomp>\r\n    tensor_array_ops.TensorArray(\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py3tf2_3\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py\", line 1071, in __init__\r\n    self._implementation = implementation(\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\py3tf2_3\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py\", line 718, in __init__\r\n    self._tensor_array = [None for _ in range(size)]\r\n\r\nTypeError: 'Tensor' object cannot be interpreted as an integer\r\n```\r\n", "comments": ["Sorry, I could resolve in this way:\r\n\r\n```\r\nimport tensorflow as tf\r\ninput = tf.keras.layers.Input(shape=[1,1,3], \r\n                              batch_size=4,\r\n                              dtype=tf.double)\r\nfcn = lambda x: x\r\nresult = tf.keras.layers.Lambda(fcn)(input)\r\n```\r\n\r\nI closed the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50694\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50694\">No</a>\n"]}, {"number": 50693, "title": "Error log when using Dataset.group_by_window with Dataset.padded_batch", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.6.0rc0\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\n\r\nIn TensorFlow 2.6.0rc0, an error log is emitted when `tf.data.Dataset.padded_batch` is used in the `reduce_func` of `tf.data.Dataset.group_by_window`.\r\n\r\nThis code did not log any error in previous TensorFlow versions.\r\n\r\n**Describe the expected behavior**\r\n\r\nNo errors should be logged for this operation.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? no\r\n- Briefly describe your candidate solution(if contributing): N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nHere's a dummy example to reproduce the log:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nbatch_size = 5\r\n\r\ndef key_func(element):\r\n    return tf.size(element, out_type=tf.int64)\r\n\r\ndef reduce_func(key, dataset):\r\n    return dataset.padded_batch(batch_size, padded_shapes=tf.TensorShape([None]))\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices([\"a\", \"bb\", \"ccc\"])\r\ndataset = dataset.map(tf.strings.bytes_split)\r\ndataset = dataset.group_by_window(key_func, reduce_func, window_size=batch_size)\r\nnext(iter(dataset))\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe code above logs the following line:\r\n\r\n> `2021-07-09 12:04:33.530961: E tensorflow/core/framework/dataset.cc:552] Failed precondition: Cannot compute input sources for dataset of type PaddedBatchDatasetV2, because sources could not be computed for input dataset of type Window`", "comments": ["@saikumarchalla This is a C++ error log which is not visible by default on Colab. Please see this [gist](https://colab.research.google.com/drive/1YTo60nZzCeXl2VCb1tOW55yGNg-o0ZQP?usp=sharing) to reproduce on Colab.", "Was able to reproduce the issue on colab. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/458b9a6b2d7f5ad51912a156999c8152/window_padded_dataset.ipynb). ", "@guillaumekln \r\nCan you please try with the [tested configurations](https://www.tensorflow.org/install/source#gpu) and let us know if the issue persist.", "I did not compile TensorFlow from the source, so I'm not sure how this link is relevant to this issue?\r\n\r\nThe issue I'm reporting is in the binary release of TensorFlow 2.6.0rc0.", "@guillaumekln \r\nIrrespective of the source, this link has the tested configurations, so that one does not face such issues, can you please try and let us know.", "I'm testing the release candidate of TensorFlow 2.6 and reporting this issue early so that developers can fix the bug before the final release. I think that is the goal of a release candidate.\r\n\r\nI know this issue did not exist before, but that is not the point. This issue should be fixed in future releases.", "Thanks for reporting this @guillaumekln! The issue was fixed in master but the fix wasn't cherry-picked to the release branch. It should be fixed by https://github.com/tensorflow/tensorflow/pull/50794", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50693\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50693\">No</a>\n"]}, {"number": 50692, "title": "Build tensorflow-lite for armv7-a (embedded linux) failed\uff1f", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n> \r\n\r\n**System information**\r\n- OS Platform and Distribution :Linux Ubuntu 16.04 LTS\r\n-target platform - embedded linux - OS Openwrt, CPU - IMX6ULL;\r\nTensorFlow installed from source:\r\nTensorFlow version: 2.4.2\r\nCross compilation using C++ and CMake. CMake version - 3.16.0\r\n\r\n\r\n\r\n**Describe the problem**\r\nTrying to cross-compile Tensorflow-lite minimal c++ example using CMake.\r\n\r\n`cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc   -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++   -DCMAKE_C_FLAGS=\"${ARMCC_FLAGS}\"   -DCMAKE_CXX_FLAGS=\"${ARMCC_FLAGS}\"   -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON   -DCMAKE_SYSTEM_NAME=Linux    -DCMAKE_SYSTEM_PROCESSOR=armv7-a   ../tensorflow-2.4.2/tensorflow/lite/\r\n`\r\n\r\nThe results are as follows\r\n\r\n> -- Available targets (use: make TARGET):\r\n-- ---------+--------------------------------------------------------------\r\n-- Target   |   Description\r\n-- ---------+--------------------------------------------------------------\r\n-- install  | Install Eigen. Headers will be installed to:\r\n--          |     <CMAKE_INSTALL_PREFIX>/<INCLUDE_INSTALL_DIR>\r\n--          |   Using the following values:\r\n--          |     CMAKE_INSTALL_PREFIX: /usr/local\r\n--          |     INCLUDE_INSTALL_DIR:  include/eigen3\r\n--          |   Change the install location of Eigen headers using:\r\n--          |     cmake . -DCMAKE_INSTALL_PREFIX=yourprefix\r\n--          |   Or:\r\n--          |     cmake . -DINCLUDE_INSTALL_DIR=yourdir\r\n-- doc      | Generate the API documentation, requires Doxygen & LaTeX\r\n-- blas     | Build BLAS library (not the same thing as Eigen)\r\n-- uninstall| Remove files installed by the install target\r\n-- ---------+--------------------------------------------------------------\r\n-- \r\n-- Performing Test FARMHASH_HAS_BUILTIN_EXPECT\r\n-- Performing Test FARMHASH_HAS_BUILTIN_EXPECT - Success\r\n**And then it doesn't go down\uff0cWhat should I do?**", "comments": ["The build was not executed. What did I do wrong\uff1f", "Are you following the steps mentioned in https://www.tensorflow.org/lite/guide/build_cmake?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50692\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50692\">No</a>\n"]}, {"number": 50690, "title": "Fix some 'Access Denied' errors for RenameFile on Windows", "body": "Calling `MoveFileExW` with the `MOVEFILE_REPLACE_EXISTING` flag can fail if another process has a handle to the file that it didn't close yet, but `DeleteFileW` + `MoveFileW` has better chances to succeed. This is because `DeleteFileW` allows the process to keep using the old handle, and since the file is gone it forces `MoveFileW` to create a new handle. This should fix issues encountered by tensorflow users on Windows when they try to run certain models:\r\n\r\nhttps://stackoverflow.com/questions/62258549/failed-to-rename-access-denied-tensorflow\r\nhttps://github.com/tensorflow/tensorflow/issues/41380", "comments": []}, {"number": 50688, "title": "Cropped Image feature in TFlite Image Segmentation Example", "body": "Hello Tf Team!,... I'm using your image segmentation Android example in which deeplab model is being used. We get result and masked image in response. Could you please tell me how we can get the cropped image as we get the masked image! Please do let me know ASAP!\r\nThanks & Regards.", "comments": ["@brittle123 \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow.](https://stackoverflow.com/questions/tagged/tensorflow)/ TF [Forum](https://discuss.tensorflow.org/) .There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks\r\n", "Thankyou for the quick response but i didn't get any help from my search on stack overflow. Please can you give me a link where to find it?", "@brittle123 \r\n\r\nCould you please post the issue in [discuss.Tensorflow](https://discuss.tensorflow.org/).Thanks"]}, {"number": 50686, "title": "InvalidArgumentError: No OpKernel was registered to support Op 'QuantizedMatMulWithBiasAndDequantize'", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Google Colab\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.7\r\n- (run on CPU)\r\n\r\n\r\nI used a quantization package [LPOT](https://github.com/intel/lpot) supported with [Intel-Tensorflow](https://github.com/Intel-tensorflow/) to quantize my model, and ran inference under native Tensorflow environment.\r\n\r\n**Describe the current behavior**\r\nThe Quantized kernel does not seem to have registered OpKernel.\r\n\r\n**Describe the expected behavior**\r\nThe session should run fine and print the MSE loss, which is the case for my trained & un-quantized TF 2.4 frozen pb model (converted from keras model).\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): No\r\n\r\n\r\n**Other info / logs** \r\n\r\nSource code:\r\n`with tf.compat.v1.Session(graph=graph) as sess:`\r\n `       output = graph.get_tensor_by_name(output_tensor_name)`\r\n`        predictions = sess.run(output, {input_tensor_name: x})`\r\n `       mse = tf.reduce_mean(tf.keras.losses.mean_squared_error(y, predictions))`\r\n  `      print(mse.eval())`\r\n\r\n**Traceback:**\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1374     try:\r\n-> 1375       return fn(*args)\r\n   1376     except errors.OpError as e:\r\n\r\n8 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1357       # Ensure any changes to the graph are reflected in the runtime.\r\n-> 1358       self._extend_graph()\r\n   1359       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py in _extend_graph(self)\r\n   1397     with self._graph._session_run_lock():  # pylint: disable=protected-access\r\n-> 1398       tf_session.ExtendSession(self._session)\r\n   1399 \r\n\r\nInvalidArgumentError: No OpKernel was registered to support Op 'QuantizedMatMulWithBiasAndDequantize' used by {{node model/dense/Tensordot/MatMul_eightbit_requantize}} with these attrs: [input_quant_mode=\"MIN_FIRST\", Toutput=DT_FLOAT, T1=DT_QUINT8, T2=DT_QINT8, Tbias=DT_QINT32, transpose_a=false, transpose_b=false]\r\nRegistered devices: [CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[model/dense/Tensordot/MatMul_eightbit_requantize]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-5-ade3637a3169> in <module>()\r\n----> 1 loss = inference(path_to_pb='./model.pb',                  \r\ninput_tensor_name='x:0', output_tensor_name='Identity:0',                  \r\nx=x[:64], y=y[:64])\r\n\r\n<ipython-input-3-c2f290fad90b> in inference(path_to_pb, input_tensor_name, output_tensor_name, x, y)\r\n     20       with tf.compat.v1.Session(graph=graph) as sess:\r\n     21         output = graph.get_tensor_by_name(output_tensor_name)\r\n---> 22         predictions = sess.run(output, {input_tensor_name: x})\r\n     23         mse = tf.reduce_mean(tf.keras.losses.mean_squared_error(y, predictions))\r\n     24         print(mse.eval())\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    966     try:\r\n    967       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 968                          run_metadata_ptr)\r\n    969       if run_metadata:\r\n    970         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1189     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1190       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1191                              feed_dict_tensor, options, run_metadata)\r\n   1192     else:\r\n   1193       results = []\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1367     if handle is None:\r\n   1368       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1369                            run_metadata)\r\n   1370     else:\r\n   1371       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1392                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1393                     'disable_meta_optimizer = True')\r\n-> 1394       raise type(e)(node_def, op, message)\r\n   1395 \r\n   1396   def _extend_graph(self):\r\n\r\nInvalidArgumentError: No OpKernel was registered to support Op 'QuantizedMatMulWithBiasAndDequantize' used by node model/dense/Tensordot/MatMul_eightbit_requantize (defined at <ipython-input-2-28418ad07e47>:24)  with these attrs: [input_quant_mode=\"MIN_FIRST\", Toutput=DT_FLOAT, T1=DT_QUINT8, T2=DT_QINT8, Tbias=DT_QINT32, transpose_a=false, transpose_b=false]\r\nRegistered devices: [CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n", "comments": ["By the way, my goal is to load this model on ML.NET framework for inference. So any help/suggestions on how this issue could be resolved on ML.NET is also greatly appreciated!", "Perhaps you want to post this issue on https://github.com/intel/lpot or https://github.com/Intel-tensorflow/tensorflow repositories for better assistance.\r\nOn a side note: TensorFlow has its own [TFMOT](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot) library for quantization related tooling.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50685, "title": "resnet50 inference result is wrong with tensorflow+oneDNN", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 x86_64\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: Python 3.7.9\r\n- Bazel version (if compiling from source): Build label: 3.1.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: run on CPU\r\n- GPU model and memory: run on CPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nwe do not use these JIT primitives: convolution,jit:sse41, convolution,jit_dw:sse41, and bnorm_jit:msa by  comment out the configurations. We rebuild tensorflow 2.4 with oneDNN and run resnet50, but  the inference result is incorrect: \r\nPredicted: [[('n02672831', 'accordion', 0.045002583), ('n03976657', 'pole', 0.03239749), ('n02966193', 'carousel', 0.029394835), ('n03063599', 'coffee_mug', 0.028039759), ('n02909870', 'bucket', 0.023439417)]]\r\npredict_result:  accordion\r\n\r\n**Describe the expected behavior**\r\nthe expected result is:\r\nPredicted: [[('n02124075', 'Egyptian_cat', 0.9613135), ('n02127052', 'lynx', 0.021235), ('n02123597', 'Siamese_cat', 0.010190687), ('n02123159', 'tiger_cat', 0.0036373248), ('n02123045', 'tabby', 0.0015628876)]]\r\npredict_result:  Egyptian_cat\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n1\u3001we found that result is correct when running tensorflow in single thread configuration;\r\n2\u3001result is also correct when open  convolution,jit:sse41\r\n", "comments": ["@gititgo \r\n\r\n\r\nThis issue is more suitable for TensorFlow Models repo. Please post it on Tensorflow Models [repo](https://github.com/tensorflow/models/issues) from here. Thanks!\r\n\r\n", "We do not use The TensorFlow Model Garden. \r\nWe just config oneDNN(comment out convolution,jit:sse41) and rebuild tensorflow. Then we found the inference result of resnet50 model is incorrect. \r\nWe speculate that it is the problem of Conv2D implementation.", "@gititgo \r\n\r\nCould you please provide the colab gist with all the dependencies to analyse the issue better.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50685\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50685\">No</a>\n"]}, {"number": 50684, "title": "element_spec in tf.data.experimental.load - Variable Image Shapes", "body": "I am facing an issue while using \r\n```\r\ncheck_val_ds = tf.data.experimental.load(val_ds_path , \r\n                                                                  element_spec=(tf.TensorSpec(shape=(224, 224, 3), dtype=tf.uint8, name=None),\r\n                                                                   tf.TensorSpec(shape=(), dtype=tf.int64, name=None)))\r\n```\r\nHere the syntax requires element_spec argument to be filled, while I have images of variable shapes in the dataset, how do I mention the shape in such case.\r\n\r\nPlease suggest.\r\n\r\n\r\n", "comments": ["Resolved it, \r\ncan be closed!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50684\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50684\">No</a>\n"]}, {"number": 50683, "title": "Add support for WebP decoding in tf.io.decode_image", "body": "**System information**\r\n- TensorFlow version (you are using): 2.5.0\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\ntf.io.decode_image does not support WebP decoding. This makes using tfds and loading lsun/car impossible to use due to the fact that all images are encoded in WebP. \r\n\r\nI'm somewhat surprised that DecodeImageV2Op doesn't support this relatively popular file format.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAll researchers trying to move away from using JPEG\r\n\r\n**Any Other info.**\r\nThe code needed to decode WebP already exists in Tensorflow in:\r\nhttps://www.tensorflow.org/io/api_docs/python/tfio/image/decode_webp\r\n", "comments": ["I think it is a dup of https://github.com/tensorflow/io/issues/930", "It is a dupe indeed. "]}, {"number": 50682, "title": "Changes to add FusedMatMul + Sigmoid fusion", "body": "These changes add a new fusion : FusedMatMul and Sigmoid,  for INTEL oneDNN enabled flow on CPU. ", "comments": []}, {"number": 50680, "title": "[CherryPick2.6][TF XLA AOT] Assume aarch64 is always available.", "body": "This is useful for e.g. saved_model_cli aot_compile_cpu with a target\narchitecture of aarch64.\n\nPreviously this failed to build but now it seems OK.\n\nWhile we're at it, remove the manual tag from saved_model_cli_test.\n\nPiperOrigin-RevId: 383569229\nChange-Id: I91dcfccfbd0c817e12dee7e45985437001f56b50", "comments": ["Thanks!"]}, {"number": 50679, "title": "Disabling MKL on XLA Devices", "body": "Mitigates compatibility issues of XLA and MKL by disabling the MKL graph rewriter for ops assigned to XLA devices.", "comments": ["@philipphack Thanks for the PR.  There are checks to disable rewriting for XLA_CPU ops https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/mkl_layout_pass.cc#L1079 \r\nIs this check not sufficient?", "@agramesh1: Thanks for the note. This should resolve the issue."]}, {"number": 50678, "title": "Keras model compiled with custom loss raises \"Cannot convert a symbolic Keras input/output to a numpy array\" error.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen trying to train a keras model compiled with a custom loss, model.train_on_batch raises the following exception (full traceback is below):\r\n```  \r\nTypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model.\r\n```\r\nThis error only occurs in the `gradient_penalty_loss` and not in the `wasserstein_loss` which is also a custom loss function\r\n\r\n**Describe the expected behavior**\r\nThe `gradient_penalty_loss` should accept an extra argument like seen **[here](https://github.com/tensorflow/tensorflow/issues/38319#issuecomment-610475806)**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nI am a bit new to the topic I am working on so I couldn't reduce the code further than this but i included `TODO`-comments to mark the important lines of code\r\n```\r\nfrom __future__ import print_function, division\r\n\r\nimport numpy as np\r\nimport tensorflow.keras.backend as K\r\nimport tensorflow.keras.layers\r\nfrom tensorflow.keras.layers import BatchNormalization, Bidirectional, LSTM\r\nfrom tensorflow.keras.layers import Input, Dense, Reshape\r\nfrom tensorflow.keras.models import Sequential, Model\r\nfrom tensorflow.keras.optimizers import RMSprop\r\nfrom tensorflow.python.keras.layers import LeakyReLU\r\nfrom tensorflow.python.keras.layers.merge import _Merge\r\n\r\n\r\nclass RandomWeightedAverage(_Merge):\r\n    \"\"\"Provides a (random) weighted average between real and generated trajectory samples\"\"\"\r\n\r\n    def _merge_function(self, inputs):\r\n        alpha = K.random_uniform((1, 144, 1))\r\n        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\r\n\r\ndef gradient_penalty_loss(averaged_samples):\r\n    def loss(y_true, y_pred):\r\n        \"\"\"\r\n        Computes gradient penalty based on prediction and weighted real / fake samples\r\n        \"\"\"\r\n        gradients = K.gradients(y_pred, averaged_samples)[0]\r\n        # compute the euclidean norm by squaring ...\r\n        gradients_sqr = K.square(gradients)\r\n        #   ... summing over the rows ...\r\n        gradients_sqr_sum = K.sum(gradients_sqr,\r\n                                  axis=np.arange(1, len(gradients_sqr.shape)))\r\n        #   ... and sqrt\r\n        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\r\n        # compute lambda * (1 - ||grad||)^2 still for each single sample\r\n        gradient_penalty = K.square(1 - gradient_l2_norm)\r\n        # return the mean as loss over all the batch samples\r\n        return K.mean(gradient_penalty)\r\n\r\n    return loss\r\n\r\n\r\ndef wasserstein_loss(y_true, y_pred):\r\n    return K.mean(y_true * y_pred)\r\n\r\n\r\nclass WGANGP():\r\n    def __init__(self):\r\n        self.max_length = 144\r\n        self.features = 1\r\n        self.traj_shape = (self.max_length, self.features)\r\n        self.latent_dim = 100\r\n\r\n        # Following parameter and optimizer set as recommended in paper\r\n        self.n_discriminator = 5\r\n        optimizer = RMSprop(learning_rate=0.00005)\r\n\r\n        # Build the generator and discriminator\r\n        self.generator = self.build_generator()\r\n        self.discriminator = self.build_discriminator()\r\n\r\n        # -------------------------------\r\n        # Construct Computational Graph\r\n        #       for the Discriminator\r\n        # -------------------------------\r\n\r\n        # Freeze generator's layers while training discriminator\r\n        self.generator.trainable = False\r\n\r\n        # Trajectory input (real sample)\r\n        real_traj = Input(shape=self.traj_shape)\r\n\r\n        # Noise input\r\n        noise_d = Input(shape=(self.latent_dim,))\r\n        # Generate trajectory based of noise (fake sample)\r\n        fake_traj = self.generator(noise_d)\r\n\r\n        # Discriminator determines validity of the real and fake trajectories\r\n        fake = self.discriminator(fake_traj)\r\n        valid = self.discriminator(real_traj)\r\n\r\n        # Construct weighted average between real and fake trajectories\r\n        interpolated_traj = RandomWeightedAverage()([real_traj, fake_traj])\r\n        # Determine validity of weighted sample\r\n        validity_interpolated = self.discriminator(interpolated_traj)\r\n\r\n        self.discriminator_model = Model(inputs=[real_traj, noise_d],\r\n                                         outputs=[valid, fake, validity_interpolated])\r\n        self.discriminator_model.compile(loss=[wasserstein_loss,\r\n                                               wasserstein_loss,\r\n                                               gradient_penalty_loss(averaged_samples=interpolated_traj)],\r\n                                         optimizer=optimizer,\r\n                                         loss_weights=[1, 1, 10])\r\n        # -------------------------------\r\n        # Construct Computational Graph\r\n        #         for Generator\r\n        # -------------------------------\r\n\r\n        # For the generator we freeze the discriminator's layers\r\n        self.discriminator.trainable = False\r\n        self.generator.trainable = True\r\n\r\n        # Sampled noise for input to generator\r\n        noise_gen = Input(shape=(self.latent_dim,))\r\n        # Generate trajectory based of noise\r\n        traj = self.generator(noise_gen)\r\n        # Discriminator determines validity\r\n        valid = self.discriminator(traj)\r\n        # Defines generator model\r\n        self.generator_model = Model(noise_gen, valid)\r\n        self.generator_model.compile(loss=wasserstein_loss, optimizer=optimizer)\r\n\r\n    def build_generator(self):\r\n\r\n        model = Sequential()\r\n\r\n        model.add(Dense(256, input_dim=self.latent_dim))\r\n        model.add(LeakyReLU(alpha=0.2))\r\n        model.add(BatchNormalization(momentum=0.8))\r\n        model.add(Dense(512))\r\n        model.add(LeakyReLU(alpha=0.2))\r\n        model.add(BatchNormalization(momentum=0.8))\r\n        model.add(Dense(1024))\r\n        model.add(LeakyReLU(alpha=0.2))\r\n        model.add(BatchNormalization(momentum=0.8))\r\n        model.add(Dense(np.prod(self.traj_shape), activation='tanh'))\r\n        model.add(Reshape(self.traj_shape))\r\n\r\n        model.summary()\r\n\r\n        noise = Input(shape=(self.latent_dim,))\r\n        img = model(noise)\r\n\r\n        return Model(noise, img)\r\n\r\n    def build_discriminator(self):\r\n\r\n        model = Sequential()\r\n\r\n        model.add(LSTM(512, input_shape=self.traj_shape, return_sequences=True))\r\n        model.add(Bidirectional(LSTM(512)))\r\n        model.add(Dense(512))\r\n        model.add(LeakyReLU(alpha=0.2))\r\n        model.add(Dense(256))\r\n        model.add(LeakyReLU(alpha=0.2))\r\n        model.add(Dense(1, activation='tanh'))\r\n\r\n        model.summary()\r\n\r\n        traj = Input(shape=self.traj_shape)\r\n        validity = model(traj)\r\n\r\n        return Model(traj, validity)\r\n\r\n    def train(self, epochs, batch_size, sample_interval=50):\r\n        # Training data\r\n        X_train = np.load('data/preprocessed/train.npy', allow_pickle=True)\r\n\r\n        # Adversarial ground truths\r\n        valid = -np.ones((batch_size, 1))\r\n        fake = np.ones((batch_size, 1))\r\n        dummy = np.zeros((batch_size, 1))  # Dummy gt for gradient penalty\r\n        for epoch in range(epochs):\r\n\r\n            for _ in range(self.n_discriminator):\r\n                # ---------------------\r\n                #  Train Discriminator\r\n                # ---------------------\r\n\r\n                # Select a random batch of trajectories\r\n                idx = np.random.randint(0, X_train.shape[0], batch_size)\r\n                trajs = X_train[idx]\r\n                # Sample generator input\r\n                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\r\n                # Train the discriminator\r\n                #TODO: Error appears here after the first iteration\r\n                d_loss = self.discriminator_model.train_on_batch([trajs, noise],\r\n                                                                 [valid, fake, dummy])\r\n\r\n            # ---------------------\r\n            #  Train Generator\r\n            # ---------------------\r\n\r\n            g_loss = self.generator_model.train_on_batch(noise, valid)\r\n\r\n            # Plot the progress\r\n            print(\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\r\n\r\n\r\nif __name__ == '__main__':\r\n    wgan = WGANGP()\r\n    wgan.train(epochs=2000, batch_size=64, sample_interval=10)\r\n```\r\n\r\n**Other info / logs**\r\nI attached some sample data to use to reproduce here: [train.zip](https://github.com/tensorflow/tensorflow/files/6785899/train.zip)\r\nI mainly copied the code from **[here](https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py)** where it seams to work using the partial function but that threw an error, so i applied the proposed changes from **[this issue](https://github.com/tensorflow/tensorflow/issues/38319#issuecomment-610475806)** but now i get the described error.\r\nFull traceback of this issue:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python38\\lib\\contextlib.py\", line 131, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2833, in variable_creator_scope\r\n    yield\r\n  File \"C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1825, in train_on_batch\r\n    logs = self.train_function(iterator)\r\n  File \"C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 889, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 933, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 763, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3050, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3444, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3279, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 999, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 672, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 986, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in user code:\r\n\r\n    C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\r\n        return step_function(self, iterator)\r\n    CC:/path/to/project/lstm_wgan.py:37 loss  *\r\n        gradients = K.gradients(y_pred, averaged_samples)[0]\r\n    C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4131 gradients  **\r\n        return gradients_module.gradients(\r\n    C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:169 gradients\r\n        return gradients_util._GradientsHelper(\r\n    C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:531 _GradientsHelper\r\n        xs = ops.internal_convert_n_to_tensor_or_indexed_slices(\r\n    C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:369 internal_convert_n_to_tensor_or_indexed_slices\r\n        internal_convert_to_tensor_or_indexed_slices(\r\n    C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:330 internal_convert_to_tensor_or_indexed_slices\r\n        return ops.convert_to_tensor(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:163 wrapped\r\n        return func(*args, **kwargs)\r\n    C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1566 convert_to_tensor\r\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:339 _constant_tensor_conversion_function\r\n        return constant(v, dtype=dtype, name=name)\r\n    C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:264 constant\r\n        return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n    C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:281 _constant_impl\r\n        tensor_util.make_tensor_proto(\r\n    C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:435 make_tensor_proto\r\n        values = np.asarray(values)\r\n    C:\\path\\to\\project\\venv\\lib\\site-packages\\numpy\\core\\_asarray.py:83 asarray\r\n        return array(a, dtype, copy=False, order=order)\r\n    C:\\path\\to\\project\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\keras_tensor.py:254 __array__\r\n        raise TypeError(\r\n\r\n    TypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model.\r\n\r\n```\r\n", "comments": ["@Alexanderstaehle ,\r\n\r\nI ran the code shared and face error, please find the [gist](https://colab.research.google.com/gist/tilakrayal/1df33312c18a68f6c7eab38d6a1be023/untitled50678.ipynb) here and share all dependencies to replicate the issue or share a colab gist with the reported error.Thanks!", "Hey @tilakrayal :) Thanks for the answer! The error you encountered occurs because the sample data was not uploaded :) i tried including it in a gist [here](https://colab.research.google.com/gist/Alexanderstaehle/afca7b85a227e676baa96116a4e07981/untitled50678.ipynb) but you can also find it in the initial description of the error under the bulletpoint \"Othee info/logs\" or under this link [here.](https://github.com/tensorflow/tensorflow/files/6785899/train.zip) Now the right error is shown at least on my side. Hope that helps :)", "@Alexanderstaehle \r\n\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "I had the same problem when i use tf==2.4.1 and numpy==1.20,but when i downgrade numpy to 1.19.5,i got \r\n`ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject`\r\nThe answer I found in past issues is to upgrade numpy to 1.20, this is too strange.I'm now stuck in a dead end with the numpy version", "@TheWindWillNotStop \r\nDoes the issue occur with tf 2.5 as well, can you please confrim.", "I was also having this issue, using tf 2.5.0 whilst attempting to make a VAE.\r\n\r\nI've found a workaround however, removing the initialisation of the custom loss function from the model compile function and instead adding it prior.\r\n\r\nSo what was:\r\n`vae.compile(optimizer=optimiser, loss = vae_custom_loss)`\r\n\r\nis now:\r\n\r\n```\r\nvae.add_loss(vae_custom_loss(inputs,outputs))\r\nvae.compile(optimizer=optimiser)\r\n```\r\n\r\nIf you require further information please let me know. I'm very new to machine learning so I may have missed something out!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Moving this to closed status due to lack of activity.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50678\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50678\">No</a>\n"]}, {"number": 50677, "title": "Disable ppc64le custom gemm_pack_rhs to use the convolution specialization", "body": "Eigen ppc64le has a gemm_pack_rhs specialization for AltiVec architecture. TF Convolution also specializes this packing routine, improving the Tensor code and avoiding a lot of index computation. Based on eigen_benchmark_cpu_test, we found a good scenario for ppc64le, where gemm_pack_rhs from TF headers together with VSX/MMA from ppc64le speeds up the performance of convolution.\r\n\r\nThis patch fixes the column computation for convolution in case of ppc64le and disables the Eigen ppc64le gemm_pack_rhs using the define EIGEN_ALTIVEC_USE_CUSTOM_PACK=0.", "comments": ["Hi, the PR #47768 fixed the issue #44626, but slowed down the convolution performance (based on eigen_benchmark_cpu_test).  This PR fixes this performance issue using the ability to disable ppc64le gemm_pack_rhs in Eigen (https://gitlab.com/libeigen/eigen/-/commit/91e99ec1e02100d07e35a7abb1b5c76707237219)\r\n\r\n@Flamefire @ezhulenev @ChipKerchner", "This is great! Thanks @maxiwell !", "Just double-checking: The mechanism in Eigen is very recent. Did you update Eigen already so this is actually used?\r\n\r\nBecause otherwise the ifdef check would return true, include the TF code but Eigen would still have its code so we are back to the conflicting specializations", "@Flamefire Yes, I have waited for Eigen update here to start the PR", "Ah, right, so this likely won't back-port to 2.5 cleanly then. Beyond the updated Eigen, are there are other dependent changes in main beyond the 2.5 tag?", "@jayfurmanek AFAIK, you just need to use both commits in this PR and change the `third_party/eigen3/workspace.bzl` to update Eigen", "Gotcha. That's good.\r\nDo you have the minimum commit of Eigen that's needed? If not, I can look it up.\r\nthanks.", "This commit https://github.com/tensorflow/tensorflow/commit/b513d76287a1067ee2a161dfac2ecfb6741417dc updates Eigen with a good commit id for that (another file is also updated in this commit, but only the workspace.bzl is necessary here)"]}, {"number": 50675, "title": "How to use weights to create a new op in Tensorflow", "body": "I'd like to define a convolutional op by following this: https://www.tensorflow.org/guide/create_op. How can I access filter weights in the Compute method? In the call to the REGISTER_OP macro, should I define filter dimensions? Where the weights are updated?", "comments": ["@Lucy20211 \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow)/ TF [Forum](https://discuss.tensorflow.org/) There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. \r\nThanks!\r\n", "Ok, sorry.", "@Lucy20211 \r\n\r\nCould you please move this to closed status and open  new issue with mentioned links above.Thanks!"]}, {"number": 50674, "title": "BigBiGAN Loss Function", "body": "Hi, \r\nI wanted to ask whether the BigBiGAN network contains KL Divergence Loss in the implementation? Because they have mentioned non-deterministic encoder architecture for sampling z vector. They used Variational Bayes for sampling as mentioned.\r\n\r\nIf anyone can help me out, it would be great!\r\n\r\nThanks in advance. \r\n", "comments": ["@shreejalt \r\nCan you please let us know, where is it mentioned about \"non-deterministic encoder architecture for sampling z vector\". ", "@Saduf2019 , \r\n\r\nAs in ALI [10], the encoder E of our Base model is nondeterministic, parametrizing a distribution N (\u00b5, \u03c3). \u00b5 and \u03c3\u02c6 are given by a linear layer at the output\r\nof the model, and the final standard deviation \u03c3 is computed from \u03c3\u02c6 using a non-negative \u201csoftplus\u201d\r\nnon-linearity \u03c3 = log(1 + exp(\u02c6\u03c3)) [9]. The final z uses the reparametrized sampling from [23],\r\nwith z = \u00b5 + \u000f\u03c3, where \u000f \u223c N (0, I). Compared to a deterministic encoder (row Deterministic E)\r\nwhich predicts z directly without sampling (effectively modeling P(z|x) as a Dirac \u03b4 distribution),\r\nthe non-deterministic Base model achieves significantly better classification performance. \r\n\r\nThis is copy-pasted from the paper.  Ablation 3.1 (latent distribution Pz and stochastic E). \r\n\r\n\r\nLet me know if i am getting something wrong...\r\n\r\nPaper Link: https://arxiv.org/pdf/1907.02544.pdf", "@shreejalt \r\nWhere in tensorflow do you expect these changes to be made.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50673, "title": "Module 'tensorflow._api.v2.summary' has no attribute 'scalar'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nCustom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nRed Hat 7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNA\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\n2.5.0\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nNA\r\n- GCC/Compiler version (if compiling from source):\r\nNA\r\n- CUDA/cuDNN version:\r\n- NA\r\n- GPU model and memory:\r\nNA\r\n\r\n**Describe the current behavior**\r\nWhen running one machine, when I execute the line `tf.summary.scalar(\"train_ll_per_seq\", ll_per_seq)` I get `module 'tensorflow._api.v2.summary' has no attribute 'scalar'.  Interestingly enough, on another machine, also RHEL 7.6, the code executes as expected.\r\n\r\n**Describe the expected behavior**\r\nThe above `scalar` should be recognized on both machines.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n`import tensorflow as tf`\r\n``tf.summary.scalar(\"train_ll_per_seq\", ll_per_seq)`\r\n", "comments": ["Picking up on a stray tensorflow-tensorboard install.  It is weird that one for computer, it apparently used the correct version and one it did not because I copied over the requirements.txt from one environment to another.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50673\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50673\">No</a>\n"]}]