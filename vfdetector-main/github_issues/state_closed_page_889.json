[{"number": 26812, "title": "Can't declare tf.Variable in @tf.function decorated function", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Archlinux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-dev20190317\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n\r\nA function that correctly works in eager execution can't be decorated with `@tf.function` if declares a `tf.Variable` in the function body.\r\n\r\nThe error message, reported below, is misleading since it talks about a non-first invocation when the function is invoked only once.\r\n\r\n```\r\nValueError: tf.function-decorated function tried to create variables on non-first call.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nCalling a function decorated with the `@tf.function` should produce the same output as the same function without the decoration.\r\n\r\n**Code to reproduce the issue**\r\n\r\nimport tensorflow as tf\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef f():\r\n    a = tf.constant([[10, 10], [11., 1.]])\r\n    x = tf.constant([[1., 0.], [0., 1.]])\r\n    b = tf.Variable(12.)\r\n    y = tf.matmul(a, x) + b\r\n    return y\r\n\r\nprint(f())\r\n```\r\n", "comments": ["see [this](https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md#functions-that-create-state)\r\n\r\nand rewrite your code to\r\n```\r\nimport tensorflow as tf\r\n\r\nb = None\r\n\r\n\r\n@tf.function\r\ndef f():\r\n    a = tf.constant([[10, 10], [11., 1.]])\r\n    x = tf.constant([[1., 0.], [0., 1.]])\r\n    global b\r\n    if b is None:\r\n        b = tf.Variable(12.)\r\n    y = tf.matmul(a, x) + b\r\n    return y\r\n\r\nprint(f())\r\n```\r\n\r\nor \r\n```\r\nclass F(object):\r\n    def __init__(self):\r\n        self.b = None\r\n\r\n    @tf.function\r\n    def __call__(self):\r\n        a = tf.constant([[10, 10], [11., 1.]])\r\n        x = tf.constant([[1., 0.], [0., 1.]])\r\n        if self.b is None:\r\n            self.b = tf.Variable(12.)\r\n        y = tf.matmul(a, x) + self.b\r\n        return y\r\n\r\n\r\nf = F()\r\nprint(f())\r\n```", "HI @zakizhou ! I'm aware that defining a function that declares a variable inside creates a status and it has to be handled differently.\r\n\r\nAs you suggested an alternative I to wrap the function in a class and make the variable a private attribute, or declaring the variable as global and check if it is `None`.\r\n\r\nHowever, as stated in the RFC:\r\n\r\n> State (like tf.Variable objects) are only created the first time the function f is called.\r\n\r\nThus, when I invoke `f()` for the first time, I should have the graph definition with a `tf.Variable` object created and executed once. And it should work according to what is written in the RFC.\r\n\r\nInstead, what happens is that even though I call the function only once, and thus this is the first call, it seems like `tf.function` is evaluating the function twice (like calling `f(), f()`), making what is stated in the RFC false.", "tf.function may evaluate your python function more than once.\r\n\r\nWhat the RFC states instead is that you are allowed to create variables as long as variable creation only happens the first time your python function is evaluated.", "And the reason for this is that if you write python code which unconditionally creates variables then I can't tell whether you mean to create a new variable every time the python function is called (eager behavior) or reuse the existing variables (what happens in graph tf 1.x) so an error felt safer.", "Thank you for the explanation @alextp - now everything is clearer. The RFC just describes the first call, but there is no guarantee that tf.function won't call the function more than once while converting it as a graph and for this reason, the developer should take care of handling the variables creation; thus If I need to reuse or not a variable it makes no difference, I have to take care of its status manually.", "Yes, exactly. With TF 2.0 in general we're trying to get TF out of your way\nas much as possible, instead of pushing every possible thing you'd want to\ndo inside TF. We think this is simpler, more modular, and easier to\nintegrate with larger codebases.\n\nOn Wed, Mar 20, 2019 at 10:29 AM Paolo Galeone <notifications@github.com>\nwrote:\n\n> Thank you for the explanation @alextp <https://github.com/alextp> - now\n> everything is clearer. The RFC just describes the first call, but there is\n> no guarantee that tf.function won't call the function more than once while\n> converting it as a graph and for this reason, the developer should take\n> care of handling the variables creation; thus If I need to reuse or not a\n> variable it makes no difference, I have to take care of its status manually.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26812#issuecomment-474943478>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxSQYBidgnZS8sTPjSbwXUWcZqKmLks5vYm_rgaJpZM4b4pkS>\n> .\n>\n\n\n-- \n - Alex\n", "I have an exact issue like this, but with using different optimizers:\r\n\r\n```\r\n@tf.function\r\ndef apply_gradients_once(optimizer, grads, vars):\r\n    optimizer.apply_gradients(zip(\r\n        grads, vars))\r\n\r\n\r\ndef apply_gradients(self, use_fast, grads_per_model):\r\n    for i in range(self.nmodels):\r\n        grads = grads_per_model[i]\r\n        vars = self.model[i].get_trainable_variables()\r\n\r\n        if use_fast[i]:\r\n            optimizer = self.fast_optimizer\r\n            apply_gradients_once(optimizer, grads, vars)\r\n        else:\r\n            optimizer = self.slow_optimizer\r\n            apply_gradients_once(optimizer, grads, vars)\r\n\r\n```\r\n\r\nOn a first epoch, I have `use_fast = [True, False]`, and then on a second one I have\r\n`use_fast = [False, True]`.\r\n\r\n\r\nWith this, I have a huge error trace on the `apply_gradients_once` call, ending with `ValueError: tf.function-decorated function tried to create variables on non-first call.`.\r\n\r\nFrom my understanding, since the function is always called with different arguments, and there are no external dependencies, this error should not be happening.", "@ericpts this is an interesting use case I hadn't thought of.\r\n\r\nAs a workaround I recommend you use two instances of tf.function here.\r\n\r\nLet's open another issue to discuss this?", "One alternative workaround is to make the boolean parameter a tensor; then\nTF will trace both sides of the conditional once and pre-create all\nvariables.\n\nOn Mon, Mar 25, 2019 at 4:11 AM Alexandre Passos <notifications@github.com>\nwrote:\n\n> @ericpts <https://github.com/ericpts> this is an interesting use case I\n> hadn't thought of.\n>\n> As a workaround I recommend you use two instances of tf.function here.\n>\n> Let's open another issue to discuss this?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26812#issuecomment-476129357>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxWcd8Q8-bfQrUYBDZzMCgTVMmbE_ks5vaKDWgaJpZM4b4pkS>\n> .\n>\n\n\n-- \n - Alex\n", "Hello, Is there a workaround for a situation like this. Here I need to create several w, b variables using add_layer method. But I get the variable reused error. \r\n\r\n\r\ndef add_layer(input, c_kdimension, c_kstrides):\r\n  w = tf.get_variable(name=\"w\", shape=c_kdimension, initializer = tf.contrib.layers.xavier_initializer()\r\n  b = tf.get_variable(name=\"b\", shape=c_kdimension[-1], initializer=tf.contrib.layers.xavier_initializer())\r\n  return tf.nn.conv2d(input, w, strides=c_kstrides, padding=\"SAME\")\r\n\r\n@tf.function \r\ndef inference(input):\r\n  model = Model()\r\n  layer_1 = add_layer(input, c_kdimension=[3, 3, 1, 32], c_kstrides=[1, 1, 1, 1])\r\n  layer_2 = add_layer(layer_1, c_kdimension=[3, 3, 32, 32], c_kstrides=[1, 1, 1, 1])\r\n  return layer2", "Hello @galeone ,\r\nAs you are using TensorFlow 2.0 you must add\r\n`tf.config.experimental_run_functions_eagerly(True)`\r\nafter import and then run the code.", "> Hello @galeone ,\r\n> As you are using TensorFlow 2.0 you must add\r\n> `tf.config.experimental_run_functions_eagerly(True)`\r\n> after import and then run the code.\r\n\r\nSorry, this is wrong, do not use it. This disables the graph construction and runs everything eagerly, like numpy. This _can_ be wanted for debugging or in certain cases, but it is surely not the workaround.\r\n\r\n@praveen-14 read again above about creating them as class variables or in the global scope."]}, {"number": 26811, "title": "[TF2.0] Bug when saving weights with custom layers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\ndocker image from tensorflow/tensorflow:2.0.0a0-gpu-jupyter\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\ngit version 'v1.12.0-9492-g2c319fb415'\r\ntensorflow version '2.0.0-alpha0'\r\n- Python version:\r\n2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\ncuda version 415.27\r\n- GPU model and memory:\r\nGTX 1080Ti\r\n\r\n**Describe the current behavior**\r\nIf custom layers are used weights of the model cannot be saved using model.save_weights() or exported to the savedmethod. I am using the code provided in the Custom Layer code provided in the documentation at https://www.tensorflow.org/alpha/guide/keras/custom_layers_and_models#you_can_optionally_enable_serialization_on_your_layers\r\n\r\n**Describe the expected behavior**\r\nThe model to save its weights properly\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nlayers = tf.keras.layers\r\nkeras = tf.keras\r\n\r\n# From documentation until the next comment\r\nclass Linear(layers.Layer):\r\n\r\n    def __init__(self, units=32, **kwargs):\r\n        super(Linear, self).__init__(**kwargs)\r\n        self.units = units\r\n\r\n    def build(self, input_shape):\r\n        self.w = self.add_weight(shape=(input_shape[-1], self.units),\r\n                                 initializer='random_normal',\r\n                                 trainable=True)\r\n        self.b = self.add_weight(shape=(self.units,),\r\n                                 initializer='random_normal',\r\n                                 trainable=True)\r\n\r\n    def call(self, inputs):\r\n        return tf.matmul(inputs, self.w) + self.b\r\n\r\n    def get_config(self):\r\n        config = super(Linear, self).get_config()\r\n        config.update({'units': self.units})\r\n        return config\r\n    \r\n\r\nlayer = Linear(10)\r\nconfig = layer.get_config()\r\nprint(config)\r\nnew_layer = Linear.from_config(config)\r\n\r\n# Creating a layer and saving its weights\r\ndata = np.random.random((1000, 10))\r\nlabels = np.random.random((1000, 10))\r\ninputs = keras.Input((10,))\r\noutputs = layer(inputs)\r\nmodel = keras.Model(inputs, outputs)\r\nconfig = model.get_config()\r\nprint(config)\r\nprint(model.summary())\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.fit(data, labels, batch_size=10, epochs=1)\r\n\r\nmodel.save_weights(\"temp/layers_weights\")\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n{'units': 10, 'dtype': None, 'trainable': True, 'name': 'linear_5'}\r\n{'layers': [{'class_name': 'InputLayer', 'config': {'dtype': 'float32', 'batch_input_shape': (None, 10), 'name': 'input_5', 'sparse': False}, 'inbound_nodes': [], 'name': 'input_5'}, {'class_name': 'Linear', 'config': {'units': 10, 'dtype': 'float32', 'trainable': True, 'name': 'linear_5'}, 'inbound_nodes': [['input_5', 0, 0, {}]], 'name': 'linear_5'}], 'input_layers': ['input_5', 0, 0], 'output_layers': ['linear_5', 0, 0], 'name': 'model_4'}\r\nModel: \"model_4\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_5 (InputLayer)         [(None, 10)]              0         \r\n_________________________________________________________________\r\nlinear_5 (Linear)            (None, 10)                110       \r\n=================================================================\r\nTotal params: 110\r\nTrainable params: 110\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\n1000/1000 [==============================] - 1s 523us/sample - loss: 37.3735 - accuracy: 0.1060\r\n\r\n------------------------------------------------------------\r\nAttributeError             Traceback (most recent call last)\r\n<ipython-input-7-32331e9dcd27> in <module>()\r\n     47 model.fit(data, labels, batch_size=10, epochs=1)\r\n     48 \r\n---> 49 model.save_weights(\"temp/layers_weights\")\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/network.pyc in save_weights(self, filepath, overwrite, save_format)\r\n   1409              'saved.\\n\\nConsider using a TensorFlow optimizer from `tf.train`.')\r\n   1410             % (optimizer,))\r\n-> 1411       self._trackable_saver.save(filepath, session=session)\r\n   1412       # Record this checkpoint so it's visible from tf.train.latest_checkpoint.\r\n   1413       checkpoint_management.update_checkpoint_state_internal(\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/util.pyc in save(self, file_prefix, checkpoint_number, session)\r\n    976     save_path, new_feed_additions = self._save_cached_when_graph_building(\r\n    977         file_prefix=file_prefix_tensor,\r\n--> 978         object_graph_tensor=object_graph_tensor)\r\n    979     if new_feed_additions:\r\n    980       feed_dict.update(new_feed_additions)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/util.pyc in _save_cached_when_graph_building(self, file_prefix, object_graph_tensor)\r\n    916     (named_saveable_objects, graph_proto,\r\n    917      feed_additions) = self._gather_saveables(\r\n--> 918          object_graph_tensor=object_graph_tensor)\r\n    919     if (self._last_save_object_graph != graph_proto\r\n    920         # When executing eagerly, we need to re-create SaveableObjects each time\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/util.pyc in _gather_saveables(self, object_graph_tensor)\r\n    882     \"\"\"Wraps _serialize_object_graph to include the object graph proto.\"\"\"\r\n    883     (named_saveable_objects, graph_proto,\r\n--> 884      feed_additions) = self._graph_view.serialize_object_graph()\r\n    885     if object_graph_tensor is None:\r\n    886       with ops.device(\"/cpu:0\"):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/graph_view.pyc in serialize_object_graph(self)\r\n    379     trackable_objects, path_to_root = self._breadth_first_traversal()\r\n    380     return self._serialize_gathered_objects(\r\n--> 381         trackable_objects, path_to_root)\r\n    382 \r\n    383   def frozen_saveable_objects(self, object_map=None, to_graph=None):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/graph_view.pyc in _serialize_gathered_objects(self, trackable_objects, path_to_root, object_map)\r\n    335     object_names = object_identity.ObjectIdentityDictionary()\r\n    336     for obj, path in path_to_root.items():\r\n--> 337       object_names[obj] = _object_prefix_from_path(path)\r\n    338     node_ids = object_identity.ObjectIdentityDictionary()\r\n    339     for node_id, node in enumerate(trackable_objects):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/graph_view.pyc in _object_prefix_from_path(path_to_root)\r\n     62   return \"/\".join(\r\n     63       (_escape_local_name(trackable.name)\r\n---> 64        for trackable in path_to_root))\r\n     65 \r\n     66 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/graph_view.pyc in <genexpr>((trackable,))\r\n     62   return \"/\".join(\r\n     63       (_escape_local_name(trackable.name)\r\n---> 64        for trackable in path_to_root))\r\n     65 \r\n     66 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/graph_view.pyc in _escape_local_name(name)\r\n     55   # edges traversed to reach the variable, so we escape forward slashes in\r\n     56   # names.\r\n---> 57   return (name.replace(_ESCAPE_CHAR, _ESCAPE_CHAR + _ESCAPE_CHAR)\r\n     58           .replace(r\"/\", _ESCAPE_CHAR + \"S\"))\r\n     59 \r\n\r\nAttributeError: 'NoneType' object has no attribute 'replace'\r\n```", "comments": ["The weights need to be saved in HDF5 format:\r\n\r\n```model.save_weights('path_to_my_weights.h5')```\r\n", "Saving in h5 format also result in an error. \r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nlayers = tf.keras.layers\r\nkeras = tf.keras\r\n\r\n# From documentation until the next comment\r\nclass Linear(layers.Layer):\r\n\r\n    def __init__(self, units=32, **kwargs):\r\n        super(Linear, self).__init__(**kwargs)\r\n        self.units = units\r\n\r\n    def build(self, input_shape):\r\n        self.w = self.add_weight(shape=(input_shape[-1], self.units),\r\n                                 initializer='random_normal',\r\n                                 trainable=True)\r\n        self.b = self.add_weight(shape=(self.units,),\r\n                                 initializer='random_normal',\r\n                                 trainable=True)\r\n\r\n    def call(self, inputs):\r\n        return tf.matmul(inputs, self.w) + self.b\r\n\r\n    def get_config(self):\r\n        config = super(Linear, self).get_config()\r\n        config.update({'units': self.units})\r\n        return config\r\n    \r\n\r\nlayer = Linear(10)\r\nconfig = layer.get_config()\r\nprint(config)\r\nnew_layer = Linear.from_config(config)\r\n\r\n# Creating a layer and saving its weights\r\ndata = np.random.random((1000, 10))\r\nlabels = np.random.random((1000, 10))\r\ninputs = keras.Input((10,))\r\noutputs = layer(inputs)\r\nmodel = keras.Model(inputs, outputs)\r\nconfig = model.get_config()\r\nprint(config)\r\nprint(model.summary())\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.fit(data, labels, batch_size=10, epochs=2)\r\nmodel.save_weights(\"temp/layers_weights_inh5format\",save_format=\"h5\")\r\n```\r\nstacktrace below\r\n```\r\n{'units': 10, 'dtype': None, 'trainable': True, 'name': 'linear_4'}\r\n{'layers': [{'class_name': 'InputLayer', 'config': {'dtype': 'float32', 'batch_input_shape': (None, 10), 'name': 'input_5', 'sparse': False}, 'inbound_nodes': [], 'name': 'input_5'}, {'class_name': 'Linear', 'config': {'units': 10, 'dtype': 'float32', 'trainable': True, 'name': 'linear_4'}, 'inbound_nodes': [['input_5', 0, 0, {}]], 'name': 'linear_4'}], 'input_layers': ['input_5', 0, 0], 'output_layers': ['linear_4', 0, 0], 'name': 'model_4'}\r\nModel: \"model_4\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_5 (InputLayer)         [(None, 10)]              0         \r\n_________________________________________________________________\r\nlinear_4 (Linear)            (None, 10)                110       \r\n=================================================================\r\nTotal params: 110\r\nTrainable params: 110\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\nEpoch 1/2\r\n1000/1000 [==============================] - 1s 550us/sample - loss: 40.7062 - accuracy: 0.1070\r\nEpoch 2/2\r\n1000/1000 [==============================] - 0s 432us/sample - loss: 41.0578 - accuracy: 0.1070\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-5-e5a7e605e04a> in <module>()\r\n     46               metrics=['accuracy'])\r\n     47 model.fit(data, labels, batch_size=10, epochs=2)\r\n---> 48 model.save_weights(\"temp/layers_weights_inh5format\",save_format=\"h5\")\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/network.pyc in save_weights(self, filepath, overwrite, save_format)\r\n   1393     if save_format == 'h5':\r\n   1394       with h5py.File(filepath, 'w') as f:\r\n-> 1395         hdf5_format.save_weights_to_hdf5_group(f, self.layers)\r\n   1396     else:\r\n   1397       if context.executing_eagerly():\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/saving/hdf5_format.pyc in save_weights_to_hdf5_group(f, layers)\r\n    691     save_attributes_to_hdf5_group(g, 'weight_names', weight_names)\r\n    692     for name, val in zip(weight_names, weight_values):\r\n--> 693       param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)\r\n    694       if not val.shape:\r\n    695         # scalar\r\n\r\n/usr/local/lib/python2.7/dist-packages/h5py/_hl/group.pyc in create_dataset(self, name, shape, dtype, data, **kwds)\r\n    137             dset = dataset.Dataset(dsid)\r\n    138             if name is not None:\r\n--> 139                 self[name] = dset\r\n    140             return dset\r\n    141 \r\n\r\n/usr/local/lib/python2.7/dist-packages/h5py/_hl/group.pyc in __setitem__(self, name, obj)\r\n    369 \r\n    370             if isinstance(obj, HLObject):\r\n--> 371                 h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)\r\n    372 \r\n    373             elif isinstance(obj, SoftLink):\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/h5o.pyx in h5py.h5o.link()\r\n\r\nRuntimeError: Unable to create link (name already exists)\r\n\r\n```", "@TahaK @jvishnuvardhan, I have tried this myself and yes this issue is reproducible. \r\nAs per my observation this issue occurred in keras aswell https://github.com/keras-team/keras/issues/6844", "Would you mind passing a name for each weight? like this\r\n```python\r\nself.w = self.add_weight(name='w',\r\n                         shape=(input_shape[-1], self.units),\r\n```", "@fchollet It seems a bug of getter:(https://github.com/tensorflow/tensorflow/blob/db6c140a7ecea6ccf4aa608404d4e69c980e8398/tensorflow/python/keras/engine/base_layer.py#L364\r\n\r\n Can we let `name` become a required field for `add_weight`?\r\n\r\n```python\r\n>>> class A(keras.layers.Layer):\r\n...   def __init__(self):\r\n...     super(A, self).__init__()\r\n...     self.weight_without_name = self.add_weight(shape=(3, 4))\r\n...     self.default_weight = self.add_weight()\r\n...\r\n>>> a = A()\r\n>>> a.weight_without_name\r\n<tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=\r\narray([[-0.09872711, -0.7932683 , -0.42732993, -0.9035983 ],\r\n       [-0.50050485,  0.37918067,  0.09857702,  0.56991994],\r\n       [ 0.7190112 ,  0.27319026, -0.29729384, -0.16420442]],\r\n      dtype=float32)>\r\n>>> a.default_weight\r\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-1.3627009>\r\n```", "> Would you mind passing a name for each weight? like this\r\n> \r\n> ```python\r\n> self.w = self.add_weight(name='w',\r\n>                          shape=(input_shape[-1], self.units),\r\n> ```\r\n\r\nYes passing a name solves the problem. I can save and load weights now, great!", "Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/ccf2b7a935d13a91fe5b4180277000eb/tf_26811.ipynb). Thanks!\r\n\r\nAutomatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26811\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26811\">No</a>\n", "> > Would you mind passing a name for each weight? like this\r\n> > ```python\r\n> > self.w = self.add_weight(name='w',\r\n> >                          shape=(input_shape[-1], self.units),\r\n> > ```\r\n> \r\n> Yes passing a name solves the problem. I can save and load weights now, great!\r\n\r\nSo, why passing a name works?\r\nI use tf.saved_model.save to save my model and encounter the same error, will it work by passing a name?", "> > > Would you mind passing a name for each weight? like this\r\n> > > ```python\r\n> > > self.w = self.add_weight(name='w',\r\n> > >                          shape=(input_shape[-1], self.units),\r\n> > > ```\r\n> > \r\n> > \r\n> > Yes passing a name solves the problem. I can save and load weights now, great!\r\n> \r\n> So, why passing a name works?\r\n> I use tf.saved_model.save to save my model and encounter the same error, will it work by passing a name?\r\n\r\nIt's related to #36650, see especially the last comment: https://github.com/tensorflow/tensorflow/issues/36650#issuecomment-585517731"]}, {"number": 26810, "title": "Gesture Classification Web App (many bugs on JS code)", "body": "- index.js : function loadModel does not exist\r\nline **32** `const mobilenet = await tf.loadModel(\r\n`\r\ni replaced it for `loadLayersModel` as the only function with a similar name\r\n`const mobilenet = await tf.loadLayersModel(`\r\n\r\n\r\n- ui.js : two missing carry return\r\nline **151** `} ui.donePredicting =`\r\nreplaced by \r\n`}`\r\n`ui.donePredicting =`\r\nline **181** `} let mouseDown = false;`\r\nreplaced by \r\n`}`\r\n`let mouseDown = false;`\r\n\r\n- webcam.js\r\nline 39 ` const webcamImage = tf.fromPixels(this.webcamElement);`\r\n\r\ni replaced it for `browser.fromPixels`, i found it inspecting tfjs.js code\r\n` const webcamImage = tf.browser.fromPixels(this.webcamElement);`\r\n\r\nafter those changes application worked correctly ...\r\n", "comments": ["Perhaps local js code (index.js, ui.js and wbecam.js) was code with respect to previous version of `tfjs.js` (i mean not for the latest at least)\r\n`<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest\"> </script>`", "This issue is more suitable on tfjs repo. The folks maintaining tfjs repo may guide you better. Please post it on tfjs from [here](https://github.com/tensorflow/tfjs/issues). Thanks!", "Done ([1412](https://github.com/tensorflow/tfjs/issues/1412))\r\nThanks"]}, {"number": 26809, "title": "[TF 2.0] Cannot load Keras Sequential model witn InputLayer from h5.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Debian Stable**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **CPU, both TF-2.0.0a0 and tf-nightly-2.0-preview-2.0.0.dev20190315**\r\n- Python version: **3.5.3**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n**Describe the current behavior**\r\nWhen a `Sequential` Keras model contains `InputLayer` and it is saved, it cannot be loaded and fails with a message\r\n```\r\nValueError: You are trying to load a weight file containing 1 layers into a model with 0 layers.\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe model can be loaded correctly.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninputs = np.arange(10)\r\noutputs = 2 * inputs\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.InputLayer(input_shape=[1]),\r\n    tf.keras.layers.Dense(1),\r\n])\r\nmodel.compile(\r\n    optimizer=tf.keras.optimizers.Adam(),\r\n    loss=tf.keras.losses.MeanSquaredError(),\r\n    metrics=[tf.keras.metrics.MeanSquaredError()]\r\n)\r\nmodel.fit(inputs, outputs)\r\nmodel.save(\"model.h5\")\r\n\r\nloaded_model = tf.keras.models.load_model(\"model.h5\")\r\n```\r\n\r\n**Other info / logs**\r\nThe problem is in the fact that `Sequential.layers` does not return the `InputLayer`. To quote comments from the method:\r\n```python\r\n  def layers(self):\r\n    # Historically, `sequential.layers` only returns layers that were added\r\n    # via `add`, and omits the auto-generated `InputLayer` that comes at the\r\n    # bottom of the stack.\r\n    # `Trackable` manages the `_layers` attributes and does filtering\r\n    # over it.\r\n```\r\nThe problem is that if `InputLayer` was added manually with a specified `input_shape`, then it is an error not to serialize it -- because then the following layers do not know what the input shape is.\r\n\r\nWorkarounds:\r\n- if instead of `InputLayer` you add `input_shape` to the `Dense` layer, it works. However, input `dtype` cannot be specified in this way and when you pass `tf.int32` on input, using an `InputLayer` is required\r\n- in Functional API the `tf.keras.layers.Input` _is_ serialized in the model (and, funnily, as a `InputLayer`).\r\n\r\nSolutions:\r\n- when serializing a `Sequential` model, all layers need to be serialized (and no filtering of `InputLayer` performed).", "comments": ["I ran into the same issue in tensorflow 1.13.1 as well. I don't understand why the InputLayer is being filtered out while serializing the model. A work around which I found online is to add the batch input shape parameter to the first layer in the model file.\r\n\r\n```\r\nimport json\r\nimport h5py\r\n\r\ndef fix_layer0(filename, batch_input_shape, dtype):\r\n    with h5py.File(filename, 'r+') as f:\r\n        model_config = json.loads(f.attrs['model_config'].decode('utf-8'))\r\n        layer0 = model_config['config']['layers'][0]['config']\r\n        layer0['batch_input_shape'] = batch_input_shape\r\n        layer0['dtype'] = dtype\r\n        f.attrs['model_config'] = json.dumps(model_config).encode('utf-8')\r\n\r\nfix_layer0('test.h5', [None, *input_shape], 'float32')\r\n```", "This is fixed in latest TF 2.0 nightly build '2.0.0-dev20190718'\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26809\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26809\">No</a>\n"]}, {"number": 26808, "title": "[TF 2.0]\u00a0unconnected_gradients = 'zero' does not work", "body": "**System information**\r\n- OS Platform and Distribution: MacOS 10.14.3 \r\n- TensorFlow installed from binary\r\n- TensorFlow version: 2.0.0a0\r\n- Python version: 3.7.2\r\n\r\nI try to get gradients w.r.t. model parameters. Though I was getting None values. Here is an example:\r\n\r\n```\r\n> import tensorflow as tf\r\n> import tensorflow.keras.layers as layers\r\n\r\n> model = tf.keras.Sequential()\r\n> model.add(layers.Dense(10, input_shape=(2,)))\r\n> with tf.GradientTape() as tape:\r\n>   loss = tf.random.normal((10, 10))\r\n> grads = tape.gradient(loss, model.trainable_variables, unconnected_gradients='zero')\r\n> print(grads)\r\n[None, None]\r\n```\r\n\r\nI expect these values to be zero. Though they are not.", "comments": ["We need to add special treatment for DT_RESOURCE tensors when building the zeros tensors [here](https://github.com/tensorflow/tensorflow/blob/919b38007ea755a5b5ec87af324c91f55dce6717/tensorflow/python/eager/pywrap_tfe_src.cc#L1764). Maybe we can return a float32 with zeros here? @akshaym could you look into this?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26808\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26808\">No</a>\n"]}, {"number": 26807, "title": "[TF 2.0] tf 2 doesn't allow static unrolling of loops which can be slower", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: Python 3.6.7\r\n- CUDA/cuDNN version: 10.0/7.4\r\n- GPU model and memory: GeForce GTX 1060, Compute Capability 6.1, 6GB RAM\r\n\r\n**Describe the current behavior**\r\n\r\nThere doesn't appear to be any way to unroll a loop statically when using `tf.function`. In tensorflow 1 I was able to create lots of nodes in a graph using a for loop, when using `tf.function` autograph replaces this with a `tf.while_loop` which seems to be slower in some cases.\r\n\r\n**Describe the expected behavior**\r\n\r\nThere should be a way to statically unroll a loop when using `tf.function` for performance reasons.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThis code calculates a matrix exponential using a taylor series. I also noticed that `tf.linalg.expm`has gotten a lot slower but that was not my main interest in this issue (I believe the implementation for that has changed to support autodiff, see https://github.com/tensorflow/tensorflow/issues/15465)\r\n\r\noutput (using the latest `tensorflow-gpu` and `tensorflow-gpu==2.0.0-alpha` respectively from pip):\r\n``` shell\r\n(tf1) $ python expm.py \r\nBENCHMARKS:\r\ntf took 0.1753 seconds for 25 iterations\r\ntaylor took 0.0146 seconds for 25 iterations\r\n\r\n(tf2) $ python expm.py \r\nBENCHMARKS:\r\ntf took 0.7331 seconds for 25 iterations\r\ntaylor took 1.0135 seconds for 25 iterations\r\ntaylor_v2 took 0.5931 seconds for 25 iterations\r\n```\r\n\r\ninput:\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time as tm\r\n\r\nMATRIX_DIM = 1000\r\nTAYLOR_SUM = 30\r\nEPS = 1e-2\r\nWARMUP = 10\r\nITERATIONS = 25\r\n\r\n\r\ndef benchmark(name, fn):\r\n    for _ in range(WARMUP):\r\n        value = fn()\r\n    start = tm.time()\r\n    for _ in range(ITERATIONS):\r\n        value = fn()\r\n    end = tm.time()\r\n    runtime = end - start\r\n    print(f\"{name} took {runtime:.4f} seconds for {ITERATIONS} iterations\")\r\n    return value\r\n\r\n\r\ndef taylor_expm(x, n):\r\n    x_0 = tf.eye(tf.shape(x)[0], dtype=x.dtype)\r\n    x_i = x\r\n    y = x_0 + x_i\r\n    for i in range(2, n + 1):\r\n        x_i = (x_i @ x) / tf.cast(i, x.dtype)\r\n        y = y + x_i\r\n    return y\r\n\r\n\r\nnp.random.seed(42)\r\n\r\nx = tf.constant(np.random.uniform(-0.5, 0.5, [MATRIX_DIM, MATRIX_DIM]), tf.float32)\r\n\r\nif tf.__version__.startswith(\"2\"):\r\n\r\n    @tf.function\r\n    def taylor_expm_v2(x, n):\r\n        return taylor_expm(x, n)\r\n\r\n    print(\"\\nBENCHMARKS:\")\r\n    tf_expm = benchmark(\"tf\", lambda: tf.linalg.expm(x))\r\n    my_expm = benchmark(\"taylor\", lambda: taylor_expm(x, TAYLOR_SUM))\r\n    my_expm_v2 = benchmark(\"taylor_v2\", lambda: taylor_expm_v2(x, TAYLOR_SUM))\r\n\r\n    np.testing.assert_allclose(tf_expm.numpy(), my_expm.numpy(), atol=EPS)\r\n    np.testing.assert_allclose(tf_expm.numpy(), my_expm_v2.numpy(), atol=EPS)\r\nelse:\r\n    tf_expm_ = tf.linalg.expm(x)\r\n    my_expm_ = taylor_expm(x, TAYLOR_SUM)\r\n\r\n    with tf.Session() as sess:\r\n        print(\"\\nBENCHMARKS:\")\r\n        tf_expm = benchmark(\"tf\", lambda: sess.run(tf_expm_))\r\n        my_expm = benchmark(\"taylor\", lambda: sess.run(my_expm_))\r\n\r\n    np.testing.assert_allclose(tf_expm, my_expm, atol=EPS)\r\n```", "comments": ["The loop is actually being unrolled in this example. You can verify this by inserting a few print statements:\r\n\r\n```\r\n    print('trace 1')\r\n    for i in range(2, n + 1):\r\n        print('trace 2')\r\n        ...\r\n```\r\n\r\nOut of curiosity, I tried a version that uses `tf.while_loop`, like so:\r\n\r\n```\r\n    for i in tf.range(2, n + 1):\r\n        ...\r\n```\r\n\r\nand that didn't seem to change the results significantly in my tests.", "@alextp \r\n\r\nOut of curiosity, I tried a mostly-empty function:\r\n\r\n```\r\ndef taylor_expm_empty(x, n):\r\n    x_0 = tf.eye(tf.shape(x)[0], dtype=x.dtype)\r\n    x_i = x\r\n    y = x_0 + x_i\r\n    return y\r\n```\r\n\r\nThe results were comparable with taylor_v2. So I wonder whether we're looking at invocation overhead here?", "@mdanatg you're most likely looking at function building / pruning / first execution overhead, yes.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26807\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26807\">No</a>\n", "Many thanks for your time looking at this. I can see now that some static unrolling of the loop is happening, however I'm still not sure how to make the v2 version as fast as the v1 version, which might make tf2 unusable for some work I am doing.\r\n\r\nIs there any way to make the tf2 version competitive?", "@jeffpollock9 can you run your code through a profiler (like cProfiler in python) to find out what the bottlenecks are?\r\n\r\nHappy to work on improving the performance of TF v2.", "@alextp I've tried to run the code through [cProfile](https://docs.python.org/3.6/library/profile.html#profile.Profile) (mostly just copied their example) and profile the same code which was being timed, but I don't think it gives anything particularly useful (likely just my misunderstanding, though). Looks like it is spending all of the time in `_pywrap_tensorflow_internal.TFE_Py_Execute`?\r\n\r\noutput:\r\n```\r\nBENCHMARKS:\r\ntf took 0.7202 seconds for 25 iterations\r\ntaylor took 1.1474 seconds for 25 iterations\r\ntaylor_v2 took 0.5886 seconds for 25 iterations\r\n\r\nPROFILES:\r\nprofile for taylor_v2\r\n         2251 function calls in 0.592 seconds\r\n\r\n   Ordered by: cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n       25    0.000    0.000    0.592    0.024 expm.py:73(<lambda>)\r\n       25    0.000    0.000    0.592    0.024 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:400(__call__)\r\n       25    0.000    0.000    0.591    0.024 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py:1267(__call__)\r\n       25    0.000    0.000    0.590    0.024 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py:542(_filtered_call)\r\n       25    0.000    0.000    0.590    0.024 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py:560(_call_flat)\r\n       25    0.000    0.000    0.589    0.024 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py:379(call)\r\n       25    0.000    0.000    0.588    0.024 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py:33(quick_execute)\r\n       25    0.588    0.024    0.588    0.024 {built-in method _pywrap_tensorflow_internal.TFE_Py_Execute}\r\n       25    0.000    0.000    0.001    0.000 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py:1505(_maybe_define_function)\r\n       25    0.000    0.000    0.001    0.000 /home/jeff/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py:1414(_cache_key)\r\n   ...\r\n```\r\n\r\ninput:\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time as tm\r\nimport cProfile as pr\r\nimport io\r\nimport pstats as ps\r\n\r\nMATRIX_DIM = 1000\r\nTAYLOR_SUM = 30\r\nEPS = 1e-2\r\nWARMUP = 10\r\nITERATIONS = 25\r\n\r\n\r\ndef benchmark(name, fn):\r\n    for _ in range(WARMUP):\r\n        value = fn()\r\n    start = tm.time()\r\n    for _ in range(ITERATIONS):\r\n        value = fn()\r\n    end = tm.time()\r\n    runtime = end - start\r\n    print(f\"{name} took {runtime:.4f} seconds for {ITERATIONS} iterations\")\r\n    return value\r\n\r\n\r\ndef profile(name, fn):\r\n    for _ in range(WARMUP):\r\n        value = fn()\r\n\r\n    profile = pr.Profile()\r\n    profile.enable()\r\n\r\n    for _ in range(ITERATIONS):\r\n        value = fn()\r\n\r\n    profile.disable()\r\n    s = io.StringIO()\r\n    sortby = \"cumulative\"\r\n    stats = ps.Stats(profile, stream=s).sort_stats(sortby)\r\n    stats.print_stats()\r\n    print(f\"profile for {name}\")\r\n    print(s.getvalue())\r\n\r\n    return value\r\n\r\n\r\ndef taylor_expm(x, n):\r\n    x_0 = tf.eye(tf.shape(x)[0], dtype=x.dtype)\r\n    x_i = x\r\n    y = x_0 + x_i\r\n    for i in range(2, n + 1):\r\n        x_i = (x_i @ x) / tf.cast(i, x.dtype)\r\n        y = y + x_i\r\n    return y\r\n\r\n\r\n@tf.function\r\ndef taylor_expm_v2(x, n):\r\n    return taylor_expm(x, n)\r\n\r\n\r\nnp.random.seed(42)\r\n\r\nx = tf.constant(np.random.uniform(-0.5, 0.5, [MATRIX_DIM, MATRIX_DIM]), tf.float32)\r\n\r\nprint(\"\\nBENCHMARKS:\")\r\ntf_expm = benchmark(\"tf\", lambda: tf.linalg.expm(x))\r\nmy_expm = benchmark(\"taylor\", lambda: taylor_expm(x, TAYLOR_SUM))\r\nmy_expm_v2_b = benchmark(\"taylor_v2\", lambda: taylor_expm_v2(x, TAYLOR_SUM))\r\n\r\nprint(\"\\nPROFILES:\")\r\nmy_expm_v2_p = profile(\"taylor_v2\", lambda: taylor_expm_v2(x, TAYLOR_SUM))\r\n\r\nnp.testing.assert_allclose(tf_expm.numpy(), my_expm.numpy(), atol=EPS)\r\nnp.testing.assert_allclose(tf_expm.numpy(), my_expm_v2_b.numpy(), atol=EPS)\r\nnp.testing.assert_allclose(tf_expm.numpy(), my_expm_v2_p.numpy(), atol=EPS)\r\n```", "This shows that most of the execution time of your program is indeed inside tensorflow's runtime, and not python overhead, so slow graph building is not the cause for your poor performance.", "Any idea what is the cause then? A sequence of matrix multiplications with some element wise addition and division being approximately 40 times slower with the tensorflow 2 runtime feels very worrying to me. I hope I have made a mistake somewhere but I am really struggling to see where.\r\n\r\nAlso please let me know if it is worth opening another issue for this.", "I think it's worth opening another issue, yes.\n\nOn Mon, Mar 25, 2019 at 3:01 PM Jeff <notifications@github.com> wrote:\n\n> Any idea what is the cause then? A sequence of matrix multiplications with\n> some element wise addition and division being approximately 40 times slower\n> with the tensorflow 2 runtime feels very worrying to me. I hope I have made\n> a mistake somewhere but I am really struggling to see where.\n>\n> Also please let me know if it is worth opening another issue for this.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26807#issuecomment-476393643>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxYinQnaXqsCAZ3f1fT4F86KDs58Dks5vaUc3gaJpZM4b4huJ>\n> .\n>\n\n\n-- \n - Alex\n", "Many thanks to everyone with your help on this, I have opened a new issue as per @alextp's suggestion (https://github.com/tensorflow/tensorflow/issues/27143) so can continue this discussion there."]}, {"number": 26806, "title": "tflite runtime error with depthwise conv2D ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows/Ubuntu\r\n- TensorFlow installed from (source or binary): Tried source and binary\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10/7.3\r\n- GPU model and memory: RTX 2070\r\n\r\n**Describe the current behavior**\r\n\r\nI run into the following error when trying to run a tflite model.\r\n`RuntimeError: tensorflow/lite/kernels/depthwise_conv.cc:104 params->depth_multiplier * SizeOfDimension(input, 3) != SizeOfDimension(filter, 3) (0 != 6)Node number 7 (DEPTHWISE_CONV_2D) failed to prepare.`\r\n\r\nThe code runs fine normally, just not after exporting to tflite.\r\nhttps://www.tensorflow.org/lite/guide/ops_compatibility -> States that the op is supported. \r\nI know it has been used for the MobileNet variants before.\r\nI specified rate=[1,1] and the input kernel is constant (tf.constant).\r\n\r\nThis error has also been reported in the comments of https://github.com/tensorflow/tensorflow/issues/20798\r\nI have tried with tf v1.12.0 and v2.0.\r\n\r\n**Describe the expected behavior**\r\nIn my case, the depth multiplier should be 1 and the size of the input channel dimension should be 6. \r\nI'm not sure why it says it is 0?\r\n\r\n**Code to reproduce the issue**\r\nI will provide a small snippet of code to reproduce the error, if it is not well known.", "comments": ["Any update on this issue? I have a similar problem with running depthwise convolutions on a Raspberry Pi.", "Please provide code to reproduce. Thanks!", "New MLIR converter should have fixed this bug. \r\nset experimental_new_converter=True", "@roymiles Please update as per the above comment and let us know if this issue still persists ? Thanks!\r\n", "My apologies, I haven't been working with tflite for a while and I do not have the code to reproduce this problem.", "@roymiles thanks for the update ,please let us know if you want to move this issue to closed status ? Thanks !", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26806\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26806\">No</a>\n"]}, {"number": 26805, "title": "updated relu function docstring", "body": "This addresses issue #26211 . Added description, example and updated parameter description. ", "comments": ["Duplicate of #26257 ", "Hi, @kyscg the issue is yet opened and have a look at the code, I have not copied it. The changes made by me are my original and as per the guidelines. ", "@GrtSid can you please check build failures ?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 26804, "title": "when using model_to_estimator on a keras model -> accuracy and loss are not stored in events.out.tfevents.xxxxxxxxxx", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Yes\r\n- TensorFlow version (use command below):\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nwhen using a keras model and transforming it with `model_to_estimator` accuracy and loss are not stored in`events.out.tfevents.xxxxxxxxxx` (used by TensorBoard for visualiztion). I am using `tf.estimator.train()` or `tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)` and in both cases I only get:\r\n`dict_keys(['global_step/sec', 'loss_1'])`\r\n\r\nusing `tf.estimator.train()` I am expecting loss and accuray in `events.out.tfevents.xxxxxxxxxx` for the training dataset\r\n\r\nusing `tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)` I am expecting loss and accuray in `events.out.tfevents.xxxxxxxxxx` for the training dataset abd testing datset.\r\n\r\nThe info are computed and display in the logfiles:\r\n```\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Evaluation [10/100]\r\nINFO:tensorflow:Evaluation [20/100]\r\nINFO:tensorflow:Evaluation [30/100]\r\nINFO:tensorflow:Evaluation [40/100]\r\nINFO:tensorflow:Evaluation [50/100]\r\nINFO:tensorflow:Evaluation [60/100]\r\nINFO:tensorflow:Evaluation [70/100]\r\nINFO:tensorflow:Finished evaluation at 2019-03-16-12:28:05\r\nINFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9766614, global_step = 1000, loss = 0.074898034\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: results/Models/Mnist/tf_1_12/estimator/ckpt/model.ckpt-1000\r\n```\r\nIs this expected ? It is documented somehwere ? without such info in TensorBoard then it is impossible to see if the model is overfitting or not. I am not even sure which info is displayed in the logfile, right ? or is there some other magic way to get such info ?\r\n\r\nwhy tf.summary information are not propagated from keras to estimator model ? We don't have much control over the estimator model when it is converted from Keras\r\n\r\nOne option is to use a estimator model using keras layer so we can control everything but since Keras is now the official high level API from TensorFlow, I was thinking I could just transform it to an estimator model. I will be happy to test TF 2.0 if this will fix my issue. \r\n\r\n**Describe the expected behavior**\r\n\r\nwhen I am using the same keras model and running the `fit` method, I got in the `events.out.tfevents.xxxxxxxxxx` file:\r\n\r\n`dict_keys(['batch_acc', 'batch_loss', 'epoch_acc', 'epoch_loss', 'epoch_val_acc', 'epoch_val_loss'])`\r\n\r\nI got 6 variables inluding accuracy and loss for the training and test datset. This is what Iam expected.\r\n\r\n**Code to reproduce the issue**\r\ncode to inspect `events.out.tfevents.xxxxxxxxxx` files:\r\n```\r\nfrom tensorboard.backend.event_processing import event_accumulator\r\nimport numpy as np\r\n\r\ndef load_data_tensorboard(path):\r\n    event_acc = event_accumulator.EventAccumulator(path)\r\n    event_acc.Reload()\r\n    data = {}\r\n    \r\n    for tag in sorted(event_acc.Tags()[\"scalars\"]):\r\n        x, y = [], []\r\n        for scalar_event in event_acc.Scalars(tag):\r\n            x.append(scalar_event.step)\r\n            y.append(scalar_event.value)\r\n        data[tag] = (np.asarray(x), np.asarray(y))\r\n    return data\r\n```\r\none notebook contain the code but this is \"work in progress\":\r\nhttps://github.com/tarrade/proj_DL_models_and_pipelines_with_GCP/blob/master/notebook/08-Mnist_keras_estimator.ipynb", "comments": ["possible duplicate of this 6 months old issue ? \r\nhttps://github.com/tensorflow/tensorflow/issues/21983\r\nor it was fixed and is now back with TF 1.12 ?", "Is this still happening in 1.14? Lots have changed.", "I saw the issue/feature with Tensorfow 2.0 alpha. I didn't check again with TF 2.0 beta or the latest nightly. I will look at it and come back to you @tanzhenyu ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26804\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26804\">No</a>\n", "@tanzhenyu sorry, I closed the wrong ticket. Just re-open it.", "@tanzhenyu, with the latest version of TF 2.0 (tf-nightly-2.0-preview==2.0.0.dev20190725) I see the same \"bug\". Maybe I am missing something  but I see no reason to not dispaly accuracy in such case. The issue is that it make the usage of keras -> estimator useless. One solution with TF 2.0  is to use with keras layer in an estimator model.\r\n\r\nI find model_to_estimator great but useless if we don't have the right metrics to monitor during training.", "Hi @tarrade ,please provide us the updated code for TF 2.0 for which you are getting the error .Thanks ! ", "Hi @sushreebarsa ,\r\n\r\nthis is a very old ticket. Estimator will not be supported in the future > TF 2.X so I am not using it anymore. Tensorflow high level API is now only Keras. Closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26804\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26804\">No</a>\n"]}, {"number": 26803, "title": "[TF 2.0] timestep-wise sample weights not working with tf.data.Dataset and tf.keras.Model", "body": "**System information**\r\n- TensorFlow version (use command below): tf-nightly-2.0-preview\r\n\r\n**Describe the current behavior**\r\n\r\nWith some model such as object detection one need to weight losses per locations. In this case, the output tensor can have the following shape `[N, T, ...]` where N the batch size and T the locations. \r\nAccording to the documentation `tf.keras.Model` can handle timestep-wise sample weight by setting ` sample_weight_mode=\"temporal\"`. \r\nBut this seems not to be the case when we use `tf.Dataset` as inputs.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(MyModel, self).__init__(name='')\r\n\r\n        self.l1 = tf.keras.layers.Conv2D(10, 3)\r\n        self.o1 = tf.keras.layers.Conv2D(2, 1)\r\n        self.o2 = tf.keras.layers.Conv2D(3, 1, name='o2')\r\n\r\n        # print(self._output_names)\r\n\r\n    def call(self, inputs):\r\n        batch_size = tf.shape(inputs)[0]\r\n        y = self.l1(inputs)\r\n        y1 = self.o1(y)\r\n        y1 = tf.reshape(y1, [batch_size, -1, 2])\r\n        y2 = self.o2(y)\r\n        y2 = tf.reshape(y2, [batch_size, -1, 3])\r\n\r\n        return y1, y2\r\n\r\n\r\nclass Loss1(tf.keras.losses.Loss):\r\n\r\n    def call(self, targets, predictions):\r\n        losses = tf.math.abs(predictions - targets)\r\n        return tf.reduce_sum(losses, axis=2)\r\n\r\n\r\nx = tf.random.uniform([10, 16, 16, 3])\r\ny1 = tf.random.uniform([10, 14*14, 2])\r\ny2 = tf.random.uniform([10, 14*14, 3])\r\nw1 = tf.random.uniform([10, 14*14, 1])\r\nw2 = tf.random.uniform([10, 14*14, 1])\r\n\r\n\r\ndata = tf.data.Dataset.from_tensor_slices((x, (y1, y2), {'output_1': w1, 'output_2': w2})).batch(5).repeat()\r\n\r\n\r\nmodel = MyModel()\r\noptimizer = tf.keras.optimizers.SGD(0.001)\r\n\r\nmodel.compile(optimizer=optimizer,\r\n              loss=[Loss1(), Loss1()],\r\n              loss_weights=[1., 1.],\r\n              sample_weight_mode=\"temporal\",\r\n              run_eagerly=True)\r\n\r\n\r\nmodel.fit(data, epochs=5, steps_per_epoch=1)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\nValueError: Found a sample_weight array with shape (5, 196, 1). In order to use timestep-wise sample weights, you should specify sample_weight_mode=\"temporal\" in compile(). If you just mean to use sample-wise weights, make sure your sample_weight array is 1D.\r\n```", "comments": ["From the error message, I think the issue is in Keras. Reassigning to @fchollet for triage.", "@fchollet  Is there any advance in this issue. I notice that if we define the model as follow\r\n```\r\ninputs = keras.Input(shape=(16, 16, 3))\r\ny = keras.layers.Conv2D(10, 3)(inputs)\r\ny1 = keras.layers.Conv2D(2, 1)(y)\r\ny1 = keras.layers.Reshape((-1, 2), name='o1')(y1)\r\ny2 = keras.layers.Conv2D(3, 1)(y)\r\ny2 = keras.layers.Reshape((-1, 3), name='o2')(y2)\r\n\r\nmodel = keras.Model(inputs=inputs, outputs=[y1, y2])\r\n```\r\n\r\nThe timestep-wise sample weight from the dataset works.\r\n\r\nIt seems that the problem occurs when we subclass `Model` directly. Furthermore the outputs names can't be customised in this case\r\n", "Any updates on this?", "> @fchollet Is there any advance in this issue. I notice that if we define the model as follow\r\n> \r\n> ```\r\n> inputs = keras.Input(shape=(16, 16, 3))\r\n> y = keras.layers.Conv2D(10, 3)(inputs)\r\n> y1 = keras.layers.Conv2D(2, 1)(y)\r\n> y1 = keras.layers.Reshape((-1, 2), name='o1')(y1)\r\n> y2 = keras.layers.Conv2D(3, 1)(y)\r\n> y2 = keras.layers.Reshape((-1, 3), name='o2')(y2)\r\n> \r\n> model = keras.Model(inputs=inputs, outputs=[y1, y2])\r\n> ```\r\n> \r\n> The timestep-wise sample weight from the dataset works.\r\n> \r\n> It seems that the problem occurs when we subclass `Model` directly. Furthermore the outputs names can't be customised in this case\r\n\r\nI have a similar issue to this. My tf version is 1.14, and I let the `sample_weight=np.zeros(len(X))` as the third element of tf.data . There is no Error happen, but the final loss is not 0, which means that the `sample_weight` argument did not work at all.", "This is fixed with TF 2.4.1 see [gist](https://colab.research.google.com/gist/ymodak/94818fff19847334fbcfe594714d7680/untitled17.ipynb) for your reference. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26803\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26803\">No</a>\n"]}, {"number": 26802, "title": "New issue cross-compiling latest tensorflow source for arm", "body": "System information:\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo.\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.10 under Hyper-V Virtual Machine (Windows 10)\r\nResource dedicated to virtual machine:\r\n4GB Ram - 4 cores \r\nTensorFlow installed from (source or binary):\r\nSource\r\nTensorFlow version (use command below):\r\nLatest from tensorflow's github page\r\nPython version:\r\n3.6.7\r\nDocker version:\r\n18.09.3\r\n\r\nI'm building the source for raspberry pi 3B+ and I followed the instructions on this page:\r\n[https://www.tensorflow.org/install/source_rpi](https://www.tensorflow.org/install/source_rpi)\r\n\r\nThe error on the precedure:\r\n\r\nERROR: /workspace/tensorflow/core/kernels/BUILD:3004:1: C++ compilation of rule '//tensorflow/core/kernels:matrix_exponential_op' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command \r\n  (cd /home/emperon/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH='' \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  /home/emperon/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/arm-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -isystem /usr/include/arm-linux-gnueabihf -isystem /usr/include/python3.4 -isystem /usr/include/ -MD -MF bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/matrix_exponential_op/matrix_exponential_op.pic.d '-frandom-seed=bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/matrix_exponential_op/matrix_exponential_op.pic.o' -fPIC -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/armeabi-opt/genfiles -iquote bazel-out/armeabi-opt/bin -iquote external/com_google_absl -iquote bazel-out/armeabi-opt/genfiles/external/com_google_absl -iquote bazel-out/armeabi-opt/bin/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out/armeabi-opt/genfiles/external/bazel_tools -iquote bazel-out/armeabi-opt/bin/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/armeabi-opt/genfiles/external/eigen_archive -iquote bazel-out/armeabi-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/armeabi-opt/genfiles/external/local_config_sycl -iquote bazel-out/armeabi-opt/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/armeabi-opt/genfiles/external/nsync -iquote bazel-out/armeabi-opt/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/armeabi-opt/genfiles/external/gif_archive -iquote bazel-out/armeabi-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/armeabi-opt/genfiles/external/jpeg -iquote bazel-out/armeabi-opt/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/armeabi-opt/genfiles/external/protobuf_archive -iquote bazel-out/armeabi-opt/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/armeabi-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/armeabi-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/armeabi-opt/genfiles/external/farmhash_archive -iquote bazel-out/armeabi-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/armeabi-opt/genfiles/external/fft2d -iquote bazel-out/armeabi-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/armeabi-opt/genfiles/external/highwayhash -iquote bazel-out/armeabi-opt/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/armeabi-opt/genfiles/external/zlib_archive -iquote bazel-out/armeabi-opt/bin/external/zlib_archive -iquote external/double_conversion -iquote bazel-out/armeabi-opt/genfiles/external/double_conversion -iquote bazel-out/armeabi-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/armeabi-opt/genfiles/external/snappy -iquote bazel-out/armeabi-opt/bin/external/snappy -iquote external/hwloc -iquote bazel-out/armeabi-opt/genfiles/external/hwloc -iquote bazel-out/armeabi-opt/bin/external/hwloc -isystem external/eigen_archive -isystem bazel-out/armeabi-opt/genfiles/external/eigen_archive -isystem bazel-out/armeabi-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/armeabi-opt/genfiles/external/nsync/public -isystem bazel-out/armeabi-opt/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/armeabi-opt/genfiles/external/gif_archive/lib -isystem bazel-out/armeabi-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/armeabi-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/armeabi-opt/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/armeabi-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/armeabi-opt/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/armeabi-opt/genfiles/external/zlib_archive -isystem bazel-out/armeabi-opt/bin/external/zlib_archive -isystem external/double_conversion -isystem bazel-out/armeabi-opt/genfiles/external/double_conversion -isystem bazel-out/armeabi-opt/bin/external/double_conversion -isystem external/hwloc/hwloc -isystem bazel-out/armeabi-opt/genfiles/external/hwloc/hwloc -isystem bazel-out/armeabi-opt/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/armeabi-opt/genfiles/external/hwloc/include -isystem bazel-out/armeabi-opt/bin/external/hwloc/include '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' -DTENSORFLOW_MONOLITHIC_BUILD -pthread -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c tensorflow/core/kernels/matrix_exponential_op.cc -o bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/matrix_exponential_op/matrix_exponential_op.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nIn file included from external/eigen_archive/unsupported/Eigen/MatrixFunctions:59:0,\r\n                 from ./third_party/eigen3/unsupported/Eigen/MatrixFunctions:1,\r\n                 from tensorflow/core/kernels/matrix_exponential_op.cc:19:\r\nexternal/eigen_archive/unsupported/Eigen/src/MatrixFunctions/MatrixFunction.h: In member function 'MatrixType Eigen::internal::MatrixFunctionAtomic<MatrixType>::compute(const MatrixType&) [with MatrixType = Eigen::Matrix<std::complex<float>, -1, -1>]':\r\nexternal/eigen_archive/unsupported/Eigen/src/MatrixFunctions/MatrixFunction.h:100:1: internal compiler error: in decompose_normal_address, at rtlanal.c:5799\r\n }\r\n ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <http://gcc.gnu.org/bugs.html> for instructions.\r\nINFO: Elapsed time: 2614.056s, Critical Path: 113.01s\r\nINFO: 3861 processes: 3861 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["I wasn't able to reproduce this error when trying locally, so closing for now. Please reopen if you're still seeing this."]}, {"number": 26801, "title": "tf.keras.losses.SparseCategoricalCrossentropy bug", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (use command below):2.0.0-alpha0 , cpu version\r\n- Python version:3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\ntry the demo in  tf.keras.losses.SparseCategoricalCrossentropy code annotation:\r\n  cce = tf.keras.losses.SparseCategoricalCrossentropy()\r\n  loss = cce([0, 1, 2],  [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\r\n  print('Loss: ', loss.numpy()) \r\n\r\ngot error:          AttributeError: 'list' object has no attribute 'dtype'\r\n\r\n", "comments": ["In order to expedite the trouble shooting process, Can you please provide the link to the demo you are referring to? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 26800, "title": "saved_model_cli tensorrt convert bug: saved_model_main_op collection and it's related operation was mistakenly pruned.", "body": "Hi, guys,\r\n\r\nTensorflow serving has released 1.13.0 recently, which adds a support for TF-TRT, and I'm trying to introduce it into our production enviroment.\r\n\r\nThere is a good introduction for this feature: https://medium.com/tensorflow/optimizing-tensorflow-serving-performance-with-nvidia-tensorrt-6d8a2347869a\r\n\r\nAccording to the post, I have to convert the SavedModel into a TRT-optimized one first with the help of `saved_model_cli`, and then serve it in Tensorflow Serving.\r\n\r\nIt all goes well with the example the post provides, but it failed in my case. **After I converted my own model and served it in Tensorflow Serving, the server threw 'Failed precondition: Table not initialized.' error.** I searched the related source code and finally figured out what happened.\r\n\r\nThere is an index_to_string subgraph in my model, which is mainly composed of a HashTableV2 Operation and a InitializeTableV2 Operation. There's also a collection named 'saved_model_main_op' which finally points to the InitializeTableV2 Operation. When TensorFlow Serving loads the SavedModel, it tries to initialize the HashTable via executing the operations in `saved_model_main_op` collection. **But after `saved_model_cli` converted the graph into a TensorRT-Optimized one, The 'saved_model_main_op' collection and its related Operation has been pruned. As a result, tf serving failed to initialize the table.**\r\n\r\nThis is the partial graph before conversion:\r\n\r\n![](https://raw.githubusercontent.com/monklof/assets/master/original-graph-tb.png \"graph before conversion\")\r\n\r\nAfter conversion:\r\n\r\n![](https://raw.githubusercontent.com/monklof/assets/master/table_not_initialized_bug.png \"graph after conversion\")\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: Cuda 9.0; cuDNN 7.3\r\n- GPU model and memory: Tesla V100, 16G\r\n\r\n**Describe the current behavior**\r\n\r\nas mentioned above.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe `saved_model_main_op` collection and it's related op should be preserved after conversion.\r\n\r\n**Code to reproduce the issue**\r\n\r\n\r\nThis is the code for exporting SavedModel:\r\n\r\n```python\r\n# -*- coding: utf-8 -*-\r\n\r\nimport os.path\r\n\r\n# This is a placeholder for a Google-internal import.\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\nfrom nets import resnet_v2\r\nfrom preprocessing import  vgg_preprocessing as vgg\r\n\r\ntf.app.flags.DEFINE_string('checkpoint_dir', '/opt/zhoulinyuan/inception_v4',\r\n                           \"\"\"Directory where to read training checkpoints.\"\"\")\r\ntf.app.flags.DEFINE_string('output_dir', '/tmp/inception_v4_porn_output',\r\n                           \"\"\"Directory where to export inference model.\"\"\")\r\ntf.app.flags.DEFINE_integer('model_version', 4,\r\n                            \"\"\"Version number of the model.\"\"\")\r\ntf.app.flags.DEFINE_integer('image_size', 224,\r\n                            \"\"\"Needs to provide same value as in training.\"\"\")\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\nNUM_CLASSES = 3\r\nNUM_TOP_CLASSES = 3\r\n\r\ndef export():\r\n  # Create index->synset mapping\r\n  synsets = []\r\n\r\n  with tf.Graph().as_default():\r\n    # Build inference model.\r\n    # Please refer to Tensorflow inception model for details.\r\n\r\n    # Input transformation.\r\n    serialized_tf_example = tf.placeholder(tf.string, name='tf_example')\r\n    feature_configs = {\r\n        'image/encoded': tf.FixedLenFeature(\r\n            shape=[], dtype=tf.string),\r\n    }\r\n    tf_example = tf.parse_example(serialized_tf_example, feature_configs)\r\n    jpegs = tf_example['image/encoded']\r\n    images = tf.map_fn(preprocess_image, jpegs, dtype=tf.float32)\r\n\r\n    # Run inference.\r\n    # logits, _ = inception_model.inference(images, NUM_CLASSES + 1)\r\n\r\n    # Run inference.\r\n    with slim.arg_scope(resnet_v2.resnet_arg_scope()):\r\n      logits, _ = resnet_v2.resnet_v2_50(images, NUM_CLASSES, is_training=False)\r\n    logits = tf.nn.softmax(logits)\r\n\r\n    # Transform output to topK result.\r\n    values, indices = tf.nn.top_k(logits, NUM_TOP_CLASSES)\r\n\r\n    class_descriptions = ['0_xx', '1_yy', '2_zz']\r\n    class_tensor = tf.constant(class_descriptions)\r\n\r\n    table = tf.contrib.lookup.index_to_string_table_from_tensor(class_tensor)\r\n    classes = table.lookup(tf.to_int64(indices))\r\n\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n      # Restore variables from training checkpoints.\r\n      saver.restore(sess, FLAGS.checkpoint_dir)\r\n      \r\n      # keys = sess.graph.get_all_collection_keys()\r\n      sess.graph.clear_collection('resnet_v2_50/_end_points')\r\n\r\n      # Export inference model.\r\n      output_path = os.path.join(\r\n          tf.compat.as_bytes(FLAGS.output_dir),\r\n          tf.compat.as_bytes(str(FLAGS.model_version)))\r\n      print 'Exporting trained model to', output_path\r\n      builder = tf.saved_model.builder.SavedModelBuilder(output_path)\r\n\r\n      # Build the signature_def_map.\r\n      classify_inputs_tensor_info = tf.saved_model.utils.build_tensor_info(\r\n          serialized_tf_example)\r\n      classes_output_tensor_info = tf.saved_model.utils.build_tensor_info(\r\n          classes)\r\n      scores_output_tensor_info = tf.saved_model.utils.build_tensor_info(values)\r\n\r\n      classification_signature = (\r\n          tf.saved_model.signature_def_utils.build_signature_def(\r\n              inputs={\r\n                  tf.saved_model.signature_constants.CLASSIFY_INPUTS:\r\n                      classify_inputs_tensor_info\r\n              },\r\n              outputs={\r\n                  tf.saved_model.signature_constants.CLASSIFY_OUTPUT_CLASSES:\r\n                      classes_output_tensor_info,\r\n                  tf.saved_model.signature_constants.CLASSIFY_OUTPUT_SCORES:\r\n                      scores_output_tensor_info\r\n              },\r\n              method_name=tf.saved_model.signature_constants.\r\n              CLASSIFY_METHOD_NAME))\r\n\r\n      predict_inputs_tensor_info = tf.saved_model.utils.build_tensor_info(jpegs)\r\n      prediction_signature = (\r\n          tf.saved_model.signature_def_utils.build_signature_def(\r\n              inputs={'images': predict_inputs_tensor_info},\r\n              outputs={\r\n                  'classes': classes_output_tensor_info,\r\n                  'scores': scores_output_tensor_info\r\n              },\r\n              method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\r\n          ))\r\n\r\n      legacy_init_op = tf.group(\r\n          tf.tables_initializer(), name='legacy_init_op')\r\n      builder.add_meta_graph_and_variables(\r\n          sess, [tf.saved_model.tag_constants.SERVING],\r\n          signature_def_map={\r\n              'predict_images':\r\n                  prediction_signature,\r\n              tf.saved_model.signature_constants.\r\n              DEFAULT_SERVING_SIGNATURE_DEF_KEY:\r\n                  classification_signature,\r\n          },\r\n          legacy_init_op=legacy_init_op)\r\n\r\n      builder.save()\r\n      print 'Successfully exported model to %s' % FLAGS.output_dir\r\n\r\n\r\ndef preprocess_image(image_buffer):\r\n  \"\"\"Preprocess JPEG encoded bytes to 3D float Tensor.\"\"\"\r\n\r\n  # Decode the string as an RGB JPEG.\r\n  # Note that the resulting image contains an unknown height and width\r\n  # that is set dynamically by decode_jpeg. In other words, the height\r\n  # and width of image is unknown at compile-time.\r\n  image = tf.image.decode_jpeg(image_buffer, channels=3)\r\n  # image = vgg._aspect_preserving_resize(image, vgg._RESIZE_SIDE_MAX)\r\n  image = vgg._aspect_preserving_resize(image, vgg._RESIZE_SIDE_MIN)\r\n  image = vgg._central_crop([image], FLAGS.image_size, FLAGS.image_size)[0]\r\n  image.set_shape([FLAGS.image_size, FLAGS.image_size, 3])\r\n  image = tf.to_float(image)\r\n  image = vgg._mean_image_subtraction(image, [vgg._R_MEAN, vgg._G_MEAN, vgg._B_MEAN])\r\n  return image\r\n\r\n\r\ndef main(unused_argv=None):\r\n  export()\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n\r\n```\r\n\r\nthe conversion command:\r\n\r\n```\r\npython /usr/lib/python2.7/site-packages/tensorflow/python/tools/saved_model_cli.py convert --dir /INPUTPATH/ --output_dir /OUTPATH/ --tag_set serve  tensorrt --max_workspace_size_bytes 1073741824 --max_batch_size 224 --precision_mode FP32 --is_dynamic_op True  --minimum_segment_size 10\r\n```\r\n\r\n**Other info / logs**\r\n\r\n$ tensorflow_model_server --port=8413 --rest_api_port=8414 --model_name=resnet --model_base_path=/workdir/tmp/trttest_pb6/  \r\n2019-03-14 23:14:17.626533: I tensorflow_serving/model_servers/server.cc:82] Building single TensorFlow model file config:  model_name: resnet model_base_path: /workdir/tmp/trttest_pb6/\r\n2019-03-14 23:14:17.626858: I tensorflow_serving/model_servers/server_core.cc:461] Adding/updating models.\r\n2019-03-14 23:14:17.626876: I tensorflow_serving/model_servers/server_core.cc:558]  (Re-)adding model: resnet\r\n2019-03-14 23:14:17.727170: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: resnet version: 1}\r\n2019-03-14 23:14:17.727195: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: resnet version: 1}\r\n2019-03-14 23:14:17.727208: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: resnet version: 1}\r\n2019-03-14 23:14:17.727228: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /workdir/tmp/trttest_pb6/1\r\n2019-03-14 23:14:17.727242: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /workdir/tmp/trttest_pb6/1\r\n2019-03-14 23:14:17.878579: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2019-03-14 23:14:18.977544: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: name: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38 pciBusID: 0000:05:00.0 totalMemory: 15.78GiB freeMemory: 15.36GiB\r\n2019-03-14 23:14:18.977591: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-03-14 23:14:19.762198: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-14 23:14:19.762240: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0\r\n2019-03-14 23:14:19.762248: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N\r\n2019-03-14 23:14:19.762739: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14843 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:05:00.0, compute capability: 7.0)\r\n2019-03-14 23:14:20.050500: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:182] Restoring SavedModel bundle.\r\n2019-03-14 23:14:20.050589: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:192] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: /workdir/tmp/trttest_pb6/1/variables/variables.index\r\n2019-03-14 23:14:20.050607: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:285] SavedModel load for tags { serve }; Status: success. Took 2323360 microseconds.\r\n2019-03-14 23:14:20.050644: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:101] No warmup data file found at /workdir/tmp/trttest_pb6/1/assets.extra/tf_serving_warmup_requests\r\n2019-03-14 23:14:20.050759: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: resnet version: 1}\r\n2019-03-14 23:14:20.081590: I tensorflow_serving/model_servers/server.cc:313] Running gRPC ModelServer at 0.0.0.0:8413 ...\r\n[warn] getaddrinfo: address family for nodename not supported\r\n2019-03-14 23:14:20.090390: I tensorflow_serving/model_servers/server.cc:333] Exporting HTTP/REST API at:localhost:8414 ...\r\n[evhttp_server.cc : 237] RAW: Entering the event loop ...\r\n2019-03-14 23:14:29.297322: I external/org_tensorflow/tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:496] Building a new TensorRT engine for map/while/TRTEngineOp_1 with batch size 224\r\n2019-03-14 23:14:29.541322: W external/org_tensorflow/tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-03-14 23:14:29.541399: W external/org_tensorflow/tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-03-14 23:14:29.541421: W external/org_tensorflow/tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-03-14 23:14:31.924073: I external/org_tensorflow/tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:496] Building a new TensorRT engine for TRTEngineOp_0 with batch size 224\r\n2019-03-14 23:14:47.935236: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at lookup_table_op.cc:809 : Failed precondition: Table not initialized.\r\n\r\n**My Solution**\r\n\r\nI have fixed this bug in my way, and it works in my case, but I'm not sure if it is the correct way to do so. Also, I'd like to contribute to tensorflow, but there are so many versions and branches of tensorflow, which one should I send a Pull-Request to?\r\n\r\nThe Patch: https://github.com/monklof/tensorflow/pull/1/files\r\n\r\nThanks for checking this issue, I'm looking forward to hearing from you soon.\r\n", "comments": ["Hi @smit-hinsu, would you please help to take a look? Thanks", "Thanks for the detailed report and also proposing a solution!\r\n\r\nPull requests should be sent to the `master` branch. You can refer to guidelines  for contributing code to TensorFlow at https://www.tensorflow.org/community/contribute/code. Let us know if you have any other questions!", "@smit-hinsu thanks for the review, I'll update the Pull request later.", "@smit-hinsu Hi, I've updated the pull request.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26800\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26800\">No</a>\n"]}, {"number": 26799, "title": "[TF 2.0] Logs from model.fit is not conform to the documentation for Molde with multiple outputs", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview\r\n\r\n**Describe the current behavior**\r\n\r\nI have a model with multiple outputs. Precisely, an object detector which outputs locations and classes of object. The model is compiled as follow:\r\n\r\n```\r\nssd.compile(optimizer=optimizer, \r\n            loss={'output_1': loc_loss, 'output_2': class_loss},\r\n            loss_weights=[1., 1.]\r\n           )\r\n```\r\n\r\nFrom the documentation, the loss to be optimized should be `loss = loc_loss + class_loss` but\r\nfit outputs the following logs to the console:\r\n\r\n```\r\nEpoch 1/5\r\n  37/1000 [>.............................] - ETA: 23:46 - loss: 176724.1514 - output_1_loss: 0.0212 - output_2_loss: 0.0449\r\n```\r\n\r\nClearly, the value of `loss` is not the sum of the two losses.\r\n\r\nIs it a real bug or a misunderstanding on the meaning of `output_1_loss` and `output_2_loss` ?\r\n", "comments": ["After some investigation, I think the problem is that I don't use the same reduction in my loss ( a custom loss with Reduction.SUM) and the loss metrics which use `SumOverBatchSize` :\r\n\r\n```\r\n    if len(self.outputs) > 1:\r\n      self._output_loss_metrics = [\r\n          metrics_module.SumOverBatchSize() if hasattr(loss_fn, 'reduction')\r\n          else metrics_module.SumOverBatchSizeMetricWrapper(loss_fn)\r\n          for loss_fn in self.loss_functions\r\n      ]\r\n```\r\n\r\nDoes it make sense to have the ability to set a custom metric here ?", "@jrabary I am closing the issue as it was resolved. Check this [resource](https://github.com/tensorflow/tensorflow/issues/30711) for custom_metric implementation. Thanks!"]}, {"number": 26798, "title": "Add scalar test for neg op", "body": "Add scalar test for neg op", "comments": ["@siju-samuel can you please re-base your branch to resolve conflicts.", "@karimnosseir @rthadur I have rebased, please check and approve again. Thanks", "Changes are merged by this commit ` 0ec5db4` this is a duplicate [PR](https://github.com/tensorflow/tensorflow/pull/27018) , thank you "]}, {"number": 26797, "title": "dilated convolution leads to incorrect graph optimization", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):archlinux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):b'v1.13.1-0-g6612da8' 1.13.1\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version: 10.0 / 7.5.0\r\n- GPU model and memory: GTX 1080Ti\r\n\r\nThe following code:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = tf.placeholder(name='input', shape=[1, 1024, None, None], dtype=tf.float32)\r\n\r\nhidden = tf.layers.Conv2D(256, 3, activation=tf.nn.relu, dilation_rate=2, data_format='channels_first', padding='SAME')(x)\r\n\r\nlabel_logits = tf.layers.Conv2D(15, 1, data_format='channels_first', padding='SAME')(hidden)\r\nshp1 = tf.shape(label_logits)\r\n\r\nlabel_logits = tf.squeeze(label_logits, 0)\r\nshp2 = tf.shape(label_logits)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nwith sess.as_default():\r\n    v = np.random.rand(1, 1024, 200, 19)\r\n    print(sess.run([shp1, shp2], feed_dict={x: v}))\r\n```\r\nruns on GPU, and prints:\r\n```\r\n[array([  1,  15, 200,  19], dtype=int32), array([ 15, 256, 256], dtype=int32)]\r\n```\r\n\r\nThe shape of tensor was changed by a `squeeze`. This is terribly wrong.\r\n\r\nThe issue was originally reported at https://github.com/tensorpack/tensorpack/issues/1110\r\n\r\n![0316-23:43:26](https://user-images.githubusercontent.com/1381301/54486423-57883b80-4845-11e9-8137-c48c20aec743.png)\r\n", "comments": ["The bug only exists in `tf.layers` & `tf.keras.layers`. It does not exist if I call the op `tf.nn.conv2d(dilations=)` directly.\r\nIt turns out that `tf.layers.Conv2D(dilations=2)` actually will do some space-to-batch/batch-to-space conversion, and call `tf.nn.conv2d(dilations=1)` instead of dilations=2.\r\nThis logic is implemented in this commit: https://github.com/tensorflow/tensorflow/commit/c0502aff716a6b7889c5eb23cd06b5bda414bf9e . And something is probably wrong there.", "Disabling the graph rewriter by `config.graph_options.rewrite_options.disable_meta_optimizer = True` can make the bug disappear.\r\nI can see from logs that the last shape op was mistakenly rewritten to a constant op.", "This seem to be related to grappler, which I'm not familiar with.", "The bug still exists in TF 1.15rc0.", "I can confirm that #32432 fixes the issue when using cuDNN and the keras API (you'll have to set the argument `fused=True`)\r\n\r\n@houtoms FYI", "Since the above commit was closed, here are some information for those who may have interest in fixing this bug.\r\n\r\nThe bug is mainly caused by the static shape inference in constant folding pass of grappler. You can close this pass instead of the whole meta optimizer to avoid it. Also, it is weird that changing `data_format` to `\"channels_last\"` or set `use_bias=False` will both make the bug disappear. From the log I dumped, the bug should be around the shape inference of `BatchToSpaceND`. However, the shape of this op relies on the input value, so I'm not sure the bug is the implementation of the shape inference function of `BatchToSpaceND` or the process of creating its input...", "I ran the exact same code in TF2.2, built with `--config=v1` and run with `TF2_BEHAVIOR=0`. The bug still exists.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26797\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26797\">No</a>\n"]}, {"number": 26796, "title": "Improve performance of clip_by_value when the input is IndexedSlices", "body": "Currently, the tf.clip_by_value converts the input to dense tensor and do the clipping after that. However, the gradients may be IndexedSlices when using tf.nn.embedding_lookup. It may consume large memory and make performance worse.\r\nIn this pull request, the clipping function will deal with IndexedSlices directly instead of converting to dense tensor.", "comments": ["@jhseu @vrv \r\nCould please take a look at this patch? It is already integrated into our internal prod environment and works fine. We think it also beneficial for the community so we made this PR.\r\n\r\nThanks\r\n\r\n "]}, {"number": 26795, "title": "Replaced get_shape() to shape.", "body": "This is the recommended method.", "comments": []}, {"number": 26794, "title": "1.13 ImportError when importing tf.contrib", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory:  GeForce 940MX, 4GB\r\n\r\n**Describe the current behavior**\r\nWhen I import tf.contrib I get the following Error:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 40, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/contrib/distribute/__init__.py\", line 33, in <module>\r\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\r\n  File \"/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 27, in <module>\r\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n  File \"/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/contrib/tpu/__init__.py\", line 73, in <module>\r\n    from tensorflow.contrib.tpu.python.tpu.keras_support import tpu_model as keras_to_tpu_model\r\n  File \"/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\", line 62, in <module>\r\n    from tensorflow.contrib.tpu.python.tpu import tpu\r\n  File \"/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 24, in <module>\r\n    from tensorflow.contrib.compiler import xla\r\n  File \"/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/contrib/compiler/xla.py\", line 28, in <module>\r\n    from tensorflow.python.estimator import model_fn as model_fn_lib\r\n  File \"/home/julian/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/estimator/model_fn.py\", line 26, in <module>\r\n    from tensorflow_estimator.python.estimator import model_fn\r\nImportError: cannot import name 'model_fn'\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n`python -c \"import tensorflow.contrib\"`\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Problem disappeared after reinstallling tensorflow-estimator!", "@jvdgoltz This doesn't seem to work for me, when I reinstall tensorflow-estimator-1.13.0"]}, {"number": 26793, "title": "@tf.function doesn't compile functions specified as parameters to other functions", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab.\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.6.7 (default, Oct 22 2018, 11:32:17) \\n[GCC 8.2.0]\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: ??\r\n- GPU model and memory: ??\r\n\r\n**Describe the current behavior**\r\n\r\nCode annotated with `@tf.function` behaves differently depending on whether functions are called and captured as variables, or called and passed directly to other functions. \r\n\r\nIn particular, it seems the autograph magic doesn't get applied to functions that are only called within the parameter list of other functions.\r\n\r\nI may be misinterpreting what exactly is going wrong, but certainly the behavior shown in the colab below is incorrect.\r\n\r\n**Describe the expected behavior**\r\n\r\nCapturing via an intermediate variable should never change code behavior.\r\n\r\n**Code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1CcWfHnGkFehUN8LYsbODf_fNSsQncE0G\r\n", "comments": ["Thank you for the nice repro! Yes, that is definitely a bug.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26793\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26793\">No</a>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26793\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26793\">No</a>\n"]}, {"number": 26792, "title": "Tensorflow Keras not adding trainable variable to Model in TF 1.13 (works in TF 1.12)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.6 (also tried 3.7)\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: GeForce 840M 8gigs ram\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI am trying to add a trainable variable y to my model. The following code adds a trainable variable y to my model in TF 1.12. It does not work in TF 1.13\r\n\r\n    y = K.variable([0.0], dtype=tf.float32, name='y')\r\n    add_y = Lambda(lambda x: tf.math.add(x,y))\r\n    add_y.trainable_weights.append(y)\r\n\r\n**Describe the expected behavior**\r\nModel should be updated with the variable y as a trainable weight\r\n\r\n**Code to reproduce the issue**\r\n\r\n    import tensorflow as tf\r\n    import tensorflow.keras.backend as K\r\n    from tensorflow.keras import Model\r\n    from tensorflow.keras.layers import Input, Lambda\r\n    inputs = Input(shape=(1,))\r\n    y = K.variable([0.0], dtype=tf.float32, name='x')\r\n    add_y = Lambda(lambda x: tf.math.add(x,y))\r\n    add_y.trainable_weights.append(y)\r\n    outputs = add_y(inputs)\r\n    model = Model(inputs=inputs, outputs=outputs)\r\n    model.summary()\r\n\r\nModel.summary() shows no trainable variables\r\n**Other info / logs**\r\nNONE\r\n", "comments": ["Hi @ablanch5, that's bc in TF1.13 `.trainable_weights` is a computed property. \r\n\r\nI have a few ideas of what you could do here. The easiest is that, as of the latest nightly (but not as of TF1.13), TF Keras Lambda Layers can actually include variables, so you could do:\r\n\r\n```\r\nadd_y = Lambda(lambda x: tf.math.add(x,K.variable([0.0], dtype=tf.float32, name='x')))\r\n```\r\n\r\nThe most robust way to do this though (and the way that would work in all versions of Keras) is to create your own subclass Layer:\r\n\r\n```\r\nclass Bias(tf.keras.layers.layer):\r\n  def build(self, input_shape):\r\n    self.bias = self.add_weight(shape=(), initializer='zeros', dtype=tf.float32, name='x')\r\n\r\n  def call(self, inputs):\r\n    return inputs + self.bias\r\n```\r\n", "Thanks so much for your answer\n\nOn Mon, Mar 18, 2019, 2:04 PM omalleyt12 <notifications@github.com> wrote:\n\n> Closed #26792 <https://github.com/tensorflow/tensorflow/issues/26792>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26792#event-2211263648>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AQe-tNpTQ95dht5OoBRKifIFIg-zElaVks5vX9U-gaJpZM4b38zy>\n> .\n>\n", "This is the only way I could find to get a single variable into a keras model in tensorflow 2.0. Seems a bit ridiculous, is there a better way than including a tensor only to multiple it by zero? What if we wanted to output something that was not aligned with batch size?\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\n\r\nclass Bias(keras.layers.Layer):\r\n    def __init__(self, output_dim=1, **kwargs):\r\n        self.output_dim = output_dim\r\n        super().__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        self.bias = self.add_weight(shape=(1,), initializer='zeros', dtype=tf.float32, name='x')\r\n        super().build(input_shape)\r\n\r\n    def call(self, x):\r\n        temp = tf.reduce_mean(x, axis=-1, keepdims=True)\r\n        return temp * 0 + self.bias\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return (input_shape[0], self.output_dim)\r\n\r\nx_input = keras.layers.Input(shape=(2,))\r\nV = Bias()(x_input)\r\nmodel = keras.models.Model(inputs=x_input, outputs=V)\r\nmodel.compile(loss=keras.losses.MeanSquaredError(), optimizer=keras.optimizers.Adam())\r\n\r\nX = np.random.randn(100,2)\r\ny = np.random.randn(100, 1) + 1.2\r\nprint(model.predict(X).squeeze())\r\nmodel.fit(X, y=y, epochs=10)\r\nprint(model.predict(X).squeeze())\r\n```", "@cottrell i.e. you want a Variable that is unconnected to the inputs of your Model? I would suggest using the subclassed API in that case. Note that in the case above, the inputs of your Model are completely ignored "]}, {"number": 26791, "title": "Tensorflow Keras get_trainable_weights.append not working in TF 1.13", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.6 (also tried 3.7)\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: GeForce 840M 8gigs ram\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI am trying to add a trainable variable y to my model. The following code adds a trainable variable y to my model in TF 1.12. It does not work in TF 1.13\r\nCurrently the operation \r\n    y = K.variable([0.0], dtype=tf.float32, name='y')\r\n    add_y = Lambda(lambda x: tf.math.add(x,y))\r\n    add_y.trainable_weights.append(y)\r\n\r\n**Describe the expected behavior**\r\nModel should be updated with the variable y as a trainable weight\r\n\r\n**Code to reproduce the issue**\r\n    import tensorflow as tf\r\n    import tensorflow.keras.backend as K\r\n    from tensorflow.keras import Model\r\n    from tensorflow.keras.layers import Input, Lambda\r\n    inputs = Input(shape=(1,))\r\n    y = K.variable([0.0], dtype=tf.float32, name='x')\r\n    add_y = Lambda(lambda x: tf.math.add(x,y))\r\n    add_y.trainable_weights.append(y)\r\n    outputs = add_y(inputs)\r\n    model = Model(inputs=inputs, outputs=outputs)\r\n    model.summary()\r\n\r\nModel.summary() shows no trainable variables\r\n**Other info / logs**\r\nNONE\r\n", "comments": []}, {"number": 26790, "title": "Failed to load native Tensorflow Runtime - Nvidia MX150 Python 3.7", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 64bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install through anaconda prompt\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?: pip install \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia GeForce MX150\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nInstalled Tensorflow, CUDA, CUDNN, tried to import tensorflow as tf, got the follow message.\r\n```\r\n(tensorflow) C:\\Users\\Shaurya>python\r\nPython 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Shaurya\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Shaurya\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Shaurya\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Shaurya\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Shaurya\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Shaurya\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Shaurya\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Shaurya\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Shaurya\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Shaurya\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Shaurya\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Shaurya\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Shaurya\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n**Any other info / logs**\r\nHaven't installed Keras yet. I activated tensorflow, which happened and then tried the same (as shown in the result above, but that didn't help either.\r\n\r\nI tried this before installing CUDNN and then after installing it as well, again to no avail.", "comments": ["TF 1.13.1 comes with pre built cuda 10.0 binaries. So can you please downgrade to cuda 10.0 and make sure to add cuda path as well. You can download [cuda 10.0 toolkit](https://developer.nvidia.com/cuda-toolkit-archive) here. If you want to use cuda 10.1 then you have to build TF from sources yourself. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26790\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26790\">No</a>\n"]}, {"number": 26789, "title": "[TF2.0] Optimizer for Linear models", "body": "\r\n**System information**\r\n- TensorFlow version: 2.0.0-alpha0\r\n- Doc Link: \r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/LinearEstimator\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/LinearClassifier\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/LinearRegressor\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nThe examples in these documents use tf.train.FtrlOptimizer which is not available in TF2.0.\r\n```python\r\n# Or estimator using the FTRL optimizer with regularization.\r\nestimator = LinearRegressor(\r\n    feature_columns=[categorical_column_a,\r\n                     categorical_feature_a_x_categorical_feature_b],\r\n    optimizer=tf.train.FtrlOptimizer(\r\n      learning_rate=0.1,\r\n      l1_regularization_strength=0.001\r\n    ))\r\n```\r\n\r\nI tried to use tf.optimizers.Ftrl instead, but it gives the following error:\r\n\r\n```python\r\n# Or estimator using the FTRL optimizer with regularization.\r\nestimator = LinearRegressor(\r\n    feature_columns=[categorical_column_a,\r\n                     categorical_feature_a_x_categorical_feature_b],\r\n    optimizer=tf.optimizers.Ftrl(\r\n      learning_rate=0.1,\r\n      l1_regularization_strength=0.001\r\n    ))\r\n\r\nValueError: The given object is not an Optimizer instance. Given: <tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object\r\nat 0x1293a32b0>\r\n```\r\n\r\nI think the documentation should be updated, but I am not sure the correct way of defining optimizer. Could please someone take a look at this?\r\n\r\nThanks,", "comments": ["@siinn `tf.optimizers.Ftrl` or `tf.optimizers.Ftrl` should be the correct optimizer to use with LinearRegressor in TF 2.0. \r\nThe error `ValueError: The given object is not an Optimizer instance. Given: <tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object\r\nat 0x1293a32b0>` indicates you are using LinearRegressorV1. Could you verify LinearRegressorV2 is indeed called? \r\n\r\nThe docstrings have been updated. Thanks."]}, {"number": 26788, "title": "[TF 2.0] Keras in graph mode crashes when fitting data with L2 regularizer set to 0.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Debian Stable**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **CPU, both TF-2.0.0a0 and tf-nightly-2.0-preview-2.0.0.dev20190315**\r\n- Python version: **3.5.3**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n\r\n**Describe the current behavior**\r\nThe below code (using zero L2 regularization) fails with\r\n```\r\nAttributeError: Tensor.graph is meaningless when eager execution is enabled.\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe code runs without a failure.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninputs = np.arange(10)\r\noutputs = 2 * inputs\r\n\r\nl2 = tf.keras.regularizers.L1L2(l2=0.0)\r\nmodel = tf.keras.Sequential(\r\n    [tf.keras.layers.Dense(1, input_shape=[1], kernel_regularizer=l2)]\r\n)\r\nmodel.compile(\r\n    optimizer=tf.keras.optimizers.Adam(),\r\n    loss=tf.keras.losses.MeanSquaredError(),\r\n    metrics=[tf.keras.metrics.MeanSquaredError()]\r\n)\r\nmodel.fit(inputs, outputs)\r\n```\r\n\r\n**Other info / logs**\r\n- Running with `model.run_eagerly` works.\r\n- Running with non-zero L2 regularization works.\r\n\r\nThe problem is in https://github.com/tensorflow/tensorflow/blob/b57c7d71eff5914a503d15130cb90a240b3bcf40/tensorflow/python/keras/engine/network.py#L651\r\nwhere a `ops.get_default_graph()` is called.\r\n\r\nThe problem is probably connected to the fact that L1L2 regularizer returns an explicit 0 (`K.constant(0.)`) when no l1/l2 is set (while otherwise summing weights and applying l1/l2).", "comments": ["Thanks for the bug! I was able to reproduce, looking into now", "I could not reproduce the issue with `!pip install tf-nightly-gpu-2.0-preview==2.0.0.dev20190808`. Please check the gist [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/5b56a644fd9e48c4cdde180460950b66/tf26788.ipynb).\r\n\r\nAutomatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26788\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26788\">No</a>\n"]}, {"number": 26787, "title": "Merge pull request #2 from tensorflow/master", "body": "update", "comments": ["Sorry, I was trying to do something else."]}, {"number": 26786, "title": "[tflite][java] Interpreter.reset_all_variables() is not supported on Android.", "body": "`Interpreter` does not support `reset_all_variables()` in the current Android API of TF Lite.\r\n\r\nThis function is critical for RNNs because without it all the batches after the first one produce wrong results.\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- Mobile device: Pixel 3 XL\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.12.0-9708-gafab5b3 1.14.1-dev20190306\r\n- Python version: Python 3.7.1", "comments": ["@MiloslavPalaxo Can you please fill the template. Thanks :)", "Done. Sorry for that. ", "We see that you are using old version of tensorflow which is out of support window, We recommend that you upgrade to 2.6.0 and let us know if the issue still persists in newer versions. Please open a new issue in case you face any errors, Hence moving this to closed status.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26786\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26786\">No</a>\n"]}, {"number": 26785, "title": "[Bug fix for DistributionStrategy] Fix bug of not respecting outer variable scope when using `get_variable` to create variable in merge_call because of thread local storage", "body": "Hi @yuefengz, \r\nThis bug was discovered in restoring model which was saved during training when using `ParameterServerStrategy`. We use [this function](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/optimizers.py#L162) to wrap `AdamOptimizer` when building our model. Its peculiarity is that it has a `variable_scope` before calling `apply_gradients`. So, the auxiliary `slot_variable` should respect the outer `variable_scope`. However, this `variable_scope` was missing after calling `merge_call` because of thread local storage which leads to restore failed when not using `DistributionStrategy`.\r\nI have fix this bug and add unit test. Please take a look. Thanks.", "comments": []}, {"number": 26784, "title": "[TF 2.0] Respect masking in Keras loss reduction (i.e., support an equivalent of default TF 1.0 loss reduction SUM_OVER_NONZERO_WEIGHTS)", "body": "**System information**\r\n- TensorFlow version (you are using): **TF 2.0.0a0**\r\n- Are you willing to contribute it (Yes/No): **Yes**\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nIn TF 1.0, default loss reduction was `SUM_OVER_NONZERO_WEIGHTS`. For NLP with sequences on inputs, it normalized losses by number of _valid elements_ (i.e., sum of non-padding words in input sentences).\r\n\r\nWith Keras API, default reduction `SUM_OVER_BATCH_SIZE` does not respect masking, so if a batch of sequences is passed on input, it is normalized by total batch size including padding (masked) elements. No reduction in Keras is available which would recover the previous TF 1.0 default `SUM_OVER_NONZERO_WEIGHTS` reduction.\r\n\r\n**Will this change the current api? How?**\r\n\r\nMy proposal is to respect masking in the `SUM_OVER_BATCH_SIZE` reduction. If a mask is set, then `BATCH_SIZE` should probably correspond to the number of unmasked elements anyway.\r\n\r\nAlternatively, a new reduction `SUM_OVER_MASKED_BATCH_SIZE` could be added.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nI think anyone using masked losses, especially if _upgrading from TF 1_.", "comments": ["@foxik Could you please confirm if you are still facing the issue.", "Yes. TF 2.1.0 still evaluates\r\n```python\r\nmse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\r\nmse([[0.], [0.], [0.]], [[1.], [1.], [1.]], sample_weight=[1, 0, 0])\r\n```\r\nto `0.333...` instead of to `1`.\r\n\r\nAt this point I am not proposing to change behaviour of `SUM_OVER_BATCH_SIZE`, which is already part of API guarantee -- but I propose to add\r\n```python\r\ntf.keras.losses.Reduction.SUM_OVER_NONZERO_WEIGHTS\r\n```\r\nwhich would compute the reduction by dividing by the number of non-zero weights, exactly what the default of TF 1 was.", "@foxik would you like to send us a PR with this change?", "@pavithrasv Well, for a single-CPU/single-GPU training, the change is trivial. But for a non-trivial distribution strategy it will take some thinking how to implement it correctly. Also there seems to be several places in Keras which manually handle SUM_OVER_BATCH_SIZE, which will probably have to be adapted -- overall I do not feel confident I can put all the pieces together.", "Closing the issue. With `train_step` one can override it and transform loss calls on higher-dimensional data (e.g., 2D loss in case of sequences as batch examples) to calls on one-dimensional data (e.g., concatenating valid words only).\r\n\r\nFurthermore, with RaggedTensors, padding values might be handled directly, instead of passing masks as weihts."]}, {"number": 26783, "title": "tf.Tensor documentation in TF2", "body": "**System information**\r\n- TensorFlow version: 2.0 alpha\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Tensor\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nThe documentation of `tf.Tensor` in the 2.0 section is still about the `tf.Tensor` as a symbolic tensor that contains the result of a  `tf.Operation` and it states that the only way to get its value is to use a `tf.Session` to run the node.\r\n\r\nThis is no more the truth in tf2:\r\n\r\n- `tf.Session` is no more available\r\n- `tf.Tensor` holds the value of the computation and we can extract it using `.numpy()`.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n\r\nYes, I can submit a PR if needed - in the PR I can remove any reference to `tf.Session` and replace the introduction, explaining what is a `tf.Tensor` in Tensorflow 2.0.", "comments": ["I just want to confirm that whether  we have to remove tf.session() from class Tensor description or from complete ops.py wherever it is ,eigther in description or in function.   ", "This is updated in nightly.  Thanks!"]}]