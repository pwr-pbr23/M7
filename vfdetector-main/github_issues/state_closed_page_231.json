[{"number": 47624, "title": "keras.callbacks.EarlyStopping fails with RuntimeError: basic_string::_S_construct null not valid.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS.\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a.\r\n- TensorFlow installed from (source or binary): Binary, using `pip install tensorflow`.\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0.\r\n- Python version: Python 3.6.9.\r\n- Bazel version (if compiling from source): N/a.\r\n- GCC/Compiler version (if compiling from source): N/a.\r\n- CUDA/cuDNN version: N/a.\r\n- GPU model and memory: N/a.\r\n\r\n**Describe the current behavior**\r\n```\r\nTraceback (most recent call last):\r\n  File \"./tune.py\", line 304, in train2\r\n    keras.callbacks.EarlyStopping(monitor='Alpha', mode=\"max\", patience=1, min_delta=0.0001)\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 855, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2943, in __call__\r\n    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 592, in call\r\n    custom_gradient.copy_handle_data(func_graph_output, outputs[i])\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/handle_data_util.py\", line 53, in copy_handle_data\r\n    if (target_t.dtype == dtypes.resource or\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py\", line 203, in __eq__\r\n    return self._type_enum == other._type_enum  # pylint: disable=protected-access\r\nRuntimeError: basic_string::_S_construct null not valid\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo exception, failure is unacceptable.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe issue happens during hyperparameter tuning. It happened in 4 trials out of 100, all of them pass the same arguments to `keras.callbacks.EarlyStopping`.\r\n\r\n**Other info / logs** \r\nException message `basic_string::_S_construct null not valid` strongly suggests that `tensorflow` library passes a null pointer into `std::basic_string` constructor, which is a programming error in `tensorflow` code. \r\n\r\n", "comments": ["@max0x7ba,\r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset you are using. Thanks!", "Looks like the issue was caused by unstable memory.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47624\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47624\">No</a>\n"]}, {"number": 47623, "title": "Tensorflow doesn't seem to work on rtx2060", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:none\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): V2.4.0rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.8.0\r\n- Bazel version (if compiling from source):nonenone\r\n- GCC/Compiler version (if compiling from source):none\r\n- CUDA/cuDNN version : cuda_11.1.1_456.81/cuDNN v8.0.5\r\n- GPU model and memory:RTX2060\uff0c6G\r\n- CPU~~~~~~~~~~~~~:AMD 2600X  16G\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI try to use tensorflow to construct mtcnn + facenet to realize face recognition, and refer to this https://github.com/kpzhang93/MTCNN_face_detection_alignment\r\nWhen running mtcnn network, I found that the performance of rtx2060 was very poor. The occupancy rate reported by Windows system was only about 3%, and the occupancy rate queried by NVIDIA SMI was less than 7%.\r\nI thought it was my code problem. Until I ran the MNIST sample, I found that this problem might be universal on my computer\r\n\r\n**Describe the expected behavior**\r\nThe utilization rate of rtx2060 should be more than 60%. In the case of MNIST, every batch should be used_ The size is set to 256, which seems to only increase the pressure on the video memory.\r\n\r\nThrough NVIDIA SMI monitoring, the efficiency of rtx2060 is only about 4%\r\nand\r\nI added the running log of my dumnist below. According to the computing power of rtx2060, the time consumed by running this code should be reduced by more than half.\r\n\r\n**Standalone code to reproduce the issue**\r\nFor the recurrence of MNIST, I do as follows:\r\n```python\r\nimport tensorflow as tf\r\nimport sys\r\nimport os\r\nimport datetime\r\ngpu = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpu[0], True)\r\n\r\n\r\n\r\nstarttime = datetime.datetime.now()\r\n\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(128, activation=\"relu\"),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\r\n              metrics=['sparse_categorical_accuracy'])\r\n\r\n\r\ncheckpoint_save_path = \"./checkpoint/mnist/mnist.ckpt\"\r\nif os.path.exists(checkpoint_save_path + '.index'):\r\n    print('------------load the model-----------')\r\n    model.load_weights(checkpoint_save_path)\r\n\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\r\n    filepath=checkpoint_save_path,\r\n    save_weights_only=True,\r\n    save_best_only=True\r\n)\r\nhistory = model.fit(x_train, y_train, batch_size=128, epochs=20,\r\n                    validation_data=(x_test, y_test), validation_freq=1,\r\n                    callbacks=[cp_callback])\r\nmodel.summary()\r\nendtime = datetime.datetime.now()\r\nprint((endtime - starttime).seconds)\r\nsys.exit()`\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nI conducted 20 rounds of tests and the log is as follows:\r\n\r\nrunfile('E:/IDEA/pythonProject/mnist.py', wdir='E:/IDEA/pythonProject')\r\n2021-03-08 00:27:44.912688: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-08 00:27:46.503950: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-08 00:27:46.505289: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-03-08 00:27:46.541927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 2060 computeCapability: 7.5\r\ncoreClock: 1.68GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s\r\n2021-03-08 00:27:46.542220: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-08 00:27:46.569789: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-08 00:27:46.569959: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-08 00:27:46.575380: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-08 00:27:46.577802: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-08 00:27:46.588545: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-08 00:27:46.593147: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-08 00:27:46.594651: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-08 00:27:46.594860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-08 00:27:46.930131: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-08 00:27:46.931110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 2060 computeCapability: 7.5\r\ncoreClock: 1.68GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s\r\n2021-03-08 00:27:46.931392: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-08 00:27:46.931527: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-08 00:27:46.931660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-08 00:27:46.931788: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-08 00:27:46.931926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-08 00:27:46.932050: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-08 00:27:46.932189: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-08 00:27:46.932316: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-08 00:27:46.932480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-08 00:27:47.489887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-08 00:27:47.490098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-03-08 00:27:47.490232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-03-08 00:27:47.490534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4720 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:27:00.0, compute capability: 7.5)\r\n2021-03-08 00:27:47.491188: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n------------load the model-----------\r\n2021-03-08 00:27:47.600796: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\nEpoch 1/20\r\n2021-03-08 00:27:47.960346: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-08 00:27:48.494384: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9939 - val_loss: 0.0778 - val_sparse_categorical_accuracy: 0.9767\r\nEpoch 2/20\r\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9952 - val_loss: 0.0765 - val_sparse_categorical_accuracy: 0.9793\r\nEpoch 3/20\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9961 - val_loss: 0.0812 - val_sparse_categorical_accuracy: 0.9766\r\nEpoch 4/20\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0110 - sparse_categorical_accuracy: 0.9968 - val_loss: 0.0808 - val_sparse_categorical_accuracy: 0.9780\r\nEpoch 5/20\r\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0102 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.0850 - val_sparse_categorical_accuracy: 0.9777\r\nEpoch 6/20\r\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0092 - sparse_categorical_accuracy: 0.9971 - val_loss: 0.0846 - val_sparse_categorical_accuracy: 0.9782\r\nEpoch 7/20\r\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0081 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.1029 - val_sparse_categorical_accuracy: 0.9765\r\nEpoch 8/20\r\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0063 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0903 - val_sparse_categorical_accuracy: 0.9786\r\nEpoch 9/20\r\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0076 - sparse_categorical_accuracy: 0.9977 - val_loss: 0.0801 - val_sparse_categorical_accuracy: 0.9811\r\nEpoch 10/20\r\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0074 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0942 - val_sparse_categorical_accuracy: 0.9792\r\nEpoch 11/20\r\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0051 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.1002 - val_sparse_categorical_accuracy: 0.9788\r\nEpoch 12/20\r\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0048 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.1038 - val_sparse_categorical_accuracy: 0.9785\r\nEpoch 13/20\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.1138 - val_sparse_categorical_accuracy: 0.9776\r\nEpoch 14/20\r\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0063 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.1081 - val_sparse_categorical_accuracy: 0.9791\r\nEpoch 15/20\r\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0034 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.1001 - val_sparse_categorical_accuracy: 0.9803\r\nEpoch 16/20\r\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0022 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.1246 - val_sparse_categorical_accuracy: 0.9783\r\nEpoch 17/20\r\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.1079 - val_sparse_categorical_accuracy: 0.9804\r\nEpoch 18/20\r\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0020 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.1111 - val_sparse_categorical_accuracy: 0.9795\r\nEpoch 19/20\r\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0034 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.1124 - val_sparse_categorical_accuracy: 0.9784\r\nEpoch 20/20\r\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.1386 - val_sparse_categorical_accuracy: 0.9756\r\nModel: \"sequential\"\r\n\r\nLayer (type)                 Output Shape              Param    \r\n=================================================================\r\nflatten (Flatten)            (None, 784)               0         \r\n\r\ndense (Dense)                (None, 128)               100480    \r\n\r\ndense_1 (Dense)              (None, 10)                1290      \r\n=================================================================\r\nTotal params: 101,770\r\nTrainable params: 101,770\r\nNon-trainable params: 0\r\n\r\n94\r\n===========================LOGs END===========================\r\n```", "comments": ["I also have a sample log to provide:\r\nThis occurs in running cifar 10\r\nFor the recurrence of cifar10, I do as follows:\r\n```python\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\nfrom matplotlib import pyplot as plt\r\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Dropout, Flatten, Dense\r\nfrom tensorflow.keras import Model\r\n\r\nimport datetime\r\nimport time\r\nimport sys\r\nstarttime = datetime.datetime.now()\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\r\n\r\nnp.set_printoptions(threshold=np.inf)\r\n\r\ncifar10 = tf.keras.datasets.cifar10\r\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\ngpu = tf.compat.v1.ConfigProto()\r\ngpu.gpu_options.allow_growth = True\r\nsession = tf.compat.v1.InteractiveSession(config=gpu)\r\n\r\n\r\nclass Baseline(Model):\r\n    def __init__(self):\r\n        super(Baseline, self).__init__()\r\n        self.c1 = Conv2D(filters=6, kernel_size=(5, 5), padding=\"same\")\r\n        self.b1 = BatchNormalization()\r\n        self.a1 = Activation('relu')\r\n        self.p1 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\r\n        self.d1 = Dropout(0.2)\r\n\r\n        self.flatten = Flatten()\r\n        self.f1 = Dense(128, activation='relu')\r\n        self.d2 = Dropout(0.2)\r\n        self.f2 = Dense(10, activation='softmax')\r\n\r\n    def call(self, x):\r\n        x = self.c1(x)\r\n        x = self.b1(x)\r\n        x = self.b1(x)\r\n        x = self.a1(x)\r\n        x = self.p1(x)\r\n        x = self.d1(x)\r\n        x = self.flatten(x)\r\n        x = self.f1(x)\r\n        x = self.d2(x)\r\n        y = self.f2(x)\r\n        return y\r\n\r\n\r\nmodel = Baseline()\r\n\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\r\n              metrics=['sparse_categorical_accuracy'])\r\n\r\ncheckpoint_save_path = \"./checkpoint/Baseline.ckpt\"\r\nif os.path.exists(checkpoint_save_path + '.index'):\r\n    print('--------------load the mode as Baseline--------------')\r\n    model.load_weights(checkpoint_save_path)\r\n\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,\r\n                                                 save_weights_only=True,\r\n                                                 save_best_only=True)\r\n\r\nhistory = model.fit(x_train, y_train,\r\n                    batch_size=128, epochs=4000,\r\n                    validation_data=(x_test, y_test), validation_freq=1,\r\n                    callbacks=[cp_callback])\r\nmodel.summary()\r\nfile = open('./weights.txt', 'w')\r\nfor i in model.trainable_variables:\r\n    file.write(str(i.name) + '\\n')\r\n    file.write(str(i.shape) + '\\n')\r\n    file.write(str(i.numpy()) + '\\n')\r\nfile.close()\r\n\r\n# -------------- show    --------------#\r\n\r\nacc = history.history['sparse_categorical_accuracy']\r\nval_acc = history.history['val_sparse_categorical_accuracy']\r\nloss = history.history['loss']\r\nval_loss = history.history['val_loss']\r\nplt.subplot(1, 2, 1)\r\nplt.plot(acc, label='Training Accuracy')\r\nplt.plot(val_acc, label='Validation Accuracy')\r\nplt.title('Training and Validation Accuracy')\r\nplt.legend()\r\nplt.subplot(1, 2, 1)\r\nplt.plot(loss, label='Training Loss')\r\nplt.plot(val_loss, label='Validation Loss')\r\nplt.title('Training and Validation Loss')\r\nplt.legend()\r\nplt.show()\r\nendtime = datetime.datetime.now()\r\nprint((endtime - starttime).seconds)\r\n\r\ntime.sleep(30)\r\nsys.exit()\r\n\r\n\r\n\r\n*********************************LOGS START*********************************\r\nrunfile('E:/IDEA/pythonProject/Cifar10_Model.py', wdir='E:/IDEA/pythonProject')\r\n2021-03-08 01:22:55.988089: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-08 01:22:58.319151: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-08 01:22:58.321647: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-03-08 01:22:58.356526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 2060 computeCapability: 7.5\r\ncoreClock: 1.68GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s\r\n2021-03-08 01:22:58.356964: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-08 01:22:58.366034: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-08 01:22:58.366261: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-08 01:22:58.371026: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-08 01:22:58.373161: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-08 01:22:58.383581: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-08 01:22:58.387809: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-08 01:22:58.389278: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-08 01:22:58.389559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-08 01:22:58.959324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-08 01:22:58.959530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-03-08 01:22:58.959678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-03-08 01:22:58.960071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4720 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:27:00.0, compute capability: 7.5)\r\n2021-03-08 01:22:58.960919: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-08 01:22:58.964583: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-08 01:22:58.964863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 2060 computeCapability: 7.5\r\ncoreClock: 1.68GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s\r\n2021-03-08 01:22:58.965261: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-08 01:22:58.965476: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-08 01:22:58.965700: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-08 01:22:58.965871: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-08 01:22:58.966116: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-08 01:22:58.966320: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-08 01:22:58.966507: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-08 01:22:58.966694: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-08 01:22:58.966937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-08 01:22:58.967419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: GeForce RTX 2060 computeCapability: 7.5\r\ncoreClock: 1.68GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s\r\n2021-03-08 01:22:58.967837: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-08 01:22:58.968044: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-08 01:22:58.968261: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-08 01:22:58.968468: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-08 01:22:58.968665: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-08 01:22:58.968876: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-08 01:22:58.969097: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-08 01:22:58.969326: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-08 01:22:58.969546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-08 01:22:58.969759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-08 01:22:58.969905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-03-08 01:22:58.969996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-03-08 01:22:58.970168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4720 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:27:00.0, compute capability: 7.5)\r\n2021-03-08 01:22:58.970540: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n--------------load the mode as Baseline--------------\r\n2021-03-08 01:22:59.294021: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\nEpoch 1/4000\r\n2021-03-08 01:22:59.933410: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-08 01:23:00.409513: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-08 01:23:00.416814: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-08 01:23:01.401150: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n2021-03-08 01:23:01.473301: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:01.474022: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code -1, output: \r\nRelying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n2021-03-08 01:23:01.549213: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:01.625852: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:01.700407: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:01.773802: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:01.851961: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:01.922854: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:01.995014: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:02.070523: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:02.149133: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:02.222853: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:02.296621: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:02.371945: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:02.441581: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:02.514017: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:02.593597: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:02.669747: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:02.748396: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:02.825343: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:02.897071: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:02.971146: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:03.049402: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:03.124899: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:03.202434: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:03.413842: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:03.487559: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:03.561893: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:03.638064: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:03.712285: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:03.782712: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:03.853222: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:03.927721: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:03.998730: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:04.073533: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:04.147819: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:04.222011: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:04.296984: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:04.369178: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:04.442802: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:04.517602: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:04.588613: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:04.658139: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:04.731727: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-03-08 01:23:04.808468: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n1556/1563 [============================>.] - ETA: 0s - loss: 0.8424 - sparse_categorical_accuracy: 0\r\n.........\r\n*************************************LOGS END*********************************************\r\n```\r\nI've come up with a new question when I'm editing the review. I need your advice, if you don't mind\uff1a\r\n\"Subprocess ended with return code: 4294967295\" reveals some errors that I can't explain. I mean, these errors happened. I only know this series of error codes, but I don't know where it happened.\r\n\r\nIn fact, when I run facenet and mtcnn, tensorflow only prompts this passage for a long time, including when I run this project https://github.com/kpzhang93/MTCNN_face_detection_alignment\r\n\r\nIn my opinion, the common feature of these problems is that the utilization rate of rtx2060 is very low (not only in Windows Task Manager, NVIDIA-SMI also gives the same conclusion, the error between the two is not more than 4%, which is certain, because the latter is 4% utilization rate, while the former is 0% QAQ)\r\n\r\n\r\nWhat should I do? Continue to try different combinations of CUDA, cudnn and tensorflow? Because of this, I have been broken down for a day QAQ", "I see you are using cuda 11.1 version can please try with cuda 11.0 as listed in [tested build config](https://www.tensorflow.org/install/source#gpu).\r\nThanks!", "> \u6211\u770b\u5230\u60a8\u4f7f\u7528\u7684\u662fcuda 11.1\u7248\u672c\uff0c\u8bf7\u5c1d\u8bd5\u4f7f\u7528cuda 11.0\uff08\u5982\u5df2[\u6d4b\u8bd5\u7684build config\u4e2d](https://www.tensorflow.org/install/source#gpu)\u6240\u5217\uff09\u3002\r\n> \u8c22\u8c22\uff01\r\n\r\nI've tried this combination before, and it doesn't change performance. With reference to your suggestion, I'd like to try cuda11.0 again.I hope it will work this time QAQ", "> \u6211\u770b\u5230\u60a8\u4f7f\u7528\u7684\u662fcuda 11.1\u7248\u672c\uff0c\u8bf7\u5c1d\u8bd5\u4f7f\u7528cuda 11.0\uff08\u5982\u5df2[\u6d4b\u8bd5\u7684build config\u4e2d](https://www.tensorflow.org/install/source#gpu)\u6240\u5217\uff09\u3002\r\n> \u8c22\u8c22\uff01\r\n\r\n\r\n![reaffirm](https://user-images.githubusercontent.com/71629843/110431711-1f8cf980-80e9-11eb-9ad7-e9ac5c0cf96c.png)\r\n![reaffirm2](https://user-images.githubusercontent.com/71629843/110431719-2156bd00-80e9-11eb-925c-a1052ed58e91.png)\r\n![reaffirm3](https://user-images.githubusercontent.com/71629843/110431722-21ef5380-80e9-11eb-8943-69528a17e346.png)\r\n\r\n", "> \u6211\u770b\u5230\u60a8\u4f7f\u7528\u7684\u662fcuda 11.1\u7248\u672c\uff0c\u8bf7\u5c1d\u8bd5\u4f7f\u7528cuda 11.0\uff08\u5982\u5df2[\u6d4b\u8bd5\u7684build config\u4e2d](https://www.tensorflow.org/install/source#gpu)\u6240\u5217\uff09\u3002\r\n> \u8c22\u8c22\uff01\r\n\r\n\r\n\r\n> I see you are using cuda 11.1 version can please try with cuda 11.0 as listed in [tested build config](https://www.tensorflow.org/install/source#gpu).\r\n> Thanks!\r\n\r\nI have re installed tensorflow and CUDA 11.0, and its conclusion seems to have no improvement. I don't know what to do", "Does adding `TF_DISABLE_RZ_CHECK=1` to the environment help?", "@GungnirASHTTTTT did you check by enabling the above flag?\r\nAlso, can you please try with recent TF versions (TF2.7 and `tf-nightly`) and let us know whether it is still an issue for you? \r\n[Here](https://www.tensorflow.org/install/source#gpu) are the tested build configs. \r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47622, "title": "Build TFLite Micro for RISC-V", "body": "@tensorflow/micro\r\n\r\nHow do I build TFLite Micro for RISC-V MCU's? I use:\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mcu_riscv TARGET_ARCH=riscv32_mcu hello_world_bin`\r\nbut the binary produced at **_/micro/tools/make/gen/mcu_riscv_riscv32_mcu_default/bin/_** seems to not have used the riscv-gcc toolchain as specified by _**mcu_riscv_makefile.inc**_. An x86-64 executable was built.\r\n", "comments": ["@ioannesKX \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "> @ioannesKX\r\n> We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04.1 LTS x86_64**\r\n- TensorFlow installed from (source or binary):  **Source**\r\n- Tensorflow version (commit SHA if source): **commit 2780b863ca82403b4c080042e9ac3e5772189881 (HEAD -> master, origin/master, origin/HEAD)**\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):  **some RISC-V**\r\n\r\n**Describe the problem**\r\nCannot compile TFLite Micro for RISC-V target. IDK if bug or if I'm following the wrong steps.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nAfter cloning, from the tensorflow directory:\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mcu_riscv TARGET_ARCH=riscv32_mcu hello_world_bin`\r\n\r\nI'd expect this command to compile TFLite Micro using the riscv64-unknown-elf toolchain (as specified in flags in **_/micro/tools/make/targets/mcu_riscv_makefile.inc_**) and produce a RISC-V executable under _**/micro/tools/make/gen/mcu_riscv_riscv32_mcu_default/bin**_ , but it actually produces a normal x86 one (executes normally on my host machine), meaning the flags were not passed properly or something similar.", "Can you try with `TARGET=risc32_mcu` and check?\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=riscv32_mcu hello_world`\r\nAlso see https://github.com/tensorflow/tensorflow/issues/32041", "> Can you try with `TARGET=risc32_mcu` and check?\r\n> `make -f tensorflow/lite/micro/tools/make/Makefile TARGET=riscv32_mcu hello_world`\r\n> Also see #32041\r\n\r\n```\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=riscv32_mcu hello_world\r\ntensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.\r\ntensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.\r\ntensorflow/lite/micro/tools/make/Makefile:565: tensorflow/lite/micro/tools/make/targets/riscv32_mcu_makefile.inc: No such file or directory\r\nmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/targets/riscv32_mcu_makefile.inc'.  Stop.\r\n```\r\nOn previous issues indeed `TARGET=riscv32_mcu` is used, but as this error implies, there should be a file `riscv32_mcu_makefile.inc` under **/targets** rather than the`mcu_riscv_makefile.inc` that exists now. If you search https://github.com/tensorflow/tensorflow/search?q=riscv32_mcu you can see that this is the TARGET expected from the rest of the code base.\r\n\r\nI suspected there was a mistake in the filename and renamed `mcu_riscv_makefile.inc` to `riscv32_mcu_makefile.inc`, now the `make` command you gave me seems to start working as intended: it downloads the SiFive gcc-riscv toolchain and starts using it to compile, but I reach another error:\r\n```\r\nmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/gen/riscv32_mcu_riscv32_micro/obj/tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/start.S', needed by 'tensorflow/lite/micro/tools/make/gen/riscv32_mcu_riscv32_micro/bin/hello_world'.  Stop.\r\n```\r\n\r\n[This seems to be the related PR.](https://github.com/tensorflow/tensorflow/commit/84b639d8582a0375db1b39edd6a50d39e21240ff) ", "Another thing, if `hello_world` is not included in the `make` flags, compilation is stuck in another stage:\r\n```\r\nIn file included from tensorflow/lite/micro/kernels/elu.cc:16:\r\n./tensorflow/lite/kernels/internal/reference/elu.h: In function 'void tflite::reference_ops::Elu(const tflite::RuntimeShape&, const float*, const tflite::RuntimeShape&, float*)':\r\n./tensorflow/lite/kernels/internal/reference/elu.h:31:40: error: 'expm1' is not a member of 'std'; did you mean 'exp'?\r\n   31 |     output_data[i] = val < 0.0f ? std::expm1(val) : val;\r\n      |                                        ^~~~~\r\n      |                                        exp\r\ntensorflow/lite/micro/kernels/elu.cc: In function 'void tflite::{anonymous}::PopulateLookupTable(const TfLiteTensor*, const TfLiteTensor*, tflite::{anonymous}::TransformFunc, tflite::{anonymous}::OpData*)':\r\ntensorflow/lite/micro/kernels/elu.cc:57:33: error: 'round' is not a member of 'std'; did you mean 'round'?\r\n   57 |     const float rescaled = std::round(transformed * inverse_scale);\r\n      |                                 ^~~~~\r\nIn file included from /opt/riscv/riscv64-unknown-elf/include/c++/10.2.0/cmath:45,\r\n                 from ./tensorflow/lite/kernels/internal/reference/elu.h:18,\r\n                 from tensorflow/lite/micro/kernels/elu.cc:16:\r\n/opt/riscv/riscv64-unknown-elf/include/math.h:308:15: note: 'round' declared here\r\n  308 | extern double round (double);\r\n      |               ^~~~~\r\nmake: *** [tensorflow/lite/micro/tools/make/Makefile:633: tensorflow/lite/micro/tools/make/gen/riscv32_mcu_riscv32_mcu_micro/obj/tensorflow/lite/micro/kernels/elu.o] Error 1\r\n```\r\n\r\nIDK if this is related or helps, but [in this (old) fork that ported TFLMicro to RISC-V](https://github.com/mars20/tensorflow/tree/riscv-mcu/tensorflow/lite/experimental/micro), the `round` function had some changes: https://github.com/mars20/tensorflow/commit/4fe29849f35cb1637f3a2dc8ea8162b1fef2ac7b#diff-d6578d7c90da9299018be4513de5fb5fcb7bdc8848df35222adb866e47eb82cb.", "I have pulled the above changes. They have fixed the issue with the round/expm functions, but a couple more have a similar problem. Result of same `make` command as above:\r\n\r\n```\r\nIn file included from ./tensorflow/lite/kernels/internal/reference/reduce.h:21,\r\n                 from tensorflow/lite/micro/kernels/reduce.cc:16:\r\n./tensorflow/lite/kernels/internal/max.h: In function 'T tflite::TfLiteMax(const T&, const T&)':\r\n./tensorflow/lite/kernels/internal/max.h:29:15: error: 'fmax' is not a member of 'std'; did you mean 'max'?\r\n   29 |   return std::fmax(x, y);\r\n      |               ^~~~\r\n      |               max\r\nIn file included from ./tensorflow/lite/kernels/internal/reference/reduce.h:22,\r\n                 from tensorflow/lite/micro/kernels/reduce.cc:16:\r\n./tensorflow/lite/kernels/internal/min.h: In function 'T tflite::TfLiteMin(const T&, const T&)':\r\n./tensorflow/lite/kernels/internal/min.h:29:15: error: 'fmin' is not a member of 'std'; did you mean 'min'?\r\n   29 |   return std::fmin(x, y);\r\n      |               ^~~~\r\n      |               min\r\nmake: *** [tensorflow/lite/micro/tools/make/Makefile:672: tensorflow/lite/micro/tools/make/gen/riscv32_mcu_riscv32_mcu_micro/obj/tensorflow/lite/micro/kernels/reduce.o] Error 1```", "> > Can you try with `TARGET=risc32_mcu` and check?\r\n> > `make -f tensorflow/lite/micro/tools/make/Makefile TARGET=riscv32_mcu hello_world`\r\n> > Also see #32041\r\n> \r\n> ```\r\n> $ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=riscv32_mcu hello_world\r\n> tensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.\r\n> tensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.\r\n> tensorflow/lite/micro/tools/make/Makefile:565: tensorflow/lite/micro/tools/make/targets/riscv32_mcu_makefile.inc: No such file or directory\r\n> make: *** No rule to make target 'tensorflow/lite/micro/tools/make/targets/riscv32_mcu_makefile.inc'.  Stop.\r\n> ```\r\n> \r\n> On previous issues indeed `TARGET=riscv32_mcu` is used, but as this error implies, there should be a file `riscv32_mcu_makefile.inc` under **/targets** rather than the`mcu_riscv_makefile.inc` that exists now. If you search https://github.com/tensorflow/tensorflow/search?q=riscv32_mcu you can see that this is the TARGET expected from the rest of the code base.\r\n> \r\n> I suspected there was a mistake in the filename and renamed `mcu_riscv_makefile.inc` to `riscv32_mcu_makefile.inc`, now the `make` command you gave me seems to start working as intended: it downloads the SiFive gcc-riscv toolchain and starts using it to compile, but I reach another error:\r\n> \r\n> ```\r\n> make: *** No rule to make target 'tensorflow/lite/micro/tools/make/gen/riscv32_mcu_riscv32_micro/obj/tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/start.S', needed by 'tensorflow/lite/micro/tools/make/gen/riscv32_mcu_riscv32_micro/bin/hello_world'.  Stop.\r\n> ```\r\n> \r\n> [This seems to be the related PR.](https://github.com/tensorflow/tensorflow/commit/84b639d8582a0375db1b39edd6a50d39e21240ff)\r\n\r\nI have the same issue with this. Has it been resolved yet?", "Managed to compile and run the examples on Spike so I think it's resolved now: https://github.com/tensorflow/tflite-micro/issues/311 @chuantunglin ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47622\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47622\">No</a>\n"]}, {"number": 47621, "title": "Fix Windows shared library build for CMake", "body": "MSVC build support with CMake", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47621) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Could you also provide cmake commands you used? Is there anything special?", "I use lite through add_subdirectory. There are not something special for cmake. "]}, {"number": 47620, "title": "tf.saved_model.load does not export SavedModels", "body": "I'm guessing it is supposed to say .save", "comments": []}, {"number": 47619, "title": "mobilenet_v3.preprocess_input does nothing!", "body": "## URL(s) with the issue:\r\n\r\n* The documentation: https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v3/preprocess_input\r\n* The source: https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/applications/mobilenet_v3.py#L556-L558\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThis function does nothing: it just returns its first argument. But the docs say it does multiple things: \"The preprocessed data are written over the input data\"; \"The inputs pixel values are scaled between -1 and 1\".\r\n\r\nThe source seems correct ... in the sense that MobileNetV3 appears to work correctly with inputs with \"3 color channels, with values in the range [0, 255]\".\r\n\r\nI'm guessing the docs here have just been copy-pasted from [the MobileNetV2 docs](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2/preprocess_input) ...?", "comments": ["You are right, the function doesn't do anything. However this is intentional.\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/mobilenet_v3.py#L559-L562 to know more.\r\nMobilenetV3 has a rescaling layer to preprocess the inputs.\r\nSee [gist](https://colab.research.google.com/gist/ymodak/897c6eaf44353e1062794f21ec147ff8/copy-of-untitled.ipynb)", "Ah yes. I suspected as much. I could update the docs", "Oh, I see the docs are generated from source and it's fixed on \"nightly\" view of the docs. I'll close this then. Thanks @ymodak ", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 47618, "title": "Can we add a check in Model.fit on dataset element_spec?", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI have this code\r\n```python\r\nimport tensorflow as tf\r\n\r\nlength = 500\r\nfeatures = list(range(length))\r\nlabels = tf.random.uniform([length], minval=0, maxval=2, dtype=tf.int32)\r\ndata = tf.transpose([range(length),\r\n\t\t\t\t\t tf.random.uniform([length], minval=0, maxval=2, dtype=tf.int32)])\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(data)\r\n\r\ndataset.shuffle(length)\r\n\r\ntrain_length = int(length / 5 * 4)\r\ntrain_data = dataset.take(train_length)\r\ntest_data = dataset.skip(train_length)\r\n\r\n\r\nassert isinstance(train_data.element_spec, tuple) and len(train_data.element_spec) > 0, \\\r\n\t'When x is dataset, its members must be a tuple of either (inputs, targets) or (inputs, targets, sample_weights). Currently your tuple size is 0.'\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(1, activation='relu'))\r\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\r\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'], run_eagerly=True)\r\nmodel.fit(train_data.batch(10), validation_data=test_data.batch(10), epochs=10)\r\n```\r\n\r\nIf we ignore the assert, running the code throws error \r\n\r\n> ValueError: No gradients provided for any variable: ['dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0'].\r\n\r\nThe error reason may not be obvious that train_data doesn't return required examples, ie. \"either (inputs, targets) or (inputs, targets, sample_weights).\"\r\n\r\n\r\nI hope the error message can be improved. Is it ok if we detect `len(train_data.element_spec)` in places like `tensorflow.python.keras.engine.data_adapter.DatasetAdapter._validate_args`? \r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nPeople writting buggy code, learning dataset, not give correct shape\r\n\r\n", "comments": ["Could reproduce the error with `Tensorflow Version 2.4`. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/bd1cd5555c30273efbe93f4f1b373db1/gh_47618.ipynb). \r\n\r\nThanks!", "Was able to reproduce  the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/d01da7f43a1b9e18f84e1dc753aea012/untitled150.ipynb)..Thanks !", "Now the error message has been improved and points to a specific problem. Please find the the gist [here](https://colab.research.google.com/gist/sachinprasadhs/9f58b5370e6bac37f9fce217064a4128/untitled150.ipynb) for reference. Thanks!", "hmmm, at least it's not talking about gradients, a step towards clarity.\r\n\r\nI'm fine with that.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47618\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47618\">No</a>\n"]}, {"number": 47617, "title": "Skip the default delegate if we already have set up one delegate.", "body": "This pr try to fix the issue at:\r\nhttps://github.com/tensorflow/tensorflow/issues/42757\r\n\r\nWhen we use the xnnpack delegate, we will see the following error message:\r\n```\r\n/tensorflow/tensorflow/lite/build$ ./examples/label_image/label_image -m ~/mobilenet_v1_1.0_224.f32.tflite -l ~/labels.txt -i /tensorflow/tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp -p 1 -x 1\r\nINFO: Loaded model /home/jerrys/mobilenet_v1_1.0_224.f32.tflite\r\nINFO: resolved reporter\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: Use XNNPACK acceleration.\r\nINFO: Applied XNNPACK delegate.\r\nERROR: ModifyGraphWithDelegate is disallowed when graph is immutable.\r\nERROR: Ignoring failed application of the default TensorFlow Lite delegate indexed at 0.\r\n```\r\n\r\nThe messages comes from the following ModifyGraphWithDelegate() and AllocateTensors() calls.\r\nhttps://github.com/tensorflow/tensorflow/blob/fb7d34de0a7a892d070104aa4e385159817e7ef5/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc#L635\r\nhttps://github.com/tensorflow/tensorflow/blob/fb7d34de0a7a892d070104aa4e385159817e7ef5/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc#L725\r\n\r\n\r\nWhen we setup xnnpack delegate in ModifyGraphWithDelegate(), the graph will become immutable. Then, we call AllocateTensors() and apply the default delegate in \"lazy_delegate_providers_\" again. That will show the error message that we are trying to update the immutable graph using the delegate in \"lazy_delegate_providers_\".\r\nSo, I just clear the \"lazy_delegate_providers_\" if we already set up a delegate.", "comments": ["cc @multiverse-tf @Maratyszcza ", "@multiverse-tf could you review this PR?"]}, {"number": 47616, "title": "Dynamic preprocessing of image inside model class. ", "body": "I want to take input of image with dynamic size and do the preprocessing on it. Especially, I want to resize the input image with dynamic size to (512x256) and then restore it back to the original input size. \r\n\r\n```\r\nimport tensorflow as tf \r\n\r\ndef resize(img, current_size, target_size, mask=False):\r\n    frame_h, frame_w = target_size\r\n    image_h, image_w = current_size\r\n    delta_h = tf.abs(image_h - frame_h)\r\n    delta_w = tf.abs(image_w - frame_w)\r\n    resized = tf.image.pad_to_bounding_box(\r\n        img, delta_h // 2, delta_w // 2, frame_h, frame_w\r\n    )\r\n    top, bottom, left, right = delta_h // 2, delta_h // 2, delta_w // 2, delta_w // 2\r\n    return resized, [top, bottom, left, right]\r\n\r\ndef restore_resize(img, size, paddings, mask=False):\r\n    top, bottom, left, right = paddings\r\n    img = img[:, top : (img.shape[1] - bottom), left : (img.shape[2] - right), :]\r\n    if mask:\r\n        img = tf.image.resize(img, size, method=\"nearest\")\r\n    else:\r\n        img = tf.image.resize(img, size, method=\"bilinear\")\r\n    return img\r\n\r\ndef test_model():\r\n   inputs = {\"person_orig\": tf.keras.layers.Input([None, None, 3])}\r\n   person = inputs[\"person_orig\"]\r\n   person_resized = tf.image.resize(person, (512, 256), preserve_aspect_ratio=True)\r\n   output, paddings = resize(\r\n      person_resized,\r\n      (person_resized.shape[1], person_resized.shape[2]),\r\n      (512, 256),\r\n  )\r\n   output = restore_resize(output, (person_resized.shape[1], person_resized.shape[2]), paddings)\r\n   return tf.keras.models.Model(inputs=inputs, outputs=output)\r\n\r\nm = test_model()\r\nprint(m.summary())\r\n\r\n\r\n\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-42-dd351506f63b> in <module>()\r\n----> 1 m = test_model()\r\n      2 print(m.summary())\r\n\r\n1 frames\r\n<ipython-input-40-ff06df5ad810> in resize(img, current_size, target_size, mask)\r\n      2     frame_h, frame_w = target_size\r\n      3     image_h, image_w = current_size\r\n----> 4     delta_h = tf.abs(image_h - frame_h)\r\n      5     delta_w = tf.abs(image_w - frame_w)\r\n      6     resized = tf.image.pad_to_bounding_box(\r\n\r\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\r\n```\r\n\r\n---\r\n[Link to reproduce error](https://colab.research.google.com/drive/1b5KEY9XTo6aA9dxii0PdH5DPpLyOn4em?usp=sharing)\r\n", "comments": ["@coreqode Try this.\r\n\r\n```python\r\nimport tensorflow as tf \r\n\r\ndef resize(img, current_size, target_size, mask=False):\r\n    frame_h, frame_w = target_size\r\n    image_h, image_w = current_size\r\n    delta_h = tf.abs(image_h - frame_h)\r\n    delta_w = tf.abs(image_w - frame_w)\r\n    resized = tf.image.pad_to_bounding_box(\r\n        img, delta_h // 2, delta_w // 2, frame_h, frame_w\r\n    )\r\n    top, bottom, left, right = delta_h // 2, delta_h // 2, delta_w // 2, delta_w // 2\r\n    return resized, [top, bottom, left, right]\r\n\r\ndef restore_resize(img, size, paddings, mask=False):\r\n    top, bottom, left, right = paddings\r\n    img = img[:, top : (tf.shape(img)[1] - bottom), left : (tf.shape(img)[2] - right), :]\r\n    if mask:\r\n        img = tf.image.resize(img, size, method=\"nearest\")\r\n    else:\r\n        img = tf.image.resize(img, size, method=\"bilinear\")\r\n    return img\r\n\r\ndef test_model():\r\n   inputs = {\"person_orig\": tf.keras.layers.Input([None, None, 3])}\r\n   person = inputs[\"person_orig\"]\r\n   person_resized = tf.image.resize(person, (512, 256), preserve_aspect_ratio=True)\r\n   output, paddings = resize(\r\n      person_resized,\r\n      (tf.shape(person_resized)[1], tf.shape(person_resized)[2]),\r\n      (512, 256),\r\n  )\r\n   output = restore_resize(output, (tf.shape(person_resized)[1], tf.shape(person_resized)[2]), paddings)\r\n   return tf.keras.models.Model(inputs=inputs, outputs=output)\r\n\r\nm = test_model()\r\nprint(m.summary())\r\n```\r\n\r\nBecause your input has dynamic height and width, you have to use `tf.shape` to get tensor's shape in runtime.", "Thanks @WindQAQ . It worked !!!! "]}, {"number": 47615, "title": "Error with cude even with tf nightly", "body": "<em>Please make sure that this is a bug. As per our\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I juste wrote `from tensorflow.keras import layers\r\n`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): W10 Family\r\n- TensorFlow installed from (source or binary): Don't\r\n- TensorFlow version (use command below):  tf-estimator-nightly-2.5.0.dev2021030601 \r\npip install tf-nightly-gpu\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: GTX 1660TI 6gb\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nI try to run script using tensorflow . But I have : \r\n\r\n> 2021-03-07 06:41:09.619717: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-03-07 06:41:09.634433: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\n\r\n**Describe the expected behavior**\r\nI should not have an error.\r\n\r\n**Standalone code to reproduce the issue**\r\nWrite a script with this code : `from tensorflow.keras import layers\r\n`\r\npip install tf-nightly-gpu\r\n", "comments": ["@LucasColas,\r\nEvery TensorFlow release is compatible with a certain CUDA/cuDNN version, for more information please take a look at the [tested build configurations](https://www.tensorflow.org/install/source_windows#gpu). \r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow_gpu-2.4.0 | 3.6-3.8 | MSVC 2019 | Bazel 3.1.0 | 8.0 | 11.0\r\ntensorflow_gpu-2.3.0 | 3.5-3.8 | MSVC 2019 | Bazel 3.1.0 | 7.6 | 10.1\r\ntensorflow_gpu-2.2.0 | 3.5-3.8 | MSVC 2019 | Bazel 2.0.0 | 7.6 | 10.1\r\n\r\nIn this case, can you please try installing **TensorFlow v2.4** with **CUDA 11.0** and **cuDNN 8** and check if you are facing the same error. Thanks!", "Also, seems like this is a duplicate of issue [#47602](https://github.com/tensorflow/tensorflow/issues/47602). Can we close this issue as it is already being tracked there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47615\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47615\">No</a>\n"]}, {"number": 47614, "title": "micro: Add ELU to AllOpsResolver", "body": "Additional fix for Issue #46323", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", ":frowning_face: Sorry, but only Googlers may change the label `cla: yes`."]}, {"number": 47613, "title": "pppppp", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Closing as spam.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47613\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47613\">No</a>\n"]}, {"number": 47612, "title": "hello world - First pull request for fun", "body": "Mew~~~", "comments": []}, {"number": 47611, "title": "\u06f1", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 47608, "title": "BatchNorm: Documentation: Problem in Inference Equation (and also training)", "body": "in the following links 'sqrt' is missing:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization#used-in-the-notebooks_1\r\n\r\n**During inference:**\r\ninstead of  :  (batch - self.moving_mean) / `sqrt`(self.moving_var + epsilon) * gamma + beta\r\nit is written:  (batch - self.moving_mean) / (self.moving_var + epsilon) * gamma + beta.\r\n\r\n**During training**\r\ninstead of:    (batch - mean(batch)) /`sqrt`(var(batch) + epsilon) * gamma + beta\r\nit is written:  (batch - mean(batch)) / (var(batch) + epsilon) * gamma + beta", "comments": ["https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/normalization.py#L57\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/normalization.py#L71\r\nThe docs generated on the website don't seem to match with docstrings.", "Hmm. I'd expect this to display in the nightly diff but don't see the update: https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization?version=nightly\r\n\r\nIs the nightly package building?", "I am looking into this. There might be a bug in the diffing algorithm.", "So, I looked through the diffing algorithm and it is working as intended.\r\n\r\nThe real problem is that tf-nightly doesn't contain the updated docstring.\r\n\r\n```\r\nIn [11]: sys.version\r\nOut[11]: '3.8.7 (default, Dec 22 2020, 10:37:26) \\n[GCC 10.2.0]'\r\n\r\nIn [12]: tf.__version__\r\nOut[12]: '2.5.0-dev20210309'\r\n\r\nIn [13]: tf.keras.layers.BatchNormalization.__doc__\r\nOut[13]: 'Layer that normalizes its inputs.\\n\\n  Batch normalization applies a transformation that maintains the mean output\\n  close to 0 and the output standard deviation close to 1.\\n\\n  Importantly, batch normalization works differently during training and\\n  during inference.\\n\\n  **During training** (i.e. when using `fit()` or when calling the layer/model\\n  with the argument `training=True`), the layer normalizes its output using\\n  the mean and standard deviation of the current batch of inputs. That is to\\n  say, for each channel being normalized, the layer returns\\n  `(batch - mean(batch)) / (var(batch) + epsilon) * gamma + beta`, where:\\n\\n  - `epsilon` is small constant (configurable as part of the constructor\\n  arguments)\\n  - `gamma` is a learned scaling factor (initialized as 1), which\\n  can be disabled by passing `scale=False` to the constructor.\\n  - `beta` is a learned offset factor (initialized as 0), which\\n  can be disabled by passing `center=False` to the constructor.\\n\\n  **During inference** (i.e. when using `evaluate()` or `predict()` or when\\n  calling the layer/model with the argument `training=False` (which is the\\n  default), the layer normalizes its output using a moving average of the\\n  mean and standard deviation of the batches it has seen during training. That\\n  is to say, it returns\\n  `(batch - self.moving_mean) / (self.moving_var + epsilon) * gamma + beta`.\\n\\n  `self.moving_mean` and `self.moving_var` are non-trainable variables that\\n  are updated each time the layer in called in training mode, as such:\\n\\n  - `moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)`\\n  - `moving_var = moving_var * momentum + var(batch) * (1 - momentum)`\\n\\n  As such, the layer will only normalize its inputs during inference\\n  *after having been trained on data that has similar statistics as the\\n  inference data*.\\n\\n  Args:\\n    axis: Integer, the axis that should be normalized (typically the features\\n      axis). For instance, after a `Conv2D` layer with\\n      `data_format=\"channels_first\"`, set `axis=1` in `BatchNormalization`.\\n    momentum: Momentum for the moving average.\\n    epsilon: Small float added to variance to avoid dividing by zero.\\n    center: If True, add offset of `beta` to normalized tensor. If False, `beta`\\n      is ignored.\\n    scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the\\n      next layer is linear (also e.g. `nn.relu`), this can be disabled since the\\n      scaling will be done by the next layer.\\n    beta_initializer: Initializer for the beta weight.\\n    gamma_initializer: Initializer for the gamma weight.\\n    moving_mean_initializer: Initializer for the moving mean.\\n    moving_variance_initializer: Initializer for the moving variance.\\n    beta_regularizer: Optional regularizer for the beta weight.\\n    gamma_regularizer: Optional regularizer for the gamma weight.\\n    beta_constraint: Optional constraint for the beta weight.\\n    gamma_constraint: Optional constraint for the gamma weight.\\n\\n  Call arguments:\\n    inputs: Input tensor (of any rank).\\n    training: Python boolean indicating whether the layer should behave in\\n      training mode or in inference mode.\\n      - `training=True`: The layer will normalize its inputs using the mean and\\n        variance of the current batch of inputs.\\n      - `training=False`: The layer will normalize its inputs using the mean and\\n        variance of its moving statistics, learned during training.\\n\\n  Input shape:\\n    Arbitrary. Use the keyword argument `input_shape` (tuple of\\n    integers, does not include the samples axis) when using this layer as the\\n    first layer in a model.\\n\\n  Output shape:\\n    Same shape as input.\\n\\n  Reference:\\n    - [Ioffe and Szegedy, 2015](https://arxiv.org/abs/1502.03167).\\n\\n  **About setting `layer.trainable = False` on a `BatchNormalization` layer:**\\n\\n  The meaning of setting `layer.trainable = False` is to freeze the layer,\\n  i.e. its internal state will not change during training:\\n  its trainable weights will not be updated\\n  during `fit()` or `train_on_batch()`, and its state updates will not be run.\\n\\n  Usually, this does not necessarily mean that the layer is run in inference\\n  mode (which is normally controlled by the `training` argument that can\\n  be passed when calling a layer). \"Frozen state\" and \"inference mode\"\\n  are two separate concepts.\\n\\n  However, in the case of the `BatchNormalization` layer, **setting\\n  `trainable = False` on the layer means that the layer will be\\n  subsequently run in inference mode** (meaning that it will use\\n  the moving mean and the moving variance to normalize the current batch,\\n  rather than using the mean and variance of the current batch).\\n\\n  This behavior has been introduced in TensorFlow 2.0, in order\\n  to enable `layer.trainable = False` to produce the most commonly\\n  expected behavior in the convnet fine-tuning use case.\\n\\n  Note that:\\n    - Setting `trainable` on an model containing other layers will\\n      recursively set the `trainable` value of all inner layers.\\n    - If the value of the `trainable`\\n      attribute is changed after calling `compile()` on a model,\\n      the new value doesn\\'t take effect for this model\\n      until `compile()` is called again.\\n  '\r\n```\r\n\r\nI can't see the `sqrt` in the equation here. @mihaimaruseac Do you know what's wrong with the tf-nightly package?", "Ohh, I know what the problem is.\r\n\r\nThis is the right docstring: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/normalization_v2.py#L205\r\n\r\nThe one that was updated doesn't contain tf_export decorator. If you update the one that I linked above, then it should show up on the site.", "While it's good that the nightly diff is working, I don't think it closes this issue.\r\n\r\nCan we update the correct place? Do we need to add a note so contributors of the future update the correct place?", "> Do we need to add a note so contributors of the future update the correct place?\r\n\r\nThe right place is where the `View Source on Github` button points too. Also the reviewers need to be on the lookout for this sort of stuff (whether the right place is being updated or not)\r\n\r\n> Can we update the correct place?\r\n\r\nI don't know what the update was. So whoever knows can do that.", "Thanks, Yash.\r\nLooks like this was added in https://github.com/tensorflow/tensorflow/commit/76094af11bffac23375ad7b08f5c7bd6df2e0c7c\r\n@Suraj-Upadhyay Can you please update the correct file? Thanks", "The docs are now updated with nightly. Thanks!\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization?version=nightly#used-in-the-notebooks_1"]}, {"number": 47606, "title": "Could not load dynamic library 'libcudnn.so.7' ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**\r\n- TensorFlow version: 2.2.2\r\n- Python version: python 3.6.13\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version:CUDA 10.1/cuDNN version 7.6.5\r\n- GPU model and memory: GeForce RTX 965M, 8GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI followed [the cuda installation guide](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) and I also used a **docker container**. Inside the container I tried to run a simple GPU test using the` tf.config.list_physical_devices('GPU')`  and I got the libcudnn error. I am very new to linux and docker so I don't know what infos I should provide to provide more insight to the situation at hand.\r\n\r\n> tf.config.list_physical_devices('GPU')\r\n> 2021-03-06 22:01:00.508031: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n> 2021-03-06 22:01:00.546788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2021-03-06 22:01:00.547204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\n> coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 74.65GiB/s\r\n> 2021-03-06 22:01:00.547439: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n> 2021-03-06 22:01:00.549211: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2021-03-06 22:01:00.550911: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n> 2021-03-06 22:01:00.551227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n> 2021-03-06 22:01:00.553568: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n> 2021-03-06 22:01:00.554930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n> 2021-03-06 22:01:00.555099: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/include:/usr/local/cuda/lib64:/usr/local/cuda-10.1/lib:/usr/local/cuda-10.1/lib64\r\n> 2021-03-06 22:01:00.555113: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n> Skipping registering GPU devices...\r\n> \r\n\r\n**Any other info**\r\nI have tried adding a bunch of path into my LD_LIBRARY_PATH and ~/.bashrc and also deleting and creating a new symlink in /usr/local/cuda/lib64 for libcudnn.so.7 and libcudnn.so to libcudnn.7.6.5 as per suggested in #20271 and none worked. Other things I have tried is to delete all the libcudnn from /usr/local/cuda-10.1/lib64 and then install the cudnn again both from tar and the debian installation, and it doesn't work either.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47606\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47606\">No</a>\n"]}, {"number": 47605, "title": "[tflite] Enable CoreML/GPU delegate on m1", "body": "To run TFLite models on M1 devices, we need CoreML delegate to offload computation to Apple's Neural Engine.\r\n\r\n```\r\nbazel build //tensorflow/lite/tools/benchmark:benchmark_model \\\r\n  --macos_cpus arm64\r\n```\r\n    \r\n```\r\n./bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model \\\r\n  --use_coreml=1 \\\r\n  --graph=/tmp/mobilenet_edgetpu_224_1.0_float.tflite\r\n```\r\nworks as expected\r\n", "comments": ["@teijeong @yyoon could you review this change?", "Testing classifiers with `label_image` + CoreML delegate also shows expected results. E.g., the following is running MobileNet EdgeTPU\r\n\r\n```\r\n% bazel-bin/tensorflow/lite/examples/label_image/label_image --use_coreml=1 -m /tmp/tflite-models-new/float/mobilenet_edgetpu_224_1.0_float.tflite -i /tmp/grace_hopper.bmp -l /tmp/labels.txt -c 50     \r\nINFO: Loaded model /tmp/tflite-models-new/float/mobilenet_edgetpu_224_1.0_float.tflite\r\nINFO: resolved reporter\r\n2021-03-07 10:44:37.676 label_image[18419:1846370] coreml_version must be 2 or 3. Setting to 3.\r\nINFO: COREML delegate created.\r\nINFO: CoreML delegate: 75 nodes delegated out of 77 nodes, with 2 partitions.\r\n\r\nINFO: Applied Delegate_Provider_COREML delegate.\r\nINFO: invoked\r\nINFO: average time: 0.96858 ms\r\nINFO: 0.78034: 653 military uniform\r\nINFO: 0.0309746: 458 bow tie\r\nINFO: 0.0292118: 835 suit\r\nINFO: 0.0196119: 440 bearskin\r\nINFO: 0.0135848: 716 pickelhaube\r\n```\r\n\r\nThe `label_image` was built with\r\n```\r\nbazel build --config opt //tensorflow/lite/examples/label_image:label_image --macos_cpus arm64\r\n```", "\r\nFloating point MobileNet V1 1.0 224 from [TFLite hosted models](https://www.tensorflow.org/lite/guide/hosted_models)\r\n\r\nSummary: unit: ms\r\n| how | average latency|\r\n|-----|------:|\r\n|CPU 1xthread| 18.847 |\r\n|CPU 4xthreads| 9.087|\r\n|GPU delegate| 2.699|\r\n|CoreML delegate| 1.051|\r\n\r\n```\r\nfreedom@myway-m1 tf-hacks % ./bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --graph=/tmp/mobilenet_v1_1.0_224.tflite       \r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [/tmp/mobilenet_v1_1.0_224.tflite]\r\nLoaded model /tmp/mobilenet_v1_1.0_224.tflite\r\nThe input model file size (MB): 16.9011\r\nInitialized session in 0.464ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=26 first=37888 curr=18859 min=18796 max=37888 avg=19709.4 std=3689\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=54 first=18877 curr=18818 min=18817 max=19111 avg=18847.4 std=41\r\n\r\nInference timings in us: Init: 464, First inference: 37888, Warmup (avg): 19709.4, Inference (avg): 18847.4\r\nfreedom@myway-m1 tf-hacks % ./bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --graph=/tmp/mobilenet_v1_1.0_224.tflite --num_threads=4\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nNum threads: [4]\r\nGraph: [/tmp/mobilenet_v1_1.0_224.tflite]\r\n#threads used for CPU inference: [4]\r\nLoaded model /tmp/mobilenet_v1_1.0_224.tflite\r\nThe input model file size (MB): 16.9011\r\nInitialized session in 0.455ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=53 first=35946 curr=9255 min=8685 max=35946 avg=9579.11 std=3664\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=110 first=8954 curr=9034 min=8832 max=9810 avg=9087.25 std=143\r\n\r\nInference timings in us: Init: 455, First inference: 35946, Warmup (avg): 9579.11, Inference (avg): 9087.25\r\nfreedom@myway-m1 tf-hacks % ./bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --graph=/tmp/mobilenet_v1_1.0_224.tflite --use_gpu=1\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [/tmp/mobilenet_v1_1.0_224.tflite]\r\nUse gpu: [1]\r\nLoaded model /tmp/mobilenet_v1_1.0_224.tflite\r\nINFO: Created TensorFlow Lite delegate for Metal.\r\nExplicitly applied GPU delegate, and the model graph will be completely executed by the delegate.\r\nThe input model file size (MB): 16.9011\r\nInitialized session in 208.191ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=275 first=4297 curr=2087 min=1429 max=4297 avg=1780.88 std=263\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=362 first=2225 curr=2711 min=1978 max=2827 avg=2698.69 std=84\r\n\r\nInference timings in us: Init: 208191, First inference: 4297, Warmup (avg): 1780.88, Inference (avg): 2698.69\r\nfreedom@myway-m1 tf-hacks % ./bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --graph=/tmp/mobilenet_v1_1.0_224.tflite --use_coreml=1\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [/tmp/mobilenet_v1_1.0_224.tflite]\r\nUse CoreML: [1]\r\nLoaded model /tmp/mobilenet_v1_1.0_224.tflite\r\n2021-03-07 11:03:42.453 benchmark_model[24456:1865700] coreml_version must be 2 or 3. Setting to 3.\r\nINFO: CoreML delegate: 29 nodes delegated out of 31 nodes, with 2 partitions.\r\n\r\nExplicitly applied COREML delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.\r\nThe input model file size (MB): 16.9011\r\nInitialized session in 341.744ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=472 first=1660 curr=1050 min=1017 max=1660 avg=1049.53 std=31\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=942 first=1039 curr=1084 min=1022 max=1154 avg=1051.01 std=15\r\n\r\nInference timings in us: Init: 341744, First inference: 1660, Warmup (avg): 1049.53, Inference (avg): 1051.01\r\n```", "@yyoon Thanks. I added additional checks to fix dependency problem reported by CI.", "@freedomtan what is required to use \"--macos_cpus arm64\" ?\r\nI used the following command on my x86 mac but the compiled binary is x86_64.\r\n```\r\n$ bazel build //tensorflow/lite/tools/benchmark:benchmark_model --macos_cpus arm64\r\n  ...\r\n$ file bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model\r\nbazel-bin/tensorflow/lite/tools/benchmark/benchmark_model: Mach-O 64-bit executable x86_64\r\n```\r\n\r\nAlso TF is using \"--config=macos_arm64\", can we use it?", "@terryheo Arrgh, yes, that's a bit tricky. I should put it down somewhere. No, `--macos_cpus arm64` is not for building arm64 binary on intel machine. I didn't try to build arm64 binary or fat binary on x86 machines.\r\n\r\nOn M1 machines, with arm64 bazel (built from its master branch), I can build \"right\" `benchmark_model` (w/o coreml and gpu delegate) without problem. But building either coreml or gpu delegate is enabled, we need update XNNPACK and some related packages and kinda need `--macos_cpus arm64` to `@//cpuinfo` as far as I can remember. I'll send another PR to update those information. And I'll check if I can build either arm64 or fat one on x86_64 machines.\r\n"]}, {"number": 47604, "title": "Fix normalization layers", "body": "Fix #46366 . `LayerNormalization` crashes on getting an empty input while `BatchNormalization` returns back the empty input. This PR will help both the normlaization layers to raise an error upon encountering an empty input.", "comments": ["@deeb02  can you please review this PR?", "@mattdangerw, @rthadur  can you please review this PR?"]}, {"number": 47603, "title": "Same random seed(s), different tensorflow versions, different results", "body": "I'm trying to get the same results using tf 1.15 and tf 2.4.1. Here are 2 examples that I'm expecting to produce similar results. The first example using `tf.layers.conv2d()` and the second using `tf.keras.layers.Conv2D()`. I tried using `kernel_initializer=some_tf_initializer(seed=seed)` and I also tried without using a kernel initializer, still the same. \r\n___\r\n\r\n**TF1**\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        seed = 1\r\n        np.random.seed(seed)\r\n        tf.set_random_seed(seed)\r\n        session = tf.InteractiveSession()\r\n        initializer = tf.initializers.orthogonal(1.0, seed)\r\n        x = np.random.random((2, 84, 84, 1))\r\n        xp = tf.placeholder(x.dtype, x.shape)\r\n        result = tf.layers.conv2d(xp, 32, 8, 4, kernel_initializer=initializer)\r\n        session.run(tf.global_variables_initializer())\r\n        print(f'Conv2D output:\\n{session.run([result], {xp: x})}\\n{100 * \"=\"}')\r\n        print(f'x:\\n{x}')\r\n\r\n\r\n**Results in**\r\n\r\n    Conv2D output:\r\n    [array([[[[-4.92592295e-01,  5.84946930e-01,  6.45957303e-01, ...,\r\n               3.65328020e-01,  4.03869755e-01, -1.06886940e+00],\r\n             [ 5.37549724e-02,  5.47611947e-01,  3.91425858e-01, ...,\r\n               1.20702194e-01,  9.21035825e-02, -1.10599980e+00],\r\n             [-4.68301348e-01,  8.54547125e-01,  2.99738041e-01, ...,\r\n              -7.47656723e-02, -3.29838560e-01, -6.43979360e-01],\r\n             ...,\r\n             [-3.61929553e-01,  5.70854964e-01,  1.49096153e-01, ...,\r\n               4.05001572e-01, -5.70764458e-01, -8.50054931e-01],\r\n             [-5.23792632e-01,  5.79328891e-01,  4.36402398e-01, ...,\r\n               4.66264733e-01, -8.00687780e-02, -1.05017933e+00],\r\n             [-9.59200260e-01,  9.77023018e-01,  7.22006476e-01, ...,\r\n               3.29199395e-01, -6.48041695e-02, -9.65927090e-01]],\r\n    \r\n            [[-3.05192775e-01,  5.62304495e-01,  2.66985564e-01, ...,\r\n               4.01866526e-01, -5.27853938e-01, -1.36294176e+00],\r\n             [-3.33627733e-01,  8.70909716e-01, -1.90685411e-01, ...,\r\n               5.82781510e-01, -9.54447222e-04, -5.93152081e-01],\r\n             [ 1.05312097e-01,  1.34995604e+00,  4.18403345e-01, ...,\r\n               7.29398371e-01, -1.52729852e-01, -1.44087877e+00],\r\n             ...,\r\n             [-2.65877698e-01,  1.30702457e+00,  2.12075863e-01, ...,\r\n               4.02235128e-01, -4.83213829e-02, -9.84811507e-01],\r\n             [-2.33065665e-01,  8.06240360e-01,  2.04250988e-01, ...,\r\n               5.11907848e-01, -2.97176027e-01, -5.89994050e-01],\r\n             [-6.06616196e-01,  7.52128441e-01,  6.05931022e-01, ...,\r\n               8.25940123e-01, -8.11965401e-01, -1.38039418e+00]],\r\n    \r\n            [[-1.14268620e-02,  9.44562211e-01,  7.30843200e-01, ...,\r\n               7.71483717e-01, -3.62299259e-01, -5.19022458e-01],\r\n             [-1.25248650e-01,  8.31288483e-01,  3.01203507e-01, ...,\r\n               6.10087249e-02, -1.37678504e-01, -1.12305468e+00],\r\n             [-2.37495350e-01,  5.80913482e-01,  2.48314393e-01, ...,\r\n               6.06333851e-01, -3.30380816e-01, -1.00515721e+00],\r\n             ...,\r\n             [ 1.95822525e-01,  9.85462620e-01,  9.30753642e-02, ...,\r\n               5.07316387e-01, -2.72563623e-01, -3.44717273e-01],\r\n             [-7.58518148e-01,  7.18211754e-01,  3.56392246e-01, ...,\r\n               5.41345861e-01, -2.84327606e-01, -5.78967512e-01],\r\n             [-9.36528091e-01,  4.51066108e-01,  3.60469945e-01, ...,\r\n               6.06252149e-01, -9.82606865e-02, -1.12673980e+00]],\r\n    \r\n            ...,\r\n    \r\n            [[-3.23906425e-01,  4.89133765e-01,  5.51718357e-01, ...,\r\n              -1.33718077e-01,  1.23015200e-03, -5.54514943e-01],\r\n             [-5.04556877e-01,  7.36618066e-01,  2.02579467e-01, ...,\r\n              -3.74142562e-01,  6.00494771e-03, -1.31111289e+00],\r\n             [-4.16236220e-01,  5.98196128e-01,  6.97848461e-01, ...,\r\n               6.23122747e-01, -1.07252840e-01, -4.83301913e-01],\r\n             ...,\r\n             [ 1.06692401e-01,  1.73388947e+00,  3.22292122e-01, ...,\r\n              -1.75720933e-01, -2.96764992e-01, -1.06044579e+00],\r\n             [-5.62972482e-01,  1.36246077e+00,  9.13179694e-01, ...,\r\n               6.32824517e-01, -4.73498606e-01, -9.47552266e-01],\r\n             [-5.11697389e-01,  6.66774542e-01,  3.82947609e-01, ...,\r\n               5.67110785e-01, -7.11579248e-03, -7.77461939e-01]],\r\n    \r\n            [[-1.11410557e-01,  1.10325702e+00,  5.50558807e-01, ...,\r\n               5.51697720e-01, -3.75442814e-01, -4.68653786e-01],\r\n             [-3.26447172e-01,  1.19972335e+00,  3.82313314e-01, ...,\r\n               2.20346417e-01, -3.39678412e-01, -8.66913575e-01],\r\n             [-3.98474415e-01,  7.14095329e-01,  8.04559140e-02, ...,\r\n               1.19488448e-01, -2.52660129e-01, -9.91879947e-01],\r\n             ...,\r\n             [-5.70385227e-01,  8.52797253e-01,  4.66251675e-01, ...,\r\n              -3.24400644e-01,  3.20423484e-03, -1.39903236e+00],\r\n             [-2.07675682e-01,  6.29120258e-01,  6.79053796e-01, ...,\r\n               1.07336154e-01,  1.87886060e-01, -3.86794041e-01],\r\n             [-2.67700849e-02,  4.49128504e-01,  3.40760390e-01, ...,\r\n               6.36000637e-01, -3.77890278e-01, -3.30207391e-01]],\r\n    \r\n            [[-3.10700139e-01,  8.18823907e-01,  3.67007768e-03, ...,\r\n               4.90558846e-01, -8.36860336e-01, -1.03869365e+00],\r\n             [-4.88181365e-01,  8.00080028e-01,  3.56097390e-01, ...,\r\n               4.76738039e-01, -3.42056012e-01, -5.30593649e-01],\r\n             [-4.60314405e-01,  7.07439805e-01,  4.22496599e-01, ...,\r\n               5.05574451e-01, -1.24705048e-01, -2.87646769e-01],\r\n             ...,\r\n             [-2.57950491e-01,  1.20126622e+00,  5.56477074e-02, ...,\r\n               6.11127028e-01, -1.93377726e-01, -1.07765356e+00],\r\n             [-6.50455755e-01,  1.29336304e+00,  8.44637456e-01, ...,\r\n               1.81637465e-01, -2.88107846e-01, -4.49444781e-01],\r\n             [-4.21331048e-01,  3.20901642e-01,  8.83963968e-01, ...,\r\n               8.65426315e-01, -4.92428131e-01, -8.49800993e-01]]],\r\n    \r\n    \r\n           [[[-5.16387221e-01,  6.99757393e-01,  7.85561831e-01, ...,\r\n               1.03644383e-01, -3.70012470e-01, -5.32289379e-01],\r\n             [-7.05045607e-02,  6.10474017e-01,  3.49680420e-01, ...,\r\n               7.92201453e-01, -5.39308419e-01, -3.42154387e-01],\r\n             [-4.22583634e-01,  8.01291482e-01,  1.48925846e-01, ...,\r\n               7.68131504e-01, -7.38695500e-01, -1.09516989e+00],\r\n             ...,\r\n             [-7.26389962e-01,  6.26188865e-01,  5.42793169e-01, ...,\r\n               3.52803865e-01, -3.46388144e-01, -7.60162696e-01],\r\n             [-9.04680334e-02,  1.75359623e-01,  8.00145598e-01, ...,\r\n               2.51379785e-01, -2.79905131e-01, -8.72223633e-01],\r\n             [-4.20990087e-01,  3.09422635e-01,  4.11481559e-01, ...,\r\n               3.69259084e-02, -1.60866470e-01, -5.05859647e-01]],\r\n    \r\n            [[-4.38932989e-01,  9.72496331e-01,  2.50341307e-01, ...,\r\n               2.83248632e-01, -3.26580435e-01, -1.03773839e+00],\r\n             [-8.19208455e-01,  8.96083502e-01,  4.24397325e-01, ...,\r\n              -5.64658536e-02, -6.09974865e-01, -1.23144756e+00],\r\n             [-9.45132686e-01,  9.13248242e-01,  8.86299832e-01, ...,\r\n               2.15502049e-01, -5.58705913e-01, -2.25369791e-01],\r\n             ...,\r\n             [-8.56722975e-01,  7.94114365e-01,  5.81065297e-01, ...,\r\n               4.96281428e-01, -1.03129758e+00, -1.05092158e+00],\r\n             [-2.82233182e-02,  9.48533077e-01,  6.68768609e-01, ...,\r\n               5.78499983e-01, -6.95651561e-01, -9.76414384e-01],\r\n             [-8.06302266e-01,  1.05147010e+00,  4.22578075e-01, ...,\r\n               2.94475123e-01,  1.74168406e-01, -1.14952206e+00]],\r\n    \r\n            [[-3.06847178e-01,  9.29719098e-01,  2.68849689e-01, ...,\r\n               3.96790007e-01, -3.59629268e-01, -9.25076133e-01],\r\n             [-2.38372207e-01,  5.78896860e-01,  2.17575482e-01, ...,\r\n              -6.97642069e-02, -2.63976905e-01, -9.10845525e-01],\r\n             [-5.40468423e-01,  1.20147694e+00,  4.04620974e-01, ...,\r\n               1.76649501e-01,  1.46967870e-01, -6.74134783e-01],\r\n             ...,\r\n             [-6.27551970e-01,  5.05884731e-01,  2.10479188e-01, ...,\r\n               3.09780840e-01,  1.46682047e-01, -1.07307698e+00],\r\n             [-6.44653887e-01,  8.90779216e-01,  3.82747674e-01, ...,\r\n               1.72155001e-01, -4.48190676e-01, -8.62522695e-01],\r\n             [-1.50221285e-01,  9.35658435e-01,  3.59703911e-01, ...,\r\n              -8.34232530e-02, -1.89649715e-01, -7.73705172e-01]],\r\n    \r\n            ...,\r\n    \r\n            [[-3.47146995e-01,  6.34597952e-02,  4.73544353e-01, ...,\r\n              -8.20012972e-02, -1.78752555e-01, -1.36296041e+00],\r\n             [-8.95298221e-01,  8.66479595e-01,  5.61457267e-01, ...,\r\n               2.22826074e-01, -4.86448503e-01, -8.56547191e-01],\r\n             [-8.62118822e-01,  4.83077034e-01,  3.60908309e-02, ...,\r\n               3.29259093e-01, -4.08073639e-02, -6.45683881e-01],\r\n             ...,\r\n             [-5.01558985e-01,  4.62753228e-01,  4.01966678e-01, ...,\r\n               6.35652593e-01, -7.45519465e-02, -5.39741380e-01],\r\n             [ 1.29982837e-01,  7.31941977e-01, -2.05750614e-01, ...,\r\n               3.16325608e-01, -3.28495177e-01, -1.09927128e+00],\r\n             [-4.90335504e-01,  4.94757973e-01,  9.40801327e-02, ...,\r\n               5.66634378e-02, -7.30613447e-01, -7.25730750e-01]],\r\n    \r\n            [[-2.75066335e-01,  7.35209409e-01,  5.99839688e-01, ...,\r\n               6.71254510e-02,  7.97677772e-02, -7.72461196e-01],\r\n             [-1.58777502e-01,  7.51857910e-01,  3.80584693e-01, ...,\r\n              -1.01868390e-01, -7.12568409e-02, -6.42932271e-01],\r\n             [-1.45951405e-01,  1.03692197e+00,  4.91333873e-01, ...,\r\n               2.98796942e-01, -5.46416283e-01, -1.04881221e+00],\r\n             ...,\r\n             [-9.75652891e-03,  8.28500896e-01,  3.48450207e-01, ...,\r\n               4.07241092e-01, -2.34265134e-01, -5.27081486e-01],\r\n             [-5.65917326e-01,  7.37496827e-01,  1.65005917e-01, ...,\r\n               6.61606291e-01, -2.20420580e-01, -1.11307865e+00],\r\n             [-2.21167732e-01,  4.83734785e-01,  4.59793140e-01, ...,\r\n               4.19304500e-01, -2.96987262e-01, -2.01353157e-01]],\r\n    \r\n            [[-6.33535626e-01,  1.11787318e+00,  6.41380845e-01, ...,\r\n               1.12652086e-01, -1.99377096e-02, -6.69645437e-01],\r\n             [-1.05662073e-01,  3.87718948e-01,  4.30142659e-01, ...,\r\n               3.27637078e-01, -3.59568870e-01, -9.63431155e-01],\r\n             [-1.26793132e-01,  1.29664648e+00,  3.28922837e-01, ...,\r\n               3.21313848e-01, -7.53446525e-01, -7.93674733e-01],\r\n             ...,\r\n             [-5.32937597e-01,  1.09915270e+00,  6.23443352e-01, ...,\r\n               9.96585500e-01, -6.21343220e-01, -1.01232184e+00],\r\n             [ 5.05322966e-02,  1.18874480e+00,  4.57358272e-01, ...,\r\n               4.80935716e-01, -2.04122013e-01, -1.13864994e+00],\r\n             [-1.03084733e-01,  1.14916096e+00,  2.73508528e-01, ...,\r\n               6.76093153e-01, -3.34324702e-01, -1.28436283e+00]]]])]\r\n    ====================================================================================================\r\n    x:\r\n    [[[[4.17022005e-01]\r\n       [7.20324493e-01]\r\n       [1.14374817e-04]\r\n       ...\r\n       [6.23672207e-01]\r\n       [7.50942434e-01]\r\n       [3.48898342e-01]]\r\n    \r\n      [[2.69927892e-01]\r\n       [8.95886218e-01]\r\n       [4.28091190e-01]\r\n       ...\r\n       [1.85762022e-02]\r\n       [7.00221437e-02]\r\n       [4.86345111e-01]]\r\n    \r\n      [[6.06329462e-01]\r\n       [5.68851437e-01]\r\n       [3.17362409e-01]\r\n       ...\r\n       [9.18601778e-01]\r\n       [4.02024891e-04]\r\n       [9.76759149e-01]]\r\n    \r\n      ...\r\n    \r\n      [[5.89549934e-01]\r\n       [3.89137609e-01]\r\n       [5.05975232e-01]\r\n       ...\r\n       [4.35888475e-01]\r\n       [7.89075202e-01]\r\n       [4.66467704e-01]]\r\n    \r\n      [[6.73554921e-01]\r\n       [8.84836452e-01]\r\n       [9.38138449e-01]\r\n       ...\r\n       [7.93970466e-01]\r\n       [2.13784215e-01]\r\n       [6.41105035e-01]]\r\n    \r\n      [[7.31134736e-01]\r\n       [9.50619892e-02]\r\n       [7.00729238e-02]\r\n       ...\r\n       [9.95522026e-01]\r\n       [4.81429517e-01]\r\n       [8.37812754e-01]]]\r\n    \r\n    \r\n     [[[6.03655452e-01]\r\n       [6.64374944e-01]\r\n       [2.72461392e-01]\r\n       ...\r\n       [1.14069927e-01]\r\n       [2.93705095e-01]\r\n       [8.78904978e-03]]\r\n    \r\n      [[2.53263696e-01]\r\n       [8.37712781e-01]\r\n       [8.07756027e-01]\r\n       ...\r\n       [7.37152445e-01]\r\n       [6.00521471e-01]\r\n       [7.37999367e-01]]\r\n    \r\n      [[3.75760828e-01]\r\n       [9.11106703e-01]\r\n       [8.72308594e-01]\r\n       ...\r\n       [9.80268932e-01]\r\n       [1.13198035e-01]\r\n       [4.65678949e-01]]\r\n    \r\n      ...\r\n    \r\n      [[4.94241516e-02]\r\n       [6.34450548e-01]\r\n       [8.93053413e-01]\r\n       ...\r\n       [7.97760317e-01]\r\n       [3.18871974e-01]\r\n       [6.47314782e-01]]\r\n    \r\n      [[4.65136696e-01]\r\n       [5.07669096e-01]\r\n       [4.23295851e-01]\r\n       ...\r\n       [2.98177206e-01]\r\n       [5.32380132e-01]\r\n       [6.12348886e-01]]\r\n    \r\n      [[2.58528146e-01]\r\n       [5.20561003e-02]\r\n       [7.82628170e-01]\r\n       ...\r\n       [8.26242775e-03]\r\n       [7.43071396e-01]\r\n       [3.29652868e-01]]]]\r\n\r\n**TF2**\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    from tensorflow.keras.layers import Conv2D\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        seed = 1\r\n        np.random.seed(seed)\r\n        tf.random.set_seed(seed)\r\n        x = np.random.random((2, 84, 84, 1))\r\n        initializer = tf.initializers.Orthogonal(1.0, seed)\r\n        print(f'Conv2D output:\\n{Conv2D(32, 8, 4, kernel_initializer=initializer)(x)}\\n{100 * \"=\"}')\r\n        print(f'x:\\n{x}')\r\n\r\n**Results in**\r\n\r\n        Conv2D output:\r\n    [[[[-5.41361034e-01  3.66543412e-01 -1.51497812e-03 ... -5.08555949e-01\r\n        -2.81920075e-01  2.03241315e-02]\r\n       [-5.59882522e-01  4.86802310e-01 -6.99946359e-02 ... -2.40252942e-01\r\n        -3.65630835e-01 -3.15424293e-01]\r\n       [-9.42213356e-01  2.15654269e-01 -1.55082747e-01 ... -4.04906332e-01\r\n        -2.01114044e-01  7.04537705e-02]\r\n       ...\r\n       [-3.46490562e-01  3.19557160e-01 -2.92775959e-01 ... -4.28674221e-01\r\n         2.79810458e-01  4.20070797e-01]\r\n       [-3.45766425e-01  1.90711677e-01 -1.57374702e-02 ... -5.71854293e-01\r\n         4.97741327e-02 -1.16687842e-01]\r\n       [-9.33221936e-01 -1.68129373e-02  1.63893417e-01 ... -4.79621142e-01\r\n        -2.14530781e-01  5.61090052e-01]]\r\n    \r\n      [[-7.58794546e-01  5.37562370e-01 -4.74378794e-01 ... -5.63471198e-01\r\n        -2.08135564e-02  3.88803691e-01]\r\n       [-4.21465009e-01  1.65287077e-01 -4.43225324e-01 ... -1.72018170e-01\r\n        -7.92833045e-02  3.92330065e-02]\r\n       [-4.81306076e-01  3.66574019e-01 -3.64440233e-01 ... -5.52142262e-01\r\n        -2.36726597e-01 -3.12441528e-01]\r\n       ...\r\n       [-8.76132190e-01  3.22705477e-01 -1.90438583e-01 ... -6.52492762e-01\r\n        -6.05543517e-02  3.59102860e-02]\r\n       [-7.17206061e-01  1.32312387e-01 -4.24121648e-01 ...  1.82857260e-01\r\n         1.84026435e-01 -1.85313985e-01]\r\n       [-1.20653558e+00  4.54987772e-02 -2.89574206e-01 ... -3.28905612e-01\r\n         3.50747108e-02  1.20641785e-02]]\r\n    \r\n      [[-5.34970939e-01  7.13559866e-01 -4.85820472e-01 ... -4.59470123e-01\r\n         3.37272674e-01  1.63189009e-01]\r\n       [-7.05033600e-01  3.56267929e-01 -4.30038758e-02 ... -5.83572984e-01\r\n        -1.40957370e-01 -2.53099173e-01]\r\n       [-5.99712431e-01  7.10705340e-01 -3.39112192e-01 ... -4.14105803e-01\r\n         1.87255502e-01 -3.33267421e-01]\r\n       ...\r\n       [-5.80564499e-01  4.36445504e-01  3.36502224e-01 ...  1.45576835e-01\r\n         3.74677926e-01 -3.91695678e-01]\r\n       [-7.22398818e-01  3.15642595e-01 -2.88131565e-01 ... -3.45985293e-01\r\n         2.34268203e-01  3.29410911e-01]\r\n       [-7.46637762e-01  2.26648629e-01 -1.73913926e-01 ... -1.78611025e-01\r\n         7.35538006e-02  2.55084813e-01]]\r\n    \r\n      ...\r\n    \r\n      [[-5.49425662e-01 -3.49880159e-02  4.77346703e-02 ... -4.04859513e-01\r\n         3.71269658e-02 -1.85562521e-02]\r\n       [-8.26125443e-01  1.29296139e-01 -2.88902789e-01 ... -8.50088000e-01\r\n        -5.04021049e-01 -4.97736454e-01]\r\n       [-7.37619460e-01  5.00219822e-01  7.76109993e-02 ... -1.41733378e-01\r\n         2.81554639e-01  3.69791657e-01]\r\n       ...\r\n       [-3.06938559e-01  4.47962463e-01 -2.05980629e-01 ... -8.48388433e-01\r\n        -1.59557834e-01  3.17021906e-02]\r\n       [-3.87109697e-01  9.46836114e-01 -2.71658182e-01 ... -6.23931110e-01\r\n        -2.16598317e-01 -1.88935712e-01]\r\n       [-1.31429803e+00  3.86178017e-01 -6.20340466e-01 ... -3.68769616e-02\r\n         1.81194678e-01 -1.06644318e-01]]\r\n    \r\n      [[-7.55118549e-01 -1.59330755e-01 -1.55888349e-01 ... -8.46641809e-02\r\n         5.47275543e-02 -3.14989388e-01]\r\n       [-7.29794323e-01  1.63924411e-01  5.99410906e-02 ...  1.31567806e-01\r\n        -4.67930377e-01  2.95255750e-01]\r\n       [-6.91133916e-01  5.20268269e-02 -7.29867145e-02 ... -1.88851178e-01\r\n         4.07196075e-01  4.01087821e-01]\r\n       ...\r\n       [-5.79473913e-01  7.94014513e-01 -7.01249093e-02 ... -6.76721931e-01\r\n        -1.01779945e-01 -5.97994268e-01]\r\n       [-6.84648752e-01  1.01270474e-01 -4.07643348e-01 ...  3.85033749e-02\r\n         8.26148912e-02  5.69675528e-02]\r\n       [-4.62479711e-01  4.19429392e-01  1.77284703e-04 ... -1.42741054e-01\r\n         3.54655176e-01  2.56564021e-01]]\r\n    \r\n      [[-2.46302426e-01  7.49420345e-01  8.12041853e-03 ... -3.32499892e-01\r\n         1.33527100e-01  6.35570288e-02]\r\n       [-9.58327591e-01  1.98065817e-01  1.93995520e-01 ... -2.94785231e-01\r\n         2.52389193e-01 -1.20501839e-01]\r\n       [-8.17039490e-01  2.07326993e-01 -5.11587203e-01 ... -4.24753636e-01\r\n         2.88008928e-01  5.03057897e-01]\r\n       ...\r\n       [-7.03616858e-01  3.78316432e-01 -5.60314238e-01 ... -4.05634254e-01\r\n        -3.88694972e-01 -1.93394765e-01]\r\n       [-6.37941658e-01  4.12149638e-01  4.30523872e-01 ... -1.07135192e-01\r\n        -4.06805426e-01  2.13325843e-01]\r\n       [-1.10811067e+00  1.87466890e-01 -4.25169766e-01 ... -3.90125453e-01\r\n        -2.32264772e-01 -1.13592006e-01]]]\r\n    \r\n    \r\n     [[[-1.12212503e+00  4.90197897e-01 -1.32902622e-01 ... -2.71363735e-01\r\n         5.97870303e-03 -1.82653069e-01]\r\n       [-5.94440103e-01 -2.69649085e-02 -8.82392377e-02 ... -2.57950276e-01\r\n        -5.64375184e-02  3.79566044e-01]\r\n       [-9.26148295e-01  7.57718146e-01 -1.35923713e-01 ... -3.23437661e-01\r\n         4.14071977e-01  1.71241298e-01]\r\n       ...\r\n       [-7.43670583e-01  5.69033444e-01  8.93335044e-02 ... -4.33211058e-01\r\n         9.38811079e-02 -2.02697456e-01]\r\n       [-7.01980829e-01  8.19573849e-02  1.15745321e-01 ... -3.05091172e-01\r\n         2.67599583e-01  4.00184661e-01]\r\n       [-1.23418115e-01 -7.21083656e-02 -2.83473641e-01 ... -5.09245217e-01\r\n         1.67642578e-01  3.97953391e-02]]\r\n    \r\n      [[-7.82243788e-01  4.62321520e-01 -1.03323981e-01 ... -6.02696836e-01\r\n         1.12790830e-01  6.60246331e-03]\r\n       [-1.18653631e+00  1.10369611e+00  4.03279841e-01 ... -3.35117340e-01\r\n         1.34733140e-01  1.24090314e-01]\r\n       [-7.56276011e-01  4.68323886e-01  2.55332291e-01 ... -5.64108253e-01\r\n        -1.23212464e-01  9.44344103e-02]\r\n       ...\r\n       [-1.07282424e+00  4.46680576e-01  2.62999147e-01 ... -4.93109912e-01\r\n         1.20379120e-01  2.00786784e-01]\r\n       [-5.91401279e-01 -4.76840436e-02 -6.31224453e-01 ... -1.33297965e-02\r\n        -6.85213029e-01  3.06965083e-01]\r\n       [-1.26961339e+00  4.00966257e-01  1.58335969e-01 ... -4.71471280e-01\r\n        -5.21351993e-01 -2.68614203e-01]]\r\n    \r\n      [[-7.59719312e-01 -2.15684757e-01 -5.51491380e-01 ... -4.05447990e-01\r\n         2.04430401e-01 -4.03630167e-01]\r\n       [-7.50933051e-01 -2.70830095e-01 -4.10136461e-01 ... -2.66372442e-01\r\n        -4.25907433e-01  2.79703408e-01]\r\n       [-1.15558457e+00  1.44730762e-01 -1.11366399e-01 ... -3.16568702e-01\r\n        -1.65987372e-01 -2.50013694e-02]\r\n       ...\r\n       [-1.26355612e+00  1.83538213e-01  2.64952302e-01 ... -4.03630674e-01\r\n        -1.93098515e-01  3.03720146e-01]\r\n       [-1.04506040e+00  9.86333251e-01 -4.76556689e-01 ... -5.65628707e-01\r\n         2.07612868e-02  7.63047487e-02]\r\n       [-1.07622218e+00  2.96826065e-01 -5.04598498e-01 ... -4.09753144e-01\r\n        -2.73265153e-01  6.33615702e-02]]\r\n    \r\n      ...\r\n    \r\n      [[-1.13511002e+00  2.11719826e-01 -8.60134482e-01 ... -3.91803771e-01\r\n        -6.59739316e-01 -2.00912848e-01]\r\n       [-1.29758418e+00  2.52690554e-01 -3.54235232e-01 ... -4.23384011e-01\r\n        -5.52129559e-02 -1.58944473e-01]\r\n       [-1.17037618e+00  3.93378854e-01 -2.55860239e-01 ... -1.89175576e-01\r\n        -3.52438241e-01  4.47871268e-01]\r\n       ...\r\n       [-6.99247956e-01 -4.01002228e-01 -9.55614746e-02 ... -5.12549996e-01\r\n        -2.93750733e-01  6.53216466e-02]\r\n       [-7.55659997e-01  6.40419498e-02 -3.80645603e-01 ... -3.38304751e-02\r\n        -2.81535774e-01  3.27474415e-01]\r\n       [-8.98592114e-01  6.04611039e-01 -2.70986766e-01 ... -3.77177030e-01\r\n         6.39083028e-01  1.36269689e-01]]\r\n    \r\n      [[-2.06768334e-01  4.33980107e-01 -2.79342651e-01 ... -5.97317517e-01\r\n        -3.16711366e-01 -3.49702388e-02]\r\n       [-8.91763270e-01  5.25228903e-02 -1.25209495e-01 ... -2.35961720e-01\r\n        -8.85192119e-03  2.04528570e-01]\r\n       [-6.51450276e-01 -3.80111784e-01 -7.60694802e-01 ... -1.16580009e+00\r\n         4.66391236e-01 -1.49288952e-01]\r\n       ...\r\n       [-8.66602838e-01 -5.96193299e-02 -1.27994597e-01 ... -4.30833064e-02\r\n         1.16992146e-01 -6.62802998e-03]\r\n       [-9.14786756e-01  1.67500958e-01 -2.82980531e-01 ... -6.82374537e-01\r\n        -1.63795985e-02  1.82587206e-01]\r\n       [-5.49697280e-01  6.72760680e-02 -7.42559731e-02 ... -1.59981385e-01\r\n         1.14814062e-02  2.45045304e-01]]\r\n    \r\n      [[-6.60416663e-01  4.52319860e-01  1.04573436e-01 ... -8.84432197e-01\r\n        -1.99507281e-01 -5.85323572e-02]\r\n       [-5.59800625e-01  4.29061443e-01  6.59240410e-02 ... -2.68544644e-01\r\n        -9.68336090e-02  1.81482792e-01]\r\n       [-5.66595554e-01  7.11271226e-01 -4.41362828e-01 ... -4.43202615e-01\r\n         8.01286697e-01 -1.05126597e-01]\r\n       ...\r\n       [-7.27143824e-01  4.69138294e-01 -3.05642545e-01 ... -3.89702499e-01\r\n        -9.79036614e-02  3.28408331e-01]\r\n       [-3.61066341e-01  4.04266417e-01 -2.91205943e-01 ...  5.48217893e-02\r\n        -9.46037620e-02 -1.06996156e-01]\r\n       [-6.50965929e-01  5.33826649e-01 -3.92165482e-01 ... -2.94952631e-01\r\n        -8.13799322e-01  1.03757977e-01]]]]\r\n    ====================================================================================================\r\n    x:\r\n    [[[[4.17022005e-01]\r\n       [7.20324493e-01]\r\n       [1.14374817e-04]\r\n       ...\r\n       [6.23672207e-01]\r\n       [7.50942434e-01]\r\n       [3.48898342e-01]]\r\n    \r\n      [[2.69927892e-01]\r\n       [8.95886218e-01]\r\n       [4.28091190e-01]\r\n       ...\r\n       [1.85762022e-02]\r\n       [7.00221437e-02]\r\n       [4.86345111e-01]]\r\n    \r\n      [[6.06329462e-01]\r\n       [5.68851437e-01]\r\n       [3.17362409e-01]\r\n       ...\r\n       [9.18601778e-01]\r\n       [4.02024891e-04]\r\n       [9.76759149e-01]]\r\n    \r\n      ...\r\n    \r\n      [[5.89549934e-01]\r\n       [3.89137609e-01]\r\n       [5.05975232e-01]\r\n       ...\r\n       [4.35888475e-01]\r\n       [7.89075202e-01]\r\n       [4.66467704e-01]]\r\n    \r\n      [[6.73554921e-01]\r\n       [8.84836452e-01]\r\n       [9.38138449e-01]\r\n       ...\r\n       [7.93970466e-01]\r\n       [2.13784215e-01]\r\n       [6.41105035e-01]]\r\n    \r\n      [[7.31134736e-01]\r\n       [9.50619892e-02]\r\n       [7.00729238e-02]\r\n       ...\r\n       [9.95522026e-01]\r\n       [4.81429517e-01]\r\n       [8.37812754e-01]]]\r\n    \r\n    \r\n     [[[6.03655452e-01]\r\n       [6.64374944e-01]\r\n       [2.72461392e-01]\r\n       ...\r\n       [1.14069927e-01]\r\n       [2.93705095e-01]\r\n       [8.78904978e-03]]\r\n    \r\n      [[2.53263696e-01]\r\n       [8.37712781e-01]\r\n       [8.07756027e-01]\r\n       ...\r\n       [7.37152445e-01]\r\n       [6.00521471e-01]\r\n       [7.37999367e-01]]\r\n    \r\n      [[3.75760828e-01]\r\n       [9.11106703e-01]\r\n       [8.72308594e-01]\r\n       ...\r\n       [9.80268932e-01]\r\n       [1.13198035e-01]\r\n       [4.65678949e-01]]\r\n    \r\n      ...\r\n    \r\n      [[4.94241516e-02]\r\n       [6.34450548e-01]\r\n       [8.93053413e-01]\r\n       ...\r\n       [7.97760317e-01]\r\n       [3.18871974e-01]\r\n       [6.47314782e-01]]\r\n    \r\n      [[4.65136696e-01]\r\n       [5.07669096e-01]\r\n       [4.23295851e-01]\r\n       ...\r\n       [2.98177206e-01]\r\n       [5.32380132e-01]\r\n       [6.12348886e-01]]\r\n    \r\n      [[2.58528146e-01]\r\n       [5.20561003e-02]\r\n       [7.82628170e-01]\r\n       ...\r\n       [8.26242775e-03]\r\n       [7.43071396e-01]\r\n       [3.29652868e-01]]]]\r\n", "comments": []}, {"number": 47602, "title": "Error to load a file related with cuda", "body": "\r\n**System information**\r\n- OS Platform and Distribution : Windows 10 Familly 1909\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install \r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.7\r\n- Installed using pip\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: 1660Ti\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI installed tensorflow with PIP.  Then I wanted to use use keras. I tried to execute a py file with this code : \r\n`from tensorflow.keras import layers\r\n`\r\n\r\nBut I got : \r\n\r\n> 2021-03-06 06:24:10.791991: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2021-03-06 06:24:10.804340: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\nAnd I have already done : \r\n\r\n> pip install tf-nightly-gpu\r\n\r\n\r\n", "comments": ["@LucasColas,\r\nEvery TensorFlow release is compatible with a certain CUDA/cuDNN version, for more information please take a look at the [tested build configurations](https://www.tensorflow.org/install/source_windows#gpu). \r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow_gpu-2.4.0 | 3.6-3.8 | MSVC 2019 | Bazel 3.1.0 | 8.0 | 11.0\r\ntensorflow_gpu-2.3.0 | 3.5-3.8 | MSVC 2019 | Bazel 3.1.0 | 7.6 | 10.1\r\ntensorflow_gpu-2.2.0 | 3.5-3.8 | MSVC 2019 | Bazel 2.0.0 | 7.6 | 10.1\r\n\r\nIn this case, can you please try installing **TensorFlow v2.4** with **CUDA 11.0** and **cuDNN 8** and check if you are facing the same error. Thanks!", "My CUDA version is 11.2", "Hi, I'm still waiting an answer.", "> My CUDA version is 11.2\r\n\r\n@LucasColas,\r\nSupport for CUDA 11.2 is being tracked in issue [#47568](https://github.com/tensorflow/tensorflow/issues/47568).\r\n\r\nAs mentioned in the previous comment, could you please try running the code either with \r\n\r\n- **TensorFlow v2.3**, **CUDA 10.1** and **cuDNN 7.6** \r\n\r\nor \r\n\r\n- **TensorFlow v2.4**, **CUDA 11.0** and **cuDNN 8.0**\r\n\r\nThanks!", "How can I change my Cuda version and cuDNN version ? \r\nThanks.", "@LucasColas,\r\nPlease uninstall the existing CUDA and cuDNN packages and follow [this guide](https://www.tensorflow.org/install/gpu#windows_setup) to install TensorFlow with GPU support. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47602\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47602\">No</a>\n"]}, {"number": 47601, "title": "Add missing includes to ensure that we export all the required headers.", "body": "These were needed when attempting to use the output folder from the project generation (with OPTIMIZED_KERNEL_DIR=cmsis_nn and TARGET=cortex_m_generic) to build binaries for the SParkfun Edge.\r\n\r\nWe do not have any CI that would catch such regressions. The issue at hand is that with large external libraries (such as CMSIS), it can becomes complex to figure out exactly what all the headers are.\r\n\r\nThis issue of managing third_party dependencies is more general that project generation so we are currently ok with not having CI coverage for the CMSIS as part of project generation.\r\n\r\nIn fact, in the case of the Sparkfun Edge, the Ambiq SDK packages its own version of (a small subset of) CMSIS headers. It is possible that folks might want to consolidate such third party dependencies and maintain their own version of CMSIS instead of relying on the headers /\r\nsources that are exported by the TFLM project generation scripts.\r\n\r\nProgress towards: #47413 ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47600, "title": "Update TOSA MLIR legalization documentation for current code", "body": "Signed-off-by: Kevin Cheng <kevin.cheng@arm.com>", "comments": ["Sorry that I should but I forgot to add @stellaraccident and @rsuderman as reviewers when I create the pull request.", "A quick note - these are just cleanups of the legalization document matching current code. There are new legalizations and supporting documentation updates to follow in further PRs. ", "Windows Bazel check seems failing. But I don't think there's any possibility that this change would affect that and causes the error. Is this a outrage on the CI system?"]}, {"number": 47599, "title": "XLA experimental_compile=True fails inside tf.data.Dataset", "body": "The `experimental_compile=True` option for `tf.function` works outside of a tf.data Dataset, but fails once I try to map it over a dataset.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef log10(x):\r\n    numerator = tf.math.log(x)\r\n    denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\r\n    return numerator / denominator\r\n\r\n@tf.function(experimental_compile=True)\r\ndef drc(\r\n    inputs,\r\n    threshold=-36.0,\r\n    release_time=80.0,\r\n    ratio=8.0,\r\n    makeup_gain=0.0,\r\n    sample_rate=44100,\r\n):\r\n    # downmix\r\n    buffer = tf.reduce_mean(inputs, axis=1, keepdims=True)\r\n    alpha_release = tf.exp(-1 / (0.001 * sample_rate * release_time))\r\n\r\n    y_prev = tf.constant([0.0], dtype=tf.float32)\r\n    length = tf.shape(inputs)[0]\r\n    c = tf.TensorArray(\r\n        dtype=tf.float32,\r\n        size=length,\r\n        element_shape=[1,]\r\n    )\r\n\r\n    for i in tf.range(length):\r\n        if tf.abs(buffer[i]) < 0.000001:\r\n            x_g = tf.constant([-120.0], dtype=tf.float32)\r\n        else:\r\n            x_g = tf.multiply(20.0, log10(tf.abs(buffer[i])))\r\n\r\n        if x_g > threshold:\r\n            y_g = tf.add(threshold, tf.divide(tf.subtract(buffer[i], threshold), ratio))\r\n        else:\r\n            y_g = x_g\r\n\r\n        x_l = tf.subtract(x_g, y_g)\r\n\r\n        y_l = tf.add(\r\n            tf.multiply(alpha_release, y_prev), tf.multiply(tf.subtract(1.0, alpha_release), x_l)\r\n        )\r\n        y_l = tf.math.pow(10.0, (makeup_gain - y_l) / 20.0)\r\n        c = c.write(i, y_l)\r\n        y_prev = y_l\r\n\r\n    out = c.stack()\r\n    return tf.stack([signal[:, 0] * tf.squeeze(out), signal[:, 1] * tf.squeeze(out)], axis=-1)\r\n\r\nsignal = tf.random.normal((100000, 2), dtype=tf.float32)\r\nout_tf = drc(signal)  # works\r\n\r\nds = tf.data.Dataset.from_tensor_slices([signal])\r\nds = ds.map(drc).batch(5)\r\nfor b in ds:\r\n  print(b)  # Fails\r\n```\r\nI get the following error:\r\n\r\n```\r\nInvalidArgumentError: Function invoked by the following node is not compilable: {{node PartitionedCall}} = PartitionedCall[Tin=[DT_FLOAT, DT_FLOAT], Tout=[DT_FLOAT], _XlaHasReferenceVars=false, _XlaMustCompile=true, _collective_manager_ids=[], _input_hostmem=[], _read_only_resource_inputs=[], config=\"\", config_proto=\"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0002\\002J\\0008\\001\\202\\001\\000\", executor_type=\"\", f=__inference_drc_250598[], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](args_0, unknown).\r\nUncompilable nodes:\r\nPartitionedCall: could not instantiate call: '__inference_drc_250598'\r\n\tStacktrace:\r\n\t\tNode: PartitionedCall, function: \r\n\r\n\t [[PartitionedCall]] [Op:MakeIterator]\r\n```\r\n\r\nIt works as expected if I set experimental_compile to False. I'm using tensorflow 2.4.0.", "comments": ["@lminer \r\nI ran the code shared and face a different error,please find the [gist here](https://colab.research.google.com/gist/Saduf2019/ed83ba0196042a2cef91f31ec5754f9e/untitled559.ipynb) and share all dependencies such that we can replicate the error reported.\r\n\r\nyou may also refer to these links and let us know if it helps, [link](https://github.com/tensorflow/tensorflow/issues/47211#issuecomment-783603301), #39852\r\n\r\n", "@Saduf2019 sorry there was a function missing. Just added it. ", "@lminer\r\nGlad the issue is resolved, can you please move this to closed status.", "@Saduf2019 it is not resolved. The error is exactly as I reported it. I just needed to add that function to get the correct error.", "I ama ble to replicate this in tf 2.3, tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b55da8652e148ea3f3998f9158b20149/untitled560.ipynb).", "@aaudiber any idea on how to fix this?", "Sorry, not familiar with experimental_compile. Reassigning to @cheshire who may have more insight about what causes the error.", "Seems the custom cache used by tf.data does not work with jit-compiled functions. Tracking internally in b/183061735", "@cheshire  is there any workaround?", "Actually this works with latest tf-nightly. Just checked using the colab link above.", "Closing as \"fixed in nightly\".", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47599\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47599\">No</a>\n"]}, {"number": 47598, "title": "Add use_bfc_allocator parameter to StreamExecutor C API", "body": "`use_bfc_allocator` parameter will decide whether to wrap pluggable device `SubAllocator` with BFC allocator or not.\r\n\r\nI am thinking it can be used as follows:\r\nSee lines here:\r\nhttps://github.com/tensorflow/tensorflow/blob/2c31420a533f839d19af384c3d83038fde3583f8/tensorflow/core/common_runtime/pluggable_device/pluggable_device_process_state.cc#L115\r\n\r\nThey should be replaced with something like\r\n```cpp\r\nAllocator * device_allocator = nullptr;\r\nif (use_bfc_allocator) {\r\n  PluggableDeviceBFCAllocator* device_bfc_allocator =\r\n        new PluggableDeviceBFCAllocator(\r\n            sub_allocator, total_bytes, options,\r\n            strings::StrCat(\"PluggableDevice_\", tf_device_id.value(), \"_bfc\"));\r\n  device_allocator = device_bfc_allocator;\r\n} else {\r\n  PluggableDeviceSimpleAllocator* simple_allocator = new PluggableDeviceSimpleAllocator(sub_allocator);\r\n  device_allocator = simple_allocator;\r\n}\r\n```\r\n\r\n`PluggableDeviceSimpleAllocator` should just be a minimal wrapper over the `sub_allocator` (currently not implemented as a part of this PR). This seems much simpler to me than what I was thinking of originally (sorry for the delay).\r\n\r\nPTAL @penpornk @kulinseth, @jzhoulon\r\n\r\n\r\n", "comments": ["@annarev  Thanks for the PR! \r\nDo you mean the PluggableDeviceSimpleAllocator will directly call the stream_exec_->AllocateArray(), which calls `SP_StreamExecutor::allocate(const SP_Device* device, uint64_t size, int64_t memory_space,SP_DeviceMemoryBase* mem )`,  and plugin authors decides whether `allocate` is build on top of a custom allocator? if it is, I think it is better to add an field such as void* custom_allocator in SP_Device? since the allocator should be per-device? Thanks \r\n", "Thanks @annarev for the PR. SimpleAllocator can be something straightforward calling into AllocateRaw and that would be fine for us. We had brief [discussion](https://github.com/tensorflow/community/pull/262#issuecomment-686301393) of this during the design phase.  To keep it simple we can leverage `SubAllocator` currently. After the pluggable device implementation merges we can use the `use_bfc_allocator` flag to pass this information in device_process_state..\r\n\r\nThere are few places in pluggable_device_process_state like `AllocatorParts` will require changes to accomodate the SimpleAllocator design, currently it assumes the Device memory allocator is BFC. I can upload a PR which I have tested locally works with our design. Please let me know. ", "> @annarev Thanks for the PR!\r\n> Do you mean the PluggableDeviceSimpleAllocator will directly call the stream_exec_->AllocateArray(), which calls `SP_StreamExecutor::allocate(const SP_Device* device, uint64_t size, int64_t memory_space,SP_DeviceMemoryBase* mem )`, and plugin authors decides whether `allocate` is build on top of a custom allocator? if it is, I think it is better to add an field such as void* custom_allocator in SP_Device? since the allocator should be per-device? Thanks\r\n\r\n@jzhoulon Sounds about right. `use_bfc_allocator` will decide whether to use a simple allocator or a BFC allocator. So, a plugin that wants to use their own custom allocation strategy and doesn't want to wrap with BFC, can set this to true. Do you mean add `use_bfc_allocator` parameter per device? I was just thinking having one setting for all device types is easier to pass through on TF end. Do we have a usecase for using or not using bfc allocator per device?\r\n\r\n\r\n> Thanks @annarev for the PR. SimpleAllocator can be something straightforward calling into AllocateRaw and that would be fine for us. We had brief [discussion](https://github.com/tensorflow/community/pull/262#issuecomment-686301393) of this during the design phase. To keep it simple we can leverage `SubAllocator` currently. After the pluggable device implementation merges we can use the `use_bfc_allocator` flag to pass this information in device_process_state..\r\n\r\n@kulinseth Yep that makes sense. I misread it back then as custom allocator wrapper itself being something customized by the plugin.\r\n\r\n> There are few places in pluggable_device_process_state like `AllocatorParts` will require changes to accomodate the SimpleAllocator design, currently it assumes the Device memory allocator is BFC. I can upload a PR which I have tested locally works with our design. Please let me know.\r\n\r\nSounds good, you can upload the PR and we can comment on it if anything needs changing.\r\n\r\n", "> > @annarev Thanks for the PR!\r\n> > Do you mean the PluggableDeviceSimpleAllocator will directly call the stream_exec_->AllocateArray(), which calls `SP_StreamExecutor::allocate(const SP_Device* device, uint64_t size, int64_t memory_space,SP_DeviceMemoryBase* mem )`, and plugin authors decides whether `allocate` is build on top of a custom allocator? if it is, I think it is better to add an field such as void* custom_allocator in SP_Device? since the allocator should be per-device? Thanks\r\n> \r\n> @jzhoulon Sounds about right. `use_bfc_allocator` will decide whether to use a simple allocator or a BFC allocator. So, a plugin that wants to use their own custom allocation strategy and doesn't want to wrap with BFC, can set this to true. Do you mean add `use_bfc_allocator` parameter per device? I was just thinking having one setting for all device types is easier to pass through on TF end. Do we have a usecase for using or not using bfc allocator per device?\r\n\r\nNo, I mean an opaque handle (void* custom_allocator) in SP_Device. See the following code example:\r\n```\r\nvoid create_device (SP_Platform platform, SE_CreateDeviceParams params, status) {\r\n params->custom_allocator = new CustomAllocator()\r\n}\r\n\r\nvoid allocate(SP_Device device, unit64_t size, ...) {\r\n  CustomAllocator* allocator = static_cast<CustomAllocator*>(device->custom_allocate);\r\n  allocator->allocate_raw(size, ...)\r\n} \r\n```\r\n\r\n", "> > > @annarev Thanks for the PR!\r\n> > > Do you mean the PluggableDeviceSimpleAllocator will directly call the stream_exec_->AllocateArray(), which calls `SP_StreamExecutor::allocate(const SP_Device* device, uint64_t size, int64_t memory_space,SP_DeviceMemoryBase* mem )`, and plugin authors decides whether `allocate` is build on top of a custom allocator? if it is, I think it is better to add an field such as void* custom_allocator in SP_Device? since the allocator should be per-device? Thanks\r\n> > \r\n> > \r\n> > @jzhoulon Sounds about right. `use_bfc_allocator` will decide whether to use a simple allocator or a BFC allocator. So, a plugin that wants to use their own custom allocation strategy and doesn't want to wrap with BFC, can set this to true. Do you mean add `use_bfc_allocator` parameter per device? I was just thinking having one setting for all device types is easier to pass through on TF end. Do we have a usecase for using or not using bfc allocator per device?\r\n> \r\n> No, I mean an opaque handle (void* custom_allocator) in SP_Device. See the following code example:\r\n> \r\n> ```\r\n> void create_device (SP_Platform platform, SE_CreateDeviceParams params, status) {\r\n>  params->custom_allocator = new CustomAllocator()\r\n> }\r\n> \r\n> void allocate(SP_Device device, unit64_t size, ...) {\r\n>   CustomAllocator* allocator = static_cast<CustomAllocator*>(device->custom_allocate);\r\n>   allocator->allocate_raw(size, ...)\r\n> } \r\n> ```\r\n\r\nI think setting a custom allocator makes things more complicated. I think we should do it only if we think of a usecase that cannot be covered by just implementing `allocate`/`deallocate` in the StreamExecutor API: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/stream_executor/stream_executor.h;l=245.\r\n\r\n", "> I think setting a custom allocator makes things more complicated. I think we should do it only if we think of a usecase that cannot be covered by just implementing `allocate`/`deallocate` in the StreamExecutor API: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/stream_executor/stream_executor.h;l=245.\r\n\r\nAgreed, I can queue up the PR after #45784 merges.\r\n", "> Agreed, I can queue up the PR after #45784 merges.\r\n\r\nThat would be awesome. Thank you very much, Kulin!"]}, {"number": 47597, "title": "Set specific weights to non-trainable instead of the whole layer", "body": "I'm trying to find a way to turn specific weights (variables) from trainable to non-trainable. For example, let's say I load a pre-trained model that includes a `Dense` layer and want to keep the `W` matrix trainable while disabling the bias term. \r\n\r\nI see only examples of how to turn a whole layer:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Make a model with 2 layers\r\nlayer1 = tf.keras.layers.Dense(3, activation=\"relu\")\r\nlayer2 = tf.keras.layers.Dense(3, activation=\"sigmoid\")\r\nmodel = tf.keras.Sequential([tf.keras.Input(shape=(3,)), layer1, layer2])\r\n\r\n# Freeze the first layer\r\nlayer1.trainable = False\r\n\r\nmodel.summary()\r\n```\r\n\r\nReturns:\r\n\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense (Dense)                (None, 3)                 12        \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 3)                 12        \r\n=================================================================\r\nTotal params: 24\r\nTrainable params: 12\r\nNon-trainable params: 12\r\n_________________________________________________________________\r\n```\r\n\r\nWhat if, I want to do something like:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Make a model with 2 layers\r\nlayer1 = tf.keras.layers.Dense(3, activation=\"relu\")\r\nlayer2 = tf.keras.layers.Dense(3, activation=\"sigmoid\")\r\nmodel = tf.keras.Sequential([tf.keras.Input(shape=(3,)), layer1, layer2])\r\n\r\n# Freeze the first layer\r\nlayer1.bias.trainable = False\r\n\r\nmodel.summary()\r\n\r\n```\r\n\r\nThis returns the error:\r\n\r\n\r\n```\r\nlayer1.bias.trainable = False.  AttributeError: can't set attribute\r\n```\r\n\r\n\r\n", "comments": ["@iliaschalkidis,\r\nCould you please take a look at [this](https://stackoverflow.com/a/65488324) similar StackOverflow query and let us know if this is what you were looking for. Thanks!", "Hi @amahendrakar,\r\n\r\nUnfortunately not. In this example, we just do not use a bias term at all. In my case, I just want to pick up a specific weight, for example, the bias term or the `W` matrix of a `Dense` layer, and turn it to no trainable, not ignore/delete it.\r\n\r\nThanks.", "@ymodak,\r\nI was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c980b54c4b3d0a8990630585cb362775/47597.ipynb). Thanks!", "Perhaps you can set arg `use_bias=False` in the layers. See [gist](https://colab.research.google.com/gist/ymodak/1bb3d6e68e98b94c00a869ecddd46e6d/untitled17.ipynb).\r\nThanks!", "Hi @ymodak, @amahendrakar already proposed this solution and this is not the case. If we set `use_bias=False`, then the layer does not include a bias term. My questions is how to turn any variable, such as a bias term, to non trainable, not omit the parameter.\r\n\r\nThanks.", "It's a vital function and it's interesting that it still doesn't.", "@ymodak \r\nAny progress on this? thanks! ", "@iliaschalkidis This is more of a feature (and not a bug with TF/keras)\r\n\r\nIf you still interested in this feature, open the issue as a feature in keras-team/keras repo as Keras development moved there to focus entirely on Keras. Also, mention more details about your use case where you would like to have this functionality.\r\n\r\nKeras development moved to another repository to focus on only keras. Could you please repost this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47597\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47597\">No</a>\n", "I almost had the same problem and I solved it this way : \r\nconv1.non_trainable_weights.append(conv1.trainable_weights.pop(0)).\r\n\r\nIt was useful for me in order to prevent depthwise weights to train in a SeparableConv1D."]}, {"number": 47596, "title": "Drastic changes in predictions during training and validation on the SAME data", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am referring to the [official example on knowledge distillation](https://keras.io/examples/vision/knowledge_distillation/) from Keras. During training the student I get expected results. However just after an iteration of training when I use the SAME dataset to validate the model, I get drastically different results. \r\n\r\n**Describe the expected behavior**\r\n\r\nThe results on the SAME data should be identical. \r\n\r\n**Standalone code to reproduce the issue**\r\n[Colab Notebook](https://colab.research.google.com/gist/sayakpaul/f1b528c77d26ac951621c2e2258c7b47/verification.ipynb).\r\n", "comments": ["Problem solved!\r\n\r\nI suspect the issue was stemming from the usage of `tf.keras.applications.mobilenet_v2.preprocess_input`. I replaced it like so - \r\n\r\n```python\r\ndef get_student_model():\r\n    base_model = MobileNetV2(weights=None, include_top=False,\r\n        input_shape=(224, 224, 3))\r\n    \r\n    inputs = Input(shape=(224, 224, 3))\r\n    x = experimental.preprocessing.Rescaling(1./127.5, offset=-1)(inputs)\r\n    x = base_model(x)\r\n    x = GlobalAveragePooling2D()(x)\r\n    x = Dense(30, activation=\"softmax\")(x)\r\n    classifier = tf.keras.Model(inputs=inputs, outputs=x)\r\n    \r\n    return classifier\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47596\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47596\">No</a>\n"]}, {"number": 47595, "title": "Tensorflow Lite Android Object Detection \u2014 Mobile SSD models are expected to have exactly 4 outputs, found 8", "body": "**Problem Encountered:**\r\n\r\n> E/AndroidRuntime:` FATAL EXCEPTION: main Process: org.tensorflow.lite.examples.detection, PID: 14719 java.lang.AssertionError: Error occurred when initializing ObjectDetector: Mobile SSD models are expected to have exactly 4 outputs, found 8\r\n\r\n**Problem Description**\r\n- Android Application Source: TensorFlow Lite Object Detection Example from Google\r\n- Error shown when starting the Example Application\r\n\r\n**Model Description**\r\n- Custom Model Used? **YES**\r\n- Pre-trained Model Used: ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\r\n- Inference type: FLOAT\r\n- Number of classes: 4\r\n\r\n**System Information**\r\n- OS Platform and Distribution: ( Linux Ubuntu 20.14)\r\n- TensorFlow Version: 2.4.1\r\n- TensorFlow installed from: Pip\r\n\r\n**Saved Model conversion commands used**\r\n\r\n1. Saved_Model.pb export command:\r\n\r\n> python ./exporter_main_v2.py\r\n--input_type image_tensor\r\n--pipeline_config_path ./models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/pipeline.config\r\n--trained_checkpoint_dir ./models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8\r\n--output_directory exported_models/tflite\r\n\r\n2. Convert saved model (.pb) to tflite command:\r\n> toco\r\n--saved_model_dir ./exported-models/tflite/saved_model\r\n--emit-select-tf-ops true\r\n--allow_custom_ops\r\n--graph_def_file ./exported-models/tflite/saved_model/saved_model.pb\r\n--output_file ./exported-models/tflite/tflite/detect.tflite\r\n--input_shapes 1,300,300,3\r\n--input_arrays normalized_input_image_tensor\r\n--output_arrays 'TFLite_Detection_PostProcess\u2019,\u2019TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'\r\n--inference_type=FLOAT\r\n--allow_custom_ops\r\n\r\n**Remarks**\r\nI am trying to use a trained custom model on the Google TensorFlow lite provided example. Just that every time I open the application, it returns such an error, Mobile SSD models are expected to have exactly 4 outputs, found 8. The model is trained to identify 4 classes, all stated in the labelmap.txt and pipeline config.\r\n\r\nDoes anybody have any clue about this error?", "comments": ["@lintian06 could you take a look at this issue?", "@lintian06 \r\nHello,\r\nMay I ask if I want to prevent this from happening, shall I change to other models? \r\nOr it is not model-related issue?", "Hi @fonghyalex,\r\n\r\nAs you've customized the model other than our built-in model, the model is different from the previous one because it doesn't contain model [metadata](https://www.tensorflow.org/lite/inference_with_metadata/overview). Therefore, it fails the previous assertion. (The inputs and outputs may be different too, which I cannot infer from your report.)\r\n\r\nFirst, please change the related code to run with the interpreter directly. In [build.gradle](\r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/build.gradle#L40),\r\n```\r\ninterpreter {\r\n    getIsDefault().set(true)\r\n    dimension \"tfliteInference\"\r\n}\r\ntaskApi {\r\n    // getIsDefault().set(true)  <- default\r\n    dimension \"tfliteInference\"\r\n}\r\n```\r\nAfter using interpreter, you can customize inputs and outputs for [TFLiteObjectDetectionAPIModel](https://github.com/tensorflow/examples/blob/cbe96424b6b930fd74dbd4ef2b1e826a031a3554/lite/examples/object_detection/android/lib_interpreter/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L56) if it is needed.\r\n\r\nSecond, please double confirm whether `--emit-select-tf-ops true` and `--allow_custom_ops` are do needed. It is because `select-tf-ops` and `custom ops`, as names indicate, require more than ordinary TFLite runtime and built-in TFLite ops. Please try removing to see if it still produces correct a model.\r\n\r\n*Notes for your information.*\r\n- For [select TF Ops](https://www.tensorflow.org/lite/guide/ops_select#android_aar), you need to add `org.tensorflow:tensorflow-lite-select-tf-ops` in aar. It means including selected TF ops in the TFLite runtime.\r\n\r\n- For [custom ops](https://www.tensorflow.org/lite/guide/ops_custom), it requires you to include your own implementation of custom ops into TFLite.\r\n\r\n------------------------------\r\nHope it helps!", "@lu-wang-g for the visibility of metadata.", "@lintian06 \r\nThank you so much for your prompt reply! =)\r\n\r\nAfter carefully following your instruction and the tutorial of [Tensorflow Example](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/README.md), I have exported the .tflite to the application once again.\r\n\r\nJust that, I have encountered another problem:\r\n> java.lang.AssertionError: Error occurred when initializing ObjectDetector: Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.\r\n\r\nFrom the error message, I learnt that the metadata is not being set up properly. Yet, I can hardly find any tutorial on setting the metadata from the Tensorflow Example instructions. \r\n\r\n**As the custom model is actually downloaded from the TensorFlow 2 Model zoo, may I ask how should I modify the metadata?**", "Please check my answer [here](https://stackoverflow.com/a/64493506/11031225) about how to use the MetadataWriter Library. We'll publish the formal documentation soon. Please stay tuned. Thanks.", "@lu-wang-g\r\nHuge thanks!\r\nI have tried using this script yesterday and the script returned a message similar to \"not a valid tflite file\". I believe there may be something wrong when I am converting the model.\r\nIn order to fully achieve the expected website from the [example](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/README.md)  instruction. I have decided to train another model with TensorFlow v1 and perform the conversion.\r\n\r\nAs I do not have any furthers questions up to now, I'll close the issue for now and open it again once there is any update.\r\n@abattery @lintian06 @lu-wang-g Thank you so much for helping me out! =)", "After further study, I believe the aforementioned issue was raised since the model has 8 tensors output but the Android application written in Java can only support 4 tensors output (at least the example provided by Google only supports 4 tensors output)\r\n\r\nI am not very certain about the number of tensors output on different models. So far as I understood and messed around with different models, models with fixed_shape_resizer of 640 x 640 are likely to require more than 4 tensors output ( 8 tensors output usually), which is not compatible with the Android application written in Java.\r\n\r\nFor any amateur users like me, please find the following prerequisites to use your custom model in the [Android application](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md)\r\nSuggested Setup ( Assume you are using TensorFlow version >= 2.3):\r\n\r\n- TensorFlow Model: *SSD model with fixed_shape_resizer of 320 x 320*\r\n(In my case, SSD MobileNet v2 320x320 works perfectly fine)\r\n(The tensors output has to be 4)\r\n\r\n- *Colab* ( Perfect for model training and conversion)\r\n( I've tried to perform the training and conversion on both Linux and Windows platform on my local machine, the incompatibility of different tools and package gives me a headache. I turned out using Colab to perform the training and conversion. It is much more powerful and has great compatibility with those training tools and script.)\r\n\r\n- The [metadata writer library](https://stackoverflow.com/questions/64097085/issue-in-creating-tflite-model-populated-with-metadata-for-object-detection/64493506#64493506) that was written by @lu-wang-g\r\n(In my case, after converting the trained model to .tflite, if you directly migrate the .tflite model to the Android application, the application will report tons of problem regarding the config of the .tflite model. Assume if you trained and converted the model correctly, all you need is the metadata writer library above. It will automatically configure the metadata for you according to the .tflite model. Then you can directly migrate the model to the application.)", "Using TFLite_Detection_PostProcess should only create a model with 4 output tensors instead of 8. I guess something was wrong during the conversion from TF to TFLite. Can you please share the script and the model?\r\n\r\nLoop in @srjoglekar246 to verify the conversion step.", "@lu-wang-g \r\nThis is the [Colab scripts](https://colab.research.google.com/drive/1bhf36Pk-YnFTPj7O_d3H0q9DYzLXfa45?usp=sharing) I have used.\r\n\r\nPlease note that I have git cloned the [TensorFlow Model Garden](https://github.com/tensorflow/models) and the [Metadata writer library](https://github.com/tensorflow/tflite-support/tree/master/tensorflow_lite_support/metadata/python/metadata_writers) to the drive beforehand.\r\n\r\nModel used:\r\nSSD MobileNet v2 320x320\r\n", "@fonghyalex \r\n\r\n1. Your checkpoint path to `export_tflite_graph_tf2` seems wrong. It should be `Model/ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint` ?\r\n\r\n2. Not sure if you can convert TF2 with quantization like that. Can you try `tflite_convert` without all the quantization parameters, and see things work on float? If yes, then the quantization params might not work. I usually use the [Python API](https://www.tensorflow.org/lite/performance/post_training_quantization) for quantization.\r\n\r\n", "@srjoglekar246 \r\n\r\nThank you for your message. I am sorry that I forgot to mention that I have successfully converted the TensorFlow model to tflite and it is working fine on the Java application.\r\n\r\nFor the post quantization part, may I ask if the correct flow is that the model has to be first quantized, then convert to the tflite format? After post-quantization, I can then add all the quantization parameter to perform tflite conversion?\r\n\r\nI will do a test on the above question tomorrow. Thanks!\r\nShall I close the issue?", "you need to quantize while converting. See the documentation for Python API I linked.", "@srjoglekar246 \r\n\r\nThank you for your prompt reply. \r\n\r\nI will follow the documentation to perform the quantization. Huge thanks!\r\n\r\nAs the issue in the title is solved now, I am going to close this issue. Thanks, everyone!", "@fonghyalex \r\nHi Dear, I'm still stuck on this problem. can you share your solution with me so I run my quantized model on android? Thanks a Lot."]}, {"number": 47594, "title": "make native bazel build work on Apple Silicon", "body": "make `bazel --config opt //tensorflow/tools/pip_package:build_pip_package` work\r\n1. include AArch64 stuff for XLA\r\n2. don't build mkl_dnn\r\n\r\nWith recent bazel arm64 binary (tested with [1]), it's possible to build pip wheel.\r\nPreviously, you need x86_64 bazel, see #45404. \r\n\r\n[1] https://github.com/bazelbuild/bazel/commit/492829", "comments": ["@freedomtan very nice.\r\nverifying this by building `bazel(arm64)` on **M1** host (latest master branch) using:\r\n\r\n> bazel build -c opt --cpu=darwin-arm64 --apple_platform_type=macos //src:bazel\r\n\r\nas a test, I tried with `bazel-3.7.1-darwin-arm64` first using:\r\n\r\n> bazel-3.7.1-darwin-arm64 --config opt //tensorflow/tools/pip_package:build_pip_package\r\n> bazel-3.7.1-darwin-arm64 --config opt --config=macos_arm64 //tensorflow/tools/pip_package:build_pip_package\r\n\r\nboth resulted in same errors. reporting back once I can try again with the latest master of `bazel` you mentioned", "@simonmaurer As far as I can tell, neither 3.7.x nor 4.0 arm64 bazel works. That's why I went for bazel master branch. Hopefully, 4.1.0 or 4.0.x will work.", "@freedomtan so I can confirm that as of [bazel #4928295](https://github.com/bazelbuild/bazel/commit/4928295b236ec8f590a7e9d863502bc2f50a77d9) it is possible to build the pip package on a M1 host.\r\n\r\n> bazel- --config opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\ndoing some tests with `python3` now.\r\nthe build files are not correctly named yet using:\r\n\r\n> ./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg\r\n\r\n`build/bdist.macosx-10.14.6-arm64/wheel/tensorflow/*`\r\nwhere it should be `build/bdist.macosx-11.0-arm64/wheel/tensorflow/*`\r\n\r\nbut the resulting wheel is correctly named `tensorflow-2.5.0-cp38-cp38-macosx_11_0_arm64.whl`.\r\ninstalling it with:\r\n> pip3 install -U /tmp/tensorflow_pkg/tensorflow-2.5.0-cp38-cp38-macosx_11_0_arm64.whl\r\n\r\nresulted in errors mainly due to _grpcio_, _h5py_ and _numpy_ (the other dependant packages are available through `pip3`) :\r\n`ERROR: Could not find a version that satisfies the requirement h5py~=3.1.0 (from tf-nightly)\r\nERROR: No matching distribution found for h5py~=3.1.0`\r\n\r\ncould we rely on installing the dependant packages manually ?\r\n> pip3 install -U /tmp/tensorflow_pkg/tensorflow-2.5.0-cp38-cp38-macosx_11_0_arm64.whl --no-dependencies", "@freedomtan did you manage to install it properly using `pip3`? what package versions did you use?\r\nfor now I got it working by manually installing binaries grpcio/h5py from the [tensorflow_macos](https://github.com/apple/tensorflow_macos/tree/master/scripts) download script\r\n\r\nsuper nice. no errors with respect to XLA (`Symbol not found: _LLVMInitializeAArch64AsmPrinter`) when doing the `import tensorflow as tf` statement in python ;) check comments [jax#5501](https://github.com/google/jax/issues/5501#issuecomment-787433899)/[tensorflow#45404](https://github.com/tensorflow/tensorflow/pull/45404#issuecomment-760484477)", "@simonmaurer my environ:\r\n1. python3 venv with python3 from Xcode command line tools\r\n2. macports for many open source components\r\n\r\nAs you said the package seems to be properly named as `tensorflow-2.5.0-cp38-cp38-macosx_11_0_arm64.whl`.\r\n\r\nI have to install many dependencies when installing the pip wheel. And yes, I have to \"manually\" build grpcio (because of [zlib and openssl](https://github.com/grpc/grpc/issues/25082)) and h5py (because of macport's paths), which I learned how to do when I installed #45404 based package before.", "@freedomtan Can you please resolve conflicts? Thanks!", "@gbaned done. Thanks for reminding.", "@freedomtan Have you tried the latest TF master with your merge to HEAD? It complains about \r\n\r\n```\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /Users/byronyi/Develop/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /Users/byronyi/Develop/tensorflow/tensorflow/workspace0.bzl:105:34: in workspace\r\n  /private/var/tmp/_bazel_byronyi/3c78a9a34f8036289e063b4c27e13075/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /private/var/tmp/_bazel_byronyi/3c78a9a34f8036289e063b4c27e13075/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nERROR: /private/var/tmp/_bazel_byronyi/3c78a9a34f8036289e063b4c27e13075/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\n```\r\n\r\nUsing your original branch based on 184cf2d642885013d533227d189232d9a5b7ffab works (but it has other issues I probably haven't figured out). Perhaps @chsigg changed `.bazelrc` and other configs during this time and broke your patch? c0b4a3a1fdbd619bd005067f6042ee24bcb59192 looks suspicious, but it could be other ones, too.\r\n\r\ncc @mihaimaruseac I just test on my M1 and it doesn't work on HEAD.", "I'm not sure which build command you expect to work and a full log with `--announce_rc` would help.\r\n\r\nI would have expected you would like to build with `--config=macos_arm64`. I removed the `--noenable_platform_specific_config` + `--config=macos` combo (the former prevents the implicit `--config=macos` assuming bazel on M1 still detects the host system as macos, the second one adds it explicitly) triggered by `--config=macos_arm64` assuming it's a no-op.\r\n\r\nIt seems like some of you are building with just `--cpu=darwin_arm64`, which should be equivalent now.\r\n\r\nThe best way to tell what makes the difference is to build with `--announce_rc` and compare the working and broken state.", "@byronyi and @chsigg thanks, I'll check against HEAD to see how to make it work.", "@freedomtan mind to share the Bazel build procedure for you to get https://github.com/bazelbuild/bazel/commit/492829? I am trying to build from HEAD but it doesn't work. ", "@byronyi  for https://github.com/bazelbuild/bazel/commit/492829\r\n1. git clone and checkout it\r\n2. with bazel-3.7.2-arm64 I built previously, `bazel-3.7.2-arm64 build //src/bazel` and wait a couple minutes, then there is `bazel-bin/src/bazel`\r\n3. with that binary, I was able to build a working tf pip wheel package from tf master about 2 weeks ago by `${BAZEL_SRC}/bazel-bin/src/bazel --config opt //tensorflow/tools/pip_package:build_pip_package`", "@byronyi FYI. I could build (`bazel clean --expunge; bazel --config opt //tensorflow/tools/pip_package:build_pip_package`) recent tf master (8077fe8b1b4a3c170a94b74d8c351c247e64a53c) with https://github.com/bazelbuild/bazel/commit/492829.", "> @byronyi FYI. I could build (`bazel clean --expunge; bazel --config opt //tensorflow/tools/pip_package:build_pip_package`) recent tf master ([8077fe8](https://github.com/tensorflow/tensorflow/commit/8077fe8b1b4a3c170a94b74d8c351c247e64a53c)) with [bazelbuild/bazel@492829](https://github.com/bazelbuild/bazel/commit/492829).\r\n\r\n@freedomtan Mind to share the output of `bazel query --output=build  @local_config_cc_toolchains//:all`?\r\n\r\nMine is \r\n\r\n```\r\n$ bazel query --output=build  @local_config_cc_toolchains//:all\r\n# /private/var/tmp/_bazel_byronyi/3c78a9a34f8036289e063b4c27e13075/external/local_config_cc_toolchains/BUILD:3:10\r\ntoolchain(\r\n  name = \"cc-toolchain-darwin\",\r\n  exec_compatible_with = [\"@platforms//cpu:aarch64\", \"@platforms//os:osx\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//cpu:aarch64\", \"@platforms//os:osx\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-darwin\",\r\n)\r\n# Rule cc-toolchain-darwin instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_byronyi/3c78a9a34f8036289e063b4c27e13075/external/local_config_cc_toolchains/BUILD:3:10 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_byronyi/3c78a9a34f8036289e063b4c27e13075/external/local_config_cc_toolchains/BUILD:11:10\r\ntoolchain(\r\n  name = \"cc-toolchain-armeabi-v7a\",\r\n  exec_compatible_with = [\"@platforms//cpu:aarch64\", \"@platforms//os:osx\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//cpu:arm\", \"@platforms//os:android\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-armeabi-v7a\",\r\n)\r\n# Rule cc-toolchain-armeabi-v7a instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_byronyi/3c78a9a34f8036289e063b4c27e13075/external/local_config_cc_toolchains/BUILD:11:10 in <toplevel>\r\n```", "I probably need to install full Xcode instead of the Xcode command line tools...", "Yes, please check your environment. It seems bazel could not find your toolchain.\r\n\r\n```\r\n$ bazel query --output=build  @local_config_cc_toolchains//:all\r\nDEBUG: /Users/freedom/work/tensorflow/tensorflow/version_check.bzl:42:14: \r\nCurrent Bazel is not a release version, cannot check for compatibility.\r\nDEBUG: /Users/freedom/work/tensorflow/tensorflow/version_check.bzl:43:14: Make sure that you are running at least Bazel 1.0.0.\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-armeabi-v7a-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//cpu:arm\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-armeabi-v7a\",\r\n)\r\n# Rule cc-toolchain-armeabi-v7a-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-armeabi-v7a-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//cpu:arm\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-armeabi-v7a\",\r\n)\r\n# Rule cc-toolchain-armeabi-v7a-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-darwin_arm64-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-darwin_arm64\",\r\n)\r\n# Rule cc-toolchain-darwin_arm64-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-darwin_arm64-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-darwin_arm64\",\r\n)\r\n# Rule cc-toolchain-darwin_arm64-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-darwin_arm64e-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-darwin_arm64e\",\r\n)\r\n# Rule cc-toolchain-darwin_arm64e-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-darwin_arm64e-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-darwin_arm64e\",\r\n)\r\n# Rule cc-toolchain-darwin_arm64e-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-darwin_x86_64-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-darwin_x86_64\",\r\n)\r\n# Rule cc-toolchain-darwin_x86_64-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-darwin_x86_64-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-darwin_x86_64\",\r\n)\r\n# Rule cc-toolchain-darwin_x86_64-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-ios_arm64-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:aarch64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-ios_arm64\",\r\n)\r\n# Rule cc-toolchain-ios_arm64-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-ios_arm64-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:aarch64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-ios_arm64\",\r\n)\r\n# Rule cc-toolchain-ios_arm64-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-ios_arm64e-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:aarch64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-ios_arm64e\",\r\n)\r\n# Rule cc-toolchain-ios_arm64e-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-ios_arm64e-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:aarch64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-ios_arm64e\",\r\n)\r\n# Rule cc-toolchain-ios_arm64e-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-ios_armv7-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:arm\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-ios_armv7\",\r\n)\r\n# Rule cc-toolchain-ios_armv7-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-ios_armv7-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:arm\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-ios_armv7\",\r\n)\r\n# Rule cc-toolchain-ios_armv7-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-ios_i386-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:x86_32\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-ios_i386\",\r\n)\r\n# Rule cc-toolchain-ios_i386-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-ios_i386-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:x86_32\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-ios_i386\",\r\n)\r\n# Rule cc-toolchain-ios_i386-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-ios_x86_64-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:x86_64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-ios_x86_64\",\r\n)\r\n# Rule cc-toolchain-ios_x86_64-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-ios_x86_64-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:x86_64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-ios_x86_64\",\r\n)\r\n# Rule cc-toolchain-ios_x86_64-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-tvos_arm64-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:aarch64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-tvos_arm64\",\r\n)\r\n# Rule cc-toolchain-tvos_arm64-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-tvos_arm64-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:aarch64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-tvos_arm64\",\r\n)\r\n# Rule cc-toolchain-tvos_arm64-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-tvos_x86_64-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:x86_64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-tvos_x86_64\",\r\n)\r\n# Rule cc-toolchain-tvos_x86_64-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-tvos_x86_64-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:x86_64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-tvos_x86_64\",\r\n)\r\n# Rule cc-toolchain-tvos_x86_64-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-watchos_arm64_32-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:aarch64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-watchos_arm64_32\",\r\n)\r\n# Rule cc-toolchain-watchos_arm64_32-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-watchos_arm64_32-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:aarch64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-watchos_arm64_32\",\r\n)\r\n# Rule cc-toolchain-watchos_arm64_32-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-watchos_armv7k-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:arm\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-watchos_armv7k\",\r\n)\r\n# Rule cc-toolchain-watchos_armv7k-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-watchos_armv7k-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:arm\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-watchos_armv7k\",\r\n)\r\n# Rule cc-toolchain-watchos_armv7k-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-watchos_i386-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:x86_32\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-watchos_i386\",\r\n)\r\n# Rule cc-toolchain-watchos_i386-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-watchos_i386-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:x86_32\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-watchos_i386\",\r\n)\r\n# Rule cc-toolchain-watchos_i386-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-watchos_x86_64-aarch64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:aarch64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:x86_64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-watchos_x86_64\",\r\n)\r\n# Rule cc-toolchain-watchos_x86_64-aarch64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n\r\n# /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14\r\ntoolchain(\r\n  name = \"cc-toolchain-watchos_x86_64-x86_64\",\r\n  exec_compatible_with = [\"@platforms//os:osx\", \"@platforms//cpu:x86_64\"],\r\n  toolchain_type = \"@bazel_tools//tools/cpp:toolchain_type\",\r\n  target_compatible_with = [\"@platforms//os:ios\", \"@platforms//cpu:x86_64\"],\r\n  toolchain = \"@local_config_cc//:cc-compiler-watchos_x86_64\",\r\n)\r\n# Rule cc-toolchain-watchos_x86_64-x86_64 instantiated at (most recent call last):\r\n#   /private/var/tmp/_bazel_freedom/4bb57b4ef1c881b777de0d15fb3c5032/external/local_config_cc_toolchains/BUILD:11:14 in <toplevel>\r\n...\r\n```", "Confirmed that could be built with latest Bazel and TF HEAD. \r\n\r\nNIT: Seeing multiple machine failures as clang eats all my memory and macOS doing extensive swaps when building with `MAX_JOBS=8` on M1 (my Mac has only 8GB RAM /sad)."]}, {"number": 47593, "title": "Given an input and get two outputs", "body": "I am using Tensorflow 2.4.0 and Ubuntu 16.04\r\n\r\nGiven this model\r\n\r\n    base_model = tf.keras.applications.EfficientNetB0(weights=\"imagenet\", include_top=False)\r\n    base_model.trainable = base_model_trainable\r\n    inputs = tf.keras.Input(shape=(IMG_SIZE,IMG_SIZE,3), name=\"input\")\r\n    x = tf.keras.applications.efficientnet.preprocess_input(inputs)\r\n    # more details - https://www.tensorflow.org/tutorials/images/transfer_learning#important_note_about_batchnormalization_layers\r\n\r\n    x = base_model(x,\r\n                   training=False)  # training=training is needed only if there are layers with different behavior during training versus inference (e.g. Dropout)\r\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\r\n    x = tf.keras.layers.Dropout(0.2)(x)\r\n    outputs = tf.keras.layers.Dense(len(class_names), name=\"output\")(x)\r\n    model = tf.keras.Model(inputs, outputs)\r\n\r\nMy objective is given the input to get the value  of `model.get_layer(\"efficientnetb0\").output`, \r\nand `model.output`\r\n\r\nif I do (a) (it does not give `model.get_layer(\"efficientnetb0\").output`)\r\n\r\n    layer_name=\"efficientnetb0\"\r\n\r\n    grad_model = tf.keras.models.Model([model.inputs], [model.output])\r\n\r\nor (b) (it requires an additional input after preprocessing)\r\n\r\n    grad_model = tf.keras.models.Model([model.inputs, model.get_layer(layer_name).input], [model.get_layer(layer_name).output, model.output])\r\n\r\nboth works\r\n\r\nbut if I do\r\n\r\n    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output])\r\n\r\nIt gives exception:\r\n\r\n    ValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") at layer \"rescaling\". The following previous layers were accessed without issue: []\r\n\r\nIt does not make sense to me - is this a bug or is there a way for me to pass in just an input and get two outputs?\r\n", "comments": ["@tianhuat,\r\nOn running the code, I am facing an error stating `NameError: name 'base_model_trainable' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f29e5b2b3bace2df45cf9653a89f5679/47593.ipynb).\r\n\r\nIn order to reproduce the issue reported, could you please provide the complete code and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47593\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47593\">No</a>\n"]}, {"number": 47592, "title": "[tf.data] move csv, assert cardinality and auto shard checkpoint tests to kernel tests", "body": "This PR is a continuation of https://github.com/tensorflow/tensorflow/pull/47314 and moves the checkpoint tests of `csv`, `assert_cardinality` and `auto_shard` datasets to kernel tests.\r\n\r\nAlso, this PR is a part of the larger cleanup as discussed in point 4 of https://github.com/tensorflow/tensorflow/pull/46761#issuecomment-770059963\r\n\r\ncc: @jsimsa", "comments": []}]