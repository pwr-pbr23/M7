[{"number": 35047, "title": "Set TRT network name to builder configs", "body": "This is useful for debugging to tell what builder configs were used when building the engine.", "comments": ["@pooyadavoodi could you please help to resolve the conflict, and make sure all the tests are passed at head with this PR? Thanks.", "> @pooyadavoodi could you please help to resolve the conflict, and make sure all the tests are passed at head with this PR? Thanks.\r\n\r\nI resolved the conflicts and ran all the tests. Thanks @aaroey "]}, {"number": 35046, "title": "Test", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 35045, "title": "add_loss bug when using tf.keras, but it is ok using keras.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- TensorFlow version (use command below): \r\ntensorflow2.0.0 keras 2.3.1\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 1080ti\r\n\r\n**Describe the current behavior**\r\nWhen i use keras, the add_loss behavior is all okay. But when i change to tf.keras, the following error\r\n is occured.\r\nI suppose that maybe the eager cause this problem, but it still occurs when i disable the eager.\r\nSo, any ideas? Thanks a lot.\r\n\r\n\r\n2019-12-12 13:02:42.334129: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: You must feed a value for placeholder tensor 'Input-Segment' with dtype float and shape [?,?]\r\n\t [[{{node Input-Segment}}]]\r\n2019-12-12 13:02:42.334488: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: You must feed a value for placeholder tensor 'Input-Segment' with dtype float and shape [?,?]\r\n\t [[{{node Input-Segment}}]]\r\n\t [[Embedding-Token/Cast/_8]]\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 33, in <module>\r\n    model.add_loss(cross_entropy)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 1132, in add_loss\r\n    self._graph_network_add_loss(symbolic_loss)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1426, in _graph_network_add_loss\r\n    new_nodes, new_layers = _map_subgraph_network(self.inputs, [symbolic_loss])\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1651, in _map_subgraph_network\r\n    base_layer_utils.create_keras_history(outputs)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 184, in create_keras_history\r\n    _, created_layers = _create_keras_history_helper(tensors, set(), [])\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 231, in _create_keras_history_helper\r\n    layer_inputs, processed_ops, created_layers)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 231, in _create_keras_history_helper\r\n    layer_inputs, processed_ops, created_layers)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 231, in _create_keras_history_helper\r\n    layer_inputs, processed_ops, created_layers)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 229, in _create_keras_history_helper\r\n    constants[i] = backend.function([], op_input)([])\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 3740, in __call__\r\n    outputs = self._graph_fn(*converted_inputs)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1081, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1121, in _call_impl\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  You must feed a value for placeholder tensor 'Input-Segment' with dtype float and shape [?,?]\r\n\t [[node Input-Segment (defined at /home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_keras_scratch_graph_190]\r\n\r\nFunction call stack:\r\nkeras_scratch_graph\r\n\r\n**Code to reproduce the issue**\r\n\r\n\r\n```\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n# import keras\r\n# import keras.backend as K\r\nx_in = keras.layers.Input(shape=(None, ), name='Input-Token')\r\ns_in = keras.layers.Input(shape=(None, ), name='Input-Segment')\r\nx, s = x_in, s_in\r\n# Embedding\r\nx = keras.layers.Embedding(input_dim=12,\r\n                           output_dim=12,\r\n                           name='Embedding-Token')(x)\r\ns = keras.layers.Embedding(input_dim=12,\r\n                           output_dim=12,\r\n                           name='Embedding-Segment')(s)\r\nx = keras.layers.Add(name='Embedding-Token-Segment')([x, s])\r\nmodel = keras.models.Model([x_in, s_in], x)\r\nmodel.summary()\r\ny_in = model.input[0][:, 1:]\r\ny_mask = model.input[1][:, 1:]\r\ny = model.output[:, :-1]\r\ncross_entropy = K.sparse_categorical_crossentropy(y_in, y)\r\nmodel.add_loss(cross_entropy)\r\n```\r\n\r\n", "comments": ["Issue is replicating with Tf 2.0.\r\nPlease find the colab gist [here](https://colab.sandbox.google.com/gist/gadagashwini/3436d5fac7808a3fec8a598bf2685ec4/untitled301.ipynb). Thanks! ", "You may try to wrap the sparse op in a Lambda layer. Something like;\r\n```python\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n# import keras\r\n# import keras.backend as K\r\nx_in = keras.layers.Input(shape=(None, ), name='Input-Token')\r\ns_in = keras.layers.Input(shape=(None, ), name='Input-Segment')\r\nx, s = x_in, s_in\r\n# Embedding\r\nx = keras.layers.Embedding(input_dim=12,\r\n                           output_dim=12,\r\n                           name='Embedding-Token')(x)\r\ns = keras.layers.Embedding(input_dim=12,\r\n                           output_dim=12,\r\n                           name='Embedding-Segment')(s)\r\nx = keras.layers.Add(name='Embedding-Token-Segment')([x, s])\r\nmodel = keras.models.Model([x_in, s_in], x)\r\nmodel.summary()\r\ny_in = model.input[0][:, 1:]\r\ny_mask = model.input[1][:, 1:]\r\ny = model.output[:, :-1]\r\ncross_entropy_loss = lambda x: K.sparse_categorical_crossentropy(y_in, y)\r\ncross_entropy_loss  = keras.layers.Lambda(cross_entropy)    \r\nmodel.add_loss(cross_entropy_loss)\r\n```", "@ymodak thanks a lot.\r\nYour original code cannot compile because the layer need an input. I modify your code like this.\r\n```\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\nimport tensorflow as tf\r\n# tf.compat.v1.disable_eager_execution()\r\n# import keras\r\n# import keras.backend as K\r\n\r\nx_in = keras.layers.Input(shape=(None, ), name='Input-Token')\r\ns_in = keras.layers.Input(shape=(None, ), name='Input-Segment')\r\nx, s = x_in, s_in\r\n# Embedding\r\nx = keras.layers.Embedding(input_dim=12,\r\n                           output_dim=12,\r\n                           name='Embedding-Token')(x)\r\ns = keras.layers.Embedding(input_dim=12,\r\n                           output_dim=12,\r\n                           name='Embedding-Segment')(s)\r\nx = keras.layers.Add(name='Embedding-Token-Segment')([x, s])\r\nmodel = keras.models.Model([x_in, s_in], x)\r\nmodel.summary()\r\n\r\ny_in = model.input[0][:, 1:]\r\ny_mask = model.input[1][:, 1:]\r\ny = model.output[:, :-1]\r\n# cross_entropy = K.sparse_categorical_crossentropy(y_in, y)\r\ndef cross_entropy_func(x):\r\n    out = K.sparse_categorical_crossentropy(x[0], x[1])\r\n    return out\r\n\r\ncross_entropy_loss = keras.layers.Lambda(cross_entropy_func)([y_in, y])\r\nmodel.add_loss(cross_entropy_loss)\r\nmodel.compile(optimizer='adam')\r\n```\r\nAlthough this operation can add loss successfully, but when compile and fit generator, another error comes out.\r\n\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1297, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py\", line 265, in model_iteration\r\n    batch_outs = batch_function(*batch_data)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 973, in train_on_batch\r\n    class_weight=class_weight, reset_metrics=reset_metrics)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 264, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 311, in train_on_batch\r\n    output_loss_metrics=output_loss_metrics))\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 268, in _process_single_batch\r\n    grads = tape.gradient(scaled_total_loss, trainable_weights)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\", line 1014, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\", line 76, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\", line 138, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py\", line 199, in _SumGrad\r\n    output_shape_kept_dims = math_ops.reduced_shape(input_shape, op.inputs[1])\r\n  File \"/home/lxy/anaconda3/envs/tf2_py36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\", line 3490, in reduced_shape\r\n    input_shape = input_shape.numpy()\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n\r\nI cannot figure it out. I think warp the loss into a Lambda layer may be not a solution. ", "Apologies for the delay in response. I successfully executed your new code in TF nightly version '2.1.0-dev20200102' . You may use google colab for a quick test. Can you please confirm?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35045\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35045\">No</a>\n", "So, what is the final solution? I have the same problem, too.", "> Issue is replicating with Tf 2.0.\r\n> Please find the colab gist [here](https://colab.sandbox.google.com/gist/gadagashwini/3436d5fac7808a3fec8a598bf2685ec4/untitled301.ipynb). Thanks!\r\n\r\nGot the same problem when using customized loss as OP's for VAE by tf 2.3, but OK for tf 1.15.2\r\nand it is OK using Lambda layer for tf 1.15.2 but NOT for tf 2.3", "> You may try to wrap the sparse op in a Lambda layer. Something like;\r\n> \r\n> ```python\r\n> import tensorflow.keras as keras\r\n> import tensorflow.keras.backend as K\r\n> import tensorflow as tf\r\n> tf.compat.v1.disable_eager_execution()\r\n> # import keras\r\n> # import keras.backend as K\r\n> x_in = keras.layers.Input(shape=(None, ), name='Input-Token')\r\n> s_in = keras.layers.Input(shape=(None, ), name='Input-Segment')\r\n> x, s = x_in, s_in\r\n> # Embedding\r\n> x = keras.layers.Embedding(input_dim=12,\r\n>                            output_dim=12,\r\n>                            name='Embedding-Token')(x)\r\n> s = keras.layers.Embedding(input_dim=12,\r\n>                            output_dim=12,\r\n>                            name='Embedding-Segment')(s)\r\n> x = keras.layers.Add(name='Embedding-Token-Segment')([x, s])\r\n> model = keras.models.Model([x_in, s_in], x)\r\n> model.summary()\r\n> y_in = model.input[0][:, 1:]\r\n> y_mask = model.input[1][:, 1:]\r\n> y = model.output[:, :-1]\r\n> cross_entropy_loss = lambda x: K.sparse_categorical_crossentropy(y_in, y)\r\n> cross_entropy_loss  = keras.layers.Lambda(cross_entropy)    \r\n> model.add_loss(cross_entropy_loss)\r\n> ```\r\n\r\nTHIS DOES NOT WORK!"]}, {"number": 35044, "title": "Memory leaks when using tf.keras.metrics update_states in multi-threads.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nThe memory increases and does not release when I use ` tf.keras.metrics.update_states` in multi-threads on Linux. I view the used memory by \"memory_profiler\" and find that the memory increase after each thread starts.\r\n\r\n![image](https://user-images.githubusercontent.com/18071380/70680739-150c6600-1cd4-11ea-9f04-6a1724b72d23.png)\r\n\r\n\r\n**Describe the expected behavior**\r\nThe memory increases greatly after only the first thread starts.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport threading\r\nimport time\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom memory_profiler import profile\r\n\r\nauc = tf.keras.metrics.AUC()\r\nlock = threading.Lock()\r\n\r\n\r\ndef update_metric(metric):\r\n    labels = np.random.randint(0, 2, 20000)\r\n    preds = np.random.random(20000)\r\n    metric.update_state(labels, preds)\r\n\r\n\r\ndef update_metric_mini(metric):\r\n    for i in range(10):\r\n        labels = np.random.randint(0, 2, 2000)\r\n        preds = np.random.random(2000)\r\n        metric.update_state(labels, preds)\r\n\r\n\r\ndef test_mem(metric):\r\n    update_metric(metric)\r\n    # update_metric_mini(metric)\r\n\r\n\r\ndef test_tf_metric():\r\n    with lock:\r\n        test_mem(auc)\r\n\r\n\r\n@profile\r\ndef test_threads():\r\n    t1 = threading.Thread(target=test_tf_metric)\r\n    t2 = threading.Thread(target=test_tf_metric)\r\n    t3 = threading.Thread(target=test_tf_metric)\r\n    t1.start()\r\n    time.sleep(0.1)\r\n    t2.start()\r\n    time.sleep(0.1)\r\n    t3.start()\r\n    time.sleep(0.1)\r\n    t1.join()\r\n    t2.join()\r\n    t3.join()\r\n    time.sleep(1)\r\n    print(\"end \")\r\n\r\n\r\ntest_threads()\r\n```\r\nIn this test code snippet, if we use the `update_metric_mini` which split the updated outputs and labels to small chunks and update metric with those chunks, the memory increases small too. I wonder whether the `update_states` in each thread creates global variables (e.g. placeholder) for input arguments `outputs` and `labels` and the variable size is proportional to the size of input arguments.\r\n\r\n***other info*\r\nWe encounter this issue when developing evaluation in [ElasticDL](https://github.com/sql-machine-learning/elasticdl) and resolving the ElasticDL [issue 1568](https://github.com/sql-machine-learning/elasticdl/issues/1568)\r\n\r\n", "comments": ["> The memory increases greatly after only the first thread starts.\r\n\r\n@workingloong,\r\nIs this still an issue?\r\n\r\nOn running the code with TF v2.3, I did not observe huge memory growth after the start of the first thread. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/da01206f8991ef773c161f7cbe437748/35044.ipynb). Thanks!", "> > The memory increases greatly after only the first thread starts.\r\n> \r\n> @workingloong,\r\n> Is this still an issue?\r\n> \r\n> On running the code with TF v2.3, I did not observe huge memory growth after the start of the first thread. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/da01206f8991ef773c161f7cbe437748/35044.ipynb). Thanks!\r\n\r\nI don't find `test.py` to execute `!python test.py` in this Ipython notebook. I have tested the code in Linux with TF v2.3 and it is still an issue.\r\n\r\n![image](https://user-images.githubusercontent.com/18071380/91514726-09240b00-e91a-11ea-99c7-f3e3cd12e9ab.png)\r\n", "@workingloong Is this still an issue for you? Can you please test with recent `TF2.7` and `tf-nightly` and let us know whether the issue persists with recent TF version. Also, share a complete standalone code to reproduce the issue. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 35042, "title": "How to know if savedmodel supports batch input and dynamic length input?", "body": "I use the follwing code:\r\n\r\n```\r\nwith tf.Session() as sess:\r\n    meta_graph_def = tf.saved_model.loader.load(sess, [\"serve\"], model_path)\r\n    signature = meta_graph_def.signature_def\r\n    print(signature[\"serving_default\"])\r\n    if str(-1) in str(signature[\"serving_default\"]):\r\n        print(\"support batch input\")\r\n    else:\r\n        print(\"Not support batch input\")\r\n```\r\nthe result:\r\n```\r\ninputs {\r\n  key: \"hist_i\"\r\n  value {\r\n    name: \"my_input/hist_item_id:0\"\r\n    dtype: DT_INT32\r\n    tensor_shape {\r\n      dim {\r\n        size: -1\r\n      }\r\n      dim {\r\n        size: -1\r\n      }\r\n    }\r\n  }\r\n}\r\ninputs {\r\n  key: \"hist_t\"\r\n  value {\r\n    name: \"my_input/hist_time:0\"\r\n    dtype: DT_INT32\r\n    tensor_shape {\r\n      dim {\r\n        size: -1\r\n      }\r\n      dim {\r\n        size: -1\r\n      }\r\n    }\r\n  }\r\n}\r\ninputs {\r\n  key: \"item_id\"\r\n  value {\r\n    name: \"my_input/item_id:0\"\r\n    dtype: DT_INT32\r\n    tensor_shape {\r\n      dim {\r\n        size: -1\r\n      }\r\n    }\r\n  }\r\n}\r\ninputs {\r\n  key: \"sl\"\r\n  value {\r\n    name: \"my_input/Placeholder:0\"\r\n    dtype: DT_INT32\r\n    tensor_shape {\r\n      dim {\r\n        size: -1\r\n      }\r\n    }\r\n  }\r\n}\r\ninputs {\r\n  key: \"user_id\"\r\n  value {\r\n    name: \"my_input/user_id:0\"\r\n    dtype: DT_INT64\r\n    tensor_shape {\r\n      dim {\r\n        size: -1\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\nBut the code can't to distinguish with batch input and dynamic  length input.Anyone can help me?Thank you very much.", "comments": ["@zxgm \r\n\r\nCan you please help us with the Tensorflow version you are using?.And also request you to provide simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "I use tensorflow1.12 or tensorflow1.14.Assume have a savedmodel format model,how to know if the savedmodel supports batch input and dynamci length input? Are there related APIs to determine whether batch processing is supported?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n"]}, {"number": 35041, "title": "[r2.1 Cherrypick]: Unexpose LossScaleGradientTape.", "body": "It doesn't support DistributionStrategy. It will be reexposed when it does. I tried to fix this in #34974, but only made the issue worse. The issue is that when taking gradients with respect to variables (which occurs almost every time), it would crash with a very long error message when DistributionStrategy is used. The unit tests only tested taking gradients w.r.t. constants, as it was assumed there would be no functional difference between taking gradients w.r.t. variables and constants.\r\n\r\nPiperOrigin-RevId: 285059221\r\nChange-Id: I9ffc5d68f092f9ff3ea634b9523b67ff2bbc4bd7", "comments": ["Looking forward to the re-exposure :)", "Same here :). This will be reexposed in 2.2, and only isn't exposed now since I failed to fixed a bug I tried to address."]}, {"number": 35040, "title": "S3 unit test build broken: gtl not declared", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master @3d79d19aa27abd1e236761a6071acec858acb6a0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 1.1.0\r\n- GCC/Compiler version (if compiling from source): gcc5\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nTest fails to build\r\n```\r\nERROR: /tensorflow/tensorflow/core/platform/s3/BUILD:96:1: C++ compilation of rule '//tensorflow/core/platform/s3:s3_file_system_test' failed (Exit 1)\r\nIn file included from ./tensorflow/core/platform/env.h:27:0,\r\n                 from ./tensorflow/core/platform/s3/s3_file_system.h:20,\r\n                 from tensorflow/core/platform/s3/s3_file_system_test.cc:16:\r\ntensorflow/core/platform/s3/s3_file_system_test.cc: In member function 'tensorflow::Status tensorflow::{anonymous}::S3FileSystemTest::ReadAll(const string&, std::__cxx11::string*)':\r\ntensorflow/core/platform/s3/s3_file_system_test.cc:59:45: error: 'gtl' has not been declared\r\n         reader->Read(0, file_size, &result, gtl::string_as_array(content)));\r\n                                             ^\r\n./tensorflow/core/platform/errors.h:73:37: note: in definition of macro 'TF_RETURN_IF_ERROR'\r\n     ::tensorflow::Status _status = (__VA_ARGS__);        \\\r\n                                     ^~~~~~~~~~~\r\nIn file included from external/com_google_googletest/googletest/include/gtest/gtest.h:388:0,\r\n                 from ./tensorflow/core/platform/test.h:31,\r\n                 from ./tensorflow/core/lib/core/status_test_util.h:20,\r\n                 from tensorflow/core/platform/s3/s3_file_system_test.cc:18:\r\ntensorflow/core/platform/s3/s3_file_system_test.cc: In member function 'virtual void tensorflow::{anonymous}::S3FileSystemTest_NewRandomAccessFile_Test::TestBody()':\r\ntensorflow/core/platform/s3/s3_file_system_test.cc:83:48: error: 'gtl' has not been declared\r\n       reader->Read(0, content.size(), &result, gtl::string_as_array(&got)));\r\n                                                ^\r\ntensorflow/core/platform/s3/s3_file_system_test.cc:82:3: note: in expansion of macro 'TF_EXPECT_OK'\r\n   TF_EXPECT_OK(\r\n   ^~~~~~~~~~~~\r\ntensorflow/core/platform/s3/s3_file_system_test.cc:89:44: error: 'gtl' has not been declared\r\n   TF_EXPECT_OK(reader->Read(2, 4, &result, gtl::string_as_array(&got)));\r\n                                            ^\r\ntensorflow/core/platform/s3/s3_file_system_test.cc:89:3: note: in expansion of macro 'TF_EXPECT_OK'\r\n   TF_EXPECT_OK(reader->Read(2, 4, &result, gtl::string_as_array(&got)));\r\n   ^~~~~~~~~~~~\r\nTarget //tensorflow/core/platform/s3:s3_file_system_test failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 5.538s, Critical Path: 5.20s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n**Describe the expected behavior**\r\nTest builds and runs\r\n\r\n**Code to reproduce the issue**\r\nbazel test -c opt //tensorflow/core/platform/s3:s3_file_system_test --action_env=S3_TEST_TMPDIR=s3://BUCKET/s3testing-results/bazel/master/\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["How to include gtl correctly for the gtl::string_as_array function used in the file?", "Added #35139 for the fix.", "My phone is jacked\n\nOn Wed, Dec 18, 2019, 03:35 TensorFlow Copybara <notifications@github.com>\nwrote:\n\n> Closed #35040 <https://github.com/tensorflow/tensorflow/issues/35040> via\n> #35139 <https://github.com/tensorflow/tensorflow/pull/35139>.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35040?email_source=notifications&email_token=AOBPXOV2MJCKFJZ47ADS7J3QZHVETA5CNFSM4JZWLOSKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOVRWQPPY#event-2892826559>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AOBPXOUSB7IYOQQBUJOS2JTQZHVETANCNFSM4JZWLOSA>\n> .\n>\n"]}, {"number": 35039, "title": "[XLA][AMDGPU] Add GemmRewriter into AMDGPU HLO optimization pipeline.", "body": "", "comments": []}, {"number": 35038, "title": "[Intel MKL] Updating MKL layout pass unit test with bfloat16 support - 2", "body": "This PR supports a few more MKL layout pass tests in BFloat16 mode.", "comments": []}, {"number": 35037, "title": "Testing new tensorflow team", "body": "Testing that issue tagged with @tensorflow/micro goes to the appropriate set of folks.\r\n\r\n", "comments": []}, {"number": 35036, "title": "Error when no GPU available in TensorFlow 2.1.0rc1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0rc1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nInstalling `tensorflow==2.1.0rc1` and trying to `import tensorflow` results in an error if there is no GPU set up on the machine.\r\n\r\n**Describe the expected behavior**\r\n\r\nIn `tensorflow==2.1.0rc0` this simply results in a warning\r\n\r\n```\r\n2019-12-11 14:50:44.677649: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2019-12-11 14:50:44.684212: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```\r\n\r\nThis should have the same warning message in 2.1.0rc1, rather than crashing.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nImportError: Traceback (most recent call last):\r\n  File \"...\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"...\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"...\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"...\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"...\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n", "comments": ["@yifeif @goldiegadde looks like we have a potential regression?\r\nCould one of you confirm this?", "I wasn't able to reproduce this on Windows 2019 Server on either Python 3.7.4 or Python 3.6.8, after uninstalling both CUDA and Visual Studio 2017 Build Tools.\r\n\r\n```\r\n$ /c/Python37/python -m pip install --user tensorflow==2.1.0rc1\r\n$ /c/Python37/python\r\nPython 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2019-12-11 23:32:39.760755: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2019-12-11 23:32:39.766966: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n>>> tf.__version__\r\n'2.1.0-rc1'\r\n```\r\n\r\n```\r\n$ /c/Python36/python -m pip install --user tensorflow==2.1.0rc1\r\n$ /c/Python36/python\r\nPython 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2019-12-11 23:36:16.730995: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2019-12-11 23:36:16.737279: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n>>> tf.__version__\r\n'2.1.0-rc1'\r\n```\r\n\r\nI did have to install as `--user`, however. Perhaps there is an underlying permissions issue?", "You can see the issue reproducing in this appveyor build, if that helps https://ci.appveyor.com/project/nengo/nengo-dl/builds/29482767/job/8054ac3871y174vn#L503", "Have the exact same issue.", "That's very interesting. @micahr123, can you tell us the same system information as @drasmuss provided in the initial report, as well as command line replication?", "Okay, I was able to reproduce this on a plainer image. It appears that building with VS 2019 updates the Windows-required Visual C++ Redistributable version to the one on the [latest redistibutable](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads) page, whereas older versions of the redistributable would work on older packages.\r\n\r\nDidn't install anything:\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\platform\\self_check.py\", line 47, in preload_check\r\n    ctypes.WinDLL(build_info.msvcp_dll_name)\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\ctypes\\__init__.py\", line 356, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\platform\\self_check.py\", line 55, in preload_check\r\n    % build_info.msvcp_dll_name)\r\nImportError: Could not find 'msvcp140.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. You may install this DLL by downloading Visual C++ 2015 Redistributable Update 3 from this URL: https://www.microsoft.com/en-us/download/details.aspx?id=53587\r\n```\r\n\r\nInstalled [Visual C++ 2015 Redistributable Update 3](https://www.microsoft.com/en-us/download/confirmation.aspx?id=53587)\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\kbuilder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nInstalled [Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n```\r\n>>> import tensorflow as tf\r\n2019-12-13 20:29:52.375565: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2019-12-13 20:29:52.381257: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n>>>\r\n```\r\n\r\n@blamb The docs on tensorflow.org/install don't seem to mention anything about installing the redistributable, would that be a useful thing to add?\r\n\r\nUnfortunately I don't understand the way this checker works myself, so I don't know what we need to do to additionally search for the correct .dll.", "That is, you can resolve this by installing [the new redistributable package](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads), but we need to update TensorFlow to ensure it checks for all the necessary DLLs on import.", "It worked for me! Thanks.", "So RC1 was built with VS 2019, which is what caused the change from RC0?", ">So RC1 was built with VS 2019, which is what caused the change from RC0?\r\n\r\nThat's correct. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35036\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35036\">No</a>\n", "This didn't work for me? Any other ideas? (\"you can resolve this by installing the new redistributable package, but we need to update TensorFlow to ensure it checks for all the necessary DLLs on import\"). I don't have a GPU.\r\n\r\n2020-07-23 23:23:51.875741: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-07-23 23:23:51.880431: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.", "The log lines you see are simply informational. TF simply can run without issues despite those.\r\n\r\nIf you do not want to see those errors on machines without GPU, you can simply install tensorflow-cpu pip package.", "@gunan so i f i get these two lines i just don't care and continue with my work ? \r\n>>> import tensorflow as tf\r\nW tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine."]}, {"number": 35035, "title": "Updating Readme.md with a link to new user survey", "body": "", "comments": ["We are going to resubmit with revised text. "]}, {"number": 35034, "title": "Fix compilation of hwloc", "body": "The `linux` specific define is wrong and the string replacement was done in the wrong order which caused defines such as:\r\n```\r\n#define HWLOC_HAVE_ATTRIBUTE 1_SENTINEL\r\n#define HWLOC_HAVE_ATTRIBUTE 1_UNUSED\r\n#define HWLOC_HAVE_ATTRIBUTE 1_WARN_UNUSED_RESULT\r\n#define HWLOC_HAVE_ATTRIBUTE 1_WEAK_ALIAS\r\n```\r\n\r\netc", "comments": ["Not a good reviewer for this."]}, {"number": 35033, "title": "Cannot use Model as a Layer", "body": "\r\nI am not able to use Model as a Keras layer. I get the following error message:\r\n\r\n**AttributeError: 'Model' object has no attribute 'shape'**\r\n\r\n\r\n<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["See [All models are callable, just like layers](https://www.tensorflow.org/guide/keras/functional#all_models_are_callable_just_like_layers) to know more.", "@nectario ,\r\nCan you please check link provided by @ymodak and let us know if it helped?Thanks!", "It did help a bit, but now a different problem resulted which I posted in a different thread \u2014 related to using models as layers.", "@nectario ,\r\nThank you, please feel free to close this issue! "]}, {"number": 35032, "title": "cuBLAS failure for large convolutions in V100 GPUs", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.15.0 or 2.0.0\r\n- **Python version**: 3.7.3\r\n- **CUDA/cuDNN version**: 10.0 and 7.6.4\r\n- **GPU model and memory**: Tesla V100 with either 16GB or 32GB\r\n- **Exact command to reproduce**: `python tf_v100_cublas_crash_tf1.py` (from the gist below)\r\n\r\n### Describe the problem\r\nIf I run a large 1x1 convolution with FP16 on a Tesla V100, cuBLAS crashes with the following kind of message:\r\n```\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal: Blas SGEMM launch failed : m=9584640, n=17, k=17\r\n     [[node graph_final_part/conv/conv2d/Conv2D (defined at ~/.virtualenvs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n     [[ExpandDims_9/_3631]]\r\n  (1) Internal: Blas SGEMM launch failed : m=9584640, n=17, k=17\r\n     [[node graph_final_part/conv/conv2d/Conv2D (defined at ~/.virtualenvs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n```\r\nThis comes from this place in the TF code:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L698\r\n\r\nThe root cause is such TensorFlow code for example:\r\n```\r\nconv_op = tf.layers.Conv2D(filters=17, kernel_size=(1, 1))\r\nin_t = tf.ones(shape=(10, 1152, 832, 17), dtype=tf.float16)\r\n```\r\nIt works again if I:\r\n* Make the input tensor smaller\r\n* Use FP32 instead of FP16\r\n* Run on a CPU instead of the V100 GPU\r\n* Use a GeForce Titan X GPU instead of a V100\r\n* Use TF 1.14 on a V100 GPU (but not TF 1.15.0 or 2.0.0)\r\n\r\n### Source code / logs\r\n\r\nA small gist with a fully working example of only ~10 lines can be found here:\r\nhttps://gist.github.com/CNugteren/c40f0f34f2af759b2d900223fefadbfd#file-tf_v100_cublas_crash_tf1-py (TF 1.15.0 version)\r\nhttps://gist.github.com/CNugteren/c40f0f34f2af759b2d900223fefadbfd#file-tf_v100_cublas_crash_tf2-py (TF 2.0.0 version)\r\n\r\nI even tried to mimic the cuBLAS call but that didn't crash, so it seems a TensorFlow bug rather than cuBLAS, unless I made a mistake of course:\r\nhttps://gist.github.com/CNugteren/c40f0f34f2af759b2d900223fefadbfd#file-tf_v100_cublas_crash_mimic-cu", "comments": ["Sample code snippet is working as expected on Colab with Tf 1.15 and Tf 2.0.\r\nPlease see the gist of [Tf 1.15](https://colab.sandbox.google.com/gist/gadagashwini/ac001417dc6c92becd9971be07f98456/untitled299.ipynb) and [Tf 2.0](https://colab.sandbox.google.com/gist/gadagashwini/0cbe3a109bb78b10a31e81f367cbfac8/untitled300.ipynb). Thanks!", "@gadagashwini, sorry I don't have a Google account so I can't run that. But are you sure this is on a V100? If so, please share all the details of the nvidia drivers and cuda/cudnn setup that you are using in this Colab environment.", "@CNugteren, Google Colab uses P100. ", "So please test it on a V100, that's where the bug manifests, as you can see from my description.", "I wasn't able to reproduce this on a Titan-V.  Can enable [cuBLAS logging](https://docs.nvidia.com/cuda/cublas/index.html#cublasLoggerConfigure) and attach the generated log?  Maybe that will uncover a smoking gun.", "OK good idea, I've done that now.\r\n\r\nRunning the official TF 1.15.0 on a V100 with CUDA 10.0 and cuDNN 7.6.4 with NVIDIA driver 418.56:\r\npython3 https://gist.github.com/CNugteren/c40f0f34f2af759b2d900223fefadbfd#file-tf_v100_cublas_crash_tf1-py\r\n\r\nResults in regular output as follows:\r\n[regular_output.txt](https://github.com/tensorflow/tensorflow/files/3966986/regular_output.txt)\r\n\r\nAnd with CUBLAS_LOGINFO_DBG=1 set also this cuBLAS specific output:\r\n[cublas_report.txt](https://github.com/tensorflow/tensorflow/files/3966990/cublas_report.txt)\r\n", "I was able to reproduce this on a TitanV.  This fails with TF 2.0.0 but passes with tf-nightly (2.1.0-dev20191226).  It could have been a TF issue that got fixed or a CUDA issue that was fixed in 10.1 (2.0.0 uses CUDA 10, TF nightly uses CUDA 10.1).\r\n\r\nI'm closing this for now, please reopen if there is something more for us to do here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35032\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35032\">No</a>\n", "Thanks @sanjoy. Just for your information: I also tried with my own compiled version of TF 1.15.0 with the latest CUDA 10.1 and that didn't solve the issue, so I doubt that that changed it. But I'm glad it is fixed in TF 2.1. So that also means you won't backport this fix to the 1.x branch (e.g. 1.15.1)?", "> So that also means you won't backport this fix to the 1.x branch (e.g. 1.15.1)?\r\n\r\nI don't think there are any plans for a 1.15.1 release (CC @goldiegadde ).\r\n\r\nAnd at this point I'm not even sure what change fixed this, given that you're saying it wasn't the CUDA upgrade.", "Exeperienced this as well with a V100, NVIDIA drivers 418, CUDA 10.1 and 2.1.0rc2:\r\n> tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found. (0) Internal: Blas GEMM launch failed : a.shape=(1000, 1024), b.shape=(1024, 3), m=1000, n=3, k=1024\r\n> [[{{node mrcnn_class_logits/dense/MatMul}}]]\r\n[[add_metric_2/Identity/_3667]] (1) Internal: Blas GEMM launch failed : a.shape=(1000, 1024), b.shape=(1024, 3), m=1000, n=3, k=1024\r\n[[{{node mrcnn_class_logits/dense/MatMul}}]]\r\n\r\nWas successful with an image based on today's nightly-gpu-py3 with digest 7cdb1957c9bf."]}, {"number": 35031, "title": "ValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.", "body": "**System information**\r\n- Windows 10 866 bit OS,x64 based processor \r\n- python 3.5.4 (working on virtual env)\r\n- pip installed version of Tensorflow\r\n- TensorFlow version: tensorflow==2.0.0\r\n- other packages:\r\nscikit-learn==0.22\r\nscipy==1.4.0rc2\r\nsix==1.12.0\r\ntb-nightly==2.1.0a20191115\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nUsed the following code to quantize the frozen graph [http://download.tensorflow.org/models/speech_commands_v0.02.zip]()\r\n```\r\n# the above frozen graph was saved in a directory as \"saved_model\"\r\n```\r\ncode:\r\nimport tensorflow as tf\r\ndef representative_dataset_gen():\r\n  for _ in range(num_calibration_steps):\r\n    # Get sample input data as a numpy array in a method of your choosing.\r\n    yield [input]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('C:/test/saved_model')\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_quant_model = converter.convert()\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nFile \"trained_model_quantize.py\", line 11, in <module>\r\n    tflite_quant_model = converter.convert()\r\n  File \"C:\\Users\\Desktop\\Projects\\Environments\\google_speech\\lib\\site-packages\\tensorflow_core\\lite\\python\\lite.py\", line 421, in convert\r\n    raise ValueError(\"This converter can only convert a single \"\r\nValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\nTutorial: [https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md]()\r\n \r\nGraph: [http://download.tensorflow.org/models/speech_commands_v0.02.zip]()\r\n**Objective**\r\nI'm trying to convert this model (frozen graph) which is in normal Float 32 to fixed point 8 bit. ", "comments": ["@daverim could you please explain what exactly \"This converter can only convert a single ConcreteFunction.\" means?\r\nWhy it happens and how to avoid it?\r\nSorry if questions are silly but I'm new to Tensorflow.", "Are there any updates on this? I'm having the same issue. ", "See this: https://github.com/tensorflow/tensorflow/issues/34350\r\nAnd this: https://stackoverflow.com/questions/58499146/how-do-i-convert-tensorflow-2-0-estimator-model-to-tensorflow-lite/59969959", "@anandsaini024  Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35031\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35031\">No</a>\n"]}, {"number": 35030, "title": "High memory consumption with model.fit in TF 2.0.0 and 2.1.0-rc0", "body": "**System information**\r\n\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Kubuntu 18.04, kernel 5.0\r\n- Mobile device: Not verified on mobile devices\r\n- TensorFlow installed from: binary via `pip install tensorflow-gpu`\r\n- TensorFlow version: `2.1.0-rc0`, however affected are also `2.0.0` and `2.0.0-rc0`, `2.0.0-rc1`, `2.0.0-rc2`\r\n- Python version: 3.6.9\r\n- CUDA version: 10.1 for TF 2.1.0-rc0; 10.0 for the earlier versions of TF\r\n- cuDNN version: 7\r\n- GPU model and memory: Nvidia GeForce GTX 1050 Ti (4GB)\r\n- CPU model: AMD Ryzen 7 1700\r\n\r\n**Describe the current behavior**\r\n\r\nModel training with the Keras API consumes high amount of system memory with TF `2.0.0` and `2.1.0-rc0`, as well as in `2.0.0-rc0`, `2.0.0-rc1` and `2.0.0-rc2`. It looks like the memory used by `model.fit` is proportional to the size of the training data provided as numpy arrays, with the proportionality constant being approximately 1. In other words, if the numpy arrays `x` and `y` are, say, 8 GB in total, then `model.fit(x,y,...)` will use another 8 GB (plus some overhead). This may suggest that `model.fit` creates unnecessary copies of the data arrays. This is in contrary to TF `1.14.0`, `2.0.0-a0`, `2.0.0-b0` and `2.0.0-b1`, where `model.fit` seems to use some amount of RAM independent of the data size (and much less than 8 GB, at least in the test code attached below).\r\n\r\nThe same concerns the validation data. If validation data are passed as numpy arrays to `model.fit` via the argument `validation_data`, then the memory use of `model.fit` seems to duplicate the size of the validation data arrays with TF from `2.0.0-rc0` to `2.1.0-rc0`.\r\n\r\nIn the code attached below, one may change the variable `K` to vary the size of the data and test the above described behaviour. It is straightforward to estimate the data size: e.g. with `K=5000` the data arrays in the below code should be ca. 7.32 GB in total. The whole Python process associated with this code uses approximately this much RAM plus some overhead when running with TF `1.14.0`, `2.0.0-a0`, `2.0.0-b0` or `2.0.0-b1`. But with TF from `2.0.0-rc0` to `2.1.0-rc0` the Python process consumes twice that much RAM. One may comment out the line containing `model.fit` to check that it is the point at which the high memory consumption starts.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe size of the memory used by `model.fit` should not duplicate the size of the training and validation data passed as numpy arrays. It should be more or less independent of the size of the data arrays, similarly as in TF `1.14.0` and in the pre-releases `2.0.0-a0`, `2.0.0-b0` and `2.0.0-b1`. \r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Lambda, Conv2D\r\n\r\nprint(\"Tensorflow version: {}\".format(tf.__version__),flush=True)\r\n\r\nK = 5000 # Number of images\r\nN = 512  # Image size\r\n\r\nMAX_SIGNAL = 5000 # The values of the training data range from 0 to this\r\n\r\ndef build_model():\r\n  '''Create a simple test model.'''\r\n  \r\n  inputs = Input((N,N,1))\r\n  s = Lambda(lambda x: x / MAX_SIGNAL) (inputs)\r\n  s = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(s)\r\n  outputs = s\r\n\r\n  return Model(inputs=[inputs], outputs=[outputs])\r\n\r\n# Generate some random data\r\nx_train = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\ny_train = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\nx_val   = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\ny_val   = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n# In total, the above arrays should be 7 680 000 kB\r\n\r\nmodel = build_model()\r\n\r\noptimizer = tf.keras.optimizers.Adam()\r\nloss = tf.keras.losses.BinaryCrossentropy()\r\n\r\nmodel.compile(optimizer=optimizer, loss=loss)\r\nmodel.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n```\r\n\r\n", "comments": ["I have just discovered that TF `2.1.0-rc1` was released today. Unfortunately, the issue persists in `2.1.0-rc1`.", "I'm also having this issue.", "@gdudziuk \r\n\r\nI have tried in colab with TF 2.0.0-beta1 where it utilizes 4% of GPU memory ([gist attached](https://colab.sandbox.google.com/gist/ravikyram/5c22e3602be2a3436dd5f9d66b734d9f/untitled474.ipynb)) and with TF 2.1.0-rc1 it utilizes 5% of GPU memory([gist attached](https://colab.sandbox.google.com/gist/ravikyram/637621c51e03531f2d7da3250ff7b053/untitled475.ipynb)). Is this the expected behavior?. Thanks!", "Not exactly. The issue is with the system RAM, not with GPU RAM.\r\n\r\nI can see that in both of your gists you get similar usage of system RAM: 9.8 GB for `2.0.0-beta1` and 10.2 GB for `2.1.0-rc1`. Yes, this is the expected behavior. Said that, I cannot reproduce this expected behavior on my workstation. I obtain 9.6 GB for `2.0.0-beta1` and 18.7 GB for `2.1.0-rc1`. The latter is twice that much as the former.", "@gdudziuk I was also not able to reproduce the issue. Can you please try to reinstall the latest tensorflow and test again. Thanks!", "No problem. I have just reinstalled with `pip install --no-cache-dir tensorflow-gpu==2.1.0-rc1`. The issue is still present. I have also tried tf-nightly (version `2.1.0-dev20191219`) and the results are the same as in `2.1.0-rc1`.\r\n\r\nThe issue apparently is system-dependent. So, to exclude a few possibilities I have also run tests with some non-GPU versions of Tensorflow (`1.14.0`, `2.0.0-b1`, `2.0.0-rc0` and `2.0.0`). My earlier results concerning RAM usage has been reproduced: for `1.14.0` and `2.0.0-b1` I have observed the expected behavior while for `2.0.0-rc0` and `2.0.0` the RAM usage was two times higher.\r\n\r\nConcerning `2.1.0-rc1`, it turned out that the allegedly non-GPU pip package (`tensorflow==2.1.0-rc1`) is actually distributed with GPU support (similarly to `tensorflow-gpu==2.1.0-rc1`). So for this version, I have manually switched off the GPU support by adding the following lines:\r\n```python\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\n```\r\nResult: the memory usage for `2.1.0-rc1` with GPU disabled by this line is as with enabled GPU.\r\n\r\nPlease let me know what further diagnostic info could be helpful for you. I will try to provide all necessary details.", "An update. I have reproduced the issue on a completely different machine (Intel CPU instead of AMD Ryzen, nvidia V100 instead of Geforce GTX, CentOS instead of Kubuntu) for TF `2.0.0` and `2.1.0-rc1`, for both GPU and non-GPU versions.\r\n\r\nAlso, I have discovered that the issue does _not_ occur on my workstation for _some_ of Anaconda binaries. I know that Anaconda is not an official Tensorflow distribution channel, but it may be a hint. Namely, when I create a conda environment via\r\n\r\n```\r\nconda create --name <some_name> tensorflow-gpu=2.0.0 cudatoolkit=10.0 python=3.6\r\n```\r\nthen the issue occurs. But when I change `python=3.6` to `python=3.7`, the RAM usage is close to the expected behavior (about 1 GB higher than with TF `1.14.0` but not 2x higher, which was the case in the affected binaries).\r\n\r\nI would like to note however that this issue is not associated with Python versions _per se_. When I install TF via `pip` (outside conda environments), the issue is present regardless of which Python version I use (3.6 or 3.7).\r\n\r\nSo, maybe it is a build problem? @jvishnuvardhan, is it possible that some build settings were changed for official TF binaries available via `pip` starting from `2.0.0-rc0`?\r\n", "Coming late to the issue.\r\n\r\n> Concerning `2.1.0-rc1`, it turned out that the allegedly non-GPU pip package (`tensorflow==2.1.0-rc1`) is actually distributed with GPU support (similarly to `tensorflow-gpu==2.1.0-rc1`).\r\n\r\nThat is expected. 1.15 and 2.1 and later will have a single pip package for both CPU and GPU builds. If you want only the cpu pip, you should install `tensorflow-cpu`. This is documented in [the release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0-rc2)\r\n\r\nI'm going to try some bisection on nightly, provided I can reproduce the issue", "Unfortunately, it seems I cannot reproduce locally.\r\n\r\nI tried the following:\r\n\r\n```\r\npip install -q tf-nightly-cpu==2.1.0.dev20200103 && LD_PRELOAD=libmemusage.so python test.py\r\n```\r\n\r\nand the trimmed output is\r\n\r\n```\r\n...\r\nMemory usage summary: heap total: 1291989141098, heap peak: 8014013934, stack peak: 130288\r\n         total calls   total memory   failed calls\r\n malloc|   60124559   1291986803935              0\r\nrealloc|       8875        1489361              0  (nomove:5457, dec:2344, free:0)\r\n calloc|        804         847802              0\r\n   free|  233626555   1284099355095\r\n...\r\n```\r\n\r\nIt looks like heap peak was ~8GB.\r\n\r\nJust for completeness, I did the same with `tf-nightly-cpu==1.15.0.dev20190815`:\r\n\r\n```\r\n...\r\nMemory usage summary: heap total: 1584739299734, heap peak: 8041192435, stack peak: 117200\r\n         total calls   total memory   failed calls\r\n malloc|   66274988   1584736207014              0\r\nrealloc|      58146        2292812              0  (nomove:54901, dec:27089, free:0)\r\n calloc|        742         799908              0\r\n   free|   66618233   1584716804113\r\n...\r\n```\r\n\r\nCan you repeat the same experiment please? Afaik you don't need to install anything to have `libmemusage.so`.\r\n\r\nI'll try it with `tf-nightly==2.1.0.dev20200103` to see if the issue comes from the GPU package.", "~8GB on that package too\r\n\r\n```\r\nMemory usage summary: heap total: 1292002083827, heap peak: 8020005624, stack peak: 130288\r\n         total calls   total memory   failed calls\r\n malloc|   60321898   1291999331873              0\r\nrealloc|       8918        1640226              0  (nomove:5448, dec:2353, free:0)\r\n calloc|       1102        1111728              0\r\n   free|  233768859   1284107320475\r\n```", "Thanks for checking this. It seems that the result depends strongly on the method of measurement of memory usage. I has been measuring memory without `libmemusage.so`: just open another terminal when the training code is running and call `top` or `ps au` therein.\r\n\r\nFor comparability with your results, @mihaimaruseac, I have tested `tf-nightly-cpu==2.1.0.dev20200103`. Using `libmemusage.so` I obtain a heap peak similar to yours:\r\n\r\n```\r\nMemory usage summary: heap total: 315947758051, heap peak: 7992963180, stack peak: 140880\r\n         total calls   total memory   failed calls\r\n malloc|   13184700   315943910418              0\r\nrealloc|      12617        3094371              0  (nomove:7595, dec:3100, free:0)\r\n calloc|        639         753262              0\r\n   free|   56942336   308059825672\r\n```\r\n\r\nHowever, at some arbitrarily chosen time during training `ps au` called in another terminal returns\r\n```\r\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\r\n...\r\ngdudziuk  7762  538 50.8 21342352 16738980 pts/1 Sl+ 16:33   6:11 python3.6 test.py\r\n...\r\n```\r\n(note the value in the RSS field), `top` returns\r\n```\r\n  PID USER   PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\r\n 7762 gdudziuk  20   0 21,014g 0,016t 124768 S  1011 52,5  34:07.51 python3.6\r\n...\r\n```\r\n(note the value in the RES field) and the following code\r\n```python\r\nimport psutil\r\nprocess = psutil.Process(7762)\r\nprint(process.memory_info().rss)\r\n```\r\ncalled in a Python interpreter returns `17648070656`.\r\n\r\nSo, with each measurement method except `libmemusage.so` I observe high memory consumption ca. 16 GB. Note that I am able to observe the issue using `psutils` in Python, in contrary to the colab gists by @ravikyram.\r\n\r\nThe question is thus which measurements are correct. Unfortunately, the training process crashes with a core dump when the memory usage reported by `ps`, `top` or Python's `psutil` hits my system limits, so it looks like the output of these reporting tools closely resembles the actual memory consumption.", "Thanks for the data points. I'll try a reproduction with `psutil`.", "So, the difference between my numbers and yours is related to the fact that `libmemusage.so` only reports heap memory whereas `psutil` reports all memory used (text, heap, stack, etc).\r\n\r\nI'll try to get a script to get all the memory usage reported and then we can do a bisection to identify a culprit.", "Want to run these numbers with you:\r\n\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     9  235.328 MiB  235.328 MiB   @profile\r\n    10                             def train():\r\n    11  235.328 MiB    0.000 MiB     K = 5000 # Number of images\r\n    12  235.328 MiB    0.000 MiB     N = 512  # Image size\r\n    13                             \r\n    14  235.328 MiB    0.000 MiB     MAX_SIGNAL = 5000 # The values of the training data range from 0 to this\r\n    15                             \r\n    16 7735.578 MiB    0.000 MiB     def build_model():\r\n    17                                 '''Create a simple test model.'''\r\n    18                             \r\n    19 7736.559 MiB    0.980 MiB       inputs = Input((N,N,1))\r\n    20 16955.355 MiB 7505.352 MiB       s = Lambda(lambda x: x / MAX_SIGNAL) (inputs)\r\n    21 7797.023 MiB   60.109 MiB       s = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(s)\r\n    22 7797.023 MiB    0.000 MiB       outputs = s\r\n    23                             \r\n    24 7797.070 MiB    0.047 MiB       return Model(inputs=[inputs], outputs=[outputs])\r\n    25                             \r\n    26                               # Generate some random data\r\n    27 2735.699 MiB 2500.371 MiB     x_train = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n    28 3985.672 MiB 1249.973 MiB     y_train = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n    29 6485.637 MiB 2499.965 MiB     x_val   = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n    30 7735.578 MiB 1249.941 MiB     y_val   = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n    31                               # In total, the above arrays should be 7 680 000 kB\r\n    32                             \r\n    33 7797.070 MiB    0.000 MiB     model = build_model()\r\n    34                             \r\n    35 7797.086 MiB    0.016 MiB     optimizer = tf.keras.optimizers.Adam()\r\n    36 7797.086 MiB    0.000 MiB     loss = tf.keras.losses.BinaryCrossentropy()\r\n    37                             \r\n    38 7797.465 MiB    0.379 MiB     model.compile(optimizer=optimizer, loss=loss)\r\n    39 22452.379 MiB 5497.023 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n```\r\n\r\nGenerated by\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Lambda, Conv2D\r\n\r\nprint(\"Tensorflow version: {}\".format(tf.__version__),flush=True)\r\n\r\n@profile\r\ndef train():\r\n  K = 5000 # Number of images\r\n  N = 512  # Image size\r\n\r\n  MAX_SIGNAL = 5000 # The values of the training data range from 0 to this\r\n\r\n  def build_model():\r\n    '''Create a simple test model.'''\r\n\r\n    inputs = Input((N,N,1))\r\n    s = Lambda(lambda x: x / MAX_SIGNAL) (inputs)\r\n    s = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(s)\r\n    outputs = s\r\n\r\n    return Model(inputs=[inputs], outputs=[outputs])\r\n\r\n  # Generate some random data\r\n  x_train = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n  y_train = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n  x_val   = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n  y_val   = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n  # In total, the above arrays should be 7 680 000 kB\r\n\r\n  model = build_model()\r\n\r\n  optimizer = tf.keras.optimizers.Adam()\r\n  loss = tf.keras.losses.BinaryCrossentropy()\r\n\r\n  model.compile(optimizer=optimizer, loss=loss)\r\n  model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\ntrain()\r\n```\r\n\r\nThey seem to match your comments so if there's nothing wrong in these I'll start a bisection.", "With `Tensorflow version: 2.1.0-dev20191003`\r\n\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     9  226.547 MiB  226.547 MiB   @profile\r\n    10                             def train():\r\n    11  226.547 MiB    0.000 MiB     K = 5000 # Number of images\r\n    12  226.547 MiB    0.000 MiB     N = 512  # Image size\r\n    13                             \r\n    14  226.547 MiB    0.000 MiB     MAX_SIGNAL = 5000 # The values of the training data range from 0 to this\r\n    15                             \r\n    16 7726.883 MiB    0.000 MiB     def build_model():\r\n    17                                 '''Create a simple test model.'''\r\n    18                             \r\n    19 7727.723 MiB    0.840 MiB       inputs = Input((N,N,1))\r\n    20 17070.504 MiB 7505.219 MiB       s = Lambda(lambda x: x / MAX_SIGNAL) (inputs)\r\n    21 7839.559 MiB  111.520 MiB       s = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(s)\r\n    22 7839.559 MiB    0.000 MiB       outputs = s\r\n    23                             \r\n    24 7839.625 MiB    0.066 MiB       return Model(inputs=[inputs], outputs=[outputs])\r\n    25                             \r\n    26                               # Generate some random data\r\n    27 2726.953 MiB 2500.406 MiB     x_train = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n    28 3976.910 MiB 1249.957 MiB     y_train = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n    29 6476.938 MiB 2500.027 MiB     x_val   = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n    30 7726.883 MiB 1249.945 MiB     y_val   = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n    31                               # In total, the above arrays should be 7 680 000 kB\r\n    32                             \r\n    33 7839.625 MiB    0.000 MiB     model = build_model()\r\n    34                             \r\n    35 7839.625 MiB    0.000 MiB     optimizer = tf.keras.optimizers.Adam()\r\n    36 7839.625 MiB    0.000 MiB     loss = tf.keras.losses.BinaryCrossentropy()\r\n    37                             \r\n    38 7840.156 MiB    0.531 MiB     model.compile(optimizer=optimizer, loss=loss)\r\n    39 21832.434 MiB 4761.930 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n```\r\n\r\nWith `Tensorflow version: 1.14.1-dev20190606`\r\n\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     9  190.207 MiB  190.207 MiB   @profile\r\n    10                             def train():\r\n    11  190.207 MiB    0.000 MiB     K = 5000 # Number of images\r\n    12  190.207 MiB    0.000 MiB     N = 512  # Image size\r\n    13                             \r\n    14  190.207 MiB    0.000 MiB     MAX_SIGNAL = 5000 # The values of the training data range from 0 to this\r\n    15                             \r\n    16 7690.289 MiB    0.000 MiB     def build_model():\r\n    17                                 '''Create a simple test model.'''\r\n    18                             \r\n    19 7693.930 MiB    3.641 MiB       inputs = Input((N,N,1))\r\n    20 7694.168 MiB    0.195 MiB       s = Lambda(lambda x: x / MAX_SIGNAL) (inputs)\r\n    21 7694.996 MiB    0.828 MiB       s = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(s)\r\n    22 7694.996 MiB    0.000 MiB       outputs = s\r\n    23                             \r\n    24 7694.996 MiB    0.000 MiB       return Model(inputs=[inputs], outputs=[outputs])\r\n    25                             \r\n    26                               # Generate some random data\r\n    27 2690.266 MiB 2500.059 MiB     x_train = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n    28 3940.262 MiB 1249.996 MiB     y_train = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n    29 6440.344 MiB 2500.082 MiB     x_val   = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n    30 7690.289 MiB 1249.945 MiB     y_val   = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n    31                               # In total, the above arrays should be 7 680 000 kB\r\n    32                             \r\n    33 7694.996 MiB    0.000 MiB     model = build_model()\r\n    34                             \r\n    35 7694.996 MiB    0.000 MiB     optimizer = tf.keras.optimizers.Adam()\r\n    36 7694.996 MiB    0.000 MiB     loss = tf.keras.losses.BinaryCrossentropy()\r\n    37                             \r\n    38 7695.430 MiB    0.434 MiB     model.compile(optimizer=optimizer, loss=loss)\r\n    39 9099.855 MiB 1404.426 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n```\r\n\r\nUnfortunately, the other pips in between these 2 packages don't work so bisection stops at this interval of 4 months.\r\n\r\nLet's see if the 2.0 preview package has something to offer", "I used this script: `version=2.0.0dev20190803; pip uninstall -y tf-nightly_2.0_preview tf-nightly tf-nightly-cpu tb-nightly tf-estimator-nightly && pip install -q tf_nightly_2.0_preview==${version} && (python -m memory_profiler test.py | tee profiles/${version})`\r\n\r\nThe output (last 2 content lines of all versions I tested, note that some are `tf-nightly` instead of `tf-nightly-2.0-preview`):\r\n\r\n```\r\n==> 1.13.0.dev20190101 <==\r\n    39 9683.285 MiB 2013.887 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\n==> 1.14.1.dev20190301 <==\r\n    39 9185.703 MiB 1500.035 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\n==> 1.14.1dev20190601 <==\r\n    39 9000.578 MiB 1301.863 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\n==> 1.14.1dev20190606 <==\r\n    39 9099.855 MiB 1404.426 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\n==> 2.0.0dev20190801 <==\r\n    39 8766.527 MiB 1017.652 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\n==> 2.0.0dev20190802 <==\r\n    39 8871.020 MiB 1116.914 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\n==> 2.0.0dev20190803 <==\r\n    39 12442.414 MiB    0.000 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\n==> 2.0.0dev20190804 <==\r\n    39 12368.871 MiB    0.000 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\n==> 2.0.0dev20190807 <==\r\n    39 12549.047 MiB    0.000 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\n==> 2.0.0dev20190815 <==\r\n    39 12368.543 MiB    0.000 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\n==> 2.0.0dev20190901 <==\r\n    39 21265.855 MiB 4352.922 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\n==> 2.1.0dev20191003 <==\r\n    39 21832.434 MiB 4761.930 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\n==> 2.1.0.dev20200106 <==\r\n    39 22483.957 MiB 5420.344 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n```\r\n\r\nSo, it looks like there was one memory consumption error introduced between 2019/08/02 and 2019/08/03 (for about 4GB) and another one somewhere in between 2019/08/15 and 2019/09/01\r\n\r\nNext step would be to identify the commits in this range and see which ones can be culprits.\r\n\r\nOr, if the bisect approach is not a good one (since these commits are too far away), I would reassign this issue to someone from keras team.\r\n\r\nCC: @fchollet", "The second memory increase happens on 2019/09/16. It seems in August we introduced both of these bugs.\r\n\r\n```\r\n==> 2.0.0dev20190815 <==\r\n    39 12368.543 MiB    0.000 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\n==> 2.0.0dev20190816 <==\r\n    39 20313.523 MiB 3606.793 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n```", "Thanks. So the issue is reproducible. Below I put some numbers obtained via `memory_profiler` on my machine, just in case they are needed. They result from your profiling script (with the difference that I have reduced the number of epochs to 1 to keep tests quick).\r\n\r\nWith `tf-nightly-cpu==2.1.0.dev20200103`:\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     9  220.074 MiB  220.074 MiB   @profile\r\n    10                             def train():\r\n    11  220.227 MiB    0.152 MiB     K = 5000 # Number of images\r\n    12  220.227 MiB    0.000 MiB     N = 512  # Image size\r\n    13                             \r\n    14  220.227 MiB    0.000 MiB     MAX_SIGNAL = 5000 # The values of the training data range from 0 to this\r\n    15                             \r\n    16 7720.363 MiB    0.000 MiB     def build_model():\r\n    17                                 '''Create a simple test model.'''\r\n    18                             \r\n    19 7721.395 MiB    1.031 MiB       inputs = Input((N,N,1))\r\n    20 16378.168 MiB 7505.426 MiB       s = Lambda(lambda x: x / MAX_SIGNAL) (inputs)\r\n    21 7729.184 MiB    7.410 MiB       s = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(s)\r\n    22 7729.184 MiB    0.000 MiB       outputs = s\r\n    23                             \r\n    24 7729.184 MiB    0.000 MiB       return Model(inputs=[inputs], outputs=[outputs])\r\n    25                             \r\n    26                               # Generate some random data\r\n    27 2720.391 MiB 2500.164 MiB     x_train = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n    28 3970.336 MiB 1249.945 MiB     y_train = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n    29 6470.418 MiB 2500.082 MiB     x_val   = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n    30 7720.363 MiB 1249.945 MiB     y_val   = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n    31                               # In total, the above arrays should be 7 680 000 kB\r\n    32                             \r\n    33 7729.184 MiB    0.000 MiB     model = build_model()\r\n    34                             \r\n    35 7729.184 MiB    0.000 MiB     optimizer = tf.keras.optimizers.Adam()\r\n    36 7729.184 MiB    0.000 MiB     loss = tf.keras.losses.BinaryCrossentropy()\r\n    37                             \r\n    38 7729.762 MiB    0.578 MiB     model.compile(optimizer=optimizer, loss=loss)\r\n    39 16941.863 MiB  563.695 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=1)\r\n```\r\n\r\nWith `tf-nightly-gpu==2.1.0.dev20200103`:\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     9  236.008 MiB  236.008 MiB   @profile\r\n    10                             def train():\r\n    11  236.008 MiB    0.000 MiB     K = 5000 # Number of images\r\n    12  236.008 MiB    0.000 MiB     N = 512  # Image size\r\n    13                             \r\n    14  236.008 MiB    0.000 MiB     MAX_SIGNAL = 5000 # The values of the training data range from 0 to this\r\n    15                             \r\n    16 7736.047 MiB    0.000 MiB     def build_model():\r\n    17                                 '''Create a simple test model.'''\r\n    18                             \r\n    19 7737.211 MiB    1.164 MiB       inputs = Input((N,N,1))\r\n    20 17164.410 MiB 7505.250 MiB       s = Lambda(lambda x: x / MAX_SIGNAL) (inputs)\r\n    21 8487.398 MiB  750.059 MiB       s = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(s)\r\n    22 8487.398 MiB    0.000 MiB       outputs = s\r\n    23                             \r\n    24 8487.398 MiB    0.000 MiB       return Model(inputs=[inputs], outputs=[outputs])\r\n    25                             \r\n    26                               # Generate some random data\r\n    27 2736.074 MiB 2500.066 MiB     x_train = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n    28 3986.020 MiB 1249.945 MiB     y_train = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n    29 6486.102 MiB 2500.082 MiB     x_val   = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n    30 7736.047 MiB 1249.945 MiB     y_val   = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n    31                               # In total, the above arrays should be 7 680 000 kB\r\n    32                             \r\n    33 8487.398 MiB    0.000 MiB     model = build_model()\r\n    34                             \r\n    35 8487.398 MiB    0.000 MiB     optimizer = tf.keras.optimizers.Adam()\r\n    36 8487.398 MiB    0.000 MiB     loss = tf.keras.losses.BinaryCrossentropy()\r\n    37                             \r\n    38 8487.973 MiB    0.574 MiB     model.compile(optimizer=optimizer, loss=loss)\r\n    39 17387.664 MiB  223.254 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=1)\r\n```", "With `tf-nightly-2.0-preview==2.0.0dev20190802`:\r\n```\r\n    38 7702.641 MiB    0.910 MiB     model.compile(optimizer=optimizer, loss=loss)\r\n    39 8771.934 MiB 1069.293 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=1)\r\n```\r\n\r\nWith `tf-nightly-2.0-preview==2.0.0dev20190803`:\r\n```\r\n    38 7701.055 MiB    1.281 MiB     model.compile(optimizer=optimizer, loss=loss)\r\n    39 12361.051 MiB    0.000 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=1)\r\n```\r\n\r\nWith `tf-nightly-2.0-preview==2.0.0dev20190815`:\r\n```\r\n    38 7703.402 MiB    0.891 MiB     model.compile(optimizer=optimizer, loss=loss)\r\n    39 12475.961 MiB    0.000 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=1)\r\n```\r\n\r\nWith `tf-nightly-2.0-preview==2.0.0dev20190816`:\r\n```\r\n    38 7703.547 MiB    1.141 MiB     model.compile(optimizer=optimizer, loss=loss)\r\n    39 16919.344 MiB  279.367 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=1)\r\n```\r\n\r\nThis confirms your conclusions: there were two points at which some bugs were introduced. But interestingly, I get different memory usage for `2.0.0dev20190816` than you (~16GB vs ~20GB).\r\n\r\nMoreover, I have observed that for `2.0.0dev20190803` and `2.0.0dev20190815` the memory usage reported by `memory_profiler` (~12GB) is significantly different than the RSS memory usage reported by `ps` (~20GB). For the remaining versions that I have tested now (`2.0.0dev20190802`, `2.0.0dev20190816`, `2.1.0.dev20200103`), the reports of `memory_profiler` more or less match the reports of `ps`.", "Just for reference, with `tensorflow==1.14.0`:\r\n```\r\n    38 7688.488 MiB    0.598 MiB     model.compile(optimizer=optimizer, loss=loss)\r\n    39 9140.660 MiB 1452.172 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=1)\r\n```\r\n\r\nand with `tensorflow=2.0.0-b1` (which was the last pre-release without the issue):\r\n```\r\n    38 7696.805 MiB    0.703 MiB     model.compile(optimizer=optimizer, loss=loss)\r\n    39 9192.766 MiB 1495.961 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=1)\r\n```", "With the latest tf-nightly as can be seen on [colab link](https://colab.research.google.com/drive/1mHK-Zhok-e4FXYaDPvgdjRkasiIdwUgz?usp=sharing#scrollTo=ouUwL95S6kYp), the memory usage is down to 10311MB vs 9140 MB (TF 1.14). ", "@yhliang2018, the issue was not reproducible on colab from the very beginning (see the discussion above), just on local machines. So closing the issue basing on colab results is not relevant. Said that, the issue is not fixed for me with tf-nigtly. I have just tested both TF 2.1.0 and TF nightly (2.2.0-dev20200123) on my local workstation and I obtain the same results as before (i.e. twice too high memory consumption). So could you reopen the issue, please? Thanks!\r\n\r\nBTW, your colab link does not grant public access so I cannot inspect it.\r\n\r\n@mihaimaruseac, can you confirm that the issue still persists with TF nightly?", "It seems Colab doesn't properly measure memory usage even in this case:\r\n\r\n```\r\nTensorflow version: 2.2.0-dev20200123\r\n2020-01-27 08:54:30.469996: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-01-27 08:54:30.807434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:04:00.0 name: Quadro M2000 computeCapability: 5.2\r\ncoreClock: 1.1625GHz coreCount: 6 deviceMemorySize: 3.93GiB deviceMemoryBandwidth: 98.44GiB/s\r\n2020-01-27 08:54:30.807751: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-01-27 08:54:30.807853: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-01-27 08:54:30.808093: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2020-01-27 08:54:30.808186: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2020-01-27 08:54:30.808274: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-01-27 08:54:30.808363: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2020-01-27 08:54:30.808452: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n2020-01-27 08:54:30.808468: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1595] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-01-27 08:54:30.809799: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-27 08:54:30.855778: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2594210000 Hz\r\n2020-01-27 08:54:30.860489: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4594760 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-27 08:54:30.860522: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-01-27 08:54:30.863169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-27 08:54:30.863195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      \r\n2020-01-27 08:54:31.060626: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2621440000 exceeds 10% of free system memory.\r\n2020-01-27 08:54:33.691666: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2621440000 exceeds 10% of free system memory.\r\nTrain on 5000 samples, validate on 5000 samples\r\nEpoch 1/10\r\n5000/5000 [==============================] - 68s 14ms/sample - loss: 0.7023 - val_loss: 0.6957\r\nEpoch 2/10\r\n5000/5000 [==============================] - 65s 13ms/sample - loss: 0.6945 - val_loss: 0.6937\r\nEpoch 3/10\r\n5000/5000 [==============================] - 65s 13ms/sample - loss: 0.6934 - val_loss: 0.6932\r\nEpoch 4/10\r\n5000/5000 [==============================] - 66s 13ms/sample - loss: 0.6932 - val_loss: 0.6932\r\nEpoch 5/10\r\n5000/5000 [==============================] - 65s 13ms/sample - loss: 0.6932 - val_loss: 0.6932\r\nEpoch 6/10\r\n5000/5000 [==============================] - 66s 13ms/sample - loss: 0.6932 - val_loss: 0.6931\r\nEpoch 7/10\r\n5000/5000 [==============================] - 66s 13ms/sample - loss: 0.6931 - val_loss: 0.6931\r\nEpoch 8/10\r\n5000/5000 [==============================] - 65s 13ms/sample - loss: 0.6931 - val_loss: 0.6931\r\nEpoch 9/10\r\n5000/5000 [==============================] - 65s 13ms/sample - loss: 0.6931 - val_loss: 0.6931\r\nEpoch 10/10\r\n5000/5000 [==============================] - 65s 13ms/sample - loss: 0.6931 - val_loss: 0.6931\r\nFilename: test.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     9  242.773 MiB  242.773 MiB   @profile\r\n    10                             def train():\r\n    11  242.969 MiB    0.195 MiB     K = 5000 # Number of images\r\n    12  242.969 MiB    0.000 MiB     N = 512  # Image size\r\n    13                             \r\n    14  242.969 MiB    0.000 MiB     MAX_SIGNAL = 5000 # The values of the training data range from 0 to this\r\n    15                             \r\n    16 7743.020 MiB    0.000 MiB     def build_model():\r\n    17                                 '''Create a simple test model.'''\r\n    18                             \r\n    19 7743.930 MiB    0.910 MiB       inputs = Input((N,N,1))\r\n    20 16946.887 MiB 7505.203 MiB       s = Lambda(lambda x: x / MAX_SIGNAL) (inputs)\r\n    21 7760.332 MiB   16.059 MiB       s = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(s)\r\n    22 7760.336 MiB    0.004 MiB       outputs = s\r\n    23                             \r\n    24 7760.379 MiB    0.043 MiB       return Model(inputs=[inputs], outputs=[outputs])\r\n    25                             \r\n    26                               # Generate some random data\r\n    27 2743.180 MiB 2500.211 MiB     x_train = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n    28 3993.039 MiB 1249.859 MiB     y_train = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n    29 6493.141 MiB 2500.102 MiB     x_val   = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n    30 7743.020 MiB 1249.879 MiB     y_val   = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n    31                               # In total, the above arrays should be 7 680 000 kB\r\n    32                             \r\n    33 7760.379 MiB    0.000 MiB     model = build_model()\r\n    34                             \r\n    35 7760.398 MiB    0.020 MiB     optimizer = tf.keras.optimizers.Adam()\r\n    36 7760.398 MiB    0.000 MiB     loss = tf.keras.losses.BinaryCrossentropy()\r\n    37                             \r\n    38 7760.859 MiB    0.461 MiB     model.compile(optimizer=optimizer, loss=loss)\r\n    39 21661.305 MiB 4714.418 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n```", "@gdudziuk Can you check [this link](https://colab.research.google.com/gist/yhliang2018/9d45291c6ff4bc061f340b754ef47815/gitissue-35030.ipynb) to access the code in colab?\r\n\r\nOkay, thanks for reporting this. I will try it with my local machine then.\r\n", "Yes, the latter link is accessible for me. Thanks.", ">     20 16946.887 MiB 7505.203 MiB       s = Lambda(lambda x: x / MAX_SIGNAL) (inputs)\r\nand \r\n>     39 21661.305 MiB 4714.418 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)\r\n\r\nIn the last reply from @mihaimaruseac , the memory usage of the Lambda layer (line 20) and the model.fit (line 39) is very suspicious. Lambda layer increases about 7.5GB in build_model(), and model.fit() has an increase of 4.7GB. Both memory cost are not reasonable.\r\n\r\nI test the code locally on my workstation (with batch_size=64, epoch=1), and get a similar number for the Lambda layer but a good number for model.fit():\r\n```\r\nTensorflow version: 2.2.0-dev20200203\r\n2020-02-03 14:38:57.581398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-02-03 14:38:57.591001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: Quadro K1200 computeCapability: 5.0\r\ncoreClock: 1.0325GHz coreCount: 4 deviceMemorySize: 3.93GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-02-03 14:38:57.591160: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2020-02-03 14:38:57.591216: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2020-02-03 14:38:57.591267: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2020-02-03 14:38:57.591316: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2020-02-03 14:38:57.591373: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2020-02-03 14:38:57.591423: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2020-02-03 14:38:57.591473: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2020-02-03 14:38:57.591483: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1595] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-02-03 14:38:57.591928: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-02-03 14:38:57.621484: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3591820000 Hz\r\n2020-02-03 14:38:57.622782: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5520620 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-02-03 14:38:57.622822: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-02-03 14:38:57.625144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-03 14:38:57.625174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      \r\nTrain on 5000 samples, validate on 5000 samples\r\n5000/5000 [==============================] - 99s 20ms/sample - loss: 0.7251 - val_loss: 0.7100\r\nFilename: gitissue_35030.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    10  284.035 MiB  284.035 MiB   @profile\r\n    11                             def train():\r\n    12  284.035 MiB    0.000 MiB     K = 5000 # Number of images\r\n    13  284.035 MiB    0.000 MiB     N = 512  # Image size\r\n    14                             \r\n    15  284.035 MiB    0.000 MiB     MAX_SIGNAL = 5000 # The values of the training data range from 0 to this\r\n    16                             \r\n    17 7784.559 MiB    0.000 MiB     def build_model():\r\n    18                                 '''Create a simple test model.'''\r\n    19                             \r\n    20 7785.520 MiB    0.961 MiB       inputs = Input((N,N,1))\r\n    21 16253.156 MiB 7505.062 MiB       s = Lambda(lambda x: x / MAX_SIGNAL) (inputs)\r\n    22 7807.395 MiB   21.781 MiB       s = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(s)\r\n    23 7807.395 MiB    0.000 MiB       outputs = s\r\n    24                             \r\n    25 7807.395 MiB    0.000 MiB       return Model(inputs=[inputs], outputs=[outputs])\r\n    26                             \r\n    27                               # Generate some random data\r\n    28 278> 39 21661.305 MiB 4714.418 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10)4.273 MiB 2500.238 MiB     x_train = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n    29 4034.531 MiB 1250.258 MiB     y_train = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n    30 6534.355 MiB 2499.824 MiB     x_val   = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\n    31 7784.559 MiB 1250.203 MiB     y_val   = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)  # Should be 1 280 000 kB\r\n    32                               # In total, the above arrays should be 7 680 000 kB\r\n    33                             \r\n    34 7807.395 MiB    0.000 MiB     model = build_model()\r\n    35                             \r\n    36 7807.395 MiB    0.000 MiB     optimizer = tf.keras.optimizers.Adam()\r\n    37 7807.746 MiB    0.352 MiB     loss = tf.keras.losses.BinaryCrossentropy()\r\n    38                             \r\n    39 7808.074 MiB    0.328 MiB     model.compile(optimizer=optimizer, loss=loss)\r\n    40 16593.883 MiB  340.727 MiB     model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=64, epochs=1)\r\n```\r\n\r\nAs you can see, eventually my test with tf-nightly-2.2.0-dev20200203 costs ~ 16GB, vs. ~21GB from tf-nightly-2.2.0-dev20200123 (@mihaimaruseac 's number in the last reply). As there is no big change from the Keras side in the past two weeks, it's strange to see the 5GB difference.\r\n\r\nLooks like the results are pretty not consistent. @gdudziuk may you help re-run the test to see what number you can get? \r\n\r\nI will continue to dig into the Lambda layer to see how the memory cost comes from.\r\n", "An update on the problem: Lambda layer has no memory usage issue, and it looks like memory_profiler has a wrong mapping between line # and memory usage/increment. When inserting `pdb.set_trace()` before `model.fit()`, `htop` (or `top`) shows no memory increase after `model = build_model()`, but it increases dramatically in `model.fit`:\r\n\r\n```\r\n  model = build_model()\r\n\r\n  optimizer = tf.keras.optimizers.Adam()\r\n  loss = tf.keras.losses.BinaryCrossentropy()\r\n\r\n  model.compile(optimizer=optimizer, loss=loss)\r\n  import pdb\r\n  pdb.set_trace()\r\n  model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=64, epochs=1, steps_per_epoch=10)\r\n```\r\n\r\nThe root cause is from the way TF handles how to convert numpy array to tensors. If you run the following code:\r\n```\r\nx_train = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16)  # Should be 2 560 000 kB\r\ny = tf.data.Dataset.from_tensor_slices(x_train)\r\n```\r\nYou will see an increase of 2.5GB on the memory usage (which is almost the dataset size). Even for `yy = tf.convert_to_tensor(x_train)`, the memory usage is increased too.\r\n\r\nWe are working on the fix. Please stay tuned. ", "Yes, I have already observed that my results measured with `memory_profiler` are different than @mihaimaruseac's. As I understand we don't trust `memory_profiler` now, so I have checked some details for `tf-nightly-2.2.0-dev20200203` using `ps`:\r\n\r\n- After the line `y_val   = ...` (the end of creation of numpy arrays) I get ~7.7 GB RSS memory usage. This matches very close the size of the numpy arrays.\r\n- After the next line, i.e. `model=model_build()`, RSS memory jumps up to  ~8.5 GB.\r\n- Then nothing changes significantly until `model.fit`.\r\n- At `model.fit` RSS memory usage increases dramatically to ~17 GB.\r\n\r\nWaiting for your fix. Thanks.", "@gdudziuk Thank you for your patience and updates!\r\n\r\nMy understanding is that, `model_build()` should not take too much memory, as it only builds the model by creating several variables (in `Conv2D` layer for this case). My guess is that the increase from 7.7 GB to 8.5GB may be some warm up of `model.fit()`, so not sure if `ps` is reliable. One way to verify this is to insert some input statements and then check the `ps`  value, like:\r\n\r\n```\r\n  Input('...')\r\n  model = build_model()\r\n  Input('...')\r\n  optimizer = tf.keras.optimizers.Adam()\r\n  loss = tf.keras.losses.BinaryCrossentropy()\r\n\r\n  model.compile(optimizer=optimizer, loss=loss)\r\n  model.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=64, epochs=2, steps_per_epoch=10)\r\n```\r\nI also measure the time of each stage, and plot the accumulative memory usage (from memory_profiler though) as follows:\r\n```\r\nnumpy time:  25.583900690078735\r\nbuild_model time:  0.07410025596618652\r\nmodel config time:  0.010079622268676758\r\nmodel.fit time:  71.99052238464355\r\n```\r\n![test](https://user-images.githubusercontent.com/36285763/73875234-cb3e8880-4809-11ea-86d7-2412194d2934.png)\r\n\r\n\r\nGiven the it's so fast for model build and config, I think the memory increase is mainly from `model.fit()`.\r\n", "I have observed something similar concerning also `model.predict`. Just add the following line at the and of the reproduction code for this issue:\r\n\r\n```python\r\ny_pred = model.predict(x_val,batch_size=8)\r\n```\r\n\r\n`y_pred.dtype` is `float32`, based on which we may compute the size of `y_pred` as 5GB (5 120 000 kB). After `model.fit` the Python process occupies ~17.5 GB of RSS memory. After `y_pred = model.predict` it occupies ~25 GB, giving an increase of ~7.5 GB, which is ~2.5 GB more than the size of `y_pred`. After additionally doing `del y_pred` the RSS memory use decreases to ~20 GB rather that the previous ~17.5 GB, so the ~2.5 GB overhead persists.\r\n\r\nIs such overhead necessary or should we consider it a bug? Do you think I should report it as a separate issue?\r\n\r\nI have tested the above behavior for TF `2.1.0` and for TF nightly version `2.2.0-dev20200212`.", "Moreover, there seems to be a memory leak. When I repeat `y_pred = model.predict` followed by `del y_pred` several times, the memory usage increases ~2.0-2.5 GB almost each time (sometimes this does not happen, but usually it does).\r\n\r\nIt seems to be already reported in the issue keras-team/keras#13118 which is still open at the moment. The latter issue concerns Keras with TF backend  `1.12.0` and has been confirmed by some participants for TF `2.0.0`. Now I observe the same in `tf.keras` using TF `2.1.0` and TF nightly `2.2.0-dev20200212`.", "I am having the same issue on all 2.0.0 version (alphas and betas). I am working with concatenated images so 320*4px by 180 px height", "I am also experiencing continually increasing memory usage with tensorflow 2.1.0 and 2.0.0\r\nI'm not using Lambda layers - just Dense and Dropout layers.\r\nAdding calls to del model, tf.keras.backend.clear_session() and gc.collect() slows the rate of growth, but it still grows endlessly.\r\n\r\nI'm using Python 3.6.9 on Linux Mint 19.1\r\nLike others, I've been unable to reproduce the issue in colab.\r\n\r\nI've been able to reproduce the issue locally with the following packages:\r\ntensorflow-2.1.0\r\ntensorflow-cpu-2.1.0\r\ntensorflow-gpu-2.1.0\r\ntensorflow-2.0.0\r\ntensorflow-1.15.0\r\n\r\n![tensorflow-2 1 0](https://user-images.githubusercontent.com/45711639/76207697-02a9b600-624a-11ea-8d33-38fdaa798cee.png)\r\n\r\nThis package gives stable memory usage:\r\ntensorflow-cpu-1.15.0\r\n\r\n![tensorflow-cpu-1 15 0](https://user-images.githubusercontent.com/45711639/76207702-03dae300-624a-11ea-9eb7-029549df9f00.png)\r\n\r\nHere's the code I used to test:\r\n\r\n```\r\nimport sys\r\nimport random\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport gc\r\nimport memory_profiler\r\n\r\n@profile\r\ndef create_neural_network_model(input_size, output_size):\r\n\r\n    print(\"Creating model: input_size={}, output_size={}\".format(input_size, output_size))\r\n\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Input(shape=(input_size,)),\r\n        tf.keras.layers.Dense(1024, activation='relu'),\r\n        tf.keras.layers.Dropout(0.2),\r\n        tf.keras.layers.Dense(output_size),\r\n    ])\r\n\r\n    model.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\n    return model\r\n\r\n\r\n@profile\r\ndef train_model():\r\n\r\n    input_size = 80 * 24\r\n    output_size = 60\r\n\r\n    print(\"Generating data.....\")\r\n    # simulate 100 moves\r\n    inputs = np.random.uniform(size=(100, input_size))\r\n    outputs = np.random.random_integers(low=0, high=output_size-1, size=(100,))\r\n\r\n    print(\"Training model.....\")\r\n\r\n    tf.keras.backend.clear_session()\r\n    gc.collect()\r\n\r\n    model = create_neural_network_model(input_size, output_size)\r\n    \r\n    model.fit(x=inputs, y=outputs, epochs=3)\r\n    return model\r\n\r\n\r\n@profile\r\ndef main():\r\n    model = None\r\n    for i in range(50):\r\n        print(\"Iteration {}...\".format(i+1))\r\n        del model\r\n        model = train_model()\r\n        \r\nmain()\r\n\r\n```\r\n", "It looks like the bulk of the original problem has been indirectly fixed in 2.2. The test\r\n\r\n```python\r\n  x_train = np.random.randint(MAX_SIGNAL+1, size=(K, N, N, 1), dtype=np.uint16)\r\n  y_train = np.random.randint(1+1, size=(K, N, N, 1), dtype=np.bool)\r\n  x_val = np.random.randint(MAX_SIGNAL+1, size=(K, N, N, 1), dtype=np.uint16)\r\n  y_val = np.random.randint(1+1, size=(K, N, N, 1), dtype=np.bool)\r\n\r\n  model = Model(inputs=[inputs], outputs=[s])\r\n\r\n  model.compile(\r\n      optimizer=tf.keras.optimizers.Adam(),\r\n      loss=tf.keras.losses.BinaryCrossentropy())\r\n\r\n  model.fit(\r\n      x=x_train, y=y_train, validation_data=(x_val, y_val),\r\n      batch_size=8, epochs=epochs)\r\n```\r\n\r\nGives the following in 2.0:\r\n\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    26   2577.0 MiB      0.2 MiB     model = Model(inputs=[inputs], outputs=[s])\r\n    27\r\n    28   2577.0 MiB      0.0 MiB     model.compile(\r\n    29   2577.0 MiB      0.0 MiB         optimizer=tf.keras.optimizers.Adam(),\r\n    30   2577.5 MiB      0.5 MiB         loss=tf.keras.losses.BinaryCrossentropy())\r\n    31\r\n    32   2577.5 MiB      0.0 MiB     model.fit(\r\n    33   2577.5 MiB      0.0 MiB         x=x_train, y=y_train, validation_data=(x_val, y_val),\r\n    34   7968.6 MiB   5391.1 MiB         batch_size=8, epochs=epochs)\r\n```\r\n\r\nand the following in 2.2:\r\n\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    28   1839.7 MiB      0.1 MiB     model = Model(inputs=[inputs], outputs=[s])\r\n    29\r\n    30   1839.7 MiB      0.0 MiB     model.compile(\r\n    31   1839.7 MiB      0.0 MiB         optimizer=tf.keras.optimizers.Adam(),\r\n    32   1839.7 MiB      0.0 MiB         loss=tf.keras.losses.BinaryCrossentropy())\r\n    33\r\n    34   1839.7 MiB      0.0 MiB     model.fit(\r\n    35   1839.7 MiB      0.0 MiB         x=x_train, y=y_train, validation_data=(x_val, y_val),\r\n    36   2475.7 MiB    636.1 MiB         batch_size=8, epochs=epochs)\r\n```\r\n\r\nI'll close this bug - the increasing memory problem, as mentioned by @gdudziuk, is already tracked in another bug.", "Excuse me for the late answer. I'm back to the topic now.\r\n\r\nFirst, let me acknowledge your effort in digging into the issue which I appreciate. But unfortunately, I cannot confirm the issue is fixed.\r\n\r\nTo reiterate briefly, the original problem was as follows. It would be reasonable to expect that the memory usage by the test code for this issue is ca. the size of the data arrays (plus some overhead). But in TF 2.0 and 2.1 the memory usage was _twice_ the size of the data arrays (plus overhead). Hence this issue.\r\n\r\nThe current state is as follows:\r\n\r\n- In TF 2.2 things indeed have changed albeit it's hard to call it fixed. Now, with the test code posted in this issue, I have a memory leak about 0.5 x data size in each epoch. In other words, if the size of the data arrays is ca. 8 GB, then the memory usage increases ca. 4 GB each epoch.\r\n\r\n- If I wrapp the numpy data arrays in datasets:\r\n\r\n  ```python\r\n  ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(8)\r\n  ds_val = tf.data.Dataset.from_tensor_slices((x_val,y_val)).batch(8)\r\n  ...\r\n  model.fit(ds_train, validation_data=ds_val, epochs=10)\r\n  ```\r\n\r\n  then the behavior is the same in TF 2.2 as in 2.1 and 2.0, namely the memory usage is twice the data size plus overhead.\r\n\r\nCan you confirm these results, @yhliang2018, @mihaimaruseac?", "Hi,\r\nI faced the exact same problems discussed above. There a considerable memory leak after each epoch. Eventually, the process is killed by the OS (Ubuntu 18) as the memory consumption goes beyond limit. I had to revert back to Tensorflow v1.15 and modify the code for compatibility. It doesn't leak memory and it's faster. I have to mention that with TF v1.15 you must import Keras independently as there is no tf.keras. I didn't test the newer TF 2.2 yet (only 2.0 and 2.1)\r\n ", "@kattan1969, there is tf.keras in TF 1.15. You can import it the same way as in TF 2.x. Do you have any problems with that? How have you installed 1.15?\r\n\r\nFor me TF 2.x is considerably faster than 1.x, the issue is just the memory usage. But note that if you train on small data comprising just one or few batches then `model.train_on_batch` performs better that `model.fit`. Unaware of that, I blamed TF 2.x to have poor performance and opened issue #35124 a couple of moths ago (note in particular [this comment in #35124](https://github.com/tensorflow/tensorflow/issues/35124#issuecomment-581537987) which explains the solution). Of course, I don't know if this applies to your situation. But if so, then it may help.", "> But with TF from 2.0.0-rc0 to 2.1.0-rc0 the Python process consumes twice that much RAM. \r\n\r\nHaving the exact problem. Also, it appears to me that each epoch the RAM usage increases more and more, to the point it crashes. ", "Btw, if you want to prevent Linux from freezing everytime it uses all the RAM, and just crash the malfunctioning program (i.e., tensorflow), I recommend disabling SWAP:\r\n\r\n```ssh\r\nsudo swapoff -a \r\n```", "This issue is very long, and it's hard to tell whether there is a memory leak/growth problem. Can you try with tf-nightly, and open a new issue with the Keras workflow if this is still an issue?", "Sure. The issue is still present in tf-nightly `2.5.0-dev20200626`, so I have opened issue #40942.", "I have the same issue here and found out that it happens when we provide validation data. if you remove validation data from fit() then memory usage will remain constant and without any increaing.\r\nI also noticed that if we provide validation data, after each epoch, memory usage goes up, but after 10 epochs a portion of it gets cleared and again it will go up until the next 10th epoch. these steps will go on until ram is full and training crashes.\r\nI should also mention I am using tf2.2", "@kaveh-es but how do you use fit() without validation? The networks needs the validation data for training, or not?", "Calling a fit (CPU, for testing mostly) a ESRGAN GAN model with 46M parameters can suddently consume +100G bytes of RAM and keep growing, finally it crashes machine with 256G  :( ... ! TF 2.4.1.\r\n\r\nHELP !\r\n\r\nSteve"]}, {"number": 35029, "title": "failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GKE with GPU / Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: **tensorflow-2.0.0-cp38-cp38-linux_x86_64**\r\n- Python version: **3.8**\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1.168-1 / 7.6.4.38-1\r\n- GPU model and memory: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe process terminated with theses errors: \r\n2019-12-11 15:23:31 classification-ld8d9 tensorflow[7] WARNING Entity <bound method ResnetPreprocess.call of <src.model.ResnetPreprocess object at 0x7fb870241d60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: invalid value for \"node\": expected \"ast.AST\", got \"<class 'NoneType'>\"; to visit lists of nodes, use \"visit_block\" instead\r\n2019-12-11 15:24:12.377087: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-11 15:24:12.377554: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-12-11 15:24:12.377749: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-12-11 15:24:12.377806: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-12-11 15:24:12.570359: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-11 15:24:28.471575: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-12-11 15:24:28.471626: W tensorflow/stream_executor/stream.cc:1919] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n2019-12-11 15:24:28.471658: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Blas GEMM launch failed : a.shape=(32, 25088), b.shape=(25088, 8), m=32, n=8, k=25088\r\n         [[{{node model/dense/MatMul}}]]\r\nTraceback (most recent call last):\r\n  File \"/opt/workspace/app.py\", line 83, in <module>\r\n    training(config[\"train\"], config[\"telegram\"], config[\"others\"])\r\n  File \"/opt/workspace/src/training.py\", line 40, in training\r\n    train_model(dataset_train.dataset, dataset_val.dataset, model, config, n_classes, config_others)\r\n  File \"/opt/workspace/src/training.py\", line 98, in train_model\r\n    loss_train, logits, reg_loss = training_step(x_batch_train, y_batch_train)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/function.py\", line 1137, in _filtered_call\r\n    return self._call_flat(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/function.py\", line 1223, in _call_flat\r\n    flat_outputs = forward_function.call(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/function.py\", line 506, in call\r\n    outputs = execute.execute(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError:  Blas GEMM launch failed : a.shape=(32, 25088), b.shape=(25088, 8), m=32, n=8, k=25088\r\n         [[node model/dense/MatMul (defined at usr/local/lib/python3.8/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_training_step_9298]\r\n\r\nFunction call stack:\r\ntraining_step\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\n\r\n", "comments": ["@kevin8100, Provide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "@kevin8100, Similar issue comment [#33916](https://github.com/tensorflow/tensorflow/issues/33916#issuecomment-548811280). ", "> !\r\n\r\nHello  Gadagashwini ,\r\n\r\nThe sequence for training step:\r\n{ load tfrecords for training) \r\n{...}\r\n  x = keras.Input(shape=(224, 224, 3), batch_size=batch_size)\r\nthen call\r\n    x = resnet18(x, 0.01)\r\n\r\nyou can find the extracted logs here for more details:\r\n\r\n2019-12-11 15:36:42 classification-xbj8h root[8] DEBUG training with option {'tf_record_absolute_prefix_train': '/opt/workspace/tfrecord/tfrecord_train_', 'tf_record_absolute_prefix_val': '/opt/workspace/tfrecord/tfrecord_val_', 'training_preprocessing': 'random_crop', 'validation_preprocessing': 'random_crop', 'log_dir': '/opt/workspace/log', 'save_dir': '/opt/workspace', 'lr': 0.001, 'model': 'resnet18', 'load_model_path': '', 'epochs': 1000, 'steps_between_validation': 100, 'steps_between_save': 1000, 'validation_steps': 50, 'log_images': True, 'classes_names': ['Haribo', 'Champagne', 'Vitakraft', 'Empty', 'Bonduelle', 'Soupe', 'Lipton', 'Dentifrice'], 'gradient': 'ADAM', 'dataset': {'random_crop_size': 224, 'loader_buffer_size': 500000, 'num_parallel_reads': 2, 'num_parallel_calls': 1, 'batch_size': 32, 'shuffle_buffer_size': 1000, 'random_flip_left_right': True, 'random_flip_up_down': True, 'random_rot': True, 'random_hue_delta': 0.0, 'random_contrast': False}}\r\n2019-12-11 15:36:46 classification-xbj8h tensorflow[8] INFO Converted call: <function _create_dataset_reader.<locals>.read_one_file at 0x7f777c8a2a60>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=string>,)\r\n    kwargs: {}\r\nINFO:tensorflow:Whitelisted: <function _create_dataset_reader.<locals>.read_one_file at 0x7f777c8a2a60>: DoNotConvert rule for tensorflow\r\n2019-12-11 15:36:46 classification-xbj8h tensorflow[8] INFO Whitelisted: <function _create_dataset_reader.<locals>.read_one_file at 0x7f777c8a2a60>: DoNotConvert rule for tensorflow\r\nINFO:tensorflow:Converted call: <bound method ClassificationDataset.map_fn of <src.dataset.ClassificationDataset object at 0x7f777c89dee0>>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=string>,)\r\n    kwargs: {}\r\n2019-12-11 15:36:46 classification-xbj8h tensorflow[8] INFO Converting <bound method ClassificationDataset.map_fn of <src.dataset.ClassificationDataset object at 0x7f777c89dee0>>\r\nINFO:tensorflow:Source code of <bound method ClassificationDataset.map_fn of <src.dataset.ClassificationDataset object at 0x7f777c89dee0>>:\r\nINFO:tensorflow:Error transforming entity <bound method ClassificationDataset.map_fn of <src.dataset.ClassificationDataset object at 0x7f777c89dee0>>\r\nTraceback (most recent call last):\r\nStart of epoch 0\r\nINFO:tensorflow:Converted call: <function train_model.<locals>.training_step at 0x7f76e8414040>\r\n    args: (<tf.Tensor 'x:0' shape=(32, 224, 224, 3) dtype=uint8>, <tf.Tensor 'y:0' shape=(32, 8) dtype=int64>)\r\n    kwargs: {}\r\n2019-12-11 15:37:39 classification-xbj8h tensorflow[8] INFO Converting <function train_model.<locals>.training_step at 0x7f76e8414040>\r\nINFO:tensorflow:Source code of <function train_model.<locals>.training_step at 0x7f76e8414040>:\r\n\r\n@tf.function\r\ndef training_step(x, y):\r\n  with tf.GradientTape() as tape:\r\n    logits = model(tf.cast(x, tf.float32), training=True)\r\n    loss_train = loss_fn(y, logits)\r\n    reg_loss = model.losses\r\n    loss_train += tf.reduce_sum(reg_loss)\r\n  grads = tape.gradient(loss_train, model.trainable_weights)\r\n  optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n\r\n  return loss_train, logits, reg_loss\r\n\r\nINFO:tensorflow:Source code of <bound method ResnetPreprocess.call of <src.model.ResnetPreprocess object at 0x7f77701a6e20>>:\r\n\r\ndef call(self, x, training=None):\r\n  x /= 127.5\r\n  x -= 1.\r\n  return x\r\n\r\nDuring handling of the above exception, another exception occurred:\r\nValueError: invalid value for \"node\": expected \"ast.AST\", got \"<class 'NoneType'>\"; to visit lists of nodes, use \"visit_block\" instead\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 321, in convert\r\n    converted_entity_info = _convert_with_cache(entity, program_ctx,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 239, in _convert_with_cache\r\n    nodes, converted_name, entity_info = convert_entity_to_ast(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 471, in convert_entity_to_ast\r\n    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 669, in convert_func_to_ast\r\n    node = node_to_graph(node, context)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 698, in node_to_graph\r\n    node = converter.standard_analysis(node, context, is_initial=True)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/core/converter.py\", line 384, in standard_analysis\r\n    node = activity.resolve(node, context, None)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py\", line 498, in resolve\r\n    return ActivityAnalyzer(context, parent_scope).visit(node)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/usr/lib/python3.8/ast.py\", line 360, in visit\r\n    return visitor(node)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py\", line 442, in visit_FunctionDef\r\n    node.body = self.visit_block(node.body)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 371, in visit_block\r\n    replacement = self.visit(node)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/usr/lib/python3.8/ast.py\", line 360, in visit\r\n    return visitor(node)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py\", line 287, in visit_AugAssign\r\n    node.value = self.visit(node.value)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 458, in visit\r\n    raise ValueError(msg)\r\nValueError: invalid value for \"node\": expected \"ast.AST\", got \"<class 'NoneType'>\"; to visit lists of nodes, use \"visit_block\" instead\r\n2019-12-11 15:38:27 classification-xbj8h tensorflow[8] INFO Source code of <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f76e8796280>:\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n@function_lib.defun\r\ndef initialize_variables():\r\n  op_map = object_identity.ObjectIdentityDictionary()\r\n  for v, init in initializer_map.items():\r\n    with ops.init_scope():\r\n      if resource_variable_ops.var_is_initialized_op(v.handle):\r\n        # Ignore variables which are already initialized at trace time.\r\n        continue\r\n    op_map = lift_to_graph.lift_to_graph(\r\n        [init], ops.get_default_graph(), op_map=op_map)\r\n    v.assign(op_map[init])\r\n\r\n2019-12-11 15:38:27 classification-xbj8h tensorflow[8] INFO Converting <bound method ResnetPreprocess.call of <src.model.ResnetPreprocess object at 0x7f77701a6e20>>\r\nINFO:tensorflow:Source code of <bound method ResnetPreprocess.call of <src.model.ResnetPreprocess object at 0x7f77701a6e20>>:\r\n\r\ndef call(self, x, training=None):\r\n  x /= 127.5\r\n  x -= 1.\r\n  return x\r\n\r\nINFO:tensorflow:Error transforming entity <bound method ResnetPreprocess.call of <src.model.ResnetPreprocess object at 0x7f77701a6e20>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 321, in convert\r\n    converted_entity_info = _convert_with_cache(entity, program_ctx,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 239, in _convert_with_cache\r\n    nodes, converted_name, entity_info = convert_entity_to_ast(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 471, in convert_entity_to_ast\r\n    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 669, in convert_func_to_ast\r\n    node = node_to_graph(node, context)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 698, in node_to_graph\r\n    node = converter.standard_analysis(node, context, is_initial=True)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/core/converter.py\", line 384, in standard_analysis\r\n    node = activity.resolve(node, context, None)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py\", line 498, in resolve\r\n    return ActivityAnalyzer(context, parent_scope).visit(node)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/usr/lib/python3.8/ast.py\", line 360, in visit\r\n    return visitor(node)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py\", line 442, in visit_FunctionDef\r\n    node.body = self.visit_block(node.body)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 371, in visit_block\r\n    replacement = self.visit(node)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/usr/lib/python3.8/ast.py\", line 360, in visit\r\n    return visitor(node)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py\", line 287, in visit_AugAssign\r\n    node.value = self.visit(node.value)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 458, in visit\r\n    raise ValueError(msg)\r\nValueError: invalid value for \"node\": expected \"ast.AST\", got \"<class 'NoneType'>\"; to visit lists of nodes, use \"visit_block\" instead\r\n2019-12-11 15:38:27 classification-xbj8h tensorflow[8] INFO Error transforming entity <bound method ResnetPreprocess.call of <src.model.ResnetPreprocess object at 0x7f77701a6e20>>\r\n2019-12-11 15:38:27 classification-xbj8h tensorflow[8] WARNING Entity <bound method ResnetPreprocess.call of <src.model.ResnetPreprocess object at 0x7f77701a6e20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: invalid value for \"node\": expected \"ast.AST\", got \"<class 'NoneType'>\"; to visit lists of nodes, use \"visit_block\" instead\r\n2019-12-11 15:39:08.678289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-11 15:39:08.678746: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-12-11 15:39:08.678892: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-12-11 15:39:08.678931: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-12-11 15:39:08.872722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-11 15:39:24.970499: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-12-11 15:39:24.970554: W tensorflow/stream_executor/stream.cc:1919] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n2019-12-11 15:39:24.970600: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Blas GEMM launch failed : a.shape=(32, 25088), b.shape=(25088, 8), m=32, n=8, k=25088\r\n\t [[{{node model/dense/MatMul}}]]\r\nTraceback (most recent call last):\r\n  File \"/opt/workspace/app.py\", line 83, in <module>\r\n    training(config[\"train\"], config[\"telegram\"], config[\"others\"])\r\n  File \"/opt/workspace/src/training.py\", line 40, in training\r\n    train_model(dataset_train.dataset, dataset_val.dataset, model, config, n_classes, config_others)\r\n  File \"/opt/workspace/src/training.py\", line 98, in train_model\r\n    loss_train, logits, reg_loss = training_step(x_batch_train, y_batch_train)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/function.py\", line 1137, in _filtered_call\r\n    return self._call_flat(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/function.py\", line 1223, in _call_flat\r\n    flat_outputs = forward_function.call(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/function.py\", line 506, in call\r\n    outputs = execute.execute(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError:  Blas GEMM launch failed : a.shape=(32, 25088), b.shape=(25088, 8), m=32, n=8, k=25088\r\n\t [[node model/dense/MatMul (defined at usr/local/lib/python3.8/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_training_step_9298]\r\n\r\nFunction call stack:\r\ntraining_step\r\n\r\n\r\nThanks & Regards\r\n\r\n\r\n\r\n", "> \r\n> \r\n> @kevin8100, Similar issue comment [#33916](https://github.com/tensorflow/tensorflow/issues/33916#issuecomment-548811280).\r\n\r\nHello,\r\n\r\nI've already added these params:\r\nTF_FORCE_GPU_ALLOW_GROWTH=true\r\nPER_PROCESS_GPU_MEMORY_FRACTION=0.8\r\n\r\n..and it's still not working", "I see you are using cuda 10.1, TF 2.0 supports cuda 10.0 Please roll back to cuda 10.0 and try again.\r\n If issue still persists then use [limit gpu memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) parameter by putting following snippet on top of your code.\r\n```python\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35029\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35029\">No</a>\n", "I am using 2.2.0-dev20200119 with CUDA 10.1 on 1080Ti. I still meet the problem when I run the following code.\r\n\r\n```python\r\n#!/usr/bin/python3\r\nimport numpy as np;\r\nimport tensorflow as tf;\r\ngpus = tf.config.experimental.list_physical_devices('GPU');\r\ntf.config.experimental.set_memory_growth(gpus[0], True);\r\na=tf.constant(np.random.normal(size=(8,100)), dtype = tf.float32)\r\nb=tf.keras.layers.Dense(units=200)(a);\r\n```\r\n", "@breadbread1984 Have you tried limited your GPU memory usage, as @ymodak suggested?\r\n\r\n```\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\n```", "yes I tried. the problem remains!", "same issue here, any update?", "Just in case you have libcublas10 version 10.2.2.89-1 installed: replacing it with an older version might fix the problem - see #37233", "Had the same issue. Resolved by reinstalling GPU Drivers and CUDA as was suggested in [https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu)\r\n\r\n1. Add repos:\r\n```\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb\r\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\nsudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb\r\nsudo apt-get update\r\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt-get update\r\n```\r\n\r\n2. Install drivers:\r\n`sudo apt-get install --no-install-recommends nvidia-driver-430`\r\n\r\n3. Reboot. Check that GPUs are visible using the command: nvidia-smi\r\n\r\n4. Install cuda and cudnn:\r\n\r\n```\r\nsudo apt-get install --no-install-recommends \\\r\n    cuda-10-1 \\\r\n    libcudnn7=7.6.4.38-1+cuda10.1  \\\r\n    libcudnn7-dev=7.6.4.38-1+cuda10.1\r\n```\r\n\r\nHope it helps someone else :)"]}, {"number": 35028, "title": "Loading models with BuildFromBuffer crashed the app.", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Android Q\r\n- Mobile device: Samsung Galaxy\r\n- TensorFlow installed from: source r20\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7\r\n- GCC/Compiler version: Android NDK19\r\n\r\nIn the sample Android project I stored .tflite binary models as constant variables:\r\n\r\nmodels.h:\r\n```\r\nextern const unsigned int  gModel1Size;\r\nextern const unsigned char gModel1Data[];\r\n\r\nextern const unsigned int  gModel2Size;\r\nextern const unsigned char gModel2Data[];\r\n```\r\nmodel1.cpp:\r\n```\r\nconst unsigned int gModel1Size = 123;\r\nconst unsigned char gModel1Data[] = { 0x00, 0x00, ... };\r\n```\r\nmodel2.cpp:\r\n```\r\nconst unsigned int gModel2Size = 123;\r\nconst unsigned char gModel2Data[] = { 0x00, 0x00, ... };\r\n```\r\n\r\nI tried to load each model using the BuildFromBuffer() API:\r\n```\r\nmModel = tflite::FlatBufferModel::BuildFromBuffer(modelData, modelSize);\r\n...\r\n```\r\n\r\nI have successfully loaded the second model, but the first one produces the following crash:\r\n\r\n```\r\n2019-12-11 12:33:47.421 8316-8316/com.test.sample A/libc: Fatal signal 7 (SIGBUS), code 1 (BUS_ADRALN), fault addr 0xc2120f31 in tid 8316 (viewsample), pid 8316 (viewsample)\r\n2019-12-11 12:33:47.473 8375-8375/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n2019-12-11 12:33:47.473 8375-8375/? A/DEBUG: Revision: '27'\r\n2019-12-11 12:33:47.473 8375-8375/? A/DEBUG: ABI: 'arm'\r\n2019-12-11 12:33:47.473 8375-8375/? A/DEBUG: pid: 8316, tid: 8316, name: viewsample  >>> com.test.sample.viewsample <<<\r\n2019-12-11 12:33:47.473 8375-8375/? A/DEBUG: signal 7 (SIGBUS), code 1 (BUS_ADRALN), fault addr 0xc2120f31\r\n2019-12-11 12:33:47.473 8375-8375/? A/DEBUG:     r0  c2120f31  r1  00040ec0  r2  c20e0053  r3  00000012\r\n2019-12-11 12:33:47.473 8375-8375/? A/DEBUG:     r4  c2120f31  r5  00000038  r6  00000000  r7  ffcfac80\r\n2019-12-11 12:33:47.473 8375-8375/? A/DEBUG:     r8  c2120f2d  r9  c2120f31  r10 00041518  r11 ffcfad04\r\n2019-12-11 12:33:47.473 8375-8375/? A/DEBUG:     ip  c23acdf4  sp  ffcfac30  lr  c22bf813  pc  c22bf82a\r\n2019-12-11 12:33:47.688 8375-8375/? A/DEBUG: backtrace:\r\n2019-12-11 12:33:47.688 8375-8375/? A/DEBUG:     #00 pc 000e482a  /data/app/com.test.sample.viewsample-taC72NjjoCTT0dQC8d9LbQ==/lib/arm/libtensorflowlite.so (tflite::InterpreterBuilder::BuildLocalIndexToRegistrationMapping()+78)\r\n2019-12-11 12:33:47.688 8375-8375/? A/DEBUG:     #01 pc 000e523f  /data/app/com.test.sample.viewsample-taC72NjjoCTT0dQC8d9LbQ==/lib/arm/libtensorflowlite.so (tflite::InterpreterBuilder::operator()(std::__ndk1::unique_ptr<tflite::Interpreter, std::__ndk1::default_delete<tflite::Interpreter>>*, int)+50)\r\n```\r\nI swapped the models to check data integrity. Every time the FIRST model produces crashes, no matter which data is in.\r\nIt seems this is a data alignment issue. \r\nI guess that the TF code is not able to get/put the data in the app's right memory successfully that caused this issue.\r\n\r\nThen I removed the \"const\" modifier for the data:\r\n\r\n`unsigned char gModel1Data[] = {...}`\r\n\r\nand cast it inside the BuildFromBuffer API:\r\n\r\n`mModel = tflite::FlatBufferModel::BuildFromBuffer((const char*)modelData, modelSize);`\r\n\r\nThis workaround works fine but looks not natural for the API that should accept const char* by default.\r\nIs it possible to use the model loading this way, or it may surprise me in a crucial moment by undefined behavior?\r\n", "comments": ["Ah, right, you need to make sure that your buffer is at the very least 4 or 8-byte aligned, but preferably 16-byte aligned. We're working on some improvements in error messages to make this requirement more explicit.", "Can you confirm what the alignment is for this buffer when you observe a crash? Thanks.", "Thanks for the quick response. Confirming this crash caused by the wrong alignment. Tracing the allocated memory shows 1-byte alignment for the first model data and at least 4-byte alignment for the next one:\r\n```\r\ngModel1Data: 0xd1fd6139\r\ngModel1Data: 0xd2017658\r\n```\r\nThe solution here could be the following:\r\n\r\n`const unsigned char gModel1Data[] __attribute__((aligned(16))) = { 0x00, ... }`\r\n\r\nBoth 4- and 8-byte alignment works as expected also.", "Yep, that should do it! As noted, we'll try to add some more meaningful error messages when we encounter unaligned model buffers in the future. Apologies for the annoyance, these can be difficult bugs to diagnose unless you've hit them before =/."]}, {"number": 35027, "title": "logging broken with Python-3.8: findCaller() takes from 1 to 2 positional arguments but 3 were given", "body": "**System information**System information\r\n- OS Platform and Distribution: Arch Linux, 5.4.2-arch1-1-ARCH\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.1.0rc0-1\r\n- Keras version: 2.2.4-tf\r\n- Python version: 3.8\r\n- GPU model and memory: 2x GTX 1080 Ti 11GB\"`\r\n\r\n**Describe the current behavior**\r\nexecution of MNIST example fails with error:\r\n\r\n> TypeError: findCaller() takes from 1 to 2 positional arguments but 3 were given\r\n\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\ndef scale(image, label):\r\n  image = tf.cast(image, tf.float32)\r\n  image /= 255\r\n  return image, label\r\n\r\ndef build_model():\r\n    filters = 56\r\n    units = 24\r\n    kernel_size = 5\r\n    learning_rate = 1e-2\r\n    model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(units, activation='relu'),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\r\n    return model\r\n\r\ndatasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\nmnist_train, mnist_test = datasets['train'], datasets['test']\r\n\r\nnum_train_examples = info.splits['train'].num_examples\r\nnum_test_examples = info.splits['test'].num_examples\r\n\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 128\r\n\r\ntrain_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\neval_dataset = mnist_test.map(scale).shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\nmodel = build_model()\r\n\r\nepochs=5\r\nmodel.fit(\r\n        train_dataset,\r\n        validation_data=eval_dataset,\r\n        steps_per_epoch=num_train_examples/epochs,\r\n        validation_steps=num_test_examples/epochs,\r\n        epochs=epochs)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\ntraceback:\r\nDownloading and preparing dataset mnist (11.06 MiB) to /home/graemer/tensorflow_datasets/mnist/1.0.0...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 23, in <module>\r\n    datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_datasets/core/api_utils.py\", line 52, in disallow_positional_args_dec\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_datasets/core/registered.py\", line 302, in load\r\n    dbuilder.download_and_prepare(**download_and_prepare_kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_datasets/core/api_utils.py\", line 52, in disallow_positional_args_dec\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py\", line 316, in download_and_prepare\r\n    logging.warning(GCS_HOSTED_MSG, self.name)\r\n  File \"/usr/lib/python3.8/site-packages/absl/logging/__init__.py\", line 322, in warning\r\n    log(WARNING, msg, *args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/absl/logging/__init__.py\", line 485, in log\r\n    _absl_logger.log(standard_level, msg, *args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/absl/logging/__init__.py\", line 1047, in log\r\n    super(ABSLLogger, self).log(level, msg, *args, **kwargs)\r\n  File \"/usr/lib/python3.8/logging/__init__.py\", line 1500, in log\r\n    self._log(level, msg, args, **kwargs)\r\n  File \"/usr/lib/python3.8/logging/__init__.py\", line 1565, in _log\r\n    fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\r\nTypeError: findCaller() takes from 1 to 2 positional arguments but 3 were given\r\n```", "comments": ["@olk, Is this still issue!", "Yes, I can reproduce it", "@gadagashwini yes still an issue", "May be issue is with Python 3.8.\r\nIt is working without any error message on Colab with Python3.6. Thanks!", "@jvishnuvardhan please assign \"tensorflow_datasets\" / TFDS issue to @rsepassi for triage.", "Note that we currently don't release python3.8 pip packages.", "should now be fixed by https://github.com/abseil/abseil-py/pull/126 ", "@olk \r\n\r\nIs it possible for you to try latest TF versions(`!pip install tensorflow==2.2-rc3`) and let us know whether the issue persists? .Windows/Linux binaries for py3.8 are available in TF 2.2 .Please verify once and close the issue if the issue was resolved. Thanks!", "@olk \r\n\r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35027\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35027\">No</a>\n", "This error was reintroduced in the latest release of TensorFlow Metadata.\r\n\r\nhttps://github.com/tensorflow/metadata/pull/10 fixes the problem by loosening the requirements on abseil.\r\n@mihaimaruseac @rsepassi @ravikyram Could you take a look at the PR?", "I don't have approval powers over that repo.", "I am seeing this issue after installing the TensorFlow Object Detection API following along the instructions on this page:\r\n\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md\r\n\r\nand running the test script:\r\n\r\npython object_detection/builders/model_builder_tf2_test.py\r\n\r\nThis is using python 3.8.5 on MacOS Catalina (10.15.6)\r\n\r\nFull error from test script:\r\nERROR: test_create_ssd_models_from_config (__main__.ModelBuilderTF2Test)\r\ntest_create_ssd_models_from_config (__main__.ModelBuilderTF2Test)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Users/matt/workspace/cv/lib/python3.8/site-packages/object_detection/builders/model_builder_test.py\", line 212, in test_create_ssd_models_from_config\r\n    model = model_builder.build(model_proto, is_training=True)\r\n  File \"/Users/matt/workspace/cv/lib/python3.8/site-packages/object_detection/builders/model_builder.py\", line 1061, in build\r\n    return build_func(getattr(model_config, meta_architecture), is_training,\r\n  File \"/Users/matt/workspace/cv/lib/python3.8/site-packages/object_detection/builders/model_builder.py\", line 372, in _build_ssd_model\r\n    feature_extractor = _build_ssd_feature_extractor(\r\n  File \"/Users/matt/workspace/cv/lib/python3.8/site-packages/object_detection/builders/model_builder.py\", line 350, in _build_ssd_feature_extractor\r\n    return feature_extractor_class(**kwargs)\r\n  File \"/Users/matt/workspace/cv/lib/python3.8/site-packages/object_detection/models/ssd_efficientnet_bifpn_feature_extractor.py\", line 294, in __init__\r\n    super(SSDEfficientNetB0BiFPNKerasFeatureExtractor, self).__init__(\r\n  File \"/Users/matt/workspace/cv/lib/python3.8/site-packages/object_detection/models/ssd_efficientnet_bifpn_feature_extractor.py\", line 143, in __init__\r\n    logging.info('EfficientDet EfficientNet backbone version: %s',\r\n  File \"/Users/matt/workspace/cv/lib/python3.8/site-packages/absl/logging/__init__.py\", line 338, in info\r\n    log(INFO, msg, *args, **kwargs)\r\n  File \"/Users/matt/workspace/cv/lib/python3.8/site-packages/absl/logging/__init__.py\", line 485, in log\r\n    _absl_logger.log(standard_level, msg, *args, **kwargs)\r\n  File \"/Users/matt/workspace/cv/lib/python3.8/site-packages/absl/logging/__init__.py\", line 1047, in log\r\n    super(ABSLLogger, self).log(level, msg, *args, **kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/logging/__init__.py\", line 1500, in log\r\n    self._log(level, msg, args, **kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/logging/__init__.py\", line 1565, in _log\r\n    fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\r\nTypeError: findCaller() takes from 1 to 2 positional arguments but 3 were given", "I recently encountered this - the root cause seems to be an incompatibility between `absl-py<0.9` and Python 3.8 (see https://github.com/icubam/icubam/pull/2). Upgrading to a newer version of `absl-py` solved the problem for me."]}, {"number": 35026, "title": "How to use a lstm layer with projection in tf2.0", "body": "Is there any way to use projection in tf.keras.layers.lstm mention in [paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf) ", "comments": ["Hi @gowthamkpr, can you tell me how to achieve this?", "@manish-kumar-garg I am going to close this issue as this is not related to bug/performance, build/install, feature request or docs related issues. Please post this question in stackoverflow .Thanks!", "is it possible now in 2021?"]}, {"number": 35025, "title": "'found gpu 0'", "body": "<h3>Is this a error ??</h3>\r\n<pre>2019-12-11 18:49:26.117674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-12-11 18:49:27.156050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\n2019-12-11 18:49:27.162481: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-12-11 18:49:27.187878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-11 18:49:27.260258: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-12-11 18:49:27.306165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\n2019-12-11 18:49:27.313377: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-12-11 18:49:27.320079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-11 18:49:39.396101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-11 18:49:39.400853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-12-11 18:49:39.405363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-12-11 18:49:39.458919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2996 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n</pre>", "comments": ["Everything is working. Your GPU was found and initialized as stated in the last line: 'Created TensorFlow device...'\r\n'Found device 0' should be read as: 'Found device #0' _not_ 'Found 0 devices'", "@WhistleWhileYouWork \r\nThank you, for your reply , i have one more query , as i have two gpu's 0 and 1 , it seems it is using gpu 0 , which is my intel graphics card , how can i switch it from gpu 0 to gpu 1 since gpu 1 is my nvidia graphic card.", "Tensorflow only recognizes Nvidia GPUs. GPU 0 is the Nvidia graphics card.", "You can also check with `nvidia-smi` which GPU gets used", "> Thank you, for your reply , i have one more query , as i have two gpu's 0 and 1 , it seems it is using gpu 0 , which is my intel graphics card , how can i switch it from gpu 0 to gpu 1 since gpu 1 is my nvidia graphic card.\r\n\r\nYou may use [tf.device](https://www.tensorflow.org/api_docs/python/tf/device)\r\n```python\r\nwith tf.device(\"/device:GPU:1\"):  #device name\r\n  # code here\r\n```\r\n", "thank you @mihaimaruseac , @WhistleWhileYouWork and @ymodak  for helping me out. ", "How do we prevent these definitions from appearing on the console screen? The `Console.Clear.()` command does not work."]}, {"number": 35024, "title": " move function DisableAllStages from protected to private", "body": " move function DisableAllStages from protected to private", "comments": ["@gbaned it has been stuck here for a long time. Please take a look at it.", "> @gbaned it has been stuck here for a long time. Please take a look at it.\r\n\r\n@leike666666 Sorry for the slow response, it is waiting for internal approval. Thank you. "]}, {"number": 35023, "title": "[MLIR] tf-opt can not run with -debug option", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):1.0.1\r\n- GCC/Compiler version (if compiling from source):6.4.0\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: V100 \r\n\r\n\r\n**Describe the current behavior**\r\nin current tensorflow dialect (implemented in tensorflow/compiler/mlir)\uff0c we use LLVM_DEBUG in various place to print some debug information during compilation , and if we run tf-opt , it's expected to print these informations with -debug option added, this is also the behavior with mlir-opt, but in current master branch , if I pass a -debug option to tf-opt ,an error occured\r\n ```\r\n#bazel-bin/tensorflow/compiler/mlir/tf-opt -debug\r\ntf-opt: Unknown command line argument '-debug'.  Try: 'bazel-bin/tensorflow/compiler/mlir/tf-opt --help'\r\ntf-opt: Did you mean '--help'?\r\n```\r\nbut if I run mlir-opt with -debug:\r\n```\r\n#bin/mlir-opt -debug\r\nArgs: bin/mlir-opt -debug\r\n```\r\nI've also compared the main function between tf-opt(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tf_mlir_opt_main.cc ) and mlir-opt  (https://github.com/tensorflow/mlir/blob/master/tools/mlir-opt/mlir-opt.cpp ) and see no apparent difference between the two files\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\ntf-opt can run with -debug so I can use the debug info to debug the compilation process\r\n\r\n\r\n", "comments": ["In InitMlir we parse the flags with the two different command libraries in use: if no ` -- ` is on the line, then all are passed to LLVM's cl, if ` -- ` then the part before ` -- ` is passed to LLVM and those after to TF's flags. Try \r\n\r\ntf-opt -debug --\r\n\r\nPlease reopen if this doesn't resolve it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35023\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35023\">No</a>\n"]}, {"number": 35022, "title": "Errors of building XLA AOT example", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: 10.0/7.5\r\n- GPU model and memory:RTX2080Ti with 11GB\r\n\r\n**Describe the current behavior**\r\nI followed this [guide](https://tensorflow.google.cn/xla/tfcompile) to test xla example, all the code are the same with this guide. In the last step, when building the cc_binary, it gives some errors\r\n```\r\nbazel build //tensorflow/compiler/aot/tests:my_binary --verbose_failures\r\n```\r\n```\r\nINFO: Analyzed target //tensorflow/compiler/aot/tests:my_binary (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/zoud/workspace/local/tf_build/tensorflow-2.0.0-cc/tensorflow/compiler/aot/tests/BUILD:16:1: Linking of rule '//tensorflow/compiler/aot/tests:my_binary' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/zoud/.cache/bazel/_bazel_zoud/8f979226e66ca56e3b01def87b6ccec0/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    LD_LIBRARY_PATH=:/usr/local/cuda/lib64:/home/zoud/program/TensorRT-5.1.5.0/lib \\\r\n    PATH=/usr/lib/jvm/jdk1.8.0_221/bin:/usr/lib/jvm/jdk1.8.0_221/jre/bin:/home/zoud/program/anaconda3/envs/tf_2.0.0_src/bin:/home/zoud/program/anaconda3/condabin:/usr/lib/jvm/jdk1.8.0_221/bin:/usr/lib/jvm/jdk1.8.0_221/jre/bin:/home/zoud/bin:/home/zoud/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin:/home/zoud/program/MATLAB/R2017b/bin:/home/zoud/program/upx-3.95-amd64_linux:/home/zoud/program/bazel-0.26.1:/usr/local/cuda/bin:/home/zoud/program/MATLAB/R2017b/bin:/home/zoud/program/upx-3.95-amd64_linux:/home/zoud/program/bazel-0.26.1 \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/zoud/program/anaconda3/envs/tf_2.0.0_src/bin/python \\\r\n    PYTHON_LIB_PATH=/home/zoud/program/anaconda3/envs/tf_2.0.0_src/lib/python3.6/site-packages \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.5 \\\r\n    TF_NEED_CUDA=1 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/k8-opt/bin/tensorflow/compiler/aot/tests/my_binary -pthread -Wl,-no-as-needed -pie -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/k8-opt/bin/tensorflow/compiler/aot/tests/my_binary-2.params)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n/usr/bin/ld: bazel-out/k8-opt/bin/external/com_google_absl/absl/strings/libstrings.a(charconv.o): undefined reference to symbol 'nanf@@GLIBC_2.2.5'\r\n//lib/x86_64-linux-gnu/libm.so.6: error adding symbols: DSO missing from command line\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/compiler/aot/tests:my_binary failed to build\r\nINFO: Elapsed time: 0.308s, Critical Path: 0.12s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\nMaybe my GCC version is too low? I noticed official environment of building TF2 is GCC7.3.\r\n\r\nAnother question, I don't understand the meaning of step1, why modify the `tf2xla.proto`, this file seems never be used in the following steps.\r\n", "comments": ["Closing this as I didn't manage to reproduce it and GCC5 is really old. The error message is saying that there's a `-lm` missing on the linker command line, but I don't know how that can happen.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35022\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35022\">No</a>\n"]}, {"number": 35021, "title": "Pass experimental_relax_shapes to instance methods decorated with `tf.function`.", "body": "Currently the `experimental_relax_shapes` is not passed, so the instance methods ignore this argument.\r\n\r\nFixes #34905.\r\n\r\nBTW, there are other options not passed, like experimental_autograph_options. However, in `eager.py:Function` the argument is `experimental_autograph_options` while in `function.py:Function` it is called just `autograph_options`, and I am not sure if the `class_method_to_instance_method` cannot be used for both. Therefore this pull request handles only `experimental_relax_shapes` which has the same name everywhere.", "comments": ["@alextp Thanks, I will add the test later. However, the builds are failing, because of the following reason:\r\n- in `def_function.py` the field is called `experimental_relax_shapes`:\r\nhttps://github.com/tensorflow/tensorflow/blob/9c5e6b7be87defd3ca7c43bff4e6f5ec79772f01/tensorflow/python/eager/def_function.py#L408\r\n- however, in `function.py` it is called `_experimental_relax_shapes`:\r\nhttps://github.com/tensorflow/tensorflow/blob/9c5e6b7be87defd3ca7c43bff4e6f5ec79772f01/tensorflow/python/eager/function.py#L2386\r\n\r\nHow do you think I should solve it? Something like testing for every one of them in turn and returning False if not defined, i.e., something like\r\n```python\r\ngetattr(original_function, \"_experimental_relax_shapes\", getattr(original_function, \"experimental_relax_shapes\", False))\r\n```\r\n?\r\nOr should one of the fields be renamed (but that seems like a large breaking change).", "Let's rename the fields as they are not on a public type.\n\nOn Wed, Dec 11, 2019 at 9:37 AM Milan Straka <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> Thanks, I will add the test later.\n> However, the builds are failing, because of the following reason:\n>\n>    - in def_function.py the field is called experimental_relax_shapes:\n>\n>    https://github.com/tensorflow/tensorflow/blob/9c5e6b7be87defd3ca7c43bff4e6f5ec79772f01/tensorflow/python/eager/def_function.py#L408\n>    - however, in function.py it is called _experimental_relax_shapes:\n>\n>    https://github.com/tensorflow/tensorflow/blob/9c5e6b7be87defd3ca7c43bff4e6f5ec79772f01/tensorflow/python/eager/function.py#L2386\n>\n> How do you think I should solve it? Something like testing for every one\n> of them in turn and returning False if not defined, i.e., something like\n>\n> getattr(original_function, \"_experimental_relax_shapes\", getattr(original_function, \"experimental_relax_shapes\", False))\n>\n> ?\n> Or should one of the fields be renamed (but that seems like a large\n> breaking change).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/35021?email_source=notifications&email_token=AAABHRK4WDXBLXA7K3AMOJLQYEQL5A5CNFSM4JZK4GR2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGT6VVI#issuecomment-564652757>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLTCRJ4SFVCWACKIDDQYEQL5ANCNFSM4JZK4GRQ>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp Thanks for guidance. I modified the commit to rename both fields to `_experimental_relax_shapes` which seems more consistent with the other fields. I also added a test. I was not sure if it is a good idea to access the `_experimental_relax_shapes` directly, so I reused the test for experimental_relax_shapes functionality (third call with a different shape should trigger a retrace with undefined batch size).\r\n\r\nI have no way to run the tests on my home laptop, so I will wait for the results here, and fix if necessary.", "@alextp Thank you for your help. Would it make sense to CP this to 2.1? I met several people who thought `experimental_relax_shapes` does not work for them, because they added it to an instance method. The commit itself is a small and clean bugfix, so I think it could be added even this late in the release cycle."]}, {"number": 35020, "title": "Fix mlir TargetLLVMIR deps.", "body": "Fix mlir TargetLLVMIR deps.", "comments": ["> Thanks for flagging this, surprised it didn't trigger any other failures before ... I need to make a different change internally to fix the export but will do so shortly.\r\n\r\nOK"]}, {"number": 35019, "title": "Relax shapes for Keras _on_batch functions.", "body": "EDIT(robieta): Moved below.", "comments": ["Updated as suggested.", "Original description:\r\n\r\nThe current `{train,test,predict}_on_batch` function use a regular `tf.function` when not in eager mode, which causes a retrace for every new batch size. Similarly, if sequences are passed on input, every different sequence size causes a retrace.\r\n\r\nPassing experimental_relax_shapes=True allow gracefully handle these cases.\r\n\r\nFixes #34907.", "I'm currently running into some infra issue getting this in. I will update when it's resolved.", "@foxik can you rewrite history to strip the commit message down to something simple? There seems to be a special character in there somewhere which is messing with our infra. Sorry for the inconvenience.", "@robieta Done, I removed all weird characters (there were an asterisk, braces and backticks), and left only underscores and an equal sign, which seem fairly benign.\r\n\r\nBTW, sorry for the delay, but I am in UTC+1 timezone :-)"]}, {"number": 35018, "title": "Can't convert Albert to tflite by missing IdentityN support", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- TensorFlow installed from (source or binary): colab\r\n- TensorFlow version (or github SHA if from source): 1.15\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nConverterError: See console for info.\r\n2019-12-11 07:22:00.278074: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: IdentityN\r\n2019-12-11 07:22:00.421599: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2198 operators, 3544 arrays (0 quantized)\r\n2019-12-11 07:22:00.563590: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 2194 operators, 3535 arrays (0 quantized)\r\n2019-12-11 07:22:00.753664: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2194 operators, 3535 arrays (0 quantized)\r\n2019-12-11 07:22:01.305107: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1366 operators, 2266 arrays (0 quantized)\r\n2019-12-11 07:22:02.013834: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 1293 operators, 2193 arrays (0 quantized)\r\n2019-12-11 07:22:02.126476: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 1256 operators, 2125 arrays (0 quantized)\r\n2019-12-11 07:22:02.230528: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 4: 1252 operators, 2120 arrays (0 quantized)\r\n2019-12-11 07:22:02.347386: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 5: 1316 operators, 2260 arrays (0 quantized)\r\n2019-12-11 07:22:02.459624: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 6: 1316 operators, 2260 arrays (0 quantized)\r\n2019-12-11 07:22:02.583975: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 7: 1379 operators, 2410 arrays (0 quantized)\r\n2019-12-11 07:22:02.710596: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 8: 1379 operators, 2410 arrays (0 quantized)\r\n2019-12-11 07:22:02.837758: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 9: 1378 operators, 2420 arrays (0 quantized)\r\n2019-12-11 07:22:02.964471: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 10: 1378 operators, 2420 arrays (0 quantized)\r\n2019-12-11 07:22:03.097633: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 11: 1330 operators, 2335 arrays (0 quantized)\r\n2019-12-11 07:22:03.219935: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 12: 1324 operators, 2329 arrays (0 quantized)\r\n2019-12-11 07:22:03.341469: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 13: 1388 operators, 2469 arrays (0 quantized)\r\n2019-12-11 07:22:03.473109: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 14: 1388 operators, 2469 arrays (0 quantized)\r\n2019-12-11 07:22:03.608375: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 15: 1451 operators, 2619 arrays (0 quantized)\r\n2019-12-11 07:22:03.746363: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 16: 1451 operators, 2619 arrays (0 quantized)\r\n2019-12-11 07:22:03.888602: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 17: 1450 operators, 2629 arrays (0 quantized)\r\n2019-12-11 07:22:04.021441: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 18: 1450 operators, 2629 arrays (0 quantized)\r\n2019-12-11 07:22:04.166493: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 19: 1402 operators, 2544 arrays (0 quantized)\r\n2019-12-11 07:22:04.296326: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 20: 1396 operators, 2538 arrays (0 quantized)\r\n2019-12-11 07:22:04.435268: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 21: 1460 operators, 2678 arrays (0 quantized)\r\n2019-12-11 07:22:04.576370: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 22: 1460 operators, 2678 arrays (0 quantized)\r\n2019-12-11 07:22:04.727069: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 23: 1523 operators, 2828 arrays (0 quantized)\r\n2019-12-11 07:22:04.884741: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 24: 1523 operators, 2828 arrays (0 quantized)\r\n2019-12-11 07:22:05.035517: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 25: 1522 operators, 2838 arrays (0 quantized)\r\n2019-12-11 07:22:05.185234: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 26: 1522 operators, 2838 arrays (0 quantized)\r\n2019-12-11 07:22:05.345311: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 27: 1474 operators, 2753 arrays (0 quantized)\r\n2019-12-11 07:22:05.490007: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 28: 1468 operators, 2747 arrays (0 quantized)\r\n2019-12-11 07:22:05.664208: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 29: 1532 operators, 2887 arrays (0 quantized)\r\n2019-12-11 07:22:05.816073: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 30: 1532 operators, 2887 arrays (0 quantized)\r\n2019-12-11 07:22:05.979330: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 31: 1595 operators, 3037 arrays (0 quantized)\r\n2019-12-11 07:22:06.146208: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 32: 1595 operators, 3037 arrays (0 quantized)\r\n2019-12-11 07:22:06.311167: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 33: 1594 operators, 3047 arrays (0 quantized)\r\n2019-12-11 07:22:06.469774: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 34: 1594 operators, 3047 arrays (0 quantized)\r\n2019-12-11 07:22:06.641870: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 35: 1546 operators, 2962 arrays (0 quantized)\r\n2019-12-11 07:22:06.794954: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 36: 1540 operators, 2956 arrays (0 quantized)\r\n2019-12-11 07:22:06.961088: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 37: 1604 operators, 3096 arrays (0 quantized)\r\n2019-12-11 07:22:07.126460: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 38: 1604 operators, 3096 arrays (0 quantized)\r\n2019-12-11 07:22:07.298514: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 39: 1667 operators, 3246 arrays (0 quantized)\r\n2019-12-11 07:22:07.480185: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 40: 1667 operators, 3246 arrays (0 quantized)\r\n2019-12-11 07:22:07.682372: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 41: 1666 operators, 3256 arrays (0 quantized)\r\n2019-12-11 07:22:07.867497: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 42: 1666 operators, 3256 arrays (0 quantized)\r\n2019-12-11 07:22:08.062627: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 43: 1618 operators, 3171 arrays (0 quantized)\r\n2019-12-11 07:22:08.239084: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 44: 1612 operators, 3165 arrays (0 quantized)\r\n2019-12-11 07:22:08.428755: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 45: 1676 operators, 3305 arrays (0 quantized)\r\n2019-12-11 07:22:08.615966: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 46: 1676 operators, 3305 arrays (0 quantized)\r\n2019-12-11 07:22:08.811813: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 47: 1739 operators, 3455 arrays (0 quantized)\r\n2019-12-11 07:22:09.014879: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 48: 1739 operators, 3455 arrays (0 quantized)\r\n2019-12-11 07:22:09.220450: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 49: 1738 operators, 3465 arrays (0 quantized)\r\n2019-12-11 07:22:09.423087: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 50: 1738 operators, 3465 arrays (0 quantized)\r\n2019-12-11 07:22:09.634820: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 51: 1690 operators, 3380 arrays (0 quantized)\r\n2019-12-11 07:22:09.829015: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 52: 1684 operators, 3374 arrays (0 quantized)\r\n2019-12-11 07:22:10.034856: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 53: 1748 operators, 3514 arrays (0 quantized)\r\n2019-12-11 07:22:10.242489: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 54: 1748 operators, 3514 arrays (0 quantized)\r\n2019-12-11 07:22:10.449573: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 55: 1811 operators, 3664 arrays (0 quantized)\r\n2019-12-11 07:22:10.656973: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 56: 1811 operators, 3664 arrays (0 quantized)\r\n2019-12-11 07:22:10.887171: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 57: 1810 operators, 3674 arrays (0 quantized)\r\n2019-12-11 07:22:11.106195: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 58: 1810 operators, 3674 arrays (0 quantized)\r\n2019-12-11 07:22:11.342466: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 59: 1762 operators, 3589 arrays (0 quantized)\r\n2019-12-11 07:22:11.555873: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 60: 1756 operators, 3583 arrays (0 quantized)\r\n2019-12-11 07:22:11.779618: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 61: 1820 operators, 3723 arrays (0 quantized)\r\n2019-12-11 07:22:12.002172: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 62: 1820 operators, 3723 arrays (0 quantized)\r\n2019-12-11 07:22:12.230472: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 63: 1883 operators, 3873 arrays (0 quantized)\r\n2019-12-11 07:22:12.475085: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 64: 1883 operators, 3873 arrays (0 quantized)\r\n2019-12-11 07:22:12.715383: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 65: 1882 operators, 3883 arrays (0 quantized)\r\n2019-12-11 07:22:12.956508: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 66: 1882 operators, 3883 arrays (0 quantized)\r\n2019-12-11 07:22:13.200556: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 67: 1834 operators, 3798 arrays (0 quantized)\r\n2019-12-11 07:22:13.431959: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 68: 1828 operators, 3792 arrays (0 quantized)\r\n2019-12-11 07:22:13.676081: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 69: 1892 operators, 3932 arrays (0 quantized)\r\n2019-12-11 07:22:13.936701: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 70: 1892 operators, 3932 arrays (0 quantized)\r\n2019-12-11 07:22:14.198596: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 71: 1955 operators, 4082 arrays (0 quantized)\r\n2019-12-11 07:22:14.455744: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 72: 1955 operators, 4082 arrays (0 quantized)\r\n2019-12-11 07:22:14.709946: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 73: 1954 operators, 4092 arrays (0 quantized)\r\n2019-12-11 07:22:14.973819: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 74: 1954 operators, 4092 arrays (0 quantized)\r\n2019-12-11 07:22:15.247743: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 75: 1906 operators, 4007 arrays (0 quantized)\r\n2019-12-11 07:22:15.508479: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 76: 1900 operators, 4001 arrays (0 quantized)\r\n2019-12-11 07:22:15.768632: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 77: 1964 operators, 4141 arrays (0 quantized)\r\n2019-12-11 07:22:16.042654: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 78: 1964 operators, 4141 arrays (0 quantized)\r\n2019-12-11 07:22:16.315995: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 79: 2027 operators, 4291 arrays (0 quantized)\r\n2019-12-11 07:22:16.599728: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 80: 2027 operators, 4291 arrays (0 quantized)\r\n2019-12-11 07:22:16.876653: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 81: 2026 operators, 4301 arrays (0 quantized)\r\n2019-12-11 07:22:17.155476: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 82: 2026 operators, 4301 arrays (0 quantized)\r\n2019-12-11 07:22:17.455543: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 83: 1978 operators, 4216 arrays (0 quantized)\r\n2019-12-11 07:22:17.732213: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 84: 1972 operators, 4210 arrays (0 quantized)\r\n2019-12-11 07:22:18.041275: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 85: 2036 operators, 4350 arrays (0 quantized)\r\n2019-12-11 07:22:18.337704: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 86: 2036 operators, 4350 arrays (0 quantized)\r\n2019-12-11 07:22:18.645178: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 87: 2099 operators, 4500 arrays (0 quantized)\r\n2019-12-11 07:22:18.960635: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 88: 2099 operators, 4500 arrays (0 quantized)\r\n2019-12-11 07:22:19.269232: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 89: 2098 operators, 4510 arrays (0 quantized)\r\n2019-12-11 07:22:19.562883: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 90: 2098 operators, 4510 arrays (0 quantized)\r\n2019-12-11 07:22:19.889353: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 91: 2050 operators, 4425 arrays (0 quantized)\r\n2019-12-11 07:22:20.190117: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 92: 2044 operators, 4416 arrays (0 quantized)\r\n2019-12-11 07:22:20.501514: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 93: 2108 operators, 4556 arrays (0 quantized)\r\n2019-12-11 07:22:20.810962: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 94: 2108 operators, 4556 arrays (0 quantized)\r\n2019-12-11 07:22:21.129394: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 95: 2171 operators, 4706 arrays (0 quantized)\r\n2019-12-11 07:22:21.450294: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 96: 2171 operators, 4706 arrays (0 quantized)\r\n2019-12-11 07:22:21.782083: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 97: 2170 operators, 4716 arrays (0 quantized)\r\n2019-12-11 07:22:22.105969: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 98: 2170 operators, 4716 arrays (0 quantized)\r\n2019-12-11 07:22:22.442836: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 99: 2158 operators, 4698 arrays (0 quantized)\r\n2019-12-11 07:22:22.764761: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 100: 2155 operators, 4692 arrays (0 quantized)\r\n2019-12-11 07:22:23.096868: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 2155 operators, 4692 arrays (0 quantized)\r\n2019-12-11 07:22:23.324249: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 2155 operators, 4692 arrays (0 quantized)\r\n2019-12-11 07:22:23.549716: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 5111808 bytes, theoretical optimal value: 4718592 bytes.\r\n2019-12-11 07:22:23.613548: I tensorflow/lite/toco/toco_tooling.cc:439] Estimated count of arithmetic ops: 22884682880 ops, equivalently 11442341440 MACs\r\n2019-12-11 07:22:23.613795: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 11065920\r\n2019-12-11 07:22:23.618965: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CAST, DIV, FULLY_CONNECTED, GATHER, MEAN, MUL, PACK, POW, RESHAPE, RSQRT, SLICE, SOFTMAX, SQUARED_DIFFERENCE, SUB, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: IdentityN.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CAST, DIV, FULLY_CONNECTED, GATHER, MEAN, MUL, PACK, POW, RESHAPE, RSQRT, SLICE, SOFTMAX, SQUARED_DIFFERENCE, SUB, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: IdentityN.\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n-> I am using \"bert-for-tf2\" to load albert_base.\r\n```\r\ndef load_pretrained_albert():\r\n    model_name = \"albert_base\"\r\n    model_dir = bert.fetch_tfhub_albert_model(model_name, \".models\")\r\n    model_params = bert.albert_params(model_name)\r\n    l_bert = bert.BertModelLayer.from_params(model_params, name=\"albert\")\r\n\r\n    # use in Keras Model here, and call model.build()\r\n    max_seq_len = 128\r\n\r\n    l_input_ids = Input(shape=(max_seq_len,), dtype='int32', name=\"l_input_ids\")\r\n    #l_token_type_ids = Input(shape=(max_seq_len,), dtype='int32')\r\n\r\n    output = l_bert(l_input_ids)                              # output: [batch_size, max_seq_len, hidden_size]\r\n    pooled_output = AveragePooling1D(pool_size=max_seq_len, data_format=\"channels_last\")(output)\r\n    pooled_output = Flatten()(pooled_output)\r\n\r\n\r\n    model = Model(inputs=[l_input_ids], outputs=[pooled_output])\r\n    model.build(input_shape=(None, max_seq_len))\r\n\r\n    bert.load_albert_weights(l_bert, model_dir)\r\n\r\n    return model\r\n```\r\n\r\n\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["It seeems that you [resolved this](https://stackoverflow.com/questions/59279738/converting-albert-to-tflite-albert-implemented-in-keras-via-bert-for-tf2) by using SELECT_TF_OPS.\r\nhttps://www.tensorflow.org/lite/guide/ops_select\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(path)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nlite_albert = converter.convert()\r\n```\r\nSo, let's close this. Please feel free to reopen it if you have an issue with this.\r\n"]}, {"number": 35017, "title": "tensorflow 2.0 use ParameterServerStrategy error", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n\r\n* **use tensorflow docker hub images:  tensorflow / tensorflow : 2.0.0-gpu**\r\n\r\n**Describe the current behavior**\r\n* when i use tf2.0 distributed training with ParameterServerStrategy,  the error occurred follow:\r\n```\r\n2019-12-11 14:53:27.569524: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-12-11 14:53:27.619245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:2f:00.0\r\n2019-12-11 14:53:27.621064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:\r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:86:00.0\r\n2019-12-11 14:53:27.622349: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-12-11 14:53:27.627608: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-11 14:53:27.632513: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-12-11 14:53:27.634123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-12-11 14:53:27.640466: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-12-11 14:53:27.643992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-12-11 14:53:27.656264: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-11 14:53:27.660960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2019-12-11 14:53:27.661734: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-12-11 14:53:27.676662: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz\r\n2019-12-11 14:53:27.684246: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x565309993340 executing computations on platform Host. Devices:\r\n2019-12-11 14:53:27.684288: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-12-11 14:53:27.955807: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5653099f5a60 executing computations on platform CUDA. Devices:\r\n2019-12-11 14:53:27.955947: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\r\n2019-12-11 14:53:27.955978: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla P100-PCIE-16GB, Compute Capability 6.0\r\n2019-12-11 14:53:27.958412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:2f:00.0\r\n2019-12-11 14:53:27.959282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:\r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:86:00.0\r\n2019-12-11 14:53:27.959338: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-12-11 14:53:27.959361: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-11 14:53:27.959381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-12-11 14:53:27.959391: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-12-11 14:53:27.959413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-12-11 14:53:27.959423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-12-11 14:53:27.959434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-11 14:53:27.962606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2019-12-11 14:53:27.962639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-12-11 14:53:27.964634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-11 14:53:27.964653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1\r\n2019-12-11 14:53:27.964663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y\r\n2019-12-11 14:53:27.964679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N\r\n2019-12-11 14:53:27.967224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 6.0)\r\n2019-12-11 14:53:27.968872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15216 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:86:00.0, compute capability: 6.0)\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nTraceback (most recent call last):\r\n  File \"tf_mnist_ps_worker.py\", line 136, in <module>\r\n    main()\r\n  File \"tf_mnist_ps_worker.py\", line 132, in main\r\n    app.train(args)\r\n  File \"tf_mnist_ps_worker.py\", line 85, in train\r\n    model = Net().model\r\n  File \"tf_mnist_ps_worker.py\", line 32, in __init__\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/sequential.py\", line 114, in __init__\r\n    self.add(layer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/sequential.py\", line 196, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 817, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2141, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/layers/core.py\", line 1027, in build\r\n    trainable=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2311, in __setattr__\r\n    if val.trainable:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 477, in trainable\r\n    raise NotImplementedError\r\nNotImplementedError\r\n```\r\n\r\n**Code to reproduce the issue**\r\n* my demo code:\r\n```\r\nimport os\r\nimport json\r\nimport argparse\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import datasets\r\nfrom tensorflow.keras import layers, models\r\nfrom tensorflow.keras import optimizers\r\n\r\nclass Net(object):\r\n    def __init__(self):\r\n        model = models.Sequential()\r\n        model.add(layers.Conv2D(\r\n            32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\r\n        model.add(layers.MaxPooling2D((2, 2)))\r\n        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n        model.add(layers.MaxPooling2D((2, 2)))\r\n        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n\r\n        model.add(layers.Flatten())\r\n        model.add(layers.Dense(64, activation='relu'))\r\n        model.add(layers.Dense(10, activation='softmax'))\r\n\r\n        model.summary()\r\n\r\n        self.model = model\r\n\r\n\r\n# inital dateset\r\nclass DataSet(object):\r\n    def __init__(self):\r\n        data_path = os.path.dirname(os.path.realpath(__file__)) \\\r\n                    + '/../../datasets/mnist/mnist.npz'\r\n        (train_images, train_labels), (test_images, test_labels) = \\\r\n            datasets.mnist.load_data(path=data_path)\r\n        train_images = train_images.reshape((60000, 28, 28, 1))\r\n        test_images = test_images.reshape((10000, 28, 28, 1))\r\n\r\n        train_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\n        self.train_images, self.train_labels = train_images, train_labels\r\n        self.test_images, self.test_labels = test_images, test_labels\r\n\r\n\r\n# train and val\r\nclass Train:\r\n    def __init__(self):\r\n        self.data = DataSet()\r\n\r\n    def train(self, args):\r\n        # Define the checkpoint directory to store the checkpoints\r\n        checkpoint_dir = args.train_dir\r\n        # Name of the checkpoint files\r\n        checkpoint_path = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n\r\n        callbacks = [\r\n            tf.keras.callbacks.TensorBoard(log_dir='./logs'),\r\n            tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\r\n                                               save_weights_only=True),\r\n        ]\r\n\r\n        with strategy.scope():\r\n            model = Net().model\r\n\r\n            model.compile(optimizer=optimizers.Adam(),\r\n                          loss='sparse_categorical_crossentropy',\r\n                          metrics=['accuracy'])\r\n\r\n        model.fit(self.data.train_images, self.data.train_labels,\r\n                  batch_size=args.batch_size,\r\n                  epochs=args.epochs,\r\n                  callbacks=callbacks,\r\n                  validation_data=(self.data.test_images, self.data.test_labels))\r\n\r\n        # EVAL\r\n        model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\r\n        eval_loss, eval_acc = model.evaluate(\r\n            self.data.test_images, self.data.test_labels, verbose=2)\r\n        print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\r\n\r\n\r\ndef main():\r\n    # training params settings\r\n    parser = argparse.ArgumentParser(description='Tensorflow 2.0 MNIST Example,'\r\n                                                 ' use Mirrorstrategy')\r\n    parser.add_argument('--batch_size', '-b', type=int, default=64,\r\n                        help='input batch size for training (default: 64)')\r\n    parser.add_argument('--test_batchsize', '-tb', type=int, default=1000,\r\n                        help='input batch size for testing (default: 1000)')\r\n    parser.add_argument('--epochs', '-e', type=int, default=10,\r\n                        help='number of epochs to train (default: 10)')\r\n    parser.add_argument('--learning_rate', '-lr', type=float, default=0.01,\r\n                        help='learning rate (default: 0.01)')\r\n    parser.add_argument('--momentum', type=float, default=0.5,\r\n                        help='SGD momentum (default: 0.5)')\r\n    parser.add_argument('--log_interval', type=int, default=10,\r\n                        help='how many batches to wait before logging training status')\r\n    parser.add_argument('--save_model', '-sm', action='store_true', default=False,\r\n                        help='For Saving the current Model')\r\n\r\n    args = parser.parse_args()\r\n\r\n    app = Train()\r\n    app.train(args)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n   main()\r\n```\r\n\r\n* and my cmd line:\r\n\r\n```\r\nTF_CONFIG='{\"cluster\": {\"worker\": [\"10.240.208.106:12345\", \"10.240.208.108:12345\"], \"ps\": [\"10.240.208.106:12346\", \"10.240.208.108:12346\"]}, \"task\": {\"index\": 1, \"type\": \"worker\"}}' python tf_mnist_ps_worker.py\r\n```\r\n\r\n", "comments": ["This should be fixed now that the associated PR has been merged. Thanks!", "thanks for you comment. Please allow me to ask whether the above PR is merged by the current master branch? Should i use the master branch code to verify my issue?", "Hi @Crisescode, it looks like you are trying to use Keras with ParameterServerStrategy. Note that this is not currently supported, but is planned to be supported planned post 2.3. You can refer to the [chart here](https://www.tensorflow.org/guide/distributed_training#types_of_strategies) to see what is currently supported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35017\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35017\">No</a>\n"]}]