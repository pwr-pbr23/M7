[{"number": 34207, "title": "Error in the document of tf.keras.layers.Dense", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense \r\n\r\n## Description of issue (what needs changing):\r\nThe tf.keras.layers.Dense can actually take input_shape as function input, but it is not shown in this document.\r\nIn addition, the example of this document has the function \"Dense\", but I tried on my Googlecolab and it is not defined in tensorflow2. ", "comments": ["Hi,\r\n\r\nThank you for your submission; I believe however that this is more of an issue with the understanding of how the docs are formatted rather than an actual issue with their content.\r\n\r\n> The tf.keras.layers.Dense can actually take input_shape as function input, but it is not shown in this document.\r\n\r\n`input_shape` is one of the default keyword arguments inherited from the parent `Layer` class (just as `dtype` for example), which is why it is not explicitly documented for `Dense` class.\r\n\r\n> In addition, the example of this document has the function \"Dense\", but I tried on my Googlecolab and it is not defined in tensorflow2.\r\n\r\nAs in most documentation examples, imports are not explicitly displayed. To reproduce, you must run `from tensorflow.keras.layers import Dense` (same thing with the `Sequential` class).", "> In addition, the example of this document has the function \"Dense\", but I tried on my Googlecolab and it is not defined in tensorflow2.\r\n\r\n```python3\r\nfrom tensorflow.python.keras.layers import Input, Dense\r\nfrom tensorflow.python.keras.models import Sequential\r\nmodel = Sequential()\r\nmodel.add(Dense(32, input_shape=(16,)))\r\nmodel.add(Dense(32))\r\n```", "I C. \r\nThank U very much.", "@changy12 As @pandrey-fr mentioned, it is clear that this is not a docs issue, its more of the way it has been formatted. I am gonna close this issue as it has been resolved. Please add additional comments and we can open this issue again. Thanks!"]}, {"number": 34206, "title": "Update docs of map_fn", "body": "Replace `tf.contrib.eager.defun` with `tf.function`", "comments": ["`FAIL: Found 2 non-whitelisted pylint errors:\r\ntensorflow/python/ops/map_fn.py:93: [C0301(line-too-long), ] Line too long (82/80)\r\n\r\ntensorflow/python/ops/map_fn.py:96: [C0301(line-too-long), ] Line too long (84/80)`\r\n\r\n@Squadrick can you please fix this sanity errors ?", "@rthadur Updated.", "@rthadur @jaingaurav Thanks for the review!"]}, {"number": 34205, "title": "C installation still suggests 1.14.0 download", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/install/lang_c\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe download instructions link to 1.14.0 libraries rather than 1.15.0; the 1.15.0 libraries appear to exist and 1.15.0 is a release according to https://github.com/tensorflow/tensorflow/releases\r\n", "comments": ["tensorflow/docs#1216", "Fix merged. tensorflow.org will updated on next site push.\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34205\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34205\">No</a>\n"]}, {"number": 34204, "title": "Fix Merge Conflicts", "body": "", "comments": []}, {"number": 34203, "title": "RuntimeError: `merge_call` called while defining a new graph or a tf.function -- Update non-trainable variable with assign under mirrored strategy scope and tf.function decorator", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Gentoo\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary, pip\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nMy purpose is to record some hidden results that no need to compute gradient but is used for the next batch. The demo code is given below.\r\nUnder the mirrored strategy context, it fails to update non-trainable variable with assign method\r\nwithin fn with tf.function decorator. If remove tf.function, it works well. If\r\nre-assign `self.record = record` within tf.function, then will hit another error:\r\n`TypeError: An op outside of the function building code is being passed`, same error like [this](https://github.com/tensorflow/tensorflow/issues/32889). I'm aware we have to do some all_reduce-like operations to merge the results from all replicas before update any variable.\r\nI tried something like `tf.distribute.get_replica_context().merge_call()`, but the doc is really unclear how to implement it, the source code of tensorflow also can not be found any useful example.\r\n\r\n**Describe the expected behavior**\r\nunder strategy and tf.function context, updating a non-trainable variable with assign method\r\nshould work\r\n\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass MyLayer(tf.keras.layers.Layer):\r\n  def __init__(self):\r\n    super(MyLayer, self).__init__()\r\n\r\n  def build(self, input_shape):\r\n    self.w = self.add_weight(\"w\", shape=[], dtype=tf.float32, initializer=tf.constant_initializer(np.random.uniform()))\r\n\r\n    # record some hidden results used by next batch\r\n    self.record = self.add_weight(\"record\", shape=[],\r\n                                  dtype=tf.float32,\r\n                                  trainable=False,\r\n                                  aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA,\r\n                                  initializer=tf.constant_initializer(np.random.uniform()))\r\n\r\n  def call(self, x):\r\n    record = self.record + self.w\r\n    y = x*self.w + record\r\n\r\n    # Hit TypeError: An op outside of the function building code is being passed a \"Graph\" tensor\r\n    #self.record = record\r\n\r\n    # Hit RuntimeError: `merge_call` called while defining a new graph or a tf.function\r\n    self.record.assign(record)\r\n    return y\r\n\r\n\r\nclass Net(tf.keras.Model):\r\n  def __init__(self):\r\n    super(Net, self).__init__()\r\n    self.my_layer = MyLayer()\r\n\r\n  def call(self, x):\r\n    y = self.my_layer(x)\r\n    y = y + tf.random.normal(shape=[])\r\n    return y\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    net = Net()\r\n    n_samples = 1000\r\n    xs = np.random.uniform(size=[n_samples])\r\n\r\n    #It works well without tf.function\r\n    @tf.function\r\n    def train_step(x):\r\n        y = net(x)\r\n        return y\r\n    for i in range(n_samples):\r\n        x = xs[i]\r\n       \r\n\r\n    # if no tf.distribute.strategy was used, it also works well no matter tf.function is used or not\r\n\r\n        y = strategy.experimental_run_v2(train_step, args=(x,))\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\n test4.py:50 train_step  *\r\n        y = net(x)\r\n    /usr/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:891 __call__\r\n        outputs = self.call(cast_inputs, *args, **kwargs)\r\n    test4.py:28 call  *\r\n        self.record.assign(record+0.1)\r\n    /usr/lib64/python3.6/site-packages/tensorflow_core/python/distribute/values.py:1036 assign\r\n        return self._assign_func(f=assign_fn, *args, **kwargs)\r\n    /usr/lib64/python3.6/site-packages/tensorflow_core/python/distribute/values.py:1024 _assign_func\r\n        merge_fn, args=args, kwargs=kwargs)\r\n    /usr/lib64/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py:1917 merge_call\r\n        return self._merge_call(merge_fn, args, kwargs)\r\n    /usr/lib64/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py:940 _merge_call\r\n        \"`merge_call` called while defining a new graph or a tf.function. \"\r\n\r\nRuntimeError: `merge_call` called while defining a new graph or a tf.function. \r\nThis can often happen if the function `fn` passed to `strategy.experimental_run()` \r\nis decorated with `@tf.function` (or contains a nested `@tf.function`), \r\nand `fn` contains a synchronization point, such as aggregating gradients. \r\nThis behavior is not yet supported. Instead, please wrap the entire call `strategy.experimental_run(fn)` in a `@tf.function`, \r\nand avoid nested `tf.function`s that may potentially cross a synchronization boundary.\r\n```", "comments": ["Closing. Solved after reading the doc and source code several times, hope the doc and example for tf.distribute will be improved.\r\n\r\nWhen defining `self.record`, specify `synchronization` argument like this:\r\n```\r\nself.record = self.add_weight(\"record\", shape=[],\r\n                              dtype=tf.float32,\r\n                              trainable=False,\r\n                              synchronization=tf.VariableSynchronization.ON_READ,\r\n                              aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA,\r\n                              initializer=tf.constant_initializer(np.random.uniform()))\r\n\r\n```\r\n`ON_READ` synchronization allows to use assign/assign_sub/assign_add methods to update its value in a replica context. The complete code:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass MyLayer(tf.keras.layers.Layer):\r\n  \"\"\"A simple linear model.\"\"\"\r\n\r\n  def __init__(self):\r\n    super(MyLayer, self).__init__()\r\n\r\n  def build(self, input_shape):\r\n    self.w = self.add_weight(\"w\", shape=[], dtype=tf.float32, initializer=tf.constant_initializer(np.random.uniform()))\r\n\r\n    # record some hidden results used by next batch\r\n    self.record = self.add_weight(\"record\", shape=[],\r\n                                  dtype=tf.float32,\r\n                                  trainable=False,\r\n                                  synchronization=tf.VariableSynchronization.ON_READ,\r\n                                  aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA,\r\n                                  initializer=tf.constant_initializer(np.random.uniform()))\r\n\r\n  def call(self, x):\r\n    record = self.record + self.w + x\r\n    y = x*self.w + record\r\n\r\n    replica_ctx = tf.distribute.get_replica_context()\r\n    tf.print(replica_ctx.replica_id_in_sync_group, end=\"\\t\")\r\n    self.record.assign(record)\r\n    tf.print(self.record)\r\n    tf.print()\r\n    return y\r\n\r\n\r\nclass Net(tf.keras.Model):\r\n  def __init__(self):\r\n    super(Net, self).__init__()\r\n    self.my_layer = MyLayer()\r\n\r\n  def call(self, x):\r\n    y = self.my_layer(x)\r\n    y = y + tf.random.normal(shape=[])\r\n    return y\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    net = Net()\r\n    n_samples = 1000\r\n    xs = np.random.uniform(size=[n_samples])\r\n    dataset = tf.data.Dataset.from_tensor_slices((xs,))\r\n    dataset = dataset.batch(2)\r\n    dataset = strategy.experimental_distribute_dataset(dataset)\r\n\r\n    @tf.function\r\n    def train_step(x):\r\n        x = tf.reshape(x, [])\r\n        y = net(x)\r\n        return y\r\n    for x in dataset:\r\n        y = strategy.experimental_run_v2(train_step, args=(x,))\r\n```", "I have exact same issue: Non-trainable variable assign ops and mirrored strategy for training.\r\nI'm wondering if this is the proper (official) solution?\r\nCan member of Tensorflow verify?"]}, {"number": 34202, "title": "Remove adding of /usr/bin to compiler paths", "body": "As the underlying Bazel issue https://github.com/bazelbuild/bazel/issues/5634 is resolved, this code can (and should) go now\r\n\r\nThis allows for better compatibility on RHEL systems with CUDA.\r\n\r\nSee https://github.com/bazelbuild/bazel/commit/c6ec22f94faaf1320f576d5658a106483b2bf19f#diff-f6852ce579394c610a139a1f38783138L158\r\n\r\nThis has been in since Bazel 0.20 and should be picked for the next TF release", "comments": ["This causes failures on Windows\r\n\r\n```\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"T:/src/github/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1291\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"T:/src/github/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1221, in _create_local_cuda_repository\r\n                verify_build_defines(<1 more arguments>)\r\n        File \"T:/src/github/tensorflow/third_party/gpus/cuda_configure.bzl\", line 100, in verify_build_defines\r\n                auto_configure_fail(<1 more arguments>)\r\n        File \"T:/src/github/tensorflow/third_party/gpus/cuda_configure.bzl\", line 340, in auto_configure_fail\r\n                fail(<1 more arguments>)\r\n\r\nCuda Configuration Error: BUILD.tpl template is missing these variables: [\"linker_bin_path\"].\r\n```", "I'm seeing the same error as @mihaimaruseac on Ubuntu 16.04.", "Oh, sorry. Seems the entry is required although it can/should be empty. I created a PR to fix this: https://github.com/tensorflow/tensorflow/pull/34218"]}, {"number": 34201, "title": "keras.backend.function with learning phase gives AttributeError", "body": "I'm using the following script from the keras docu page to get the output of an intermediate layer in training phase:\r\n``` python\r\nfrom tensorflow.keras import backend as K\r\n\r\nK.function([model.layers[0].input, K.learning_phase()],\r\n           [model.layers[1].output])\r\n```\r\n\r\nWhich throws this error:\r\n``` python\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-d35d4dbf76cf> in <module>\r\n      2\r\n      3 K.function([model.layers[0].input, K.learning_phase()],\r\n----> 4            [model.layers[1].output])\r\n\r\n/home/woody/capn/mppi013h/conda_envs/orca_cpu_env/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py in function(inputs, outputs, updates, name, **kwargs)\r\n   3771       raise ValueError('Session keyword arguments are not support during '\r\n   3772                        'eager execution. You passed: %s' % (kwargs,))\r\n-> 3773     return EagerExecutionFunction(inputs, outputs, updates=updates, name=name)\r\n   3774\r\n   3775   if kwargs:\r\n\r\n/home/woody/capn/mppi013h/conda_envs/orca_cpu_env/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py in __init__(self, inputs, outputs, updates, name)\r\n   3668             add_sources=True,\r\n   3669             handle_captures=True,\r\n-> 3670             base_graph=source_graph)\r\n   3671\r\n   3672         inputs = [lifted_map[i] for i in inputs]\r\n\r\n/home/woody/capn/mppi013h/conda_envs/orca_cpu_env/lib/python3.6/site-packages/tensorflow_core/python/eager/lift_to_graph.py in lift_to_graph(tensors, graph, sources, disallowed_placeholders, add_sources, handle_captures, base_graph, op_map)\r\n    247   # Check that the initializer does not depend on any placeholders.\r\n    248   sources = object_identity.ObjectIdentitySet(sources or [])\r\n--> 249   visited_ops = set([x.op for x in sources])\r\n    250   op_outputs = collections.defaultdict(set)\r\n    251\r\n\r\n/home/woody/capn/mppi013h/conda_envs/orca_cpu_env/lib/python3.6/site-packages/tensorflow_core/python/eager/lift_to_graph.py in <listcomp>(.0)\r\n    247   # Check that the initializer does not depend on any placeholders.\r\n    248   sources = object_identity.ObjectIdentitySet(sources or [])\r\n--> 249   visited_ops = set([x.op for x in sources])\r\n    250   op_outputs = collections.defaultdict(set)\r\n    251\r\n\r\nAttributeError: 'int' object has no attribute 'op'\r\n``` \r\nUsing this simple model:\r\n``` python\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Activation\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(32, input_dim=784))\r\nmodel.add(Activation('relu'))\r\n``` \r\nIf I use keras instead of tensorflow.keras, it works fine. \r\n\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.8\r\n`\r\n", "comments": ["Issue  is replicating with Tensorflow 2.0.\r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/ca116dc97ea159632f759757e5c235a9/untitled257.ipynb). Thanks!", "Is there any solution for that issue?", "DIY it... write all of the functions and objects without using Tensorflow\n\nOn Thu, Nov 21, 2019 at 10:19 PM kaibrach <notifications@github.com> wrote:\n\n> Is there any solution for that issue?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34201?email_source=notifications&email_token=ADKNJ3SKVSGJ443H72NLMQDQU4CO3A5CNFSM4JMEEYG2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE323DI#issuecomment-557297037>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ADKNJ3RUMKIMNBHZK5WDIGTQU4CO3ANCNFSM4JMEEYGQ>\n> .\n>\n", "The only solution I found is to disable eager execution on startup.\r\n`tf.compat.v1.disable_eager_execution()`\r\n\r\nAfterwards \r\n```python\r\n  f = K.function([self.model.layers[0].input, K.learning_phase()],\r\n                       [self.model.layers[-1].output])\r\n```\r\n\r\ncan be used as usual", "I found a way (workaround) how the `K.function()` can be executed with eager_execution enabled.\r\nI tried this with **TF 2.1 Version**.\r\n\r\nWe need to import private functions from `tensorflow.python.keras` instead of `tf.keras` to get access to `symbolic_learning_phase()`. \r\n\r\n```python\r\n        f = K.function([self.model.layers[0].input, \r\n                        #K.learning_phase()],\r\n                        K.symbolic_learning_phase()], # Workaround (maybe slowdown)\r\n                        [self.model.output])\r\n    \r\n        # Run the function for the number of mc_samples with learning_phase enabled\r\n        Yt_hat = np.array([f((X_test, True))[0] for _ in range(mc_samples)])\r\n```\r\n\r\nThis workaround is ok for development but **DO NOT USE** private functions when writing custom functions/classes!\r\n\r\n`tf.keras` (Public)\r\n`tf.pyhton.keras` (Private)\r\n", "I successfully used the below until TF 2.2, thanks! \r\n\r\nHowever, in TF 2.3 this functionality is broken. \r\nError raised: \r\n```\r\n*** ValueError: Input tensors to a Functional must come from `tf.keras.Input`. Received: Tensor(\"keras_learning_phase:0\", shape=(), dtype=bool) (missing previous layer metadata).\r\n\r\n```\r\nrelated issue: https://github.com/tensorflow/tensorflow/issues/41929 \r\n\r\n> I found a way (workaround) how the `K.function()` can be executed with eager_execution enabled.\r\n> I tried this with **TF 2.1 Version**.\r\n> \r\n> We need to import private functions from `tensorflow.python.keras` instead of `tf.keras` to get access to `symbolic_learning_phase()`.\r\n> \r\n> ```python\r\n>         f = K.function([self.model.layers[0].input, \r\n>                         #K.learning_phase()],\r\n>                         K.symbolic_learning_phase()], # Workaround (maybe slowdown)\r\n>                         [self.model.output])\r\n>     \r\n>         # Run the function for the number of mc_samples with learning_phase enabled\r\n>         Yt_hat = np.array([f((X_test, True))[0] for _ in range(mc_samples)])\r\n> ```\r\n> \r\n> This workaround is ok for development but **DO NOT USE** private functions when writing custom functions/classes!\r\n> \r\n> `tf.keras` (Public)\r\n> `tf.pyhton.keras` (Private)\r\n\r\n", "This will be useful, thanks!\n\nOn Fri, Jul 31, 2020 at 10:23 AM Jordy Van Landeghem <\nnotifications@github.com> wrote:\n\n> I successfully used the below until TF 2.2, thanks!\n>\n> However, in TF 2.3 this functionality is broken.\n> Error raised:\n>\n> *** ValueError: Input tensors to a Functional must come from `tf.keras.Input`. Received: Tensor(\"keras_learning_phase:0\", shape=(), dtype=bool) (missing previous layer metadata).\n>\n>\n> I found a way (workaround) how the K.function() can be executed with\n> eager_execution enabled.\n> I tried this with *TF 2.1 Version*.\n>\n> We need to import private functions from tensorflow.python.keras instead\n> of tf.keras to get access to symbolic_learning_phase().\n>\n>         f = K.function([self.model.layers[0].input,\n>                         #K.learning_phase()],\n>                         K.symbolic_learning_phase()], # Workaround (maybe slowdown)\n>                         [self.model.output])\n>\n>         # Run the function for the number of mc_samples with learning_phase enabled\n>         Yt_hat = np.array([f((X_test, True))[0] for _ in range(mc_samples)])\n>\n> This workaround is ok for development but *DO NOT USE* private functions\n> when writing custom functions/classes!\n>\n> tf.keras (Public)\n> tf.pyhton.keras (Private)\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34201#issuecomment-667026561>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ADKNJ3U2ELHMR3IF6ORB6U3R6KESBANCNFSM4JMEEYGQ>\n> .\n>\n", "Any update? It is blocking factor for upgrading to TF2.3\r\n", "Hello @Jordy-VL and @superJen99\r\n\r\nFor Tensorflow 2.**3** you have to do the following for using the Keras Backend function in eager execution mode:\r\n\r\n```python\r\n        from tensorflow.python.keras.backend import eager_learning_phase_scope\r\n        f = K.function([self.model.layers[0].input],      \r\n                              [self.model.output])\r\n    \r\n        # Run the function for the number of mc_samples with learning_phase enabled\r\n        with eager_learning_phase_scope(value=1): # 0=test, 1=train\r\n            Yt_hat = np.array([f((X))[0] for _ in range(mc_samples)])\r\n```\r\n", "@kaibrach Life-Saver! \r\n\r\nThank you very much :) ", "@StefReck This is an old issue. I think this was resolved in recent `tf-nightly`. I have updated the code little bit to comply the  modifications in TF and Keras. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/95aec95cf616daacd7f5470b8b21a7a1/untitled990.ipynb).  You can find complete code [here](https://github.com/jvishnuvardhan/Stackoverflow_Questions/blob/master/mnist_getting_output_of_each_layer.ipynb) on accessing intermediate layers of a model.\r\n\r\nPlease verify and let us know if the issue persists. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34201\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34201\">No</a>\n"]}, {"number": 34199, "title": "Named dictionary outputs in tf.keras.Model do not work", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): any\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.5\r\n\r\n**Describe the current behavior**\r\nUsing a custom model with named outputs does not work in TF2.\r\n\r\n**Describe the expected behavior**\r\nWhile using tuples for multi output model works fine, using a dictionary fails. Dict inputs and outputs seem to be allowed in the code (and in the tf.keras documentation), however the functionality seem not to be functional yet.\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/gist/kpe/501901b5197675818a2e8a0e0bc8f3a6/keras-named-output-dict.ipynb\r\n\r\n```python\r\n%tensorflow_version 2.x\r\nimport tensorflow as tf\r\n\r\n\r\nmax_seq_len    = 8\r\nchannels_count = 11\r\n\r\nclass MultiOutputModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MultiOutputModel, self).__init__()\r\n        self.dense_a = tf.keras.layers.Dense(3)\r\n        self.dense_b = tf.keras.layers.Dense(4)\r\n        \r\n    def call(self, inputs):\r\n        seq = inputs[\"F\"]\r\n        out_a = self.dense_a(seq)\r\n        out_b = self.dense_b(seq)\r\n        return {\"A\": out_a, \"B\": out_b}\r\n    \r\ndef ds_gen():\r\n    while True:\r\n        inputs  = {\"F\": tf.random.uniform((max_seq_len, channels_count))}\r\n        outputs = {\"A\": tf.random.uniform((), minval=0, maxval=3, dtype=tf.int32), \r\n                   \"B\": tf.random.uniform((), minval=0, maxval=4, dtype=tf.int32)}\r\n        yield inputs, outputs\r\n        \r\nds = tf.data.Dataset.from_generator(ds_gen, \r\n                                    output_types=({\"F\": tf.float32}, \r\n                                                  {\"A\": tf.int32, \"B\":tf.int32}), \r\n                                    output_shapes=({\"F\": tf.TensorShape([max_seq_len, channels_count])}, \r\n                                                   {\"A\":tf.TensorShape([]), \"B\":tf.TensorShape([])}))\r\n# check dataset - a (features, labels) tuple\r\nfor inp, out in ds.batch(8).take(1):\r\n    for ndx, (name, val) in enumerate(inp.items()):\r\n        print(\"features {}: {}: {}\".format(ndx, name, val.shape), val.dtype)\r\n    for ndx, (name, val) in enumerate(out.items()):\r\n        print(\"  labels {}: {}: {}\".format(ndx, name, val.shape), val.dtype)\r\n    \r\nmodel = MultiOutputModel()\r\n\r\ndef features_only(feat, lab):\r\n    return feat\r\n\r\npred = model.predict(ds.map(features_only).batch(8).take(1))\r\nprint(\" prediction:\", type(pred))\r\nfor ndx, out in enumerate(pred):\r\n    print(\" pred out {}: {}\".format(ndx, out.shape), out.dtype)\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),\r\n              loss={\"A\": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                    \"B\": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),})\r\n\r\nmodel.predict({\"seq\": tf.constant([[2],[1]])})\r\nmodel.fit(ds.batch(1))\r\n```\r\n\r\nThe output from the above code is:\r\n```\r\nfeatures 0: F: (8, 8, 11) <dtype: 'float32'>\r\n  labels 0: A: (8,) <dtype: 'int32'>\r\n  labels 1: B: (8,) <dtype: 'int32'>\r\n prediction: <class 'list'>\r\n pred out 0: (8, 8, 3) float32\r\n pred out 1: (8, 8, 4) float32\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-28-948267c0fc05> in <module>()\r\n     49 model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n     50               loss={\"A\": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n---> 51                     \"B\": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),})\r\n     52 \r\n     53 model.fit(ds.batch(8))\r\n\r\n3 frames\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/utils/generic_utils.py in check_for_unexpected_keys(name, input_dict, expected_values)\r\n    589     raise ValueError('Unknown entries in {} dictionary: {}. Only expected '\r\n    590                      'following keys: {}'.format(name, list(unknown),\r\n--> 591                                                  expected_values))\r\n    592 \r\n    593 \r\n\r\nValueError: Unknown entries in loss dictionary: ['A', 'B']. Only expected following keys: ['output_1', 'output_2']\r\n```", "comments": ["@kpe \r\n\r\nI tried reproducing the issue in colab with TF 2.0 . I am seeing the below error message.\r\n`AttributeError: 'str' object has no attribute 'dtype'` . Is this the expected behavior?. Thanks!", "@ravikyram - I update the example and added a colab link for easier reproducibility.", "I have tried on colab with TF version 2.0 ,2.1.0-dev20191119 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/d767a6d43b7280b9b2dbdff119b6afe1/keras-named-output-dict.ipynb). Thanks!", "I would also like to add that we could have resolved this issue with a simple workaround by simply modifying the model.output_names and model._output_layers (in tensorflow 1.15 and below)\r\n\r\ne.g:\r\n```\r\nclass CustomLayer()\r\n\r\n  def __init__(self, ...):\r\n     self.nested_layer_1 = tf.keras.layers.Dense(...)\r\n     self.nested_layer_2 = tf.keras.layers.Dense(...)\r\n     # for each nested layer that produces output we add it to a 'nested_layers' dict mapping names to layers\r\n     self.nested_layers_dict[nested_layer_1.name] = self.nested_layer_1\r\n     self.nested_layers_dict[nested_layer_2.name] = self.nested_layer_2\r\n      ...\r\n\r\n  def call(self, inputs):\r\n      ...\r\n     outputs_dict[self.nested_layer1.name] = self.nested_layer_1(inputs)\r\n     outputs_dict[self.nested_layer2.name] = self.nested_layer_2(inputs)\r\n     return outputs_dict\r\n\r\ndef __main__():\r\n  ...\r\n  inputs = [...]\r\n  custom_layer =CustomLayer()\r\n  outputs = custom_layer(inputs)\r\n  model = tf.keras.models.Model(inputs=inputs, outputs=outputs )\r\n  model.output_names = list(custom_layer.output.keys())\r\n  model._output_layers = [custom_layer.nested_layers_dict[output_name] for output_name in model.output_names]\r\n```\r\nNow when we do that on tensorflow 2.0: after changing the _output_layers the model adds the nested layers into itself and creates extra layers on top of the original model.\r\n", "Any news regarding the bug?", "Thanks for the issue! Support for arbitrary nested structures (including dicts) is available in the latest tf-nightly: `pip install -U tf-nightly`.\r\n\r\nI think there are some issues in the code provided regarding the loss used (I don't think the Model is outputting the shape of data that SparseCategoricalAccuracy expects), but confirmed that prediction is working as expected. \r\n\r\nAlso see this bug: https://github.com/tensorflow/tensorflow/issues/33245\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nmax_seq_len    = 8\r\nchannels_count = 11\r\n\r\nclass MultiOutputModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MultiOutputModel, self).__init__()\r\n        self.dense_a = tf.keras.layers.Dense(3)\r\n        self.dense_b = tf.keras.layers.Dense(4)\r\n        \r\n    def call(self, inputs):\r\n        seq = inputs[\"F\"]\r\n        out_a = self.dense_a(seq)\r\n        out_b = self.dense_b(seq)\r\n        return {\"A\": out_a, \"B\": out_b}\r\n    \r\ndef ds_gen():\r\n    while True:\r\n        inputs  = {\"F\": tf.random.uniform((max_seq_len, channels_count))}\r\n        outputs = {\"A\": tf.random.uniform((), minval=0, maxval=3, dtype=tf.int32), \r\n                   \"B\": tf.random.uniform((), minval=0, maxval=4, dtype=tf.int32)}\r\n        yield inputs, outputs\r\n        \r\nds = tf.data.Dataset.from_generator(ds_gen, \r\n                                    output_types=({\"F\": tf.float32}, \r\n                                                  {\"A\": tf.int32, \"B\":tf.int32}), \r\n                                    output_shapes=({\"F\": tf.TensorShape([max_seq_len, channels_count])}, \r\n                                                   {\"A\":tf.TensorShape([]), \"B\":tf.TensorShape([])}))\r\n# check dataset - a (features, labels) tuple\r\nfor inp, out in ds.batch(8).take(1):\r\n    for ndx, (name, val) in enumerate(inp.items()):\r\n        print(\"features {}: {}: {}\".format(ndx, name, val.shape), val.dtype)\r\n    for ndx, (name, val) in enumerate(out.items()):\r\n        print(\"  labels {}: {}: {}\".format(ndx, name, val.shape), val.dtype)\r\n    \r\nmodel = MultiOutputModel()\r\n\r\ndef features_only(feat, lab):\r\n    return feat\r\n\r\npred = model.predict(ds.map(features_only).batch(8).take(1))\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34199\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34199\">No</a>\n"]}, {"number": 34198, "title": "Not found: Op type not registered 'BatchMatMulV2'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- TensorFlow version (use command below): 1.13.1  (docker build inside)\r\n- Docker  tensorrtserver:19.06-py3\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:  CUDA Version 10.1.243 / 7.6.3\r\n- GPU model and memory: T4 16G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI am using tensorrt-inference-server to deploy my tensorflow savedmodel files( tf version : 1.11.1).\r\n\r\nthe config.pbtxt \r\n\r\n`name: \"jingjing\"\r\nplatform: \"tensorflow_savedmodel\"\r\nmax_batch_size: 0\r\ninput [\r\n  {\r\n    name: \"inputs\"\r\n    data_type: TYPE_INT32\r\n    dims: [ -1]\r\n  },\r\n  {\r\n    name: \"input_lengths\"\r\n    data_type: TYPE_INT32\r\n    dims: [ -1 ]\r\n  },\r\n  {\r\n    name: \"split_infos\"\r\n    data_type: TYPE_INT32\r\n    dims: [ 1, -1 ]\r\n  }\r\n]\r\noutput [\r\n  {\r\n    name: \"linear_wav_outputs\"\r\n    data_type: TYPE_FP32\r\n    dims: [ -1 ]\r\n  }\r\n]\r\nversion_policy: { all { }}`\r\nwhen trying to run the tensorrt inference server , errors will occur like below :\r\n`failed to load 'jingjing' version 1: Not found: Op type not registered 'BatchMatMulV2' in binary running on fa60ca095bbf. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`\r\n\r\nand when curl \r\n``id: \"inference:0\"\r\nversion: \"1.3.0\"\r\nuptime_ns: 16498837986\r\nmodel_status {\r\n  key: \"jingjing\"\r\n  value {\r\n    config {\r\n      name: \"jingjing\"\r\n      platform: \"tensorflow_savedmodel\"\r\n      version_policy {\r\n        all {\r\n        }\r\n      }\r\n      max_batch_size: 1\r\n      input {\r\n        name: \"inputs\"\r\n        data_type: TYPE_INT32\r\n        dims: -1\r\n        dims: -1\r\n      }\r\n      input {\r\n        name: \"input_lengths\"\r\n        data_type: TYPE_INT32\r\n        dims: -1\r\n      }\r\n      input {\r\n        name: \"split_infos\"\r\n        data_type: TYPE_INT32\r\n        dims: 1\r\n        dims: -1\r\n      }\r\n      output {\r\n        name: \"linear_wav_outputs\"\r\n        data_type: TYPE_FP32\r\n        dims: -1\r\n      }\r\n      instance_group {\r\n        name: \"jingjing\"\r\n        count: 1\r\n        gpus: 0\r\n        kind: KIND_GPU\r\n      }\r\n      default_model_filename: \"model.savedmodel\"\r\n    }\r\n    version_status {\r\n      key: 1\r\n      value {\r\n        ready_state: MODEL_UNAVAILABLE\r\n      }\r\n    }\r\n    version_status {\r\n      key: 2\r\n      value {\r\n        ready_state: MODEL_UNAVAILABLE\r\n      }\r\n    }\r\n    version_status {\r\n      key: 3\r\n      value {\r\n        ready_state: MODEL_UNAVAILABLE\r\n      }\r\n    }\r\n    version_status {\r\n      key: 4\r\n      value {\r\n        ready_state: MODEL_UNAVAILABLE\r\n      }\r\n    }\r\n  }\r\n}\r\nready_state: SERVER_READY``\r\n\r\n**Describe the expected behavior**\r\n\r\nthe ready_state of model should be MODEL_READY\r\n\r\n**Code to reproduce the issue**\r\n`docker run --rm --gpus \"device=3\" --shm-size=1g --ulimit memlock=-1 \\\r\n--ulimit stack=67108864 -p8000:8000 -p8001:8001 -p8002:8002 \\\r\n-v/home/yichao.li/models:/models nvcr.io/nvidia/tensorrtserver:19.06-py3 \\\r\ntrtserver --model-store=/models &`\r\n\r\nplease help to solve the \" Op type not registered 'BatchMatMulV2'\" issue\r\n", "comments": ["@superhg2012 ,\r\nCan you share a standalone code to reproduce the error reported here?Thanks!", "@oanush There is just one savedmodel.pb and variables files. I deployed it to tensorrtinference server container. then the error occurs.", "@superhg2012 This is duplicate of this issue that you have created [here](https://github.com/NVIDIA/tensorrt-inference-server/issues/855).  I am afraid you have to do as mentioned in the comment [here](https://github.com/NVIDIA/tensorrt-inference-server/issues/855#issuecomment-553210106). Thats the only fix as of now.\r\n\r\nAlso please note that this is not a tensorflow issue.  Thanks!", "Closing this issue as its not cause due to native tensorflow. Please add additional comments and we can open this issue again. Thanks!"]}, {"number": 34197, "title": "Building Tensorflow 1.13 for python 3.8 fails on nullptr conversion.", "body": "**System information**\r\n- OS Platform and Distribution: Centos 7\r\n- TensorFlow installed from : source\r\n- TensorFlow version: 1.13.2\r\n- Python version: 3.8.0\r\n- Bazel version : 0.21.0\r\n- GCC/Compiler version : gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39) \r\n- CUDA/cuDNN version: 9.1/7.0\r\n- GPU model and memory: 1080ti\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen attempting to build the tensorflow branch r1.13 targeting python 3.8 the build fails with the following error:\r\n\r\nndarray_tensor_bridge.cc:108:1: error: cannot convert 'std::nullptr_t' to 'Py_ssize_t {aka long int}' in initialization\r\n };\r\n\r\nThis error occurs in three files that I found: bfloat16.cc, ndarray_tensor_bridge.cc, pywrap_tfe_src.cc\r\n\r\nThis appears to be the same problem fixed in issue #33543 \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI am willing to provide more details, but this appears to be the exact same issue. I am including the git diff from the version that compiled and works for me.\r\n\r\n```patch\r\ndiff --git a/tensorflow/python/eager/pywrap_tfe_src.cc b/tensorflow/python/eager/pywrap_tfe_src.cc\r\nindex 9ce500b..726c4c5 100644\r\n--- a/tensorflow/python/eager/pywrap_tfe_src.cc\r\n+++ b/tensorflow/python/eager/pywrap_tfe_src.cc\r\n@@ -1216,7 +1216,7 @@ static PyTypeObject TFE_Py_Tape_Type = {\r\n     sizeof(TFE_Py_Tape),                          /* tp_basicsize */\r\n     0,                                            /* tp_itemsize */\r\n     &TFE_Py_Tape_Delete,                          /* tp_dealloc */\r\n-    nullptr,                                      /* tp_print */\r\n+    NULL,                                         /* tp_print */\r\n     nullptr,                                      /* tp_getattr */\r\n     nullptr,                                      /* tp_setattr */\r\n     nullptr,                                      /* tp_reserved */\r\ndiff --git a/tensorflow/python/lib/core/bfloat16.cc b/tensorflow/python/lib/core/bfloat16.cc\r\nindex fde3a83..e0da0f4 100644\r\n--- a/tensorflow/python/lib/core/bfloat16.cc\r\n+++ b/tensorflow/python/lib/core/bfloat16.cc\r\n@@ -317,7 +317,7 @@ PyTypeObject PyBfloat16_Type = {\r\n     sizeof(PyBfloat16),                        // tp_basicsize\r\n     0,                                         // tp_itemsize\r\n     nullptr,                                   // tp_dealloc\r\n-    nullptr,                                   // tp_print\r\n+    NULL,                                      // tp_print\r\n     nullptr,                                   // tp_getattr\r\n     nullptr,                                   // tp_setattr\r\n     nullptr,                                   // tp_compare / tp_reserved\r\ndiff --git a/tensorflow/python/lib/core/ndarray_tensor_bridge.cc b/tensorflow/python/lib/core/ndarray_tensor_bridge.cc\r\nindex 0d58385..43ab92c 100644\r\n--- a/tensorflow/python/lib/core/ndarray_tensor_bridge.cc\r\n+++ b/tensorflow/python/lib/core/ndarray_tensor_bridge.cc\r\n@@ -86,7 +86,7 @@ PyTypeObject TensorReleaserType = {\r\n     0,                                /* tp_itemsize */\r\n     /* methods */\r\n     TensorReleaser_dealloc,      /* tp_dealloc */\r\n-    nullptr,                     /* tp_print */\r\n+    NULL,                        /* tp_print */\r\n     nullptr,                     /* tp_getattr */\r\n     nullptr,                     /* tp_setattr */\r\n     nullptr,                     /* tp_compare */\r\n```\r\n", "comments": ["@odinsbane thanks for those diffs, i'm using 1.14 and 3.8 as well and was able to complete a successful build using your changes. ", "CentOS7, TF1.14, CUDA10.1, PYTHON3.8, cuDNN7.5, TRT6\r\nAfter applying this diff patch, successfully built.", "As the old branches are closed, we will not backport the compilation fix to these branches.\r\nOnly security issues will be patched and released for old branches.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34197\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34197\">No</a>\n", "The documentation here, https://www.tensorflow.org/guide/upgrade says to\nupgrade to 1.14 for 2.0 migration. Is that the recommended route to\nupgrade, or is 1.15 preferred? I am not ready to port to 2.X yet, but I\nwould like to be ready to.\n\nOn Tue, Jan 14, 2020 at 10:58 PM tensorflow-bot[bot] <\nnotifications@github.com> wrote:\n\n> Are you satisfied with the resolution of your issue?\n> Yes\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34197>\n> No\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34197>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34197?email_source=notifications&email_token=AA2NNEOK336NW4AQYEHRMVDQ5Y7SVA5CNFSM4JMB72GKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEI6NWLQ#issuecomment-574413614>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AA2NNEMJ7GZ6LGHRA3CBZ4TQ5Y7SVANCNFSM4JMB72GA>\n> .\n>\n", "1.15 should be better, as there are bugs we have fixed after 1.14."]}, {"number": 34196, "title": "Keras fails to combine predicted variable-sized sequences", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-17764-gae26958 2.1.0-dev20191111\r\n- Python version: Google Colab py3\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: Google Colab\r\n- GPU model and memory: Google Colab\r\n\r\n**Describe the current behavior**\r\nException raised (from numpy concatenate) when predicting sequence labels with variable sequence length batches.\r\n\r\n**Describe the expected behavior**\r\nThere should be no error, as in fit/evaluate methods work fine.\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1IPIkj5PlUWKCpuTm9NWM5BLO_suL5VI3\r\n\r\n**Other info / logs**\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-23-e5e9890a3d75> in <module>()\r\n----> 1 model.predict(get_dataset(labels=False))\r\n\r\n5 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n    972         max_queue_size=max_queue_size,\r\n    973         workers=workers,\r\n--> 974         use_multiprocessing=use_multiprocessing)\r\n    975 \r\n    976   def reset_metrics(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in predict(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    496         model, ModeKeys.PREDICT, x=x, batch_size=batch_size, verbose=verbose,\r\n    497         steps=steps, callbacks=callbacks, max_queue_size=max_queue_size,\r\n--> 498         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\r\n    499 \r\n    500 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in _model_iteration(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    473               mode=mode,\r\n    474               training_context=training_context,\r\n--> 475               total_epochs=1)\r\n    476           cbks.make_logs(model, epoch_logs, result, mode)\r\n    477 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    185 \r\n    186   # End of an epoch.\r\n--> 187   aggregator.finalize()\r\n    188   return aggregator.results\r\n    189 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py in finalize(self)\r\n    349   def finalize(self):\r\n    350     for result in self.results:\r\n--> 351       result.finalize()\r\n    352     self.results = [i.results for i in self.results]\r\n    353     self.results = nest.pack_sequence_as(self._structure, self.results)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py in finalize(self)\r\n    187 \r\n    188     else:\r\n--> 189       self.results = np.concatenate(self.results, axis=0)\r\n    190 \r\n    191     if isinstance(self.results, ops.EagerTensor):\r\n\r\n<__array_function__ internals> in concatenate(*args, **kwargs)\r\n\r\nValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 7 and the array at index 1 has size 9\r\n```", "comments": ["I have tried on colab with TF version 2.0 ,2.1.0-dev20191111 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/19ccefa635122accffea177617bfc459/untitled358.ipynb). Thanks!", "Issue still here with tf-nightly, but exception stack changed\r\n```python\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-16-e5e9890a3d75> in <module>()\r\n----> 1 model.predict(get_dataset(labels=False))\r\n\r\n11 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     92       raise ValueError('{} is not supported in multi-worker mode.'.format(\r\n     93           method.__name__))\r\n---> 94     return method(self, *args, **kwargs)\r\n     95 \r\n     96   return tf_decorator.make_decorator(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n   1394             callbacks.on_predict_batch_end(end_step, {'outputs': batch_outputs})\r\n   1395       callbacks.on_predict_end()\r\n-> 1396     all_outputs = nest.map_structure_up_to(batch_outputs, concat, outputs)\r\n   1397     return tf_utils.to_numpy_or_python_type(all_outputs)\r\n   1398 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in map_structure_up_to(shallow_tree, func, *inputs, **kwargs)\r\n   1129       lambda _, *values: func(*values),  # Discards the path arg.\r\n   1130       *inputs,\r\n-> 1131       **kwargs)\r\n   1132 \r\n   1133 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in map_structure_with_tuple_paths_up_to(shallow_tree, func, *inputs, **kwargs)\r\n   1225                     in _yield_flat_up_to(shallow_tree, inputs[0], is_seq)]\r\n   1226   results = [func(*args, **kwargs) for args in zip(flat_path_list,\r\n-> 1227                                                    *flat_value_lists)]\r\n   1228   return pack_sequence_as(structure=shallow_tree, flat_sequence=results,\r\n   1229                           expand_composites=expand_composites)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in <listcomp>(.0)\r\n   1224   flat_path_list = [path for path, _\r\n   1225                     in _yield_flat_up_to(shallow_tree, inputs[0], is_seq)]\r\n-> 1226   results = [func(*args, **kwargs) for args in zip(flat_path_list,\r\n   1227                                                    *flat_value_lists)]\r\n   1228   return pack_sequence_as(structure=shallow_tree, flat_sequence=results,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in <lambda>(_, *values)\r\n   1127   return map_structure_with_tuple_paths_up_to(\r\n   1128       shallow_tree,\r\n-> 1129       lambda _, *values: func(*values),  # Discards the path arg.\r\n   1130       *inputs,\r\n   1131       **kwargs)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in concat(tensors, axis)\r\n   1882   if isinstance(tensors[0], ragged_tensor.RaggedTensor):\r\n   1883     return ragged_concat_ops.concat(tensors, axis=axis)\r\n-> 1884   return array_ops.concat(tensors, axis=axis)\r\n   1885 \r\n   1886 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    178     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    179     try:\r\n--> 180       return target(*args, **kwargs)\r\n    181     except (TypeError, ValueError):\r\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in concat(values, axis, name)\r\n   1628           dtype=dtypes.int32).get_shape().assert_has_rank(0)\r\n   1629       return identity(values[0], name=name)\r\n-> 1630   return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n   1631 \r\n   1632 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in concat_v2(values, axis, name)\r\n   1196       return _result\r\n   1197     except _core._NotOkStatusException as e:\r\n-> 1198       _ops.raise_from_not_ok_status(e, name)\r\n   1199     except _core._FallbackException:\r\n   1200       pass\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6810   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6811   # pylint: disable=protected-access\r\n-> 6812   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6813   # pylint: enable=protected-access\r\n   6814 \r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [5,6,1] vs. shape[2] = [5,10,1] [Op:ConcatV2] name: concat\r\n```", "@shkarupa-alex,\r\nSeems like the error has been fixed in stable version of TF v2.2. I was able to run the code without any issues with TF v2.2, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a07d17dcda435b1b3dcb48730a27e8fa/34196-2-2.ipynb). Thanks!", "> @shkarupa-alex,\r\n> Seems like the error has been fixed in stable version of TF v2.2. I was able to run the code without any issues with TF v2.2, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a07d17dcda435b1b3dcb48730a27e8fa/34196-2-2.ipynb). Thanks!\r\n\r\nNope.\r\nIssue is still present in tf-nightly (see link to colab in issue head).", "I have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20200728`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/f4fb7a0cbee6adca8d6525eb9eb434cd/untitled195.ipynb#scrollTo=b70Y0hpr4d6I).Thanks!", "@shkarupa-alex Thanks for the issue! In order to combine Tensors of different shapes, your Model will need to output RaggedTensors. You can do this by adding a Layer at the end of your Model to convert each Tensor to a RaggedTensor. The Dense layer currently at the end of the Model is outputting regular Tensors, and to concat these for `Model.predict`, only the 0-axis can have different shapes", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34196\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34196\">No</a>\n", "> @shkarupa-alex Thanks for the issue! In order to combine Tensors of different shapes, your Model will need to output RaggedTensors. You can do this by adding a Layer at the end of your Model to convert each Tensor to a RaggedTensor. The Dense layer currently at the end of the Model is outputting regular Tensors, and to concat these for `Model.predict`, only the 0-axis can have different shapes\r\n\r\nhow do you output a ragged tensor from your dense layer with Keras Tensorflow? \r\n\r\nex. my last layer is: \r\n\r\n model.add(TimeDistributed(Dense(2, activation='sigmoid')))\r\n\r\nhow do i specify this layer to output ragged? ", "Dense layers don't support ragged tensors.\r\nSo, i wrote a small wrapper\r\n```python\r\nclass MapFlat(layers.Wrapper):\r\n    def __init__(self, layer, **kwargs):\r\n        super().__init__(layer, **kwargs)\r\n        self._supports_ragged_inputs = True\r\n\r\n    def build(self, input_shape=None):\r\n        super(MapFlat, self).build([None])\r\n\r\n    def call(self, inputs, **kwargs):\r\n        return tf.ragged.map_flat_values(self.layer, inputs)\r\n\r\n    @shape_type_conversion\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape + self.layer.compute_output_shape([None])[1:]\r\n```\r\n\r\nUse it like so:\r\n```python\r\nmodel.add(WithRagged(Dense(2, activation='sigmoid')))\r\n```", "> Dense layers don't support ragged tensors. So, i wrote a small wrapper\r\n> \r\n> ```python\r\n> class MapFlat(layers.Wrapper):\r\n>     def __init__(self, layer, **kwargs):\r\n>         super().__init__(layer, **kwargs)\r\n>         self._supports_ragged_inputs = True\r\n> \r\n>     def build(self, input_shape=None):\r\n>         super(MapFlat, self).build([None])\r\n> \r\n>     def call(self, inputs, **kwargs):\r\n>         return tf.ragged.map_flat_values(self.layer, inputs)\r\n> \r\n>     @shape_type_conversion\r\n>     def compute_output_shape(self, input_shape):\r\n>         return input_shape + self.layer.compute_output_shape([None])[1:]\r\n> ```\r\n> \r\n> Use it like so:\r\n> \r\n> ```python\r\n> model.add(WithRagged(Dense(2, activation='sigmoid')))\r\n> ```\r\nThank you so much for creating this code example for me ! I really appreciate it. \r\n\r\nUnfortunately it did not run, WithRagged is undefined in this example, did you mean to use MapFlat instead like this? \r\n\r\n```python\r\nmodel.add(MapFlat(Dense(2, activation='sigmoid')))\r\n```\r\n\r\nif my model requires the use of TimeDistributed layer would this still work with a RaggedTensor?\r\n\r\n```python\r\ndef action_model():\r\n\r\n    model = keras.Sequential()\r\n\r\n    model.add(Bidirectional(LSTM(101,return_sequences=True),input_shape=(None,101)))\r\n    model.add(Bidirectional(LSTM(25,return_sequences=True)))\r\n    model.add(TimeDistributed(Dense(20,activation='relu')))\r\n    model.add(Dropout(0.4))\r\n    model.add(TimeDistributed(Dense(16,activation='relu')))\r\n    model.add(Dropout(0.4))\r\n    model.add(TimeDistributed((MapFlat(Dense(2, activation='sigmoid')))))\r\n```\r\nI tried to run this model but got the following error:\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"rnn-tf.py\", line 133, in <module>\r\n    model = action_model()\r\n  File \"rnn-tf.py\", line 128, in action_model\r\n    model.add(MapFlat(Dense(2, activation='sigmoid')))\r\n  File \"/opt/anaconda3/envs/nnml38/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 517, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/opt/anaconda3/envs/nnml38/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\", line 223, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"/opt/anaconda3/envs/nnml38/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 951, in __call__\r\n    return self._functional_construction_call(inputs, args, kwargs,\r\n  File \"/opt/anaconda3/envs/nnml38/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1090, in _functional_construction_call\r\n    outputs = self._keras_tensor_symbolic_call(\r\n  File \"/opt/anaconda3/envs/nnml38/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 822, in _keras_tensor_symbolic_call\r\n    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n  File \"/opt/anaconda3/envs/nnml38/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 862, in _infer_output_signature\r\n    self._maybe_build(inputs)\r\n  File \"/opt/anaconda3/envs/nnml38/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2710, in _maybe_build\r\n    self.build(input_shapes)  # pylint:disable=not-callable\r\n  File \"rnn-tf.py\", line 109, in build\r\n    super(MapFlat, self).build([None])\r\n  File \"/opt/anaconda3/envs/nnml38/lib/python3.8/site-packages/tensorflow/python/keras/layers/wrappers.py\", line 59, in build\r\n    self.layer.build(input_shape)\r\n  File \"/opt/anaconda3/envs/nnml38/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\", line 1182, in build\r\n    raise ValueError('The last dimension of the inputs to `Dense` '\r\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n```\r\n\r\nThis whole issue is very bizarre because I am able to train my network using this model, with variable-sized sequences, but when it comes to model.predict there is a failure. \r\n\r\n# UPDATE: \r\n\r\nWhen I input 1 single file into model.predict, it works as expected \r\n\r\neverything breaks when I have more than 1 file. \r\n\r\na hacky fix I did was just to make a loop and call model.predict on each individual file. This is of course not practical. I would very much appreciate a way to get model.predict to work with many files.  ", "Sorry, seems i posted wrong class. Here it is correct one:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom keras import backend, layers\r\nfrom keras.utils.generic_utils import has_arg, register_keras_serializable\r\nfrom keras.utils.tf_utils import shape_type_conversion\r\n\r\n\r\nclass WithRagged(layers.Wrapper):\r\n    \"\"\" Passes ragged tensor to layer that accepts only dense one.\r\n\r\n    Arguments:\r\n      layer: The `Layer` instance to be wrapped.\r\n    \"\"\"\r\n\r\n    def __init__(self, layer, **kwargs):\r\n        super(WithRagged, self).__init__(layer, **kwargs)\r\n        self.input_spec = layer.input_spec\r\n        self.supports_masking = layer.supports_masking\r\n        self._supports_ragged_inputs = True\r\n\r\n        if not isinstance(layer, layers.Layer):\r\n            raise ValueError(\r\n                'Please initialize `WithRagged` layer with a '\r\n                '`Layer` instance. You passed: {input}'.format(input=layer))\r\n\r\n    def build(self, input_shape=None):\r\n        if not self.layer.built:\r\n            self.layer.build(input_shape)\r\n\r\n        self.input_spec = self.layer.input_spec\r\n\r\n        zero = '' if self.layer.dtype == tf.string else 0\r\n        self.masking_layer = layers.Masking(mask_value=zero)\r\n\r\n        super(WithRagged, self).build()\r\n\r\n    def call(self, inputs, **kwargs):\r\n        layer_kwargs = {}\r\n        for key in kwargs.keys():\r\n            if has_arg(self.layer.call, key):\r\n                layer_kwargs[key] = kwargs[key]\r\n\r\n        inputs_dense, row_lengths = backend.convert_inputs_if_ragged(inputs)\r\n        inputs_dense = self.masking_layer(inputs_dense)\r\n        outputs_dense = self.layer.call(inputs_dense, **layer_kwargs)\r\n        outputs = backend.maybe_convert_to_ragged(row_lengths is not None, outputs_dense, row_lengths)\r\n\r\n        return outputs\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return self.layer.compute_output_shape(input_shape)\r\n\r\n    def compute_mask(self, inputs, mask=None):\r\n        return self.layer.compute_mask(inputs, mask)\r\n\r\n    def get_config(self):\r\n        return super(WithRagged, self).get_config()\r\n\r\n```"]}, {"number": 34195, "title": "Problem with the Run API  in  C++", "body": "Hello, I have a problem with the Run API in C++ with the following code:\r\n\r\n```\r\nstd::unique_ptr<tensorflow::Tensor> out_t;\r\nstd::vector<Tensor> res;\r\nstd::string entry_name = \"name\";\r\nstd::string target_ops_name = \"ops_name\";     \r\nTF_CHECK_OK(session.Run({{entry_name, *out_t}},{target_ops_name},{},&res));\r\n```\r\nNB: I have 2 Output tensors with the corresponding names (WithOpName(...)) \"name\" and \"ops_name\". \r\n\r\nI receive the following compilation error when I compile my project in bazel.\r\n```\r\nerror: no matching function for call to 'tensorflow::ClientSession::Run(<brace-enclosed initializer list>, <brace-enclosed initializer list>, <brace-enclosed initializer list>, std::vector<tensorflow::Tensor>*)'\r\n         TF_CHECK_OK(session.Run({{entry_name, *out_t}},{target_ops_name},{},&res));\r\n\r\n```\r\nI use the latest version of tensorflow compiled with bazel. Would be glad if someone could give my hints on what the problem could be. Thanks.\r\n", "comments": ["@Yuhala Did you manage to figure out the issue? I'm facing the same, thanks", " facing the same too ~~~", "@Yuhala,\r\n\r\nWe are checking to see if this is still an issue. We recommend that you can update the TF to latest stable version. \r\n\r\nCan you confirm if you had already taken a look at this [link](https://stackoverflow.com/questions/44775176/initialize-variables-in-tensorflow-c-api) which discusses about a similar issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34195\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34195\">No</a>\n"]}, {"number": 34194, "title": "tf.size() has no documentation", "body": "https://www.tensorflow.org/api_docs/python/tf/size\r\n\r\nExample: \"Returns the number of elements in the tensor. It equals the length of the flattened tensor.\"", "comments": ["@danijar, I want to contribute to this issue. Can you help me in doing so?\r\nI am completely new to this world of open source.\r\n", "Sorry, I can't assist you with the workflow. [CONTRIBUTING.md](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md) would be a good place to get started.", "@lamberta @goldiegadde Is this issue being worked on? If not, can I submit a PR? ", "Submitted a PR @danijar @lamberta ", "Thank you @copperwiring \ud83d\ude80"]}, {"number": 34193, "title": "[TF-TRT] [Tensorflow1.14] [InceptionV3] INT8 Top1 accuracy has a big drop compared to original.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Nvidia NGC docker 19.10\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.5.0\r\n- GPU model and memory: TitanXp 12GB\r\n\r\n**Describe the current behavior**\r\n\r\nMy environment is based on NVidia NGC docker 19.10 [[NGC]](https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow/tags) , which uses Tensorflow 1.14, TensorRT6, CUDA10.1.\r\n\r\nI tried to quantize a normal InceptionV3 155 classes classification model to FP32/FP16/INT8 and test the Top1 accuracy over a dataset of 17000 images. It turns out that the Top1 class accuracy of FP32 and FP16 is exactly 100% identical with the original InceptionV3 model. However, the INT8 quantized model is much different with original model, Top1 miss rate is 5037/17000=29.6%. \r\n\r\nThe INT8 model is calibrated by randomly selected 1000 imaged from image dataset. So I'm confused whether it's a TF-TRT bug? Do you have experimental record of INT8 Top1 miss accuracy?\r\n\r\nMy TF-TRT conversion script:\r\n\r\n```\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nfrom tensorflow.python.framework import importer as importer\r\n\r\nprebuilt_graph_path = \"frozen_graph.pb\"\r\nfrozen_graph = tf.GraphDef()\r\nif os.path.isfile(prebuilt_graph_path):\r\n    with tf.gfile.GFile(prebuilt_graph_path, \"rb\") as f:\r\n        frozen_graph.ParseFromString(f.read())\r\n\r\nconverter = trt.TrtGraphConverter(\r\n    input_graph_def=frozen_graph,\r\n    nodes_blacklist=[\"InceptionV3/Logits/SpatialSqueeze\"],\r\n    max_batch_size=16,\r\n    max_workspace_size_bytes=1<<20,\r\n    precision_mode=\"INT8\",\r\n    minimum_segment_size=2,\r\n    is_dynamic_op=False\r\n    )\r\n\r\n# Conversion                                                                                                                      \r\nfrozen_graph = converter.convert()\r\n\r\n# Calibration                                                                                                                     \r\ncalibrate_dir = 'calibrate_set'\r\ncalibrate_image_list = []\r\nfor filename in glob.glob(calibrate_dir+'/*.jpg'):\r\n    im=Image.open(filename)\r\n    img=img.resize((299, 299), Image.NEAREST)\r\n    image=np.array(img, dtype=float)\r\n    image=(image-127)/128\r\n    calibrate_image_list.append(image)\r\n\r\nclass CalibrationData(object):\r\n    def __init__(self):\r\n        self.pointer = 0\r\n        self.data = calibrate_image_list\r\n\r\n    def next(self):\r\n        self.now = self.pointer\r\n        self.pointer += 16\r\n        print(\"Calibration data length: {}, Calibrate on seg: {}~{}\".format(len(self.data), self.now, self.pointer-1))\r\n        return {'Placeholder:0': np.array(self.data[self.now : self.pointer])}\r\n\r\nfrozen_graph = converter.calibrate(\r\n    fetch_names=[\"InceptionV3/Logits/SpatialSqueeze\"],\r\n    num_runs=10,                                             \r\n    feed_dict_fn=CalibrationData().next\r\n)\r\n\r\nwith tf.gfile.GFile(\"converted_int8.pb\", \"wb\") as f:\r\n    f.write(frozen_graph.SerializeToString())\r\n```\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n    For the InceptionV3 model, the Top1 miss rate of TF-TRT INT8 quantized model shall be smaller than 3~5% after calibration, however, I got Top1 miss rate >29%\r\n\r\n**Code to reproduce the issue**\r\n\r\n\r\n**Other info / logs**\r\n\r\n", "comments": ["Close the thread due to incorrect method."]}, {"number": 34192, "title": "Bug in function estimator_train when 'evaluator' included in TF_CONFIG ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): rc1.13\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.0/7.6\r\n- GPU model and memory: Run in GPU\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nwith the following configurations:\r\n\r\n `os.environ['TF_CONFIG'] = json.dumps({\"cluster\": {\"evaluator\": [\"10.0.26.2:4000\"], \"ps\": [\"10.0.26.2:5000\"], \"worker\": [\"10.0.26.2:6000\"]}, \"rpc_layer\": \"grpc\", \"task\": {\"index\": \"0\", \"type\": \"worker\"})`\r\n\r\nRunning estimator.train(), the output from the command line is\r\n\r\n```\r\nTraceback (most recent call last):\r\n  ...\r\n    model_estimator.train(input_fn=lambda: input_fn(image_ids=train_id), steps=global_step)\r\n  File \"/home/zxy/steel/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 405, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/zxy/steel/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1169, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/home/zxy/steel/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1227, in _train_model_distributed\r\n    hooks)\r\n  File \"/home/zxy/steel/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py\", line 304, in estimator_train\r\n    if 'evaluator' in cluster_spec:\r\nTypeError: argument of type 'ClusterSpec' is not iterable\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen calling estimator.train with 'evaluator' included in RunConfig, a ValueError should be reported. The following is the fragment code in estimator_training.py. \r\n    \r\n    if 'evaluator' in cluster_spec:\r\n        raise ValueError(\"'evaluator' job is not supported if you don't use \"\r\n                         '`train_and_evaluate`')\r\n\r\nHere the value of cluster_spec is like ClusterSpec({'evaluator': ['10.0.26.2:4000'], 'ps': ['10.0.26.2:5000'], 'worker': ['10.0.26.2:6000']})\r\n\r\nif 'evaluator' in cluster_spec.jobs which is the way it should be.\r\n\r\n\r\n", "comments": ["Only r1.13 seems to have this problem", "@mandyclustar, Please provide the complete code to reproduce the reported issue here. Thanks!", "@mandyclustar, Any update. ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34191, "title": "TF2.1 build from source failure", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: `No`\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: `Linux Ubuntu 18.0.4`\r\n- **Python version tried**: `3.6/3.7`\r\n- **gcc/g++**: `7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1)`\r\n- **TensorFlow installed from (source or binary)**: `source`\r\n- **TensorFlow version (use command below)**: `2.1`\r\ngit clone https://github.com/tensorflow/tensorflow tensorflow  -b r2.1\r\n- **Bazel version (if compiling from source)**: `0.29.1`\r\nBuild label: 0.29.1\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Sep 10 13:44:39 2019 (1568123079)\r\n- **CUDA/cuDNN version**: `V10.1.243/7.6.1`\r\n- **GPU model and memory**: `GeForce GTX 1080 TX 12Gb`\r\n- **Exact command to reproduce**:\r\n\r\n> bazel build --linkopt='-lrt' --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-mavx --copt=-msse4.1 --copt=-msse4.2 --config=noaws --config=nohdfs --config=nonccl --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --config=cuda -k //tensorflow/tools/pip_package:build_pip_package --verbose_failures 2>&1 | tee bazel.log\r\n\r\n\r\n### Describe the problem\r\nFailed to build from source TF2.1 with CUDA (though able to builf TF2.0 without a problem)\r\n\r\n### Source code / logs\r\n\r\nConfig (all default options except enabling CUDA):\r\n\r\n(base) sergey@sergey-Bionic:~/tensorflow$ ./configure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.29.1 installed.\r\nPlease specify the location of python. [Default is /home/sergey/anaconda3/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /home/sergey/anaconda3/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/sergey/anaconda3/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: \r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: \r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\nFound cuDNN 7 in:\r\n    /usr/lib64\r\n    /usr/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: \r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\n\r\n### Example of Bazel error messages:\r\n\r\n> ERROR: /home/sergey/tensorflow/tensorflow/python/BUILD:2640:1: Couldn't build file tensorflow/python/gen_tpu_ops_py_wrappers_cc: Linking of rule '//tensorflow/python:gen_tpu_ops_py_wrappers_cc' failed (Exit 1)\r\n--\r\nbazel-out/host/bin/tensorflow/core/protobuf/tpu/libtpu_embedding_output_layout_proto_cc.lo(tpu_embedding_output_layout.pb.o):(.data.rel.ro._ZTVN10tensorflow3tpu24TPUEmbeddingOutputLayoutE[_ZTVN10tensorflow3tpu24TPUEmbeddingOutputLayoutE]+0x60): undefined reference to `google::protobuf::Message::CheckTypeAndMergeFrom(google::protobuf::MessageLite const&)'\r\nbazel-out/host/bin/tensorflow/core/protobuf/tpu/libtpu_embedding_output_layout_proto_cc.lo(tpu_embedding_output_layout.pb.o):(.data.rel.ro._ZTVN10tensorflow3tpu24TPUEmbeddingOutputLayoutE[_ZTVN10tensorflow3tpu24TPUEmbeddingOutputLayoutE]+0x80): undefined reference to `google::protobuf::MessageLite::SerializeWithCachedSizesToArray(unsigned char*) const'\r\nbazel-out/host/bin/tensorflow/core/protobuf/tpu/libtpu_embedding_output_layout_proto_cc.lo(tpu_embedding_output_layout.pb.o):(.data.rel.ro._ZTVN10tensorflow3tpu24TPUEmbeddingOutputLayoutE[_ZTVN10tensorflow3tpu24TPUEmbeddingOutputLayoutE]+0xb0): undefined reference to `google::protobuf::Message::DiscardUnknownFields()'\r\nbazel-out/host/bin/tensorflow/core/protobuf/tpu/libtpu_embedding_output_layout_proto_cc.lo(tpu_embedding_output_layout.pb.o):(.data.rel.ro._ZTVN10tensorflow3tpu24TPUEmbeddingOutputLayoutE[_ZTVN10tensorflow3tpu24TPUEmbeddingOutputLayoutE]+0xb8): undefined reference to `google::protobuf::Message::SpaceUsedLong() const'\r\ncollect2: error: ld returned 1 exit status\r\nERROR: /home/sergey/tensorflow/tensorflow/python/BUILD:2359:1: Couldn't build file tensorflow/python/gen_checkpoint_ops_py_wrappers_cc: Linking of rule '//tensorflow/python:gen_checkpoint_ops_py_wrappers_cc' failed (Exit 1)\r\n--\r\nbazel-out/host/bin/tensorflow/core/libcheckpoint_ops_op_lib.lo(checkpoint_ops.o): In function `__static_initialization_and_destruction_0(int, int) [clone .constprop.45]':\r\ncheckpoint_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.45+0x19b): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\ncheckpoint_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.45+0x3ff): undefined reference to `tensorflow::OpDefBuilder::SetIsStateful()'\r\ncheckpoint_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.45+0x424): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\ncollect2: error: ld returned 1 exit status\r\nERROR: /home/sergey/tensorflow/tensorflow/python/BUILD:2339:1: Couldn't build file tensorflow/python/gen_audio_ops_py_wrappers_cc: Linking of rule '//tensorflow/python:gen_audio_ops_py_wrappers_cc' failed (Exit 1)\r\n--\r\naudio_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.63+0x140): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\naudio_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.63+0x250): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\naudio_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.63+0x386): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\naudio_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.63+0x562): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\ncollect2: error: ld returned 1 exit status\r\n\r\n### Full bazel error log see attached.\r\n[bazel_error.l og](https://github.com/tensorflow/models/files/3834933/bazel_error.log)\r\n ", "comments": ["same here\r\n\r\nI've never succeeded build since master got 2.0.0 ( before I successfully build 2.0.0 in r2.0 branch )\r\n\r\nfunny thing is, I succeeded building for windows 10 last night ( 2.1.0-rc0)", "Same error here but under python 3.7\r\nTried with bazel 0.27.1 and 0.29.1. Used the r2.1 branch, while the r2.0 branch worked fine for me last time. Did some of you manage to compile or is it a bug?", "It would be nice if someone who succeeded with building TF2.1 shared their config (CUDA and Bazel versions at least) and cast some light if building with CUDA 10.2 is possible.", "@sbushmanov \r\nIt seems we might observe something similar 1-2 weeks ago after default Bazel version upgrade (possibly related issues: https://github.com/tensorflow/tensorflow/issues/34117 https://github.com/tensorflow/tensorflow/issues/34222). About a week ago the issue has disappeared but we changed build environment to CUDA 10.1 and Bazel 1.1.0 (installed automatically via [Bazelisk](https://github.com/bazelbuild/bazelisk) as recommended in the [TensorFlow building instructions](https://www.tensorflow.org/install/source#install_bazel)). We use TensorFlow master branch.\r\nFor TensorFlow building with CUDA 10.2 I can refer to [NGC TensorFlow docker image](https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow) - there're scripts to build TensorFlow inside (see /opt/tensorflow/, with CUDA 10.2).\r\nPS: You can trigger [Mellanox TensorFlow CI](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build#mellanox-tensorflow-ci) to get build log file with settings we use.", "add --noincompatible_do_not_split_linking_cmdline to bazel flags ", "Finally successfully built tf 2.1.0 rc0 with CUDA 10.2, Bazel 0.29.1, with `--noincompatible_do_not_split_linking_cmdline`  to bazel flags (thanks to @powderluv) and\r\nhttps://github.com/tensorflow/tensorflow/issues/34429#issuecomment-557408498\r\n\r\n```\r\nINFO: Elapsed time: 10509.148s, Critical Path: 317.59s\r\nINFO: 25874 processes: 25874 local.\r\nINFO: Build completed successfully, 40666 total actions\r\nINFO: Build completed successfully, 40666 total actions\r\n```", "> Finally successfully built tf 2.1.0 rc0 with CUDA 10.2, Bazel 0.29.1\r\n\r\n@sbushmanov,\r\nClosing this issue as it is resolved. Please feel free to re-open if mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34191\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34191\">No</a>\n"]}, {"number": 34190, "title": "TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorboard.SummaryMetadata got tensorflow.SummaryMetadata.", "body": "Hey, I'm trying to use Hyperparameter tuning with the HParams Dashboard. Trying to follow the guidelines on https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams \r\nalthough I'm using Pytorch instead of a Keras model. \r\n\r\nTo start with I'm only focusing on one hyperparameter (lambda_L1), following the tutorial this is what I've got:   \r\n\r\n```\r\nHP_LAMBDA_L1 = hp.HParam('lambda_L1', hp.IntInterval(0, 200))\r\n\r\n\"\"\"\"Preparations for HP tuning\"\"\"\r\ntrain_writer = tf.summary.create_file_writer(opt.log_dir) \r\nwith train_writer.as_default():\r\n    hp.hparams_config(\r\n        hparams=[HP_LAMBDA_L1],\r\n        metrics=[hp.Metric('accuracy', display_name='Accuracy')],  # TODO\r\n    )\r\n\r\n\"\"\"For each run, log an hparams summary with the hyperparameters and final accuracy:\"\"\"\r\ndef run(run_dir, hparams):\r\n    with tf.summary.create_file_writer(run_dir).as_default():\r\n        hp.hparams(hparams)  # record the values used in this trial\r\n        mse_accuracy = train_model(hparams)\r\n        tf.summary.scalar('accuracy', mse_accuracy, step=1)\r\n\r\nrun_num = 0\r\nfor lambda_L1 in np.linspace(HP_LAMBDA_L1.domain.min_value,\r\n                                 HP_LAMBDA_L1.domain.max_value, 5,):\r\n    hparams = {\r\n        HP_LAMBDA_L1: lambda_L1\r\n    }\r\n    run_name = \"run-%d\" % run_num\r\n    print('--- Starting trial: %s' % run_name)\r\n    print({h.name: hparams[h] for h in hparams})\r\n    run('logs/test_lambdaL1/' + run_name, hparams)\r\n    run_num += 1\r\n```\r\nThe `train_model()  ` is in Pytorch. I'm using `!pip install -q tensorflow==2.0.0-alpha0 ` in Google colab wich hasn't given me any trubles before. \r\n\r\nThe error I've got: \r\n\r\n`Traceback (most recent call last):\r\n  File \"/content/drive/My Drive/Colab Notebooks/pix2pix_HPtuning/train_tb_HPtuning.py\", line 69, in <module>\r\n    metrics=[hp.Metric('accuracy', display_name='Accuracy')],  # TODO\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/plugins/hparams/summary_v2.py\", line 135, in hparams_config\r\n    time_created_secs=time_created_secs,\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/plugins/hparams/summary_v2.py\", line 177, in hparams_config_pb\r\n    plugin_data_pb2.HParamsPluginData(experiment=experiment),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/plugins/hparams/summary_v2.py\", line 250, in _summary_pb\r\n    summary.value.add(tag=tag, metadata=summary_metadata)\r\nTypeError: Parameter to MergeFrom() must be instance of same class: expected tensorboard.SummaryMetadata got tensorflow.SummaryMetadata.`\r\n\r\nDoes anyone know how to solve the issue?\r\n", "comments": ["As an additional note: \r\nI've also tried the following for tensorflow: \r\n```\r\ntry:\r\n  # %tensorflow_version only exists in Colab.\r\n  %tensorflow_version 2.x\r\nexcept Exception:\r\n  pass\r\n```\r\nWhich also gives me errors, but these complain that there's something wrong in `train_model(hparams)` which I know can't be the case since `train_model` works perfectly fine without the HyperParameter tuning. ", "@sbj028 ,\r\nThank you for reporting the issue, can you please share the complete code to reproduce the error reported here? \r\nI also see that there is no issue faced in colab running on 2.0alpha version. Please try using latest  `tf-2.0` or` tf -nightly `version and check if the issue is being faced. Thanks!", "Thank you for the answer. \r\n\r\nI've now tried the following:\r\n`!pip install tensorflow-gpu==2.0`\r\nor\r\n`try:\r\n  %tensorflow_version 2.x\r\nexcept Exception:\r\n  pass`\r\nor\r\n`!pip install tf-nightly`\r\nor\r\n`!pip install tf-nightly-2.0-preview`\r\nor\r\n`!pip install tensorflow-gpu`\r\nwhich all work fine using Google Colab. \r\n\r\nThe following version: `!pip install -q tensorflow==2.0.0-alpha0 ` still gives me\r\n\r\n> TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorboard.SummaryMetadata got tensorflow.SummaryMetadata.\r\n\r\nmentioned above. But since I'm able to continue trying out the hyperparameter tuning using all other versions for tensorflow I will close this issue.\r\n \r\nThe `train_model(hparams)` calls the Pix2pix script `train.py` that can be found at [https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix](url) and passing the hparams that I'm interested in. \r\n\r\nThanks!\r\n\r\n"]}, {"number": 34189, "title": "ValueError: Unknown layer: DenseFeatures for tensorflow2.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Mac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):N\r\n- TensorFlow version (use command below):2.0.0(stable)\r\n- Python version:3.7.3\r\n- Bazel version (if compiling from source):Y/N\r\n- GCC/Compiler version (if compiling from source):Y/N\r\n- CUDA/cuDNN version:N\r\n- GPU model and memory:N\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nloading a saved .h5 model which includes a DenseFeatures Layer fails:\r\nValueError: Unknown layer: DenseFeatures\r\n\r\n**Describe the expected behavior**\r\nmodel is loading, and i can use the pre_trained model for online service\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nimport pandas as pd\r\n\r\n# pip install -q tensorflow==2.0.0-alpha0\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column\r\nfrom tensorflow.keras import layers\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\ndataframe = pd.read_csv(URL)\r\ntrain, test = train_test_split(dataframe, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\n\r\n# A utility method to create a tf.data dataset from a Pandas Dataframe\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('target')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\n\r\nfeature_columns = []\r\n\r\n# numeric cols\r\nfor header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\r\n  feature_columns.append(feature_column.numeric_column(header))\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n\r\nbatch_size = 32\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nmodel = tf.keras.Sequential([\r\n  feature_layer,\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(1, activation='sigmoid')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy')\r\n\r\nmodel.fit(train_ds, \r\n          validation_data=val_ds, \r\n          epochs=5)\r\n\r\n\r\nmodel.save('my_model.h5')\r\nfrom tensorflow import keras\r\nnew_model = keras.models.load_model('my_model.h5')\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n    ValueError: ('We expected a dictionary here. Instead we got: ', <tf.Tensor 'Placeholder:0' shape=(None,) dtype=float32>)\r\n\r\n", "comments": ["I also met the Error   Unknown layer: DenseFeatures for tensorflow2.0 in issue https://github.com/tensorflow/tensorflow/issues/27008, but i have not seen any solutions to this issue ,\r\n", "@muzixizhu, I tried replicating the issue on colab with Tf 2.0 but got a different error. Please take a look at colab gist [here](https://colab.sandbox.google.com/gist/gadagashwini/31c284ef598a07e9d96b734d7e3a81c3/untitled256.ipynb).\r\nThanks!", "change your code for my_input_fn()\n\n\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n\n\nit should be:\nif shuffle:\nds = ds.shuffle(buffer_size=len(dataframe))\nds = ds.batch(batch_size)\n\n\n\n\n\n\nAt 2019-11-13 16:17:54, \"gadagashwini\" <notifications@github.com> wrote:\n\n\n@muzixizhu, I tried replicating the issue on colab with Tf 2.0 but got a different error. Please take a look at colab gist here.\nThanks!\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or unsubscribe.", "sorry for my paste, the original code for df_to_dataset() is\r\nif shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n\r\nif shuffle codition just for ds = ds.shufflue(buffer_size=len(dataframe)) not for the following code", "Issue is replicating on colab with Tensorflow 2.0.\r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/409e82576732d06e907d0042b070dae9/untitled256.ipynb). Thanks!", "The script executes successfully if you save/load the model in ```tf``` format.", "I have tried in colab with TF version 2.2, nightly version(2.3.0-dev20200612) .I am seeing the below error message(`ValueError: You are trying to load a weight file containing 3 layers into a model with 0 layers.`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/666a1221a9d12cc80f58ba38be70cd83/untitled15.ipynb).Thanks!", "I tried on Colab with [TF v2.5](https://colab.research.google.com/gist/sushreebarsa/bb6d741ad7c2e6859d59ff35bccb9e59/untitled15.ipynb#scrollTo=Qbam-MZGLGfH) & TF [2.6.0-dev20210529](https://colab.research.google.com/gist/sushreebarsa/82adca7cac43fd8b8250962ab7fcd3f1/untitled78.ipynb#scrollTo=tkDyLiyv-gep).I faced different error ,please find the gists attached here..Thanks !", "@muzixizhu,\r\n\r\nI modified your code to remove .h5 extension while saving the model and it worked without any errors. Please take a look at the [gist here](https://colab.research.google.com/gist/sanatmpa1/5156de008111bff37c238b87a4fb7e00/34189.ipynb). Thanks ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34189\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34189\">No</a>\n"]}, {"number": 34188, "title": "TF2.0 GPU - Segmentation fault - Object Detection", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: Python 3.7.5\r\n- CUDA/cuDNN version: 10.0.130/7.6.2\r\n- GPU model and memory: GeForce 940MX / 2004MiB\r\n\r\n**Describe the current behavior**\r\nWhen I try to run [this](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb) example, a segmentation fault happens during inference. I just formatted my Ubuntu 18.04 and did a clean install.\r\n\r\n**Describe the expected behavior**\r\nThe images marked with the boxes.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n...\r\n# Run inference\r\n  output_dict = model(input_tensor)\r\n...\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n2019-11-12 04:53:51.925933: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-11-12 04:53:51.935688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-12 04:53:51.935937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.189\r\npciBusID: 0000:01:00.0\r\n2019-11-12 04:53:51.936117: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-12 04:53:51.937181: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-12 04:53:51.938180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-12 04:53:51.938498: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-12 04:53:51.939671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-12 04:53:51.940624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-12 04:53:51.943488: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-12 04:53:51.943615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-12 04:53:51.944225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-12 04:53:51.944738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-12 04:53:53.716082: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-12 04:53:53.741945: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz\r\n2019-11-12 04:53:53.742225: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fe34207b30 executing computations on platform Host. Devices:\r\n2019-11-12 04:53:53.742249: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-12 04:53:53.792897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-12 04:53:53.793392: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fe33fa3880 executing computations on platform CUDA. Devices:\r\n2019-11-12 04:53:53.793424: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce 940MX, Compute Capability 5.0\r\n2019-11-12 04:53:53.793636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-12 04:53:53.794038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.189\r\npciBusID: 0000:01:00.0\r\n2019-11-12 04:53:53.794108: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-12 04:53:53.794141: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-12 04:53:53.794163: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-12 04:53:53.794182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-12 04:53:53.794203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-12 04:53:53.794224: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-12 04:53:53.794245: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-12 04:53:53.794346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-12 04:53:53.794887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-12 04:53:53.795348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-12 04:53:53.795437: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-12 04:53:53.796297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-12 04:53:53.796327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-11-12 04:53:53.796337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-11-12 04:53:53.796644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-12 04:53:53.796975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-12 04:53:53.797255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1306 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2019-11-12 04:54:11.319157: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-12 04:54:12.995202: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-11-12 04:54:13.169311: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-11-12 04:54:13.328776: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.15GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-11-12 04:54:13.366495: W tensorflow/core/common_runtime/bfc_allocator.cc:305] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\r\n2019-11-12 04:54:13.786988: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-11-12 04:54:13.848769: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.22GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\nSegmentation fault (core dumped)\r\n```", "comments": ["@Icaro-Lima, Its working fine on Colab. Can you check once and let us know. Thanks!", "I tested a few times and without success, what worked for me was [that](https://github.com/tensorflow/tensorflow/issues/32261#issuecomment-531411532).", "@Icaro-Lima, Glad it worked. Are you happy to close if no issue persists. Thanks!", "> I tested a few times and without success, what worked for me was [that](https://github.com/tensorflow/tensorflow/issues/32261#issuecomment-531411532).\r\n\r\nIcaro, tenho um notebook com a msm GPU e to com o exatmo msm problema com a API de Object Detection, s\u00f3 rodar esse c\u00f3digo ja fez dar certo?", "> > I tested a few times and without success, what worked for me was [that](https://github.com/tensorflow/tensorflow/issues/32261#issuecomment-531411532).\r\n> \r\n> Icaro, tenho um notebook com a msm GPU e to com o exatmo msm problema com a API de Object Detection, s\u00f3 rodar esse c\u00f3digo ja fez dar certo?\r\n\r\nOl\u00e1 @francisconog, faz muito tempo que eu mexi com isso, ent\u00e3o n\u00e3o lembro o que fiz para dar certo. Se tem curiosidade de olhar o c\u00f3digo fonte do projeto, eles est\u00e3o [aqui](https://github.com/Coofy/sight-bot-train) e [aqui](https://github.com/Coofy/sight-bot).\r\n\r\nOutra coisa, em vez de fazer um setup local, considere uma das seguintes op\u00e7\u00f5es:\r\n1. Caso o que voc\u00ea quer fazer seja simples: rodar em um notebook do google colab, [aqui](https://colab.research.google.com/).\r\n2. Caso exiga um poder de processamento maior e voc\u00ea n\u00e3o queira pagar por isso (extremamente recomendado!): Rodar dentro de uma imagem docker. Tem um tutorial [aqui](https://www.tensorflow.org/install/docker), e como o pr\u00f3prio tutorial diz: \r\n> Docker is the easiest way to enable TensorFlow GPU support on Linux since only the NVIDIA\u00ae GPU driver is required on the host machine.", "Hi I have a very similar problem except I'm using 930 MX graphics card and ** tensorflow gpu 2.0.1 **, mine does not give segmentation error however shows something like\r\n`2020-06-04 19:15:45.661495: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n`\r\nI upgraded tensorflow from 2.0.0 to 2.0.2 but to no avail. What seems to be the problem here?", "I'm pretty sure that I also had these warnings even after resolving the segmentation fault.", "> I'm pretty sure that I also had these warnings even after resolving the segmentation fault.\r\n\r\n@Icaro-Lima , were you finally able to get the output? I get these warnings but it doesnt process after\r\n`output_dict = model(input_tensor)`"]}, {"number": 34187, "title": "SparseCategoricalAccuracy() & InvalidArgumentError", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Win10**\r\n- TensorFlow version (use command below): **2.0.0**\r\n- Python version: **3.7.5**\r\n\r\nWhen I try to run the following code, an error occurs,\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nclass MNISTLoader():\r\n    def __init__(self):\r\n        mnist = tf.keras.datasets.mnist\r\n        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\r\n        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)\r\n        self.test_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)\r\n        self.train_label = self.train_label.astype(np.int32)\r\n        self.test_label = self.test_label.astype(np.int32)\r\n        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\r\n    \r\n    def get_batch(self, batch_size):\r\n        index = np.random.randint(0, np.shape(self.train_data)[0], batch_size)\r\n        return self.train_data[index, :], self.train_label[index]\r\n\r\nclass MLP(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.flatten = tf.keras.layers.Flatten()\r\n        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\r\n        self.dense2 = tf.keras.layers.Dense(units=10)\r\n        \r\n    def call(self, inputs):\r\n        x = self.flatten(inputs)\r\n        x = self.dense1(x)\r\n        x = self.dense2(x)\r\n        output = tf.nn.softmax(x)\r\n        return output\r\n\r\nnum_epochs = 5\r\nbatch_size = 50\r\nlearning_rate = 0.001\r\nmodel = MLP()\r\ndata_loader = MNISTLoader()\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\r\n\r\nnum_batches = int(data_loader.num_train_data // batch_size * num_epochs)\r\nfor batch_index in range(num_batches):\r\n    X, y = data_loader.get_batch(batch_size)\r\n    with tf.GradientTape() as tape:\r\n        y_pred = model(X)\r\n        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\r\n        loss = tf.reduce_mean(loss)\r\n        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\r\n    grads = tape.gradient(loss, model.variables)\r\n    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\r\n\r\nsparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\r\nnum_batches = int(data_loader.num_test_data // batch_size)\r\nfor batch_index in range(num_batches):\r\n    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\r\n    y_pred = model.predict(data_loader.test_data[start_index: end_index])\r\n    y_true = data_loader.test_label[start_index: end_index]\r\n    #y_true = y_true.reshape(-1,1)\r\n    sparse_categorical_accuracy.update_state(y_true=y_true, y_pred=y_pred)\r\n    if batch_index == 1:\r\n        print(\"y_pred\", y_pred.shape)\r\n        print(y_pred)\r\n        print(\"y_true\", y_true.shape)\r\n        print(y_true)\r\nprint(\"test accuracy: %f\" % sparse_categorical_accuracy.result())\r\n\r\n```\r\n**Describe the current behavior**  \r\n\r\nThe information is as follows:  \r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-15-e321ee460d57> in <module>\r\n      6     y_true = data_loader.test_label[start_index: end_index]\r\n      7     #y_true = y_true.reshape(-1,1)\r\n----> 8     sparse_categorical_accuracy.update_state(y_true=y_true, y_pred=y_pred)\r\n      9 \r\n     10     if batch_index == 1:\r\n\r\nD:\\software\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\metrics_utils.py in decorated(metric_obj, *args, **kwargs)\r\n     73 \r\n     74     with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):\r\n---> 75       update_op = update_state_fn(*args, **kwargs)\r\n     76     if update_op is not None:  # update_op will be None in eager execution.\r\n     77       metric_obj.add_update(update_op)\r\n\r\nD:\\software\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in update_state(self, y_true, y_pred, sample_weight)\r\n    579         y_pred, y_true)\r\n    580 \r\n--> 581     matches = self._fn(y_true, y_pred, **self._fn_kwargs)\r\n    582     return super(MeanMetricWrapper, self).update_state(\r\n    583         matches, sample_weight=sample_weight)\r\n\r\nD:\\software\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in sparse_categorical_accuracy(y_true, y_pred)\r\n   2784     y_pred = math_ops.cast(y_pred, K.dtype(y_true))\r\n   2785 \r\n-> 2786   return math_ops.cast(math_ops.equal(y_true, y_pred), K.floatx())\r\n   2787 \r\n   2788 \r\n\r\nD:\\software\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py in wrapper(*args, **kwargs)\r\n    178     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    179     try:\r\n--> 180       return target(*args, **kwargs)\r\n    181     except (TypeError, ValueError):\r\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\nD:\\software\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py in equal(x, y, name)\r\n   1304     A `Tensor` of type bool with the same size as that of x or y.\r\n   1305   \"\"\"\r\n-> 1306   return gen_math_ops.equal(x, y, name=name)\r\n   1307 \r\n   1308 \r\n\r\nD:\\software\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py in equal(x, y, incompatible_shape_error, name)\r\n   3617       else:\r\n   3618         message = e.message\r\n-> 3619       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n   3620   # Add nodes to the TensorFlow graph.\r\n   3621   if incompatible_shape_error is None:\r\n\r\nD:\\software\\Anaconda3\\envs\\tensorflow2.0\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Incompatible shapes: [0] vs. [50] [Op:Equal]\r\nI don't know why this happens. I have checked the API and tried to reshape the `y_true`, it doesn't work either.\r\n", "comments": ["Issue is replicating on colab with Tensorflow 2.0.0.\r\nPlease take a look at [gist](https://colab.sandbox.google.com/gist/gadagashwini/3f4e3f19e4399bc568213932499af2c0/untitled253.ipynb). Thanks!", "If you reduce the ```batch_size = 1``` the script executes successfully. Tested with tf-nightly version ```2.1.0-dev20191115```", "With the current batch size the shape of y_pred and y_true are : y_pred (50, 10), y_true (50,)\r\nExcept for the last batch where the shape is: y_pred (50, 10) y_true (0,)\r\n\r\nThis is the reason for the error. Fixing the input data will fix the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34187\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34187\">No</a>\n"]}, {"number": 34186, "title": "How to use golang to implement python's tf.train.Example().serializetostring.", "body": "I'm trying to  using the python training model, deploy services with golang. However, the following problems arise when using golang for prediction.\r\n\r\npredict with python:\r\n```\r\n    def predict_np(self):\r\n        predict_fn = self.get_predict_fn(self.np_path)\r\n        inputs = np.array([[6.4, 3.2, 4.5, 1.5], [6.4, 3.2, 4.5, 1.5]])\r\n        for item in inputs:\r\n            examples = []\r\n            feature = {\"your_input\": tf.train.Feature(float_list=tf.train.FloatList(value=item))}\r\n            example = tf.train.Example(features=tf.train.Features(feature=feature))\r\n            examples.append(example.SerializeToString())\r\n            print(type(example.SerializeToString()))\r\n            predictions = predict_fn({\"inputs\": examples})\r\n            print(predictions)\r\n```\r\nit works ok.\r\n\r\npredict with golang\r\n```\r\npackage main\r\n\r\nimport (\r\n\t\"fmt\"\r\n\ttg \"github.com/galeone/tfgo\"\r\n\t\"github.com/gogo/protobuf/proto\"\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n\t\"github.com/tensorflow/tensorflow/tensorflow/go/core/example\"\r\n\t\"loadEstimator/train\"\r\n)\r\n\r\nfunc main() {\r\n\tdata := [][]float32{{6.4, 3.2, 4.5, 1.5}, {6.4, 3.2, 4.5, 1.5}}\r\n\tcolumnsName := []string{\"a\", \"b\", \"c\", \"d\"}\r\n\tfor _, item := range data {\r\n\t\t// npData:numpy data like in python {\"inputs\":[6.4,3.2,4.5,1.5]}\r\n\t\tnpData := make(map[string][]float32)\r\n\t\tnpData[\"your_input\"] = item\r\n\t\tpredictNP(npData)\r\n\t}\r\n\r\n}\r\n\r\nfunc loadModeSavedPB(path string) (model *tg.Model) {\r\n\tmodel = tg.LoadModel(path, []string{\"serve\"}, nil)\r\n\treturn\r\n}\r\n\r\nfunc predictNP(data map[string][]float32) {\r\n\tpdModePath := \"./static/1\"\r\n\tmodel := loadModeSavedPB(pdModePath)\r\n\tsequence := sequenceNP(data)\r\n\tfmt.Println(sequence)\r\n\tfakeInput, _ := tf.NewTensor([]string{sequence})\r\n\tresults := model.Exec([]tf.Output{\r\n\t\tmodel.Op(\"dnn/head/Tile\", 0),\r\n\t}, map[tf.Output]*tf.Tensor{\r\n\t\tmodel.Op(\"input_example_tensor\", 0): fakeInput,\r\n\t})\r\n\tpredictions := results[0].Value().([][]float32)\r\n\tfmt.Println(predictions)\r\n}\r\n\r\nfunc sequenceNP(featureInfo map[string][]float32) (seq string) {\r\n\tfeature := make(map[string]*example.Feature)\r\n\tfor k, v := range featureInfo {\r\n\t\tvalFormat := train.Float32ToFeature(v)\r\n\t\tfeature[k] = valFormat\r\n\t}\r\n\tFeatures := example.Features{Feature: feature}\r\n\tmyExample := example.Example{Features: &Features}\r\n\tseq = proto.MarshalTextString(&myExample)\r\n\treturn\r\n}\r\n\r\n```\r\n\r\n\r\n```\r\npackage train\r\n\r\nimport \"github.com/tensorflow/tensorflow/tensorflow/go/core/example\"\r\nfunc Float32ToFeature(value []float32) (exampleFeature *example.Feature) {\r\n\tfloatList := example.FloatList{Value: value}\r\n\tfeatureFloatList := example.Feature_FloatList{FloatList: &floatList}\r\n\texampleFeature = &example.Feature{Kind: &featureFloatList}\r\n\treturn\r\n}\r\n```\r\n\r\nThere is an error\r\n```\r\n2019-11-12 11:42:05.021196: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Could not parse example input, value: 'features: <\r\n  feature: <\r\n    key: \"your_input\"\r\n    value: <\r\n      float_list: <\r\n        value: 6.4\r\n        value: 3.2\r\n        value: 4.5\r\n        value: 1.5\r\n      >\r\n    >\r\n  >\r\n>\r\n'\r\npanic: Could not parse example input, value: 'features: <\r\n  feature: <\r\n    key: \"your_input\"\r\n    value: <\r\n      float_list: <\r\n        value: 6.4\r\n        value: 3.2\r\n        value: 4.5\r\n        value: 1.5\r\n      >\r\n    >\r\n  >\r\n>\r\n'\r\n\t [[{{node ParseExample/ParseExample}}]]\r\n```\r\n\r\ni'm using `proto.MarshalTextString(&myExample)`  in golang,and using \r\n`\r\nexample = tf.train.Example(features=tf.train.Features(feature=feature))\r\nexamples.append(example.SerializeToString())\r\n`\r\nin python.\r\nI am not sure whether serialization in these two languages is equivalent, and if not, how can I implement `SerializeToString` in golang? pls\r\n", "comments": ["i'm sorry,in fact,i need use `seq,err = proto.Marshal(&myExample)`\r\ni got []byte in golang is \r\n`[10 36 10 34 10 10 121 111 117 114 95 105 110 112 117 116 18 20 18 18 10 16 205 204 204 64 205 204 76 64 0 0 144 64 0 0 192 63]`\r\nbut got \r\n`b'\\n$\\n\"\\n\\nyour_input\\x12\\x14\\x12\\x12\\n\\x10\\xcd\\xcc\\xcc@\\xcd\\xccL@\\x00\\x00\\x90@\\x00\\x00\\xc0?'` in python\r\nthen golang panic\r\n```\r\n2019-11-12 15:50:31.436302: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: ./static/1\r\n2019-11-12 15:50:31.440917: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2019-11-12 15:50:31.442798: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\n2019-11-12 15:50:31.445938: I tensorflow/cc/saved_model/loader.cc:182] Restoring SavedModel bundle.\r\n2019-11-12 15:50:31.461115: I tensorflow/cc/saved_model/loader.cc:132] Running initialization op on SavedModel bundle.\r\n2019-11-12 15:50:31.465338: I tensorflow/cc/saved_model/loader.cc:285] SavedModel load for tags { serve }; Status: success. Took 29047 microseconds.\r\npanic: Expects arg[0] to be string but uint8 is provided\r\n\r\ngoroutine 1 [running]:\r\ngithub.com/galeone/tfgo.(*Model).Exec(0xc00000e028, 0xc0000b1c50, 0x1, 0x1, 0xc0000b1cb8, 0x0, 0x2, 0x400cd3f)\r\n\t/Users/wqq/go/src/github.com/galeone/tfgo/model.go:74 +0xd1\r\nmain.predictNP(0xc00008c6f0)\r\n\t/Users/wqq/go/src/loadEstimator/main.go:46 +0x34a\r\nmain.main()\r\n\t/Users/wqq/go/src/loadEstimator/main.go:19 +0x27f\r\n```\r\nSo I think byte is different in golang and python", "i sloved", "@atlantiswqq  how does sloved it ,i got the same error ! thank you very much", "@atlantiswqq ", "> @atlantiswqq\r\n\r\nhttps://github.com/galeone/tfgo#train-with-tfestimator-serve-in-go, tfgo might help you.", "ping @atlantiswqq id love to know as well how you fixed it", "In fact, I have added this serialization operation into [tfgo](https://github.com/galeone/tfgo) In the source code of TensorFlow, I saw several Protobuf files, and I compiled them into the go version. I also wrote some methods to help you serialize the data in the form of array and dictionary.\r\n@roffe @gao8954 ", "@atlantiswqq do you have any writeup or similar on how to reproduce & build a image trainer in Go?", "> @atlantiswqq do you have any writeup or similar on how to reproduce & build a image trainer in Go?\r\n\r\nI don't recommend you use go for training, I just use go to train some models of natural language"]}, {"number": 34185, "title": "TF-TRT optimized graph get wrong results when using batch size <=7", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.2\r\n- GPU model and memory:Tesla T4 with 15001MiB memory\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen using TF-TRT optimized graph to do inference, it only works as expected when batch size >=7(I tested batch size from 1-16). And the result is completely wrong when batch size <7\r\n**Describe the expected behavior**\r\nSmall batch size should also give me the correct result since my original un-optimized graph can handle small batch size.\r\n**Code to reproduce the issue**\r\nSetup according to  https://github.com/tensorflow/tensorrt/tree/master/tftrt/examples/object_detection:\r\n```\r\n# in path/to/tftrt/examples/object_detection/\r\nimport tensorflow as tf\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nimport cv2\r\nfrom collections import namedtuple\r\nfrom PIL import Image\r\nimport numpy as np\r\nimport time\r\nimport json\r\nimport subprocess\r\nimport os\r\nimport glob\r\nfrom tftrt.examples.object_detection.graph_utils import force_nms_cpu as f_force_nms_cpu\r\nfrom tftrt.examples.object_detection.graph_utils import replace_relu6 as f_replace_relu6\r\nfrom tftrt.examples.object_detection.graph_utils import remove_assert as f_remove_assert\r\n\r\ndef optimize_model(frozen_graph,\r\n                   use_trt=True,\r\n                   force_nms_cpu=True,\r\n                   replace_relu6=True,\r\n                   remove_assert=True,\r\n                   precision_mode='FP32',\r\n                   minimum_segment_size=2,\r\n                   max_workspace_size_bytes=1 << 32,\r\n                   maximum_cached_engines=100,\r\n                   calib_images_dir=None,\r\n                   num_calib_images=None,\r\n                   calib_batch_size=1,\r\n                   calib_image_shape=None,\r\n                   output_path=None):\r\n    #same function copied from  path/to/tftrt/examples/object_detection/object_detection.py#L328\r\n    pass\r\n# optimized a customed frozen graph\r\nfrozen_graph_path = 'path/to/frozen_graph.pb'\r\nfrozen_graph = tf.GraphDef()\r\nwith open(frozen_graph_path, 'rb') as f:\r\n    frozen_graph.ParseFromString(f.read())\r\nfrozen_graph = optimize_model(\r\n    frozen_graph,\r\n    force_nms_cpu=False,\r\n    replace_relu6=True,\r\n    remove_assert=True,\r\n    use_trt=True,\r\n    precision_mode=\"FP16\",\r\n    max_workspace_size_bytes=17179869184,\r\n    output_path='./trt_frozen_graph.pb'\r\n)\r\n\r\n# run inference\r\nfrozen_graph_path = './trt_frozen_graph.pb'\r\nfrozen_graph = tf.GraphDef()\r\nwith open(frozen_graph_path, 'rb') as f:\r\n    frozen_graph.ParseFromString(f.read())\r\n\r\n# images = [bs, h, w, 3]\r\nINPUT_NAME = 'input'\r\nBOXES_NAME = 'output/boxes'\r\nCLASSES_NAME = 'output/labels'\r\nSCORES_NAME = 'output/scores'\r\nNUM_DETECTIONS_NAME = 'output/num_detections'\r\nwith tf.Graph().as_default() as tf_graph:\r\n    with tf.Session(config=tf_config) as tf_sess:\r\n        tf.import_graph_def(frozen_graph, name='')\r\n        tf_input = tf_graph.get_tensor_by_name(INPUT_NAME + ':0')\r\n        tf_boxes = tf_graph.get_tensor_by_name(BOXES_NAME + ':0')\r\n        tf_classes = tf_graph.get_tensor_by_name(CLASSES_NAME + ':0')\r\n        tf_scores = tf_graph.get_tensor_by_name(SCORES_NAME + ':0')\r\n        tf_num_detections = tf_graph.get_tensor_by_name(\r\n            NUM_DETECTIONS_NAME + ':0')\r\n        boxes, classes, scores, num_detections = tf_sess.run(\r\n                    [tf_boxes, tf_classes, tf_scores, tf_num_detections],\r\n                    feed_dict={tf_input: batch_images})\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nMy frozen graph can be download at https://drive.google.com/open?id=1s_laMhRw8GC6I9brQjn6apKTfhWe734J\r\n```\r\nWARNING:tensorflow:From /notebooks/tensorrt/tftrt/examples/object_detection/graph_utils.py:31: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\nWARNING:tensorflow:From /notebooks/tensorrt/tftrt/examples/object_detection/graph_utils.py:31: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nINFO:tensorflow:Linked TensorRT version: (6, 0, 1)\r\nINFO:tensorflow:Loaded TensorRT version: (6, 0, 1)\r\nINFO:tensorflow:Running against TensorRT version 6.0.1\r\ngraph_size(MB)(native_tf): 54.5\r\ngraph_size(MB)(trt): 106.0\r\nnum_nodes(native_tf): 10992\r\nnum_nodes(tftrt_total): 10156\r\nnum_nodes(trt_only): 290\r\ntime(s) (trt_conversion): 15.8726\r\n```\r\n", "comments": ["It seems that `tf.image.combined_non_max_suppression()` is problematic after optimization, since if i change to batch size 1 and use `tf.image.non_max_suppression()` , the result is what I expected. But this still not ideal since I do want input batch size >1. Any ideas?", "@ArtificialNotImbecile Can you please try `TF1.15` and let us know whether it resolved or not. Thanks!", "> @ArtificialNotImbecile Can you please try `TF1.15` and let us know whether it resolved or not. Thanks!\r\n\r\nI can not optimize the graph in `tensorflow=1.15.0`, it breaks my jupyter notebook every time I call the `trt.TrtGraphConverter()` function. I'll report the result If I can get the optimization work in `TF1.15`.", "@pooyadavoodi any ideas?", "I know that SSD is supported by combined_nms, and faster/mask-rcnn are not supported (some arguments used in object detection API are not supported).\r\nI am not sure if combined_nms works with yolo.\r\nIf combined_nms is giving bad results for yolo, while other types of nms such as batched_nms give correct output, I suggest to not use combined_nms.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34185\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34185\">No</a>\n"]}, {"number": 34184, "title": "How to apply feature extraction in tensorflow (not use tf.keras)", "body": "I want to apply feature extraction with pre-trained model (VGG16) by removing the last layer and replacing other FC layer (less classes).\r\nI don't want to use tf.keras. How to deal it?\r\nHelp me!\r\nThanks a lot!", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 34183, "title": "How to add custom ops to Tensorflow Lite Swift/ObjC ? ", "body": "I can't find any straight forwards docs on how to add a custom op to tensorflow lite, swift/objc. I see the docs for adding to the c++ lib, but it seems unclear how to add them to the swift pods? \r\n\r\nDoes anyone have any pointers? ", "comments": ["@scm-ns, Please take a look at this [link](https://www.tensorflow.org/lite/guide/ops_custom). Thanks!", "@gadagashwini It tells me how to add it to C++. Not how to run it within an iOS app. \r\n\r\nIf you look at this line `builtins.AddCustom(\"Sin\", Register_SIN());`. I am not sure how to do this within an iOS app. ", "Hi, \r\n\r\nI trained a model and converted it to TFLite with select ops model with TF2.0. In Android it works well, after including tensorflow-lite-select-tf-ops.aar from repository, but as I understand, for iOS the lite-with-select-ops CocoaPod is not yet published and we should compile it ourselves. The problem is that in TF 2.0 [documentation ](https://www.tensorflow.org/lite/guide/ops_select) it suggests to run `tensorflow/contrib/makefile/build_all_ios_with_tflite.sh`, which was removed in the new version of Tensorflow. Any suggestions how to compile it? When could we expect CocoaPod for lite-with-select-ops?\r\n\r\nThanks!", "@scm-ns Could you please have a look at the [link1](https://www.tensorflow.org/lite/guide/ops_select?hl=hu#using_cocoapods),[link2](https://www.tensorflow.org/lite/guide/build_ios), [link3](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/c_api_experimental.h#L68) and let us know if it helps ?Thanks!", "I believe this could be done by using the experimental C API, both in Swift and ObjC.\r\nPlease refer to the header documentation here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/c_api_experimental.h#L68", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34183\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34183\">No</a>\n"]}, {"number": 34182, "title": "Can't load optimizer weights after adding layer without parameters", "body": "**MODEL A**:\r\n```python\r\nipt = Input(batch_shape=(32, 240, 4))\r\nx1  = Conv1D(16, 20,  strides=200, padding='same')(ipt)\r\nx1  = BatchNormalization()(x1)\r\nx2  = Conv1D(16, 200, strides=120, padding='same')(ipt)\r\nx2  = BatchNormalization()(x2) # ...\r\n```\r\n**MODEL B**:\r\n```python\r\nipt = Input(batch_shape=(32, 250, 4))\r\nx1  = Conv1D(16, 20,  strides=200)(ipt)\r\nx1  = BatchNormalization()(x1)\r\nx2  = Conv1D(16, 200, strides=120)(ipt)\r\nx2  = BatchNormalization()(x2) # ...\r\n```\r\n<hr>\r\nThe two have identical weight shapes - however, A's optimizer `weights` _cannot_ be loaded onto B, as B has a different build order (images & code below). \r\n\r\nThis is a tiny snippet of a much larger model which needs its `timesteps` parameter changed every X epochs, and `ZeroPadding1D` appears to _change layer build order_ whenever it's used; this doesn't affect _model_ weights, as they're mapped via a dictionary - whereas optimizer weights are mapped sequentially, list-to-list.\r\n\r\nReproducible in both TF1 & TF2, and w/ `keras` & `tf.keras` imports. What's the problem, and how to fix? [Relevant SO](https://stackoverflow.com/questions/58811292/why-does-zeropadding-change-optimizer-weights-order)\r\n\r\n\r\n<hr>\r\n\r\n**Environment**: Win-10 OS, CUDA 10.0.130, cuDNN 7.6.0, Python 3.7.4, GTX 1070\r\n\r\n**Observations**:\r\n\r\n - Swaps any other layer, not just `BatchNormalization` - and any _number of layers_ before `concatenate`; optimizer weights end up being simply swapped in `.get_weights()`\r\n - Can change `strides` instead of `batch_shape[1]`\r\n - Can use `MaxPooling1D` w/ `strides > 1`\r\n - `padding='valid'` leads to `ZeroPadding1D`, but it _doesn't_ change build order (don't know why)\r\n\r\n\r\n<hr>\r\n\r\n**`model_A.summary()`**:\r\n\r\n```python\r\nLayer (type)                    Output Shape         Param #     Connected to     \r\n==================================================================================\r\ninput_1 (InputLayer)            [(32, 240, 4)]       0                            \r\n__________________________________________________________________________________\r\nconv1d (Conv1D)                 (32, 2, 16)          1296        input_1[0][0]    \r\n__________________________________________________________________________________\r\nconv1d_1 (Conv1D)               (32, 2, 16)          12816       input_1[0][0]    \r\n__________________________________________________________________________________\r\nbn_1 (BatchNormalization)       (32, 2, 16)          64          conv1d[0][0]     \r\n__________________________________________________________________________________\r\nbn_2 (BatchNormalization)       (32, 2, 16)          64          conv1d_1[0][0]   \r\n__________________________________________________________________________________\r\nconcatenate (Concatenate)       (32, 2, 32)          0           bn_1[0][0]       \r\n                                                                 bn_2[0][0]       \r\n__________________________________________________________________________________\r\ngap_0 (GlobalAveragePooling1D)  (32, 32)             0           concatenate[0][0]\r\n__________________________________________________________________________________\r\ndense (Dense)                   (32, 1)              33          gap_0[0][0]      \r\n```\r\n\r\n**`model_B.summary()`** _(note the swapped layers)_\r\n\r\n```python\r\ninput_2 (InputLayer)            [(32, 250, 4)]       0                               \r\n_____________________________________________________________________________________\r\nconv1d_2 (Conv1D)               (32, 2, 16)          1296        input_2[0][0]       \r\n_____________________________________________________________________________________\r\nbn_1 (BatchNormalization)       (32, 2, 16)          64          conv1d_2[0][0]      \r\n_____________________________________________________________________________________\r\nconv1d_3 (Conv1D)               (32, 3, 16)          12816       input_2[0][0]       \r\n_____________________________________________________________________________________\r\nzero_padding1d (ZeroPadding1D)  (32, 3, 16)          0           bn_1[0][0]          \r\n_____________________________________________________________________________________\r\nbn_2 (BatchNormalization)       (32, 3, 16)          64          conv1d_3[0][0]      \r\n_____________________________________________________________________________________\r\nconcatenate_1 (Concatenate)     (32, 3, 32)          0           zero_padding1d[0][0]\r\n                                                                 bn_2[0][0]          \r\n_____________________________________________________________________________________\r\ngap_0 (GlobalAveragePooling1D)  (32, 32)             0           concatenate_1[0][0] \r\n_____________________________________________________________________________________\r\ndense_1 (Dense)                 (32, 1)              33          gap_0[0][0]  \r\n```\r\n\r\n<hr>\r\n\r\n**Minimally reproducible code**:\r\n\r\n```python\r\n# also works with `from keras`\r\nfrom tensorflow.keras.layers import Input, Conv1D, ZeroPadding1D, concatenate\r\nfrom tensorflow.keras.layers import BatchNormalization, Dense, GlobalAveragePooling1D\r\nfrom tensorflow.keras.models import Model\r\nimport numpy as np\r\n\r\ndef make_model(batch_shape):\r\n    ipt = Input(batch_shape=batch_shape)\r\n\r\n    x1  = Conv1D(16, 20,  strides=200, padding='same')(ipt)\r\n    x1  = BatchNormalization()(x1)\r\n    x2  = Conv1D(16, 200, strides=120, padding='same')(ipt)\r\n    x2  = BatchNormalization()(x2)\r\n\r\n    x1, x2 = zero_pad(x1, x2)\r\n    preout = concatenate([x1, x2])\r\n    preout = GlobalAveragePooling1D()(preout)\r\n    out    = Dense(1)(preout)\r\n\r\n    model  = Model(ipt, out)\r\n    model.compile('adam', 'mse')\r\n    return model \r\n\r\ndef zero_pad(x1, x2):\r\n    diff = int(x2.shape[1]) - int(x1.shape[1])\r\n    if   diff > 0:\r\n        x1 = ZeroPadding1D((diff, 0))(x1)\r\n    elif diff < 0:\r\n        x2 = ZeroPadding1D((abs(diff), 0))(x2)\r\n    return x1, x2\r\n    \r\ndef make_data(batch_shape):\r\n    return (np.random.randn(*batch_shape), \r\n            np.random.randint(0, 2, (batch_shape[0], 1)))\r\n\r\nbatch_shape_A = (32, 240, 4)\r\nbatch_shape_B = (32, 250, 4)\r\nbatch_shape_C = (32, 240, 4)\r\nmodel_A  = make_model(batch_shape_A)\r\nmodel_B  = make_model(batch_shape_B)\r\nmodel_C  = make_model(batch_shape_C) # 'control group'\r\nx_A, y_A = make_data(batch_shape_A)\r\nx_B, y_B = make_data(batch_shape_B)\r\nx_C, y_C = make_data(batch_shape_C)\r\n\r\nmodel_A.train_on_batch(x_A, y_A)\r\nmodel_B.train_on_batch(x_B, y_B)\r\nmodel_C.train_on_batch(x_C, y_C)\r\n\r\noptimizer_weights_A = model_A.optimizer.get_weights()\r\n\r\nmodel_C.optimizer.set_weights(optimizer_weights_A)\r\nprint(\"model_C optimizer weights set successfully\")\r\n\r\nmodel_B.optimizer.set_weights(optimizer_weights_A)\r\nprint(\"model_B optimizer weights set successfully\") # will not print\r\n```\r\n**Output**:\r\n```python\r\nmodel_C optimizer weights set successfully\r\n\r\nValueError: Optimizer weight shape (16,) not compatible with provided \r\nweight shape (200, 4, 16)\r\n```", "comments": ["Found a workaround, and a form of explanation; it isn't about `ZeroPadding1D`, but about there being an additional layer in one 'branch' and not the other - as revealed by `plot_model()`; see below. \r\n\r\nKeras appears to build layers via _vertical traversal_ - note that the numbered layer graphs match exactly w/ `.summary()` ordering. Order change could still have occurred toward the end of the 'branch' - I suppose the reasoning is, the two branches' layer nodes should be at the same depth before merging to a common layer. _However_, this isn't the full story - see disclaimer at bottom.\r\n\r\n**Workaround**: insert a 'pseudolayer' to equalize # of layers in each branch; I'll stick with the z-padding:\r\n\r\n```python\r\ndef zero_pad(x1, x2):\r\n    diff = int(x2.shape[1]) - int(x1.shape[1])\r\n    if   diff > 0:\r\n        x1 = ZeroPadding1D((diff, 0))(x1)\r\n        x2 = ZeroPadding1D((0, 0))(x2)\r\n    elif diff < 0:\r\n        x2 = ZeroPadding1D((abs(diff), 0))(x2)\r\n        x1 = ZeroPadding1D((0, 0))(x1)\r\n    return x1, x2\r\n```\r\nRunning the code in the question:\r\n\r\n```python\r\nmodel_C optimizer weights set successfully\r\nmodel_B optimizer weights set successfully  # SUCCESS\r\n```\r\n<hr>\r\n\r\n**Model graphs**: via `from tensorflow.keras.utils import plot_model; plot_model(model_A) ...`\r\n\r\n[![enter image description here][1]][1]\r\n\r\n<hr>\r\n\r\n**Explanation disclaimer**: I haven't confirmed it in  exact lines of source code, and `.summary()` doesn't always agree w/ `plot_model()`; for example, using `padding='valid'`, we get `model_B` graph above for both `model_A` and `model_B`, yet summary shows `model_A`'s build order. Also, `padding='valid'` works without the fix because both models end up using `ZeroPadding1D`, so the layer structure is (seemingly) identical.\r\n\r\n  [1]: https://i.stack.imgur.com/DU2tZ.png", "The functional API builds the layers from a DFS algorithm, and treats output as depth=0 and walks backward. When a certain upstream layer is connected to two downstream layer, its depth is determined by the max depth of them, + 1, i.e., if layer_C's outputs are connected to both layer_A and layer_B, then depth(layer_C) = max(depth(layer_A), depth(layer_B)) + 1.\r\nSo in this particular case, if we do a layers_by_depth dict, you can see for model_A:\r\n```python\r\ndefaultdict(list,\r\n            {0: [<tensorflow.python.keras.layers.core.Dense>],\r\n             1: [<tensorflow.python.keras.layers.pooling.GlobalAveragePooling1D>],\r\n             2: [<tensorflow.python.keras.layers.merge.Concatenate>],\r\n             3: [<tensorflow.python.keras.layers.normalization_v2.BatchNormalization>,\r\n              <tensorflow.python.keras.layers.normalization_v2.BatchNormalization>],\r\n             4: [<tensorflow.python.keras.layers.convolutional.Conv1D>,\r\n              <tensorflow.python.keras.layers.convolutional.Conv1D>],\r\n             5: [<tensorflow.python.keras.engine.input_layer.InputLayer>]})\r\n```\r\nand for model_B:\r\n```python\r\ndefaultdict(list,\r\n            {0: [<tensorflow.python.keras.layers.core.Dense>],\r\n             1: [<tensorflow.python.keras.layers.pooling.GlobalAveragePooling1D>],\r\n             2: [<tensorflow.python.keras.layers.merge.Concatenate>],\r\n             3: [<tensorflow.python.keras.layers.convolutional.ZeroPadding1D>,\r\n              <tensorflow.python.keras.layers.normalization_v2.BatchNormalization>],\r\n             4: [<tensorflow.python.keras.layers.normalization_v2.BatchNormalization>,\r\n              <tensorflow.python.keras.layers.convolutional.Conv1D>],\r\n             5: [<tensorflow.python.keras.layers.convolutional.Conv1D>],\r\n             6: [<tensorflow.python.keras.engine.input_layer.InputLayer>]})\r\n```\r\n\r\nIn summary, the models' structure don't match, which is pretty much the same as what you described. I don't think we can support this use case easily.\r\nSo yep the workaround you have is pretty neat. Does that answer your question?", "@tanzhenyu Is there a method to generate these dicts, or you built them manually?", "> @tanzhenyu Is there a method to generate these dicts, or you built them manually?\r\n\r\nThere is a `_map_graph_network` private method under keras/engine/network.py which you can use to verify it", "@tanzhenyu Alright - makes sense, thanks for the explanation.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34182\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34182\">No</a>\n"]}, {"number": 34181, "title": "transformer model for mobile crashes in iPhone XS, XR and 11 series", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone XS, XR, iPhone 11 series\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.5.1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.5.4\r\n- GCC/Compiler version (if compiling from source): 4.8.5 20150623\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI have transformer model trained with tensorflow==1.5.1. Frozen graph is optimized and further quantized for mobile usage. Together with quantized model, I have tensorflow==1.5.1 library built for ios via `./tensorflow/contrib/makefile/build_all_ios.sh`. \r\n\r\nModels from iPhone 6 up to 9 all works fine, but iPhone XS, XR and beyond crashes during inference (inference is equivalent to running `m_pSession->Run`) . When running the model N times, it randomly emits error as below : \r\n\r\n```\r\n# ./tensorflow/core/framework/function.cc:855\r\nInvalid argument: Retval[0] does not have value\r\n\r\n# this function is called from : \r\n./tensorflow/core/common_runtime/direct_session.cc:630\r\n```\r\n\r\n**Describe the expected behavior**\r\ninference should not crash, as it does not in other iOS devices\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\n// Initialize a session\r\nStatus status = NewSession(SessionOptions(), &m_pSession);\r\n    \r\n// Read binary proto file\r\nGraphDef graph_def;\r\nstatus = ReadBinaryProto(Env::Default(), model_file_path, &graph_def);\r\n\r\n// Create session\r\nstatus = m_pSession->Create(graph_def);\r\n\r\n// Setup inputs and outputs\r\nstring input_op_name = TENSORFLOW_INPUT_OP;\r\nstring input_length_op_name = TENSORFLOW_INPUT_LENGTH_OP;\r\nTensor input_tensor(DT_INT32, TensorShape({1, inputWordCnt}));\r\nTensor input_length_tensor(DT_INT32, TensorShape({1}));\r\n\r\n// inputWordArray is a vector with source words to translate\r\nauto input_tensor_mapped = input_tensor.tensor<int, 2>();\r\nfor(int i=0; i<inputWordCnt; i++) {\r\n    input_tensor_mapped(0, i) = inputWordArray[i];\r\n}\r\nauto input_length_tensor_mapped = input_length_tensor.tensor<int, 1>();\r\ninput_length_tensor_mapped(0) = inputWordCnt;\r\n\r\nstd::vector<Tensor> output_tensors;\r\n\r\n// Do Translate!\r\nStatus run_status = m_pSession->Run({{input_op_name, input_tensor},\r\n                                     {input_length_op_name, input_length_tensor}},\r\n                                     {TENSORFLOW_ONEBEST_OUTPUT_OP, TENSORFLOW_ATTENTION_MAP_OUTPUT_OP},\r\n                                     {},\r\n                                     &output_tensors);\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI intentionally added `std::cout` to track the value of `rets_`, and as you can see from the below, `rets_.[i]has_val` suddenly has value 0 in random chance.\r\n\r\n```\r\n[tensorflow | function.cc] rets_.size() = 2\r\n[tensorflow | function.cc] i = 0\r\n[tensorflow | function.cc] rets = 0x170a01c40\r\n[tensorflow | function.cc] rets.size() = 0\r\n[tensorflow | function.cc] rets_.[i]has_val = 1\r\n[tensorflow | function.cc] rets_[i].val) = Tensor<type: int32 shape: [1,53] values: [0 1 14]...>\r\n[tensorflow | function.cc] i = 1\r\n[tensorflow | function.cc] rets = 0x170a01c40\r\n[tensorflow | function.cc] rets.size() = 1\r\n[tensorflow | function.cc] rets_.[i]has_val = 1\r\n[tensorflow | function.cc] rets_[i].val) = Tensor<type: int32 shape: [1,53] values: [1 15939 0]...>\r\n\r\n[tensorflow | function.cc] rets_.size() = 2\r\n[tensorflow | function.cc] i = 0\r\n[tensorflow | function.cc] rets = 0x170029c40\r\n[tensorflow | function.cc] rets.size() = 0\r\n[tensorflow | function.cc] rets_.[i]has_val = 0\r\n[tensorflow | function.cc] rets_[i].val) = Tensor<type: float shape: [0] values: >\r\n[tensorflow_inference.cpp] Running model failed: Invalid argument: Retval[0] does not have value\r\n```\r\n\r\nError occurs due to accessing a missing value, which should not be missing in the first place.\r\nIs iPhone XS/XR interrupting the memory values of tensorflow ops?", "comments": ["Your TensorFlow version `1.5.1` is a very old one from 2 years ago, which would make it very difficult to investigate or fix the issue.\r\n\r\nCould you try using your model in a more recent version of TensorFlow, such as `1.15.0` or `2.0.0`? Are there any particular reasons why you can't use a more recent version?", "@kweonwooj \r\n\r\nAny update on this issue please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 34180, "title": "Fix bug in output ordering from interleave with num_parallel_calls", "body": "Before this fix, Dataset.interleave(..., num_parallel_calls=2) would result\r\nin non-deterministic output order, even when\r\noptions.experimental_deterministic=True. The non-determinism is seen\r\nwhen the cycle length of the interleave exceeds the number of input elements.\r\n\r\nPiperOrigin-RevId: 279838573\r\nChange-Id: I6dc94b071a8eeb269e24b192aa65be105022444c", "comments": []}, {"number": 34179, "title": "Clean up DatasetOpsTestBase and refactor AutoShardDatasetOpTest", "body": "As all the tests for tf.data kernels have been switched to be using DatasetParams, this PR merges `DatasetOpsTestBaseV2` into `DatasetOpsTestBase` and cleans up the codes. Also, `AutoShardDatasetOpTest` is refactored. ", "comments": ["@aaudiber A commit (https://github.com/tensorflow/tensorflow/pull/34179/commits/95e3f9747b10c82536de68c5aeddc14cdc837e56) is submitted to change `DatasetOpsTestBaseV2` to `DatasetOpsTestBase` in `UniqueDatasetOpTest`."]}, {"number": 34178, "title": "Bug with tf.function and complex variables", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): installed from conda using `conda install tensorflow==2.0`\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: Python 3.6.0 :: Continuum Analytics, Inc.\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: using CPU only\r\n- GPU model and memory: using CPU only\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen running following code:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n@tf.function\r\ndef fn():\r\n    some_variable = tf.cast(1.0, tf.complex64)\r\n    T = 1j * np.eye(8, dtype=np.complex64)\r\n    return tf.convert_to_tensor(T)\r\n\r\nfn().numpy()\r\n```\r\nI got incorrect output:\r\n```\r\narray([[0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\r\n       [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\r\n       [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\r\n       [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\r\n       [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\r\n       [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\r\n       [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\r\n       [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j]],\r\n      dtype=complex64)\r\n```\r\n\r\nI have installed fresh conda enviroment with python 3.6 and tensorflow 2.0.0. GPU \r\nis disabled with ` export CUDA_VISIBLE_DEVICES=\"-1\"`\r\n\r\n**Describe the expected behavior**\r\n\r\nThe output should be following:\r\n```\r\narray([[0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\r\n       [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\r\n       [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\r\n       [0.+0.j, 0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\r\n       [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\r\n       [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j],\r\n       [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j],\r\n       [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+1.j]],\r\n      dtype=complex64)\r\n```\r\n\r\nI got correct output for following versions of the function:\r\n```\r\n@tf.function\r\ndef fn():\r\n    T = 1j * np.eye(8, dtype=np.complex64)\r\n    return tf.convert_to_tensor(T)\r\n```\r\nand (when tf.function is removed)\r\n```\r\ndef fn():\r\n    some_variable = tf.cast(1.0, tf.complex64)\r\n    T = 1j * np.eye(8, dtype=np.complex64)\r\n    return tf.convert_to_tensor(T)\r\n```\r\nand when I use small value for np.eye\r\n```\r\n@tf.function\r\ndef fn():\r\n    some_variable = tf.cast(1.0, tf.complex64)\r\n    T = 1j * np.eye(7, dtype=np.complex64)\r\n    return tf.convert_to_tensor(T)\r\n```\r\n\r\nCan you reproduce my error ? \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nAs above. \r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThe fragment of the output of `conda list` command:\r\n```\r\ntensorflow                2.0.0           gpu_py36h6b29c10_0  \r\ntensorflow-addons         0.5.0                     <pip>\r\ntensorflow-base           2.0.0           gpu_py36h0ec5d1f_0  \r\ntensorflow-estimator      2.0.0              pyh2649769_0  \r\ntensorflow-probability    0.8.0                     <pip>\r\n```\r\n", "comments": ["@kmkolasinski ,\r\nplease try using the latest ` tf-nightly 2.1.0.dev20191110 version-` `pip install tf-nightly` , issue seem to be fixed. Kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/54566ac084555f5edb9aaf6adaf5301a/34178.ipynb) of colab.Thanks!", "Thank you @oanush it works correctly with the latest version. This issue can be closed. ", "Closing since issue is resolved,Thanks!"]}, {"number": 34177, "title": "[ROCm] Fix for the broken ROCm CSB.", "body": "The following commit breaks the --config=rocm build\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/72d0facc37e377d89d8034bb825bf0e93a4f810d\r\n\r\nThe above commit introduces a new linker flag `-Wl,--gc-sections` which seems to result in the linker discarding? some of the information related to the gpu kernel binary blobs, consequently leading to the runtime errors (example shown below) in most (all?) of the regression tests.\r\n\r\n```\r\nterminate called after throwing an instance of 'std::runtime_error'\r\nwhat():  Missing metadata for __global__ function: _ZN5Eigen8internal15EigenMetaKernelINS_15TensorEvaluatorIKNS_14TensorAssignOpINS_9TensorMapINS_6TensorIfLi1ELi1EiEELi16ENS_11MakePointerEEEKNS_18TensorCwiseUnaryOpINS0_12scalar_rightIffNS0_17scalar_product\\\r\n_opIffEELb0EEEKNS4_INS5_IKfLi1ELi1EiEELi16ES7_EEEEEENS_9GpuDeviceEEEiEEvT_T0_\r\nFatal Python error: Aborted\r\n```\r\n\r\nThe \"fix\" is to remove that linker flag from the ROCm build.\r\n\r\nNote that the flag in question was not used for the ROCm build, prior to the commit mentioned above.\r\n\r\n\r\n---------------------\r\n\r\n/cc @chsigg @meteorcloudy @whchung ", "comments": ["@deven-amd  Could you please sign cla ? Thanks!", "To make things faster, I'll send a change from internal and update [the preconfigured ROCm toolchain](https://github.com/tensorflow/tensorflow/blob/master/third_party/toolchains/preconfig/ubuntu16.04/gcc5-rocm/BUILD#L125) as well.", "@gbaned I do have the cla signed :)\r\n\r\n@meteorcloudy, thank you for getting this update in. appreciate it.\r\n\r\nw.r.t to your suggestion [here](https://github.com/tensorflow/tensorflow/issues/34082#issuecomment-552810210) for enabling some CI tests for ROCm, the ideal thing to do would be to run this script \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/rocm/run_csb_tests.sh\r\n\r\n/cc @sunway513 ", "I was told by @chsigg that we don't have the hardware on our CI machines to run those tests, so we probably won't be able to enable those tests in presubmit currently."]}]