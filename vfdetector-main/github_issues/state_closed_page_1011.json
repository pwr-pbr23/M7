[{"number": 23022, "title": "Add ability to start TensorFlow server from Java API", "body": "[Standalone Client Mode](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/README.md#standalone-client-mode) allows to easily train model utilizing distributed resources. Only one thing we need to do that is to start TensorFlow server on every cluster node we'd like to participate in training. Because of that it's very important to have an ability to start TensorFlow server in different ways and from any environment. I prepared this request as result of [discussion on Development List](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/TVtFC94bC8k).\r\n\r\nThe following example demonstrates how to use TensorFlow server in Java:\r\n```java\r\nClusterDef clusterDef = ClusterDef.newBuilder()\r\n  .addJob(JobDef.newBuilder()\r\n  .setName(\"worker\")\r\n  .putTasks(0, \"localhost:4321\")\r\n  .build()\r\n).build();\r\n\r\nServerDef serverDef = ServerDef.newBuilder()\r\n  .setCluster(clusterDef)\r\n  .setJobName(\"worker\")\r\n  .setTaskIndex(0)\r\n  .setProtocol(\"grpc\")\r\n  .build();\r\n\r\nServer server = new Server(serverDef.toByteArray());\r\nserver.start();\r\nserver.join();\r\n```\r\n\r\nPlease, feel free to comment and suggest changes.", "comments": ["Hi @asimshankar, thank you for review. I fixed all your comments, so please have a look.", "Hi @asimshankar. Any updates?", "Hi, @asimshankar, @ymodak. I don't clearly understand current state. Do you have some concerns about this PR or it's ready to be merged?", "@dmitrievanthony - apologies for the delay, I'll aim to take a look at the updated PR soon", "Hi, @asimshankar. Thanks for comments. I updated the code, you can have a look. I didn't fixed only concurrency things in `Server.java` because from my perspective your approach is much more complicated and error prone. Well, anyway, if you prefer it, I can use it.\r\n\r\nSpeaking about Python API, I'd be glad to propagate this change into it also, but I think it makes sense to do it in other PR (there are some our internal tasks that depends on this PR, so would be great to merge it as soon as it possible).\r\n\r\nBTW, I'm also trying to fix GRPC server `stop` issue in PR #23190, but it's unclear so far how to do it correctly. If you could have a look and give an advice it would be great.", "Hi, @asimshankar. I think I fixed all your comments, could you please have a look?", "Thanks, I eliminated leaks. Please have a look.", "Hi @asimshankar. Looks like tests failed:\r\n\r\n- Android build failed because `c_api_internal.h` contained include of `distributed_runtime/server_lib.h`. I wrapped it into `#ifndef __ANDROID__`, so now it should work.\r\n- Clang check failed on `Server.java` file, I fixed style.\r\n- Python GPU test failed on some GPU things not related to my changes, don't think I can do something with it.\r\n- Ubuntu Sanity failed on BUILD (I've fixed) and with the following exception:\r\n\r\n```\r\nMissing the licenses for the following external dependencies:\r\n@grpc//\r\n@grpc//third_party/address_sorting\r\n@grpc//third_party/nanopb\r\n```\r\nI don't clearly understand how to fix it. Do you?", "Hi, @ymodak, @asimshankar. I've updated the code, could you please have a look and rerun tests.", "Thanks @dmitrievanthony : I'll run the tests once we have a fix for the licenses failure.\r\n\r\nSome background: When we package the JNI library in a tarball, we include the LICENSE files of all libraries that the target depends on. You can probably run [`ci_sanity.sh`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_sanity.sh) locally to reproduce the failure seen in the \"Ubuntu Sanity\" build. \r\n\r\nThe LICENSE files are explicitly listed in the [`//tensorflow/tools/lib_package:jnilicenses_generate`](https://github.com/tensorflow/tensorflow/blob/ee1263ea3a0d8cb0bccf824d78d25d6c9d545561/tensorflow/tools/lib_package/BUILD#L181) BUILD target and the sanity check script aims to ensure that this list is in [sync with the actual dependencies of `//tensorflow/java:libtensorflow_jni.so` target](https://github.com/tensorflow/tensorflow/blob/ee1263ea3a0d8cb0bccf824d78d25d6c9d545561/tensorflow/tools/ci_build/ci_sanity.sh#L414).\r\n\r\nSince we're adding a dependency to another third party library (grpc) we need to include the license for that as well - listed in the logs of the failing test:\r\n```\r\nFAIL: mismatch in packaged licenses and external dependencies\r\nMissing the licenses for the following external dependencies:\r\n@grpc//\r\n@grpc//third_party/address_sorting\r\n@grpc//third_party/nanopb\r\n```\r\n\r\n(Which means we need the LICENSE file for grpc as well as _its_ external dependencies on address_sorting and nanopb). \r\n\r\nSo I think simply adding these to the dependencies of the [`//tensorflow/tools/lib_package:jnilicenses_generate`](https://github.com/tensorflow/tensorflow/blob/ee1263ea3a0d8cb0bccf824d78d25d6c9d545561/tensorflow/tools/lib_package/BUILD#L181) target should do.\r\n\r\nLet me know if you can do that (and run `ci_sanity.sh` locally to iterate faster). If not, and you need some help, I can try to patch that in before merging this PR.\r\n\r\nThanks!", "Thanks for very details explanation, @asimshankar! I fixed the problem and checked `ci_sanity.sh` locally. Please have a look and rerun tests.", "Hi @asimshankar, looks like something not related to my code failed. Some allocation problems as far as I see.", "@dmitrievanthony : Yeah, that seems unrelated. I'll take it from here :). Thanks for the contribution, will hopefully figure that out and get it merged in the next day or two. Will ping back if I discover any issues.", "(Some more comments based on the failing tests)", "Hi, @asimshankar. Do I understand correctly that the PR will be merged soon by you or someone else? Do I need to make some additional changes?", "@dmitrievanthony : Yup, and done! Thanks for the contribution.", "Hi @asimshankar, I just checked my libtensorflow v1.12 installation and see no additional C APIs introduced in this PR. @dmitrievanthony Have you been successfully building the Java part of this PR (possibly in tf-io) without source dependency on TF core?\r\n\r\nEDIT: sorry, did not notice that r1.12 was cut before this PR merged. I will try master instead.", "@byronyi : the merge commit isn't part of the 1.12 release branch since it landed after that branch was cut. So, these features will be part of 1.13 or when built from source. "]}, {"number": 23021, "title": "I am getting the same prediction value for all of my test data", "body": "I tried with many combinations of hyper parameters, optimizers and cost functions but I'm getting R2 = 0.33 at max. Please help me finding a solution.\r\n[Python code.txt](https://github.com/tensorflow/tensorflow/files/2482551/Python.code.txt)\r\n[Gholamnejad_dataset1.xlsx](https://github.com/tensorflow/tensorflow/files/2482564/Gholamnejad_dataset1.xlsx)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@shreyassks Hi, I'd request you to perform scaling/data preprocessing for the input data. Also try reducing the learning rate(different values < 0.004) and reverify. Thanks !\r\n", "@harshini-gadige I have tried normalizing the data to (0,1) but the data is not equally distributed so I have normalized it with mean and standard deviation even though now i'm getting r2 = 0.42 at max. I have also tried till 1e-5 learning rates.", "This should be a stackoverflow question, not a github issue. Please ask there."]}, {"number": 23020, "title": "Error in C++ Tensorflow to load model trained by python", "body": "Using tensorflow1.8 in C++,  I load the model (freezed in *.pb) and creat session with graph. Than when I run the session, error occurs as \"Error: RUN FAILED...Invalid argument: Expects arg[0] to be bool but float is peovided\".\r\nAnyone suggestions is needed, Please help me!", "comments": ["Did you by any chance use the \"optimize_for_inference\" script? I recently got a similar error when using this script. What worked for me was to just freeze the model and not run the optimize_for_inference script.", "> Did you by any chance use the \"optimize_for_inference\" script? I recently got a similar error when using this script. What worked for me was to just freeze the model and not run the optimize_for_inference script.\r\n\r\nThanks for your reply, but I don't use the script, so the problem maybe wrong pb model in freezing process? I also try to load the model by MetaGraphDef class to load \"*.meta\" file, but occurs \"Invalid argument: No Opkernel was registered to support Op 'PyFunc' with these attrs. Registered devices: [CPU], Registered kernels\", which confused me...", "I am using r1.10. I am using pre-intalled freeze_graph binary, it works well. I guess you may be put wrong output node when during freezing ?", "@dvicini @robinqhuang good suggestions, optimize_for_inference and freeze_graph binary should work. Please post your code snippet here if this is still an issue.", "@wt-huang I posted a comment in issue https://github.com/tensorflow/tensorflow/issues/19838", "@dvicini You can use [Graph Transform Tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms) to optimize graph for inference then import it.", "Closing as this is resolved, free to reopen if problem persists."]}, {"number": 23019, "title": "`tf.layers.Conv2D`'s padding doesn't allow variable reuse in some cases", "body": "Minimal code to reproduce:\r\n```python\r\nimport tensorflow as tf\r\n\r\nexample_image = tf.zeros([1, 32, 32, 1])\r\nexample_image2 = tf.zeros([1, 41, 41, 1])\r\n\r\n\r\nconvolution_op = tf.layers.Conv2D(32, 3, padding=\"SAME\", dilation_rate=4)\r\n\r\nconvolution_op(example_image)\r\nconvolution_op(example_image2)\r\n```\r\nError:\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1575   try:\r\n-> 1576     c_op = c_api.TF_FinishOperation(op_desc)\r\n   1577   except errors.InvalidArgumentError as e:\r\n\r\nInvalidArgumentError: Dimension size must be evenly divisible by 4 but is 49 for 'conv2d/SpaceToBatchND_1' (op: 'SpaceToBatchND') with input shapes: [1,41,41,1], [2], [2,2] and with computed input tensors: input[1] = <4 4>, input[2] = <[4 4][4 4]>.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-05cd8b9e8ce2> in <module>()\r\n      8 \r\n      9 convolution_op(example_image)\r\n---> 10 convolution_op(example_image2)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    360 \r\n    361       # Actually call layer\r\n--> 362       outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n    363 \r\n    364     if not context.executing_eagerly():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    734 \r\n    735       if not in_deferred_mode:\r\n--> 736         outputs = self.call(inputs, *args, **kwargs)\r\n    737         if outputs is None:\r\n    738           raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py in call(self, inputs)\r\n    184 \r\n    185   def call(self, inputs):\r\n--> 186     outputs = self._convolution_op(inputs, self.kernel)\r\n    187 \r\n    188     if self.use_bias:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py in __call__(self, inp, filter)\r\n    866 \r\n    867   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin\r\n--> 868     return self.conv_op(inp, filter)\r\n    869 \r\n    870 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py in __call__(self, inp, filter)\r\n    518 \r\n    519   def __call__(self, inp, filter):  # pylint: disable=redefined-builtin\r\n--> 520     return self.call(inp, filter)\r\n    521 \r\n    522 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py in _with_space_to_batch_call(self, inp, filter)\r\n    501     crops = _with_space_to_batch_adjust(crops, 0, spatial_dims)\r\n    502     input_converted = array_ops.space_to_batch_nd(\r\n--> 503         input=inp, block_shape=dilation_rate, paddings=paddings)\r\n    504 \r\n    505     result = self.op(input_converted, filter)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in space_to_batch_nd(input, block_shape, paddings, name)\r\n   7568     _, _, _op = _op_def_lib._apply_op_helper(\r\n   7569         \"SpaceToBatchND\", input=input, block_shape=block_shape,\r\n-> 7570         paddings=paddings, name=name)\r\n   7571     _result = _op.outputs[:]\r\n   7572     _inputs_flat = _op.inputs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,\r\n    786                          input_types=input_types, attrs=attr_protos,\r\n--> 787                          op_def=op_def)\r\n    788       return output_structure, op_def.is_stateful, op\r\n    789 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    452                 'in a future version' if date is None else ('after %s' % date),\r\n    453                 instructions)\r\n--> 454       return func(*args, **kwargs)\r\n    455     return tf_decorator.make_decorator(func, new_func, 'deprecated',\r\n    456                                        _add_deprecated_arg_notice_to_docstring(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in create_op(***failed resolving arguments***)\r\n   3153           input_types=input_types,\r\n   3154           original_op=self._default_original_op,\r\n-> 3155           op_def=op_def)\r\n   3156       self._create_op_helper(ret, compute_device=compute_device)\r\n   3157     return ret\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1729           op_def, inputs, node_def.attr)\r\n   1730       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\r\n-> 1731                                 control_input_ops)\r\n   1732 \r\n   1733     # Initialize self._outputs.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1577   except errors.InvalidArgumentError as e:\r\n   1578     # Convert to ValueError for backwards compatibility.\r\n-> 1579     raise ValueError(str(e))\r\n   1580 \r\n   1581   return c_op\r\n\r\nValueError: Dimension size must be evenly divisible by 4 but is 49 for 'conv2d/SpaceToBatchND_1' (op: 'SpaceToBatchND') with input shapes: [1,41,41,1], [2], [2,2] and with computed input tensors: input[1] = <4 4>, input[2] = <[4 4][4 4]>.\r\n\r\n```\r\n\r\nA different error happens if the order of calls to `convolution_op` is inverted:\r\n```\r\nimport tensorflow as tf\r\n\r\nexample_image = tf.zeros([1, 32, 32, 1])\r\nexample_image2 = tf.zeros([1, 41, 41, 1])\r\n\r\n\r\nconvolution_op = tf.layers.Conv2D(32, 3, padding=\"SAME\", dilation_rate=4)\r\n\r\nconvolution_op(example_image2)\r\nconvolution_op(example_image)\r\n```\r\nwhich is in short\r\n```\r\nInvalidArgumentError: Dimension size must be evenly divisible by 4 but is 43 for 'conv2d/SpaceToBatchND_1' (op: 'SpaceToBatchND') with input shapes: [1,32,32,1], [2], [2,2] and with computed input tensors: input[1] = <4 4>, input[2] = <[4 7][4 7]>.\r\n\r\n```\r\n\r\nI don't think this is the correct behaviour. I understand that the object oriented tf.layers classes such as `tf.layers.Conv2D` are meant to be the preferred way of doing variable reuse, but the padding seems to be set with the first call to the function, reducing utility of those functions.\r\n\r\n\r\nHave I written custom code\r\nN/A\r\nOS Platform and Distribution\r\nN/A\r\nTensorFlow installed from\r\npip\r\nTensorFlow version\r\n1.10.1\r\nBazel version \r\nN/A\r\nCUDA/cuDNN version\r\nN/A\r\nGPU model and memory\r\nN/A\r\nExact command to reproduce\r\nN/A\r\nMobile device\r\nN/A\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Updated.", "@fabio12345 When using `dilation_rate = 4` and `padding=\"SAME\"`, the width and height of your image have to be divisible by 4. You'll have to pad, crop, or reshape `example_image2`. Something like:\r\n\r\n```python\r\nimport tensorflow as tf\r\nexample_image = tf.zeros([1, 32, 32, 1])\r\nexample_image2 = tf.zeros([1, 41, 41, 1])\r\nexample_image2_resized = tf.image.resize_images(example_image2, [40, 40])\r\n\r\nconvolution_op = tf.layers.Conv2D(32, 3, padding=\"SAME\", dilation_rate=4)\r\nconvolution_op(example_image)\r\nconvolution_op(example_image2_resized)\r\n```\r\n\r\n", "Hi @omalleyt12, thanks for your comment. \r\nI understand that the function is expecting a size divisible by 4 at some point, but that\u2019s not at the input of __call__. It is already doing some padding internally, which is dependent on the size of input in the first invocation of __call__ on the layer object. \r\nAs shown above, the layer works on the 41x41 input if the invocation of call on that input comes first, and doesn\u2019t work on the subsequent invocation of call on a 32x32 input in that case. \r\nThis means that Conv layers don\u2019t support the use case where you want to __call__ the layer on another tensor that has got a different size - I would expect the layer to do the padding internally on each invocation of call. Is this a bug, or is there any explanation of why this would be a feature?\r\n"]}, {"number": 23018, "title": ".", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 23017, "title": "tf.estimator.train's incompatibility with distributed training on Cloud ML Engine is not well-documented", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 28\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.5.0\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nThe page https://cloud.google.com/ml-engine/docs/tensorflow/distributed-training-details#tensorflow-config notes that \"The tf.estimator.train method doesn't work with distributed training on Cloud ML Engine. Please use train_and_evaluate instead.\". This is not documented on the Estimator page (https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) or anywhere else I've seen. I believe it would be helpful to document it more prominently, as I can't be the only one who didn't read the \"Using TF_CONFIG for Distributed Training Details\" page and wasted time debugging why a model wouldn't work when distributed.\r\n\r\nIf possible, it would be even more helpful to make tf.estimator.train raise an exception when run in a distributed ML Engine context, or log a warning. It's unreasonable to expect the user to figure this out themselves as from an API perspective there's no reason to expect `train_and_evaluate` would work where `train` fails (one might reasonably assume `train_and_evaluate` calls `train`).", "comments": ["Thanks for reporting this @logicchains.\r\n\r\nYou seem to have a clear opinion on how this can be improved, is there any change you can send a PR with a fix? The api_docs are generated from the python doc-strings.\r\n\r\n@karmel this is outside my area of expertise. Can you assign someone with a little more subject matter knowledge? That fix would have to go in the [estimator repo](https://github.com/tensorflow/estimator), right?", "Nagging Assignee @karmel: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Help with clarifying docs is always appreciated-- @logicchains , let us know if you would like to contribute a fix here.", "I'm happy to submit a change to the documentation if that's suitable. That would be https://github.com/tensorflow/estimator?", "Yes, thank you.", "@logicchains, Sorry for late response. \r\n\r\n\r\nThere is limited support for training with Estimator using all strategies except `TPUStrategy`.Basic training and evaluation should work, but a number of advanced features such as [v1.train.Scaffold](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Scaffold) do not. There may also be a number of bugs in this integration and there are no plans to actively improve this support (the focus is on Keras and custom training loop support). If at all possible, you should prefer to use [tf.distribute](https://www.tensorflow.org/api_docs/python/tf/distribute) with those APIs instead.\r\n\r\nUsing [tf.distribute.Strategy](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) with Estimator has limited support. For more details please refer [here](https://www.tensorflow.org/guide/estimator#using_tfdistributestrategy_with_estimator_limited_support). Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23016, "title": "ImportError: DLL load failed: The specified module could not be found | lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10 x64\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: CUDA9 / cuDNN7.0.5\r\n- **GPU model and memory**: Nvidia GeForce 1050 Ti\r\n- **Exact command to reproduce**:\r\npip install tensorflow-gpu\r\npython\r\nimport tensorflow as tf\r\n\r\n### Describe the problem\r\nI'm trying to install TensorFlow for my study. I tried a few version of python and CUDA/cuDNN but I am still getting the same error. This [blog](https://medium.com/@lmoroney_40129/installing-tensorflow-with-gpu-on-windows-10-3309fec55a00) is used to install properly. I have done each steps exactly the same with the blog. I have read and tried solutions from the tensorflow error list and almost all solution provided on the internet. Result is the same.\r\n\r\n### Source code / logs\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Umit Kilic\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Umit Kilic\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Umit Kilic\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Umit Kilic\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Umit Kilic\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Umit Kilic\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Umit Kilic\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Umit Kilic\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Umit Kilic\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Umit Kilic\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Umit Kilic\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Umit Kilic\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Umit Kilic\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found\r\n\r\n", "comments": ["First, I intalled CUDA10 and cuDNN 7.3.1 and the problem was still there. Then, I uninstalled both of them and reinstalled CUDA9 and cuDNN 7.0.5 again and restarted the computer. The related problem has just gone for now. Probably this is not a real solution but it works for me. Actually, finding the real causes of the errors will help other people that this solution does not work for.", "Thanks for sharing your solution. The compatible configuration fo TF 1.11 is cuda 9 and cudnn 7 which helped to resolve your issue. We have captured this information in Tested build configurations table.\r\nSee https://www.tensorflow.org/install/source#tested_build_configurations.\r\nClosing this issue now since it's resolved. Thanks!"]}, {"number": 23015, "title": "change libcudnn7 version to 7.0.5.15", "body": "Founded incompatibilities with my laptop GPU in ubuntu 18. \"cuda_dnn.cc:360] Possibly insufficient driver version: 390.48.0\". It was solved downgrading CuDNN", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Hi @juanluisrosaramos, can you upgrade your driver version instead? cuDNN 7.1+ is required for TensorRT 4, and we usually don't rollback cuDNN version.", "@juanluisrosaramos Did you get a chance to work on this?\r\n\r\n> Hi @juanluisrosaramos, can you upgrade your driver version instead? cuDNN 7.1+ is required for TensorRT 4, and we usually don't rollback cuDNN version.\r\n\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on."]}, {"number": 23014, "title": "Tensorflow Eager Execution Gradient Calculation slower each epoch", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: 1.10 binary\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version**: 9.0 / 7.2\r\n- **GPU model and memory**: 1080Ti - 11 Gb\r\n- Bazel version - NA\r\n- Exact command to reproduce - NA\r\n- Mobile device - NA\r\n### Describe the problem\r\n\r\nHi to all!\r\n\r\nI have been implementing Temporal Ensembling for Semi-Supervised Learning by Laine et al. with eager execution and a couple of GitHub users noticed that the computations of the gradients is taking gradually more time each epoch. I don't see what is causing this. After benchmarking I could confirm this issue, but I have no idea why this should be happening. Is anyone faced this problem with `tf.GradientTape()` ? Or is this is an issue related to eager execution?\r\n\r\nThe code for the loss and gradients is the following:\r\n\r\n```\r\ndef temporal_ensembling_loss(X_train_labeled, y_train_labeled, X_train_unlabeled, model, unsupervised_weight, ensembling_targets):\r\n    \"\"\" Gets the loss for the temporal ensembling model\r\n    Arguments:\r\n        X_train_labeled {tensor} -- labeled samples\r\n        y_train_labeled {tensor} -- labeled train labels\r\n        X_train_unlabeled {tensor} -- unlabeled samples \r\n        model {tf.keras.Model} -- temporal ensembling model\r\n        unsupervised_weight {float} -- weight of the unsupervised loss\r\n        ensembling_targets {np.array} --  ensembling targets\r\n    Returns:\r\n        {tensor} -- predictions for the ensembles\r\n        {tensor} -- loss value\r\n    \"\"\"\r\n\r\n    z_labeled = model(X_train_labeled)\r\n    z_unlabeled = model(X_train_unlabeled)\r\n\r\n    current_predictions = tf.concat([z_labeled, z_unlabeled], 0)\r\n\r\n    return current_predictions, tf.losses.softmax_cross_entropy(\r\n        y_train_labeled, z_labeled) + unsupervised_weight * (\r\n            tf.losses.mean_squared_error(ensembling_targets, current_predictions))\r\n\r\n\r\ndef temporal_ensembling_gradients(X_train_labeled, y_train_labeled, X_train_unlabeled, model, unsupervised_weight, ensembling_targets):\r\n    \"\"\" Gets the gradients for the temporal ensembling model\r\n    Arguments:\r\n        X_train_labeled {tensor} -- labeled samples\r\n        y_train_labeled {tensor} -- labeled train labels\r\n        X_train_unlabeled {tensor} -- unlabeled samples \r\n        model {tf.keras.Model} -- temporal ensembling model\r\n        unsupervised_weight {float} -- weight of the unsupervised loss\r\n        ensembling_targets {np.array} --  ensembling targets\r\n    Returns:\r\n        {tensor} -- predictions for the ensembles\r\n        {tensor} -- loss value\r\n        {tensor} -- gradients for each model variables\r\n    \"\"\"\r\n\r\n    with tf.GradientTape() as tape:\r\n        ensemble_precitions, loss_value = temporal_ensembling_loss(X_train_labeled, y_train_labeled, X_train_unlabeled,\r\n                                                                   model, unsupervised_weight, ensembling_targets)\r\n\r\n    return ensemble_precitions, loss_value, tape.gradient(loss_value, model.variables)\r\n```\r\n\r\nI tested it in multiple machines (and in tensorflow 1.10 and 1.11) and the problem persists. \r\n\r\nThe gradient calculation takes gradually more time each epoch. Is something related to my code that is causing this or is this a bug with eager execution?\r\n\r\nBest Regards", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nTensorFlow version\nBazel version\nExact command to reproduce\nMobile device", "Can you share the definition of your model? It's probably something getting recreated in every training step for some reason.\r\n\r\n@akshaym can you take a look?", "Hi thanks for answering! This my model definition:\r\n\r\n```\r\nclass PiModel(tf.keras.Model):\r\n    \"\"\" Class for defining eager compatible tfrecords file\r\n        I did not use tfe.Network since it will be depracated in the\r\n        future by tensorflow.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        \"\"\" Init\r\n            Set all the layers that need to be tracked in the process of\r\n            gradients descent (pooling and dropout for example dont need\r\n            to be stored)\r\n        \"\"\"\r\n\r\n        super(PiModel, self).__init__()\r\n        self._conv1a = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n        self._conv1b = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n        self._conv1c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n\r\n        self._conv2a = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n        self._conv2b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n        self._conv2c = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n\r\n        self._conv3a = weight_norm_layers.Conv2D.Conv2D(filters=512, kernel_size=[3, 3],\r\n                                                        padding=\"valid\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n        self._conv3b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[1, 1],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\r\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n        self._conv3c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[1, 1],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n\r\n        self._dense = weight_norm_layers.Dense.Dense(units=10, activation=tf.nn.softmax, \r\n                                                     kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                     bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                     weight_norm=True, mean_only_batch_norm=True)\r\n\r\n    def __aditive_gaussian_noise(self, input, std):\r\n        \"\"\" Function to add additive zero mean noise as described in the paper\r\n        Arguments:\r\n            input {tensor} -- image\r\n            std {int} -- std to use in the random_normal\r\n        Returns:\r\n            {tensor} -- image with added noise\r\n        \"\"\"\r\n\r\n        noise = tf.random_normal(shape=tf.shape(\r\n            input), mean=0.0, stddev=std, dtype=tf.float32)\r\n        return input + noise\r\n\r\n    def __apply_image_augmentation(self, image):\r\n        \"\"\" Applies random transformation to the image\r\n        Arguments:\r\n            image {tensor} -- image\r\n        Returns:\r\n            {tensor} -- transformed image\r\n        \"\"\"\r\n\r\n        random_translation = tf.random_uniform(\r\n            [1, 2], minval=-2.0, maxval=2.0, dtype=tf.float32)\r\n        image = tf.contrib.image.translate(\r\n            image, random_translation, 'NEAREST')\r\n        return image\r\n\r\n    def call(self, input, training=True):\r\n        \"\"\" Function that allows running a tensor through the pi model\r\n        Arguments:\r\n            input {[tensor]} -- batch of images\r\n            training {bool} -- if true applies augmentaton and additive noise\r\n        Returns:\r\n            [tensor] -- predictions\r\n        \"\"\"\r\n\r\n        if training:\r\n            h = self.__aditive_gaussian_noise(input, 0.15)\r\n            h = self.__apply_image_augmentation(h)\r\n        else:\r\n            h = input\r\n\r\n        h = self._conv1a(h, training)\r\n        h = self._conv1b(h, training)\r\n        h = self._conv1c(h, training)\r\n        h = tf.layers.max_pooling2d(\r\n            inputs=h,  pool_size=[2, 2], strides=[2, 2])\r\n        h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\r\n\r\n        h = self._conv2a(h, training)\r\n        h = self._conv2b(h, training)\r\n        h = self._conv2c(h, training)\r\n        h = tf.layers.max_pooling2d(\r\n            inputs=h,  pool_size=[2, 2], strides=[2, 2])\r\n        h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\r\n\r\n        h = self._conv3a(h, training)\r\n        h = self._conv3b(h, training)\r\n        h = self._conv3c(h, training)\r\n\r\n        # Average Pooling\r\n        h = tf.reduce_mean(h, reduction_indices=[1, 2])\r\n        return self._dense(h, training)\r\n```\r\n\r\nThe weight_norm_layers.Conv2D.Conv2D is just the edited wrapper layer for weight and mean only batch normalization.", "Can you replace tf.layers.max_pooling2d with tf.keras.layers.MaxPool2D?\nWe've seen issues where tf.layers layers leak in eager. If this replacing\nfixes your problem then we know where to look.\n\nOn Wed, Oct 17, 2018 at 12:23 AM Tiago Freitas <notifications@github.com>\nwrote:\n\n> Hi thanks for answering! This my model definition:\n>\n> class PiModel(tf.keras.Model):\n>     \"\"\" Class for defining eager compatible tfrecords file\n>         I did not use tfe.Network since it will be depracated in the\n>         future by tensorflow.\n>     \"\"\"\n>\n>     def __init__(self):\n>         \"\"\" Init\n>             Set all the layers that need to be tracked in the process of\n>             gradients descent (pooling and dropout for example dont need\n>             to be stored)\n>         \"\"\"\n>\n>         super(PiModel, self).__init__()\n>         self._conv1a = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>         self._conv1b = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>         self._conv1c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>\n>         self._conv2a = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>         self._conv2b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>         self._conv2c = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>\n>         self._conv3a = weight_norm_layers.Conv2D.Conv2D(filters=512, kernel_size=[3, 3],\n>                                                         padding=\"valid\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>         self._conv3b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[1, 1],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                          kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                          bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>         self._conv3c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[1, 1],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>\n>         self._dense = weight_norm_layers.Dense.Dense(units=10, activation=tf.nn.softmax,\n>                                                      kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                      bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                      weight_norm=True, mean_only_batch_norm=True)\n>\n>     def __aditive_gaussian_noise(self, input, std):\n>         \"\"\" Function to add additive zero mean noise as described in the paper\n>         Arguments:\n>             input {tensor} -- image\n>             std {int} -- std to use in the random_normal\n>         Returns:\n>             {tensor} -- image with added noise\n>         \"\"\"\n>\n>         noise = tf.random_normal(shape=tf.shape(\n>             input), mean=0.0, stddev=std, dtype=tf.float32)\n>         return input + noise\n>\n>     def __apply_image_augmentation(self, image):\n>         \"\"\" Applies random transformation to the image\n>         Arguments:\n>             image {tensor} -- image\n>         Returns:\n>             {tensor} -- transformed image\n>         \"\"\"\n>\n>         random_translation = tf.random_uniform(\n>             [1, 2], minval=-2.0, maxval=2.0, dtype=tf.float32)\n>         image = tf.contrib.image.translate(\n>             image, random_translation, 'NEAREST')\n>         return image\n>\n>     def call(self, input, training=True):\n>         \"\"\" Function that allows running a tensor through the pi model\n>         Arguments:\n>             input {[tensor]} -- batch of images\n>             training {bool} -- if true applies augmentaton and additive noise\n>         Returns:\n>             [tensor] -- predictions\n>         \"\"\"\n>\n>         if training:\n>             h = self.__aditive_gaussian_noise(input, 0.15)\n>             h = self.__apply_image_augmentation(h)\n>         else:\n>             h = input\n>\n>         h = self._conv1a(h, training)\n>         h = self._conv1b(h, training)\n>         h = self._conv1c(h, training)\n>         h = tf.layers.max_pooling2d(\n>             inputs=h,  pool_size=[2, 2], strides=[2, 2])\n>         h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\n>\n>         h = self._conv2a(h, training)\n>         h = self._conv2b(h, training)\n>         h = self._conv2c(h, training)\n>         h = tf.layers.max_pooling2d(\n>             inputs=h,  pool_size=[2, 2], strides=[2, 2])\n>         h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\n>\n>         h = self._conv3a(h, training)\n>         h = self._conv3b(h, training)\n>         h = self._conv3c(h, training)\n>\n>         # Average Pooling\n>         h = tf.reduce_mean(h, reduction_indices=[1, 2])\n>         return self._dense(h, training)\n>\n> The weight_norm_layers.Conv2D.Conv2D is just the edited wrapper layer for\n> weight and mean only batch normalization.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23014#issuecomment-430515886>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxQ99sw39bY2iuzVECUFQUcf4NrUaks5ultrtgaJpZM4Xd7Ui>\n> .\n>\n\n\n-- \n - Alex\n", "> Can you replace tf.layers.max_pooling2d with tf.keras.layers.MaxPool2D? We've seen issues where tf.layers layers leak in eager. If this replacing fixes your problem then we know where to look.\r\n> [\u2026](#)\r\n> On Wed, Oct 17, 2018 at 12:23 AM Tiago Freitas ***@***.***> wrote: Hi thanks for answering! This my model definition: class PiModel(tf.keras.Model): \"\"\" Class for defining eager compatible tfrecords file I did not use tfe.Network since it will be depracated in the future by tensorflow. \"\"\" def __init__(self): \"\"\" Init Set all the layers that need to be tracked in the process of gradients descent (pooling and dropout for example dont need to be stored) \"\"\" super(PiModel, self).__init__() self._conv1a = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv1b = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv1c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv2a = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv2b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv2c = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv3a = weight_norm_layers.Conv2D.Conv2D(filters=512, kernel_size=[3, 3], padding=\"valid\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv3b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[1, 1], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv3c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[1, 1], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._dense = weight_norm_layers.Dense.Dense(units=10, activation=tf.nn.softmax, kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) def __aditive_gaussian_noise(self, input, std): \"\"\" Function to add additive zero mean noise as described in the paper Arguments: input {tensor} -- image std {int} -- std to use in the random_normal Returns: {tensor} -- image with added noise \"\"\" noise = tf.random_normal(shape=tf.shape( input), mean=0.0, stddev=std, dtype=tf.float32) return input + noise def __apply_image_augmentation(self, image): \"\"\" Applies random transformation to the image Arguments: image {tensor} -- image Returns: {tensor} -- transformed image \"\"\" random_translation = tf.random_uniform( [1, 2], minval=-2.0, maxval=2.0, dtype=tf.float32) image = tf.contrib.image.translate( image, random_translation, 'NEAREST') return image def call(self, input, training=True): \"\"\" Function that allows running a tensor through the pi model Arguments: input {[tensor]} -- batch of images training {bool} -- if true applies augmentaton and additive noise Returns: [tensor] -- predictions \"\"\" if training: h = self.__aditive_gaussian_noise(input, 0.15) h = self.__apply_image_augmentation(h) else: h = input h = self._conv1a(h, training) h = self._conv1b(h, training) h = self._conv1c(h, training) h = tf.layers.max_pooling2d( inputs=h, pool_size=[2, 2], strides=[2, 2]) h = tf.layers.dropout(inputs=h, rate=0.5, training=training) h = self._conv2a(h, training) h = self._conv2b(h, training) h = self._conv2c(h, training) h = tf.layers.max_pooling2d( inputs=h, pool_size=[2, 2], strides=[2, 2]) h = tf.layers.dropout(inputs=h, rate=0.5, training=training) h = self._conv3a(h, training) h = self._conv3b(h, training) h = self._conv3c(h, training) # Average Pooling h = tf.reduce_mean(h, reduction_indices=[1, 2]) return self._dense(h, training) The weight_norm_layers.Conv2D.Conv2D is just the edited wrapper layer for weight and mean only batch normalization. \u2014 You are receiving this because you were assigned. Reply to this email directly, view it on GitHub <[#23014 (comment)](https://github.com/tensorflow/tensorflow/issues/23014#issuecomment-430515886)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAATxQ99sw39bY2iuzVECUFQUcf4NrUaks5ultrtgaJpZM4Xd7Ui> .\r\n> -- - Alex\r\n\r\nAfter replacing the layers with tf.keras.layers.MaxPool2D the problem was not totally solved (the increase was still there but in a smaller magnitude). But after replacing the tf.layers.dropout by Keras one, the problem was solved. \r\n\r\nThanks for the support. I think the issue was resolved!"]}, {"number": 23013, "title": "LSTM weights not loaded correctly in tf.keras", "body": "### Description\r\nRunning into this problem when I:\r\n- build model A containing an LSTM layer\r\n- save the model weights with `tf.keras.save_weights()` in TensorFlow format\r\n- at some later point create an identical model A' and load the previously saved weights\r\n\r\nInspection by eye shows the lstm weights differ between model A and A' while weights of other layers (e.g. dense) are loaded correctly from file. When running through the same procedure as above but saving the weights in HDF5 format everything works as expected.\r\n\r\n### System Information:\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Mobile device: NA\r\n- TensorFlow installed from (source or binary): binary (pip package)\r\n- TensorFlow version: 1.10.0\r\n- Python version: 3.6.6\r\n- Bazel version: NA\r\n- GCC/Compiler version: NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n- Exact command to reproduce:\r\n```\r\nimport tensorflow as tf\r\n\r\ninput_layer = tf.keras.layers.Input(shape=(None, 5))\r\nlstm_layer = tf.keras.layers.LSTM(units=2)(input_layer)\r\noutput_layer = tf.keras.layers.Dense(units=2, activation='softmax')(lstm_layer)\r\n\r\nmy_model = tf.keras.Model(input_layer, output_layer)\r\n\r\nmy_model.save_weights(\"weights_test/my_weights\")\r\n\r\nprint('\\nlstm:')\r\nprint(my_model.layers[1].get_weights())\r\nprint('\\ndense:')\r\nprint(my_model.layers[2].get_weights())\r\n\r\n#lstm:\r\n#[array([[ 0.38913965,  0.4081726 , -0.23971727,  ...\r\n#dense:\r\n#[array([[-0.5567662 , -0.38940358],\r\n#       [ 0.16435587,  1.1092712 ]], dtype=float32), array([0., 0.], dtype=float32)]\r\n\r\nnew_input_layer = tf.keras.layers.Input(shape=(None, 5))\r\nnew_lstm_layer = tf.keras.layers.LSTM(units=2)(new_input_layer)\r\nnew_output_layer = tf.keras.layers.Dense(units=2, activation='softmax')(new_lstm_layer)\r\n\r\nmy_new_model = tf.keras.Model(new_input_layer, new_output_layer)\r\nmy_new_model.load_weights(\"weights_test/my_weights\")\r\n\r\nprint('\\nlstm:')\r\nprint(my_new_model.layers[1].get_weights())\r\nprint('\\ndense:')\r\nprint(my_new_model.layers[2].get_weights())\r\n\r\n#lstm:\r\n#[array([[-0.41712055,  0.67933106, -0.5161066 , ...\r\n#dense:\r\n#[array([[-0.5567662 , -0.38940358],\r\n#       [ 0.16435587,  1.1092712 ]], dtype=float32), array([0., 0.], dtype=float32)]\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "just added the System Information to the issue", "@ptiwald I tried with tf-nightly and it looks to be fine. Can you give it a try and see if the issue still exists?\r\n```\r\nroot@ubuntu:/v# python3 -c 'import tensorflow as tf; print(tf.VERSION)'\r\n1.13.0-dev20181017\r\nroot@ubuntu:/v# python3 23013.py \r\n2018-10-18 03:23:24.961961: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n\r\nlstm:\r\n[array([[-0.4377299 ,  0.422243  ,  0.6181992 , -0.37643573,  0.15646869,\r\n         0.5818784 , -0.22846544, -0.42004013],\r\n       [ 0.1357246 ,  0.5908847 , -0.34941968, -0.34077594,  0.32373846,\r\n         0.03020209, -0.28660843,  0.5177108 ],\r\n       [ 0.07117409,  0.49272335, -0.04296476, -0.5014585 , -0.14278275,\r\n         0.5817479 ,  0.21550333,  0.63276565],\r\n       [ 0.22573894, -0.614386  , -0.33345684,  0.31065148,  0.23170364,\r\n        -0.44417936,  0.4421481 , -0.43041533],\r\n       [ 0.14171016, -0.02059168, -0.3175577 , -0.5303349 ,  0.1774472 ,\r\n        -0.3553942 , -0.40374607, -0.675953  ]], dtype=float32), array([[ 0.19912148, -0.44535518, -0.19622174, -0.321691  , -0.25978547,\r\n         0.30768728,  0.35949802, -0.5732561 ],\r\n       [-0.34544852,  0.13254327,  0.19136222,  0.56944406,  0.03732488,\r\n         0.631071  , -0.04842368, -0.3165805 ]], dtype=float32), array([0., 0., 1., 1., 0., 0., 0., 0.], dtype=float32)]\r\n\r\ndense:\r\n[array([[-0.35512227,  0.13987267],\r\n       [-0.42357975, -0.46372974]], dtype=float32), array([0., 0.], dtype=float32)]\r\n\r\nlstm:\r\n[array([[-0.4377299 ,  0.422243  ,  0.6181992 , -0.37643573,  0.15646869,\r\n         0.5818784 , -0.22846544, -0.42004013],\r\n       [ 0.1357246 ,  0.5908847 , -0.34941968, -0.34077594,  0.32373846,\r\n         0.03020209, -0.28660843,  0.5177108 ],\r\n       [ 0.07117409,  0.49272335, -0.04296476, -0.5014585 , -0.14278275,\r\n         0.5817479 ,  0.21550333,  0.63276565],\r\n       [ 0.22573894, -0.614386  , -0.33345684,  0.31065148,  0.23170364,\r\n        -0.44417936,  0.4421481 , -0.43041533],\r\n       [ 0.14171016, -0.02059168, -0.3175577 , -0.5303349 ,  0.1774472 ,\r\n        -0.3553942 , -0.40374607, -0.675953  ]], dtype=float32), array([[ 0.19912148, -0.44535518, -0.19622174, -0.321691  , -0.25978547,\r\n         0.30768728,  0.35949802, -0.5732561 ],\r\n       [-0.34544852,  0.13254327,  0.19136222,  0.56944406,  0.03732488,\r\n         0.631071  , -0.04842368, -0.3165805 ]], dtype=float32), array([0., 0., 1., 1., 0., 0., 0., 0.], dtype=float32)]\r\n\r\ndense:\r\n[array([[-0.35512227,  0.13987267],\r\n       [-0.42357975, -0.46372974]], dtype=float32), array([0., 0.], dtype=float32)]\r\nroot@ubuntu:/v# cat 23013.py \r\nimport tensorflow as tf\r\n\r\ninput_layer = tf.keras.layers.Input(shape=(None, 5))\r\nlstm_layer = tf.keras.layers.LSTM(units=2)(input_layer)\r\noutput_layer = tf.keras.layers.Dense(units=2, activation='softmax')(lstm_layer)\r\n\r\nmy_model = tf.keras.Model(input_layer, output_layer)\r\n\r\nmy_model.save_weights(\"weights_test/my_weights\")\r\n\r\nprint('\\nlstm:')\r\nprint(my_model.layers[1].get_weights())\r\nprint('\\ndense:')\r\nprint(my_model.layers[2].get_weights())\r\n\r\nnew_input_layer = tf.keras.layers.Input(shape=(None, 5))\r\nnew_lstm_layer = tf.keras.layers.LSTM(units=2)(new_input_layer)\r\nnew_output_layer = tf.keras.layers.Dense(units=2, activation='softmax')(new_lstm_layer)\r\n\r\nmy_new_model = tf.keras.Model(new_input_layer, new_output_layer)\r\nmy_new_model.load_weights(\"weights_test/my_weights\")\r\n\r\nprint('\\nlstm:')\r\nprint(my_new_model.layers[1].get_weights())\r\nprint('\\ndense:')\r\nprint(my_new_model.layers[2].get_weights())\r\nroot@ubuntu:/v# \r\n```", "@yongtang  Tried different versions of tensorflow. Seems the bug is fixed from 1.11. onward. Thanks and sorry for not checking.", "@ptiwald Glad it works. \ud83d\udc4d I will closing this issue, but feel free to reopen if there is still issues related."]}, {"number": 23012, "title": "tf.make_tensor_proto doesn't work as expected", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: none\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.11.0-rc2-4-gc19e29306c 1.11.0\r\n- **Python version**: Python 3.6.4\r\n- **Bazel version (if compiling from source)**: none\r\n- **GCC/Compiler version (if compiling from source)**: none\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**:\r\n`print(tf.make_tensor_proto([1,2,3,4], dtype=tf.uint32))`\r\n\r\n\r\n### Describe the problem\r\n\r\nAccording to TensorProto message definition there are specific fields for different DataType e.g. for DT_UINT32 (https://github.com/tensorflow/tensorflow/blob/903a6399aab19b549fefd0ead836af644f3d00f8/tensorflow/core/framework/tensor.proto#L79-L80) \r\nHowever, `make_tensor_proto` puts content for most numeric types into `tensor_content` field.\r\n\r\nIn addition to that there is same inconsistency in `tf.make_ndarray` function: it tries to deserealize tensor_content and then it searches for correct fields with DataType. https://github.com/tensorflow/tensorflow/blob/903a6399aab19b549fefd0ead836af644f3d00f8/tensorflow/python/framework/tensor_util.py#L548\r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.core.framework import tensor_pb2\r\nfrom tensorflow.python.framework import dtypes\r\n\r\ncurrent = tf.make_tensor_proto([1,2,3,4], dtype=tf.uint32)\r\nprint(current)\r\n#dtype: DT_UINT32\r\n#tensor_shape {\r\n#  dim {\r\n#    size: 4\r\n#  }\r\n#}\r\n#tensor_content: \"\\001\\000\\000\\000\\002\\000\\000\\000\\003\\000\\000\\000\\004\\000\\000\\000\"\r\n\r\nexpected = tensor_pb2.TensorProto(dtype=dtypes.uint32.as_datatype_enum, uint32_val=[1,2,3,4])\r\nprint(expected)\r\n#dtype: DT_UINT32\r\n#uint32_val: 1\r\n#uint32_val: 2\r\n#uint32_val: 3\r\n#uint32_val: 4\r\n```\r\n", "comments": ["In addition to that, I can't make ndarray from manually composed uint32 tensor.\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.core.framework import tensor_pb2\r\nfrom tensorflow.python.framework import dtypes\r\n\r\nexpected_tensor = tensor_pb2.TensorProto(dtype=dtypes.uint32.as_datatype_enum, uint32_val=[1,2,3,4])\r\nexpected_array = tf.make_ndarray(expected_tensor)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/bulat/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 649, in MakeNdarray\r\n    raise TypeError(\"Unsupported tensor type: %s\" % tensor.dtype)\r\nTypeError: Unsupported tensor type: 22\r\n```", "I fixed the `tensor_content` issue in my code by using the forked version of the function.\r\nBy removing the following `if` clause I restored the documented behavior.\r\nhttps://github.com/tensorflow/tensorflow/blob/4bb45e6cdddfd8f818ee037216e732f3fd133413/tensorflow/python/framework/tensor_util.py#L503-L508\r\n\r\nHowever, I'm don't know what is the purpose of`_TENSOR_CONTENT_TYPES`.", "@josh11b may be able to comment further, but I believe that the type specific fields are mostly historical and are not required to be used. If you use tensor_content you don't use the int specific one. So the output given is expected and valid.\r\n\r\n", "Hi @KineticCookie!\r\nIt seems you are using 1.x versions of Tensorflow. We recommend that you upgrade  your code base to 2.x  versions  as as Many features and bug fixes has been done in newer versions and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23012\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23012\">No</a>\n"]}, {"number": 23011, "title": "Fix multiprocess support in data_utils.py Enqueuer classes", "body": "This was originally to fix #22804, but after some googling it seems multi-process support has been an issue for awhile.  See also:\r\n\r\n* https://github.com/matterport/Mask_RCNN/issues/13\r\n* https://github.com/keras-team/keras/issues/4142\r\n* https://github.com/keras-team/keras/issues/1638\r\n* https://github.com/keras-team/keras/issues/3962\r\n\r\nThis PR should address all of these (on Linux) and fixes the Enqueuer classes to use the correct multiprocessing constructs. I've done my best to follow the threading/multiprocessing best-practices in the Python docs.\r\n\r\n#### Approach\r\n\r\nRather than go down the rabbit hole of complex if/else logic to handle the different threading/multiprocessing use cases I've instead broken the logic out into separate, dedicated classes.  The top level GeneratorEnqueuer and OrderedEnqueuer classes simply dispatch to the appropriate implementation depending on whether the user specifies `use_multiprocessing` or not.\r\n\r\nIn addition to fixing the problems with multi-processing support (hanging, not sharing/synchronizing state correctly), I've also cleaned up and refactored the code to try and eliminate duplicated logic and misc other bugs.  For example I encountered some thread starvation issues due to mistakes with how locks were being used (the tests 'passed' and returned data in the 'right order,' but a peek under the hood showed all or most of the work was actually being done by a single thread).\r\n\r\nSome final points:\r\n\r\n* I replaced the class global variables that were being used to share state with standard mechanisms from the `threading` and `multiprocessing` modules (and class globals don't work across processes anyways)\r\n\r\n* As a side effect of using the right synchronization mechanisms it should no longer matter if generators passed to GeneratorEnqueuer are thread-safe or not, the new implementation should handle things correctly regardless.\r\n\r\n* The tests have mostly been left as-is to show that the API is maintained and none of the regressions break.  One thing I did notice is that the test for the multi-processed GeneratorEnqueuer used the wrong assertion.  I think this is because multi-processing wasn't working and it was done to get the test suite to pass?  Regardless I've updated it to use the same assertion as in the threaded version of the test:\r\n\r\n```diff\r\n   def test_generator_enqueuer_threads(self):\r\n     enqueuer = keras.utils.data_utils.GeneratorEnqueuer(\r\n         create_generator_from_sequence_threads(TestSequence([3, 200, 200, 3])),\r\n         use_multiprocessing=False)\r\n     enqueuer.start(3, 10)\r\n     gen_output = enqueuer.get()\r\n     acc = []\r\n     for _ in range(100):\r\n       acc.append(int(next(gen_output)[0, 0, 0, 0]))\r\n\r\n     self.assertEqual(len(set(acc) - set(range(100))), 0)\r\n     enqueuer.stop()\r\n\r\n   @unittest.skipIf(\r\n       os.name == 'nt',\r\n       'use_multiprocessing=True does not work on windows properly.')\r\n   def test_generator_enqueuer_processes(self):\r\n     enqueuer = keras.utils.data_utils.GeneratorEnqueuer(\r\n         create_generator_from_sequence_pcs(TestSequence([3, 200, 200, 3])),\r\n         use_multiprocessing=True)\r\n     enqueuer.start(3, 10)\r\n     gen_output = enqueuer.get()\r\n     acc = []\r\n     for _ in range(100):\r\n       acc.append(int(next(gen_output)[0, 0, 0, 0]))\r\n-    self.assertNotEqual(acc, list(range(100)))\r\n+    self.assertEqual(len(set(acc) - set(range(100))), 0)\r\n     enqueuer.stop()\r\n```\r\n\r\nTested directly and under bazel using Python 2.7.15 and 3.6.6.\r\n\r\nAny questions, mistakes I've made, or ways I can improve the PR let me know.\r\n\r\nThanks!\r\n", "comments": ["@Dref360: could you take a look?", "The tests are deadlock on my PC or they do not pass. Maybe merging this to keras for easier testing would be preferable.\r\n\r\nAlso, I think the order is not kept as with promises. ", "@Dref360 updated to use named parameters.\r\n\r\nI ran the tests in a loop and was not able to reproduce the deadlock or failed tests.  Which tests are failing for you?  Can you provide test output?\r\n\r\nWhen you say \"merging this to keras for easier testing would be preferable\" are you saying I should open this as a PR on keras instead?\r\n\r\nCould you also clarify \"I think the order is not kept as with promises.\"  Order of what, and which promises?\r\n\r\nThanks", "Ubuntu 16.04 Python 3.6\r\nI'm able to reproduce the bug with this script:\r\n```python\r\nfrom tensorflow.keras.utils import GeneratorEnqueuer, OrderedEnqueuer, Sequence\r\nimport numpy as np\r\nimport time\r\n\r\n\r\nclass MySeq(Sequence):\r\n    def __getitem__(self, idx):\r\n        time.sleep(1)\r\n        return idx\r\n    def __len__(self):\r\n        return 100\r\n\r\ndef my_gen(seq):\r\n    while True:\r\n        for i in seq:\r\n            yield i\r\n\r\n\r\nmyseq = MySeq()\r\ngen = GeneratorEnqueuer(my_gen(myseq),use_multiprocessing=True)\r\ngen.start(10,10)\r\ng = gen.get()\r\n\r\ns = time.time()\r\nacc = [i for i,_ in zip(g,range(100))]\r\nprint(\"Gen\", time.time() - s)\r\n\r\ngen = OrderedEnqueuer(myseq,use_multiprocessing=True)\r\ngen.start(10,10)\r\ng = gen.get()\r\n\r\ns = time.time()\r\nacc = [i for i,_ in zip(g,range(100))]\r\nprint(\"Seq\", time.time() - s)\r\n```\r\n\r\nI get a bunch of exception like EOFError, BrokenPipe.\r\n\r\nRunning the CI takes 5-10 days on TF, if you submit this PR on keras-team/keras, we would be able to iterate faster on your PR.\r\nAlso, I don't get how we could get a speedup with your implementation of GeneratorEnqueuer? If we take the lock between each next call we won't get a speedup?\r\n\r\n", "It looks like the difference between your code and the unit test code is the unit test code explicitly calls `enqueuer.stop()`.  The  EOF/BrokenPipe is because the MultiProcess Manager continues to run in the background trying to put new data from the generator on the queue, but it exits once the generator is exhausted.  Calling stop after `acc` is populated shuts it down cleanly.  That is, this fixed things for me:\r\n\r\n```diff\r\nacc = [i for i,_ in zip(g,range(100))]\r\n+ gen.stop()\r\n```\r\n\r\nAs far as the performance, my code is not fundamentally different from the original aside from synchronizing things correctly.  Keep in mind the old code was really \"fast\" but it was wrong because it was running a duplicate generator in each process (since with multiprocess you are forking and creating a copy of the process memory, not shared memory as with threading), this returns duplicate values and caused the original `test_generator_enqueuer_processes` to fail... I'm guessing why it was disabled.  If I enable it (flip the assertion to be `assertEqual`) with the old code I get:\r\n\r\n```\r\nAssertionError: Lists differ: [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 5, 6,[320 chars], 30] != [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13[340 chars], 99]\r\n```\r\n\r\nWhich shows the duplicate results.\r\n\r\nI'm not sure how you  would parallelize the generator itself?  What you can for sure parallelize is any preprocessing of what the generator produces, which I'm guessing is what your sleep is meant to simulate?  Right now there is no hook for that, but it would be easy to add support for passing in a user defined function that encapsulates any preprocesing logic, and parallelize that across the workers.  What do you think?\r\n\r\nNoted about the CI time on tensorflow, I'll go ahead and open a separate PR on keras.\r\n", "It works when we add a `stop()` Thanks!\r\n\r\nA user is allowed to define an object which implements `next` and we should see a speedup when using threads. \r\n\r\nUsing generators with processes is allowed for legacy purposes. (We do raise a warning inside fit_generator.)\r\n\r\nExample with threads\r\n```python\r\nimport threading\r\n\r\nfrom keras.utils import GeneratorEnqueuer\r\nimport numpy as np\r\nimport time\r\n\r\nclass PowTwo:\r\n    \"\"\"Class to implement an iterator\r\n    of powers of two\"\"\"\r\n\r\n    def __init__(self, max = 0):\r\n        self.n = 0\r\n        self.max = max\r\n        self.lock = threading.Lock()\r\n\r\n    def __iter__(self):\r\n        return self\r\n\r\n    def __next__(self):\r\n        return self.next()\r\n\r\n    def next(self):\r\n        if self.n <= self.max:\r\n            # Exemple: Get the paths to a list of images\r\n            with self.lock:\r\n                result = 2 ** self.n\r\n                self.n += 1\r\n            # Load the images, resize, data augmentation\r\n            time.sleep(1)\r\n            # Return the batch.\r\n            return result\r\n        else:\r\n            raise StopIteration\r\n\r\ngen2 = GeneratorEnqueuer(PowTwo(40),use_multiprocessing=False)\r\ngen2.start(10,10)\r\ng2 = gen2.get()\r\n\r\ns = time.time()\r\nacc = [i for i,_ in zip(g2, range(20))]\r\ngen2.stop()\r\nprint(\"Gen\", time.time() - s)\r\n```\r\n\r\nI do see that the new version has a slowdown factor of x10 while this is entirely threadsafe. (3 sec on master,30 secs for the new version).\r\n\r\nI do see a similar behavior with Sequences which are read-only structures and should not require any locking mechanism.\r\n\r\n", "Ah, ok I understand.  So the difference is the new code tries to be defensive and handle locking/synchronization internally, whereas your example handles locking in the user defined `next` function.  But you're right, locking the entire generator `next` means the threads can't parallelize expensive operations that are thread-safe within `next`.  If it's a precondition the user must handle locking themselves (if necessary) that changes things.  I was under the impression the Enqueuer classes were supposed to handle all synchronization.\r\n\r\nSo if I understood you correctly, I can remove the internal locking around the generator and it's up to the user to synchronize correctly in their `next`, if necessary?  Is this documented?  If not let's add that?", "@Dref360 PR on keras opened here: https://github.com/keras-team/keras/pull/11523", "Closing as the PR has moved to keras (which I'm still working on)"]}, {"number": 23010, "title": "TensorFlow Lite installation on Raspberry Pi", "body": "Hi,\r\n\r\nI've tried stackoverflow but thus far I've not had a reply - so feel free to close off is you feel this is unnecessary.\r\n\r\n[https://stackoverflow.com/questions/52804669/tensor-flow-lite-raspberry-pi-installation](url)\r\n\r\nI'm just looking from some clarity. I've build TF Lite on my RPi3 and the static lib's been created without issue.\r\n\r\nAt the moment python is returning the error:\r\n\r\n> Traceback (most recent call last): \r\n>    File \"label_image.py\", line 23, in\r\n>       <module> import tensorflow as tf\r\n\r\nWhat I'm uncertain of is if it pip install TF then how can I know/be certain that I will get/be using TF Lite as apposed to the full version?\r\n\r\nAlso are there any stats re: TF Lite on the Pi vs full version for image classification?\r\n\r\nThanks & Regards\r\n\r\nMark\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23009, "title": "Update feature_column.py", "body": "Allow multiple calls of input_layer to have the same root variable scope.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@yunjiangster Can you please sign the CLA to proceed with PR? Please let me know if you are facing any issues. Thanks!", "@yunjiangster Gentle reminder to sign CLA. We would like to see progress with this PR. Please let me know if you are facing any issues. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 23008, "title": "Removes reference to TFProf C++ API", "body": "As mentioned in #16821", "comments": []}, {"number": 23007, "title": "tf.estimator package not installed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.6.6\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Mobile device**: N/A\r\n- **Exact command to reproduce**: \r\n`pip install --upgrade tensorflow`\r\n`import tensorflow as tf` \r\n\r\n### Describe the problem\r\nI installed tensorflow via pip in a conda environment. Trying to `import tensorflow` results in a `tf.estimator package not installed.` error. I had a look a #22342 and checked my pandas version to be 0.23.4. \r\n\r\n```python\r\n>>> import tensorflow as tf\r\ntf.estimator package not installed.\r\n```\r\n\r\nInterestingly enough if I import pandas before importing tensorflow there is no error\r\n```python\r\n>>> import pandas as pd \r\n>>> import tensorflow as tf \r\n# No Error\r\n```\r\n\r\nOn an added note I can't import pandas after I import tensorflow \r\n```python\r\n>>> import tensorflow as tf\r\ntf.estimator package not installed.\r\n>>> import pandas as pd\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home1/irteam/anaconda3/envs/umap/lib/python3.6/site-packages/pandas/__init__.py\", line 42, in <module>\r\n    from pandas.core.api import *\r\n  File \"/home1/irteam/anaconda3/envs/umap/lib/python3.6/site-packages/pandas/core/api.py\", line 10, in <module>\r\n    from pandas.core.groupby.groupby import Grouper\r\n  File \"/home1/irteam/anaconda3/envs/umap/lib/python3.6/site-packages/pandas/core/groupby/__init__.py\", line 2, in <module>\r\n    from pandas.core.groupby.groupby import (\r\n  File \"/home1/irteam/anaconda3/envs/umap/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\", line 49, in <module>\r\n    from pandas.core.frame import DataFrame\r\n  File \"/home1/irteam/anaconda3/envs/umap/lib/python3.6/site-packages/pandas/core/frame.py\", line 74, in <module>\r\n    from pandas.core.series import Series\r\n  File \"/home1/irteam/anaconda3/envs/umap/lib/python3.6/site-packages/pandas/core/series.py\", line 67, in <module>\r\n    import pandas.core.ops as ops\r\nAttributeError: module 'pandas' has no attribute 'core'\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nMobile device", "@greyowl This is likely anaconda related. Make sure to install pandas, numpy, matplotlib and matplotlib at minimum if not entire scipy during anaconda setup. \r\n\r\nAlso, check whether GLIB version is up-to-date with Centos.\r\n\r\n\r\n", "same here", "```\r\njulia> using PyCall\r\njulia> pyimport(\"tensorflow\")\r\ntf.estimator package not installed.\r\nPyObject <module 'tensorflow' from '/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py'>\r\n```", "I did fix it reinstalling pandas after installing tensorflow-gpu, both from conda.", "I hit the exact same issue. Here are the steps I followed:\r\n1. Installed Anaconda3-5.3.0-Windws-x86_64 on Windows 10 PC.\r\n2. Downgrade python to 3.6.7 since python 3.7 is not supported yet.\r\n3. pip install tensorflow\r\n\r\n4. when do \"import tensorflow as tf\" in jupyter notebook, it complains \r\ntf.estimator package not installed.\r\ntf.estimator package not installed.\r\n\r\n5. when do \"import pandas as pd\", it complains the following. Even after I uninstalled tensorflow, it still has the same issue.\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-7dd3504c366f> in <module>\r\n----> 1 import pandas as pd\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py in <module>\r\n     40 import pandas.core.config_init\r\n     41 \r\n---> 42 from pandas.core.api import *\r\n     43 from pandas.core.sparse.api import *\r\n     44 from pandas.tseries.api import *\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\api.py in <module>\r\n      8 from pandas.core.dtypes.missing import isna, isnull, notna, notnull\r\n      9 from pandas.core.arrays import Categorical\r\n---> 10 from pandas.core.groupby.groupby import Grouper\r\n     11 from pandas.io.formats.format import set_eng_float_format\r\n     12 from pandas.core.index import (Index, CategoricalIndex, Int64Index,\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\__init__.py in <module>\r\n      1 # flake8: noqa\r\n----> 2 from pandas.core.groupby.groupby import (\r\n      3     Grouper, GroupBy, SeriesGroupBy, DataFrameGroupBy\r\n      4 )\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py in <module>\r\n     47                                CategoricalIndex, _ensure_index)\r\n     48 from pandas.core.arrays import ExtensionArray, Categorical\r\n---> 49 from pandas.core.frame import DataFrame\r\n     50 from pandas.core.generic import NDFrame, _shared_docs\r\n     51 from pandas.core.internals import BlockManager, make_block\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in <module>\r\n     72                                    create_block_manager_from_arrays,\r\n     73                                    create_block_manager_from_blocks)\r\n---> 74 from pandas.core.series import Series\r\n     75 from pandas.core.arrays import Categorical, ExtensionArray\r\n     76 import pandas.core.algorithms as algorithms\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py in <module>\r\n     65 from pandas.compat.numpy import function as nv\r\n     66 \r\n---> 67 import pandas.core.ops as ops\r\n     68 import pandas.core.algorithms as algorithms\r\n     69 \r\n\r\nAttributeError: module 'pandas' has no attribute 'core'", "Amazingly, the problem seemed have gone away after I did the following:\r\n- downgrade pandas from 0.23.4 to 0.23.0\r\n- upgrade matplotlib to 3.0.0", "Closing as this is resolved, free to reopen if problem persists.", "Hi, i've done :\r\ndowngrade pandas from 0.23.4 to 0.23.0\r\nupgrade matplotlib to 3.0.0\r\n\r\nissue still exist, exactly the same as \"HubberDev\" described X(", "Hey, Solved .\r\njust update matplotlib \ud83d\udc4d \r\nmatplotlib: 3.0.0-py36hd159220_0 --> 3.0.1-py36hc8f65d3_0 = fixed\r\n(panda remained 0.23.0)", "> Hey, Solved .\r\n> just update matplotlib \ud83d\udc4d\r\n> matplotlib: 3.0.0-py36hd159220_0 --> 3.0.1-py36hc8f65d3_0 = fixed\r\n> (panda remained 0.23.0)\r\n\r\nI can confirm this. \r\nYou still have the problem with matplotlib 3.0.0 but it's solved with 3.0.1!", "same problem \r\n     import tensorflow as tf\r\n     tf.estimator package not installed.\r\n     tf.estimator package not installed.", "I'm also seeing this with TensorFlow 1.11.  I'm building TensorFlow from source following the instructions at https://www.tensorflow.org/install/source", "my is tensorflow-gpu==1.12\uff0ctensorflow-estimator==1.13\uff0ci change  tensorflow-estimator==1.10.12\r\n", "> Hey, Solved .\r\njust update matplotlib \ud83d\udc4d\r\nmatplotlib: 3.0.0-py36hd159220_0 --> 3.0.1-py36hc8f65d3_0 = fixed\r\n(panda remained 0.23.0)\r\n\r\nI solve this problem the same way, like pandas need 3.0.1 too. Others can import pandas to see if it works.", "For me this had nothing to do with pandas or matplotlib. The issue was that I had installed a newer version of TF then downgraded. Uninstalling `tensorflow-estimator` and `tensorflow-gpu`, then reinstalling `tensorflow-gpu` fixed it.", "I fixed it by uninstalling `tensorflow-estimator` ", "_**pip uninstall tensorflow-estimator**_ worked for me", "**SOLUTION**\r\n```\r\npip install tensorflow==1.13.1\r\npip install tensorflow-estimator-1.13.0\r\n```\r\n\r\nEnsure no previous version of tensorflow-estimator is available apart from the above..To check use\r\n```\r\nconda list\r\npip uninstall tensorflow-estimator-x.xx.x\r\n```", "kennedyCzar's solution above worked for me.", "> _**pip uninstall tensorflow-estimator**_ worked for me\r\n\r\nIt works for me too ... "]}, {"number": 23006, "title": "Make api_compatibility_test pass in Python 3", "body": "I was (perhaps masochistically) curious what it would take.\r\n\r\nThis is a bunch of hacks, but it isn't that much overall code.  I leave it up to others whether it's worth ignoring, cleaning up, or adopting with minimal changes.  Main plus: nearly everyone outside Google uses Python 3, and this would make it easier for people to contribute.", "comments": ["Thank you for the fixes!", "To clarify: this doesn't remotely pass pylint, so it shouldn't be approved without some modifications.  Happy to make those modifications if we think syntax issues are the only problem, but I'd want explicit acknowledge that this sort of hackery is okay. :)", "@girving I left some comments including lint issues caught by \"Ubuntu Sanity\" build.\r\nIn addition to these comments, can you also remove `import unittest` in api_compatibility_test.py? It is no longer used with `@unittest.skipUnless` decorators removed.\r\n\r\nOtherwise, this change is ok. Indeed having these manually-specified replacements doesn't look very nice. But I don't see a better way to have this test work with both Python 2 and Python 3.", "Thanks for the comments!  Should be all resolved.  I agree that taking the hit once of updating the goldens to get code simplicity (and symmetry) is good.", "Thank you for fixing the test to work with python 3!", "You're welcome!", "There's a couple of failures. I haven't look too deeply, is this just a concurrent change or is this a difference to Py2 (maybe preferring more local classes in superclass lists?):\r\n\r\n```\r\n  path: \"tensorflow.train.MonitoredSession.StepContext\"\r\n  tf_class {\r\n-   is_instance: \"<class \\'google3.third_party.tensorflow.python.training.monitored_session.StepContext\\'>\"\r\n+   is_instance: \"<class \\'google3.third_party.tensorflow.python.training.monitored_session._MonitoredSession.StepContext\\'>\"\r\n```\r\n\r\nand \r\n\r\n```\r\n  path: \"tensorflow.train.SingularMonitoredSession.StepContext\"\r\n  tf_class {\r\n-   is_instance: \"<class \\'google3.third_party.tensorflow.python.training.monitored_session.StepContext\\'>\"\r\n+   is_instance: \"<class \\'google3.third_party.tensorflow.python.training.monitored_session._MonitoredSession.StepContext\\'>\"\r\n```\r\n\r\nand\r\n\r\n```\r\n  path: \"tensorflow.Variable.SaveSliceInfo\"\r\n  tf_class {\r\n-   is_instance: \"<class \\'google3.third_party.tensorflow.python.ops.variables.SaveSliceInfo\\'>\"\r\n+   is_instance: \"<class \\'google3.third_party.tensorflow.python.ops.variables.Variable.SaveSliceInfo\\'>\"\r\n```", "@martinwicke There's specific logic to fix those differences, which is why the tests pass in both Python 2 and Python 3: https://github.com/tensorflow/tensorflow/pull/23006/files#diff-66076c6ed184cb1cc7997df09d0b1c6fR50.  I'm not sure why the copybara test introduces new failures that the normal Python 2/3 unit tests don't catch."]}, {"number": 23005, "title": "tf.gradients vs. np.gradient", "body": "Please go to Stack Overflow f\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nIs there a way to get tensorflow to give the same gradient value as numpy when the relationship between two arrays is not known? For example:\r\n\r\nimport numpy as np\r\nx = np.linspace(0, 1, 10)\r\ny = np.linspace(1, 2, 10)\r\ngrad = np.gradient(y, x)\r\n\r\ngives [1., 1. ........1.].\r\n\r\nIs there a way to do this in tensorflow?\r\n\r\n", "comments": ["@NoOne69 In TensorFlow, tf.linspace() doesn't have gradient op, one option is to write your own gradient function then use RegisterGradient to register it as the gradient of linspace. \r\n\r\nAlternatively, linspace() between two variables always hold linear relationship so it is easily perceived that y=ax+b. Since b doesn't matter in gradients calculation, you can use `y=(y_end-y_start)/(x_end-x_start)x` then call tf.gradients() where _end and _start are the ranges in linspace().\r\n\r\n\r\n"]}, {"number": 23004, "title": "support dynamic shape for conv3d", "body": "Fix #16834, #15572, #15655, #15696, #16834, #20379, #22771.\r\n\r\nRelated PR: #21610, #15595\r\n\r\nBecause #22127 has been merged (thank @yongtang), we can fix the dynamic shape issue easily here.", "comments": ["@fchollet Hi, could you take another look? Thanks.", "@fchollet Friendly ping. Can you please take another look? Thanks!", "Nagging Assignee @ymodak: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@yongtang Can you please take a look at this PR? Thanks!", "Thanks, everybody :-)", "Hi, I don't see these changes in in v1.13-rc1 release, even though it seems to be merged more than a month ago.  are they going to appear in v1.13-rc2 ?\r\n\r\nthanks", "@myron I think you're right, because r1.13 is cut on 2018-12-13.  https://groups.google.com/a/tensorflow.org/forum/#!searchin/developers/1.13|sort:date/developers/VSlH3CZVDfQ/jb2CAmJzEAAJ\r\n\r\n@aselle Can you answer the question about rc2?", "I can see this merge is not included in 1.13-rc2 either.\r\n\r\nCan you please add this merge to the next rc, or 1.13 final release ??", "@myron Because the PR has been merged (and closed). Could you create an issue to request it for 1.13-rc2? ", "@facaiy the 1.13-rc2 is already released 6 days ago. \r\nDo you mean to create an issue to have it in 1.13-rc3 ? (I didn't realize we need a separate issue just to include the already merged updates)", "Yes, you have to create an issue for your request if necessary. I'm afraid that the feature will only released in the next version with high probability.  Sorry for the inconvenience. "]}, {"number": 23003, "title": "Problem on running tensorflow", "body": "Traceback (most recent call last):\r\n  File \"retrain.py\", line 132, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23002, "title": "Add Apache Arrow Support to TensorFlow Dataset", "body": "Apache Arrow is a standard format for in-memory columnar data. It provides a cross-language platform for systems to communicate and operate on data efficiently.\r\n\r\nAdding Arrow support in TensorFlow Dataset will allow systems to interface with TensorFlow in a well defined way, without the need to develop custom converters, serialize data, or write to specialized files.\r\n\r\nThis PR defines an Arrow base layer that will create a TensorFlow Dataset with an iterator over Arrow record batches to produce Tensor values for each column. This base layer is then extended to implement three Dataset Ops that consume Arrow record batches: 1) from Python memory / Pandas DataFrames, 2) Reading Arrow Feather files, 3) Input stream with a socket client to connect to a server streaming Arrow record batches. These are implemented now because they are straightforward and can provide some good initial functionality, but the design here should allow for more Arrow Ops in the future.\r\n\r\nfixes #23001 ", "comments": ["This is currently a WIP, but wanted to open the PR to open discussion.  Still needed:\r\n\r\n- [ ] Make sure all reasonable data types are supported\r\n- [ ] Tests are currently very simple, need to expand them\r\n- [ ] Socket impl is only enabled for posix\r\n- [ ] Clean up some rough edges", "@yongtang I saw that you are adding a Parquet Dataset in #19461. There is a lot of overlap since the parquet-cpp reader is based on Arrow. I believe that the parquet-cpp reader can produce Arrow record batches that could then leverage the base classes here that iterate over the batches. It would be nice to have a common Arrow layer in TensorFlow Dataset that is easy to extend for specific Ops. Would you be interested in collaborating?", "cc @frreiss @feihugis ", "@BryanCutler Yes it would be great to have a common layer for Arrow. PR #19461 has been approved, and is awaiting for bazel update (to 0.17.1) in #22449 to be merged. I think the arrow part could be consolidated once in.", "Add @dmitrievanthony for I/O related discussion.", "Thanks @yongtang. \r\n\r\nHi @BryanCutler, PR looks very promising! I see you implemented only Posix sockets so far, let me suggest you to have a look at Apache Ignite Dataset (#22210). In this dataset I implemented Windows sockets as well as Posix, it might be helpful. ", "> Yes it would be great to have a common layer for Arrow. PR #19461 has been approved, and is awaiting for bazel update (to 0.17.1) in #22449 to be merged. I think the arrow part could be consolidated once in.\r\n\r\nGreat, thanks @yongtang! I will keep an eye out for #19461 being merged and then we can think about consolidating it in this PR or a followup, if this one gets approved.", "> I see you implemented only Posix sockets so far, let me suggest you to have a look at Apache Ignite Dataset\r\n\r\nThanks @dmitrievanthony !  I would have liked to use boost asio, but I'm not sure if it's worth adding the dependency for that. If it's been done for Windows too in Apache Ignite Dataset, I'll take a look there and try to do the same.", "@BryanCutler The PR #19461 has boost dependency in place so it might be less of an issue for adding asio. Though boost dependency only works with bazel 0.17.1+ (that is also the reason merge of #19461 is blocked now).", "As a suggestion, you could consider making this a separate python package, which depends on both tensorflow and apache arrow to build. The tooling around these kinds of python packages with binary dependencies is still very new, but doable. Tensorflow *does* export its dataset header files. They are not under the API stability guarantees, but they have been fairly stable from what I observed (two method signatures changed trivially in 1.10, I believe). OF course, @mrry knows more, as the author.\r\n\r\nI myself have made my own Dataset in an external project, using cmake. I don't have a ready-made example, but I know that arrow use cmake as well, so it is very easily doable to have this live separately from the tensorflow repo. You can follow up with me about that.\r\n\r\nI mention this because tensorflow/contrib is going away.\r\n\r\nSuper cool work, by the way! I remember chatting with a Databricks engineer at the Spark Summit about how Arrow could be a great complement to Project Hydrogen.", "@BryanCutler See related discussion about `tensorflow/contrib` and dataset related`tensorflow/io`, that will likely happen in tensorflow 2.0: \r\n\r\nhttps://github.com/tensorflow/community/pull/18\r\n\r\nFYI @dmitrievanthony @mrry @ewilderj @martinwicke ", "Thanks for the info @galv and @yongtang, it sounds like a lot of things are in transition right now. Since `contrib` is going away, are no new additions being accepted there now?", "@BryanCutler When `tensorflow/contrib` is deprecated, Dataset related components could be moved to the planned `tensorflow/io` so it will not be a concern at that time. /cc @dmitrievanthony @mrry \r\n\r\nIn the short term though, I guess it may depend on the length of the transition period. If 2.0 is imminent then PR may not be accepted. (the PR could be opened to `tensorflow/io` then).\r\n\r\nDon't know the planned date of 2.0 yet. Maybe @martinwicke have some insight about the 2.0 schedule?\r\n", "We announced we wouldn't accept new projects into contrib when we announced 2.0. \r\n\r\nI think this is an obvious candidate for sig-io. @ewilderj how far are we with sig-io? I think we're looking for a date, right?\r\n\r\nMoving there will delay this for at least a few weeks as we have to set up the sig-io repo and integration. But if we were to accept this here, it would simply have to be moved to sig-io anyway, and I don't think there will be another TF release which could include this before sig-io is off the ground, so I think the extra work is not too useful.", "I created the mailing list for SIG IO (announcement on developers@ list) and am recruiting for interest for that group. Once we've got a few folks signed up we can kick off the SIG with a conference call and get things moving. ", "Thanks for the replies @ewilderj and @martinwicke. I signed up to the SIG and am looking forward to the discussions, thanks!", "Nagging Reviewer @mrry: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 39 days with no activity and the `awaiting review` label has been applied.", "I removed myself as reviewer, since it looks like this is going to go through the SIG IO process instead.", "I am not involved in sig-io\r\nI think you are looking for @yongtang ", "This PR has been moved to https://github.com/tensorflow/io/pull/36 and can be closed"]}, {"number": 23001, "title": "Add Support for Apache Arrow in TensorFlow Dataset", "body": "Apache Arrow is a standard format for in-memory columnar data. It provides a cross-language platform for systems to communicate and operate on data efficiently.\r\n\r\nAdding Arrow support in TensorFlow Dataset will allow systems to interface with TensorFlow in a well defined way, without the need to develop custom converters, serialize data, or write to specialized files.\r\n\r\nIt would be straightforward to add a base layer of Arrow support that works on Arrow record batches (a common struct for Arrow IPC) and extend that layer to support different kinds of Arrow Ops:\r\n\r\n* Python memory / Pandas DataFrames\r\n* Arrow Feather files\r\n* Parquet files\r\n* Socket / Pipes\r\n\r\nA slightly more involved Op could use Arrow Flight - Arrow-based messaging over gRPC.  Additionally, it would possible to define Ops to connect directly to other systems that can export Arrow data.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master branch\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.17.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.1\r\n- **GPU model and memory**: Quadro M1000M 4G\r\n- **Exact command to reproduce**: N/A\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I hope that [Rapids](https://rapids.ai/index.html) support tensorflow if Apache Arrow integration is planned. Currently, only PyTorch and Chainer seem to work with Apache Arrow. ", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Issue has been moved to https://github.com/tensorflow/io/issues/13"]}, {"number": 23000, "title": "Update the documentation for tf.fft", "body": "This fix is related to https://github.com/tensorflow/tensorflow/issues/17332#issuecomment-430029872. While complex128 has been added in tf, the documentation still says:\r\n```\r\ntf.spectral.fft\r\ninput: A Tensor. Must be one of the following types:\r\n    complex64, complex128. A complex64 tensor.\r\n```\r\n\r\nThis fix updates the documentation so that it matches the actual behavior.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": ["Nagging Reviewer @martinwicke: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@martinwicke Can you please take a look at this PR? Thanks!"]}, {"number": 22999, "title": "Tensor is not an element of this graph.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10, Version 10.0.17134.285.\r\n- **TensorFlow installed from (source or binary)**: Binary.\r\n- **TensorFlow version (use command below)**: b'v1.11.0-rc2-4-gc19e29306c' 1.11.0.\r\n- **Python version**: Python 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD64)] on win32.\r\n- **CUDA/cuDNN version**: CUDA V9.0.176, cuDNN v7.3.1.\r\n- **GPU model and memory**: Nvidia GTX 970 4 GB.\r\n- **Bazel version**: N/A.\r\n- **Mobile platform**: N/A.\r\n- **Exact command to reproduce**: `python test.py training_dry.wav training_wet.wav validation_dry.wav validation_wet.wav`\r\n\r\n### Description\r\n\r\nI'm probably doing something horribly wrong with regard to graph/session handling. I tried to copy examples from the documentation but my code is still failing with the following error:\r\n\r\n```\r\nC:\\Users\\Markov\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\scipy\\io\\wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\r\n  WavFileWarning)\r\n2018-10-16 00:11:07.807782: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-10-16 00:11:08.164098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:\r\nname: GeForce GTX 970 major: 5 minor: 2 memoryClockRate(GHz): 1.253\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.31GiB\r\n2018-10-16 00:11:08.172047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-10-16 00:11:09.229492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-10-16 00:11:09.234965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0\r\n2018-10-16 00:11:09.237217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N\r\n2018-10-16 00:11:09.240415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3023 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\nCommencing training.\r\nIteration 1\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Markov\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1050, in _run\r\n    subfeed, allow_tensor=True, allow_operation=False)\r\n  File \"C:\\Users\\Markov\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3488, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"C:\\Users\\Markov\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3567, in _as_graph_element_locked\r\n    raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\r\nValueError: Tensor Tensor(\"Const:0\", shape=(24085052,), dtype=float32) is not an element of this graph.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 92, in <module>\r\n    train(dry_training_wav, wet_training_wav, dry_validation_wav, wet_validation_wav)\r\n  File \"train.py\", line 66, in train\r\n    run_operation(dry_training_wav, wet_training_wav, batch_size, minimize, session)\r\n  File \"train.py\", line 26, in run_operation\r\n    operation_output = session.run(operation, feed)\r\n  File \"C:\\Users\\Markov\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\Markov\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1053, in _run\r\n    'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\r\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Const:0\", shape=(24085052,), dtype=float32) is not an element of this graph.\r\n```\r\n\r\n### Reproduction\r\n```\r\nimport sys\r\n\r\nimport scipy.io.wavfile\r\nimport tensorflow as tf\r\n\r\ndef read_wav(path):\r\n\trate, data = scipy.io.wavfile.read(path)\r\n\treturn tf.convert_to_tensor(data, dtype = tf.float32)\r\n\r\ndef get_batch(data, offset, batch_size):\r\n\treturn data[offset : offset + batch_size]\r\n\r\ndef get_length(tensor):\r\n\treturn tensor.shape[0].value\r\n\r\ndef run_operation(dry_data, wet_data, batch_size, operation, session):\r\n\toffset = 0\r\n\toutput = []\r\n\twhile offset + batch_size < get_length(dry_training_wav):\r\n\t\tdry_batch = get_batch(dry_data, offset, batch_size)\r\n\t\twet_batch = get_batch(wet_data, offset, batch_size)\r\n\t\tfeed = {\r\n\t\t\tdry_data: dry_batch,\r\n\t\t\twet_data: wet_batch\r\n\t\t}\r\n\t\toperation_output = session.run(operation, feed)\r\n\t\toutput.append(operation_output)\r\n\t\toffset += batch_size\r\n\treturn output\r\n\r\ndef get_graph():\r\n\tgraph = tf.Graph()\r\n\twith graph.as_default():\r\n\t\tframe_count = 96\r\n\t\tlstm_layers = 64\r\n\r\n\t\tframe_shape = [frame_count]\r\n\r\n\t\tdry_data = tf.placeholder(tf.float32, frame_shape, 'dry_data')\r\n\t\twet_data = tf.placeholder(tf.float32, frame_shape, 'wet_data')\r\n\r\n\t\tlstm = tf.contrib.cudnn_rnn.CudnnLSTM(lstm_layers, frame_count)\r\n\t\treshaped_dry_data = tf.reshape(dry_data, [1, 1, frame_count])\r\n\t\tlstm_output, _ = lstm(reshaped_dry_data)\r\n\t\tflat_lstm_output = tf.reshape(lstm_output, frame_shape)\r\n\t\tprediction = tf.nn.elu(flat_lstm_output)\r\n\r\n\t\tloss = tf.sqrt(tf.losses.mean_squared_error(prediction, wet_data), name = 'loss')\r\n\t\toptimizer = tf.train.AdamOptimizer()\r\n\t\tminimize = optimizer.minimize(loss, name = 'minimize')\r\n\treturn graph\r\n\r\ndef train(dry_training_wav, wet_training_wav, dry_validation_wav, wet_validation_wav):\r\n\tgraph = get_graph()\r\n\twith tf.Session(graph = graph) as session:\r\n\t\tinitializer = tf.global_variables_initializer()\r\n\t\tsession.run(initializer)\r\n\t\titeration = 1\r\n\t\tdry_data_placeholder = graph.get_operation_by_name('dry_data')\r\n\t\tbatch_size = dry_data_placeholder.outputs[0].shape[0].value\r\n\t\tloss = graph.get_operation_by_name('loss')\r\n\t\tminimize = graph.get_operation_by_name('minimize')\r\n\t\tprint('Commencing training.')\r\n\t\twhile True:\r\n\t\t\tprint(f'Iteration {iteration}')\r\n\t\t\trun_operation(dry_training_wav, wet_training_wav, batch_size, minimize, session)\r\n\t\t\tlosses = run_operation(dry_validation_wav, wet_validation_wav, batch_size, loss, session)\r\n\t\t\tvalidation_loss = sum(losses)\r\n\t\t\tprint(f'Validation: {validation_loss}')\r\n\t\t\titeration += 1\r\n\r\nif len(sys.argv) != 5:\r\n\tprint('Usage:')\r\n\tprint(f'{sys.argv[0]} <dry training WAV file> <wet training WAV file> <dry validation WAV file> <wet validation WAV file>')\r\n\tsys.exit(1)\r\n\r\ndry_training_wav_path = sys.argv[1]\r\nwet_training_wav_path = sys.argv[2]\r\n\r\ndry_validation_wav_path = sys.argv[3]\r\nwet_validation_wav_path = sys.argv[4]\r\n\r\ndry_training_wav = read_wav(dry_training_wav_path)\r\nwet_training_wav = read_wav(wet_training_wav_path)\r\n\r\ndry_validation_wav = read_wav(dry_validation_wav_path)\r\nwet_validation_wav = read_wav(wet_validation_wav_path)\r\n\r\nif get_length(dry_training_wav) != get_length(wet_training_wav) or get_length(dry_validation_wav) != get_length(wet_validation_wav):\r\n\traise Exception('Dry and wet WAVs must be same length.')\r\n\r\ntrain(dry_training_wav, wet_training_wav, dry_validation_wav, wet_validation_wav)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce\nMobile device", "I added the missing fields but the command probably wouldn't help you much, unless you added some bogus WAV files to run my test script with. I doubt this a bug. I'm afraid I just don't fully grasp the graph/session documentation.\r\n\r\nWhy is it rejecting my input tensor? Isn't this the right way to feed input into the graph for execution on the GPU?\r\n\r\n- **Bazel version**: N/A.\r\n- **Mobile platform**: N/A.\r\n- **Exact command to reproduce**: `python test.py training_dry.wav training_wet.wav validation_dry.wav validation_wet.wav`\r\n\r\n", "Figured it out. There are several horrible mistakes in that code. First off, you're not supposed to create tensors outside the session context, second, those don't go into the feed dictionary anyway, from what I gathered, third, I confused JavaScript object notation with Python dictionary syntax, causing me to use PCM frames rather than placeholders from the graph as feed keys. Humiliating. Ouch."]}, {"number": 22998, "title": "[tf.keras] Remove ominous load warning when using .hdf5 extension", "body": "**Description**: See issue - https://github.com/tensorflow/tensorflow/issues/22947\r\n\r\ncc @allenlavoie \r\n\r\n", "comments": []}, {"number": 22997, "title": "Undefined reference to stream_executor::Stream::ThenBlasGemm in _gru_ops.so", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Adjustments for build\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux <HOST> 2.6.32-573.7.1.el6.centos.plus.x86_64 #1 SMP Wed Sep 23 03:02:55 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source git rev: ac7b84de8803edbb2d4da573b3f8704e9fad8fa8\r\n- **TensorFlow version (use command below)**: b'v1.9.0-rc2-5328-gac7b84d' 1.11.0-rc1\r\n- **Python version**: Python 3.6.6 :: Anaconda, Inc.\r\n- **Bazel version (if compiling from source)**: bazel-0.17.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc version 4.9.3 (GCC)\r\n- **CUDA/cuDNN version**: cuda-9.0, libcudnn.so.7.3.1\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: `python -c \"import tensorflow.contrib`\r\n\r\n\r\n\r\n### Describe the problem\r\n\r\nProblem seems similar to the now closed #19840\r\n\r\n#### Modifications to Tensorflow source\r\nFrom the checked out source I made the following modifications:\r\n\r\n- added `-lrt` to linkopts  in `tensorflow.bzl` for compatibility with CentOS following https://github.com/tensorflow/tensorflow/issues/15129\r\n\r\n- Added `use_default_shell_env = True to `ctx.action()` in  in `tensorflow.bzl` for problems with swig following https://github.com/aiqu/devsetting/commit/f927890e05e06382ef1606decf38f192a75bac71\r\n\r\n- compiled with --monolithic because of an issue with `load_library` throwing a `ByteSizeConsistencyError` when serializing `_bigtable.so` perhaps among others when running on my compute cluster vs. workstation where tensorflow is compiled.\r\n\r\n- changed the paths for `gcc` to point to `v4.9` installed in `~/opt` in various `CROSSTOOL` config files.\r\n\r\nHere is a [gist](https://gist.github.com/momeara/2edb2d606ea9490d3bbb358ed2b57855) of the diffs.\r\n\r\n\r\n#### Set up the environment\r\n```\r\nexport CC=/mnt/nfs/home/momeara/opt/bin/gcc\r\nexport CXX=/mnt/nfs/home/momeara/opt/bin/g++\r\nexport PYTHON_BIN_PATH=/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/bin/python\r\nexport PYTHON_LIB_PATH=/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages\r\nexport TF_NEED_AWS=0\r\nexport TF_NEED_KAFKA=0\r\nexport TF_NEED_NGRAPH=0\r\nexport TF_NEED_IGNITE=0\r\nexport TF_NEED_ROCM=0\r\nexport TF_NEED_TENSORRT=0\r\nexport TF_NEED_JEMALLOC=0\r\nexport TF_NEED_GCP=0\r\nexport TF_NEED_HDFS=0\r\nexport TF_NEED_S3=0\r\nexport TF_ENABLE_XLA=0\r\nexport TF_NEED_GDR=0\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport TF_NCCL_VERSION=\"2.3\"\r\nexport NCCL_INSTALL_PATH=/mnt/nfs/ex9/work/momeara/collaborations/DeepSEA/nccl/nccl_2.3.5-2+cuda9.0_x86_64\r\nexport TF_SET_ANDROID_WORKSPACE=0\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_VERSION=10.0\r\nexport CUDA_TOOLKIT_PATH=/nfs/soft/cuda-10.0\r\nexport CUDA_HOME=/nfs/soft/cuda-10.0\r\nexport TF_CUDNN_VERSION=7.3.1\r\nexport CUDNN_INSTALL_PATH=/mnt/nfs/ex9/work/momeara/collaborations/DeepSEA/cuda\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=5.2\r\nexport TF_CUDA_CLANG=0\r\nexport TF_NEED_MPI=0\r\nexport GCC_HOST_COMPILER_PATH=/mnt/nfs/home/momeara/opt/bin/gcc\r\nexport CC_OPT_FLAGS='-march=native'\r\n# path to /nfs/soft is for /nfs/soft/cuda/include/cudnn.h\r\nexport CPLUS_INCLUDE_PATH=/mnt/nfs/home/momeara/opt/include:/nfs/soft:/nfs/soft/cuda/include\r\nexport C_INCLUDE_PATH=/mnt/nfs/home/momeara/opt/include:/nfs/soft:/nfs/soft/cuda/include\r\nexport LIBRARY_PATH=/nfs/soft/cuda-10.0/lib64:/nfs/soft/cuda-10.0/extras/CUPTI/lib64:/mnt/nfs/home/momeara/opt/lib:/mnt/nfs/home/momeara/opt/lib64:/nfs/soft/cuda/nfs/soft/cuda/lib64\r\nexport LD_LIBRARY_PATH=/nfs/soft/cuda-10.0/lib64:/nfs/ex9/work/momeara/collaborations/DeepSEA/cuda/lib64:/nfs/soft/cuda-10.0/extras/CUPTI/lib64:/mnt/nfs/home/momeara/opt/lib:/mnt/nfs/home/momeara/opt/lib64:/nfs/soft/cuda/nfs/soft/cuda/lib64\r\nexport PATH=/nfs/soft/cuda-10.0/bin:/mnt/nfs/ex9/work/momeara/collaborations/DeepSEA/tensorflow/bazel-0.17.1/output:/nfs/home/momeara/opt/ncbi-blast-2.2.30+/bin:/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/bin:/nfs/ex9/work/momeara/tools/anaconda3/bin:/mnt/nfs/home/momeara/.local/bin:/mnt/nfs/home/momeara/opt/node-v4.5.0-linux-x64/bin:/mnt/nfs/home/momeara/opt/bin:/usr/lib64/qt-3.3/bin:/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/bin:/bin:/usr/bin\r\n```\r\n\r\n\r\n#### Build and install `tensorflow`\r\n```\r\n./configure\r\nbazel \\\r\n  --output_user_root=/nfs/ex9/work/momeara/collaborations/DeepSEA/tensorflow/.cache \\\r\n  build \\\r\n  --config=cuda \\\r\n  --config=opt \\\r\n  --config=monolithic \\\r\n  //tensorflow/tools/pip_package:build_pip_package\r\n\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package \\\r\n  --src /scratch/momeara/tensorflow_pkg_dbg_monolithic_src \\\r\n  /scratch/momeara/tensorflow_pkg_dbg_monolithic\r\npip install --upgrade /scratch/momeara/tensorflow_pkg_dbg_monolithic/tensorflow-*.whl\r\n\r\npython -c \"import tensorflow.contrib\"\r\n```\r\ngives this error (see below for full backtrace)\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: /nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/_gru_ops.so: undefined symbol: _ZN15stream_executor6Stream12ThenBlasGemmENS_4blas9TransposeES2_yyyfRKNS_12DeviceMemoryIfEEiS6_ifPS4_i\r\n```\r\n\r\n#### Differential testing\r\n- The same problem only arises with the `--config=monolithic` flags.\r\n- Including debug flags `-c dbg --copt=-DNDEBUG --strip=never` makes no difference.\r\n\r\n### Source code / logs\r\n\r\n```\r\npython -c \"import tensorflow.contrib\"\r\n# ...generic warnings...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 45, in <module>\r\n    from tensorflow.contrib import cudnn_rnn\r\n  File \"/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/__init__.py\", line 34, in <module>\r\n    from tensorflow.contrib.cudnn_rnn.python.layers import *\r\n  File \"/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/__init__.py\", line 23, in <module>\r\n    from tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import *\r\n  File \"/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 20, in <module>\r\n    from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\r\n  File \"/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 22, in <module>\r\n    from tensorflow.contrib.rnn.python.ops import lstm_ops\r\n  File \"/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/rnn/__init__.py\", line 92, in <module>\r\n    from tensorflow.contrib.rnn.python.ops.gru_ops import *\r\n  File \"/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/gru_ops.py\", line 33, in <module>\r\n    resource_loader.get_path_to_datafile(\"_gru_ops.so\"))\r\n  File \"/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 60, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/_gru_ops.so: undefined symbol: _ZN15stream_executor6Stream12ThenBlasGemmENS_4blas9TransposeES2_yyyfRKNS_12DeviceMemoryIfEEiS6_ifPS4_i\r\n```\r\n\r\nFor what it's worth here the symbols in `_gru_ops.so` missing containing `stream_executor`:\r\n\r\n```\r\nnm -C -u /nfs/ex9/work/momeara/tools/anaconda3/envs/DeepSEA/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/_gru_ops.so | grep stream_executor                                                                                                \r\n                 U stream_executor::Stream::ThenBlasGemm(stream_executor::blas::Transpose, stream_executor::blas::Transpose, unsigned long long, unsigned long long, unsigned long long, double, stream_executor::DeviceMemory<double> const&, int, stream_executor::DeviceMemory<double> const&, int, double, stream_executor::DeviceMemory<double>*, int)\r\n                 U stream_executor::Stream::ThenBlasGemm(stream_executor::blas::Transpose, stream_executor::blas::Transpose, unsigned long long, unsigned long long, unsigned long long, float, stream_executor::DeviceMemory<float> const&, int, stream_executor::DeviceMemory<float> const&, int, float, stream_executor::DeviceMemory<float>*, int)\r\n```\r\n", "comments": ["Oh thank you @momeara for filling the issue. \r\nI just wanted to do it by myself. \r\nWe have the same problem in our  home-compiled TF since the 1.9.rc0 or rc1.\r\nBTW I think the commit introduced it is https://github.com/tensorflow/tensorflow/commit/80fc661853f9a0844faf95eb68438dc85a5879e3#diff-06268ec57b0c7871d7e7cab011778559\r\nand there is a related issue that was closed with no resolution https://github.com/tensorflow/tensorflow/issues/19840", "Same issue with TF1.11, with/without monolithic\r\nCUDA 9.2\r\nCUDNN 7.1.4\r\nRHEL 7.3\r\nPower 8\r\ngcc (GCC) 4.8.5\r\nbazel-0.18.0\r\n...../tensorflow/contrib/rnn/python/ops/_gru_ops.so: undefined symbol: _ZN15stream_executor6Stream12ThenBlasGemmENS_4blas9TransposeES2_yyydRKNS_12DeviceMemoryIdEEiS6_idPS4_i\r\n", "Actually @agansa I managed to get around it. \r\nThe problem is it if you don't specify your `--config` it's monolithic by default.\r\nCheck my building command here:\r\n```\r\nbazel build \\\r\n            --config=opt \\\r\n            --copt=-msse4.2 \\\r\n            --copt=-mavx \\\r\n            --copt=-mavx2 \\\r\n            --copt=-mfma \\\r\n            --copt=-O3 \\\r\n            //tensorflow/tools/pip_package:build_pip_package\r\n```", "I tried with --config=opt\r\nthe other options are not available but it gives the same error.", "As @eryshev suggested, I tried with building with the parent of 80fc661, and it looks like it may not have the issue.", "Looks like this is resolved by suggestion from @eryshev. You can also use the latest TensorFlow version or leverage docker "]}, {"number": 22996, "title": "tensboard not working on google colab   it shows a error :-No dashboards are active for the current data set.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@sundberg You would need to direct traffic to localhost for now, [ngrok](https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab) could be an option among others."]}, {"number": 22995, "title": "Model is still floating point after Post-training quantization ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nAndroid 7\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nSamsung Galaxy S6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11.0 on PC, 'org.tensorflow:tensorflow-lite:0.0.0-nightly' on Android \r\n- **Python version**: 2.7.15\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: tfLite.run(imgData, labelProb);\r\n\r\n### Describe the problem\r\nAfter applying post-training quantization, my custom CNN model was shrinked to 1/4 of its original size (from 56.1MB to 14MB). I put the image(100x100x3) that is to be predicted into ByteBuffer as 100x100x3=30,000 bytes. However, I got the following error during inference:\r\n\r\n```\r\n**java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 120000 bytes and a ByteBuffer with 30000 bytes.**\r\n        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:221)\r\n        at org.tensorflow.lite.Tensor.setTo(Tensor.java:93)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:136)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:216)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:195)\r\n        at gov.nih.nlm.malaria_screener.imageProcessing.TFClassifier_Lite.recongnize(TFClassifier_Lite.java:102)\r\n        at gov.nih.nlm.malaria_screener.imageProcessing.TFClassifier_Lite.process_by_batch(TFClassifier_Lite.java:145)\r\n        at gov.nih.nlm.malaria_screener.Cells.runCells(Cells.java:269)\r\n        at gov.nih.nlm.malaria_screener.CameraActivity.ProcessThinSmearImage(CameraActivity.java:1020)\r\n        at gov.nih.nlm.malaria_screener.CameraActivity.access$600(CameraActivity.java:75)\r\n        at gov.nih.nlm.malaria_screener.CameraActivity$8.run(CameraActivity.java:810)\r\n        at java.lang.Thread.run(Thread.java:762) \r\n```\r\n\r\nThe imput image size to the model is: 100x100x3. I'm currently predicting one image at a time. So, if I'm making the Bytebuffer: 100x100x3 = 30,000 bytes. However, the log info above says the TensorFlowLite buffer has 120,000 bytes. This makes me suspect that the converted tflite model is still in float format. Is this expected behavior? How can I get a quantized model that take input image in 8 pit precision like it does in the [example](https://github.com/tensorflow/tensorflow/blob/307c83106445ab2c52847f08d35a66c51aff19d9/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java#L163) from TensorFlow official repository ?\r\n\r\nIn the example code, the ByteBuffer used as input for tflite.run() is in 8 bit precision for the quantized model. \r\nBut I also read from the google doc saying, \"At inference, weights are converted from 8-bits of precision to floating-point and computed using floating point kernels.\" This two instances seems to contradict each other. \r\n\r\n### Source code / logs\r\n\r\n```\r\nprivate static final int BATCH_SIZE = 1;\r\n\r\nprivate static final int DIM_IMG_SIZE = 100;\r\n\r\nprivate static final int DIM_PIXEL_SIZE = 3;\r\n\r\nprivate static final int BYTE_NUM = 1;\r\n\r\nimgData = ByteBuffer.allocateDirect(BYTE_NUM * BATCH_SIZE * DIM_IMG_SIZE * DIM_IMG_SIZE * DIM_PIXEL_SIZE);\r\nimgData.order(ByteOrder.nativeOrder());\r\n\r\n... ...\r\n\r\nint pixel = 0;\r\n\r\n        for (int i = 0; i < DIM_IMG_SIZE; ++i) {\r\n            for (int j = 0; j < DIM_IMG_SIZE; ++j) {\r\n\r\n                final int val = intValues[pixel++];\r\n\r\n                imgData.put((byte)((val >> 16) & 0xFF));\r\n                imgData.put((byte)((val >> 8) & 0xFF));\r\n                imgData.put((byte)(val & 0xFF));\r\n\r\n//                imgData.putFloat(((val >> 16) & 0xFF) / 255.0f);\r\n//                imgData.putFloat(((val >> 8) & 0xFF) / 255.0f);\r\n//                imgData.putFloat((val & 0xFF) / 255.0f);\r\n\r\n            }\r\n        }\r\n\r\n... ...\r\n\r\ntfLite.run(imgData, labelProb);\r\n```\r\n\r\nPost-training quantization code:\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\nimport os\r\n\r\nsaved_model_dir = '/home/yuh5/Downloads/malaria_thinsmear.h5.pb'\r\n\r\ninput_arrays = [\"input_2\"]\r\n\r\noutput_arrays = [\"output_node0\"]\r\n\r\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(saved_model_dir, input_arrays, output_arrays)\r\n\r\nconverter.post_training_quantize = True\r\n\r\ntflite_model = converter.convert()\r\nopen(\"thinSmear_100.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n", "comments": ["If I'm not mistaken, it's an expected behavior because according to the documentation (https://www.tensorflow.org/performance/post_training_quantization)\r\n\"At inference, weights are converted from 8-bits of precision to floating-point and computed using floating point kernels. This conversion is done once and cached to reduce latency. \". So it comes out that the model should still be treated as floating point model.", "@jazzystring1 I read that part too. But if you look at the example from TensorFlow github repo that I referred to in my post. You can see that for quantized model, the example [code](https://github.com/tensorflow/tensorflow/blob/63baef56878bda2e1e631855513221c7d5b36018/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java#L164) indicates that the input to tflite.run() is expected to be in 8-bit precision.", "@Yu-Hang : it is as jazzystring1@ suggested. With post-training quantization, the inputs and outputs continue to be float32. \r\n\r\nThe code you pointed to in the example demo app is using a \"fully-quantized\" models, created with the process described [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/README.md) as opposed to post-training quantization.\r\n\r\nI think this is somewhere where we could improve the documentation around the demo, now that there are more variants to how we do quantization.\r\n\r\n", "See this issue, where other users faced the same issue as you. https://github.com/tensorflow/tensorflow/issues/22535", "@alanchiao I see. Thanks a lot for the answer. Hope you guys can update the doc soon to make it more clear so others won't run into the same issue again.", "@Yu-Hang Hi, can this issue be closed now ?", "Yes, thanks for the help!"]}, {"number": 22994, "title": "Tensorflow freeze_graph.py: NodeDef mentions attr 'Truncate' not in Op", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs High Sierra, MacBook Pro, 3.1 GHz Intel Core i5, 8 GB 2133 MHz LPDDR3, Intel Iris Plus Graphics 650 1536 MB\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow==1.10\r\n- **TensorFlow version (use command below)**: v1.10.0-rc1-19-g656e7a2b34 1.10.0\r\n- **Python version**: python 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: \r\npython3 /Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py --input_graph=/Users/duckhahwang/Downloads/resnet_imagenet_v2_20180928/graph.pbtxt --input_binary=false --input_checkpoint=/Users/duckhahwang/Downloads/resnet_imagenet_v2_20180928/model.ckpt-56286 --input_binary=false --output_graph=/Users/duckhahwang/Downloads/frozen_graph.pb --output_node_names=softmax\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI downloaded the pre-trained tensorflow model (ResNet-50 v2 (fp32)) from the below link\r\n\r\nhttp://download.tensorflow.org/models/official/20181001_resnet/checkpoints/resnet_imagenet_v2_fp32_20181001.tar.gz\r\n\r\nAnd using freeze_graph.py to freeze the pre-trained model. but I received the error message and not working. \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nDUCKHAs-MacBook-Pro:site-packages duckhahwang$ python3 /Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py --input_graph=/Users/duckhahwang/Downloads/resnet_imagenet_v2_20180928/graph.pbtxt --input_binary=false --input_checkpoint=/Users/duckhahwang/Downloads/resnet_imagenet_v2_20180928/model.ckpt-56286 --input_binary=false --output_graph=/Users/duckhahwang/Downloads/frozen_graph.pb --output_node_names=softmax\r\n\r\n/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.23) or chardet (3.0.4) doesn't match a supported version!\r\n  RequestsDependencyWarning)\r\nTraceback (most recent call last):\r\n  File \"/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 418, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: resnet_model/conv2d/kernel_cast = Cast[DstT=DT_HALF, SrcT=DT_FLOAT, Truncate=false, _output_shapes=[[7,7,3,64]]](resnet_model/conv2d/kernel_cast/ReadVariableOp). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 382, in <module>\r\n    run_main()\r\n  File \"/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 379, in run_main\r\n    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 378, in <lambda>\r\n    my_main = lambda unused_args: main(unused_args, flags)\r\n  File \"/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 272, in main\r\n    flags.saved_model_tags, checkpoint_version)\r\n  File \"/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 254, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 99, in freeze_graph_with_def_protos\r\n    _ = importer.import_graph_def(input_graph_def, name=\"\")\r\n  File \"/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/duckhahwang/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 422, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: resnet_model/conv2d/kernel_cast = Cast[DstT=DT_HALF, SrcT=DT_FLOAT, Truncate=false, _output_shapes=[[7,7,3,64]]](resnet_model/conv2d/kernel_cast/ReadVariableOp). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).", "comments": ["@personableduck Hi, I suspect this issue is due to the difference in the tensorflow versions between trained model and the tensorflow version which you are using. Please install a lower version of tensorflow(to make sure both the tf versions are same) and reverify. Thank you !", "Same issue here using TF 1.8. How does one tell which version a frozen graph is saved in?", "@allenlavoie  - PTAL", "I have the same issue with TF 1.9.0", "You can used TF 1.11 and abpve , which will fix the issue.", "Also try to load the model from same version in which you created the frozen graph,that might solve this", "Hi, I am getting similar issue with resnet-101. Frozen graph is created on same version that it is run on TF 1.13.\r\nValueError: NodeDef mentions attr 'explicit_paddings' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=dilations:list(int),default=[1, 1, 1, 1]>; NodeDef:\r\n{{node resnet_v1_101/conv1/Conv2D}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.)./home/t/MLxBench/Modules/Deep-Learning/workloads/resnet101/bin/run_task.py:37: UserWarning: genfromtxt: Empty input file: \"/home/t/MLxBench/Modules/Deep-Learning/workloads/resnet101/result/output/resnet101_batch_1_fp32_concurrent_instance0.csv\"\r\n  csv_data = genfromtxt(path, delimiter=',')", "Same issue here, with TF 1.13 . I also tried to train with the same version, training on a Windows machine and predicting on a Linux machine, but it gives me the error above", "Same issue here too!", "I resolved, there was actually a version mismatch between the training system and the prediction system", "Same issue. Version is same.", "I have the same issue!!\r\nPS: I'v used the same version while training and using the trained model !!", "Any updates on this issue ?\r\nI too have same version of tensorflow, still getting error ??", "Unassigning myself since I have no plans to work on this. Freezing is not an operation we're planning to support going forward.", "same issue", "Same issue even though I am using the same version for freezing and loading.\r\nAny updates??", "i am also getting same error even though version is same \r\nhow can i solve this problem ?", "In some cases you can do remove_attribute(attribute_name=some_attr)  before to strip unused and unsupported attr, but it has to be unused by tf and you should verify it", "I also get this error when I train and freeze the model using `1.14`, but works if train and freeze using `1.13.1`", "I also get this error when I train and freeze the model using 1.14, but works if train and freeze using 1.13.1", "> I also get this error when I train and freeze the model using 1.14, but works if train and freeze using 1.13.1\r\n\r\nMe too", "Same issue on tf 1.10 and tf.14. It works with tf 1.4, but this version doesn't have other features I require.", "Noting this https://github.com/tensorflow/serving/issues/831#issuecomment-378986705\r\n\r\n", "Am also experiencing this on 1.14.0 but not 1.13.1\r\n\r\nRemoving the attributes it complains about with `remove_attribute(attribute_name=some_attr)` like suggested above gets it to work. Unclear whether it affects model output though...\r\n\r\nedit: Turns out it was actually loading an old libtensorflow_framework.so, deleting it solved the problem but the above workaround still seemed to work.", "@somewacko @seovchinnikov Hello guys. Please how do you remove_attribute? Do you do that on the generated graph (model.pb) in python? Or you do that on Android? I have checked and I have not found how to do that.\r\nPlease can you detail more how to remove attribute? I am using tensorflow 2.3.0\r\nThanks", "Hello, guys. I'm having the same issue but saying \"NodeDef mentions attr 'allowed_devices' not in Op'. Does anyone knows how to fix it? I'm trying to deploy my model to AI Platform but no success. Using Tensorflow 2.3.0", "> \r\n> \r\n> @somewacko @seovchinnikov Hello guys. Please how do you remove_attribute? Do you do that on the generated graph (model.pb) in python? Or you do that on Android? I have checked and I have not found how to do that.\r\n> Please can you detail more how to remove attribute? I am using tensorflow 2.3.0\r\n> Thanks\r\n\r\nHello @gedji did you solve the problem? I'm having a similar issue ", "Hello @JonasJunnior0310 no i have not. Have you found something that can help?", "@gedji Not yet. I will downgrade my tensorflow to 2.1 and check if it is still an issue.", "Ok. Pls let me know if that works for you", "@gedji It did work. I downgraded to 2.1 and it worked", "@JonasJunnior0310 I wi try also what you did to see if that's work for me\r\n\r\nThx", "@personableduck , You can switch to Tensorflow 2 version, in Tensorflow 2 primary export of model is done using [savedmodel](https://www.tensorflow.org/guide/saved_model) and API is designed to support the same. You can find the pretrained model for `ResNet50V2` [here](https://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50V2) and follow [this](https://www.tensorflow.org/tutorials/images/transfer_learning) transfer learning method to implement on your data.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22994\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22994\">No</a>\n"]}, {"number": 22993, "title": "TOCO failed see console for info. ", "body": "INFO:\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\nTensorFlow installed from (source or binary): Anaconda \r\nTensorFlow version (use command below): 1.11\r\nPython version: 3.5\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce:\r\n\r\nHi, \r\n I've used the example given in [converter python api guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/convert/python_api.md)\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Dense(2, input_shape=(3,)))\r\nmodel.add(tf.keras.layers.RepeatVector(3))\r\nmodel.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3)))\r\nmodel.compile(loss=tf.keras.losses.MSE,\r\n              optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\r\n              metrics=[tf.keras.metrics.categorical_accuracy],\r\n              sample_weight_mode='temporal')\r\nx = np.random.random((1, 3))\r\ny = np.random.random((1, 3, 3))\r\nmodel.train_on_batch(x, y)\r\nmodel.predict(x)\r\nkeras_file = \"keras_model.h5\"\r\ntf.keras.models.save_model(model, keras_file)\r\nconverter = tf.contrib.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\ntflite_model = converter.convert()`\r\n\r\n\r\nI ran this code and this is the error I get:\r\n\r\n> RuntimeError: TOCO failed see console for info.\r\nb'Traceback (most recent call last):\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Roaming\\\\Python\\\\Python35\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 18, in swig_import_helper\\r\\n    fp, pathname, description = imp.find_module(\\'_tensorflow_wrap_toco\\', [dirname(__file__)])\\r\\n  File \"c:\\\\users\\\\sgavvala\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\imp.py\", line 297, in find_module\\r\\n    raise ImportError(_ERR_MSG.format(name), name=name)\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nTraceback (most recent call last):\\r\\n  File \"c:\\\\users\\\\sgavvala\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\runpy.py\", line 193, in _run_module_as_main\\r\\n    \"__main__\", mod_spec)\\r\\n  File \"c:\\\\users\\\\sgavvala\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\runpy.py\", line 85, in _run_code\\r\\n    exec(code, run_globals)\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\tensorflow\\\\Scripts\\\\toco_from_protos.exe\\\\__main__.py\", line 5, in <module>\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Roaming\\\\Python\\\\Python35\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\toco_from_protos.py\", line 22, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Roaming\\\\Python\\\\Python35\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 28, in <module>\\r\\n    _tensorflow_wrap_toco = swig_import_helper()\\r\\n  File \"C:\\\\Users\\\\sgavvala\\\\AppData\\\\Roaming\\\\Python\\\\Python35\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 20, in swig_import_helper\\r\\n    import _tensorflow_wrap_toco\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n'\r\nNone\r\n\r\nMy plan is to convert a keras model with my custom layers into tflite quantized version. Figured I would start with a given example but that doesn't seem to run.", "comments": ["Please try installing tf-nightly and check if its still an issue. In addition take a look at similar issue #22897 for updates.", "> Please try installing tf-nightly and check if its still an issue. In addition take a look at similar issue #22897 for updates.\r\n\r\n@ymodak I do have tf-nightly installed. I've noticed in the additional instructions for tensorflow version 1.9 to 1.11 to use **TocoConverter** instead of **TFLiteConverte**r and the problem still persists. ", "Thanks for the information. Your issue should be solved once the latest fix is pushed to tf nightly.\r\nhttps://github.com/tensorflow/tensorflow/issues/22897#issuecomment-429971818", "I have same issue. Is there a work around for now?", "are the github pages missing for all the things related to TOCO?", "@soumy94 Can you please use the latest tf -nightly build and check? This issue should be fixed now.", "I have tried it, it still has errors on windows 64-bit", "@maverik-akagami Apologies for the delay in response. The markdown files have been moved to tensorflow/tensorflow/lite/g3doc/convert/. I will close this issue due lack of activity. I recommend to open a new issue from [here](https://github.com/tensorflow/tensorflow/issues/new/choose) by providing all the information asked by the template. Thanks!"]}]