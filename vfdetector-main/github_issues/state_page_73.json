[{"number": 19965, "title": "[feature request] Quasi recurrent neural networks (QRNN) in tensorflow ", "body": "It would be cool to have native Quasi recurrent neural networks (QRNN) implementation included in the tensorflow package.\r\n\r\nFollowing is the paper : Bradbury, James, et al. \"Quasi-recurrent neural networks.\" arXiv preprint arXiv:1611.01576 (2016). [link](https://arxiv.org/pdf/1611.01576)\r\nIt is claimed in the paper that QRNN is 14-16x faster than cudnn_lstm while giving same accuracies.\r\n\r\nreference : https://github.com/salesforce/pytorch-qrnn\r\n\r\n\r\n**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 1.8.0\r\nPython version: 3.6.5\r\nBazel version (if compiling from source): none\r\nGCC/Compiler version (if compiling from source): none\r\nCUDA/cuDNN version: none\r\nGPU model and memory: none\r\nExact command to reproduce: none", "comments": ["I fully agree with that.", "If it helps @ebrevdo, someone has tried implementing it in Tensorflow already:\r\nhttps://github.com/JonathanRaiman/tensorflow_qrnn", "Nice!  Yeah; @JonathanRaiman has done some really great stuff.  At this point, I think getting QRNN into Keras (as that's where it would have to land in TF2) would be an uphill climb; but I'll let @martinwicke comment.  I know QRNNs are quite popular so it may be possible.  If not, then I'm sure it would be a good fit for the TF-Addons repo."]}, {"number": 19944, "title": "c++ gradient is not implemented for concat/stack", "body": "Feature Request:\r\nI noticed that c++ gradient for concat or stack is not implemented while i am using tensorflow sharp. I think those are quite important operations.  Could you please somehow consider implement those? Or is there any other replacement?\r\n\r\nThank you!\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\nTensorFlow installed from (source or binary):\r\npip and built from coursecode\r\nTensorFlow version (use command below):\r\n1.8.0\r\nPython version:\r\n3.6\r\nBazel version (if compiling from source):\r\n0.14.1\r\nGCC/Compiler version (if compiling from source):\r\nVS??\r\nCUDA/cuDNN version:\r\nN/A\r\nGPU model and memory:\r\nNvdia 960m\r\nExact command to reproduce:\r\nNA", "Are you sure the concat grad is not implemented? I think it's implemented [here](https://github.com/tensorflow/tensorflow/blob/8212404a47e17a0ad1822e520c990be1cd712e91/tensorflow/core/ops/array_grad.cc#L224).", "Thanks! I dont know why I got this error. \r\nI am using the dll built from using the script from [here](https://github.com/tensorflow/tensorflow/tree/8212404a47e17a0ad1822e520c990be1cd712e91/tensorflow/tools/ci_build/windows). The error message I saw when call the gradient is:\r\n\r\n> _TFException: No gradient defined for op: Concat. Please see https://www.tensorflow.org/code/tensorflow/cc/gradients/README.md for instructions on how to add C++ gradients._\r\n\r\nLooks like that build is not using the gradient implementaion from tensorflow/core.  Is  it because  the build i use does not have include the core library?", "/CC @suharshs, any idea why gradients wouldn't be included?", "Some operations aren't implemented yet. Pull requests are welcome and instructions for adding new gradients are available here: https://www.tensorflow.org/code/tensorflow/cc/gradients/README.md", "@suharshs Does your team have any plan implementing the gradient for concat or stack in the near future? I would love to implement it myself but the code seems not very intuitive...", "@tcmxx Currently this has not been prioritized.\r\n\r\nIf you want to give it a try, you can look at some previous open source contributions: https://github.com/tensorflow/tensorflow/pull/17592/files to gain some inspiration :)", "@suharshs Ok. I will give a try soon. ", "Just for anyone who is interested, I made a very simplified version to be added to array_grad.cc based on python implementation....sanity checks/optimizations are removed, not working for all cases....\r\n\r\n\r\n```\r\n\r\nstd::vector<Output> ExtractInputShapes(const Scope& scope, const std::vector<Output>& inputs){\r\n  //Extract the shapes of a set of input tensors.\r\n  bool fully_known = true;\r\n  std::vector<Output> sizes;\r\n  for(int i = 0; i < inputs.size();++i){\r\n    auto input_shape = Shape(scope, inputs[i]);\r\n    //removed because I dont konw how to check the type in c++...Assume that fully_know is always true..\r\n    //if(dynamic_cast<Const*>((&input_shape.op())) == nullptr || dynamic_cast<Tensor*>((&input_shape)) == nullptr){\r\n    /*if(dynamic_cast<Tensor*>((&input_shape)) == nullptr){\r\n      fully_known = false;\r\n      break;\r\n    }*/\r\n    sizes.push_back(input_shape);\r\n  }\r\n\r\n  if(fully_known){\r\n    return sizes;\r\n  }else{\r\n    auto result = ShapeN(scope, InputList(inputs)).output;\r\n    return result;\r\n  }\r\n\r\n}\r\n\r\nStatus ConcatGradHelper(const Scope& scope, const Operation& op,\r\n                 const std::vector<Output>& grad_inputs,\r\n                 std::vector<Output>* grad_outputs,\r\n               const int start_value_index, const int end_value_index, const int dim_index) {\r\n  // Degenerate concatenation, just return grad.\r\n  if(op.num_inputs() == 2){\r\n    if(end_value_index > dim_index){\r\n      grad_outputs->push_back(NoGradient());\r\n    }\r\n    for (int i = 0; i < grad_inputs.size();++i){\r\n      grad_outputs->push_back(grad_inputs[i]);\r\n    }\r\n    if(end_value_index <= dim_index){\r\n      grad_outputs->push_back(NoGradient());\r\n    }\r\n    return scope.status();\r\n  }\r\n\r\n  auto concat_dim = op.input(dim_index);\r\n  std::vector<Output> input_values;\r\n  for(int i = 0; i < end_value_index - start_value_index;++i){\r\n    input_values.push_back(op.input(i+start_value_index));\r\n  }\r\n  std::vector<Output> out_grads;\r\n\r\n  //No support for sparse or eagerly executing\r\n  auto non_neg_concat_dim = FloorMod(scope, concat_dim, Rank(scope, input_values[0]));\r\n  auto sizes = ExtractInputShapes(scope, input_values);\r\n\r\n\r\n  //temperary use this implementation only...the python implementation uese two different ones for different input size for optimization....\r\n  auto stackedSized= Stack(scope,sizes,Stack::Axis(1));\r\n  std::vector<Output> sliceBegin = {non_neg_concat_dim.z, Const(scope, 0)};\r\n  auto sliceBeginStacked = Stack(scope,InputList(sliceBegin),Stack::Axis(0));\r\n  auto slicedSizes = Slice(scope,stackedSized, sliceBeginStacked,{1, -1});\r\n  auto squeezedSizes = Squeeze(scope,slicedSizes);\r\n  out_grads = SplitV(scope,grad_inputs[0],squeezedSizes,  non_neg_concat_dim.z, sizes.size()).output;\r\n\r\n  if(end_value_index > dim_index){\r\n    grad_outputs->push_back(NoGradient());\r\n  }\r\n  for (int i = 0; i < out_grads.size();++i){\r\n    grad_outputs->push_back(out_grads[i]);\r\n  }\r\n  if(end_value_index <= dim_index){\r\n    grad_outputs->push_back(NoGradient());\r\n  }\r\n  return scope.status();\r\n}\r\n\r\nStatus ConcatGrad(const Scope& scope, const Operation& op,\r\n                    const std::vector<Output>& grad_inputs,\r\n                    std::vector<Output>* grad_outputs){\r\n  return ConcatGradHelper(scope, op,grad_inputs,grad_outputs,1,op.num_inputs(),0);\r\n}\r\nREGISTER_GRADIENT_OP(\"Concat\", ConcatGrad);\r\n\r\nStatus ConcatGradV2(const Scope& scope, const Operation& op,\r\n                    const std::vector<Output>& grad_inputs,\r\n                    std::vector<Output>* grad_outputs){\r\n  return ConcatGradHelper(scope, op,grad_inputs,grad_outputs,0,op.num_inputs()-1,op.num_inputs()-1);\r\n}\r\nREGISTER_GRADIENT_OP(\"ConcatV2\", ConcatGradV2);\r\n```", "Hi @tcmxx , did you have any luck putting together a pull request?", "Hi @suharshs This implementation is not as good as the python implementation because it not optimized, and I am not sure if it is usable under all circumstances, so I was not planning to create a pull request, unless you think it is ok.", "Hi @tcmxx this would be very useful for me as well. Any chance you'll raise a PR or do you mind if I try and pick it up?", "@gordoncaleb Nice to hear it is helpful. You can try and pick it up as you want, just keep me updated~", "**ConcatGrad** has [already been implemented](https://github.com/tensorflow/tensorflow/commit/084075e4e967677fc0be9fe921a0157126e69617) in 2016 and has shipped in all Tensorflow versions since then.\r\n\r\nThe error **No gradient defined for op: Concat** is printed by the Lookup() method of a [**GradOpRegistry**](https://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/cc/framework/grad_op_registry.cc#L32)  singleton. The gradient functions managed there are registered via a [**REGISTER_GRADIENT_OP**](https://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/cc/framework/grad_op_registry.h#L60) macro.\r\n\r\nThe existing **ConcatGrad** function is registered via a [**REGISTER_OP_GRADIENT**](https://github.com/tensorflow/tensorflow/blob/3b508ee367c992b64e6d76936f4eefc9c7dc5503/tensorflow/core/framework/function.h#L927) macro instead, which feeds an [**OpGradFactory**](https://github.com/tensorflow/tensorflow/blob/3b508ee367c992b64e6d76936f4eefc9c7dc5503/tensorflow/core/framework/function.cc#L1887) singleton.\r\n\r\nNow some questions:\r\n- how are the different gradient factories and registries supposed to interact?\r\n- can they be merged?\r\n- is the existing concat gradient just registered wrongly, i.e. should it use the **REGISTER_GRADIENT_OP** instead of the **REGISTER_OP_GRADIENT** macro?"]}, {"number": 19904, "title": "Multiple Prediction With Tensorflow Model On iOS Swift, Objective-C", "body": "I am trying to do character recognition mobile app with CNN. My model predict with %99,6 accuracy on python.\r\nNow, I am trying to implement my model on iOS. \r\nI used this project for sample project [Tensorflow Camera Example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios/camera)\r\nI created an app, it can predict right single character on screen. But I need to predict 5 character (like a code ABCDE) at same time. And when I change settings to send 5 CVPixelBuffer value to model, it predict wrong. When I send wrong predicted images to computer and try with python , it predicts true. I think there should be some special setting which I do not know, or something wrong about my function to get CVPixelBuffer value from UIImage.\r\nI trained my model with 45x45 images. Now I crop 5 images (45x45 pixels) and send them to model.\r\n\r\nMy function to get CVPixelBuffer : \r\n\r\n    extension UIImage {\r\n    /**\r\n     Resizes the image to width x height and converts it to an BGR CVPixelBuffer.\r\n     */\r\n    public func pixelBuffer(width: Int, height: Int) -> CVPixelBuffer? {\r\n        return pixelBuffer(width: width, height: height,\r\n                           pixelFormatType: kCVPixelFormatType_32BGRA,\r\n                           colorSpace: CGColorSpaceCreateDeviceRGB(),\r\n                           alphaInfo: .noneSkipLast)\r\n    }\r\n    func pixelBuffer(width: Int, height: Int, pixelFormatType: OSType,\r\n                     colorSpace: CGColorSpace, alphaInfo: CGImageAlphaInfo) -> CVPixelBuffer? {\r\n        var maybePixelBuffer: CVPixelBuffer?\r\n        let attrs = [kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue,\r\n                     kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue]\r\n        let status = CVPixelBufferCreate(kCFAllocatorDefault,\r\n                                         width,\r\n                                         height,\r\n                                         pixelFormatType,\r\n                                         attrs as CFDictionary,\r\n                                         &maybePixelBuffer)\r\n        \r\n        guard status == kCVReturnSuccess, let pixelBuffer = maybePixelBuffer else {\r\n            return nil\r\n        }\r\n        \r\n        CVPixelBufferLockBaseAddress(pixelBuffer, CVPixelBufferLockFlags(rawValue: 0))\r\n        let pixelData = CVPixelBufferGetBaseAddress(pixelBuffer)\r\n        \r\n        guard let context = CGContext(data: pixelData,\r\n                                      width: width,\r\n                                      height: height,\r\n                                      bitsPerComponent: 8,\r\n                                      bytesPerRow: CVPixelBufferGetBytesPerRow(pixelBuffer),\r\n                                      space: colorSpace,\r\n                                      bitmapInfo: alphaInfo.rawValue)\r\n            else {\r\n                return nil\r\n        }\r\n        \r\n        UIGraphicsPushContext(context)\r\n        context.translateBy(x: 0, y: CGFloat(height))\r\n        context.scaleBy(x: 1, y: -1)\r\n        self.draw(in: CGRect(x: 0, y: 0, width: width, height: height))\r\n        UIGraphicsPopContext()\r\n        \r\n        CVPixelBufferUnlockBaseAddress(pixelBuffer, CVPixelBufferLockFlags(rawValue: 0))\r\n        return pixelBuffer\r\n    }\r\n\r\nThanks.\r\n\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Mac OS 10.13.3 High Sierra\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.1.0 on Mac (Tensorflow-Experimental on iOS)\r\n- Python version: 3\r\n- Bazel version: 0.13.0\r\n- CUDA/cuDNN: N/A\r\n- GPU model and Memory: N/A\r\n- Exact command to reproduce: Single image prediction works well. But cannot predict multiple image.(Prediction)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nCUDA/cuDNN version", "I have MacBook Pro 13\" doesn't have Nvidia GPU for Machine Learning. So I don't have CUDA/cuDNN\r\nCUDA/cuDNN version: N/A", "@mhyttsten did you contribute the example?", "@mhyttsten could you take a look?", "@cagrigider \r\nI have been trying to build a swift project with this framework. Even i created bridging  file and while i am trying to do that, there are some errors in cocoapods .h files(Tensor flow header c++ files). I am being with this issue for past two days. Could u please tell me how u achieved\r\n\r\nThanks and advance", "@cagrigider I have the same issue. It seems the problem is with your function which should convert `UIImage` -> pixel buffer. I tried various implementations for it - they all return different results.\r\nIn the same time iOS example \"Simple\" from this library detects objects correctly on the same images. But that library works for local images only.\r\nThe problem is someone from this library's developers is lazy and don't want to post universal code which will be ready to use"]}, {"number": 19893, "title": "\u201cgrp c++/grpc++.h\u201d: No such file or directory ( D:\\tensorflow-master\\tensorflow\\core\\common_runtime\\eager\\context.cc)", "body": "\r\n\u201cD:\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\tf_tutorials_example_trainer.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (1) ->\r\n\u201cD:\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\tf_core_cpu.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (130) ->\r\n(ClCompile \u76ee\u6807) ->\r\n  D:\\tensorflow-master\\tensorflow/core/distributed_runtime/rpc/grpc_server_lib.h(21): fatal error C1083: \u65e0\u6cd5\u6253\u5f00\u5305\u62ec\u6587\u4ef6: \u201cgrp\r\nc++/grpc++.h\u201d: No such file or directory (\u7f16\u8bd1\u6e90\u6587\u4ef6 D:\\tensorflow-master\\tensorflow\\core\\common_runtime\\eager\\context.cc) [\r\nD:\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\tf_core_cpu.vcxproj]\r\n  D:\\tensorflow-master\\tensorflow/core/distributed_runtime/rpc/grpc_server_lib.h(21): fatal error C1083: \u65e0\u6cd5\u6253\u5f00\u5305\u62ec\u6587\u4ef6: \u201cgrp\r\nc++/grpc++.h\u201d: No such file or directory (\u7f16\u8bd1\u6e90\u6587\u4ef6 D:\\tensorflow-master\\tensorflow\\core\\common_runtime\\eager\\eager_operati\r\non.cc) [D:\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\tf_core_cpu.vcxproj]\r\n  D:\\tensorflow-master\\tensorflow/core/distributed_runtime/rpc/grpc_server_lib.h(21): fatal error C1083: \u65e0\u6cd5\u6253\u5f00\u5305\u62ec\u6587\u4ef6: \u201cgrp\r\nc++/grpc++.h\u201d: No such file or directory (\u7f16\u8bd1\u6e90\u6587\u4ef6 D:\\tensorflow-master\\tensorflow\\core\\common_runtime\\eager\\execute.cc) [\r\nD:\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\tf_core_cpu.vcxproj]\r\n  D:\\tensorflow-master\\tensorflow/core/distributed_runtime/rpc/grpc_server_lib.h(21): fatal error C1083: \u65e0\u6cd5\u6253\u5f00\u5305\u62ec\u6587\u4ef6: \u201cgrp\r\nc++/grpc++.h\u201d: No such file or directory (\u7f16\u8bd1\u6e90\u6587\u4ef6 D:\\tensorflow-master\\tensorflow\\core\\common_runtime\\eager\\tensor_handle\r\n.cc) [D:\\tensorflow-master\\tensorflow\\contrib\\cmake\\build\\tf_core_cpu.vcxproj]", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thank you\uff0c\r\ni want to cmake c++ api at win10 ,setup like this:\r\n1.install git,CMake ,SWIG,vs2015update3 and set system environment variable\r\n2.-Dtensorflow_ENABLE_GRPC_SUPPORT OFF,-Dtensorflow_ENABLE_SSL_SUPPORT OFF,-Dtensorflow_ENABLE_GPU OFF,tensorflow_BUILD_SHARED_LIB ON\r\n3.D:\\temp> cd tensorflow\\tensorflow\\contrib\\cmake\r\nD:\\temp\\tensorflow\\tensorflow\\contrib\\cmake> mkdir build\r\nD:\\temp\\tensorflow\\tensorflow\\contrib\\cmake> cd build\r\nD:\\temp\\tensorflow\\tensorflow\\contrib\\cmake\\build>cmake ..\r\n4.MSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj\r\n\r\n\r\n\r\n", "I believe this may be addressed by #16480 ?\r\nFeel free to reopen if I'm mistaken.", "I am having the same issue.\r\n\r\nI don't believe this is covered by #16480, because in the present case the build explicitly excludes support of GRPC:\r\n\r\n    -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF\r\n\r\nCould you please reopen the issue? @asimshankar \r\n", "I see, IIUC, you're having trouble building TensorFlow on Windows 10 when setting a non-default value for a build option (GRPC)?\r\n\r\nIf so, marking this as \"Contributions Welcome\"", "-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF or -Dtensorflow_ENABLE_GRPC_SUPPORT=On, having the same exception", "I encountered the same exact issue. Any way to avoid building distributive module?\r\n\r\nEdit: \r\nAdd more specific settings and descriptions.\r\n\r\nOS Platform and Distribution: Windows 10 Enterprise\r\nTensorFlow installed: from GitHub\r\nTensorFlow version: 1.5.0\r\nSWIG version: 3.0.12\r\nBazel version: not used (cmake)\r\nCUDA/cuDNN version: not used\r\nGPU model and memory: not used\r\nExact command to reproduce: build with -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF\r\n\r\nError Message:\r\nc:\\users\\user\\test\\v1.6.0-rc0\\tensorflow\\core\\distributed_runtime\\rpc\\grpc_server_lib.h(21): fatal error C1083: Cannot open include file: 'grpcpp/grpcpp.h': No such file or directory [C:\\Users\\User\\test\\v1.6.0-rc0\\tensorflow\\contrib\\cmake\\build\\tf_c.vcxproj]\r\n", "I encoutered the same issue under the following environment.\r\n\r\nOS Platform and Distribution: Windows 10 Enterprise\r\nTensorFlow installed: from GitHub\r\nTensorFlow version: 1.9.0\r\nBazel version: not used (cmake)\r\nCUDA/cuDNN version: not used\r\nGPU model and memory: not used\r\nExact command to reproduce: build with -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF", "I am having the same issue when\r\nMSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj\r\nwhatever -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF or On.\r\n\r\nBut I compile succussfully with Dtensorflow_ENABLE_GRPC_SUPPORT=OFF in\r\nmsbuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj\r\n", "me too,how to solve it?", "(BTW, bazel can now be used to build TensorFlow on Windows. @gunan I believe is in the process of updating the build instructions for that)", "Sorry for the inconvenience.\r\n@meteorcloudy for bazel instructions.\r\nFor cmake build, if you build from the 1.10 branch, it is still working.", "i using tensorflowsharp now,build file has been put in NUGET,this is a good idea", "Hello\r\nHave you solved this problem? \r\nI meet the same", "@LJXLJXLJX We'll soon publish a documentation for Windows Bazel build, people can then build TensorFlow following that instruction.", "@meteorcloudy Thank you , that's wonderful ! This is driving me crazy \ud83d\ude35 ", "@LJXLJXLJX If you're interested, I have a preview of the doc here you can try:\r\nhttps://gist.github.com/meteorcloudy/8e5f1aab7c7fa87b16ae28e6f8fd3fd2\r\n\r\nI'll be glad to hear some feedback. ;)", "@meteorcloudy The bazel build will allow you to build the C++/C APIs for windows or just the pip package? I tried to build the shared lib but with no success. ", "@Goldesel23 Yes, it should be able to build `//tensorflow:libtensorflow.so` and `//tensorflow/java:libtensorflow_jni.so`. We have CI job running for them, it should work.\r\n\r\nCan you file another issue to report the exact problem you encountered?", "@meteorcloudy, after some research I was able to build the `//tensorflow:libtensorflow_cc.so` project after applying a Eigen patch (already discussed in #19198 ) \r\n\r\nThe generated files Is there going to be possible to generate a dll similar to cmake build for windows? The project only builds the so libs not dlls. ", "@Goldesel23 Glad it worked! The .so file is in fact the .dll file you need. You can just rename the file or the target to libtensorflow.dll. :)", "@meteorcloudy Thanks! I wanted to link the shared lib in a project without bazel. I generated the .lib file with the references from the dll, but I am having some  linking problems. I will try to work them out. Thanks for the help! ", "@meteorcloudy Thanks for your instruction,  I have built the libtensorflow.so and libtensorflow.so.if.lib successfully, but when I tried to use them in a test project , there are 4101 errors , like\r\n\r\n\u4e25\u91cd\u6027\t\u4ee3\u7801\t\u8bf4\u660e\t\u9879\u76ee\t\u6587\u4ef6\t\u884c\t\u7981\u6b62\u663e\u793a\u72b6\u6001\r\n\u9519\u8bef(\u6d3b\u52a8)\tE1696\t\u65e0\u6cd5\u6253\u5f00 \u6e90 \u6587\u4ef6 \"google/protobuf/util/type_resolver_util.h\"\ttf_test\td:\\tensorflow\\tensorflow\\core\\platform\\default\\protobuf.h\t33\t\r\n\r\n\u4e25\u91cd\u6027\t\u4ee3\u7801\t\u8bf4\u660e\t\u9879\u76ee\t\u6587\u4ef6\t\u884c\t\u7981\u6b62\u663e\u793a\u72b6\u6001\r\n\u9519\u8bef\tC1083\t\u65e0\u6cd5\u6253\u5f00\u5305\u62ec\u6587\u4ef6: \u201cunsupported/Eigen/CXX11/Tensor\u201d: No such file or directory\ttf_test\td:\\tensorflow\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\tensor\t1\t\r\n", "@LJXLJXLJX You need to add the tensorflow, bazel-genfiles, eigen-archive and protobuf/src to the include directories of the project.", "@Goldesel23  Thanks for replying .  But there is only an empty file named \"BUILD\" in my third_party/protobuf folder. Should I build protobuf myself?", "@LJXLJXLJX sorry for the late response. You'll probably will need to add these to the include paths:\r\n\r\n```\r\ntensorflow\r\ntensorflow/bazel-tensorflow/external/eigen_archive\r\ntensorflow/bazel-tensorflow/external/protobuf_archive/src\r\ntensorflow/bazel-genfiles\r\n```", "After adding the include headers I still get some linker errors: \r\n\r\n`unresolved external symbol \"char const * __cdecl tensorflow::core::GetVarint32PtrFallback`\r\n\r\nand a lot of member `google::protobuf ........ may not be initialized`\r\n\r\nAny ideia what the problem could be? Is that a problem with the version of the protobuf? \r\n", "@Goldesel23 I guess you don't added the lib files to the project. \r\ntensorflow\\bazel-bin\\tensorflow\\libtensorflow.so.if.lib", "I think the my configuration is correct\uff0c but there are still some errors in logging.h \r\n`TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\r\n                        ==);  // Compilation error with CHECK_EQ(NULL, x)?\r\nTF_DEFINE_CHECK_OP_IMPL(Check_NE, !=)  // Use CHECK(x == NULL) instead.\r\nTF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)\r\nTF_DEFINE_CHECK_OP_IMPL(Check_LT, <)\r\nTF_DEFINE_CHECK_OP_IMPL(Check_GE, >=)\r\nTF_DEFINE_CHECK_OP_IMPL(Check_GT, >)\r\n`\r\n\u4e25\u91cd\u6027\t\u4ee3\u7801\t\u8bf4\u660e\t\u9879\u76ee\t\u6587\u4ef6\t\u884c\t\u7981\u6b62\u663e\u793a\u72b6\u6001\r\n\u9519\u8bef(\u6d3b\u52a8)\tE0040\t\u5e94\u8f93\u5165\u6807\u8bc6\u7b26\ttf_test\td:\\tensorflow\\tensorflow\\core\\platform\\default\\logging.h\t231\t\r\n", "Actually I did! But I got those linking errors. ", "> I am having the same issue\r\n\r\nI am having the same issue,how to solve it?\u3002", "\r\n\r\n\r\n\r\n> @Goldesel23 I guess you don't added the lib files to the project.\r\n> tensorflow\\bazel-bin\\tensorflow\\libtensorflow.so.if.lib\r\n\r\n\r\nbut i don't have tensorflow\\bazel-bin\\tensorflow\\libtensorflow.so.if.lib file\r\n", "@LJXLJXLJX but i don't have tensorflow\\bazel-bin\\tensorflow\\libtensorflow.so.if.lib file", "@taotaolin tensorflow\\bazel-bin\\tensorflow\\libtensorflow.so.if.lib should be generated when you build libtensorflow.so. Look into your `tensorflow\\bazel-bin/tensorflow` directory.", "> > @Goldesel23 I guess you don't added the lib files to the project.\r\n> > tensorflow\\bazel-bin\\tensorflow\\libtensorflow.so.if.lib\r\n> \r\n> but i don't have tensorflow\\bazel-bin\\tensorflow\\libtensorflow.so.if.lib file\r\n\r\n\u8fd9\u91cc\u53c8\u770b\u5230\u4f60\u4e86\u554a", "I meet the same problem, how to solve it? Anyone can give me some good idea.", "Enable the `GRPC_SUPPORT` solved this  for me. My Tensorflow version is 1.9."]}, {"number": 19832, "title": "set_intersection doesn't work as expectation - tensorflow 1.6.0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.6.0\r\n- **Python version**: \r\n3.6.5 (anaconda)\r\n- **Bazel version (if compiling from source)**:\r\n0.11\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc-5.4.0\r\n- **CUDA/cuDNN version**:\r\nCUDA9.1 and cuDNN 7.1.4\r\n- **GPU model and memory**:\r\nGPU model and 16G memory\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nimport collections\r\n\r\na = collections.OrderedDict([\r\n      ((0, 0), 1),\r\n      ((0, 3), 1),\r\n      ((1, 1), 1),\r\n      ((1, 3), 1),\r\n      ((2, 0), 1),\r\n      ((2, 1), 1)\r\n  ])\r\na = tf.SparseTensor(list(a.keys()), list(a.values()), dense_shape=[3, 4])\r\n\r\nb = collections.OrderedDict([\r\n      ((0, 0), 1),\r\n      ((0, 3), 1),\r\n      ((1, 1), 1),\r\n      ((1, 2), 1),\r\n      ((1, 3), 1),\r\n      ((2, 0), 1),\r\n      ((2, 1), 1)\r\n  ])\r\nb = tf.SparseTensor(list(b.keys()), list(b.values()), dense_shape=[3, 4])\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(a))\r\n    print(sess.run(b))\r\n    print(sess.run(tf.contrib.metrics.set_intersection(a, b)))\r\n\r\n```\r\n\r\n### Describe the problem\r\nFor set_intersection, for my own understanding, output should be \r\n\r\n```bash\r\nSparseTensorValue(indices=array([[0, 0],\r\n       [0, 3],\r\n       [1, 1],\r\n       [1, 3],\r\n       [2, 0],\r\n       [2, 1]]), values=array([1, 1, 1, 1, 1, 1], dtype=int32), dense_shape=array([3, 4]))\r\n```\r\n\r\nHowever, I get the result as follow:\r\n\r\n```bash\r\nSparseTensorValue(indices=array([[0, 0],\r\n       [1, 0],\r\n       [2, 0]]), values=array([1, 1, 1], dtype=int32), dense_shape=array([3, 1]))\r\n```\r\n\r\nI don't understand if I'm not fully understanding for the points....\r\n\r\nBest Regards\r\nOrlando", "comments": ["Sorry for the very slow response. @sguada can you help?", "Not sure maybe @ebrevdo can.", "@llv22 Is this still an issue ?\r\nWe see that you are using old version of tensorflow 1.x which is not actively supported, We recommend that you upgrade to 2.4 or later version.Please have a look at the [migration](https://www.tensorflow.org/guide/migrate) guide for reference to migrate from TensorFlow 1.x to TensorFlow 2.Thank you!", "@sushreebarsa I will check on tf 2.4 and let you know the result", "@sushreebarsa I'm afraid that this issue hasn't been settled down.\r\nThis is the result given on tf 2.4:\r\n```bash\r\nIn [52]: import collections\r\n    ...: \r\n    ...: a = collections.OrderedDict([\r\n    ...:       ((0, 0), 1),\r\n    ...:       ((0, 3), 1),\r\n    ...:       ((1, 1), 1),\r\n    ...:       ((1, 3), 1),\r\n    ...:       ((2, 0), 1),\r\n    ...:       ((2, 1), 1)\r\n    ...:   ])\r\n    ...: a = tf.SparseTensor(list(a.keys()), list(a.values()), dense_shape=[3, 4])\r\n    ...: \r\n    ...: b = collections.OrderedDict([\r\n    ...:       ((0, 0), 1),\r\n    ...:       ((0, 3), 1),\r\n    ...:       ((1, 1), 1),\r\n    ...:       ((1, 2), 1),\r\n    ...:       ((1, 3), 1),\r\n    ...:       ((2, 0), 1),\r\n    ...:       ((2, 1), 1)\r\n    ...:   ])\r\n    ...: b = tf.SparseTensor(list(b.keys()), list(b.values()), dense_shape=[3, 4])\r\n\r\nIn [53]: tf.sparse.to_dense(a).numpy()\r\nOut[53]: \r\narray([[1, 0, 0, 1],\r\n       [0, 1, 0, 1],\r\n       [1, 1, 0, 0]], dtype=int32)\r\n\r\nIn [54]: tf.sparse.to_dense(b).numpy()\r\nOut[54]: \r\narray([[1, 0, 0, 1],\r\n       [0, 1, 1, 1],\r\n       [1, 1, 0, 0]], dtype=int32)\r\n\r\nIn [55]: tf.sparse.to_dense(tf.sets.intersection(a, b)).numpy()\r\nOut[55]: \r\narray([[1],\r\n       [1],\r\n       [1]], dtype=int32)\r\n\r\nIn [68]: tf.print(tf.sets.intersection(a, b))\r\n'SparseTensor(indices=[[0 0]\r\n [1 0]\r\n [2 0]], values=[1 1 1], shape=[3 1])'\r\n```\r\n\r\nFor a, b with the same 3*4 dimension,   their expected intersection based on sparseTensor indices should contain sparseTensor with 6 bits. It seems that the c++ implementation has merged intersection results in the last dimension together and reshaped its size to 1. I suspected it's the right behavior. Could you double-check with it?\r\n\r\nIn the meanwhile, another trial has also been taken:\r\n```bash\r\nIn [59]: a1 = tf.sparse.reshape(a, (3,4,1))\r\n\r\nIn [60]: b1 = tf.sparse.reshape(b, (3,4,1))\r\n\r\nIn [61]: tf.print(a1)\r\n'SparseTensor(indices=[[0 0 0]\r\n [0 3 0]\r\n [1 1 0]\r\n [1 3 0]\r\n [2 0 0]\r\n [2 1 0]], values=[1 1 1 1 1 1], shape=[3 4 1])'\r\n\r\nIn [62]: tf.print(b1)\r\n'SparseTensor(indices=[[0 0 0]\r\n [0 3 0]\r\n [1 1 0]\r\n ...\r\n [1 3 0]\r\n [2 0 0]\r\n [2 1 0]], values=[1 1 1 ... 1 1 1], shape=[3 4 1])'\r\n\r\nIn [67]: tf.print(tf.sets.intersection(a1, b1))\r\n'SparseTensor(indices=[[0 0 0]\r\n [0 3 0]\r\n [1 1 0]\r\n [1 3 0]\r\n [2 0 0]\r\n [2 1 0]], values=[1 1 1 1 1 1], shape=[3 4 1])'\r\n\r\n```\r\nAfter transforming both of tensor to 3*4*1, now the result is correct. \r\nThat's really weird.\r\n\r\nCould you clarify it further?\r\n\r\nBest Regards\r\nOrlando\r\n"]}, {"number": 19825, "title": "Feature request: Concurrently serving models with optimizers", "body": "==Problem==\r\n\r\nI want to use TensorFlow Serving to serve an output given certain placeholder inputs with a single run. The graph looks like this:\r\n\r\nInput vector -> embedding lookup -> iterative inference with optimizer and tf.while_loop -> matrix math using inferred vector -> top K -> output\r\n\r\nThe problem is that I can't simply use a Tensor in the loop because tf.Optimizer requires a Variable. This in turn forces me to modify global state in each prediction, effectively prohibiting concurrency.\r\n\r\n==Feature Request==\r\n\r\nThis could be solved by implementing:\r\n* a version of Optimizer.minimize() that returns a list of Tensors and takes Tensors instead of Variables as input and\r\n* an optimization to make Optimizer.minimize() recycle Tensors that are loop variables re-assigned to the return of Optimizer.minimize()\r\n\r\n==For tensorflowbutler==\r\n\r\nSince this is a feature request, the following fields are irrelevant:\r\nHave I written custom code: NA\r\nOS Platform and Distribution: NA\r\nTensorFlow installed from: NA\r\nTensorFlow version: NA\r\nBazel version: NA\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce: NA", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Since this is a feature request, the following fields are irrelevant:\r\nHave I written custom code: NA\r\nOS Platform and Distribution: NA\r\nTensorFlow installed from: NA\r\nTensorFlow version: NA\r\nBazel version: NA\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce: NA"]}, {"number": 19774, "title": "metagraph loading fails with 'No op named ImageProjectiveTransform' message", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary, installed with pip3\r\n- **TensorFlow version (use command below)**:\r\nv1.8.0-0-g93bc2e2072\r\n- **Python version**: \r\n3.5\r\n\r\n### Describe the problem\r\nI'm trying to load a metagraph file saved with tf-v1.4.0-rc1-11-g130a514. Loading fails with the following error: \r\n\r\n```\r\n  File \"/home/vyal/.local/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1955, in import_meta_graph\r\n    **kwargs)\r\n  File \"/home/vyal/.local/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\", line 743, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n  File \"/home/vyal/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/vyal/.local/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 460, in import_graph_def\r\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n  File \"/home/vyal/.local/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 227, in _RemoveDefaultAttrs\r\n    op_def = op_dict[node.op]\r\nKeyError: 'ImageProjectiveTransform'\r\n```\r\n\r\nI thought it's the result of some backward incompability so I retested on python2.7 and tf-v1.4.0-rc1-11-g130a514. Getting very similar message:\r\n\r\n```\r\nIn [3]: tf.train.import_meta_graph('00000000000001107000.meta')\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-6d533993d6fe> in <module>()\r\n----> 1 tf.train.import_meta_graph('00000000000001107000.meta')\r\n\r\n/home/marin/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc in import_meta_graph(meta_graph_or_file, clear_devices, import_scope, **kwargs)\r\n   1808                                       clear_devices=clear_devices,\r\n   1809                                       import_scope=import_scope,\r\n-> 1810                                       **kwargs)\r\n   1811   if meta_graph_def.HasField(\"saver_def\"):\r\n   1812     return Saver(saver_def=meta_graph_def.saver_def, name=import_scope)\r\n\r\n/home/marin/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.pyc in import_scoped_meta_graph(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate)\r\n    658     importer.import_graph_def(\r\n    659         input_graph_def, name=(import_scope or \"\"), input_map=input_map,\r\n--> 660         producer_op_list=producer_op_list)\r\n    661 \r\n    662     scope_to_prepend_to_names = \"/\".join(\r\n\r\n/home/marin/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/importer.pyc in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n    283       # Set any default attr values that aren't present.\r\n    284       if node.op not in op_dict:\r\n--> 285         raise ValueError('No op named %s in defined operations.' % node.op)\r\n    286       op_def = op_dict[node.op]\r\n    287       for attr_def in op_def.attr:\r\n\r\nValueError: No op named ImageProjectiveTransform in defined operations.\r\n```\r\n\r\n", "comments": ["Similar error when trying to load a meta file using C++ interface\r\nFYI: https://github.com/tensorflow/tensorflow/issues/17942\r\n\r\n`  \r\n        MetaGraphDef graph_def;\r\n        auto session = NewSession(SessionOptions());\r\n        status = ReadBinaryProto(Env::Default(), GRAPH_PATH, &graph_def);\r\n        status = session->Create(graph_def.graph_def());\r\n        if (!status.ok()) {\r\n                **log_err(\"Error creating graph: %s\", status.ToString());**\r\n                return -1;\r\n        }`\r\n\r\n**System information**\r\n\r\n    TensorFlow build using make \r\n    built as a static library. **libtensorflow-core.a**\r\n\r\n    TensorFlow version (use command below):\r\n    v1.8.0-0\r\n\r\nBelow are the error message\r\n\r\n Apr 10 16:17:22  E tensorflow/core/framework/op_kernel.cc:1242] OpKernel ('op: \"MutableDenseHashTable\" device_type: \"CPU\" constraint { name: \"key_\r\ndtype\" allowed_values { list { type: DT_INT64 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_DOUBLE } } }') for unknown op: MutableDenseHashTable\r\nApr 10 16:17:22: E tensorflow/core/framework/op_kernel.cc:1242] OpKernel ('op: \"HashTableV2\" device_type: \"CPU\" constraint { name: \"key_dtype\" all\r\nowed_values { list { type: DT_STRING } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_DOUBLE } } }') for unknown op: HashTableV2\r\npr 10 16:17:22  E tensorflow/core/framework/op_kernel.cc:1242] OpKernel ('op: \"LookupTableImportV2\" device_type: \"CPU\"') for unknown op: LookupTa\r\nbleImportV2\r\nApr 10 16:17:22: E tensorflow/core/framework/op_kernel.cc:1242] OpKernel ('op: \"LookupTableImport\" device_type: \"CPU\"') for unknown op: LookupTabl\r\neImport\r\nApr 10 16:17:22 : E tensorflow/core/framework/op_kernel.cc:1242] OpKernel ('op: \"LookupTableExport\" device_type: \"CPU\"') for unknown op: LookupTabl\r\neExport\r\nApr 10 16:17:22 : E tensorflow/core/framework/op_kernel.cc:1242] OpKernel ('op: \"LookupTableSize\" device_type: \"CPU\"') for unknown op: LookupTableS\r\nize\r\nApr 10 16:17:22 : E tensorflow/core/framework/op_kernel.cc:1242] OpKernel ('op: \"LookupTableInsertV2\" device_type: \"CPU\"') for unknown op: LookupTa\r\nbleInsertV2\r\nApr 10 16:17:22 : E tensorflow/core/framework/op_kernel.cc:1242] OpKernel ('op: \"LookupTableInsert\" device_type: \"CPU\"') for unknown op: LookupTabl\r\neInsert\r\nApr 10 16:17:22E tensorflow/core/framework/op_kernel.cc:1242] OpKernel ('op: \"LookupTableFindV2\" device_type: \"CPU\"') for unknown op: LookupTabl\r\neFindV2\r\nApr 10 16:17:22 E tensorflow/core/framework/op_kernel.cc:1242] OpKernel ('op: \"LookupTableFind\" device_type: \"CPU\"') for unknown op: LookupTableF\r\nind\r\n\r\nHave checked the tensorflow/contrib/makefile/tf_op_file.txt and all the necessary operation files are available.", "I found the culprit. It was call to tf.contrib.image.rotate. Importing tf.contrib.image before loading the metagraph helps.\r\n\r\nLooks to me like quite unintuitive behavior.", "Thank you very much @akamaus for sharing the workaround!", "Is it OK to close this issue? I'm not sure what TF would do differently to avoid the problem since ops are loaded on demand.", "A warning saying something like 'perhaps you forgot to import contrib module' would have saved a day for me.", "Nagging Assignee @michaelisard: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This will be fixed once we automatically load op libraries from a directory instead of relying on (lazy) python imports to load them. I'm ok with adding a warning as you describe. Happy to approve a PR.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 19771, "title": "tensordot/conj interplay", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux 7, Kernel 3.10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: Python 3.6.2 :: Continuum Analytics, Inc.\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0.176 (but not in use)\r\n- **GPU model and memory**: GeForce GTX 1080 (but not in use)\r\n- **Exact command to reproduce**: execute python script below\r\n\r\n### Describe the problem\r\nI am using the code below to contract four tensors `A[l,m,n] B[q,l,i] C[j,p,m,q] conj(D[p,n,k])` using a series of tensordot operations. To verify the result, I compare tensorflow with pure numpy. It seems like the `tf.conj` node is ignored (or an additional conjugation is executed) when executing tensorflow's `tensordot`s, as the first `print` gives me `False` and the second one `True`, although it should be the other way round. Just as a remark: The third part of my code uses `einsum` instead and works as intended (output of the third `print` should be `True` and is indeed `True`). Thank you for your help!\r\n\r\n### Source code / logs\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nn = 100\r\nm = 4\r\ndtype = np.complex128\r\n\r\n### generate random tensors ###\r\n\r\ndef rand(*shape):\r\n    return np.asarray(\r\n        np.random.random(shape) + 1j*np.random.random(shape),\r\n        dtype\r\n    )\r\n\r\nA = rand(n, m, n)\r\nB = rand(m, n, n)\r\nC = rand(m, m, m, m)\r\nD = rand(m, n, n)\r\n\r\n### perform numpy computation ###\r\n\r\nX_np = np.tensordot(A, B, (0,1))\r\nY_np = np.tensordot(X_np, C, ((0,2),(2,3)))\r\nZ_np = np.tensordot(Y_np, D.conj(), ((3,0),(0,1)))\r\n\r\n### build tensorflow computation graph ###\r\n\r\nA_node = tf.placeholder(dtype, (n, m, n))\r\nB_node = tf.placeholder(dtype, (m, n, n))\r\nC_node = tf.placeholder(dtype, (m, m, m, m))\r\nD_node = tf.placeholder(dtype, (m, n, n))\r\n\r\nX_node = tf.tensordot(A_node, B_node, (0,1))\r\nY_node = tf.tensordot(X_node, C_node, ((0,2),(2,3)))\r\nZ_node = tf.tensordot(Y_node, tf.conj(D_node), ((3,0),(0,1)))\r\n\r\n### run tensorflow computation ###\r\n\r\nsession = tf.Session()\r\nZ_tf = session.run(\r\n    Z_node, feed_dict={A_node: A, B_node: B, C_node: C, D_node: D}\r\n)\r\nsession.close()\r\n\r\n### compare results ###\r\n\r\nprint(np.allclose(Z_np, Z_tf))\r\n\r\n### run tensorflow computation again with numpy conj ###\r\n\r\nsession = tf.Session()\r\nZ_tf = session.run(\r\n    Z_node, feed_dict={A_node: A, B_node: B, C_node: C, D_node: D.conj()}\r\n)\r\nsession.close()\r\n\r\n### compare results ###\r\n\r\nprint(np.allclose(Z_np, Z_tf))\r\n\r\n### build tensorflow computation graph using einsum ###\r\n\r\nW_node = tf.einsum(\r\n    \"lmn,qli,jpmq,pnk->ijk\", A_node, B_node, C_node, tf.conj(D_node)\r\n)\r\n\r\nsession = tf.Session()\r\nW_tf = session.run(\r\n    W_node, feed_dict={A_node: A, B_node: B, C_node: C, D_node: D}\r\n)\r\nsession.close()\r\n\r\nprint(np.allclose(Z_np, W_tf))\r\n```\r\n### Output\r\n```\r\nFalse\r\nTrue\r\nTrue\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "Sorry, I deleted the fields Bazel version and GCC/Compiler version, as I was not installing tensorflow from source.", "Hi @mrader1248 ! This issue is getting resolved in 2.8 version using compat.v1 mode. Attaching [gist](https://colab.sandbox.google.com/gist/mohantym/af6bfffddad9426b1652a27091d9376e/git_19771.ipynb#scrollTo=jgg-kJ6h1fvB) for reference . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 19748, "title": "DirectSession::Run with Saver restore operation crashes during nsync wait", "body": "### System information\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Tizen\r\nTensorFlow installed from: Source\r\nTensorFlow version: 1.4\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\n\r\n### Problem\r\nI am using Tensorflow 1.4 C++ API to read graph from meta file and load weights from checkpoint file.\r\n\r\n```\r\n#include <tensorflow/cc/ops/standard_ops.h>\r\n#include <tensorflow/core/framework/tensor.h>\r\n#include <tensorflow/core/public/session.h>\r\n#include <tensorflow/core/protobuf/meta_graph.pb.h>\r\n\r\n#define GRAPH_PATH \"path-to-graph.meta\"\r\n#define CHECKPOINT_PATH \"path-to-checkpoint.data-00000-of-00001\"\r\n\r\nMetaGraphDef graph_def;\r\nTensor checkpoint_path_tensor(DT_STRING, TensorShape());\r\nauto session = NewSession(SessionOptions());\r\nstatus = ReadBinaryProto(Env::Default(), GRAPH_PATH, &graph_def);\r\nstatus = session->Create(graph_def.graph_def());\r\ncheckpoint_path_tensor.scalar<string>()() = CHECKPOINT_PATH;\r\nstatus = session->Run(\r\n        {{ graph_def.saver_def().filename_tensor_name(), checkpoint_path_tensor },},\r\n        {},\r\n        {graph_def.saver_def().restore_op_name()},\r\n        nullptr);\r\n```\r\n\r\nWhile loading weights into graph from checkpoint, the program crashes during session->Run() with following trace:\r\n```\r\n#0  0xf1268e14 in std::condition_variable::condition_variable() () from /lib/libstdc++.so.6\r\n#1  0xf555701c in nsync::nsync_mu_semaphore_init(nsync::nsync_semaphore_s_*) () from /lib/libpywrap_tensorflow_internal.so\r\n#2  0xf5557ae4 in nsync::nsync_waiter_new_() () from /lib/libpywrap_tensorflow_internal.so\r\n#3  0xf55558fc in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from /lib/libpywrap_tensorflow_internal.so\r\n#4  0xf5556004 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from /lib/libpywrap_tensorflow_internal.so\r\n#5  0xf5556040 in nsync::nsync_cv_wait(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*) () from /lib/libpywrap_tensorflow_internal.so\r\n#6  0xf2d6ba30 in tensorflow::condition_variable::wait(tensorflow::mutex_lock&) () from /lib/libpywrap_tensorflow_internal.so\r\n#7  0xf2d6c0d4 in tensorflow::Notification::WaitForNotification() () from /lib/libpywrap_tensorflow_internal.so\r\n#8  0xf34f2078 in tensorflow::DirectSession::WaitForNotification(tensorflow::Notification*, long long) () from /lib/libpywrap_tensorflow_internal.so\r\n#9  0xf34f1f14 in tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, tensorflow::CancellationManager*, long long) () from /lib/libpywrap_tensorflow_internal.so\r\n#10 0xf34ebfc4 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /lib/libpywrap_tensorflow_internal.so\r\n#11 0xf34eadfc in tensorflow::DirectSession::Run(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) () from /lib/libpywrap_tensorflow_internal.so\r\n```\r\n", "comments": []}, {"number": 19720, "title": "Library not loaded: @rpath/libcublas.8.0.dylib when running TF GPU on MacOS ", "body": "\r\n### System information\r\n\r\n- **System: Mac OS 10.13.4**:\r\n- **Xcode:9.2 .  Apple LLVM version 9.0.0 (clang-900.0.39.2)**:\r\n- **Cuda:**:\r\n- **nvcc: NVIDIA (R) Cuda compiler driver**:\r\n- **Copyright (c) 2005-2018 NVIDIA Corporation**:\r\n- **Built on Sun_Mar_18_21:08:25_CDT_2018**:\r\n- **Cuda compilation tools, release 9.2, V9.2.64**:\r\n- **cudnn:7.1.4**:\r\n- **gcc g++ : 4.2.1**:\r\n\r\n\r\n- **Question:**:\r\nWhen i use anaconda install tensorflow-gpu,the terminal say i was successful! Such as:\r\nInstalling collected packages: numpy, six, werkzeug, protobuf, tensorflow-gpu\r\nSuccessfully installed numpy-1.14.3 protobuf-3.5.2.post1 six-1.11.0 tensorflow-gpu-1.1.0 werkzeug-0.14.1\r\n\r\nBut when is write \"import tensorflow as tf\",some erroes is show.\r\n\r\n> (tensorflowGPU) jhmdeMacBook-Air:Sources jhm$ python\r\nPython 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37) \r\n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib\r\n  Referenced from: /Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Reason: image not found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n>Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib\r\n  Referenced from: /Users/jhm/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Reason: image not found\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n\r\nBugLog/tensorflowGPU-01.log\r\n\r\nI don't konw what to do!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are not supporting GPUs on macos anymore, because apple and nvidia do not provide official support for each other. I will mark this community support.\r\nYou may have better luck trying to get support for this from stackoverflow.", "So the cause of this issue is the SIP in Mac OS, basically when python launches, due to SIP, the subprocess of Tensorflow doesn't get passed the environment variable for the library locations, so _pywrap_tensorflow_internal.so as referenced in the error message can't find the library locations.\r\n\r\nThe solution is to link _pywrap_tensorflow_internal.so so rather than relying on environment variables, it checks the file path directly (in this case looking for /usr/local/cuda/lib/<dylib name>).\r\n\r\nThe workaround, so you don't have to rebuild everything is to use the command line tool \"install_name_tool\" to update the object file linking. To do this you just run this command, using the appropriate path for where your tensorflow wheel installed, you'll need to do this for 2 object files actually:\r\n`install_name_tool -change @rpath/libcusolver.9.2.dylib /usr/local/cuda/lib/libcusolver.9.2.dylib -change @rpath/libcudart.9.2.dylib /usr/local/cuda/lib/libcudart.9.2.dylib   -change @rpath/libcublas.9.2.dylib /usr/local/cuda/lib/libcublas.9.2.dylib **CHANGETHISPATH**/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n`\r\n\r\n\r\n`install_name_tool -change @rpath/libcudart.9.2.dylib /usr/local/cuda/lib/libcudart.9.2.dylib -change @rpath/libcublas.9.2.dylib /usr/local/cuda/lib/libcublas.9.2.dylib -change @rpath/libcudnn.7.dylib /usr/local/cuda/lib/libcudnn.7.dylib -change @rpath/libcufft.9.2.dylib /usr/local/cuda/lib/libcufft.9.2.dylib -change @rpath/libcurand.9.2.dylib /usr/local/cuda/lib/libcurand.9.2.dylib -change @rpath/libcudart.9.2.dylib /usr/local/cuda/lib/libcudart.9.2.dylib  **CHANGETHISPATH**/python3.5/site-packages/tensorflow/libtensorflow_framework.so`\r\n\r\nThe permanent solution seems to be to add a flag to the compile per https://stackoverflow.com/questions/39927235/alternative-for-the-dyld-library-path-trick-since-mac-os-10-11-el-capitan-with-s but I don't know enough about the tensorflow build to say where this needs to go", "I have encounter similar problem in my hackintosh, I am not sure if it is related to SIP,  while I am use 0x67 in clover config.\r\n\r\nthe simplest way is : \r\n\r\nexport DYLD_LIBRARY_PATH=\"/usr/local/cuda/lib\":$DYLD_LIBRARY_PATH\r\n\r\nthen everything is fine.\r\n"]}, {"number": 19717, "title": "`ScipyOptimizerInterface` fails with `scatter_add` and `scatter_update`", "body": "If I try to build some loss function which in its calculations includes an `scatter_add` or `scatter_update` and optimize it using `ScipyOptimizerInterface`, TensorFlow will fail in calculating the gradient.\r\n\r\nHere's an example with a variant of low-rank matrix factorization in which I add an extra variable, which adds to some of the rows of one of the matrices in the low-rank factorization:\r\n\r\n```python\r\nimport numpy as np, tensorflow as tf\r\n\r\nnrow = 20\r\nncol = 30\r\nk = 15\r\nnrow_add = 10\r\nncol_mult_add = 5\r\nregularization = 1e-6\r\n\r\nnp.random.seed(1)\r\nX = np.random.normal(size = (nrow, ncol)).astype('float32')\r\nX2 = np.random.normal(size = (nrow_add, ncol_mult_add)).astype('float32')\r\nix_row_add = np.random.choice(np.arange(nrow), replace=False, size=nrow_add).astype('int32')\r\n\r\nA = tf.Variable(tf.random_normal([nrow, k]))\r\nB = tf.Variable(tf.random_normal([ncol, k]))\r\nA2 = tf.Variable(tf.random_normal([ncol_mult_add, k]))\r\n\r\nXtf = tf.placeholder(tf.float32)\r\nX2tf = tf.placeholder(tf.float32)\r\n\r\nA_added = tf.scatter_add(A, ix_row_add, tf.matmul(X2, A2))\r\npred_x = tf.matmul(A_added, B, transpose_b=True)\r\nloss = tf.losses.mean_squared_error(Xtf, pred_x)\r\nloss += regularization * (tf.nn.l2_loss(A) + tf.nn.l2_loss(A2) + tf.nn.l2_loss(B))\r\n\r\noptimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, method='L-BFGS-B')\r\nmodel = tf.global_variables_initializer()\r\nsess=tf.Session()\r\nsess.run(model)\r\nwith sess:\r\n    sess.run(model)\r\n    optimizer.minimize(sess, feed_dict={Xtf:X, X2tf:X2})\r\n    Aopt = A.eval(session=sess)\r\n    A2opt = A2.eval(session=sess)\r\n    Bopt = B.eval(session=sess)\r\n``` \r\n\r\nThrows:\r\n```\r\nInvalidArgumentError: Input 0 of node ScatterAdd was passed float from _arg_Variable_0_2:0 incompatible with expected float_ref.\r\n```\r\n\r\nInformation about my system:\r\n* Have I written custom code: No\r\n* OS Platform and Distribution: Debian buster/sid 64-bit\r\n* TensorFlow installed from: binary (PyPI)\r\n* TensorFlow version: 1.7.0\r\n* Python version: 3.6.3\r\n* Bazel version: N/A\r\n* GCC/Compiler version: N/A\r\n* CUDA/cuDNN version: N/A\r\n* GPU model and memory: N/A\r\n* Exact command to reproduce: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Sorry, added the platform details now.", "This is part of contrib so is officially unsupported (and I have no knowledge of it). That being said perhaps @JasperSnoek, can suggest some advice.", "Apologies!  I'm not sure why I'm associated with this optimizer but it\nlooks a lot like the scipyoptimizer from\nhttps://github.com/GPflow/GPflow/blob/master/gpflow/training/scipy_optimizer.py.\nPerhaps the developers there know more?\n\n\nOn Sat, Jul 28, 2018 at 2:53 PM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Nagging Assignee @JasperSnoek <https://github.com/JasperSnoek>: It has\n> been 14 days with no activity and this issue has an assignee. Please update\n> the label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19717#issuecomment-408627840>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AC7L28F8eL259TMbN-2tMAtcz-0HKEonks5uLLNFgaJpZM4UX-_p>\n> .\n>\n", "I don't think so, the source code looks very different from theirs: https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/opt/python/training/external_optimizer.py\r\nWould it then be possible to assign this to the people who wrote that file? (Jonathan Hseu and some unnamed developer(s) according to git blame)", "Nagging Assignee @jhseu: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm probably on blame due to merge conflicts, so I'm unassigning myself, but contributions are welcome."]}, {"number": 19679, "title": "Build fails if Nvidia nccl doc files (NCCL-SLA.txt) are relocated", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Slackware 14.2+ (-current)\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n\r\n- **Python version**: \r\n2.7.15\r\n- **Bazel version (if compiling from source)**:\r\n0.13.1- (@non-git)\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (GCC) 7.3.0\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.2/cuDNN 7.1\r\n- **GPU model and memory**:\r\nTitan X Pascal 16 GB\r\n- **Exact command to reproduce**:\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nTensorflow build process with nccl enabled is too \"picky\" about location of Nvidia NCLL doc file(s) - for ex. NCCL-SLA.txt.\r\nIt expects to find the text file(s) in the root of the nccl install dir (in my case `/opt/nvidia/nccl`) and the build fails if I relocate the txt file(s) (to for ex. a `doc` dir in the nccl install dir (for ex. `/opt/nvidia/nccl/doc`)\r\n\r\nWould be great if the build process would also look for the file(s) in subdirs of the `nccl` install directory. This would also make it possible to install nccl in a prefix such as `/usr/` and put the docs in `/usr/doc`. Not a big deal though, considering there are always more important issues to worry about.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nERROR: missing input file '@local_config_nccl//:nccl/NCCL-SLA.txt'\r\nERROR: /usr/local/src/tensorflow/tensorflow-git/tensorflow/tools/pip_package/BUILD:167:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@local_config_nccl//:nccl/NCCL-SLA.txt'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n", "comments": ["Same  failed~", "copy the NCCL-SLA.txt to tensorflow/third_party/nccl, works ", "oddly, copying NCCL-SLA.txt didn't work for me:\r\n\r\n```\r\nERROR: /home/mike/Downloads/tensorflow/tensorflow/tools/pip_package/BUILD:166:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@local_config_nccl//:nccl/NCCL-SLA.txt'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/mike/Downloads/tensorflow/tensorflow/tools/pip_package/BUILD:166:1 1 input file(s) do not exist\r\n```\r\n\r\nbut it's there...\r\n\r\n```\r\n[mike@monster nccl]$ pwd\r\n/home/mike/Downloads/tensorflow/third_party/nccl   \r\n[mike@monster nccl]$ ls -la    \r\ntotal 76  \r\ndrwxr-xr-x  2 mike mike  4096 Jun 14 09:41 .  \r\ndrwxr-xr-x 25 mike mike  4096 Jun 14 09:38 ..  \r\n-rw-r--r--  1 mike mike 11416 Jun 14 09:02 LICENSE  \r\n-rw-r--r--  1 mike mike  1733 Jun 14 09:02 nccl_archive.BUILD  \r\n-rw-r--r--  1 mike mike  5142 Jun 14 09:02 nccl_configure.bzl  \r\n-rw-rw-rw-  1 mike mike 41927 Jun 14 09:41 NCCL-SLA.txt  \r\n```\r\n\r\n", "And for me on manjaro, it must be in ```/opt/cuda```", "failed too on fedora 28", "It seems working for me by copying `NCCL-SLA.txt` from `/usr/share/doc/libnccl2/` to `/usr` where I installed nccl2.\r\n\r\nref: https://github.com/tensorflow/serving/issues/327", "I think that it needs to be where you told `./configure` the nccl was installed, in a subdirectory called nccl. For me that was in `/usr/local/cuda` I added a directory at `/usr/local/cuda/nccl` and put it there.", "In Tensorflow version 1.9.0, `set_tf_nccl_install_path` in `configure.py` isn't robust enough to handle NCCL installs under Cuda 9.2 standards. NCCL (version 2.x.x) is installed in standard system locations. That is, `nccl.h` is in `/usr/include` and the shared library is in `/usr/lib/x86_64-linux-gnu/`. However, the python configure code expects `NCCL` to have it's own install directory with `lib` and `include` subdirectories. \r\n\r\nMy fix for this was to create a NCCL directory under the Cuda root install (in my case, `/usr/local/cuda-9.2`) and add symbolic links to `/usr/include` and `/usr/lib/x86_64-linux-gnu/` (called `lib` in this later case).\r\n\r\nEDIT: I should note I also STILL need to copy a copy of NCCL-SLA.txt into the `/usr/local/cuda-9.2/nccl` directory.", "@jimfcarroll everything works.\r\nThere is only a minor addition. While running `./configure` the `nccl` directory should be changed.\r\n\r\n`Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:/usr/local/cuda/nccl`", "That looks like what I did. I've scripted this build to run inside of a docker container and then export the final results. I run it in nvidia/cuda:9.2-cudnn7-devel-ubuntu18.04 and set it up exactly like you have.\r\n\r\nIf it helps, and while it's a work in progress, the project is here: https://github.com/jimfcarroll/tensorflow-packaging . The script that runs inside of the container to do the build is: https://github.com/jimfcarroll/tensorflow-packaging/blob/master/container-files/build-tensorflow.sh\r\n\r\nNotice the few lines starting at 71. That sets up the NCCL hack to get the build to find the files, including `NCCL-SLA.txt`\r\n\r\n", "@jimfcarroll please check the comment above", "Ah. great.", "In my case, there was no NCCL-SLA.txt file. Instead, there was a LICENCE.txt file in /usr/local/cuda-9.2/targets/x86_64-linux\r\nI created a new identical file in the same directory with the name NCCL-SLA.txt and the problem has been solved, that is, I managed to compile tensorflow with cuda-9.2", "> It seems working for me by copying `NCCL-SLA.txt` from `/usr/share/doc/libnccl2/` to `/usr` where I installed nccl2.\r\n\r\nThis method also make sense for me. Thanks!", "> In my case, there was no NCCL-SLA.txt file. Instead, there was a LICENCE.txt file in /usr/local/cuda-9.2/targets/x86_64-linux\r\n> I created a new identical file in the same directory with the name NCCL-SLA.txt and the problem has been solved, that is, I managed to compile tensorflow with cuda-9.2\r\n\r\nThis solved my problem. Thank you for sharing!", "For compilation of CUDA 10.0 and Tensorflow 1.8 I had to perform the same trick as @tanguofu renaming the LICENSE.txt file and copying it into the main CUDA 10 directory. Although I feel @jimfcarroll's fix is probably more sensible. "]}, {"number": 19645, "title": "Feature: Exiting current variable_scope (parent scope)", "body": "Is it possible to exit a current variable_scope?\r\n\r\nI know it is possible to get to the root `name_scope` with something like `tf.name_scope(None)` but I don't seem to find a way to do that for `tf.variable_scope`.\r\nThis feature is interesting to me because I would like to share certain variables across parts of my model without passing around the `variable_scope` object. I would like to just configure a name for my shared `variable_scope` and then wherever I need to use that shared scope call\r\n`with tf.variable_scope(shared, reuse=tf.AUTO_REUSE)` without having to care which scope I am currently in.\r\n\r\nI look through all the issues and stackoverflow but I have not found anything in that direction.", "comments": ["To partly answer my question.\r\nThere is an option to change to a different `variable_scope`. It is just confusing to use.\r\n```python\r\nwith tf.variable_scope(\r\n              tf.VariableScope(tf.AUTO_REUSE, 'shared'), reuse=tf.AUTO_REUSE,\r\n              auxiliary_name_scope=False):\r\n```\r\nThis will always set the `variable_scope` to `shared` no matter what scope you are in."]}, {"number": 19640, "title": "Tensorflow Serving not using multi GPU/CUDA cores ", "body": "I'm using an AWS g3.8xlarge instance which has 2 GPUs.\r\n\r\nTF serving is able to detect both GPUs and initialise them but while running the model it only uses 1 GPU to the maximum.\r\n\r\nWe are on version 1.7, even though the client sends upto 32 requests in parallel, the model server only uses the first GPU \r\n![40657873-faca0f68-6366-11e8-963c-3d5ba1db4e2c](https://user-images.githubusercontent.com/6717323/40708575-66e52d36-6411-11e8-9e01-d95a0861827d.jpg)\r\n\r\n06_09_21", "comments": ["try this command :\r\n`CUDA_VISIBLE_DEVICES=1  ./tensorflow_model_server  --port=9003 --model_name=mnist --model_base_path=/path`\r\nto assign gpu ", "@DXZ , hi, I would like to ask do you have any experience on running a separate tensorflow serving server for each GPU.\r\n\r\nI have a machine with two 1080 Tis. My TF-Serving is able to correctly identify both of the GPUs when the `CUDA_VISIBLE_DEVICES` flag is not set. And I was trying to run one TF-Serving server per GPU, so I opened two terminals to run the following two commands:\r\n\r\n> CUDA_VISIBLE_DEVICES=0 bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=inception --model_base_path=/path/to/inception_model\r\n\r\n> CUDA_VISIBLE_DEVICES=1 bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9001 --model_name=mnist --model_base_path=/path/to/mnist_model\r\n\r\nThe first command was running fine, but the second one will reach error says\r\n\r\n> terminate called after throwing an instance of 'std::system_error'\r\n  what():  Resource temporarily unavailable\r\n[1]    4021 abort (core dumped)  CUDA_VISIBLE_DEVICES=1  --port=9001 --model_name=mnist\r\n\r\nAnd idea or suggestions? Thanks in advance!", "any updates ", "> @DXZ , hi, I would like to ask do you have any experience on running a separate tensorflow serving server for each GPU.\r\n> \r\n> I have a machine with two 1080 Tis. My TF-Serving is able to correctly identify both of the GPUs when the `CUDA_VISIBLE_DEVICES` flag is not set. And I was trying to run one TF-Serving server per GPU, so I opened two terminals to run the following two commands:\r\n> \r\n> > CUDA_VISIBLE_DEVICES=0 bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=inception --model_base_path=/path/to/inception_model\r\n> \r\n> > CUDA_VISIBLE_DEVICES=1 bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9001 --model_name=mnist --model_base_path=/path/to/mnist_model\r\n> \r\n> The first command was running fine, but the second one will reach error says\r\n> \r\n> > terminate called after throwing an instance of 'std::system_error'\r\n> > what():  Resource temporarily unavailable\r\n> > [1]    4021 abort (core dumped)  CUDA_VISIBLE_DEVICES=1  --port=9001 --model_name=mnist\r\n> \r\n> And idea or suggestions? Thanks in advance!\r\n\r\nsry,I've never seen this situation. But can you try only one model run on gpu 2? And try command  \r\n`nvidia-smi`\r\nto check your drive for two gpu work well\uff1f"]}, {"number": 19616, "title": "[Feature Request] print_tensors_in_checkpoint_file should accept Google Cloud Bucket addresses of the form 'gs://...'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMacOS High Sierra\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\nv1.8.0-1660-ga543d94710;  1.9.0-dev20180510\r\n- **Python version**:\r\n3.6.4 \r\nBazel version:\r\nN/A\r\nCUDA/cuDNN version:\r\nN/A\r\nGPU model and memory:\r\nN/A\r\nExact command to reproduce:\r\n```python\r\nfrom tensorflow.python.tools import inspect_checkpoint as chkp\r\nchkp.print_tensors_in_checkpoint_file(\"gs://...\",all_tensors=True)\r\n```\r\n\r\n### Describe the problem\r\nThe function `print_tensors_in_checkpoint_file` does not seem to handle Google Cloud Bucket addresses of the form 'gs://...', but rather gives a \"not found\" error. This does not match the behavior of tf.train.Saver.save() and .restore() \r\n\r\n### Source code / logs\r\nN/A\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I edited the first comment as requested"]}, {"number": 19584, "title": "Prebuilt binaries do not work with CPUs that do not have AVX instruction sets.", "body": "As announced in release notes, TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets. This means on any CPU that do not have these instruction sets either CPU or GPU version of TF will fail to load with any of the following errors:\r\n\r\n- `ImportError: DLL load failed:`\r\n-  A crash with return code 132\r\n\r\nOur recommendation is to build TF from sources on these systems.\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu/windows/macos\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6 and up\r\n- **Python version**:  2.7, 3.3, 3.4, 3.5, 3.6 and any newer\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: any\r\n- **GPU model and memory**: any\r\n- **Exact command to reproduce**: python -c \"import tensorflow as tf\"", "comments": ["We encourage the community to build and share binaries for older CPU models.", "Any chance for you to make the build for the community?", "FYI, here is a Docker image that can build TensorFlow https://github.com/hadim/docker-tensorflow-builder. It can help to compile TF on a wide range of configurations as long as you have Docker installed on it.", "We encourage community supported wheels because for the officially blessed binaries we would like to run rigorous tests.\r\nFor just building, we encourage the community to build and share.", "@gunan Will this change with https://github.com/tensorflow/community/pull/2?", "In the short term that involves this immediate design, no.\r\nHowever this design is a part of a larger effort that will bring back support for older CPUs.\r\nThat will likely take longer to implement.", "CMake version on the other side on windows does not have AVX at all. I needed to enable it explictly locally.", "I've gone ahead and compiled *GPU only* builds for Tensorflow `v1.10.1` against `CUDA 9.1 + cuDNN 7.1`, and `CUDA 9.2 + cuDNN 7.2` and made the wheels and build info available [here](https://github.com/metral/tensorflow-wheels).\r\n\r\nHuge thanks to all of the contributors and the supporting open source community.\r\n\r\nSpecial thanks to @hadim for making his [docker-tensorflow-builder](https://github.com/hadim/docker-tensorflow-builder) available - it served as a great basis to generate these wheels. \ud83c\udf89 ", "Is there any way to detect this and print an appropriate error message?", "We do have some code for this, but unfortunately when compiled with AVX, the machine code generated uses AVX instructions. We are looking into alternatives.", "Well, why just you do not build a matrix of combinations?\r\nAVX -> AVX2 -> GPU -> CPU Only\r\nIf the builds take too much time, prepare docker images which are ready for building and people will build them themselves. ", "@kingofthebongo2008 please see https://github.com/tensorflow/tensorflow/issues/19584#issuecomment-394062139 above", "I've built tf 1.13 for CPU without any fancy instructions here:\r\n\r\nPython 2.7: https://github.com/yaroslavvb/tensorflow-community-wheels/issues/97\r\nPython 3.6: https://github.com/yaroslavvb/tensorflow-community-wheels/issues/103\r\n\r\nHope it helps someone", "I have the same issue. Tried to build binary with bazel but failed. Try this \"pip install tensorflow==1.5\". It works for me. ", "Here's a TensorFlow 1.13.1 binary package for Westmere CPUs (no AVX) and Python 2.7: [yaroslavvb/tensorflow-community-wheels#105](https://github.com/yaroslavvb/tensorflow-community-wheels/issues/105)\r\n\r\n\r\n", "Any compilation for Linux (no AVX) and Python 3.5 (Ubuntu 16.04)?", "Are there plans to release binaries without AVX instructions set for Windows?", "Do you mean without AVX?\r\nOur prebuilt binaries should already be built with AVX.", "Yes, many people complain about not being able to use latest versions of tensorflow because their CPUs don't support AVX. I created an issue on Anaconda github to ask them if they could provide the binaries that are suitable to the user's CPU.", "We have no plans to build and release non-AVX binaries at the moment.", "Too bad, that's prejudicial for many users.", "@Mark531 you can find unofficial built wheel from this [link](https://github.com/fo40225/tensorflow-windows-wheel/), hope it helps you", "@liminai awesome, thank you!", "I have the same problem under Debian 10 Buster, Python v2.7, tensorflow v1.14.0:\r\n```\r\nlsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID: Debian\r\nDescription:    Debian GNU/Linux 10 (buster)\r\nRelease:        10\r\nCodename:       buster\r\n\r\n```\r\n\r\n```\r\npython -c 'import tensorflow'\r\nIllegal instruction\r\n\r\nError log:\r\nNov  4 15:59:00 moodle37 kernel: [9768885.859206] traps: python[2419] trap invalid opcode ip:7f2fc9fbba59 sp:7ffd402810b0 error:0 in libtensorflow_framework.so.1[7f2fc9896000+18f8000]\r\n\r\n```\r\n\r\n\r\n```\r\nsudo lshw -class cpu\r\n  *-cpu\r\n       description: CPU\r\n       product: Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz\r\n       vendor: Intel Corp.\r\n       physical id: 4\r\n       bus info: cpu@0\r\n       version: Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz\r\n       slot: CPU #000\r\n       size: 2100MHz\r\n       capacity: 4230MHz\r\n       width: 64 bits\r\n       capabilities: lm fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp x86-64 constant_tsc arch_perfmon pebs bts nopl xtopology tsc_reliable nonstop_tsc cpuid pni ssse3 cx16 sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer hypervisor lahf_lm pti ssbd ibrs ibpb stibp tsc_adjust arat flush_l1d arch_capabilities\r\n       configuration: cores=1 enabledcores=1\r\n\r\n```\r\n\r\n```\r\npython -V\r\nPython 2.7.16\r\n\r\n```\r\n\r\n\r\n```\r\npip list\r\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Ple                                                                                                                                                             ase upgrade your Python as Python 2.7 won't be maintained after that date. A fut                                                                                                                                                             ure version of pip will drop support for Python 2.7. More details about Python 2                                                                                                                                                              support in pip, can be found at https://pip.pypa.io/en/latest/development/relea                                                                                                                                                             se-process/#python-2-support\r\nPackage              Version\r\n-------------------- ---------\r\nabsl-py              0.8.1\r\nasn1crypto           0.24.0\r\nastor                0.8.0\r\nbackports.weakref    1.0.post1\r\nbzr-etckeeper        0.0.0\r\nconfigparser         3.5.0b2\r\ncryptography         2.6.1\r\ncycler               0.10.0\r\nentrypoints          0.3\r\nenum34               1.1.6\r\nfuncsigs             1.0.2\r\nfuture               0.17.1\r\nfutures              3.3.0\r\ngast                 0.3.2\r\ngoogle-pasta         0.1.7\r\ngrpcio               1.24.3\r\nh5py                 2.10.0\r\nipaddress            1.0.17\r\nKeras-Applications   1.0.6\r\nKeras-Preprocessing  1.1.0\r\nkeyring              17.1.1\r\nkeyrings.alt         3.1.1\r\nMarkdown             3.1.1\r\nmatplotlib           1.5.3\r\nmock                 3.0.5\r\nmoodlemlbackend      1.0.1\r\nnumpy                1.16.5\r\npip                  19.3.1\r\nprotobuf             3.9.1\r\npycrypto             2.6.1\r\nPyGObject            3.30.4\r\npyparsing            2.4.2\r\npython-dateutil      2.8.0\r\npytz                 2019.2\r\npyxdg                0.25\r\nscikit-learn         0.20.4\r\nscipy                0.17.1\r\nSecretStorage        2.3.1\r\nsetuptools           41.2.0\r\nsix                  1.12.0\r\ntensorboard          1.14.0\r\ntensorflow           1.14.0\r\ntensorflow-estimator 1.14.0\r\ntermcolor            1.1.0\r\nWerkzeug             0.16.0\r\nwheel                0.33.6\r\nwrapt                1.11.2\r\n\r\n```", "hi! can i built tensorflow 2.0 without AVX for windows with docker-tensorflow-builder ?", "@s-afanasiev for windows you can find wheel here https://github.com/fo40225/tensorflow-windows-wheel", "Check this repo for more unofficial wheels: https://github.com/yaroslavvb/tensorflow-community-wheels/issues\r\nI found the right one for our server. YAY!", "Something really strange happened to me regarding this issue: I have an i5-3230M CPU, and I was using tensorflow 2 without any problem until yesterday, that I decided to reinstall ubuntu in the machine. As a result, now pip-installed tensorflow cannot be imported. Same machine, same CPU.", "Here's a tensorflow-gpu 1.13.2 binary that I compiled for Linux X86/64 without AV / SSE4.1 / SSE4.2, for Python 3.6 / CUDA 10.0 / CUDNN 7.6.4 / Compute Capability 5.2 and above:  https://github.com/yaroslavvb/tensorflow-community-wheels/issues/69#issuecomment-804988569"]}, {"number": 19556, "title": "[Feature request] Backpropagation through Dataset API", "body": "It would be cool if the Dataset API would support backpropagation.\r\n\r\nCurrenty I'm working on a statistical model with per-sample weights. \r\nTherefore, I have to pass sample indices through the Dataset API and index the weights manually.\r\n\r\n[See my question (especially Allen Lavoie's comment) on stackoverflow.](https://stackoverflow.com/questions/50155021/tensorflow-dataset-api-gradient-is-none?noredirect=1#comment87464556_50155021)\r\n\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: none\r\n- **GCC/Compiler version (if compiling from source)**: none\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: none\r\n", "comments": ["Derek @mrry, please take a look and see if this feature request makes sense to your dev plan.", "This sounds like it could be useful, but it's not something we're planning to work on in the near term. The major complexity arises from our main gradients implementation (`tf.gradients()`), which only works on purely function ops, and is currently unaware of stateful operations like `Iterator.get_next()`. Perhaps the `tfe.GradientTape`\u2013based gradients used in eager execution mode (and which work in graph mode) could more easily be extended to support this, but it's not something anybody's looked at yet.\r\n\r\nI'll throw this open to community contributions, since it would be great if someone took the time to figure out how this could work."]}, {"number": 19543, "title": "TPUEstimator.evaluate() docstring incorrect for steps param", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: TPUv2-8\r\n- **TensorFlow installed from (source or binary)**: VM disk image as configured by ctpu\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: my_tpu_estimator.evaluate(my_input_fn, steps=None)\r\n\r\n### Describe the problem\r\nThe docstring for https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimator#evaluate incorrectly states that ```steps``` can be None, but when called in this way it causes a ```ValueError: Evaluate `steps` must be set on TPU. Cannot be `None`.```\r\n", "comments": ["@rxsang is fixing it to support out-of-range exceptions in the input_fn.", "@rxsang  Was this issue ever fixed? I'm still getting this error on tf 1.15.0."]}, {"number": 19490, "title": "tensorflow cpu module's speed lower on windows than linux", "body": "System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows7 64bit and ubuntu 16.04 64bit\r\n- TensorFlow installed from (source or binary):build tensorflow source to shared lib\r\n- TensorFlow version (use command below):tensorflow v1.3.0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n- Exact command to reproduce:N/A\r\n\r\nDescribe the problem\r\nTraining tensorflow module and detect faces both on windows7 and ubuntu 16.04, but it costs about twice time on windows7 than ubuntu16.04. So we want to know this issue is normal or not? And if it is normal, what's the reason?\r\nwindows7 PC environment:\r\nCPU: Intel Core i3 2120\r\ntime: 80~160 ms\r\n\r\nubuntu16.04 PC environment:\r\nCPU: Intel(R) Core(TM) i3-3220 CPU@3.30GHz\r\ntime: 40~100 ms\r\n", "comments": ["Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "There isn't any fundamental reason, I believe, why performance should be different. However, in general, it's hard to say since the same code is generated using different compilers with different options, so it's possible that some kernels behave differently based on these differences in compiler behavior.\r\n\r\nIf you could profile the code and identify potential differences that might help provide a clue.\r\n\r\nFor now, marking this as \"Community Support\", since hopefully someone out there will have some cycles to dig into this and identify the root cause of the difference.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you."]}, {"number": 19457, "title": "Slot variables used in an optimizer must have the same shape with the variable to be optimized?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu, MacOS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**:  2.7.5\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**: CPU\r\n- **Exact command to reproduce**: When a optimizer is created.\r\n\r\n### Describe the problem\r\n\r\nI write a new optimizer to try some strategies for gradients apply. Some slots are used in the implementation. But I find that slots must have the same shape with the variables to be optimized. Otherwise, an error will be thrown out with message \"shape not match\" when I try to save model.\r\n\r\nThe problem happens in version 1.4. I try the same code in version 1.2, it works correctly.\r\n\r\nSo I want to figure out the reason.\r\n\r\n### Source code / logs\r\n \r\n```python\r\n 1 def _create_slots(self, var_list):\r\n 2   for v in var_list:\r\n 3     with ops.colocate_with(v):\r\n 4       dtype = v.dtype.base_dtype\r\n 5       v_shape = v.get_shape()\r\n 6       if v_shape.is_fully_defined():\r\n 7         init = init_ops.constant_initializer(self._initial_accumulator_value, dtype=dtype)\r\n 8       else:                                  \r\n 9         init_constant = gen_array_ops.fill(array_ops.shape(v), self._initial_accumulator_value)\r\n10         init = math_ops.cast(init_constant, dtype)\r\n11 \r\n12     self._get_or_make_slot_with_initializer(\r\n13        v, init, v_shape, dtype, \"accumulator\", self._name)\r\n14     self._get_or_make_slot_with_initializer(\r\n15        v, init_ops.zeros_initializer(self._global_step.dtype),-\r\n16        v_shape, self._global_step.dtype, \"accumulator_decay_power\", self._name)\r\n```\r\n\r\nIn line 16, if I change 'v_shape' with other value, an error will be got. For example, v_shape=[512, 256], but only [512] is needed to create this slot. \r\n", "comments": ["@asimshankar Hi, what is the progress?", "@allenlavoie : Mind providing some advice here? Thanks.", "Sounds like a reasonable thing to support. What's the shape error stack trace? Worst case I imagine you could just put your own variables in the slot dictionary.", "![stack](https://user-images.githubusercontent.com/9108860/40759117-2d4cd148-64c3-11e8-95d7-2664952e109d.png)\r\n\r\nVariable shape is [44, 512], but I just use [44] as the slot shape. The error stack is as above. No other  variables were used except the slots. \r\n@allenlavoie @asimshankar  Thanks.", "So you came up with some workaround to get the slot variable to have shape [44], it only partly worked, so now the Saver is confused?\r\n\r\nI just hacked a reproduction together, and saving/restoring a smaller slot variable works for me with the most recently nightly:\r\n\r\n```\r\n      self._get_or_make_slot_with_initializer(\r\n          v, init, v_shape[:1], dtype, \"accumulator\", self._name)\r\n      self._get_or_make_slot_with_initializer(\r\n          v, init_ops.zeros_initializer(self._global_step.dtype),\r\n          v_shape, self._global_step.dtype, \"accumulator_decay_power\", self._name)\r\n```\r\n\r\n```\r\nopt = GradientDescentOptimizer(0.01)\r\nopt.minimize(v)\r\nprint(opt.variables())\r\n```\r\n\r\n`[<tf.Variable 'v/GradientDescent:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'v/GradientDescent_1:0' shape=(512, 256) dtype=int64_ref>]`\r\n\r\n```\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as session:\r\n  tf.global_variables_initializer().run()\r\n  saver.save(session, '/tmp/ckpt')\r\n  saver.restore(session, '/tmp/ckpt')\r\n```\r\n\r\nSeems to work with no error. Worth trying an upgrade.", "@jackonan Could you give a minimal reproducible example?", "@allenlavoie I workaround by setting the slot shape with [44, 512]. Waste of memory. The difference from your code is that I use MonitoredTrainingSession and run in distributed mode. The saver object is called in the implement of the MonitoredTrainingSession. I'm not sure whether it affects.", "Still works for me with a `MonitoredTrainingSession`:\r\n\r\n```\r\nglobal_step = tf.train.get_or_create_global_step()\r\ntrain_op = opt.minimize(v, global_step=global_step)\r\nwith tf.train.MonitoredTrainingSession(checkpoint_dir='/tmp/mts/') as session:\r\n  print(session.run(v))\r\n  session.run(train_op)\r\n```\r\n\r\nSeems to read and write checkpoints fine:\r\n\r\n```\r\n>>> reader = tf.train.NewCheckpointReader('/tmp/mts/model.ckpt-3')\r\n>>> reader.get_variable_to_shape_map()\r\n{'v/GradientDescent_1': [512, 256], 'v/GradientDescent': [512], 'v': [512, 256], 'global_step': []}\r\n```\r\n\r\nWithout knowing more about your setup I don't see a way to figure out what the issue is.", "Closing for now, but feel free to re-open with more information if it's still an issue.", "@allenlavoie we have some new infos. @candyzone update", "@allenlavoie I met the same situation.\r\nWhen the Variable is ParititonedVariable, it reproduces in UT\r\n```\r\nimport tensorflow as tf\r\na=tf.get_variable(\"a\", [10],partitioner=tf.fixed_size_partitioner(2))\r\nopt = tf.train.XOptimizer()\r\ntrain_op = opt.minimize(a, name='minimize')\r\nsaver=tf.train.Saver()\r\n```\r\n`XOptimizer` is new optimizer with slot which has the different shape with primary.\r\n\r\nReason:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/training/slot_creator.py#L81\r\n`_create_slot_var` create slot and assign the primary's slice_info to slot's slice_info. In tf.train.Saver() API, it validates the shape and it casues `ValueError: Shapes must be equal rank, but are 0 and 1 for 'save/Assign_6' (op: 'Assign') with input shapes: [], [5].`\r\n\r\nI think the \"slot support different shape with primary\" is reasonable. I will create a PR soon.\r\n", "This is still an issue for optimizerV2, any updates, or workarounds?", "@jackonan We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "@kumariko this is also an issue in tf2"]}, {"number": 19441, "title": "Feature request: Generate java classes from .protos for java library", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Yes\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 5.3.1\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nIt would be nice to be able to handle protobufs within java as java classes, rather than byte arrays, as is currently done. Concretely, this would be useful because we could extract the input and output variable names from, e.g.: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/meta_graph.proto, instead of having to hard-code them in our java code, hoping that they python code which produced the MetaGraphDef hasn't changed the input or output names. I am sure there are other helpful things one could do with programmatic access to their protobufs.\r\n\r\nI don't see a better way to do this than to modify tensorflow's BUILD files to call the rule `java_proto_library` in [tf_proto_library](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/build_config.bzl). Since .proto files are not exported in the python wheel, or the java jar, end-users do not currently have the ability to generate their own files from the .proto files.\r\n\r\nI would be happy to do this if the maintainers think it is a reasonable request and don't consider it to be a burdensome thing to maintain.\r\n\r\n### Source code / logs\r\nN/A\r\n", "comments": []}, {"number": 19421, "title": "Feature Request:  128-bit floats", "body": "I have posted a question and answer on [SO](https://stackoverflow.com/questions/50438071/cost-function-convergence-in-tensorflow-using-softmax-cross-entropy-with-logits/50438072#50438072) showing why this is needed.  Admittedly it is probably not a common case, but I do research and the problems I deal with usually require creative solutions and so I'm generally pushing the boundaries of packages like Tensorflow (which is a wonderful package!).  I ran into this with soft-target classification.  \r\n\r\n> with soft targets, especially ones that aren't close to 1 or zero, cross entropy loss doesn't change significantly as the algorithm improves. Let's say the targets are [0.39019628, 0.44301641, 0.16678731]. Well, using the formula for cross entropy\r\n\r\n`cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))`\r\n\r\n> but then using the targets \"y_\" in place of the predicted probabilities \"y\" we arrive at the true entropy value of 1.0266190072458234. If you're predictions are just slightly off of target....lets say they are [0.39511779, 0.44509024, 0.15979198], then the cross entropy is 1.026805558049737.\r\n\r\nBasically even with 64-bit floats, the loss function shows evidence that it would continue to train if further significant digits were available.  This is just to support the cost function during the training process since the final trained values would not need such precision, but to get to an optimal convergence I need it.\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Yes sorry...\r\n\r\nHave I written custom code:  ?? Don't know what this means\r\nOS Platform and Distribution:  Windows 2016 Server\r\nTensorFlow installed from:  Pycharm\r\nTensorFlow version:  1.8\r\nBazel version\r\nCUDA/cuDNN version:  9.0\r\nGPU model and memory:  GTX 1080 TI (I'm using the CPU in this case though for optimization)\r\nExact command to reproduce:  Well, I fit the model with softmax cross entropy for 3 categories and used soft targets with two of the categories *close* to 0.5 (~0.36-0.46).  This lead to issues with significant figures since the loss does not change much even when the model improves significantly."]}, {"number": 19395, "title": "Gridrnn (Grid2LSTM) tied behaviour is inverted", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen trying to create a unrolled sequence for a `Grid2LSTMCell`, weights are shared among two states (the two dimensions) for a given step, when the property `tied` is set to *False* and they are not shared when `tied` is set to *True*. If this is indeed the behaviour that was desired, then it deviates from the original paper based on which it is implemented, otherwise it seems to be a bug in variable reuse.\r\n\r\n**Expected Behaviour**: Share weights when `tied` is True and do not share when `tied` is False\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.rnn.python.ops import rnn_cell\r\nfrom tensorflow.contrib import grid_rnn\r\nfrom tensorflow.python.ops import variable_scope\r\n\r\nbatch_size = 32\r\nseq_len = 10\r\n# inp => sequence_length x batch_size x embedding_size\r\ninp_grid = tf.placeholder(tf.float32, shape=(seq_len, batch_size, 2048)) \r\nrnnargs = {\r\n            'use_peepholes': True, 'forget_bias': 1.0,\r\n            'state_is_tuple': False, 'output_is_tuple': False,\r\n            'output_is_tuple': True, 'state_is_tuple': True,\r\n            'tied': False\r\n            }\r\nglstm = grid_rnn.Grid2LSTMCell(1024, **rnnargs)\r\nstate = glstm.zero_state(batch_size, tf.float32)\r\nfor i in range(seq_len):\r\n    if i>0:\r\n        variable_scope.get_variable_scope().reuse_variables()\r\n    state_left, state_top = state[0], state[1] # since two dimensions\r\n    out, state = glstm(inp_grid[i], (state_left, state_top))\r\n    # If you observe the actual tensor (name) of the two states, \r\n    # they will be same when tied is False \r\n    # and different when tied is true\r\n    print(state[0],\"\\t\",state[1],\"\\n\")\r\n```\r\n\r\nI think this is a bug related to variable reuse instead of how cells are defined in `grid_rnn` constructor. I would like to work on this issue if this indeed is an issue, or else a explanation of why this is the correct behaviour will do.", "comments": ["@ebrevdo would you please take a look or reassign to someone appropriate? ", "@phvu can you PTAL?"]}, {"number": 19391, "title": "Feature Request: Use hwloc to query CPU topologies and support thread/memory binding for improved performance", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nFeature request to add hwloc (https://www.open-mpi.org/projects/hwloc/) to TensorFlow's third-party dependencies, use hwloc APIs to query CPU topologies in a portable manner and bind threads/memory for improved performance\r\n\r\nThis is following up on some of the discussion here: https://github.com/tensorflow/tensorflow/pull/19136", "comments": ["We're looking into adding NUMA support to TF via extending the platform interface, rather than adopting a library that isn't universally supported.  The contemplated functionality is a subset of that provided by libnuma.  I'll update this thread when there's something definitive.", "The nature of the NUMA extension is becoming more clear.\r\nNew functions have been added to the platform interface for NUMA support, that do not yet have an implementation, but will necessarily be platform specific.   There's an upcoming change to process_state.cc and some ThreadPool related classes that will optionally enable NUMA specific CPU devices when an implementation of this interface is available.   \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/numa.h\r\n\r\nIt's not possible to include libnuma with TF due to license restrictions.  I don't yet know whether we might be able to use hwloc to implement the platform functions for posix environments.", "Nagging Assignee @poxvoculi: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The interface tensorflow/core/platform/numa.h needs an implementation.  Eventually I will write one, probably using hwloc, but it would be great if someone else would do it first.  So I'm going to mark this as contributions-welcome for now.", "@poxvoculi Are you currently working on this? I saw the BUILD file for `hwloc` but not the actual implementation. If you are busy working on other stuff, I could take a shot.", "@gunan I tried compiling `//tensorflow/core:platform_port` with `@hwloc` in its deps, and I got this error:\r\n\r\n```\r\nSUBCOMMAND: # @hwloc//:hwloc [action 'Compiling external/hwloc/hwloc/components.c']\r\n(cd /home/byronyi/.cache/bazel/_bazel_byronyi/294c5cbab5ae82f218d48ba2056277d9/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/bin:/usr/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/byronyi/.virtualenv/tf/bin/python \\\r\n    PYTHON_LIB_PATH=/home/byronyi/.virtualenv/tf/lib/python2.7/site-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/k8-opt/bin/external/hwloc/_objs/hwloc/components.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/hwloc/_objs/hwloc/components.pic.o' -fPIC -iquote external/hwloc -iquote bazel-out/k8-opt/genfiles/external/hwloc -iquote bazel-out/k8-opt/bin/external/hwloc -I. -Ihwloc -Iinclude -Wno-vla '-DHWLOC_DUMPED_HWDATA_DIR=' '-DRUNSTATEDIR=' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/hwloc/hwloc/components.c -o bazel-out/k8-opt/bin/external/hwloc/_objs/hwloc/components.pic.o)\r\nERROR: /home/byronyi/.cache/bazel/_bazel_byronyi/294c5cbab5ae82f218d48ba2056277d9/external/hwloc/BUILD.bazel:26:1: C++ compilation of rule '@hwloc//:hwloc' failed (Exit 1)\r\nexternal/hwloc/hwloc/bind.c:9:10: fatal error: private/autogen/config.h: No such file or directory\r\n #include <private/autogen/config.h>\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nTarget //tensorflow/core:platform_port failed to build\r\n```\r\n\r\nAny suggestions from your side?", "@poxvoculi\r\n\r\nWould it be acceptable to implement NUMA aware scheduling and memory allocation using Linux API directly? It seems to me `hwloc` provides way more features than necessary to implement functions in `tensorflow/core/platform/numa.h`, and it seems more difficult than I expect to introduce `hwloc` as a third party dependency.\r\n\r\nIt seems to me that the key recipes of implementing the functions are\r\n\r\n* `sched_setaffinity/sched_getaffinity` in glibc\r\n* `__NR_mbind/__NR_get_mempolicy` in Linux syscall\r\n\r\nof which the header files are already included in `tensorflow/core/platform/posix/port.cc`.\r\n\r\nThe necessary information of NUMA nodes, in particular, associated CPU bitset, could be read from `/sys/devices/system/node/nodeX/cpuY`.", "I think I have a change that adds a build file for hwloc on linux.\r\nIt does not work yet on macos and windows. But maybe I can submit that and we can work together to add the windows and macos support to it?", "@gunan The above message was obtained in Linux. Maybe I made a mistake when adding it as a dep in BUILD file; do you  have a quick test command to show that it\u2019s working?", "It has not been merged yet, let me merge that and then we can retry.\n\nOn Tue, Jan 22, 2019 at 2:03 PM Bairen Yi <notifications@github.com> wrote:\n\n> @gunan <https://github.com/gunan> The above message was obtained in\n> Linux. Maybe I made a mistake when adding it as a dep in BUILD file; do you\n> have a quick test command to show that it\u2019s working?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19391#issuecomment-456580423>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOb25ozQMZi2kRiUkJBTUY7LdpZVRks5vF4rLgaJpZM4UFDt5>\n> .\n>\n", "@gunan Sounds great. I could try to get it working on macOS after I take a look at it.\r\n\r\nI'd love to do Windows too, but I do not have a Windows box :)", "@gunan Thanks, I see your CL in b682bdd374ff453f2ebb85d9e8a2b3eeed79df4f.", "Thanks for your patience, it took a while to get it merged.\r\nAs far as I can tell, this should be all we need to be able to build hwloc on linux.", "@byronyi Sorry for not keeping up with this thread.  I've had an internal change under review since the beginning of December to add an hwloc based implementation of the port/numa.h interface.  The blocking problem when I last worked on it was bazel difficulties.  Then came the holidays, vacations, and on my part a record number of close-spaced respiratory virus infections.  I've been out sick for a week and I'm just starting to look at email again.  I'll see whether my change can move forward now.", "@poxvoculi Sorry to hear that and hope you feel better soon. There is no hurry :P", "@jbobba I believe NUMA support is added in a6bf9c8476a4acd37fe5f400bc01d9d4beacdfc2", "Note that I got the following error from the commit above:\r\n\r\n```\r\nERROR: /home/byronyi/.cache/bazel/_bazel_byronyi/294c5cbab5ae82f218d48ba2056277d9/external/hwloc/BUILD.bazel:213:1: C++ compilation of rule '@hwloc//:hwloc' failed (Exit 1)\r\nIn file included from external/hwloc/include/private/private.h:29,\r\n                 from external/hwloc/hwloc/topology-noos.c:11:\r\nexternal/hwloc/include/private/misc.h:491:10: fatal error: xlocale.h: No such file or directory\r\n #include \"xlocale.h\"\r\n          ^~~~~~~~~~~\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nIt builds after reverting that.", "Nevermind it seems to be fixed by d624b1d75a640c50be281d8b921d62dbd6149f2a.", "> @jbobba I believe NUMA support is added in [a6bf9c8](https://github.com/tensorflow/tensorflow/commit/a6bf9c8476a4acd37fe5f400bc01d9d4beacdfc2)\r\n\r\nThank you for your interest @byronyi and thanks @poxvoculi for the implementation. Quite excited to see this.", "Hello, I am starting to implement numa support for mkl allocators(CPU's) and have questions on tensor allocations. Current tensor allocations through OpKernelContext::allocate_output(...) use GetAllocator() and there appears to be no way to pass the numa_node dynamically through this interface. Path we take today is the one below.\r\nallocate_output->GetAllocator->MklCPUAllocator->SubAllocator->Alloc->NUMAMalloc\r\nThe  current implementation of SubAllocator->Alloc(...) does not determine the numa node based on the request but rather uses a member variable. \r\nSo is the plan to have an allocator instance for each numa node?\r\n ", "The intended approach is to define a separate CPU device for each NUMA node, then explicitly place operations on a specific device.   The allocator for each device will allocate all memory local to that node, and its execution threads will be bound to that node.\r\n\r\nFor example, something like the following (may contain python syntax errors):\r\n\r\nwith tf.device('/cpu:0'):\r\n    a = ...\r\n\r\nwith tf.device('/cpu:1'):\r\n   b = ...\r\n\r\nconfig = tf.ConfigProto\r\nconfig.experimental_use_numa_affinity = True\r\nconfig.device_count['cpu;] = 2\r\nsess = tf.Session(config=config)", "Thanks for the clarification.", "Also why is numa not enabled in ProcessState? \r\nProcessState::ProcessState() : numa_enabled_(false) {}\r\n\r\nThanks\r\n"]}, {"number": 19390, "title": "depthwise_conv2d_native too slow", "body": "***edit***: Simplified the example, added system info\r\n\r\nAccording to this [thread](https://github.com/tensorflow/tensorflow/pull/17961) Tensorflow now uses the CuDNN accelerations of group convolutions for depthwise_conv2d_native. Thank you for working on this! However, I am having a hard time reproducing any gains from the accelerated version. Both the native and accelerated versions of depthwise_conv2d_native is about 3-4 times slower than doing a full (dense) convolution.\r\n\r\nIn the example below, a dense 3x3 convolution with 64-in and 64-out channels should do about 16 times more multiplications compared to a group convolution with the same dimensions and 16 groups (64x3x3x64 vs 16x4x3x3x4). The latter can be implemented with depthwise_conv2d_native with channel_multiplier of 4 followed by a sum. So I expect a fully amortized depthwise_conv2d_native to be 16 times faster than a dense convolution, and yet, it is about 4 times slower. It is also substantially slower than naive slice/convolve/concat implementation of group convolution\r\n\r\n\r\n### System information\r\n- **Have I written custom code**: no\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source)**: from May 15, 2018 commit 1521eeb676383417b33ad55ad73b152bd5b046ca\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 9.1, 7.1.3\r\n- **GPU model and memory**: GeForce GTX 1080ti (tf compiled for compute capability 6.1) and Volta (V100, tf compiled for compute capability 7.0)\r\n- **Exact command to reproduce**: see code below\r\n\r\n\r\nHere are my results on GTX 1080ti:\r\n```\r\n           depthwise : 2.72s\r\n     depthwise_cudnn : 2.59s\r\n   manual_group_conv : 0.84s\r\n          dense_conv : 0.72s\r\n```\r\nHere is the code I used to test performance:\r\n```\r\nimport numpy as np, time, os\r\nimport tensorflow as tf\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\ndef conv3x3(bottom, filters):\r\n    return tf.layers.conv2d(bottom, filters, kernel_size = 3, padding='same', use_bias = False, data_format = 'channels_first')\r\n\r\n# this is not even group convolution, just the depthwise part without the sum\r\ndef depthwise(bottom, num_groups):\r\n    input_chans = bottom.shape[1]\r\n    group_size = input_chans / num_groups\r\n    w0 = tf.get_variable(name='var', shape=[3, 3, input_chans, group_size])\r\n    return tf.nn.depthwise_conv2d_native(bottom, w0, strides = [1,1,1,1], padding = 'SAME', data_format = 'NCHW')\r\n\r\ndef depthwise_cudnn(bottom, num_groups):\r\n    with tf.get_default_graph()._kernel_label_map({\"DepthwiseConv2dNative\": \"cudnn_grouped_convolution\"}):\r\n        return depthwise(bottom, num_groups)\r\n\r\ndef manual_group_conv(bottom, num_groups):\r\n    input_chans = bottom.shape[1]\r\n    group_size = input_chans / num_groups\r\n    slices = [bottom[:,i:(i+group_size)] for i in range(0, input_chans, group_size)]\r\n    convs = [conv3x3(sl, group_size) for sl in slices]\r\n    return tf.concat(convs, axis = 1)\r\n\r\ndef dense_conv(bottom, num_groups):\r\n    return conv3x3(bottom, bottom.shape[1])\r\n\r\ninput_shape = [16, 64, 128, 128]\r\ngroups = 16\r\ndtype = tf.float32\r\n\r\nfor cnv_type in [depthwise, depthwise_cudnn, manual_group_conv, dense_conv]:\r\n    tf.reset_default_graph()\r\n\r\n    cnv = cnv_type(tf.constant(np.zeros(input_shape), dtype), groups)\r\n\r\n    N = 100\r\n    with tf.Session('') as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        sess.run(cnv) # initialization run\r\n\r\n        start = time.time()\r\n        for i in range(N):\r\n            sess.run(cnv)\r\n\r\n        print \"%20s : %4.2fs\" % (cnv_type.func_name, time.time() - start)\r\n\r\n```\r\n\r\nI also tested with NHWC with similar results. Let me know if I am doing something wrong or whether you can replicate my perf results. Thanks!\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tfboyd could you take a look?", "@zheng-xq any idea?", "@protoget do you know who would be able to handle that?", "@chsigg Christian, I wonder if you worked on a custom kernel for depthwise conv before?", "same problem, need group conv"]}, {"number": 19376, "title": "The channel dimension of the inputs is `None` when tf.layers.conv2d after tf.slice with tf.shape instead of constant value", "body": "Here is my sample code to test the case:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nif __name__ == '__main__':\r\n    n_channel = 3\r\n    n_layer = 5\r\n    input_tensor = tf.placeholder(tf.float32, shape=[None, None, None, n_channel])\r\n    input_image = np.random.rand(4, 512, 512,n_channel)\r\n\r\n    slice = tf.slice(input_tensor, [0,0,0,0], [-1, tf.shape(input_tensor)[1], tf.shape(input_tensor)[2], -1])\r\n    conv = tf.layers.conv2d(slice,filters=16,kernel_size=3,strides=1,padding='valid')\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        output_image = sess.run(conv,feed_dict={input_tensor:input_image})\r\n    print(output_image.shape)\r\n```\r\nHere is the error:\r\n```python-traceback\r\nTraceback (most recent call last):\r\n  File \"test_concat_then_conv.py\", line 11, in <module>\r\n    conv = tf.layers.conv2d(slice,filters=16,kernel_size=3,strides=1,padding='valid')\r\n  File \"/home/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 619, in conv2d\r\n    return layer.apply(inputs)\r\n  File \"/home/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 825, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/home/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 696, in __call__\r\n    self.build(input_shapes)\r\n  File \"/home/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 133, in build\r\n    raise ValueError('The channel dimension of the inputs '\r\nValueError: The channel dimension of the inputs should be defined. Found `None`.\r\n```\r\nThis happen only when I used `tf.layers.conv2d`. It works with `tf.nn.conv2d`.\r\nI wonder what is the root of this problem?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "- Have I written custom code?\r\nYes\r\n- OS Platform and Distribution?\r\nLinux - Ubuntu 16.04\r\n- TensorFlow installed from?\r\nAnaconda3\r\n- TensorFlow version?\r\n1.7.1\r\n- Bazel version?\r\nI don't know.\r\n- CUDA/cuDNN version?\r\nCUDA 9.0, cuDNN 7.1\r\n- GPU model and memory?\r\nGPU GTX 1050 Ti\r\n- Exact command to reproduce?\r\nI run the above code.", "Part of the issue is that `tf.slice` is not able to figure out the shape if the size is `-1`. So `slice` tensor in the code snippet has the shape of `[None, None, None, None]`. That triggers the error.\r\n\r\nThere was a PR #13561 to improve the shape function of `tf.slice` though it was not a complete fix."]}, {"number": 19324, "title": "[feature request] large scale embedding for sparse features", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.0 release\r\n- **Python version**: python2.7\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc4.8.5\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: -\r\n\r\nIn my scene, training data is very large, we have 10e10 unique values(string) for embedding in one sparse column. In short, model structure is \u201cinput->embedding->NNs\u201d. I set hash_bucket_size to 15e10 for low hash collision. sample code:\r\n```\r\ncol0=tf.contrib.layers.sparse_column_with_hash_bucket(\"feature_id\", hash_bucket_size=1.5e11)\r\ncols=[tf.feature_column.embedding_column(categorical_column=col0, dimension=8)]\r\n```\r\nin above example, Tensorflow new a Variable with shape [1.5e11, 8]\r\nSome problems:\r\n1. memory waste, more than 0.5e11\\*8\\*4Bytes memory not used. \r\n2. hash collision, though enlarging the `hash_bucket_size`, it might occurs conflict. I don't know how much it influence on model. \r\n\r\nIs there any suggestions in Tensorflow in this case ?\r\nIn my opinion: \r\nDefine a new Variable for embedding, need not define the first demension. It will malloc memory for this Variable when the new value is embedding_lookup. It will solve the two problems mentioned above.\r\n\r\nThanks", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler @tatianashp \r\nupdated, thanks", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "@tatianashp \r\nTensorFlow cannot solve the problem that two features hash collision when the we set fixed hash_bucket_size in current TensorFlow version.\r\nI think this is a \"feature request\". We need an API without the parameter \u201chash_bucket_size\u201d, and its size dynamic grows when a new feature is coming. ", "cc @ispirmustafa @roumposg, who may have more thoughts on this issue / feature request.\r\n\r\nI question I'm wondering is whether hash-based embedding is appropriate for sparse features with number of unique values as large as 10e10.", "(I am re-opening this issue because it might be a legitimate feature request.)", "Hi @candyzone ,\r\nI would use multiple hash keys to mitigate hash collisions. It will let model to differentiate collisions. \r\nIt's implemented as `hash_keys` argument of `sparse_column_with_hash_bucket`.", "@caisq @ispirmustafa thanks, I see that API's usage, but it cannot solve my problem.\r\nFirst is memory waste, we store embedding in Variable (a Tensor), only part of Variable is valid, the others wouldn't be lookup, this part of Variable is invalid, waste.\r\nSecond is Variable's first dimension(API's parameter hash_bucket_size) is fixed, so that it may cause hash collision and limit feature number in OnlineLearning case.\r\nI think TensorFlow can provide a EmbeddingVariable (inherit from tf.Variable) that has an unknown first dimension and fixed other dimensions in Variable definition, it can implement using hash-base structure. ", "I have the similar requirements and it is also an essential problem in online learning.", "We currently don't have anybody working on this. It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!\r\n", "@yuefengz Here is my issue.", "Reopen this issue since we get huge number of similar feature requests in China. But we will still need to think about how to support it. Contribution is welcomed.", "Anyone starts working on this? Looks like this is a common requirements for online learning.", "> I have the similar requirements and it is also an essential problem in online learning.\r\n\r\nme too", "paddlepaddle has the similar feature in paddle.fluid.layers.embedding", "@candyzone TensorFlow use static computation graph, which means that TensorShape of inputs and outputs in each Op must be inferred at graph construction. So, I think it's not possible to dynamically grow embedding tensor's dim size in online learning case. (Correct me if I'm wrong..LOL)", "> @candyzone TensorFlow use static computation graph, which means that TensorShape of inputs and outputs in each Op must be inferred at graph construction. So, I think it's not possible to dynamically grow embedding tensor's dim size in online learning case. (Correct me if I'm wrong..LOL)\r\n\r\nFor dense layer, Variable's shape must be fixed, because the auto differential depend on the Variable\u2018s shape, gradient are dense Tensor.\r\nBut for sparse layer, gradients is list<pair<index, gradient>>, it allow us to make a SparseTensor that first dim is mutable, other dims is fixed. It seems that works well in Variable update.", "@alextp \r\nI saw RFCs authored by you (https://github.com/tensorflow/community/blob/master/rfcs/20180817-variables-20.md), Does TensorFlow support this kind of Variable in TensorFlow 2.0?", "@candyzone I think make mutable_hash_table_of_tensors trainable should work, and I open a new issue here: https://github.com/tensorflow/tensorflow/issues/24539. Would you please help me to see if my idea is reasonable?\r\n\r\n", "In principle that's implementable, but the current tf Variable ops do not support this type of dynamic growth.\r\n\r\nIf you'd like to extend them to do so please write an RFC and we'll discuss there. I agree this is potentially very useful."]}, {"number": 19293, "title": "Feature request: Mask R-CNN support on TensorFlow Lite", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux openSUSE Leap 42.3\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nThis is a feature request about supporting Mask R-CNN on TensorFlow Lite. Thanks.\r\n\r\n### Source code / logs\r\nCurrently, the following operators are not supported by TensorFlow Lite: `ResizeNearestNeighbor`, `Stack`, and `TensorFlowShape`.\r\n\r\n`Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: ResizeNearestNeighbor, Stack, TensorFlowShape.`\r\n\r\n", "comments": ["The Stack op is now supported in nightly and we are looking into adding more op support however RNNs introduce other challenges and we are hoping to provide examples to make this easier.", "@anitha-v This is a CNN..It will work if we have the operations in Tensorflow Lite", "@JaviBonilla Stack and Shape are supported in Tensorflow Lite now. We are looking in to ResizeNearestNeighbor", "@achowdhery Great! Thanks for letting me know, I\u2019ll stay tuned for the remaining operator", "Still working on prioritizing the ResizeNearestNeighbor op, though it might be a number of weeks until it's ready.", "Is there any update on the ResizeNearestNeighbor op? Thanks!", "Yes! This op was added in https://github.com/tensorflow/tensorflow/commit/86a7867840d290f6d80b20765b4635c217700bd4. It's available either by building from head or in the next TensorFlow release (though there's no ETA for the latter yet).", "@JaviBonilla Please let us know if you are successfully able to convert the model to Tensorflow Lite now.", "@achowdhery I will check in the next weeks and let you know how it goes :-), thanks a lot!", "I am trying to generate the **tflite** file from [mask rcnn inception v2](http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz)  with the following command,\r\n\r\n    bazel run -c opt tensorflow/lite/toco:toco -- \\ \r\n    --input_file=frozen_inference_graph.pb \\  \r\n    --output_file=mask_rcnn_inception.tflite \\\r\n    --input_shapes=1,300,300,3 \\\r\n    --input_arrays=image_tensor \\ \r\n    --output_arrays='num_detections','detection_classes','detection_scores','detection_boxes','detection_masks' \\\r\n    --inference_type=QUANTIZED_UINT8 \\\r\n    --mean_values=128 \\\r\n    --std_values=128 \\\r\n    --change_concat_input_ranges=false \\\r\n    --allow_custom_ops \\\r\n    --experimental_local_memory_estimate\r\n\r\nbut I am getting this error, am I missing any previous step?\r\n\r\n    .......\r\n    I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: CropAndResize\r\n    I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 7262 operators, 13617 arrays (0 quantized)\r\n    I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 7208 operators, 13526 arrays (0 quantized)\r\n    I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 7208 operators, 13526 arrays (0 quantized)\r\n    F tensorflow/lite/toco/graph_transformations/resolve_constant_slice.cc:59] Check failed: dim_size >= 1 (0 vs. 1)", "I think this issue is related to #24407. I will have a look at the info over there.", "> I am trying to generate the **tflite** file from [mask rcnn inception v2](http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz) with the following command,\r\n> \r\n> ```\r\n> bazel run -c opt tensorflow/lite/toco:toco -- \\ \r\n> --input_file=frozen_inference_graph.pb \\  \r\n> --output_file=mask_rcnn_inception.tflite \\\r\n> --input_shapes=1,300,300,3 \\\r\n> --input_arrays=image_tensor \\ \r\n> --output_arrays='num_detections','detection_classes','detection_scores','detection_boxes','detection_masks' \\\r\n> --inference_type=QUANTIZED_UINT8 \\\r\n> --mean_values=128 \\\r\n> --std_values=128 \\\r\n> --change_concat_input_ranges=false \\\r\n> --allow_custom_ops \\\r\n> --experimental_local_memory_estimate\r\n> ```\r\n> but I am getting this error, am I missing any previous step?\r\n> \r\n> ```\r\n> .......\r\n> I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: CropAndResize\r\n> I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 7262 operators, 13617 arrays (0 quantized)\r\n> I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 7208 operators, 13526 arrays (0 quantized)\r\n> I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 7208 operators, 13526 arrays (0 quantized)\r\n> F tensorflow/lite/toco/graph_transformations/resolve_constant_slice.cc:59] Check failed: dim_size >= 1 (0 vs. 1)\r\n> ```\r\n\r\n@JaviBonilla hi, have you solved this problem?", "Hi @noah003,\r\n\r\nNot yet, I didn't have much time to investigate it though, I'll let you know if I find something.", "The generic commands will not work. You will need to inspect the graph to find the correct input/output nodes.\r\nFor segmentation, you may be able to leverage the GPU version of the model released here:\r\nhttps://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7", "> The generic commands will not work. You will need to inspect the graph to find the correct input/output nodes.\r\n> For segmentation, you may be able to leverage the GPU version of the model released here:\r\n> https://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7\r\n\r\n@achowdhery it's hard to find the input/output nodes even by the tensorboard, it's there a more convenient way?", "> Hi @noah003,\r\n> \r\n> Not yet, I didn't have much time to investigate it though, I'll let you know if I find something.\r\n\r\n@JaviBonilla Thanks!", "@achowdhery I'm use the output_node_names form research/object_detection/exporter.py, but still cannot work...", "Hi it seems that `ResizeNearestNeighbor` still not work? thanks!", "> Hi it seems that ResizeNearestNeighbor still not work? thanks!\r\n\r\nWhat version of TensorFlow(Lite) are you using? You'll need to be on the latest nightly to use this TensorFlow Lite op.\r\n", "> > Hi it seems that ResizeNearestNeighbor still not work? thanks!\r\n> \r\n> What version of TensorFlow(Lite) are you using? You'll need to be on the latest nightly to use this TensorFlow Lite op.\r\n\r\nI am on 1.12.0. So shall I use 1.13.0, 1.14.0 or 2.0? Thanks!", "I would give 1.14.0 a try and let me know how it goes, thanks.", "> I would give 1.14.0 a try and let me know how it goes, thanks.\r\n\r\nOK I will try that, thanks! (btw, yesterday I use another workaround, change nearest neighbor to bilinear upsampling and it works)", "does anyone already know how to make it work?", "Hi @jdduke \r\n\r\nIt is not working for me with `tf-nightly 1.14.1.dev20190405`.\r\n\r\n\r\n```\r\ntflite_convert \\\r\n  --graph_def_file=./mask_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb \\\r\n  --output_file=./mask_rcnn_inception.tflite \\\r\n  --input_arrays=image_tensor \\\r\n  --input_shapes=1,224,224,3 \\\r\n  --output_arrays=detection_masks,detection_classes,detection_boxes,detection_scores,num_detections \\\r\n  --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS\r\n```\r\n\r\nError message:\r\n\r\n\r\n```\r\n...........\r\n2019-04-05 16:05:57.074330: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3\r\n2019-04-05 16:05:58.130320: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 6566 operators, 12219 arrays (0 quantized)\r\n2019-04-05 16:05:59.072672: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 6529 operators, 12161 arrays (0 quantized)\r\n2019-04-05 16:06:00.249379: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 6529 operators, 12161 arrays (0 quantized)\r\n2019-04-05 16:06:01.669340: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 6100 operators, 11326 arrays (0 quantized)\r\n2019-04-05 16:06:03.167713: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 6099 operators, 11325 arrays (0 quantized)\r\n2019-04-05 16:06:04.447879: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 6098 operators, 11323 arrays (0 quantized)\r\n2019-04-05 16:06:05.733437: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 6098 operators, 11323 arrays (0 quantized)\r\n2019-04-05 16:06:06.712519: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 6098 operators, 11323 arrays (0 quantized)\r\n2019-04-05 16:06:07.448395: F tensorflow/lite/toco/tooling_util.cc:641] Check failed: dim >= 1 (0 vs. 1)\r\nFatal Python error: Aborted\r\n```", "See https://www.tensorflow.org/lite/models/segmentation/overview for a more detailed overview of using the DeepLab segmentation, for which we have a .tflite model available. We'll take a look at the MaskRCNN conversion failure.", "Oh, that\u2019s great! Thanks for the info @jdduke, I am going to check it \ud83d\ude0a", "@JaviBonilla @jdduke \r\nHi Guys, just checking back any updates on this thread?\r\nMillion thanks!", "No updates, we're still investigating.", "Hi @JaviBonilla @jdduke , any updates? Thanks!", "Hi @jdduke,\r\n\r\nI've tested with `tf-nightly 1.14.1.dev20190612`\r\n\r\n```\r\ntflite_convert \\\r\n  --graph_def_file=./mask_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb \\\r\n  --output_file=./mask_rcnn_inception.tflite \\\r\n  --input_arrays=image_tensor \\\r\n  --input_shapes=1,224,224,3 \\\r\n  --output_arrays=detection_masks,detection_classes,detection_boxes,detection_scores,num_detections \\\r\n  --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS\r\n```\r\n\r\nand I get a different error.\r\n\r\n```\r\n..........\r\n2019-06-14 11:43:28.172266: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: NonMaxSuppressionV2\r\n2019-06-14 11:43:28.172790: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-06-14 11:43:28.173179: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3\r\n2019-06-14 11:43:28.173452: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3\r\n2019-06-14 11:43:28.173469: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3\r\n2019-06-14 11:43:28.173483: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3\r\n2019-06-14 11:43:29.230199: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 6800 operators, 13059 arrays (0 quantized)\r\n2019-06-14 11:43:30.278421: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 6750 operators, 12970 arrays (0 quantized)\r\n2019-06-14 11:43:32.153509: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 6750 operators, 12970 arrays (0 quantized)\r\n2019-06-14 11:43:32.539125: F tensorflow/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:98] Check failed: other_op->type == OperatorType::kMerge Found Sub as non-selected output from Switch, but only Merge supported.\r\nFatal Python error: Aborted\r\n```\r\nI am reporting in case this helps.", "Any updates yet @JaviBonilla? Currently working on a project and I need to convert or find a way to run on my TPU.", "@mali30 did u solve it? I tried freezing the graph and converting to .pb to convert into .tflite. Didnt work. Im also trying to deploy on edge tpu.", "no I could not get it to work. I am sorry my friend @NicholaiStaalung ", "@mali30 ok, thx for letting me know! Good luck", "@mali30 sorry, somehow I missed your comment. I didn\u2019t have luck yet, I will inform here if I make any progress.", "Hi all,\r\n\r\nJust wanted to check if any progress has been made here since August? Just tried locally converting a Mask RCNN network to TFLite format and unfortunately, no luck.\r\n\r\nThank you,\r\n\r\nEdgars", "While it is possible to convert a MaskRCNN model to TFLite using our new converter, there are some major drawbacks:\r\n1. It requires using the [Select TF ops in TFLite](https://www.tensorflow.org/lite/guide/ops_select) feature, as not all ops in MaskRCNN are supported on TFLite (Size, CropandResize, LinSpace). This adds binary size & latency.\r\n2. The latency is *very* high (read: >8s), mainly because MaskRCNN uses heavy convolutions and the pre-trained models available on our model zoo have large inputs (1365*800). Since the graph has dynamic tensors, acceleration on NNAPI/GPU isn't currently supported. If I modify the inference parameters in `pipeline.config` to make inference faster, it degrades the high accuracies of MaskRCNN.\r\n\r\nSo to keep things feasible, I would like to understand the requirements here - is it bounding boxes (in that case, our SSD/SSDLite models are much better) or is it segmentation (we have some decent models for that). Otherwise, we could also try to convert research-y (not supported by the object detection team) models like [these](https://github.com/gustavz/Mobile_Mask_RCNN), but the mAP/latency tradeoff doesn't seem that good, considering we might get comparable accuracy with the SSD floating point models.", "Hi @srjoglekar246,\r\n\r\nThanks for the effort and the information. It is great that it is already working. For our use case, a latency of seconds is probably enough.\r\n\r\nWe are doing research in solar tracking for concentrating solar power, so basically we have a Raspberry Pi and a camera on each mirror (called heliostat) to automatically track the Sun and detect a receiver (at the top of a tower) to reflect and concentrate the solar irradiance onto the receiver, where high temperature is reached to produce high-pressure steam, move a turbine and produce electricity.\r\n\r\nWe have a mobile app to do manual tests in the solar field, a 3D-printed mockup for preliminary tests and a prototype for testing in the real plant. Some pictures are in these articles.\r\n\r\nhttps://aip.scitation.org/doi/pdf/10.1063/1.5117524?class=pdf (Figure 6, mobile app example, Figure 5 real plant example).\r\n\r\nhttps://arxiv.org/pdf/1809.07048.pdf (Figure 6, prototype at the real plant).\r\n\r\nhttps://lanochedelosinvestigadores.fundaciondescubre.es/actividades/heliostato-inteligente/ (Pictures of the 3D-printed mockup, we are using here the Coral USB accelerator to have a really fast response).\r\n\r\nWe are already testing with retrained SSD models, but we would also like to test if Mask R-CNN can better frame the objects and achieve a higher accuracy. A latency of seconds is probably ok because the heliostats commonly move in minutes to cope with the apparent Sun trajectory in the sky.\r\n\r\nDo you think it is a good idea to test Mask R-CNN? Are there other models that would be better for this use case? Thanks!\r\n", "@JaviBonilla If better bounding-box based tracking is your goal, you could try `ssd_resnet_50_fpn_coco` from our [detection page](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md). It gives a COCO mAP value of 35 at a relatively lower latency (while still being pretty good with the TFLite infra) - the latency is ~14s on a Pixel 2, to give some context.\r\n\r\nTo squeeze the maximum accuracy from the model, use 'regular' NMS (which does one round of NMS per class instead of combining all classes into one NMS round) by using `--use_regular_nms=true` with the [`export_tflite_ssd_graph`](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md) script. This should yield a good TF graphdef suitable for TFLite.\r\n\r\nThen proceed to convert as per the instructions for floating-point models [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md).\r\n*If* latency is too high, you could go without regular NMS :-).\r\n\r\nLet me know how it goes!", "@srjoglekar246 thanks a lot for the info! We will do some experiments in our solar field with the model you mentioned and Mask RCNN to compare and evaluate accuracy and latency.\r\n\r\nI keep you inform about the results :-).", "@srjoglekar246 another thanks for the info! I'm also trying to squeeze as much performance out of a model that can be compiled to tflite, which FasterRCNN and the like are not supported for. Your tips should be pretty useful for us so cheers!\r\n", "@srjoglekar246 @jdduke Is there any updates on this issue? I'm also exploring the possibility of using MaskRCNN on Android with TFLite. I checked [Running TF2 Detection API Models on mobile](models/research/object_detection/g3doc/running_on_mobile_tf2.md) and [Running on mobile with TensorFlow Lite](models/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md). Both docs still says TF Lite only support SSD models.", "@Yu-Hang Are you interested in the masks output from MaskRCNN? If its bounding boxes, is it on your own dataset?", "@srjoglekar246 Yes, I'm interested in the object masks. Right now I just want to convert a pre-trained model, and run inference on Android and see the performance. We have a trained MaskRCNN model with our own dataset. The goal is to run inference with this model on Android.\r\n\r\n", "@srjoglekar246 Our app does not process images in real-time. The accuracy of MaskRCNN is appealing to us even though it is computationally expensive.", "Did you train the MaskRCNN from the [detection zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)? Or did you use some other code?\r\nIn any case, you can probably try converting the model with [Select TF ops](https://www.tensorflow.org/lite/guide/ops_select) to check whether it converts, and report back errors?", "@srjoglekar246 No, we trained using the [matterport](https://github.com/matterport/Mask_RCNN) code base. Does this make a difference since that codebase is using TF1? Do you suggest we retrain using the one from the TF2 model zoo?\r\n\r\nOkay, I will try converting the MaskRCNN model from TF2 model zoo using Select TF ops.", "TF1 support can be messy, if it contains control flow etc (MaskRCNN most probably does, but you can check)\r\n\r\nTrying the one from the detection zoo makes more sense, we can debug the errors if they arise :-)", "@srjoglekar246 I tried to convert the model, but failed at generating the intermediate SavedModel.\r\n\r\ncode:\r\n`tensorflow@3e9a683ad236:~/models/research$ python object_detection/export_tflite_graph_tf2.py --pipeline_config_path ~/data/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/pipeline.config --trained_checkpoint_dir ~/data/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/checkpoint --output_directory ~/data/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/output`\r\n\r\nError message:\r\n`Traceback (most recent call last):\r\n  File \"object_detection/export_tflite_graph_tf2.py\", line 161, in <module>\r\n    app.run(main)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"object_detection/export_tflite_graph_tf2.py\", line 149, in main\r\n    text_format.Parse(f.read(), pipeline_config)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 699, in Parse\r\n    allow_unknown_field=allow_unknown_field)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 771, in ParseLines\r\n    return parser.ParseLines(lines, message)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 824, in ParseLines\r\n    self._ParseOrMerge(lines, message)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 852, in _ParseOrMerge\r\n    self._MergeField(tokenizer, message)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 982, in _MergeField\r\n    merger(tokenizer, message, field)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 1057, in _MergeMessageField\r\n    self._MergeField(tokenizer, sub_message)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 982, in _MergeField\r\n    merger(tokenizer, message, field)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 1057, in _MergeMessageField\r\n    self._MergeField(tokenizer, sub_message)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 949, in _MergeField\r\n    (message_descriptor.full_name, name))\r\ngoogle.protobuf.text_format.ParseError: 153:40 : Message type \"object_detection.protos.TFRecordInputReader\" has no field named \"s\".`\r\n\r\n", "Oh yeah.. the TFLite tools don't work on MaskRCNN, you need to use [export_inference_graph](https://github.com/tensorflow/models/blob/master/research/object_detection/export_inference_graph.py) instead.", "@srjoglekar246 I changed command to:\r\n`python object_detection/export_inference_graph.py --input_type image_tensor --pipeline_config_path ~/data/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/pipeline.config --trained_checkpoint_prefix ~/data/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/checkpoint --output_directory ~/data/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/output/`\r\n\r\nSeems like the same error:\r\n`Traceback (most recent call last):\r\n  File \"object_detection/export_inference_graph.py\", line 206, in <module>\r\n    tf.app.run()\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"object_detection/export_inference_graph.py\", line 170, in main\r\n    text_format.Merge(f.read(), pipeline_config)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 737, in Merge\r\n    allow_unknown_field=allow_unknown_field)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 805, in MergeLines\r\n    return parser.MergeLines(lines, message)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 830, in MergeLines\r\n    self._ParseOrMerge(lines, message)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 852, in _ParseOrMerge\r\n    self._MergeField(tokenizer, message)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 982, in _MergeField\r\n    merger(tokenizer, message, field)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 1057, in _MergeMessageField\r\n    self._MergeField(tokenizer, sub_message)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 982, in _MergeField\r\n    merger(tokenizer, message, field)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 1057, in _MergeMessageField\r\n    self._MergeField(tokenizer, sub_message)\r\n  File \"/home/tensorflow/.local/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 949, in _MergeField\r\n    (message_descriptor.full_name, name))\r\ngoogle.protobuf.text_format.ParseError: 153:40 : Message type \"object_detection.protos.TFRecordInputReader\" has no field named \"s\".`\r\n\r\nNot sure if I have setup checkpoint path correctly. The example for export_inference_graph.py has the following for checkpoint path:\r\n\"--trained_checkpoint_prefix path/to/model.ckpt \". However, the checkpoint files for TF 2 models look a bit different from TF 1 models as shown in the image below. Please explain on how to set checkpoint path here.\r\n\r\n<img width=\"780\" alt=\"Screen Shot 2021-03-05 at 6 45 45 PM\" src=\"https://user-images.githubusercontent.com/5137261/110186451-4db9c300-7de3-11eb-8416-4e46984b8bd7.png\">\r\n\r\n\r\n", "It should be `~/data/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/checkpoint/ckpt`, I believe.", "@srjoglekar246 hmmm...looks like checkpoint path is not the cause of the error. I still have the same error: \r\n\r\n`google.protobuf.text_format.ParseError: 153:40 : Message type \"object_detection.protos.TFRecordInputReader\" has no field named \"s\".`", "@JaviBonilla @noah003 @mali30 Hi fellas, did any of you had any luck getting this to work?", "@srjoglekar246 @tombstone I found out from this [issue](https://github.com/tensorflow/models/issues/9546) that the above error is because \r\n\r\n> In the pipeline.config file of the Mask R-CNN Inception ResNet V2 1024x1024 there is an extra 's' placed", "@srjoglekar246 Can you convert models that are not from the the model hub? I tried convert my own MaskRCNN model, but the output tensors of the model seems to be wrong. It would be nice if you can take a look my [issue](https://github.com/tensorflow/tensorflow/issues/48025#issue-839067869)"]}, {"number": 19262, "title": "PosixFileSystem::CreateDir should create directory respecting umask", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below\r\n\r\n### The Problem\r\nWhen using the EventFileWriter to write event files to a non-existent directory, the directory will be created but will not have group or other permissions, even if the parent directory has the sticky bit set and umask is set to 0777.\r\n\r\nThe problem originates from [posix_file_system.cc:244](https://github.com/tensorflow/tensorflow/blob/e7f158858479400f17a1b6351e9827e3aa83e7ff/tensorflow/core/platform/posix/posix_file_system.cc#L244). Here the mask is set to 0755, which may not be desired. Instead setting this to 0777 will respect the umask, thus work as expected.\r\n\r\n### Source code / logs\r\nThe following code will create the directory \"nonexistent-directory\" with permissions 0755 even when umask is set to 0775 or 0755:\r\n```\r\nfrom tensorflow.python.training.summary_io import SummaryWriterCache\r\nSummaryWriterCache.get(\"nonexistent-directory\")\r\n```\r\n\r\n### Workaround\r\nAs a workaround, the directory can be created before the `SummaryWriterCache` is used:\r\n```\r\nfrom tensorflow.python.training.summary_io import SummaryWriterCache\r\nimport os\r\nos.makedirs(\"nonexistent-directory\", exist_ok=True)\r\nSummaryWriterCache.get(\"nonexistent-directory\")\r\n```\r\nIn this case the umask is respected correctly.\r\n\r\n### Fix\r\nReplacing [posix_file_system.cc:244](https://github.com/tensorflow/tensorflow/blob/e7f158858479400f17a1b6351e9827e3aa83e7ff/tensorflow/core/platform/posix/posix_file_system.cc#L244) by `if (mkdir(TranslateName(name).c_str(), 0777) != 0) {` should fix the problem.", "comments": []}, {"number": 19233, "title": "`foldl` disallows mixing different types of `elems`", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Redhat\r\n- **TensorFlow installed from (source or binary)**:\r\nconda-forge\r\n- **TensorFlow version (use command below)**:\r\n1.6\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A (cpu only)\r\n- **GPU model and memory**:\r\nN/A (cpu only)\r\n- **Exact command to reproduce**:\r\n### Describe the problem\r\n### Source code / logs\r\nLook at the following two code snippets. \r\n\r\n```python\r\nscore = tf.foldl(lambda s, x: transitions[x[0], x[1]] + x[2][x[0]] + s, (label[1:], label[:-1], logits), initializer=np.zeros((), dtype=np.float64))\r\n```\r\n\r\n```python\r\nscore = tf.reduce_sum(tf.map_fn(lambda x: transitions[x[0], x[1]] + x[2][x[0]], (label[1:], label[:-1], logits), dtype=tf.float64))\r\n```\r\n\r\nLong story short, the 1st one does not compile, while the second one does. From my view, these 2 operations essentially do the same thing. That said, from the error trace, it seems to me `foldl` disallows mixing different types for the argument of `elems=`, since indeed `label` is of type `tf.int32`, while `logits` is of type `tf.float64`. However the documentation does not indicate that is the case. Could you confirm if that is true ? If yes, can we update the documentation to make it explicit ?\r\n\r\n```python\r\nXXX/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py in foldl(fn, elems, initializer, parallel_iterations, back_prop, swap_memory, name)\r\n    107 \r\n    108     # Convert elems to tensor array.\r\n--> 109     elems = ops.convert_to_tensor(elems, name=\"elems\")\r\n    110     n = array_ops.shape(elems)[0]\r\n    111     elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,\r\n\r\nXXX/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n    944       name=name,\r\n    945       preferred_dtype=preferred_dtype,\r\n--> 946       as_ref=False)\r\n    947 \r\n    948 \r\n\r\nXXX/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1034 \r\n   1035     if ret is None:\r\n-> 1036       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1037 \r\n   1038     if ret is NotImplemented:\r\n\r\nXXX/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)\r\n   1018   if dtype is not None and dtype != inferred_dtype:\r\n   1019     return NotImplemented\r\n-> 1020   return _autopacking_helper(v, inferred_dtype, name or \"packed\")\r\n   1021 \r\n   1022 \r\n\r\nXXX/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)\r\n    960           raise TypeError(\"Cannot convert a list containing a tensor of dtype \"\r\n    961                           \"%s to %s (Tensor is: %r)\" % (elem.dtype, dtype,\r\n--> 962                                                         elem))\r\n    963         converted_elems.append(elem)\r\n    964         must_pack = True\r\n\r\nTypeError: Cannot convert a list containing a tensor of dtype <dtype: 'float64'> to <dtype: 'int32'> (Tensor is: <tf.Tensor 'loss/BiasAdd:0' shape=(?, 5) dtype=float64>)\r\n```", "comments": ["@skye, are you familiar with functional ops?"]}]