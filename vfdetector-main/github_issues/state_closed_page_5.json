[{"number": 55398, "title": "Calling `model.compile()` multiple times leads to memory leak", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04, macOS 12.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): `pip install tensorflow==2.8.0`\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.9.7\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: - \r\n- GPU model and memory:- \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n[Stackoverflow](https://stackoverflow.com/posts/71633201/timeline)\r\n\r\nI have tried resetting the state of the optimizer by re-compiling the model. Then I found that it could lead to a memory leak.\r\n\r\nreproducible code attached.\r\n\r\n**Describe the expected behavior**\r\n\r\nSince there are no active variables that could reach the old generated model graph. The old graph in RAM should be freed.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): -\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport objgraph\r\n\r\nm = tf.keras.models.Sequential([tf.keras.layers.InputLayer(input_shape=(20,)), tf.keras.layers.Dense(10),tf.keras.layers.Dense(10),tf.keras.layers.Dense(10),tf.keras.layers.Dense(10)])\r\nobjgraph.show_growth()\r\n\r\nfor i in range(100):\r\n    tf.keras.backend.clear_session()\r\n    m.compile('adam', loss='mse')\r\n    data = np.arange(32*20).reshape(32, 20)\r\n    labels = np.zeros(32)\r\n    results = m.fit(data, labels, epochs=10)\r\n    objgraph.show_growth()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@maghsk .\r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues).\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "@maghsk ,\r\nplease feel free to move this issue to closed status, so that we can track the issue there. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55398\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55398\">No</a>\n"]}, {"number": 55397, "title": "Grammar Correction", "body": "Corrected \"The width the output...\" to \"The width of the output...\"", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCc @mihaimaruseac "]}, {"number": 55396, "title": "Enable pylint warnings", "body": "It is quite annoying to check some warning only with the internal CI (Kokoro).\r\n\r\nThis is going to slowdown the review rate, increase reviewers noise/rounds and wasting CI cycles (as currently we cannot invoke isolated CI Linting jobs).\r\n\r\nE.g. see https://github.com/tensorflow/tensorflow/pull/55363#pullrequestreview-922485535\r\n\r\n/cc @mdanatg", "comments": ["/cc @mihaimaruseac @angerson ", "Copybara again..", "@mdanatg Can we rollback this? The problem was that we cannot check space errors anymore  [with pylint only since 2.6](https://github.com/PyCQA/pylint/commit/28a5c2e417ebdf239712859c9f699d602411233b). \r\nWe need to intorduce  Black or something else.\r\n\r\nhttps://github.com/PyCQA/pylint/commit/28a5c2e417ebdf239712859c9f699d602411233b\r\n\r\nMore in general here the full list of deprecated checks https://github.com/PyCQA/pylint/blob/main/pylint/constants.py#L90-L180\r\n\r\n/cc @angerson @mihaimaruseac \r\n"]}, {"number": 55395, "title": "[PluggableDevice] Add TF_IsRefInput", "body": "This change attemps to fill some gaps in the [Kernel Extension for Variable Operations API](https://github.com/tensorflow/community/blob/master/rfcs/20210504-kernel-extension-variable-ops.md) by adding a `TF_IsRefInput` function to the pluggable device API in order to make it easier for plugins to implement operators that use ref variables. Since ref variables must be retrieved by the `TF_GetInputTensorFromVariable` function instead of `TF_GetInput`, being able to know whether an input is a normal tensor, a resource tensor or a ref tensor makes it easier for plugins to implement operators in a generic way and while reducing code duplication.", "comments": ["Similar to #55379, please add a PR description and (in it) a link to the original RFC(s) that introduced APIs such as `TF_AssignVariable`.", "> Similar to #55379, please add a PR description and (in it) a link to the original RFC(s) that introduced APIs such as `TF_AssignVariable`.\r\n\r\n@wangpengmit Thanks! I updated the description."]}, {"number": 55393, "title": "[PluggableDevice] Move Max DEVICE_DEFAULT registration outside of CUDA ifdef", "body": null, "comments": []}, {"number": 55392, "title": "[PluggableDevice] Move Min DEVICE_DEFAULT registration outside of CUDA ifdef", "body": null, "comments": []}, {"number": 55391, "title": "Fixed bug that the return value of tanh over 1 in xla .", "body": "https://github.com/tensorflow/tensorflow/issues/55390", "comments": []}, {"number": 55388, "title": "module 'tensorflow_federated.python.learning' has no attribute 'reconstruction'", "body": "Hello, I tried running this piece of code from the tutorial \"Federated Reconstruction with Matrix Factorization\" and an error arises. \r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-18-c29214822d98> in <module>\r\n     23 def get_matrix_factorization_model(\r\n     24     num_items: int,\r\n---> 25     num_latent_factors: int) -> tff.learning.reconstruction.Model:\r\n     26   \"\"\"Defines a Keras matrix factorization model.\"\"\"\r\n     27   # Layers with variables will be partitioned into global and local layers.\r\n\r\nAttributeError: module 'tensorflow_federated.python.learning' has no attribute 'reconstruction'\r\n\r\nWhy does this happen? And how to solve this? \r\nThank you ", "comments": ["@emila2021 ,\r\nIn order to expedite the trouble-shooting process, could you please provide a complete code and the TensorFlow version you are using.\r\n", "@emila2021 ,\r\nI have tried to execute the tutorial `\"Federated Reconstruction with Matrix Factorization\"` with stable v2.8 and noticed that code has been executed without any issues. Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/ce5a2943ecf5eb9c28ccd44231bfa7dd/federated_reconstruction_for_matrix_factorization.ipynb). Thanks!", "> @emila2021 , I have tried to execute the tutorial `\"Federated Reconstruction with Matrix Factorization\"` with stable v2.8 and noticed that code has been executed without any issues. Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/ce5a2943ecf5eb9c28ccd44231bfa7dd/federated_reconstruction_for_matrix_factorization.ipynb). Thanks!\r\n\r\n@tilakrayal I have tried to update my TensorFlow version to 2.8.0 but I continue to have problems with TensorFlow Federated. I tried running the code on my mac but I seem to have problems with dependencies and now it is appearing this: \r\n\r\n[Untitled.pdf](https://github.com/tensorflow/tensorflow/files/8404400/Untitled.pdf)\r\n\r\nI tried creating a virtual environment with the steps provided in the https://github.com/tensorflow/federated/tree/main/docs/tutorials\r\n\r\n# Create a virtual environment\r\npython3 -m venv \"venv\"\r\nsource \"venv/bin/activate\"\r\npip install --upgrade pip\r\n\r\n# Install the required Python package\r\npip install --upgrade \\\r\n    tensorflow-federated \\\r\n    nest-asyncio\r\n\r\n# Install Jupyter\r\npip install --upgrade jupyter\r\n\r\n# Build IPython kernel\r\npython -m ipykernel install \\\r\n    --user \\\r\n    --name tff_kernel \\\r\n    --display-name \"TFF Kernel\"\r\n\r\nBut had no success \r\n", "@emila2021 ,\r\nThis issue is more related to tensorflow/federated.I request you to please post in federated repo from [here](https://github.com/tensorflow/federated/issues).\r\nAlso as mentioned above, I was able to execute the tutorial [code](https://colab.research.google.com/gist/tilakrayal/ce5a2943ecf5eb9c28ccd44231bfa7dd/federated_reconstruction_for_matrix_factorization.ipynb) without any issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55388\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55388\">No</a>\n"]}, {"number": 55387, "title": "[PluggableDevice] Move Unpack DEVICE_DEFAULT registration outside of CUDA ifdef", "body": null, "comments": []}, {"number": 55386, "title": "[PluggableDevice] Move ZerosLike DEVICE_DEFAULT registration outside of CUDA ifdef", "body": null, "comments": []}, {"number": 55385, "title": "[PluggableDevice] Add DEVICE_DEFAULT int32 registration for Concat and ConcatV2", "body": null, "comments": []}, {"number": 55384, "title": "[PluggableDevice] Add DEVICE_DEFAULT int32 registration for Slice", "body": null, "comments": []}, {"number": 55382, "title": "[PluggableDevice] Add DEVICE_DEFAULT int32 registration for strided slice ops", "body": null, "comments": ["@penpornk That makes sense. That change is not critical so I'm not too worried about it. On the other hand, does that mean that there's absolutely no chances that the other PR for `TF_AssignVariable` (https://github.com/tensorflow/tensorflow/pull/55379) can make it through for 2.9? This other PR is much more important for us because one of the main benchmarks that people use to evaluate GPU providers still uses ref variables. Unfortunately we didn't find that it was an issue until very recently, but since it is in the experimental header, is there a chance that it could make it through?", "@PatriceVignola I don't think `TF_AssignRefVariable` will make it. It's too short notice and it touches C API (but at least it's in experimental). Branch cut is this coming Monday (3/28) or Tuesday (3/29) if Monday's nightly isn't clean. I can check with @wangpengmit though.", "@penpornk Since it doesn't look like the `TF_AssignRefVariable` PR will be able to make it in TF 2.9, is there a possibility to prioritize the review of this PR for early 2.10 nightly builds, maybe by the end of the week? We want to at least be able to point users to nightly builds if they want to run the benchmarks that still use those ops.", "@penpornk Can you please assist on above comments from @PatriceVignola. Thank you!", "@gbaned The TF_AssignRefVariable PR just went in (https://github.com/tensorflow/tensorflow/pull/55640). As for this PR, it can't be merged until we investigate the failure of https://github.com/tensorflow/tensorflow/pull/53561. So I'm temporarily closing it until we have time to revisit."]}, {"number": 55381, "title": "[PluggableDevice] Add DEVICE_DEFAULT int32 registration for Pack", "body": null, "comments": ["This PR got reverted because it broke internal tests. Will add this to the list of `DEVICE_DEFAULT` ops to revisit / investigate later.", "@penpornk Where can we track that list?"]}, {"number": 55380, "title": "Add error handling to TF_GetInputTensorFromVariable", "body": null, "comments": []}, {"number": 55379, "title": "Add TF_AssignRefVariable", "body": "The [Kernel Extension for Variable Operations API](https://github.com/tensorflow/community/blob/master/rfcs/20210504-kernel-extension-variable-ops.md) added pluggable device support for resource variables and incomplete support for ref variables (`TF_OpKernelContext_ForwardRefInputToRefOutput`), but it misses one endpoint to make ref variables usable: `TF_AssignRefVariable`.\r\n\r\nThis change attempts to fill the gaps in the RFC by adding `TF_AssignRefVariable`, which is analogous to `TF_AssignVariable` but for ref variables instead of resource variables. It uses the same semantics where the user has to pass a copy function to be called when copying the value tensor to the ref tensor.", "comments": ["I need more context. Please add a PR description and (in it) a link to the original RFC(s) that introduced APIs such as `TF_AssignVariable`.", "> I need more context. Please add a PR description and (in it) a link to the original RFC(s) that introduced APIs such as `TF_AssignVariable`.\r\n\r\n@wangpengmit Got it, thanks! I updated the description.", "I'm not sure why the Windows Bazel GPU build is failing. I built it locally yesterday and it completed just fine, and the Linux GPU build completed successfully. Is it possible to look at the failures or is it unrelated to this PR?", "Hi @mihaimaruseac , is there a way to see the Windows Bazel GPU error messages?", "@wangpengmit Usually I just rerun the tests and hope the log appears. Will rerun them now since it's evening anyway.", "For Googlers, if you retrigger the job and click on the details link before the job finishes you are taken to the internal log page (external people will get an invalid link).\r\n\r\nOnce the job finishes, the link switched to the public version, but if the job fails in certain ways there is no public log. This is usually infra failure and could be ignored.", "The build failed again without a \"Details\" link. Is there anything I can do to expedite this PR? I successfully built it and ran the tests locally but maybe I'm missing something. Is there a way to confirm whether it is an infra issue or not?", "@PatriceVignola I will manually import the PR and run the tests internally. I'll let you know if anything fails there.", "This PR was reverted because it broke internal tests. We will try to follow up with failure details soon. ", "Why was the PR merged in the first place? I thought you would only merge it if the internal tests passed. Please let us know what tests where failing when you have the info, I can debug it locally if I know what they are.", "@PatriceVignola We only run TensorFlow presubmit tests before merging. After merging, PRs could still be reverted if they broke TensorFlow nightly tests or other internal tests (e.g., unit / integration tests of internal applications that use TensorFlow). In this case, it broke an internal test.", "Here is the complete workflow for a PR:\r\n\r\n![7XNWZfbTauSmBXn](https://user-images.githubusercontent.com/323199/161792027-84b5c488-5747-4cff-931b-01047d3ec90c.png)\r\n\r\nWe have this many steps due to CI complexity and needing to support both multiple platforms x python versions x {GPU,CPU} and a requirement to have short presubmits to not negatively impact developer velocity.", "Hi, I'm bumping this to know if it's possible to get the failure details, or the command line that was used to run the internal tests that failed. We are committed to investigate the issue and resolve it.", "Sorry for the delay. It took a while to extract a proxy test. Please add the following test case to [variable_ops_test.py](https://github.com/tensorflow/tensorflow/blob/d7307015d3ab7ad82305c46c259fc711c3948ce8/tensorflow/python/kernel_tests/variables/variable_ops_test.py#L65)\r\n\r\n```python\r\n  @test_util.run_deprecated_v1\r\n  def testString(self):\r\n    data = array_ops.stack([b\"data\"])\r\n    buffer_var = variables.VariableV1(\r\n        initial_value=array_ops.zeros(shape=(), dtype=dtypes.string),\r\n        trainable=False,\r\n        collections=[ops.GraphKeys.LOCAL_VARIABLES],\r\n        name=\"buffer\",\r\n        dtype=dtypes.string,\r\n        validate_shape=False,\r\n        use_resource=False)\r\n    result = state_ops.assign(buffer_var, data, validate_shape=False)\r\n    with self.cached_session() as sess:\r\n      sess.run(variables.local_variables_initializer())\r\n      self.assertEqual(result.eval(), b\"data\")\r\n```\r\nTo run:\r\n```\r\nbazel test --config=opt //tensorflow/python/kernel_tests/variables:variable_ops_test --test_filter=testString\r\n```\r\n", "Thank you for the snippet. What kind of error am I supposed to see? When I run the test command from the master branch, I get the following error so I'm not able to compare with my branch:\r\n\r\n```\r\nthis rule is missing dependency declarations for the following files included by 'mlir/lib/Dialect/GPU/IR/GPUDialect.cpp':\r\n  'bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUBaseIncGen/mlir/Dialect/GPU/GPUOpsDialect.h.inc'\r\n  'bazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUBaseIncGen/mlir/Dialect/GPU/GPUOpsDialect.cpp.inc'\r\n```\r\n\r\nBut otherwise, when I manually install the python package built from my branch and run the test manually, it seems to pass without issues. I'm just not sure what error I'm supposed to see here since my AssignRefVariable PR merely refactored the existing code around and didn't handle strings differently or change the registration.", "Oops sorry. I forgot to post the error message. I'm not sure why it's happening either. I don't see anything apparently wrong in your refactor code.\r\n```\r\ntensorflow/core/framework/tensor.cc:725] Check failed: dtype() == expected_dtype (1 vs. 7) string expected, got float\r\n```\r\n\r\nI could reproduce the error with the TensorFlow docker image [here](https://www.tensorflow.org/install/source#cpu-only).\r\nThe error happened with commit da9a9d69bb1c98f0e88883b3d0274b139d455e49 (merged pull request) but not the commit before (2d040ce3dde8553125eaaa49a97355cc2e05a445).\r\n\r\nTo summarize, these series of commands should be able to reproduce the error:\r\n```\r\n$ docker pull tensorflow/tensorflow:devel\r\n$ docker run -it -w /tensorflow_src -v $PWD:/mnt -e HOST_PERMS=\"$(id -u):$(id -g)\" \\\r\n    tensorflow/tensorflow:devel bash\r\n$ git pull\r\n$ git checkout da9a9d69bb1c98f0e88883b3d0274b139d455e49\r\n$ yes \"\" | ./configure\r\n$ bazel test --config=opt //tensorflow/python/kernel_tests/variables:variable_ops_test --test_filter=testString\r\n```\r\n", "I found the problem, but it wasn't caused by me. In my PR, [line 76](https://github.com/PatriceVignola/tensorflow/blob/2d2dd95d8443154d5a4f1fa3def01246ecf5b2ce/tensorflow/core/framework/ref_var.cc#L76) correctly uses the `CHECK` macro. But somehow, when committing, the TensorFlow gardener decided to change the `CHECK` for a `DCHECK` on [line 76](https://github.com/PatriceVignola/tensorflow/blob/da9a9d69bb1c98f0e88883b3d0274b139d455e49/tensorflow/core/framework/ref_var.cc#L76), which obviously gets removed in release builds. And when we look at the [git blame](https://github.com/PatriceVignola/tensorflow/blame/da9a9d69bb1c98f0e88883b3d0274b139d455e49/tensorflow/core/framework/ref_var.cc#L76), we see that all lines are attributed to me, except for that infamous `CHECK` line.\r\n\r\nWhat could cause this behavior? Is there a script that converts `CHECK` statements to `DCHECK` for performance purposes and it erroneously changed this line, or is there something I have to do to force it to not be removed? I created a [brand new PR](https://github.com/tensorflow/tensorflow/pull/55640) where I replaced the `CHECK` statement with an `OP_REQUIRES` statement, but I'd still like to understand why the `CHECK` seems to have magically been replaced with a `DCHECK`.", "`CHECK`s have been causing issues in the past and are discouraged (there is automation that blocks submit with them, unless there are additional steps taken). `DCHECK` is a workaround with some differences\r\n\r\n`OP_REQUIRES` is the best approach to use.", "It would have been fine if the submission had been blocked, but the automation silently changing `CHECK` to `DCHECK` can be hard to debug when the change gets reverted, especially when refactoring old code that already contained them.", "It wasn't automation, it was a deliberate change during PR import.", "Sorry for the late reply and thank you for looking into it! \r\nYes, it was me changing `CHECK` to `DCHECK` during merge. Sorry I forgot to mention that. :("]}, {"number": 55378, "title": "Update the RBE images to the latest container versions", "body": "This PR was created by a GitHub Actions workflow to update all the SIG Build-based RBE containers to the most recent containers. See:\n\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/toolchains/remote_config/configs.bzl\n- https://github.com/tensorflow/tensorflow/blob/master/.github/workflows/update-rbe.yml", "comments": []}, {"number": 55377, "title": "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: v2.4.4\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: pip3\r\n- Bazel version (if compiling from source): 3.1.0 (bazel-3.1.0-linux-x86_64)\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the problem**\r\n\r\nI installed moodlemlbackend v3.0.2 (I use Moodle v3.9).\r\n`pip3 install moodlemlbackend==3.0.2`\r\n\r\nNow I try to meet the requirements:\r\nhttps://github.com/moodlehq/moodle-mlbackend-python/blob/3.0.2/requirements.txt\r\nRequires:\r\ntensorflow>=2.4.2,<2.5\r\nnumpy>=1.19.2,<1.20\r\n\r\nI had to recompile tensorflow v2.4.4 with noavx option.\r\nUsed options: -march=nehalem -msse4.1 -msse4.2 -mpclmul -mpopcnt -maes -mno-avx -mno-avx2\r\n\r\n`pip3 install /git/tensorflow/tensorflow_output/tensorflow-2.4.4-cp37-cp37m-linux_x86_64.whl`\r\n\r\nAlso installed the required numpy v1.19.5:\r\n`pip3 install numpy==1.19.5`\r\n\r\nNow I get the following error:\r\n\r\n```\r\npython3 -c 'import tensorflow'\r\n# RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\r\n# RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\r\n# ImportError: numpy.core._multiarray_umath failed to import\r\n# ImportError: numpy.core.umath failed to import\r\n# 2022-03-25 17:43:52.642579: F tensorflow/python/lib/core/bfloat16.cc:714] Check failed: PyBfloat16_Type.tp_base != nullptr\r\n# Aborted\r\n```\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@klorinczi,\r\n\r\n> RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\r\n\r\nThis looks like numpy version 1.19.5 and TensorFlow 2.4 not playing well together. \r\n\r\nUse `numpy ~= 1.19.2`", "It seems, the problem was with sudo.\r\nI have different module versions installed with sudo and with user install.\r\n\r\nWith sudo:\r\n```\r\nsudo pip3 list | grep -e tensorflow -e numpy\r\nnumpy                        1.21.5\r\ntensorflow-estimator         2.4.0\r\ntensorflow-io-gcs-filesystem 0.24.0\r\n```\r\n\r\nWith user installed (without sudo):\r\n```\r\npip3 list | grep -e tensorflow -e numpy\r\nnumpy                        1.19.5\r\ntensorflow                   2.4.4\r\ntensorflow-estimator         2.4.0\r\ntensorflow-io-gcs-filesystem 0.24.0\r\n```\r\n\r\nBecause I compiled with sudo:\r\n`date && sudo bazel build --disk_cache ./cache //tensorflow/tools/pip_package:build_pip_package && date`\r\n\r\nI compile again without sudo and works fine:\r\n`date && bazel build --disk_cache ./cache //tensorflow/tools/pip_package:build_pip_package && date`\r\n\r\nNo error executing:\r\n`python3 -c 'import tensorflow'`\r\n\r\n```\r\npython3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n2022-03-28 11:32:37.148566: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\ntf.Tensor(-1334.6855, shape=(), dtype=float32)\r\n\r\n```\r\n", "@klorinczi,\r\nGlad that its resolved. \r\nSeems like this is not related to sudo. Its regarding the numpy version configured with sudo and pip3 binary package. \r\nClosing as issue is resolved. Please feel free to reopen if still faces an error. Thanks!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55377\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55377\">No</a>\n", "Solution to error: \"RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\"\r\n\r\nmoodlemlbackend v3.0.2 is not compatible with Moodle v3.9.x.\r\nMoodle v3.9.x requires moodlemlbackend v2.6.5\r\nAlso as described on:\r\nhttps://docs.moodle.org/39/en/Analytics_settings#Versions\r\n`sudo -H python3 -m pip3 install \"moodlemlbackend==2.6.5\"`\r\n\r\nSolution steps:\r\n```\r\n# uninstall in case we have had version installed with sudo\r\nsudo pip3 uninstall moodlemlbackend\r\nsudo pip3 uninstall tensorflow\r\nsudo pip3 uninstall numpy\r\n\r\n# install for current user\r\npip3 install moodlemlbackend==2.6.5\r\n\r\n# uninstall tensorflow with avx requirement\r\npip3 uninstall tensorflow\r\n\r\n# install my compiled tensorflow v2.4.4 without avx support \r\n# used options: -march=nehalem -msse4.1 -msse4.2 -mpclmul -mpopcnt -maes -mno-avx -mno-avx2)\r\npip3 install /git/tensorflow/tensorflow_output/tensorflow-2.4.4-cp37-cp37m-linux_x86_64.whl\r\n\r\npython3 -c 'import tensorflow'\r\n# Empty, so import was successful\r\n\r\n# install with sudo -H \r\nsudo -H pip3 install moodlemlbackend==2.6.5\r\n\r\n# uninstall original tensorflow \r\nsudo -H pip3 uninstall tensorflow\r\n\r\n# install my compiled tensorflow v2.4.4 without avx support \r\n# used options: -march=nehalem -msse4.1 -msse4.2 -mpclmul -mpopcnt -maes -mno-avx -mno-avx2)\r\nsudo -H pip3 install /git/tensorflow/tensorflow_output/tensorflow-2.4.4-cp37-cp37m-linux_x86_64.whl\r\n\r\nsudo -H python3 -c 'import tensorflow'\r\n# SUCCESS\r\n```\r\n\r\n"]}, {"number": 55376, "title": "'Activation' object has no attribute 'kernel' applying regularization to MobileNetV3", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2, tf-nightly-2.9.0.dev20220325\r\n- Python version: 3.9.7\r\n- CUDA/cuDNN version: N/A \r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nAttempting to add regularization to MobileNetV3. The regularization appears to be applied correctly (applied to 43 Conv2D and Dense layers), but calling `model.fit` fails with the error `AttributeError: 'Activation' object has no attribute 'kernel'`.\r\n\r\nIt appears that calling `model.fit` is reapplying the regularization, but on the last `layer`, which is the output Activation. Adding regularization this way works on MobileNetV2 and custom models I've built, but only fails on MobileNetV3 (small or large).\r\n\r\nIf a default argument is used in the lambda (`layer.add_loss(lambda l=layer: regularizer(l.kernel))`), I can build and train the model, but saving the model with the Saved Model API fails with `tensorflow.python.saved_model.nested_structure_coder.NotEncodableError: No encoder for object <keras.layers.convolutional.Conv2D object at 0x7f0c4083be80> of type <class 'keras.layers.convolutional.Conv2D'>.`\r\n\r\n**Describe the expected behavior**\r\n\r\nThe sample script should run without fail, or at least fail similarly between MobieNetV2 and MobileNetV3.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nColab notebook (2.9.0 nightly) https://colab.research.google.com/drive/1JQxl6KyCf5vWUnFjo0qB7v_I5maMMspb#scrollTo=HAlKVbk08W3X\r\n\r\nStandalone code:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nmodel = tf.keras.applications.MobileNetV3Small(input_shape=(96, 96, 3))\r\n# model = tf.keras.applications.MobileNetV2(input_shape=(96, 96, 3))\r\nregularizer = tf.keras.regularizers.L1(1e-3)\r\n\r\nfor layer in model.layers:\r\n    if hasattr(layer, \"kernel\"):\r\n        layer.add_loss(lambda: regularizer(layer.kernel))  # Fails building V3\r\n        # layer.add_loss(lambda l=layer: regularizer(l.kernel))  # Fails saving SavedModel all versions\r\n\r\n# Ensure regularization only applied to Conv2D or Dense layers\r\nregularization_count = 0\r\nfor layer in model.layers:\r\n    if layer.losses:\r\n        regularization_count += 1\r\n        assert isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.Dense)\r\n        assert not isinstance(layer, tf.keras.layers.Activation)\r\n\r\nprint(f\"Regularization added to {regularization_count} layers\")\r\n\r\nmodel.compile(loss=\"mse\")\r\nmodel.fit(\r\n    np.random.randn(10, 96, 96, 3),\r\n    np.random.randn(10, 1000),\r\n)\r\nmodel.save(\"test\")\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nTraceback from Colab notebook:\r\n\r\n```\r\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\r\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_small_224_1.0_float.h5\r\n10734624/10734624 [==============================] - 0s 0us/step\r\nRegularization added to 43 layers\r\n\r\n---------------------------------------------------------------------------\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n\r\n[<ipython-input-3-0ea365886076>](https://localhost:8080/#) in <module>()\r\n     23 model.fit(\r\n     24     np.random.randn(10, 96, 96, 3),\r\n---> 25     np.random.randn(10, 1000),\r\n     26 )\r\n\r\n2 frames\r\n\r\n[<ipython-input-3-0ea365886076>](https://localhost:8080/#) in <lambda>()\r\n      8 for layer in model.layers:\r\n      9     if hasattr(layer, \"kernel\"):\r\n---> 10         layer.add_loss(lambda: regularizer(layer.kernel))\r\n     11 \r\n     12 # Ensure regularization only applied to Conv2D or Dense layers\r\n\r\nAttributeError: in user code:\r\n\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\r\n        return step_function(self, iterator)\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\r\n        outputs = model.train_step(data)\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 890, in train_step\r\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 949, in compute_loss\r\n        y, y_pred, sample_weight, regularization_losses=self.losses)\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1268, in losses\r\n        loss_tensor = regularizer()\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1342, in _tag_callable\r\n        loss = loss()\r\n    File \"<ipython-input-3-0ea365886076>\", line 10, in <lambda>\r\n        layer.add_loss(lambda: regularizer(layer.kernel))\r\n\r\n    AttributeError: 'Activation' object has no attribute 'kernel'\r\n```\r\n", "comments": ["@mattpotma \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!", "Sorry I missed that. Cross posted to https://github.com/keras-team/keras/issues/16316. Closing this now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55376\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55376\">No</a>\n"]}, {"number": 55375, "title": "tf.load_op_library doesn't load .so file and throws file not found error although the file exists", "body": "```\r\n**System information**\r\n== check python ===================================================\r\npython version: 3.7.7\r\npython branch: \r\npython build version: ('default', 'Jan 13 2021 07:47:31')\r\npython compiler version: GCC 7.5.0\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: 69-18.04.1-Ubuntu SMP Wed Feb 9 15:36:54 UTC 2022\r\nos release version: 5.4.0-1066-aws\r\nos platform: Linux-5.4.0-1066-aws-x86_64-with-debian-buster-sid\r\nlinux distribution: ('debian', 'buster/sid', '')\r\nlinux os distribution: ('debian', 'buster/sid', '')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n== check pips ===================================================\r\nnumpy                            1.21.5\r\nprotobuf                         3.14.0\r\ntensorflow                       2.8.0\r\ntensorflow-estimator             1.14.0\r\ntensorflow-hub                   0.11.0\r\ntensorflow-io-gcs-filesystem     0.24.0\r\nwsproto                          1.0.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.8.0\r\ntf.version.GIT_VERSION = v2.8.0-rc1-32-g3f878cff5b6\r\ntf.version.COMPILER_VERSION = 7.3.1 20180303\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- TensorFlow installed from (source or binary):binary\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib:/opt/amazon/efa/lib:/usr/local/mpi/lib:/opt/amazon/openmpi/lib:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib:/opt/amazon/efa/lib:/usr/local/mpi/lib:/opt/amazon/openmpi/lib:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nNVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\r\n\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/lib64/libcudart_static.a\r\n/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2.89\r\n/usr/local/cuda-10.2/doc/man/man7/libcudart.7\r\n/usr/local/cuda-10.2/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart.so.10.1.243\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-10.1/doc/man/man7/libcudart.7\r\n/usr/local/cuda-10.1/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.2/lib64/libcudart_static.a\r\n/usr/local/cuda-9.2/lib64/libcudart.so.9.2.148\r\n/usr/local/cuda-9.2/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.2/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-10.0/lib64/libcudart.so.10.0.130\r\n/usr/local/cuda-10.0/lib64/libcudart_static.a\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.8.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/ubuntu/venvpipe1v1/lib/python3.7/site-packages\r\nRequired-by: spacy-universal-sentence-encoder\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 7, 7, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n```\r\n**Describe the current behavior**\r\n\r\nWhen the code below is executed\r\n```\r\nprint(config.COREF_KERNELS_SO_FILE_PATH.is_file())\r\ncoref_op_library = tf.load_op_library(str(config.COREF_KERNELS_SO_FILE_PATH))\r\n```\r\nThe first line prints `True` which means that the file exists. But the second line throws the following error.\r\n\r\n`\r\n File \"/home/ubuntu/venvpipe1v1/lib/python3.7/site-packages/tensorflow/python/framework/load_library.py\", line 54, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: libtensorflow_framework.so.1: cannot open shared object file: No such file or directory\r\n`\r\n\r\n**Describe the expected behavior**\r\n\r\nThe .so file should load\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\ncoref_op_library = tf.load_op_librarypath to .so file')\r\n\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n2022-03-24 17:39:30.761737: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2022-03-24 17:39:30.761779: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```", "comments": ["@nirnayr ,\r\nCan you please take a look at this [issue1](https://github.com/tensorflow/tensorflow/issues/30488), [2](https://github.com/tensorflow/tensorflow/issues/45930) and also TensorFlow release is compatible with a certain version, for more information please take a look at the tested build [configurations](https://www.tensorflow.org/install/source#gpu). ", "Tried linking libtensorflow_framework.so and libtensorflow_framework.so.2 using ln as suggested by [https://stackoverflow.com/questions/56888781/tensorflow-notfounderror-libtensorflow-framework-so-cannot-open-shared-file-or?noredirect=1&lq=1](https://www.stackoverflow.com/)\r\n\r\nI have tried appending LD_LIBRARY_PATH with the libtensorflow_framework.so file as suggested by this thread [https://github.com/tensorflow/tensorflow/issues/30488](https://www.stackoverflow.com/)\r\n\r\nI have tried downgrading to tensorflow 1.14, with Python 3.7. As that was the latest compatible according to [https://www.tensorflow.org/install/source#gpu](https://www.stackoverflow.com/)", "@nirnayr, \r\n`tf.sysconfig.get_link_flags()`. returns ```\r\n['-L/usr/local/lib/python3.7/dist-packages/tensorflow',\r\n '-l:libtensorflow_framework.so.2'] ```\r\n\r\n Here, `library directory` is  `-L` and `library` is `-l`. Now replace the `-L` and `-l`  with `library_dirs` and `libraries `arguments to your Extension object in `setup.py`.\r\n\r\nFinally, set your `LD_LIBRARY_PATH` environment variable to point to the directory with the `libtensorflow_framework.so` library is located.\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55375\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55375\">No</a>\n"]}, {"number": 55372, "title": "Wrong definition of arg fixed_length in api_docs/python/tf/raw_ops/DecodePaddedRaw", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/raw_ops/DecodePaddedRaw\r\n\r\n## Description of issue (what needs changing):\r\n\r\nWrong type decription of `fixed_length` .\r\n\r\n### Clear description\r\n\r\n`fixed_length` should be a positive int, but the doc has following description:\r\n````\r\nA Tensor of type int32. Length in bytes for each element of the decoded output. Must be a multiple of the size of the output type.\r\n````\r\n\r\n\r\n### Parameters defined\r\n\r\n`fixed_length` should be a positive int.\r\n\r\n\r\n### Submit a pull request?\r\n\r\nno\r\n", "comments": ["@shijy16 \r\nCould you please mention the exact changes that are needed for the documentation and please specify some use cases ?Thanks!", "@sushreebarsa \r\nThe  argument `fixed_length`  is defined as a Tensor in the doc:\r\n````\r\nA Tensor of type int32. Length in bytes for each element of the decoded output. Must be a multiple of the size of the output type.\r\n````\r\nAccording to the execute result  of following code, `fixed_length` should be a positive int:\r\n\r\n````python\r\n import tensorflow as tf\r\n \r\ninput_bytes = ['123', '1234', '12345']\r\nfixed_length = [3, 3, 3]\r\nout_type = tf.int8\r\nprint(tf.raw_ops.DecodePaddedRaw(input_bytes=input_bytes, fixed_length=fixed_length, out_type=out_type))\r\n````\r\nExecute Result:\r\n````\r\npython test.py\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 6, in <module>\r\n    print(tf.raw_ops.DecodePaddedRaw(input_bytes=input_bytes, fixed_length=fixed_length, out_type=out_type))\r\n  File \"/home/sjy/.conda/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/python/util/tf_export.py\", line 400, in wrapper\r\n    return f(**kwargs)\r\n  File \"/home/sjy/.conda/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/python/ops/gen_parsing_ops.py\", line 343, in decode_padded_raw\r\n    return decode_padded_raw_eager_fallback(\r\n  File \"/home/sjy/.conda/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/python/ops/gen_parsing_ops.py\", line 379, in decode_padded_raw_eager_fallback\r\n    _result = _execute.execute(b\"DecodePaddedRaw\", 1, inputs=_inputs_flat,\r\n  File \"/home/sjy/.conda/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 54, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DecodePaddedRaw_device_/job:localhost/replica:0/task:0/device:CPU:0}} k must be scalar, got shape [3] [Op:DecodePaddedRaw]\r\n````\r\nObviously, the argument `fixed_length` should be a scalar instead of a Tensor. After some research on the source code of DecodePaddedRawOp, I think it should be defined as:\r\n````\r\nA\u00a0positive int. Length in bytes for each element of the decoded output. Must be a multiple of the size of the output type.\r\n````\r\nAnd it works fine when the argument `fixed_length` is defined as a positive int:\r\n````python\r\n import tensorflow as tf\r\n \r\ninput_bytes = ['123', '1234', '12345']\r\nfixed_length = 3\r\nout_type = tf.int8\r\nprint(tf.raw_ops.DecodePaddedRaw(input_bytes=input_bytes, fixed_length=fixed_length, out_type=out_type))\r\n````\r\nResult:\r\n````\r\npython test.py\r\n\r\ntf.Tensor(\r\n[[49 50 51]\r\n [49 50 51]\r\n [49 50 51]], shape=(3, 3), dtype=int8)\r\n````", "@shijy16 in the code you sent above you tried to pass a list for fixed_length however the documentation mentions to pass a tensor of type int32. The following code works.\r\n\r\n`import tensorflow as tf\r\n \r\ninput_bytes = ['123', '1234', '12345']\r\nfixed_length = tf.constant(3)\r\nout_type = tf.int8\r\nprint(tf.raw_ops.DecodePaddedRaw(input_bytes=input_bytes, fixed_length=fixed_length, out_type=out_type))`", "@Yashashree304  You are right, thank you.\r\nI thought a tensor was meant to be a multi-dimensional arrays. After reading the definition of tensor, I just realize it can be a scalar. And in the definition of `DecaodePaddedRaw`, it is a scalar obviously.\r\nSorry for bothering. XD", "@Yashashree304 Thank you for your response!\r\n@shijy16  The description in the [documentation](https://www.tensorflow.org/api_docs/python/tf/raw_ops/DecodePaddedRaw) for `fixed_length` is correct as it says to pass a tensor of type int32 and the example you have shared is passing a list for fixed_length.\r\nCould you please move this issue to closed status if it is resolved for you?\r\nThanks!\r\n", "OK, I'll close this issue.\r\nThank you all!"]}, {"number": 55369, "title": "Failed to build libtensorflow (v2.8.0 + bazel 4.2.1) from source on Apple M1 ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n`Apple M1`\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n`Source: v2.8.0 branch`\r\n\r\n- TensorFlow version:\r\n- Python version:\r\n`3.9.9`\r\n\r\n- Installed using virtualenv? pip? conda?\r\n- Bazel version (if compiling from source):\r\n`4.2.1`\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n```\r\nApple clang version 13.1.6 (clang-1316.0.21.2)\r\nTarget: arm64-apple-darwin21.4.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n```\r\n\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n```\r\nbazel build --jobs 10 --config opt --cpu=darwin_arm64 --host_cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow\r\n\r\nERROR: /private/var/tmp/_bazel_leonard/dbb496db86443ebdf640b47e39125a9a/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64'\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\nINFO: Elapsed time: 131.509s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (126 packages loaded, 2996 targets configured)\r\n    Fetching @icu; fetching 6s\r\n    Fetching ...6db86443ebdf640b47e39125a9a/external/icu; Extracting /private/var/tmp/_bazel_leonard/dbb496db86443ebdf640b47e39125a9a/external/icu/temp15584703296969244655/release-69-1.zip\r\n    Fetching @com_google_absl; Restarting.\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n./configure\r\n\r\nbazel build --jobs 10 --config opt --cpu=darwin_arm64 --host_cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@lnshi,\r\nMake sure you have installed Xcode app correctly from app store.\r\n\r\nTry workaround mention on similar issue thread [#13514](https://github.com/bazelbuild/bazel/issues/13514#issuecomment-847917936)", "@gadagashwini thanks for the prompt reply.\r\n\r\nActually what is the difference between the below two commands:\r\n```\r\nbazel build --jobs=10 --compilation_mode=opt --copt=-march=native //tensorflow/tools/lib_package:libtensorflow\r\n```\r\n\r\nand\r\n```\r\nbazel build --jobs 10 --config opt --cpu=darwin_arm64 --host_cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow\r\n```\r\n\r\nThe first one all is ok, but the latter one i run into this reported issue.", "@lnshi,\r\n`bazel build --jobs=10 --compilation_mode=opt --copt=-march=native //tensorflow/tools/lib_package:libtensorflow`\r\n\r\n`--copt `or `--compilation_mode opt` is for passing args to to the compiler. This flag is for Bazel to build with optimization settings enabled with no debug information. \r\n\r\n`bazel build --jobs 10 --config opt --cpu=darwin_arm64 --host_cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow`\r\n\r\n`--config opt` is for selecting a configuration. You can have a .bazelrc which defines default options. Some of them are not always active, but some only if you activate them by the --config=configname command line option. There are some [preconfigured build configs](https://www.tensorflow.org/install/source#preconfigured_configurations) available that can be added to the bazel build command, for example `--config=dbg,` `--config=mkl`. \r\n\r\nHere you are configuring with `darwin_arm64`, you can configure this with bazel build as mentioned [here](https://github.com/bazelbuild/bazel/issues/13514#issuecomment-847917936)", "@gadagashwini thanks for the explanations\r\n\r\nin my `tensorflow/.bazelrc` i can see the following configs:\r\n```\r\n# Settings for MacOS on ARM CPUs.\r\nbuild:macos_arm64 --cpu=darwin_arm64\r\n```\r\n\r\nso can i assume both of the two commands are building the TF C libs for `M1 darwin_arm64`? but why the 1st run without any problem (i got all the 6 dylibs), but the latter one has such errors?", "@lnshi, Seems like issue is with Bazel.\r\nBuild Tensorflow with Bazel version 4.2.2 or 5.0.0. \r\nTensorflow 2.8.0 reuires Bazel version `_TF_MIN_BAZEL_VERSION = '4.2.2'` Take a look at [Tensorflow config](https://github.com/tensorflow/tensorflow/blob/master/configure.py#L48). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55369\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55369\">No</a>\n"]}, {"number": 55368, "title": "TensorFlow android throw java.lang.OutOfMemoryError when building project, but there's free 20GB memory.", "body": "![image](https://user-images.githubusercontent.com/12729184/160032113-66a91d3c-81cf-4ac0-a5e7-f4046f5da39c.png)\r\n\r\nMy process:\r\n- Clone project from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/android/test\r\n- Click build button (My PC memory : 11.7/31.9 GB (37%))\r\n- When building the memory only use 2GB, then it throwed error : \r\n```\r\n:packageDebug' `\r\n> A failure occurred while executing com.android.build.gradle.internal.tasks.Workers$ActionFacade\r\n> java.lang.OutOfMemoryError (no error message)\r\n```\r\n- My PC has hyper-v\r\n\r\nDid I miss some key steps?  Really appreciate Tensorflow team's efforts.", "comments": ["@shps951023 ,\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 55367, "title": "removing tpu_executor_dlsym_initializer ", "body": null, "comments": []}, {"number": 55366, "title": "DOC: explain the meaning of model.fit() history tracking in a distributed scenario", "body": "## URL(s) with the issue:\r\n\r\nPerhaps most closely related to some of these docs pages:\r\n  - https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\r\n  - https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#overview\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThere are fairly substantial docs about distributed training with tensorflow, including docs related to the `MultiWorkerMirroredStrategy`. But, as far as I can tell, reading those docs still doesn't prepare the end user for some of the perils of distributed training. One thing I'd like to see described/explained by an expert, is what it means to track the `history` result of `model.fit()` from many different distributed workers.\r\n\r\nFor example, what does it mean that the learning curves are different between different workers? Should they be? What about accuracy results exceeding 100 % like in the two-worker example results below? Should we even be looking at these individually?\r\n\r\nNode 0 `model.fit()` results:\r\n\r\n![bench_epoch_rank_0_hostname_cn662](https://user-images.githubusercontent.com/7903078/160017020-721ca1a4-a28c-48d2-bf43-9dadcc14c6f7.png)\r\n\r\nNode 1 `model.fit()` results:\r\n\r\n![bench_epoch_rank_1_hostname_cn663](https://user-images.githubusercontent.com/7903078/160017031-3300d4b9-de47-4180-a4ac-a02c18aeb298.png)\r\n\r\nShould the results be condensed/reduced somehow? For that matter, is there any troubleshooting documentation on what an accuracy greater than 100 % means?\r\n\r\n### Correct links\r\n\r\nNot related to links.\r\n\r\n### Parameters defined\r\n\r\nNot related to parameters.\r\n\r\n### Returns defined\r\n\r\nPerhaps for `model.fit()` there could be a side-note of some sort on how to interpret `history.history` in distributed scenarios, and what kinds of fluctuations between workers are normal vs. suspicious?\r\n\r\n### Raises listed and defined\r\n\r\nNo exception handling issues reported here.\r\n\r\n### Usage example\r\n\r\nI don't think I've actually seen a good usage example that shows the distributed `history.history` tracking plots differ between workers?\r\n\r\n### Request visuals, if applicable\r\n\r\nI could probably provide some sample loss/accuracy plots for distributed cases if that would help.\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? \r\n\r\nIf you can offer some guidance on how this is supposed to work, I think that would be useful enough for me to justify helping improve the docs as well.\r\n", "comments": ["To make it a bit clearer why this can be confusing, the **exact** same code on a single MPI rank/node produces what looks to me like fairly sensible ML progress curves--sure the overfitting is extreme because it is one GPU for a few minutes and a microscopic sub-sample of `imagenet2012`, but not nearly as wild as the multi-node/multi-GPU case.\r\n\r\n![bench_epoch_rank_0_hostname_cn660](https://user-images.githubusercontent.com/7903078/160023369-aa9c9eec-edfb-4eb9-81bf-ac7a85542682.png)\r\n", "I also almost wonder if an accuracy > 100 % warrants a warning message with suggestions for improvements/likely fixes, but maybe there are custom/debugging ML scenarios where that is valid.", "@tylerjereddy ,\r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues).\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 55363, "title": "[Autograph] add min max builtins", "body": "This will try to add min/max builtins in Autograph.\n\n/cc @mdanatg ", "comments": ["Sorry but my local `bazel` cache in the container was invalidated again. I could do some \"blind edit\" if you want but we need to abuse the CI in the meantime.", "I've refactored this a little bit and extended the tensor test. We could also add some extra tests as  `@unittest.expectedFailure` with a `TODO`", "@mdanatg Pylint is happy. Anything specific?", "> These are the ones that caught my eye. The internal lint is most likely to flag them.\r\n\r\nSo it seems that `C0326` is active. What is missing here?\r\nhttps://github.com/tensorflow/tensorflow/blob/dfc736bb7cb5df9d6c83dfd11a1ea88970d2815b/tensorflow/tools/ci_build/pylintrc#L45-L48", "Ok now it seems clear do you know why we have `W`, and so white spaces warnings checks, totally disabled? \r\nCause I suppose it is better to check these things locally instead of wasting internal CI cycles (as we cannot trigger on demand only the Pylint job).\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/dfc736bb7cb5df9d6c83dfd11a1ea88970d2815b/tensorflow/tools/ci_build/pylintrc#L36", "No idea why those are disabled. They shouldn't be. I suspect there might be some old code in violation of those rules, that would break them. But maybe it's just an oversight.", "Is this ready to pull?", "> Is this ready to pull?\r\n\r\nHi @bhack  It is in awaiting review status because there is a recent commit after the PR approved.  Thank you!", "Odd, I don't see any changes since the last approval.", "`feedback/copybara`  is failing again", "Let me know if I could do something else as copybara is not accessible to the community."]}, {"number": 55362, "title": "Add error handling to TF_GetInputTensorFromVariable", "body": null, "comments": []}, {"number": 55361, "title": "About how does buffer size from Dataset.shuffle() influence Dataset.cache()", "body": "Hi,\r\nI find when I set `train_ds = train_ds.cache(filename='...')`\r\n`train_ds = train_ds.shuffle(buffer_size=int(training_size/10)).batch(batch_size).prefetch(buffer_size=AUTOTUNE)`\r\nDuring the first epoch of model fitting, what happened looks strange.\r\n\r\n`Epoch 1/10\r\n2022-03-24 16:54:22.982546: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8101\r\n245/274 [=========================>....] - ETA: 8s - loss: 1.5153 - accuracy: 0.5639`\r\n\r\n`2022-03-24 16:55:42.858875: W tensorflow/core/kernels/data/cache_dataset_ops.cc:233] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.`\r\n\r\n`274/274 [==============================] - 109s 352ms/step - loss: 1.4703 - accuracy: 0.5753 - val_loss: 2.4110 - val_accuracy: 0.3950`\r\n\r\n`Epoch 2/10\r\n274/274 [==============================] - 93s 336ms/step - loss: 0.9126 - accuracy: 0.7168 - val_loss: 0.7763 - val_accuracy: 0.7676`\r\n\r\n`Epoch 3/10\r\n274/274 [==============================] - 92s 334ms/step - loss: 0.7067 - accuracy: 0.7886 - val_loss: 0.6996 - val_accuracy: 0.7922`\r\n`\r\n\r\nIt seems that the model fitting ends before the feeding of the last 1/10 batches (this proportion is same as the proportion used in buffer size, I set this number in buffer size to save memory). Then the model fitting is restarted and the first epoch model fitting is finish.\r\n\r\nI would like to ask 1) why this happens? 2) Is all my dataset successfully been cached rather than only the dataset for the first 90% batches is cached?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@xjlupeter ,\r\nCan you please take a look at this SO [link](https://stackoverflow.com/questions/64372390/what-does-buffer-size-do-in-tensorflow-dataset-shuffling) and [link](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) which delivers the required information.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55361\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55361\">No</a>\n"]}, {"number": 55360, "title": "DeeplabV3 custom dataset, inference problem black images", "body": "Good morning,\r\n\r\nI want to train a custom dataset using deeplabV3.\r\n\r\nI'm following this tutorial (https://sanjayparajuli27.medium.com/how-to-train-deeplab-on-custom-dataset-a40c41c4c6a3) for this dataset (https://www.kaggle.com/datasets/dansbecker/cityscapes-image-pairs) that I found on Kaggle, based on cityscapes.\r\n\r\nThere are images of 256x256 pixel in RGB colors, divided in 2975 imgs for training and 500 for validation, and I created the respective mask using this script\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom PIL import Image\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\n\r\nimport os, shutil\r\n\r\n# palette (color map) describes the (R, G, B): Label pair\r\npalette = {(0,   0,   0) : 0 ,\r\n           (128,  0, 0) : 1,\r\n           (0, 128, 0): 2,\r\n           (128, 128, 0): 3,\r\n           (0, 0, 128): 4,\r\n           (0, 128, 128): 5,\r\n           (128, 128, 128): 6,\r\n           (64, 0, 0): 7,\r\n           (192, 0, 0): 8,\r\n           (64, 128, 0): 9,\r\n           (192, 128, 0): 10,\r\n           (64, 0, 128): 11,\r\n           (192, 0, 128): 12,\r\n           (64, 128, 128): 13,\r\n           (0, 64, 0): 14,\r\n           (128, 64, 0): 15,\r\n           (0, 192, 0): 16,\r\n           (128, 192, 0): 17,\r\n           (0, 64, 128): 18,\r\n           (128, 0, 128): 19\r\n         }\r\n\r\ndef convert_from_color_segmentation(arr_3d):\r\n    arr_2d = np.zeros((arr_3d.shape[0], arr_3d.shape[1]), dtype=np.uint8)\r\n\r\n    for c, i in palette.items():\r\n        m = np.all(arr_3d == np.array(c).reshape(1, 1, 3), axis=2)\r\n        arr_2d[m] = i\r\n    return arr_2d\r\n\r\n\r\nlabel_dir = \"C:/Users/paolo.david/Desktop/Datasets/final/Validation/Mask_real/\" #don't forget the '/' at the end\r\nnew_label_dir = \"C:/Users/paolo.david/Desktop/Datasets/final/Validation/Mask_RAW2/\"\r\n\r\nif not os.path.isdir(new_label_dir):\r\n    print(\"creating folder: \",new_label_dir)\r\n    os.mkdir(new_label_dir)\r\nelse:\r\n    print(\"Folder alread exists. Delete the folder and re-run the code!!!\")\r\n\r\n\r\nlabel_files = os.listdir(label_dir)\r\n\r\nfor l_f in tqdm(label_files):\r\n    #arr = np.array(Image.open(l_f))\r\n    arr = np.array(Image.open(label_dir + l_f))\r\n    arr = arr[:,:,0:3]\r\n    arr_2d = convert_from_color_segmentation(arr)\r\n    #Image.fromarray(arr_2d).save(label_dir)\r\n    Image.fromarray(arr_2d).save(new_label_dir + l_f)\r\n```\r\n\r\nEach image in the dataset contain its same mask, so before to launch the new notebook I divided the image and the mask to have a situation like in the tutorial.\r\n\r\nYou can find my code here: https://drive.google.com/drive/folders/105JMDmujY6lknH3D74WM8R8S7jTb51qX?usp=sharing and this is the notebook https://drive.google.com/file/d/1xmUtLB-XPj4mZdqbx9SAQOXKSwxtxCLX/view?usp=sharing\r\n\r\nI have a problem with the inference. Every time I launch the notebook with few epochs (less then 10) I receive good results, but trying to increase the number of epoch I have all black images.\r\n\r\nThese are the parameters that I used for the train:\r\n\r\n```\r\n--model_variant=\"xception_65\" \\\r\n--atrous_rates=6 \\\r\n--atrous_rates=12 \\\r\n--atrous_rates=18 \\\r\n--output_stride=16 \\\r\n--decoder_output_stride=4 \\\r\n--train_crop_size=\"256,256\" \\\r\n--train_batch_size=4 \\\r\n--training_number_of_steps=30 \\\r\n--initialize_last_layer=False \\\r\n--last_layers_contain_logits_only=True \\\r\n--fine_tune_batch_norm=False \\\r\n```\r\n\r\nI edited the data_generator.py file putting\r\n\r\n```\r\n_CUSTOM_INFORMATION = DatasetDescriptor(\r\n    splits_to_sizes={\r\n        'train': 270,  # num of samples in train.txt\r\n        'val': 30,  # num of samples in val.txt\r\n    },\r\n    num_classes=21, # classes+bg+ignore_label\r\n    ignore_label=255,\r\n)\r\n\r\n_DATASETS_INFORMATION = {\r\n    'cityscapes': _CITYSCAPES_INFORMATION,\r\n    'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,\r\n    'ade20k': _ADE20K_INFORMATION,\r\n    'custom': _CUSTOM_INFORMATION  # custom dataset\r\n}\r\n```\r\n\r\nI'm using a pretrained model downloaded from here: http://download.tensorflow.org/models/deeplabv3_cityscapes_train_2018_02_06.tar.gz\r\n\r\nI keep the batch size at 4, and I don't know if it is correct or not. Can you tell me where could be the possible error?", "comments": ["@paolodavid Could you please post this issue in the tensorflow/models [repo](https://github.com/tensorflow/models/issues?q=is%3Aissue+is%3Aopen) to get the right help there?\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55360\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55360\">No</a>\n"]}, {"number": 55359, "title": "tf2.0  dataset disable_eager_execution", "body": "tf is 2.4.0\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\nwith tf.device('/cpu'):\r\n    dataset = tf.data.Dataset.from_tensor_slices([1,3,4,5,6,7,8])\r\n\r\n@tf.function(autograph=False)\r\ndef iter_funs(ds):\r\n    iterator = iter(dataset)\r\n    return iterator.get_next()\r\n\r\nwith tf.compat.v1.InteractiveSession().as_default():\r\n    with tf.device('/cpu'):\r\n        xx = iter_funs(dataset)\r\n        print(xx.eval())\r\n        print(xx.eval())\r\n\r\nthe output is always 1 ,xx not iterator.\r\nso how can i do to iterator the data", "comments": ["@yanmxiang,\r\nWhen we call function `iter_funs()`, it returns always first element in the list. When we print iterator.get_next() inside the function it shows values of the datalist. \r\n\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices([1,3,4,5,6,7,8])\r\n\r\n@tf.function\r\ndef iter_funs(dataset):\r\n  iterator = iter(dataset)\r\n  print(iterator.get_next())\r\n  return iterator.get_next()\r\n\r\nwith tf.compat.v1.InteractiveSession().as_default():\r\n  xx = iter_funs(dataset)\r\n  \r\nprint(xx.eval())\r\n\r\n```\r\n**Output**\r\n```\r\nTensor(\"IteratorGetNext:0\", shape=(), dtype=int32)\r\n3\r\n```\r\nPrint the 3rd element\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices([1,3,4,5,6,7,8])\r\n\r\n@tf.function\r\ndef iter_funs(dataset):\r\n  iterator = iter(dataset)\r\n  print(iterator.get_next())\r\n  print(iterator.get_next())\r\n  return iterator.get_next()\r\n\r\n\r\nwith tf.compat.v1.InteractiveSession().as_default():\r\n  xx = iter_funs(dataset)\r\n  \r\nprint(xx.eval())\r\n```\r\n**Output**\r\n```\r\nTensor(\"IteratorGetNext:0\", shape=(), dtype=int32)\r\nTensor(\"IteratorGetNext_1:0\", shape=(), dtype=int32)\r\n4\r\n```", "thank you very much answer my question.\r\nanother question is where to place the training process.\r\nplaced in&nbsp;&nbsp;iter_funs is error\r\nplaced outside of  iter_funs couldn't iterator\r\nimport tensorflow as tf tf.compat.v1.disable_eager_execution() dataset = tf.data.Dataset.from_tensor_slices([1,3,4,5,6,7,8]) @tf.function def iter_funs(dataset):   iterator = iter(dataset)   print(iterator.get_next())   print(iterator.get_next())   return iterator.get_next() with tf.compat.v1.InteractiveSession().as_default():   xx = iter_funs(dataset)  sess.run(model(xx))\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba:                                                                                                                        \"tensorflow/tensorflow\"                                                                                    ***@***.***&gt;;\r\n\u53d1\u9001\u65f6\u95f4:&nbsp;2022\u5e743\u670825\u65e5(\u661f\u671f\u4e94) \u4e0b\u53482:45\r\n***@***.***&gt;;\r\n***@***.******@***.***&gt;;\r\n\u4e3b\u9898:&nbsp;Re: [tensorflow/tensorflow] tf2.0  dataset disable_eager_execution (Issue #55359)\r\n\r\n\r\n\r\n\r\n\r\n \r\n@yanmxiang,\r\n When we call function iter_funs(), it returns always first element in the list. When we print iterator.get_next() inside the function it shows values of the datalist.\r\n import tensorflow as tf tf.compat.v1.disable_eager_execution() dataset = tf.data.Dataset.from_tensor_slices([1,3,4,5,6,7,8]) @tf.function def iter_funs(dataset):   iterator = iter(dataset)   print(iterator.get_next())   return iterator.get_next() with tf.compat.v1.InteractiveSession().as_default():   xx = iter_funs(dataset)    print(xx.eval())  \r\nOutput\r\n Tensor(\"IteratorGetNext:0\", shape=(), dtype=int32) 3  \r\nPrint the 3rd element\r\n import tensorflow as tf tf.compat.v1.disable_eager_execution() dataset = tf.data.Dataset.from_tensor_slices([1,3,4,5,6,7,8]) @tf.function def iter_funs(dataset):   iterator = iter(dataset)   print(iterator.get_next())   print(iterator.get_next())   return iterator.get_next() with tf.compat.v1.InteractiveSession().as_default():   xx = iter_funs(dataset)    print(xx.eval())  \r\nOutput\r\n Tensor(\"IteratorGetNext:0\", shape=(), dtype=int32) Tensor(\"IteratorGetNext_1:0\", shape=(), dtype=int32) 4  \r\n\u2014\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***&gt;", "@yanmxiang, Glad that it answered your question. Will close this issue. \r\nCould you open new issue by filling the template , Since the title of the issue is resolved. Thanks!"]}]