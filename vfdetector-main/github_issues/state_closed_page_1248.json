[{"number": 15711, "title": "Update estimator.md", "body": "The anchor \"#fit_dnnclassifier\" can not locate to a valid position. You can try the following two links:\r\n\r\nhttps://www.tensorflow.org/get_started/estimator#fit_dnnclassifier\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/estimator.md#fit-dnnclassifier", "comments": ["/CC @MarkDaoust ", "Jenkins, test this please.", "Can one of the admins verify this patch?"]}, {"number": 15710, "title": "fix doc `tf.data.contrib.map_and_batch` to `tf.contrib.data.map_and_batch`", "body": "change from unexist `tf.data.contrib.map_and_batch` to `tf.contrib.data.map_and_batch`", "comments": ["Jenkins, test this please.", "Jenkins, test this please.", "Can one of the admins verify this patch?", "GPU CC fails without link, trying again.", "Unrelated to this change, ignoring."]}, {"number": 15709, "title": "Update graphs.md", "body": "", "comments": ["Jenkins, test this please.", "Jenkins, test this please.", "Can one of the admins verify this patch?"]}, {"number": 15708, "title": "Update estimator.md", "body": "", "comments": ["Jenkins, test this please.", "Jenkins, test this please.", "Can one of the admins verify this patch?"]}, {"number": 15707, "title": "updated the latest mkldnn", "body": "This commit will pull the latest changes from the mkl-dnn tree.\r\n\r\nthe mirror.bazel.build URL doesn't exist: \"https://mirror.bazel.build/github.com/01org/mkl-dnn/archive/e0bfcaa7fcb2b1e1558f5f0676933c1db807a729.tar.gz\" can you create it?", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I have signed it. Can you verify it? Thanks.", "I cannot find you in our database. Please go on the site and check again.", "@jinghuangintel Your commit email looks like it was not set correctly. Could you update your commit author information using `git commit --amend`?", "@gunan thanks for your reminder and have changed the commit email. Thanks."]}, {"number": 15706, "title": "Feature request: unsafe_div op", "body": "As discussed with @drpngx in #14667 , we found that several PRs are related to division by zero problem, for example, #15443, #14865. \r\n\r\n#### 1. why do we need the op\r\n\r\nIn `metrics`, `loss` and `math_ops` modules, all create a `_safe_div` or `_safe_scalar_div` method with help of `array_ops.where` to resolve the problem. However, we think those implementations have three disadvantages:\r\n1. Lack of generality: most are designed for only scalar.\r\n2. Not efficient.\r\n3. Since `where` ops propagates NaNs in gradients (#2540), we have to use nested `where` trick which is a little counter-intuitive.\r\n\r\n#### 2. what behavior do we expect it\r\n\r\nAbove all, we propose to create new c++ op: `safe_div`:\r\n+ When denominator is zero, return 0 or numerator. The behavior can be selected by user themselves.\r\n+ Optional: Treat negative as zero, which is sometime useful for loss calculation.\r\n+ Create its own gradient op to avoid NaN problem for `where`.\r\n\r\nI think for metric, loss and gradient calculation, they could benefit from the op and get rid of NaN.\r\nAnd I'd like to contribute the op in **contrib** at first if the proposition is approved by tensorflowers.\r\n\r\n#### 3. how to implement it\r\n\r\nBy the way, since  the name`safe_div` has been used in cwise_ops.h, whose behavior is opposite: raise an exception for integer when divide by zero. \r\nhttps://github.com/tensorflow/tensorflow/blob/3629fc4e98254c37e614ac3f77fa250b75c70f8d/tensorflow/core/kernels/cwise_ops.h#L703\r\nSo perhaps we need to either rename it or create a new name for our op. Does anyone has a good idea?", "comments": ["@aselle has thought a lot about division in the past if I recall correctly. Thoughts?", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@aselle any advice?", "@aselle could you make a comment? Thanks.", "I wouldn't be oposed to a safe_div option. The main thing I've thoguht about is how div is bound by say operator overloading in python especially with regards to integer division. I would say the existing / and // functions should stay the same. I would also not call this safe_div, because in general it isn't really safe to ignore divison by zeros. Some other name might be better like tf.div_no_nan() or tf.div_or_zero()\r\n\r\n1. Don't bind it to /\r\n2. Think about how you want to handle div's with integers. Is this a floating point only op. Probably a good idea for now. Otherwise, you may need a truncate versoin and a flooring version.\r\n\r\n@martinwicke, may have some opinions as well.", "That sounds sensible. I'm a little concerned about all the different behaviors: div by zero yields 0 or denominator, treat negatives as zero...\r\n\r\nI'd like it better if we could simplify this a bit (definitely treat negatives as zero should be handled by simply running max(0, x) beforehand).", "Thanks for your feedback.\r\n\r\n@aselle `tf.div_no_nan` sounds better.\r\n\r\n1. I think we don't want to bind the op to Tensor, hence it seems unrelated to / or // function, right?\r\n2. The op is designed for float value. I agree that we can restrict it to float32, float64 only for now.\r\n\r\n@martinwicke `max(0, x)` looks good. \r\n\r\nI think we can, for now, only support that return 0 when denominator is zero, and for float32, float 64 only. How do you think? \r\n\r\nIf approved, I'd like to revise PR #19105 according to our discussion here.", "@tensorflow/api-owners to get a definitive answer.", "(For tf-api-owners): The API addition is fine (presumably the PR will be updated to include GPU registrations as well?)", "@asimshankar Thanks. How about adding GPU kernel later?", "Closed because it's fixed by #21621."]}, {"number": 15705, "title": "updated the latest mkldnn", "body": "This commit will pull the latest changes from the mkl-dnn tree.\r\n\r\nthe mirror.bazel.build URL doesn't exist: \"https://mirror.bazel.build/github.com/01org/mkl-dnn/archive/e0bfcaa7fcb2b1e1558f5f0676933c1db807a729.tar.gz\" can you create it?", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "This commit will pull the latest changes from the mkl-dnn tree."]}, {"number": 15704, "title": "Used template version of SafeStringToNumeric to reduce code duplication", "body": "This fix uses template version of SafeStringToNumeric to avoid customerized ConvertHelper, for the purpose of reduce code duplication.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@tensorflow-jenkins test this please.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 15703, "title": "Branch 180304271", "body": "", "comments": ["Jenkins, test this please.", "Windows CMake seems stuck... Trying again.\r\n\r\nJenkins, test this please.", "Still stuck.\r\n\r\nJenkins, test this please.", "Jenkins, test this please.", "Yep, @yifeif @gunan it looks like all jenkins builds are frozen for some reason.", "@tensorflow-jenkins test this please", "Jenkins, test this please.", "There was a problem with ssl certificates. it should be working now.", "Thanks!\n\nOn Sat, Dec 30, 2017, 9:37 PM Gunhan Gulsoy <notifications@github.com>\nwrote:\n\n> There was a problem with ssl certificates. it should be working now.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15703#issuecomment-354586335>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_Sbd_f-oiD2AIJEGiY3-GFnmTsN5UTks5tFx2bgaJpZM4ROyKR>\n> .\n>\n", "Woohoo!"]}, {"number": 15702, "title": "Fix out of memory issue on Tegra devices", "body": "TF used to allocate (available memory - 300MB) or (available memory - 225MB)\r\nfor TF to use. This is fine for graphic cards, but will cause out of memory\r\nissue on Tegra.\r\nModify to allocate (available memory - 1GB) for Tegra.\r\n1GB should be enough for OS and other apps\r\n(available memory - 1GB) should be 0.8-1.5GB which is enough for most graph for TF.", "comments": ["Can one of the admins verify this patch?", "Not sure if it's the best way to handle this.", "Jenkins, test this please.", "I think it would be better if there is a way to estimate how much mem TF is going to use", "Jenkins, test this please."]}, {"number": 15701, "title": "Cannot run mobilenet_0.25_128_quantized image retrain", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: python tensorflow/examples/image_retraining/retrain.py --image_dir /path/to/image/dir/  --architecture mobilenet_0.25_128_quantized\r\n\r\n### Describe the problem\r\nwhile trying to create a mobilenet model, getting this error. The url being created in the retrain.py file, is not accessible via browser either.\r\n\r\n### Source code / logs\r\ntensorflow.python.framework.errors_impl.NotFoundError: /tmp/imagenet/mobilenet_v1_0.25_128_quantized_frozen/quantized_frozen_graph.pb; No such file or directory\r\n\r\n  ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "It is actually a bug, if only in the hosting, since the file [http://download.tensorflow.org/models/mobilenet_v1_0.25_128_quantized_frozen.tgz](http://download.tensorflow.org/models/mobilenet_v1_0.25_128_quantized_frozen.tgz) is not reachable, whereas eg. [http://download.tensorflow.org/models/mobilenet_v1_1.0_224_quantized_frozen.tgz](http://download.tensorflow.org/models/mobilenet_v1_1.0_224_quantized_frozen.tgz) is.", "+1", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Yes, this is still an issue: [http://download.tensorflow.org/models/mobilenet_v1_0.25_128_quantized_frozen.tgz](http://download.tensorflow.org/models/mobilenet_v1_0.25_128_quantized_frozen.tgz) and many of the other smaller models aren't reachable.", "+1", "Duplicate of #15009", "We have published new models here: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\r\n\r\nI have pushed changes to the retrain script to support these, once the next sync happens.\r\n\r\nCommands like\r\n```\r\npython tensorflow/examples/image_retraining/retrain.py \\\r\n    --image_dir ~/flower_photos/   --architecture mobilenet_1.0_224_quant\r\n```\r\nshould work for all depth multipliers and image sizes.\r\n\r\nPlease create a new issue if things don't work once github pulls in the changes to retrain.py\r\n"]}, {"number": 15700, "title": "Cherrypicks", "body": "Fixes for release tests.", "comments": []}, {"number": 15699, "title": "[Android] Dex cannot parse version 52 byte code", "body": "When including:\r\n\r\n`compile 'org.tensorflow:tensorflow-lite:0.1.1'`\r\n\r\nI get the error:\r\n\r\n> [Android] Dex cannot parse version 52 byte code\r\n\r\nIf I use AGP 3.0.0+ the problem goes away since it has support for Java 8, but the problem exists if I try to use 2.3.3. \r\n", "comments": ["By AGP do you mean Android Studio? TensorFlow Lite requires Android Studio 3 (from [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite#building-in-android-studio-using-tensorflow-lite-aar-from-jcenter)).\r\n\r\nFYI @aselle @andrehentz ", "Yes, that's what I was asking. Thanks for the help!"]}, {"number": 15698, "title": "Exception trying to import a retrained model in android classifier demo app", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n('v1.3.0-rc1-5733-gb43d0f3', '1.4.0')\r\n- **Python version**: \r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n0.9.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\nI have retrained a mobilenet_v1_1.0_224 frozen_graph.pb model with the following script:\r\n\r\nexport PYTHONPATH=/usr/local/lib/python2.7/dist-packages:/usr/lib/python2.7/dist-packages\r\nexport IMAGE_SIZE=224\r\nexport WIDTH_MUL=1.0 # 0.75 0.50 0.25\r\nexport ARCHITECTURE=\"mobilenet_${WIDTH_MUL}_${IMAGE_SIZE}\"\r\nexport BASE_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\"\r\nexport TF_FILES_DIR=\"tf_files\"\r\npython -m scripts.retrain \\\r\n  --bottleneck_dir=\"${TF_FILES_DIR}\"/bottlenecks \\\r\n  --how_many_training_steps=4000 \\\r\n  --model_dir=\"${TF_FILES_DIR}\"/models/ \\\r\n  --summaries_dir=\"${TF_FILES_DIR}\"/training_summaries/\"${ARCHITECTURE}\" \\\r\n  --output_graph=\"${TF_FILES_DIR}\"/retrained_graph.pb \\\r\n  --output_labels=\"${TF_FILES_DIR}\"/retrained_labels.txt \\\r\n  --architecture=\"${ARCHITECTURE}\" \\\r\n  --image_dir=\"${TF_FILES_DIR}\"/flower_photos\r\n\r\nI copied \"retrained_graph.pb\" and \"retrained_labels.txt\" in the \"assets\" directory of the android demo app in tensorflow/examples/android and modified the source code in ClassifierActivity.java as follows:\r\n\r\n/* Original code:\r\n  private static final String INPUT_NAME = \"input\";\r\n  private static final String OUTPUT_NAME = \"output\";\r\n  private static final String MODEL_FILE = \"file:///android_asset/tensorflow_inception_graph.pb\";\r\n  private static final String LABEL_FILE = \"file:///android_asset/imagenet_comp_graph_label_strings.txt\";\r\n*/\r\n/* Replaced by: */\r\n  private static final String INPUT_NAME = \"input\";\r\n  private static final String OUTPUT_NAME = \"final_result\";\r\n  private static final String MODEL_FILE = \"file:///android_asset/retrained_graph.pb\";\r\n  private static final String LABEL_FILE = \"file:///android_asset/retrained_labels.txt\";\r\n/* End */\r\n\r\nI cleaned and rebuilt the whole project in Android Studio, successfully.\r\n\r\nThe app was then loaded on a Huawei P8 Lite (Android 7.0) and starting TF Classify, the following error occurs:\r\n\r\nE/tensorflow: CameraActivity: Exception!\r\n              java.lang.RuntimeException: Failed to load model from 'file:///android_asset/retrained_graph.pb'\r\n                  at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:113)\r\n                  at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:103)\r\n                  at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:118)\r\n                  at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:120)\r\n                  at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1204)\r\n                  at android.os.Handler.dispatchMessage(Handler.java:105)\r\n                  at android.os.Looper.loop(Looper.java:156)\r\n                  at android.app.ActivityThread.main(ActivityThread.java:6523)\r\n                  at java.lang.reflect.Method.invoke(Native Method)\r\n                  at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:942)\r\n                  at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:832)\r\n               **Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_0/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true](input, MobilenetV1/Conv2d_0/weights/read)**. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n                  at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:535)\r\n                  at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)\r\n                  at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:103)\u00a0\r\n                  at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:118)\u00a0\r\n                  at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:120)\u00a0\r\n                  at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1204)\u00a0\r\n                  at android.os.Handler.dispatchMessage(Handler.java:105)\u00a0\r\n                  at android.os.Looper.loop(Looper.java:156)\u00a0\r\n                  at android.app.ActivityThread.main(ActivityThread.java:6523)\u00a0\r\n                  at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n                  at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:942)\u00a0\r\n                  at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:832)\u00a0\r\nApplication terminated.\r\n\r\nHow to fix this error?", "comments": ["Is it possible that there is a version mismatch between the version of TensorFlow used to run the training script and the version of the library included in your Android application? (The exception message says \"Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary\").\r\n\r\nThe `dilations` attribute was added in commit https://github.com/tensorflow/tensorflow/commit/cb4ef362e4a18b3c42a2c90bdad8754d5ead4caf#diff-632987400e5affbcdba4533444460b0e - so if the graph was generated with a version of TensorFlow with that commit but the Android library didn't contain that commit, then you'd see this error.\r\n\r\n", "Thanks @asimshankar, yes, a version mismatch is possible. However, cleaning the Android TF Demo project completely and rebuilding, I expect that an updated and aligned version of libtensorflow_inference.so and libtensorflow_demo.so are built from local sources or downloaded anew, (depending on the value of \"nativeBuildSystem\" in build.gradle? I only succeed setting it to \"none\") and the mismatch should disappear. But this is not what happens to me and the error is still there. Am I missing something?", "@ecahub - what is the value of `nativeBuildSystem` you're using? I think with `none` the TensorFlow runtime isn't actually built from source and instead you're using the one from the dependencies in gradle file. If that is something like:\r\n\r\n```\r\ndependencies {\r\n    compile 'org.tensorflow:tensorflow-android:+'\r\n}\r\n```\r\n\r\nthen, as of today, this would resolve to [version 1.4.0](https://bintray.com/google/tensorflow/tensorflow/1.4.0), which does not contain the `dilations` commit.\r\n\r\nI believe your options are to either:\r\n\r\n(a) Generate the graph with TensorFlow 1.4.0, or\r\n(b) Build the TensorFlow runtime from sources for your Android app by setting `nativeBuildSystem` to `bazel` or `cmake`, or\r\n(c) Try the \"nightly\" build of the Android builds as outlined in https://www.tensorflow.org/mobile/linking_libs#android\r\n\r\nHope that helps", "@asimshankar - Thank you so much for your great support. I tried several options for nativeBuildSystem rebuilding Android demo app, with the following results:\r\n\r\n1) nativeBuildSystem = 'bazel', gives the following error\r\n`ERROR: /home/eca/.cache/bazel/_bazel_eca/0ad9491e973410d424f29db1d78c1796/external/protobuf_archive/BUILD:135:1: C++ compilation of rule '@protobuf_archive//:protobuf' failed (Exit 1)\r\nIn file included from external/protobuf_archive/src/google/protobuf/util/internal/object_writer.cc:31:\r\nIn file included from external/protobuf_archive/src/google/protobuf/util/internal/object_writer.h:34:\r\nIn file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:38:\r\nIn file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/algorithm:62:\r\nIn file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/stl_algo.h:59:\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cstdlib:72:10: fatal error: 'stdlib.h' file not found\r\n#include <stdlib.h>\r\n         ^~~~~~~~~~\r\n1 error generated.\r\nTarget //tensorflow/examples/android:tensorflow_native_libs failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 55.677s, Critical Path: 0.37s\r\nFAILED: Build did NOT complete successfully\r\n`\r\n\r\nThis could perhaps be due to a version of NDK installed by Android Studio (ndk version 16.1.4479499) that is apparently incompatible with bazel (bazel version 0.9.0) - organization of header directories?\r\n\r\n2) nativeBuildSystem = 'cmake' seems to have the effect of downloading the library, like 'none', according to the the latest lines of build.gradle:\r\n`dependencies {\r\n    if (nativeBuildSystem == 'cmake' || nativeBuildSystem == 'none') {\r\n        compile 'org.tensorflow:tensorflow-android:+'\r\n    }\r\n}\r\n`\r\n\r\nIn this case, the retrained model is incompatible with the original libtensorflow_inference.so and libtensorflow_demo.so\r\n\r\n3) nativeBuildSystem = 'makefile' gives another error:\r\n`libtool: compile:  arm-linux-androideabi-g++ --sysroot /home/eca/Android/Sdk/ndk-bundle/platforms/android-21/arch-arm -std=c++11 -DHAVE_CONFIG_H -I. -I.. -Wall -Wno-sign-compare -frtti -fexceptions -march=armv7-a -I/home/eca/Android/Sdk/ndk-bundle/sources/android/support/include -I/home/eca/Android/Sdk/ndk-bundle/sources/cxx-stl/gnu-libstdc++/4.9/include -I/home/eca/Android/Sdk/ndk-bundle/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -MT google/protobuf/stubs/common.lo -MD -MP -MF google/protobuf/stubs/.deps/common.Tpo -c google/protobuf/stubs/common.cc -o google/protobuf/stubs/common.o\r\nIn file included from /home/eca/Android/Sdk/ndk-bundle/sources/cxx-stl/gnu-libstdc++/4.9/include/cstdlib:72:0,\r\n                 from /home/eca/Android/Sdk/ndk-bundle/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/stl_algo.h:59,\r\n                 from /home/eca/Android/Sdk/ndk-bundle/sources/cxx-stl/gnu-libstdc++/4.9/include/algorithm:62,\r\n                 from ./google/protobuf/stubs/common.h:38,\r\n                 from ./google/protobuf/stubs/atomicops.h:59,\r\n                 from google/protobuf/stubs/atomicops_internals_x86_msvc.cc:37:\r\n/home/eca/Android/Sdk/ndk-bundle/sources/android/support/include/stdlib.h:32:25: fatal error: stdlib.h: No such file or directory\r\n #include_next <stdlib.h>\r\n                         ^\r\ncompilation terminated.\r\n`\r\n\r\nThis seems to be due to the fact that `#include_next` only checks the paths that are after where the current header was found, so apparently there is some issue in the include path order - organization of headers?\r\n\r\n4) nativeBuildSystem = 'none' compiles successfully and runs smoothly with original models. To work with retrained models I have done the following:\r\n- Downloaded the nightly build from http://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/\r\n- Created a directory called \"libs\" in the root project directory (besides \"assets\", \"src\", \"jni\", etc.) and copied the in it the following files:\r\n`libs\r\n\r\n    |-- armeabi-v7a\r\n\r\n    |   |-- libtensorflow_demo.so\r\n\r\n    |   |   \\-- libtensorflow_inference.so\r\n\r\n    \\-- libandroid_tensorflow_inference_java.jar\r\n`\r\n\r\n- In build.gradle, modified the last \"dependencies\" as follows:\r\n`dependencies {\r\n    if (nativeBuildSystem == 'cmake' || nativeBuildSystem == 'none') {\r\n        implementation files('libs/libandroid_tensorflow_inference_java.jar')\r\n    }\r\n}\r\n`\r\n\r\nThen, compile, upload to the test device and run TF Classifier: it works!\r\n\r\n  ", "Hey, I'm having the same problem and was wondering if you could clarify on your solution. You mention nativeBuildSystem, but my build.gradle file does not mention that anywhere. Do I not understand what you mean, or are we looking at different pieces of code? If it's the latter, can you direct me to where you downloaded your code? I've been following the TensorFlow for Poets 2 codelab and have cloned the github repository here, https://github.com/googlecodelabs/tensorflow-for-poets-2 ", "Hi @Jerry2001Qu \r\nThe \"nativeBuildSystem\" I am referring to is in file tensorflow/examples/android/build.gradle and you can see it in the Android Studio's Project window as \"build.gradle (Module:android)\" in the \"Gradle Scripts\" branch. It is around line 43 in my version.\r\nI solved my problem as explained in point 4 of my previous post, i.e. downloaded the nightly build of tensorflow libraries from the URL indicated above, then made two changes in build.gradle:\r\na) set `nativeBuildSystem = 'none'`\r\nb) at the end of file, changed as follows:\r\n```\r\ndependencies {\r\n    if (nativeBuildSystem == 'cmake' || nativeBuildSystem == 'none') {\r\n//        implementation 'org.tensorflow:tensorflow-android:+' <-- this was the original content\r\n        implementation files('libs/libandroid_tensorflow_inference_java.jar') // <-- this is the new content\r\n    }\r\n}\r\n```\r\nThe latter, in short, avoids recompiling tensorflow libraries and uses those downloaded from the nightly build.\r\nHTH!", "Hi there,\r\nI had the same problem and attempted to follow the above workaround.  (without success)\r\n\r\nBut then thinking about the original error.  \"Version miss match with Tensorflow\"\r\n\r\nEasy solution which worked for me.\r\n\r\nVerify the version of Tensorflow used on the Laptop to build  1.5.0\r\n\r\nSet same version in Android build.   Slap forehead....\r\n\r\ndependencies {\r\ncompile 'org.tensorflow:tensorflow-android:1.5.0'\r\n}\r\n\r\ndone ....", "I not have laptop\n\nPada tanggal 11 Feb 2018 4.14 PM, \"ericwhiteau\" <notifications@github.com>\nmenulis:\n\nHi there,\nI had the same problem and attempted to follow the above workaround.\n(without success)\n\nBut then thinking about the original error. \"Version miss match with\nTensorflow\"\n\nEasy solution which worked for me.\n\nVerify the version of Tensorflow used on the Laptop to build 1.5.0\n\nSet same version in Android build. Slap forehead....\n\ndependencies {\ncompile 'org.tensorflow:tensorflow-android:1.5.0'\n}\n\ndone ....\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/15698#issuecomment-364735908>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/Ail_Euq1iL4onJlYa6VcBtiV5I_xM5Meks5tTq9fgaJpZM4ROc5z>\n.\n", "Sorry - More generic.\r\nWhatever system you used to build the model for the sample application.  As it is retraining a network.\r\n\r\nYou need the version of the Tensorflow that you built the model with and set that version.", "please share your number whatsapp you make it easier\n\nPada tanggal 11 Feb 2018 5.48 PM, \"ericwhiteau\" <notifications@github.com>\nmenulis:\n\n> Sorry - More generic.\n> Whatever system you used to build the model for the sample application. As\n> it is retraining a network.\n>\n> You need the version of the Tensorflow that you built the model with and\n> set that version.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15698#issuecomment-364741771>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Ail_EnK8zPfBUHCFJylmTOOMtl4CINYuks5tTsVggaJpZM4ROc5z>\n> .\n>\n", "if anyone faced the same problem,my step might help you.\r\nas i was facing the same error,so what i did\r\nchanged this line\r\ndependencies {\r\ncompile 'org.tensorflow:tensorflow-android:1.2.0-preview'\r\n}\r\nto\r\ndependencies {\r\ncompile 'org.tensorflow:tensorflow-android:1.6.0'\r\n}\r\nsince i'm using tensorflow 1.6.0\r\nthen i was getting deprecated warning about compile ,so therefore i changed it to \"implementation\"\r\ntherefore now it look like this\r\n\r\ndependencies {\r\nimplementation 'org.tensorflow:tensorflow-android:1.6.0'\r\n}\r\nbut then as soon i run my app i faced another problem which was\r\n\"Execution failed for task ':transformDexArchiveWithExternalLibsDexMergerForDebug'. > com.android.builder.dexing.DexArchiveMergerException: Unable to merge dex\"\r\n\r\nso,what i did i went to build options ,choose clean project ,then after cleaning was performed by Android Studio,\r\ni clicked on rebuild project,\r\n\r\nthen i clicked on run, and then app started running fine with no crashing\r\n\r\ni hope it helps ,anyone who faced the problem like me,here the proof of my result\r\n![image](https://user-images.githubusercontent.com/26957301/37251484-f6c55a80-2536-11e8-90a2-f11b862a6729.png)\r\n", "Hey @ElephantHunters works for me! Ty!", "Hi @ElephantHunters \r\nI did follow your suggestion, which is:\r\nI modified `compile 'org.tensorflow:tensorflow-android:1.2.0-preview'` by \r\n`implementation 'org.tensorflow:tensorflow-android:1.6.0'`\r\nI created a folder named libs and copied files in this order:\r\n```\r\nlibs\r\n-libtensorflow_demo.so\r\n-libtensorflow_inference.so\r\n-libandroid_tensorflow_inference_java.jar\r\n```\r\nI cleaned the project, rebuilt project successfully, but it crashed on Android after a second with this error:\r\n```\r\n04-10 11:07:24.301 4249-4391/org.tensorflow.demo E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[input], outputs:[final_result]\r\n04-10 11:07:24.301 4249-4391/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: inference\r\n                                                                   Process: org.tensorflow.demo, PID: 4249\r\n                                                                   java.lang.IllegalArgumentException: Tried to explicitly squeeze dimension 1 but dimension was not 1: 2\r\n                                                                   \t [[Node: MobilenetV1/Logits/SpatialSqueeze = Squeeze[T=DT_FLOAT, squeeze_dims=[1, 2], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](MobilenetV1/Logits/Conv2d_1c_1x1/BiasAdd)]]\r\n                                                                       at org.tensorflow.Session.run(Native Method)\r\n                                                                       at org.tensorflow.Session.access$100(Session.java:48)\r\n                                                                       at org.tensorflow.Session$Runner.runHelper(Session.java:298)\r\n                                                                       at org.tensorflow.Session$Runner.run(Session.java:248)\r\n                                                                       at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:230)\r\n                                                                       at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n                                                                       at org.tensorflow.demo.TensorFlowImageClassifier.recognizeImage(TensorFlowImageClassifier.java:150)\r\n                                                                       at org.tensorflow.demo.ClassifierActivity$2.run(ClassifierActivity.java:239)\r\n                                                                       at android.os.Handler.handleCallback(Handler.java:739)\r\n                                                                       at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                                                                       at android.os.Looper.loop(Looper.java:158)\r\n                                                                       at android.os.HandlerThread.run(HandlerThread.java:61)\r\n```\r\nPlease advice me in this problem.\r\nThanks", "hi @AliceDinh ,thanks for reaching me out\r\nfor this tensorflow for poet2: optimize for mobile tutorial I didn't created a folder named libs and copied files in this order:\r\n\r\nlibs\r\n-libtensorflow_demo.so\r\n-libtensorflow_inference.so\r\n-libandroid_tensorflow_inference_java.jar\r\n\r\nbecause it was not required at all. As per the direction given in the tutorial I only made changes in ClassifierActivity.java file,but due to deprecation warning and tensorflow version problem I made changes in build.gradle script one of the script given in Gradle Scripts.\r\nI hope this explanation might help you in resolving your issue.\r\nGodspeed :)\r\n\r\n", "@ElephantHunters \r\nThanks for your response. I confirm that the project is working after I followed your solution and the error I posted above relating to `INPUT_SIZE` which by accidentally I changed to `299` instead of `224`.", "`build.gradle`\r\n\r\ni change \r\n\r\n`compile 'org.tensorflow:tensorflow-android:1.4.0'`\r\n\r\nto \r\n\r\n`compile 'org.tensorflow:tensorflow-android:1.6.0'`\r\n\r\nFixed.", "I have used TFilte model in my Android Project,\r\nSuccessfully I have done all steps:\r\n(1. Load my file from the asset,\r\n 2. Converting my images to ByteBuffer, \r\n3.feed my ByteBuffer into the interpreter.run() method)\r\nBut the Main Issue is I am getting same output values for any images, How to solve it\r\n\r\nI know the main thing is This is an Issue or not? If its issue its come from Android or Tflite Model\r\nPlease give idea how to solve the issue."]}, {"number": 15697, "title": "Very slow tf.transpose on CPU (compared to numpy)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Tested on Linux Ubuntu 16.04 and Mac OS\r\n- **TensorFlow installed from (source or binary)**: Both affected\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**:  3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nTensorflow transpose is 10000 slower than numpy transpose on my example.\r\n\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\na = np.random.randn(*(10, 10, 10, 100, 10, 10, 10))\r\n%timeit np.transpose(a, [3, 0, 1, 2, 4, 5, 6])\r\n# 744 ns on my machine\r\nb = tf.Variable(tf.random_normal((10, 10, 10, 100, 10, 10, 10))) # variable to avoid generating random numbers while measuring time\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nop = tf.transpose(b, [3, 0, 1, 2, 4, 5, 6]).op\r\n%timeit sess.run(op)\r\n# 7.94 s\r\n```", "comments": ["Numpy and TensorFlow use different memory models, so whereas transpose in NumPy does not require copying an array's data, a copy is always required by TensorFlow. So this behavior is entirely expected.", "Consider use static graph instead. \r\nb = tf.Variable(tf.random_normal((10, 10, 10, 100, 10, 10, 10))) # variable to avoid generating random numbers while measuring time\r\nop = tf.transpose(b, [3, 0, 1, 2, 4, 5, 6]).op\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n%timeit sess.run(op)", "It still very slow with the static graph approach above as well. Also, copying memory doesn't explain the operation being _that_ slower (10000x!): e.g. tf.tile(b, [1, 2, 1, 1, 1, 1, 1]).op (which should be much slower if memory copying is the bottleneck) takes 261 ms while transpose takes 7.9 s.\r\n\r\nMy friend @mfigurnov suggests that the reason for this behavior may be here: https://github.com/tensorflow/tensorflow/blob/f284c708a8f3e2672d41dd3e7f6f03b9d26f0c80/tensorflow/core/kernels/transpose_functor_cpu.cc#L70\r\n\r\nFor <= 5 dimension tf uses efficient transpose, and in other cases, it uses some baseline code."]}, {"number": 15696, "title": "A fix for error in tf.layers.conv3d_transpose when inferred batch size", "body": "## Context\r\nWhen using the `tf.layers.conv3d_tranpose` Op with a dynamic batch size and when `use_bias=True` then there is a [well known error that occurs](https://github.com/tensorflow/tensorflow/issues/10520).\r\nThe error is due to a [tf.reshape Op that chunks together some of the axes before adding the bias for a slight performance improvement](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L1628).\r\nE.g. in the case of `data_format == 'channels_last'`, \r\n```\r\noutputs_4d = array_ops.reshape(outputs, [\r\n            outputs_shape[0], outputs_shape[1],\r\n            outputs_shape[2] * outputs_shape[3], outputs_shape[4]\r\n        ])\r\n```\r\ngives an error when `outputs_shape[0] == None`, i.e. when batch size is inferred. \r\n\r\n## Simple fix\r\nTo fix this with minimal modification to other code, it would be great if someone could replace `outputs_shape[0]` with `-1` in these two lines:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L1627\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L1632", "comments": ["Similar with #15655 \uff1f", "Yes precisely the same issue. ", "@fchollet I see that you're assigned to #10520, are you able to look at this?", "Is there an open PR for this? If not, please create one.", "@fchollet It seems that conv3d_transpose gets in trouble with unknown dimension,  see #15655,  #15572 as well. Perhaps we'd better find a general solution to fix all those problems.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "This has been fixed by #16307, thus this issue can be closed."]}, {"number": 15695, "title": "LookupError: No gradient defined for operation for assign", "body": "I wish to do assign ops in a layer such as below \r\n           **with tf.name_scope('conv1_1') as scope:\r\n                kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\r\n                                                         stddev=1e-1), name='weights')\r\n                kernel = kernel[:, :, : 0:32].assign(tf.zeros([3, 3, 3, 32]))\r\n                conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\r\n                biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\r\n                                     trainable=True, name='biases')\r\n                out = tf.nn.bias_add(conv, biases)\r\n                self.conv1_1 = tf.nn.relu(out, name=scope)\r\n                self.parameters += [kernel, biases]**\r\n\r\nBut later I train again. I will have error.\r\n**LookupError: No gradient defined for operation for assign**", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 139 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 15693, "title": "Pruning some features", "body": "I'm doing some work about pruning VGG16. And my method is set some kernel to be zero.\r\n               **kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\r\n                                                         stddev=1e-1), name='weights')\r\n                kernel1 = tf.Variable(tf.zeros([3, 3, 3, 64], dtype=tf.float32), name='pruning_weights',\r\n                                      trainable=False)\r\n                kernel1 = kernel1[:, :, :, 0:32].assign(kernel[:, :, :, 0:32])**\r\n\r\nBut I want to train VGG again. the problem said that, \r\n**LookupError: No gradient defined for operation 'conv1/conv1_1/conv1/conv1_1/strided_slice/_assign' (op type: StridedSliceAssign)**\r\n\r\nAnybody came across such issues? Or have some idea to set the kernel last dim to be zero?\r\nThanks a lot! ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15692, "title": "tf.image.draw_bounding_boxes don't support boxes with different color", "body": "I want to draw boxes with different color for different classes, but it seems that  `tf.image.draw_bounding_boxes` don't support, could you add color paramters to `tf.image.draw_bounding_boxes`?\r\n", "comments": ["The operation [cycles between colors](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/core/kernels/draw_bounding_box_op.cc#L68) when drawing multiple boxes.\r\n\r\nThat said, the set of colors isn't parameterized yet. \r\nContributions for changing this are welcome.\r\n\r\nFYI @shlens ", "Added a PR #15778 for the fix. Please take a look."]}, {"number": 15691, "title": "The problem of the tensorboard", "body": "Hi:\r\n  I am the newbie of the tensorflow,  there is a problem during learning the usage of  tensorboard. \r\n  There is no problem with the first code compilation after start the Compiler(Anaconda spyder), but recompiling the code will go wrong, that is very strange. can anyone help me?\r\n\r\nthank you very much!\r\n\r\n**First compilation:**\r\n![1](https://user-images.githubusercontent.com/31270354/34410598-60c20af0-ec0c-11e7-9acc-b3a5757b2101.PNG)\r\n![2](https://user-images.githubusercontent.com/31270354/34410606-6776095a-ec0c-11e7-8dcb-6607336bd7d2.PNG)\r\n![3](https://user-images.githubusercontent.com/31270354/34410611-6cab0768-ec0c-11e7-8f39-38a7cee2087f.PNG)\r\neverything is ok during first compilation\r\n\r\n**Second compilation:**\r\n![4](https://user-images.githubusercontent.com/31270354/34410636-8f812b64-ec0c-11e7-91c9-22ad651245d9.PNG)\r\nSomething was wrong!!!\r\n\r\n**the code of the tensorflow:**\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.placeholder(tf.int32)\r\ny = x + 2\r\n                   \r\nsess = tf.Session()\r\n\r\ntf.summary.scalar('Accuracy' , y)\r\nmerged = tf.summary.merge_all()\r\nwriter = tf.summary.FileWriter(\"logs/\", sess.graph)\r\n\r\nfor i in range(200):     \r\n    rs = sess.run(merged , feed_dict = {x: 10})\r\n    writer.add_summary(rs, i)\r\n```\r\n**Error report:**\r\n```\r\nrunfile('C:/Users/Administrator/.spyder-py3/temp.py', wdir='C:/Users/Administrator/.spyder-py3')\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-2-66eb2d566452>\", line 1, in <module>\r\n    runfile('C:/Users/Administrator/.spyder-py3/temp.py', wdir='C:/Users/Administrator/.spyder-py3')\r\n\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 866, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 102, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/Administrator/.spyder-py3/temp.py\", line 13, in <module>\r\n    rs = sess.run(merged , feed_dict = {x: 10})\r\n\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n\r\nInvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype int32\r\n\t [[Node: Placeholder = Placeholder[dtype=DT_INT32, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op 'Placeholder', defined at:\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\spyder\\utils\\ipython\\start_kernel.py\", line 223, in <module>\r\n    main()\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\spyder\\utils\\ipython\\start_kernel.py\", line 219, in main\r\n    kernel.start()\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 162, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\r\n    handler_func(fd_obj, events)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2827, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-66eb2d566452>\", line 1, in <module>\r\n    runfile('C:/Users/Administrator/.spyder-py3/temp.py', wdir='C:/Users/Administrator/.spyder-py3')\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 866, in runfile\r\n    execfile(filename, namespace)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 102, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n  File \"C:/Users/Administrator/.spyder-py3/temp.py\", line 3, in <module>\r\n    x = tf.placeholder(tf.int32)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1548, in placeholder\r\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2094, in _placeholder\r\n    name=name)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype int32\r\n\t [[Node: Placeholder = Placeholder[dtype=DT_INT32, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]    \r\n```\r\n\r\n\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "OK , thanks\uff01", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied."]}, {"number": 15690, "title": "mkl_cpu_allocator.h is not compiled under windows anymoe", "body": "git branch v1.5/master\r\n\r\n[mkl_cpu_allocator.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/mkl_cpu_allocator.h)\r\n\r\nis not compiled on windows anymore since in includes unistd.h, which dosn't exist in windows.\r\n", "comments": ["@Ariel-Lieberman Intel MKL optimization is only available on Linux for now, how did you get it to work on Windows in the past?", "Using the Cmake compilation I've managed (with no modifications to the source files themselves i.e. .cc, .h) to build the previous version (i.e. v1.3.0).  Now, MKL have added references to uint and other (like above, with no real reason) that breaks the compilation.  :-)", "The dependency on `unistd.h` was introduced in #13674 by @jbobba , perhaps @jbobba has some thoughts on making this work for Windows.\r\n\r\nFYI @rmlarsen \r\n\r\n@gunan may also want to comment on our ability to support Windows+MKL builds. \r\n\r\nThanks!", "We have nothing official on MKL build yet.\r\nEven the build only works for only linux out of the box, so no guarantees for anything else.\r\n\r\nHowever, contributions are certainly welcome.", "@Ariel-Lieberman unistd is used to get information about available system memory (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/mkl_cpu_allocator.h#L71). You can work around it by replacing with equivalent functionality on windows. https://stackoverflow.com/questions/2513505/how-to-get-available-memory-c-g provides one possible option. ", "@Ariel-Lieberman Could you please let us know if this issue still persists ? If it is resolved then please feel free to move this issue to close status ? check https://github.com/tensorflow/tensorflow/issues/15690#issuecomment-354615226, [**`link`**](https://github.com/tensorflow/tensorflow/issues/26001) Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/15690\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/15690\">No</a>\n", "fixed here - https://github.com/tensorflow/tensorflow/commit/dce9a49c19f406ba45919e8c94474e55dc5ccd54\r\n"]}, {"number": 15689, "title": "TensorFlowInferenceInterface: readNodeFloat error", "body": "This is part of my Tensorflow frozen graph, I have named the input and output nodes.\r\n\r\n    >>> g.ParseFromString(open('frozen_graph.pb','rb').read())\r\n    >>> g\r\n    node {\r\n      name: \"input\"\r\n      op: \"Placeholder\"\r\n      attr {\r\n        key: \"dtype\"\r\n        value {\r\n          type: DT_FLOAT\r\n        }\r\n      }\r\n      attr {\r\n        key: \"shape\"\r\n        value {\r\n          shape {\r\n            dim {\r\n              size: -1\r\n            }\r\n            dim {\r\n              size: 68\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    ...\r\n    node {\r\n      name: \"output\"\r\n      op: \"Softmax\"\r\n      input: \"add\"\r\n      attr {\r\n        key: \"T\"\r\n        value {\r\n          type: DT_FLOAT\r\n        }\r\n      }\r\n    }\r\n\r\nI ran this model by the following code\r\n(CELL is name of directory where my file is located)\r\n\r\n    final String MODEL_FILE = \"file:///android_asset/\" + CELL + \"/optimized_graph.pb\" ;\r\n    final String INPUT_NODE = \"input\" ;\r\n    final String OUTPUT_NODE = \"output\" ;\r\n    final int[] INPUT_SIZE = {1,68} ;\r\n    float[] RESULT = new float[8];\r\n    \r\n    inferenceInterface = new TensorFlowInferenceInterface();\r\n    inferenceInterface.initializeTensorFlow(getAssets(),MODEL_FILE) ;\r\n    inferenceInterface.fillNodeFloat(INPUT_NODE,INPUT_SIZE,input);\r\n\r\nand finally\r\n\r\n    inferenceInterface.readNodeFloat(OUTPUT_NODE,RESULT);\r\n\r\n\r\nBut I get this error in Log\r\n\r\n    12-28 16:42:48.622 9890-12178/com.getfocus.signalsimilarity I/native: tensorflow_inference_jni.cc:151 Initialization done in 52.275ms\r\n    12-28 16:42:51.048 9890-12178/com.getfocus.signalsimilarity E/native: tensorflow_inference_jni.cc:170 Output [output] not found, aborting!\r\n\r\n\r\nI have searched a lot for the solution but nothing seems to solve this. Thanks in advance", "comments": ["Please pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks.\r\n\r\nIn particular, it would be helpful to know more about the version of TensorFlow you're using. It seems the Android code is from an older version of TensorFlow (1.0 or prior? since `tensorflow_inference_jni.cc` was removed by version 1.1).\r\n\r\nAnyway, from the error message it seems that perhaps the model file being used by your Android application (`optimized_graph.pb`) is different from the model you described earlier (`frozen_graph.pb`). It is possible that the graph in the Android asset does not have any node named `output`?", "Thanks for your response, I have solved it, the problem was in inference interface.run() which I had did not execute in an orderly manner.\r\nUnfortunately, there was no bug this time"]}, {"number": 15688, "title": "Reduced accuracy with retrained Inception v3 model on Android ", "body": "I have follow instructions on TensorFlow website and the source code from examples on Github to retrain my own image classifier model which is basing on Inception v3.\r\n\r\nThe result is, for same picture, if I use python script for prediction I got the right category with confidence 93.3%. But I use Android Inference interface can only get the right category with 81.3% confidence.\r\n\r\nI think the problem comes from the way that how to use the model.\r\n\r\nIn Github code, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/ClassifierActivity.java\r\n\r\nHere is a snippet of comments to indicate how to user Inception v3 model:\r\n```java\r\n  // These are the settings for the original v1 Inception model. If you want to\r\n  // use a model that's been produced from the TensorFlow for Poets codelab,\r\n  // you'll need to set IMAGE_SIZE = 299, IMAGE_MEAN = 128, IMAGE_STD = 128,\r\n  // INPUT_NAME = \"Mul\", and OUTPUT_NAME = \"final_result\".\r\n  // You'll also need to update the MODEL_FILE and LABEL_FILE paths to point to\r\n  // the ones you produced.\r\n  //\r\n  // To use v3 Inception model, strip the DecodeJpeg Op from your retrained\r\n  // model first:\r\n  //\r\n  // python strip_unused.py \\\r\n  // --input_graph=<retrained-pb-file> \\\r\n  // --output_graph=<your-stripped-pb-file> \\\r\n  // --input_node_names=\"Mul\" \\\r\n  // --output_node_names=\"final_result\" \\\r\n  // --input_binary=true\r\n```\r\nWe start from Input named \"Mul\" because DecodeJpeg is NOT supported in Android. So, we need to decode bitmap and resize it to 299 x 299 and flatten it with Android way.  I think that is the difference between python script and Android Inference interface. In python script, we use tf.gFile to get the content of image and direct pass to the start node \"DecodeJpeg\"\r\n\r\nI review the graph of retrained model, node \"Mul\" is not the direct successor of DecodeJpeg. There are four nodes \"Cast\", \"ExpandDims\", \"ResizeBilinear\", \"Sub\" between \"Mul\" and \"DecodeJpeg\". I think it does the same thing I mentioned to preprocess the image. I think may be we could pass input data a little bit earlier than \"Mul\".\r\n\r\nFirst, I strip the mode with follow command:\r\n```shell\r\nstrip_unused \\\r\n--input_graph=tf_files/retrained_graph.pb \\\r\n--output_graph=tf_files/stripped_retrained_graph..pb \\\r\n--input_node_names=\"Cast\" \\\r\n--output_node_names=\"final_result\" \\\r\n--input_binary=true\r\n```\r\nThen I change the recognizeImage() to pass input to node Cast\r\n```Java\r\n  @Override\r\n  public List<Recognition> recognizeImage(final Bitmap bitmap) {\r\n    // Log this method so that it can be analyzed with systrace.\r\n    Trace.beginSection(\"recognizeImage\");\r\n\r\n    Trace.beginSection(\"preprocessBitmap\");\r\n    // Preprocess the image data from 0-255 int to normalized float based\r\n    // on the provided parameters.\r\n    int[] origIntValues = new int[bitmap.getWidth() * bitmap.getHeight()];\r\n    float[] flatValues = new float[bitmap.getWidth() * bitmap.getHeight() * 3];\r\n    bitmap.getPixels(origIntValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n    for (int i = 0; i < origIntValues.length; ++i) {\r\n        final int val = origIntValues[i];\r\n        flatValues[i * 3 + 0] = ((val >> 16) & 0xFF);\r\n        flatValues[i * 3 + 1] = ((val >> 8) & 0xFF) ;\r\n        flatValues[i * 3 + 2] = (val & 0xFF);\r\n     }\r\n    Trace.endSection();\r\n\r\n    // Copy the input data into TensorFlow.\r\n    Trace.beginSection(\"feed\");\r\n    inferenceInterface.feed(inputName, flatValues, new long[] { bitmap.getHeight(), bitmap.getWidth() , 3  });\r\n    Trace.endSection();\r\n```\r\nHere the inputNames is \"Cast\", not \"Mul\". After that I get the exact the same result as python script. \r\n\r\nConclusion, I think the way currently we used on Android side to preprocess image is NOT doing the same tasks as the nodes \"Cast\", \"ExpandDims\", \"ResizeBilinear\", \"Sub\" do. I suggest to update the code of Android example to fix this problem.\r\n", "comments": ["@asimshankar do you maintain the android examples or know who does?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Very disappointed with your answer. This issue has been reported for months but no one indeed to investigate why. \r\nFrom my deep investigation and understanding, I feel that is absolutely a bug in your Demo.\r\nThe Op ResizeBilinear in Tensorflow is squeeze the image and what you have done in Android Demo is not. Why there is no problem in your demo? You capture images with size is nearly a square, which the width and height are close to each other. So that the two algorithm will not affect the result much.\r\nBut, in my test, I use images with rectangle size, which width is much large than the height. So these two different algorithm of resizing image will bring 10% inaccuracy in the result.\r\n\r\n"]}, {"number": 15687, "title": "How to load a metagraph via C++", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2\r\n- **Python version**: 2.7\r\n\r\n### Describe the problem\r\nI want to load a metagraph via C++ code, and later the checkpoint weights. To load the metagraph, I first generate a pb file from it\r\n\r\n```\r\nwith tf.Session() as sess:\r\n\tnew_saver = tf.train.import_meta_graph(root_dir + meta_graph)\r\n\ttf.train.write_graph(sess.graph_def, root_dir, export_pb, as_text=False)\r\n```\r\nThen I use selective registration to generate the ops and kernels needed\r\n\r\n`bazel-bin/tensorflow/python/tools/print_selective_registration_header --graphs=xinmei/rnn_dict/model_test.pb > tensorflow/core/framework/ops_to_register.h`\r\nNext, I compile my runnable using this registration. However, when I run the executable on my Android device, it says\r\n\r\n```\r\nError creating graph: Invalid argument: No OpKernel was registered to support Op 'Const' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: save/RestoreV2_8/shape_and_slices = Const[_output_shapes=[[1]], dtype=DT_STRING, value=Tensor<type: string shape: [1] values: >]()]]\r\n```\r\nHow can I load the metagraph? The motivation of this is that I want to continue training the model on my Android device.\r\n", "comments": ["metagraph contains some python specific objects that can't be loaded with c++ currently.\r\n@mrry, anything to add or correct?\r\n", "While there is no specific API for loading a `MetaGraphDef` in C++, you can either package it as a SavedModel and [load it in C++](https://www.tensorflow.org/programmers_guide/saved_model#loading_a_savedmodel_in_c) or extract the [`GraphDef`](https://github.com/tensorflow/tensorflow/blob/271917a124a5b86c9ff6444415d20402a5db4ac0/tensorflow/core/protobuf/meta_graph.proto#L68) from inside the `MetaGraphDef` and load that directly.", "@bignamehyp @mrry Thanks for your replies. I actually tried the second way you suggested: just loading the pb file rather than the meta file, but it says\r\n\r\n`[libprotobuf ERROR external/protobuf/src/google/protobuf/wire_format_lite.cc:621] String field 'tensorflow.MetaGraphDef.MetaInfoDef.tensorflow_version' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes. Error reading graph definition from model_test.pb: Data loss: Can't parse model_test.pb as binary proto`\r\n\r\nWhy comes this error?\r\n\r\nBtw, I assume it is feasible that I load the graph & weights for continuing training on a Android device?", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@xumengwei the common cause is either you have the wrong proto, or you're trying to load a binary as text.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 15686, "title": "fix description of HASHTABLE_LOOKUP in smartreply doc", "body": "HASHTABLE_LOOKUP is a builtin op rather than a custom one.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 15685, "title": "fake_quant_with_min_max_vars doesn't change min, max vars ", "body": "# System Information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip3 install --upgrade tensorflow-gpu\r\n- TensorFlow version (use command below): v1.4.0-19-ga52c8d9, 1.4.1\r\n- Python version:3.5.2 \r\n- Bazel version (if compiling from source): 0.7.0\r\n- GCC/Compiler version (if compiling from source): GCC 5.4.0\r\n- CUDA/cuDNN version: Cuda compilation tools, release 8.0, V8.0.61, cuDNN : 6.0.21\r\n- GPU model and memory: Two GeForce GTX 1080 Ti devices.\r\n- Exact command to reproduce: N/A\r\n# Problem description\r\nI've got a problem with tf-lite conversion tool. There is learned graph which should be converted in tflite format and quantinized.\r\nI have read the answer https://stackoverflow.com/questions/47463204/tensorflow-lite-convert-error-for-the-quantized-graphdef where authors recommend to create new network with fake_quant operations and retrain it. However variables passed in tf.fake_quant_with_min_max_vars op did not change their values during training. Here is modeling code which shows the problem.\r\n# Code to reproduce\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nkhe_init = tf.random_normal_initializer(mean=0.0, stddev=np.sqrt(2.0 / 1000))\r\n\r\n\r\ndef quant_conv_op(inpt, num_filters, filter_size=[3, 3], strides=[1, 1, 1, 1], padding=\"VALID\", layer_name=\"layer1\", use_acivation=True):\r\n    num_input_map = inpt.get_shape().as_list()[-1]\r\n    kernel_shape = filter_size + [num_input_map, num_filters]    \r\n    with tf.variable_scope(layer_name):\r\n        W = tf.get_variable(\"weights\", shape=kernel_shape, initializer=khe_init)\r\n        max_w = tf.get_variable(\"max_quant_weights\", shape=[], initializer=tf.constant_initializer(1), trainable=True)\r\n        min_w = tf.get_variable(\"min_quant_weights\", shape=[], initializer=tf.constant_initializer(-1), trainable=True)\r\n        b = tf.get_variable(\"bias\", shape=[num_filters, ], initializer=tf.zeros_initializer)\r\n \r\n        q_W = tf.fake_quant_with_min_max_vars(W, min_w, max_w)\r\n        out = tf.nn.conv2d(inpt, q_W, strides=strides, padding=padding, name=\"2d_convolution_operation\")\r\n        out = tf.nn.bias_add(out, b)\r\n        \r\n        if use_acivation: \r\n            out = tf.nn.relu6(out, name=layer_name+\"out\")\r\n            out = tf.fake_quant_with_min_max_args(out, 0, 6)\r\n        else:\r\n            max_out = tf.get_variable(\"max_quant_output\", shape=[], initializer=tf.constant_initializer(1), trainable=True)\r\n            min_out = tf.get_variable(\"min_quant_output\", shape=[], initializer=tf.constant_initializer(-1), trainable=True)  \r\n            out = tf.fake_quant_with_min_max_vars(out, min_out, max_out, name=\"fake_quant_with_min_max_out_quantinization\")\r\n    \r\n    return out, max_w, min_w, W\r\n\r\n\r\ndef loss(logits, batch):\r\n    return tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.ones(shape=[batch, 1]))\r\n\r\n\r\ndef build_net(ph_input):\r\n    inp = tf.fake_quant_with_min_max_args(ph_input, -1, 1)\r\n    out, max_w1, min_w1, W1 = quant_conv_op(inp, num_filters=64, filter_size=[3, 3], layer_name=\"layer1\")    \r\n    out, max_w2, min_w2, W2 = quant_conv_op(out, num_filters=128, filter_size=[3, 3], layer_name=\"layer2\")\r\n    out, max_w3, min_w3, W3 = quant_conv_op(out, num_filters=256, filter_size=[3, 3], layer_name=\"layer3\")\r\n    out = tf.reduce_mean(out, axis=[1, 2], keep_dims=True, name=\"avg_pool\")\r\n    out = tf.fake_quant_with_min_max_args(out, 0, 6, name=\"fake_quant_with_min_max_avgpool_quantinization\")\r\n    logits, max_w4, min_w4, W4 = quant_conv_op(out, num_filters=1, filter_size=[1, 1], layer_name=\"layer4\", use_acivation=False)\r\n    \r\n    max_list = [max_w1, max_w2, max_w3, max_w4]\r\n    min_list = [min_w1, min_w2, min_w3, min_w4]\r\n    W_list = [W1, W2, W3, W4]\r\n    logits = tf.reshape(logits, [-1, 1])\r\n    sig_loss = tf.reduce_mean(loss(logits, batch=64))\r\n    adam_op = tf.train.AdamOptimizer(10**-0)\r\n    train_op = adam_op.minimize(sig_loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)) \r\n    return train_op, max_list, min_list, sig_loss, W_list\r\n\r\n\r\ndef main():\r\n    sess = tf.InteractiveSession()\r\n    ph_input = tf.placeholder(tf.float32, [None, 28, 28, 3], name=\"network_input\") \r\n    train_op, max_list, min_list, sig_loss, W_list = build_net(ph_input)\r\n    sess.run(tf.global_variables_initializer())\r\n    tf.summary.FileWriter('.', graph=tf.get_default_graph())\r\n    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\r\n    for t in trainable_vars:\r\n        if len(t.shape) == 0:\r\n           print(t.name, sess.run(t))\r\n        if len(t.shape) == 4:\r\n           v = sess.run(t)\r\n           print(t.name, np.max(v), np.min(v))\r\n\r\n    for i in range(10):\r\n        res = sess.run([train_op, sig_loss, W_list[0]]+max_list, feed_dict={ph_input:np.random.uniform(low=-1, high=1, size=[64, 28, 28, 3])})\r\n    \r\n    print(\"=========\")\r\n    for t in trainable_vars:\r\n        if len(t.shape) == 0:\r\n           print(t.name, sess.run(t))\r\n        if len(t.shape) == 4:\r\n           v = sess.run(t)\r\n           print(t.name, np.max(v), np.min(v))\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n  \r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hello\uff0c I meet similar problems. Have you solved this problem?"]}, {"number": 15684, "title": "speech commands check_nans doesnt work", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I'm running the simple audio recognition tutorial code.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:tf-nightly-gpu15\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:CUDA8.0/cuDNNv7\r\n- **GPU model and memory**: 1060 6GB\r\n- **Exact command to reproduce**:python train.py --check-nans=True\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nAs the tutorial said, by enabling the --check_nans flag I could track down the source of the not-a-number error in model tuning. But I got following errors instead, which I guess maybe a bug or something?\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nTraceback (most recent call last):\r\n  File \"/home/renq/.conda/envs/py35_tfnightly_gpu15/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/home/renq/.conda/envs/py35_tfnightly_gpu15/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/renq/.conda/envs/py35_tfnightly_gpu15/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder_1' with dtype float\r\n         [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n         [[Node: global_step/read/_18 = _Send[T=DT_INT64, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_4_global_step/read\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](global_step/read)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 432, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/renq/.conda/envs/py35_tfnightly_gpu15/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 124, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 217, in main\r\n    dropout_prob: 0.5\r\n  File \"/home/renq/.conda/envs/py35_tfnightly_gpu15/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/renq/.conda/envs/py35_tfnightly_gpu15/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/renq/.conda/envs/py35_tfnightly_gpu15/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/renq/.conda/envs/py35_tfnightly_gpu15/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder_1' with dtype float\r\n         [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n         [[Node: global_step/read/_18 = _Send[T=DT_INT64, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_4_global_step/read\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](global_step/read)]]\r\n\r\nCaused by op 'Placeholder_1', defined at:\r\n  File \"train.py\", line 432, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/renq/.conda/envs/py35_tfnightly_gpu15/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 124, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 106, in main\r\n    FLAGS.testing_percentage, model_settings)\r\n  File \"/home/renq/tensorflow/tensorflow/examples/speech_commands/input_data.py\", line 163, in __init__\r\n    self.prepare_processing_graph(model_settings)\r\n  File \"/home/renq/tensorflow/tensorflow/examples/speech_commands/input_data.py\", line 355, in prepare_processing_graph\r\n    self.foreground_volume_placeholder_ = tf.placeholder(tf.float32, [])\r\n  File \"/home/renq/.conda/envs/py35_tfnightly_gpu15/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1680, in placeholder\r\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"/home/renq/.conda/envs/py35_tfnightly_gpu15/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3141, in _placeholder\r\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\r\n  File \"/home/renq/.conda/envs/py35_tfnightly_gpu15/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/renq/.conda/envs/py35_tfnightly_gpu15/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3143, in create_op\r\n    op_def=op_def)\r\n  File \"/home/renq/.conda/envs/py35_tfnightly_gpu15/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1611, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_1' with dtype float\r\n         [[Node: Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n         [[Node: global_step/read/_18 = _Send[T=DT_INT64, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_4_global_step/read\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](global_step/read)]]\r\n\r\n", "comments": ["Can you post the exact command that produces this error?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 15683, "title": "R1.4", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "R1.4 has already been merged back into master."]}, {"number": 15682, "title": "Eager: tfe.implicit_value_and_gradients uses functions operating on raw tf variables", "body": "## System information\r\n- Tensorflow version: 1.5.0-dev20171126\r\n- Python version: Python 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015, 11:00:19)\r\n\r\n## Problem\r\nForgive me if I'm re-iterating something that's discussed before. Even though I don't think the issue described here is a bug, I nevertheless believe it is worthy to point out. The specific issue is that when we pass a loss function, e.g. ```loss```, to ```tfe.implicit_value_and_gradients```, it seems that backprop only happens if the variables used by ```loss``` are is their \"raw states\". Here's an example:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\nv = tf.get_variable(name='v', initializer=1., trainable=True)\r\nv_add_1 = v + 1.  # this causes the problem\r\n\r\ndef loss():\r\n    return 2. * v_add_1\r\nvalue_and_gradients_fn = tfe.implicit_value_and_gradients(loss)\r\nprint (value_and_gradients_fn())\r\n```\r\nIn this case I get the error as follows:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 17, in <module>\r\n    val = g()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\", line 360, in grad_fn\r\n    raise ValueError(\"No trainable variables were accessed while the \"\r\nValueError: No trainable variables were accessed while the function was being computed.\r\n```\r\nAfter a little bit of pondering, I found the problem to be the line ```v = v + 1.```. As soon as we **delete** this line, the program runs without bugs. My understanding of this behavior is that, the gradients and the backprop process somehow only \"live\" in the scope of the loss function. We **cannot** backprop to some variable that is modified outside of ```loss```, even if, implicitly, the computed loss depends on that variable. \r\n\r\nHere's a more obscure example:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\nv = tf.get_variable(name='v', initializer=1., trainable=True)\r\nv_add_1 = v + 1.\r\nu = tf.get_variable(name='u', initializer=20., trainable=True)\r\n\r\n\r\ndef loss():\r\n    result = v_add_1 + u\r\n    return 2. * result\r\n\r\nvalue_and_gradients_fn = tfe.implicit_value_and_gradients(loss)\r\noptimizer = tf.train.AdamOptimizer(1e-1)\r\n\r\n# before training\r\nprint (v)\r\nprint (u)\r\n\r\nfor i in range(100):\r\n    _, gradients_and_variables = value_and_gradients_fn()\r\n    optimizer.apply_gradients(gradients_and_variables)\r\n\r\n# after training\r\nprint (v)\r\nprint (u)\r\n```\r\nAfter running, we could see that ```u``` is updated and ```v``` is not:\r\n```\r\n<tf.Variable 'v:0' shape=() dtype=float32, numpy=1.0>\r\n<tf.Variable 'u:0' shape=() dtype=float32, numpy=20.0>\r\n<tf.Variable 'v:0' shape=() dtype=float32, numpy=1.0>\r\n<tf.Variable 'u:0' shape=() dtype=float32, numpy=9.9999638>\r\n```\r\n\r\nThis might be irrelevant, but is there a way we could by pass the restriction and have gradient pass outside the function given to ```tfe.implicit_value_and_gradients```?", "comments": ["eager need to record outputs to calculate grads, all forward operations must run in loss() but the line ```v_add_1 = v + 1.``` doesn't in record scope. you can try my [simple wrapper function](https://gist.github.com/traveller59/8849c5aa7b2db6ec705e9dc9b1fb3591) which offers a clear grad calculating.", "Right, as @traveller59 pointed out - `implicit_value_and_gradients` can compute gradients only for operations that are recorded, and it only records what happens inside the function. When you use `v_add_1`, the `loss()` function is operating on the `Tensor` `v_add_1`, which is effectively a constant value (1 + the value of the variable at the time v_add_1 was created).\r\n\r\nIf you'd like finer grained control over what is recorded, consider using `tfe.GradientTape` for something like:\r\n\r\n```python\r\nwith tfe.GradientTape() as g:\r\n  v = tf.get_variable(name='v', initializer=1., trainable=True)\r\n  v_add_1 = v + 1\r\n  loss = v_add_1 * 2\r\ndloss = g.gradient(loss, [u, v])\r\n```\r\nClosing this out since this is intended behavior of `implicit_value_and_gradients`.\r\n\r\nFYI @alextp "]}, {"number": 15681, "title": "tf.image.decode_image does not support png grayscale 16bit.", "body": "Code to reproduce:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint (\"TF Version: %s\" % tf.__version__)\r\n\r\nimport urllib\r\n\r\npng16 = urllib.request.urlopen('http://www.schaik.com/pngsuite/basn0g16.png').read(1000)\r\nwith tf.Session() as session:\r\n    content = tf.placeholder(tf.string)\r\n    tensor = tf.image.decode_image(\r\n        contents=content,\r\n        channels=1\r\n    )\r\n    \r\n    out = (session.run(tensor, {content: png16}))\r\n    np_out = np.array(out)\r\n    print ('Shape', np_out.shape)\r\n    print ('Max', np_out.max())\r\n    \r\n\r\n    \r\n```\r\nOutputs:\r\n``` \r\nTF Version: 1.4.1\r\nShape (32, 32, 1)\r\nMax 255\r\n```\r\nSame file, file command:\r\n```\r\nfile basn0g16.png\r\nbasn0g16.png: PNG image data, 32 x 32, 16-bit grayscale, non-interlaced\r\n```\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS\r\n- **TensorFlow installed from (source or binary)**: pip3 install tensorflow\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: nope\r\n- **GCC/Compiler version (if compiling from source)**: nope\r\n- **CUDA/cuDNN version**: no gpu\r\n- **GPU model and memory**: no gpu\r\n- **Exact command to reproduce**: see abve\r\n\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.4.0-19-ga52c8d9 1.4.1\r\n\r\n### Describe the problem\r\nIt should return uint16, but returns uint8.\r\n\r\n\r\n### Source code / logs\r\nsee above\r\n", "comments": ["Ok, I have that issue because slim does not support png16\r\ntensorflow.contrib.slim.python.slim.data.tfexample_decoder.Image", "Set dtype in decode_png\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/image/decode_png"]}]