[{"number": 5754, "title": "Disable evaluation_test from CI.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @ebrevdo and @nathansilberman to be potential reviewers."]}, {"number": 5753, "title": "Disable evaluation_test from CI.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @ebrevdo and @nathansilberman to be potential reviewers.", "Test failure unrelated. merging."]}, {"number": 5752, "title": "Tensorboard - Black box for graphs after upgrading to TF 0.11", "body": "\r\n![tensorboard_issue_0 11](https://cloud.githubusercontent.com/assets/1653419/20492832/46047212-afe4-11e6-8eb0-2938e86390ea.png)\r\n\r\nI am currently getting a black box where a graph should be when using Tensorboard. I was previously running TF 0.9 and upgraded last night to 0.11 via the pip link below. When I hover over the black box I do see the onHover messages displaying valid values, but I just can't see anything.\r\n\r\n--The error received is below and the same thing shown in the screenshot above:\r\n\r\n> All candidate resources failed to load. Media load paused.  127.0.1.1:6006\r\n> Unexpected value  parsing viewBox attribute. d3.js:662:6\r\n> NS_ERROR_FAILURE:  tf-tensorboard.html-28.js:199\r\n> \tLineChart.prototype.setupTooltips/< http://127.0.1.1:6006/dist/tf-tensorboard.html-28.js:199:34\r\n> \tCallbackSet</CallbackSet.prototype.callCallbacks/< http://127.0.1.1:6006/plottable/plottable.js:643:21\r\n> \tSet</Set.prototype.forEach/callbackWrapper http://127.0.1.1:6006/plottable/plottable.js:257:77\r\n> \tforEach self-hosted\r\n> \tSet</Set.prototype.forEach http://127.0.1.1:6006/plottable/plottable.js:258:21\r\n> \tCallbackSet</CallbackSet.prototype.callCallbacks http://127.0.1.1:6006/plottable/plottable.js:642:17\r\n> \tPointer</Pointer.prototype._handlePointerEvent http://127.0.1.1:6006/plottable/plottable.js:11190:21\r\n> \tPointer/this._mouseMoveCallback http://127.0.1.1:6006/plottable/plottable.js:11166:65\r\n> \tCallbackSet</CallbackSet.prototype.callCallbacks/< http://127.0.1.1:6006/plottable/plottable.js:643:21\r\n> \tSet</Set.prototype.forEach/callbackWrapper http://127.0.1.1:6006/plottable/plottable.js:257:77\r\n> \tforEach self-hosted\r\n> \tSet</Set.prototype.forEach http://127.0.1.1:6006/plottable/plottable.js:258:21\r\n> \tCallbackSet</CallbackSet.prototype.callCallbacks http://127.0.1.1:6006/plottable/plottable.js:642:17\r\n> \tMouse</Mouse.prototype._measureAndDispatch http://127.0.1.1:6006/plottable/plottable.js:10501:25\r\n> \tMouse/processMoveCallback http://127.0.1.1:6006/plottable/plottable.js:10363:65\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI have not found anyone having this as of yet when performing a search. I was previously running\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nCUDA 8 and Cudnn 5\r\noutput of above ls:\r\n\r\n> total 64\r\n> drwxr-xr-x  3 root root 4096 Nov 20 21:59 bin\r\n> drwxr-xr-x  5 root root 4096 Nov 20 21:59 doc\r\n> drwxr-xr-x  5 root root 4096 Nov 20 21:59 extras\r\n> drwxr-xr-x  5 root root 4096 Nov 20 22:19 include\r\n> drwxr-xr-x  5 root root 4096 Nov 20 21:59 jre\r\n> drwxr-xr-x  3 root root 4096 Nov 20 22:19 lib64\r\n> drwxr-xr-x  8 root root 4096 Nov 20 21:59 libnsight\r\n> drwxr-xr-x  7 root root 4096 Nov 20 21:59 libnvvp\r\n> drwxr-xr-x  3 root root 4096 Nov 20 21:59 nvml\r\n> drwxr-xr-x  7 root root 4096 Nov 20 21:59 nvvm\r\n> drwxr-xr-x  2 root root 4096 Nov 20 21:59 pkgconfig\r\n> drwxr-xr-x 11 root root 4096 Nov 20 21:59 samples\r\n> drwxr-xr-x  3 root root 4096 Nov 20 21:59 share\r\n> drwxr-xr-x  2 root root 4096 Nov 20 21:59 src\r\n> drwxr-xr-x  2 root root 4096 Nov 20 21:59 tools\r\n> -rw-r--r--  1 root root   20 Nov 20 21:59 version.txt\r\n\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nUbuntu/Linux 64-bit, GPU enabled, Python 2.7\r\nRequires CUDA toolkit 8.0 and CuDNN v5. For other versions, see \"Install from sources\" below.\r\nTF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.11\r\n", "comments": ["I would love to know why this is happening still in FF should anyone have any answers, but I just found that this is working fine in Google Chrome.  I am going to close this since I do have a solution.", "@xtr33me I've found the issue in Firefox. It will be fixed in the 0.12 release.", "@danmane Thats awesome news! Thanks for the info"]}, {"number": 5751, "title": "failed sess.run error \"Cannot feed value of shape (50, 2352) for Tensor 'Placeholder:0', which has shape '(?, 784)'\"", "body": "Hi \r\nPlease Help me...\r\nI learning to tensorflow using my own data based on tutorial expert.\r\nfollowing my code:\r\n```\r\n#datasets define\r\nNUM_CLASSES = 65535\r\nIMAGE_SIZE = 28\r\nIMAGE_PIXELS = IMAGE_SIZE*IMAGE_SIZE*1\r\n```\r\n\r\n```\r\n#read datasets\r\n    with open(FLAGS.train, 'r') as f: # train.txt\r\n        train_image = []\r\n        train_label = []\r\n        num = 0\r\n        for line in f:\r\n            if num == 500:\r\n                break\r\n            line = line.rstrip()\r\n            l = line.split(',')\r\n            print(l[0])\r\n            img = cv2.imread(l[0])\r\n            img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\r\n            train_image.append(img.flatten().astype(np.float32)/255.0)\r\n            tmp = np.zeros(NUM_CLASSES)\r\n            tmp[int(l[1])] = 1\r\n            train_label.append(tmp)\r\n            num += 1\r\n        train_image = np.asarray(train_image)\r\n        train_label = np.asarray(train_label)\r\n        train_len = len(train_image)\r\n```\r\n\r\n```\r\ndef inference(images_placeholder, keep_prob):\r\n    def weight_variable(shape):\r\n        initial = tf.truncated_normal(shape, stddev=0.1)\r\n        return tf.Variable(initial)\r\n    def bias_variable(shape):\r\n        initial = tf.constant(0.1, shape=shape)\r\n        return tf.Variable(initial)\r\n    def conv2d(x, W):\r\n        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\r\n    def max_pool_2x2(x):\r\n        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\r\n                              strides=[1, 2, 2, 1], padding='SAME')\r\n    x_images = tf.reshape(images_placeholder, [-1, IMAGE_SIZE, IMAGE_SIZE, 1])\r\n    with tf.name_scope('conv1') as scope:\r\n        W_conv1 = weight_variable([5, 5, 1, 32])\r\n        b_conv1 = bias_variable([32])\r\n        h_conv1 = tf.nn.relu(conv2d(x_images, W_conv1) + b_conv1)\r\n    with tf.name_scope('pool1') as scope:\r\n        h_pool1 = max_pool_2x2(h_conv1)\r\n    with tf.name_scope('conv2') as scope:\r\n        W_conv2 = weight_variable([5, 5, 32, 64])\r\n        b_conv2 = bias_variable([64])\r\n        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\r\n    with tf.name_scope('pool2') as scope:\r\n        h_pool2 = max_pool_2x2(h_conv2)\r\n    with tf.name_scope('fc1') as scope:\r\n        W_fc1 = weight_variable([7*7*64, 1024])\r\n        b_fc1 = bias_variable([1024])\r\n        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\r\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\r\n        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\r\n    with tf.name_scope('fc2') as scope:\r\n        W_fc2 = weight_variable([1024, NUM_CLASSES])\r\n        b_fc2 = bias_variable([NUM_CLASSES])\r\n    with tf.name_scope('softmax') as scope:\r\n        y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\r\n    return y_conv\r\n```\r\n\r\n```\r\n#learn\r\n    with tf.Graph().as_default():\r\n        images_placeholder = tf.placeholder(\"float\", shape=(None, IMAGE_PIXELS))\r\n        labels_placeholder = tf.placeholder(\"float\", shape=(None, NUM_CLASSES))\r\n        keep_prob = tf.placeholder(\"float\")\r\n        \r\n        logits = inference(images_placeholder, keep_prob)\r\n        loss_value = loss(logits, labels_placeholder)\r\n        train_op = training(loss_value, FLAGS.learning_rate)\r\n        print(\"train_op =\", train_op)\r\n\r\n        acc = accuracy(logits, labels_placeholder)\r\n        \r\n        saver = tf.train.Saver()\r\n        sess = tf.Session()\r\n        sess.run(tf.initialize_all_variables())\r\n        summary_op = tf.merge_all_summaries()\r\n        summary_writer = tf.train.SummaryWriter(FLAGS.train_dir, sess.graph_def)\r\n        \r\n        if train_len % FLAGS.batch_size is 0:\r\n            train_batch = train_len/FLAGS.batch_size\r\n        else:\r\n            train_batch = (train_len/FLAGS.batch_size)+1\r\n        print(\"train_batch = %d\",str(train_batch))\r\n        for step in range(FLAGS.max_steps):\r\n            for i in range(int(train_batch)):\r\n                batch = FLAGS.batch_size*i\r\n                batch_plus = FLAGS.batch_size*(i+1)\r\n                print(\"batch_plus =\", batch_plus)\r\n                if batch_plus > train_len: batch_plus = train_len\r\n                sess.run(train_op, feed_dict={\r\n                         images_placeholder: train_image[batch:batch_plus],\r\n                         labels_placeholder: train_label[batch:batch_plus],\r\n                         keep_prob: 0.5})\r\n            \r\n            if step % 10 == 0:\r\n                train_accuracy = 0.0\r\n                for i in range(train_batch):\r\n                    batch = FLAGS.batch_size*i\r\n                    batch_plus = FLAGS.batch_size*(i+1)\r\n                    if batch_plus > train_len: batch_plus = train_len\r\n                    train_accuracy += sess.run(acc, feed_dict={\r\n                                               images_placeholder: train_image[batch:batch_plus],\r\n                                               labels_placeholder: train_label[batch:batch_plus],\r\n                                               keep_prob: 1.0})\r\n                    if i is not 0: train_accuracy /= 2.0\r\n                #summary_str = sess.run(summary_op, feed_dict={\r\n                #    images_placeholder: train_image,\r\n                #    labels_placeholder: train_label,\r\n                #    keep_prob: 1.0})\r\n                #summary_writer.add_summary(summary_str, step)\r\n                print(\"step %d, training accuracy %g\",(step, train_accuracy))\r\n\r\n    if test_len % FLAGS.batch_size is 0:\r\n        test_batch = test_len/FLAGS.batch_size\r\n    else:\r\n        test_batch = (test_len/FLAGS.batch_size)+1\r\n        print(\"test_batch = \",str(test_batch))\r\n        test_accuracy = 0.0\r\n    for i in range(test_batch):\r\n        batch = FLAGS.batch_size*i\r\n        batch_plus = FLAGS.batch_size*(i+1)\r\n        if batch_plus > train_len: batch_plus = train_len\r\n        test_accuracy += sess.run(acc, feed_dict={\r\n                                  images_placeholder: test_image[batch:batch_plus],\r\n                                  labels_placeholder: test_label[batch:batch_plus],\r\n                                  keep_prob: 1.0})\r\n        if i is not 0: test_accuracy /= 2.0\r\n    print(\"test accuracy %g\",(test_accuracy))\r\n    save_path = saver.save(sess, FLAGS.save_model)\r\n```\r\nbut when I try to run it I gives me an error:\r\n`ValueError:Cannot feed value of shape (50, 2352) for Tensor 'Placeholder:0', which has shape '(?, 784)'`\r\n\r\nI feel like i'm overlooking something small but I don't see it.\r\n", "comments": ["This is a general usage question best suited to StackOverflow.  Could you please ask your question there."]}, {"number": 5750, "title": "Default optimizer initialization in the LinearRegressor fix", "body": "Default optimizer of the LinearRegressor should be set only if the users have not provided custom optimizer.", "comments": ["@b0noI, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @philstahlfeld and @ispirmustafa to be potential reviewers.", "Can one of the admins verify this patch?", "This is a cosmetic change, right? Is there a situation in which setting the default and then overwriting it is harmful?", "Jenkins, test this please.", "martinwicke@ this is a tiny performance improvement, ```_get_default_optimizer``` allocates memory and executes small calculation.  However the improvement should be so tiny so I guess changes can be called as pure cosmetic."]}, {"number": 5749, "title": "Update documentation for new API #5669", "body": "Small typo in documentation #5669\r\n", "comments": ["@hholst80, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @strategist333 and @sherrym to be potential reviewers.", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please."]}, {"number": 5748, "title": "Bazel build on Windows fails with \"Can't do inplace edit without backup.\" error.", "body": "\r\n\r\n### Environment info\r\nOperating System: Windows 10\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):  No\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)  - c7edafccc793bf87e29aaec90db64471a7a4bb02\r\n2. The output of `bazel version`: 0.4.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nBazel build on Windows fails with \"Can't do inplace edit without backup.\" error.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n$ /d/tf/tensorflow/tensorflow/tools/ci_build/windows/cpu/pip/build_tf_windows.sh\r\n+ set -e\r\n++ dirname /d/tf/tensorflow/tensorflow/tools/ci_build/windows/cpu/pip/build_tf_windows.sh\r\n+ script_dir=/d/tf/tensorflow/tensorflow/tools/ci_build/windows/cpu/pip\r\n+ cd /d/tf/tensorflow/\r\n+ export TMPDIR=C:/tmp\r\n+ TMPDIR=C:/tmp\r\n+ export BAZEL_SH=C:/tools/msys64/usr/bin/bash\r\n+ BAZEL_SH=C:/tools/msys64/usr/bin/bash\r\n+ export PYTHON_BIN_PATH=I:/Anaconda3/python\r\n+ PYTHON_BIN_PATH=I:/Anaconda3/python\r\n+ export BAZEL_PYTHON=I:/Anaconda3/python\r\n+ BAZEL_PYTHON=I:/Anaconda3/python\r\n+ export 'BAZEL_VS=E:/Program Files (x86)/Microsoft Visual Studio 14.0'\r\n+ BAZEL_VS='E:/Program Files (x86)/Microsoft Visual Studio 14.0'\r\n+ export 'PATH=/I/Anaconda3:/c/Perl64/site/bin:/c/Perl64/bin:/c/Python27:/c/Python27/Scripts:/c/WINDOWS/system32:/c/WINDOWS:/c/WINDOWS/System32/Wbem:/c/WINDOWS/System32/WindowsPowerShell/v1.0:/c/Program Files (x86)/Skype/Phone:/c/Program Files/Git/cmd:/c/Program Files (x86)/Windows Kits/8.1/Windows Performance Toolkit:/c/Program Files/Microsoft SQL Server/110/Tools/Binn:/c/Program Files (x86)/Microsoft SDKs/TypeScript/1.0:/c/Program Files/Microsoft SQL Server/120/Tools/Binn:/c/Users/pavel/.dnx/bin:/c/Program Files/Microsoft DNX/Dnvm:/c/Program Files/Microsoft SQL Server/130/Tools/Binn:/c/WINDOWS/SysWOW64/WindowsPowerShell/v1.0/Modules/TShell/TShell:/c/Program Files/Samsung/AllShare Framework DMS/1.3.23:/c/Program Files/Samsung/AllShare Framework DMS/1.3.23/64bit:/c/Program Files/TortoiseGit/bin:/c/ProgramData/chocolatey/bin:/c/Program Files/CMake/bin:/c/Program Files/Java/jdk1.8.0_112/bin:/usr/bin:/i/Anaconda3:/i/Anaconda3/Scripts:/i/Anaconda3/Library/bin:/c/Users/pavel/AppData/Local/Programs/Python/Python35-32/Scripts:/c/Users/pavel/AppData/Local/Programs/Python/Python35-32:/c/Users/pavel/AppData/Local/Microsoft/WindowsApps:/c/Program Files/CMake/bin:/c/ProgramData/chocolatey/lib/msys2:/:/usr/bin'\r\n+ PATH='/I/Anaconda3:/c/Perl64/site/bin:/c/Perl64/bin:/c/Python27:/c/Python27/Scripts:/c/WINDOWS/system32:/c/WINDOWS:/c/WINDOWS/System32/Wbem:/c/WINDOWS/System32/WindowsPowerShell/v1.0:/c/Program Files (x86)/Skype/Phone:/c/Program Files/Git/cmd:/c/Program Files (x86)/Windows Kits/8.1/Windows Performance Toolkit:/c/Program Files/Microsoft SQL Server/110/Tools/Binn:/c/Program Files (x86)/Microsoft SDKs/TypeScript/1.0:/c/Program Files/Microsoft SQL Server/120/Tools/Binn:/c/Users/pavel/.dnx/bin:/c/Program Files/Microsoft DNX/Dnvm:/c/Program Files/Microsoft SQL Server/130/Tools/Binn:/c/WINDOWS/SysWOW64/WindowsPowerShell/v1.0/Modules/TShell/TShell:/c/Program Files/Samsung/AllShare Framework DMS/1.3.23:/c/Program Files/Samsung/AllShare Framework DMS/1.3.23/64bit:/c/Program Files/TortoiseGit/bin:/c/ProgramData/chocolatey/bin:/c/Program Files/CMake/bin:/c/Program Files/Java/jdk1.8.0_112/bin:/usr/bin:/i/Anaconda3:/i/Anaconda3/Scripts:/i/Anaconda3/Library/bin:/c/Users/pavel/AppData/Local/Programs/Python/Python35-32/Scripts:/c/Users/pavel/AppData/Local/Programs/Python/Python35-32:/c/Users/pavel/AppData/Local/Microsoft/WindowsApps:/c/Program Files/CMake/bin:/c/ProgramData/chocolatey/lib/msys2:/:/usr/bin'\r\n+ export TF_NEED_CUDA=0\r\n+ TF_NEED_CUDA=0\r\n+ bazel clean\r\nERROR: CreateFile(C:\\tmp\\Bazel\\UUElXoeN\\install): The system cannot find the file specified.\r\n (2)\r\n..............\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n++ bazel info output_base\r\n+ output_base=C:/tmp/Bazel/UUElXoeN\r\n+ bazel shutdown\r\n+ sleep 5\r\n+ rm -rf C:/tmp/Bazel/UUElXoeN\r\n+ echo ''\r\n+ ./configure\r\n/d/tf/tensorflow /d/tf/tensorflow\r\nCan't do inplace edit without backup.\r\n", "comments": ["I had this issue when I was trying to use the perl I installed on windows. Once I 'pacman -Syu perl' on msys2, this issue was fixed for me.", "@maccam912 Thanks!  I never had this error before, @pkonovalov-softheme Can you try the suggested solution?", "@meteorcloudy  This problem was solved by running **pacman -Syu perl'** as  @maccam912  suggested AND after uninstalling Windows active Perl.\r\n\r\nNow build failes with:\r\nERROR: D:/tf/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/BUILD:159:1: no such target '//external:libssl': target 'libssl' not declared in package 'external' defined by D:/tf/tensorflow/WORKSPACE and referenced by '//tensorflow/contrib/cmake/build/grpc/src/grpc:grpc'.\r\nERROR: D:/tf/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/BUILD:159:1: no such target '//external:libssl': target 'libssl' not declared in package 'external' defined by D:/tf/tensorflow/WORKSPACE and referenced by '//tensorflow/contrib/cmake/build/grpc/src/grpc:grpc'.\r\nERROR: Evaluation of query \"deps((//tensorflow/... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\r\n\r\nLooks like that is an other issue.", "Looks like you tried to run CMake in the same repository? Can you rebuild from a clean TF repo?", "Yes, and \"git clean -fxd\" did not help. After I have cloned repos in other place- those errors have gon, but got new error:\r\nERROR: /DEFAULT.WORKSPACE:11:1: no such package '@local_jdk//': com.google.devtools.build.lib.skyframe.InconsistentFilesystemException: Inconsistent filesystem operations. C:/Program Files/Java/jdk1.8.0_112 is no longer an existing directory. Did you delete it during the build? The results of the build are not guaranteed to be correct. You should probably run 'blaze clean' and investigate the filesystem inconsistency (likely due to filesytem updates concurrent with the build) and referenced by '//external:bootclasspath'.\r\nERROR: /DEFAULT.WORKSPACE:59:1: no such package '@local_jdk//': com.google.devtools.build.lib.skyframe.InconsistentFilesystemException: Inconsistent filesystem operations. C:/Program Files/Java/jdk1.8.0_112 is no longer an existing directory. Did you delete it during the build? The results of the build are not guaranteed to be correct. You should probably run 'blaze clean' and investigate the filesystem inconsistency (likely due to filesytem updates concurrent with the build) and referenced by '//external:jdk'.\r\nERROR: Evaluation of query \"deps((//tensorflow/... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\r\n\r\nThe folder C:/Program Files/Java/jdk1.8.0_112  exists and I have installed jdk-8u112-windows-x64.exe on windows. ", "Ah, you are hitting this error https://github.com/bazelbuild/bazel/issues/1463, it's already fixed at HEAD, but the fix is not included in 0.4.0. You can work around this by cloning the repo at C drive, or build Bazel from HEAD. And we'll have 0.4.1 release soon, sorry for the trouble.", "Thank you @meteorcloudy ! \r\nI have moved repos to C:\\ and now I am able to build farther. Now I got:\r\n\r\nINFO: All external dependencies fetched successfully.\r\n+ bazel build -c opt --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=/w --verbose_failures --experimental_ui tensorflow/tools/pip_package:build_pip_package\r\nERROR: C:/tmp/Bazel/Dlaez6KK/external/local_config_cuda/cuda/BUILD:94:12: in srcs attribute of cc_library rule @local_config_cuda//cuda:cudnn: file '@local_config_cuda//cuda:lib/x64/cudnn.lib' is misplaced here (expected .cc, .cpp, .cxx, .c++, .C, .c, .h, .hh, .hpp, .hxx, .inc, .S, .s, .asm, .a, .pic.a, .lo, .pic.lo, .so, .dylib, .dll, .o or .pic.o)\r\nERROR: C:/tmp/Bazel/Dlaez6KK/external/local_config_cuda/cuda/BUILD:94:12: in srcs attribute of cc_library rule @local_config_cuda//cuda:cudnn: '@local_config_cuda//cuda:lib/x64/cudnn.lib' does not produce any cc_library srcs files (expected .cc, .cpp, .cxx, .c++, .C, .c, .h, .hh, .hpp, .hxx, .inc, .S, .s, .asm, .a, .pic.a, .lo, .pic.lo, .so, .dylib, .dll, .o or .pic.o)\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted\r\nINFO: Elapsed time: 2.749s\r\nFAILED: Build did NOT complete successfully\r\n+ exit 1\r\n\r\nI am building CPU version without CUDA support:\r\n\" export TF_NEED_CUDA=0\r\n+ TF_NEED_CUDA=0\"", "Oh, I see. It's the same error here:\r\nhttp://ci.tensorflow.org/job/tf-master-win-bzl/77/console\r\n\r\nThe reason is Bazel 0.4.0 doesn't allow `*.lib` files to be sources of cc_library, this is also fixed at HEAD in Bazel repo. Is it ok for you to build Bazel from HEAD, or you can download the binary built by our ci here:\r\nhttp://ci.bazel.io/view/Bazel%20bootstrap%20and%20maintenance/job/Bazel/JAVA_VERSION=1.8,PLATFORM_NAME=windows-x86_64/\r\n\r\nThanks for your patience with Bazel, we'll make it better soon!", "<Thanks for your patience with Bazel, we'll make it better soon!\r\nWorking as software developer taught me to be extra patient :)\r\n\r\nI have used the Bazel binary from the link above. And now got error:\r\n\r\nERROR: C:/tmp/_bazel_pavel/bga0GFiy/external/nanopb_git/BUILD:6:1: C++ compilation of rule '@nanopb_git//:nanopb' failed: msvc_cl.bat failed: error executing command\r\n  cd C:/tmp/_bazel_pavel/bga0GFiy/execroot/tensorflow\r\n  SET PATH=external/local_config_cc/wrapper/bin\r\n  external/local_config_cc/wrapper/bin/msvc_cl.bat -m64 /D__inline__=__inline /DOS_WINDOWS=OS_WINDOWS /DCOMPILER_MSVC /DNOGDI /DNOMINMAX /DPRAGMA_SUPPORTED /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /D_USE_MATH_DEFINES /nologo /bigobj /Zm500 /J /Gy /GF /W3 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /DNDEBUG /O2 -Xcompilation-mode=opt /w /Iexternal/nanopb_git /Ibazel-out/vc_14_0_x64-py3-opt/genfiles/external/nanopb_git /Iexternal/bazel_tools /Ibazel-out/vc_14_0_x64-py3-opt/genfiles/external/bazel_tools /Iexternal/nanopb_git /Ibazel-out/vc_14_0_x64-py3-opt/genfiles/external/nanopb_git /Iexternal/bazel_tools/tools/cpp/gcc3 /DEPENDENCY_FILE bazel-out/vc_14_0_x64-py3-opt/bin/external/nanopb_git/_objs/nanopb/external/nanopb_git/pb_encode.d /c external/nanopb_git/pb_encode.c /Fobazel-out/vc_14_0_x64-py3-opt/bin/external/nanopb_git/_objs/nanopb/external/nanopb_git/pb_encode.o: com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2\r\nc:\\tmp\\_bazel_pavel\\bga0gfiy\\execroot\\tensorflow\\external\\nanopb_git\\pb.h(66): fatal error C1083: Cannot open include file: 'stddef.h': No such file or directory\r\nERROR: C:/tmp/_bazel_pavel/bga0GFiy/external/nanopb_git/BUILD:6:1: output 'external/nanopb_git/_objs/nanopb/external/nanopb_git/pb_common.o' was not created\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\nAnd out of topic question: is tensorboard build on windows with Bazel supported? Looks like it is not building by default.", "Hmm.. I am not sure why this is happening. Can you try to run `bazel clean --expunge` and then rebuild again? The command will fail with an error saying something like `ERROR: C:/temp/_bazel_pcloudy/Pm9yvx2g/server/jvm.out (Permission denied)`, but you can safely ignore it, we'll fix this.\r\n\r\nHere is what I tested with our latest[ Bazel release candidate ](http://ci.bazel.io/view/Bazel%20bootstrap%20and%20maintenance/job/Bazel/JAVA_VERSION=1.8,PLATFORM_NAME=windows-x86_64/1014/) today and I succeeded:\r\n\r\n-  Clone TF reop\r\n-  Run `./configure` without enabling GPU support\r\n-  My `~/.bazelrc` file:\r\n```\r\nbuild -c opt\r\nbuild --cpu=x64_windows_msvc\r\nbuild --host_cpu=x64_windows_msvc\r\nbuild --copt=\"/w\"                  # Suppress some warning messages\r\nbuild --experimental_ui            # Enable a nice UI\r\nbuild --verbose_failures\r\n```\r\n- Run `bazel build tensorflow/tools/pip_package:build_pip_package`\r\n\r\nAnd I am not quite familiar with tensorboard, but I thinks it's not working on Windows yet, I'll look into it.\r\n\r\n", "Now bash is just exiting out to windows console during  ./configure command looks something went wrong with setting up python library path.\r\n\r\npavel@brans  /C/tensorflow\r\n$ ./configure\r\n/C/tensorflow /C/tensorflow\r\nPlease specify the location of python. [Default is /c/Python27/python]: /i/Anaconda3/python\r\nFound possible Python library paths:\r\n  I:\\Anaconda3\r\n  I:\\Anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [I:\\Anaconda3]\r\n/i/Anaconda3/libs/python35.lib\r\n\r\n\r\nERROR: Invalid python library path: /i/Anaconda3/libs/python35.lib.\r\n\r\npavel@brans  /C/tensorflow\r\n$ ./configure\r\n/C/tensorflow /C/tensorflow\r\nPlease specify the location of python. [Default is /c/Python27/python]: /i/Anaconda3/python\r\nFound possible Python library paths:\r\n  I:\\Anaconda3\r\n  I:\\Anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [I:\\Anaconda3]\r\n/i/Anaconda3/libs/\r\nMicrosoft Windows [Version 10.0.14393] **(here the bash existed)**\r\n(c) 2016 Microsoft Corporation. All rights reserved.", "Oh, for python library path, you should just use the default one by entering a blank line or enter either `I:\\Anaconda3` or `I:\\Anaconda3\\lib\\site-packages`, both places works on my machine.", "I have tried that also. If I leaving the blank line - bash exiting to windows. Also I have tried  I:\\Anaconda3 and  \\i\\Anaconda3 paths and got errors:\r\n\r\npavel@brans  C:/tensorflow\r\n$ ./configure\r\nC:/tensorflow C:/tensorflow\r\nFound possible Python library paths:\r\n  I:\\Anaconda3\r\n  I:\\Anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [I:\\Anaconda3]\r\n\\i\\Anaconda3\r\n\r\n\r\nERROR: Invalid python library path:i.Anaconda3\r\n\r\npavel@brans  C:/tensorflow\r\n$ ./configure\r\nC:/tensorflow C:/tensorflow\r\nFound possible Python library paths:\r\n  I:\\Anaconda3\\lib\\site-packages\r\n  I:\\Anaconda3\r\nPlease input the desired Python library path to use.  Default is [I:\\Anaconda3\\lib\\site-packages]\r\n I:\\Anaconda3\r\nERROR: Invalid python library path: I:Anaconda3.\r\n\r\n\r\nAlso I have tried using export command to set up vars:\r\npavel@brans  C:/tensorflow\r\n$ export TMPDIR=\"C:/tmp\"\r\n\r\npavel@brans  C:/tensorflow\r\n$ export BAZEL_SH=\"C:/tools/msys64/usr/bin/bash\"\r\n\r\npavel@brans  C:/tensorflow\r\n$ export PYTHON_BIN_PATH=\"I:/Anaconda3/python\"\r\n\r\npavel@brans  C:/tensorflow\r\n$ export BAZEL_PYTHON=\"I:/Anaconda3/python\"\r\n\r\npavel@brans  C:/tensorflow\r\n$ export BAZEL_VS=\"E:/Program Files (x86)/Microsoft Visual Studio 14.0\" __(by the way you can take it from %VS140COMNTOOLS%\\\\..\\\\.. environment var)__\r\n\r\npavel@brans  C:/tensorflow\r\n$ export PATH=\"/I/Anaconda3:$PATH\"\r\n\r\n**then:** \r\n\r\nbazel clean --expunge\r\n\r\n**and:**\r\n\r\n$ bazel build -c opt --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=\"/w\" --verbose_failures --experimental_u\r\ni tensorflow/tools/pip_package:build_pip_package\r\nERROR: CreateFile(C:\\tmp\\_bazel_pavel\\s7kGbK8z\\install): The system cannot find the file specified.\r\n (2)\r\nKilled non-responsive server process (pid=286036)\r\n...............\r\nWARNING: C:/tensorflow/tensorflow/tensorflow.bzl:29:5:\r\nCurrent Bazel is not a release version, cannot check for compatibility.\r\nWARNING: C:/tensorflow/tensorflow/tensorflow.bzl:30:5: Make sure that you are running at least Bazel 0.3.2.\r\nERROR: C:/tensorflow/tensorflow/core/BUILD:1117:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by C:/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'\r\nERROR: C:/tensorflow/tensorflow/core/BUILD:1117:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by C:/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'\r\nERROR: C:/tensorflow/tensorflow/core/BUILD:1117:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by C:/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted\r\nINFO: Elapsed time: 25.718s\r\nFAILED: Build did NOT complete successfully\r\n\r\nMaybe msys64->Win10 issue?", "Can you try `I:\\\\Anaconda3` or `I:/Anaconda3`, I think it is because we have to escape `\\` when running in MSYS.\r\nOnce `./configure` succeeds, the bazel failure should go away.\r\n\r\nExporting those environment variables is a good idea.\r\n\r\nAnd thanks for reminding `%VS140COMNTOOLS%` environment variable, that could be very helpful.", "Looks like it is the problem:\r\n```\r\npcloudy@PCLOUDY1-W MSYS ~/meteorcloudy/tensorflow\r\n$ ./configure\r\n~/meteorcloudy/tensorflow ~/meteorcloudy/tensorflow\r\nPlease specify the location of python. [Default is /c/Program Files/Anaconda3/python]:\r\nFound possible Python library paths:\r\n  C:\\Program Files\\Anaconda3\\lib\\site-packages\r\n  C:\\Program Files\\Anaconda3\r\nPlease input the desired Python library path to use.  Default is [C:\\Program Files\\Anaconda3\\lib\\site-packages]\r\nC:\\Program Files\\Anaconda3\r\n\r\n\r\nERROR: Invalid python library path: C:Program FilesAnaconda3.\r\n\r\npcloudy@PCLOUDY1-W MSYS ~/meteorcloudy/tensorflow\r\n$ ./configure\r\n~/meteorcloudy/tensorflow ~/meteorcloudy/tensorflow\r\nPlease specify the location of python. [Default is /c/Program Files/Anaconda3/python]:\r\nFound possible Python library paths:\r\n  C:\\Program Files\\Anaconda3\\lib\\site-packages\r\n  C:\\Program Files\\Anaconda3\r\nPlease input the desired Python library path to use.  Default is [C:\\Program Files\\Anaconda3\\lib\\site-packages]\r\nC:\\\\Program Files\\\\Anaconda3\r\nJunction created for util\\python\\python_include <<===>> C:\\Program Files\\Anaconda3\\include\r\nJunction created for util\\python\\python_lib <<===>> C:\\Program Files\\Anaconda3\r\nJunction created for third_party\\py\\numpy\\numpy_include <<===>> C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\include\r\nDo you wish to build TensorFlow with GPU support? [y/N] n\r\nNo GPU support will be enabled for TensorFlow\r\nConfiguration finished\r\nINFO: All external dependencies fetched successfully.\r\n\r\n```", "I have tried all this with no luck :( Bash is exiting after:\r\n\r\nPlease input the desired Python library path to use.  Default is [I:\\Anaconda3]\r\nI:\\\\\\\\Anaconda3\r\nMicrosoft Windows [Version 10.0.14393]\r\n(c) 2016 Microsoft Corporation. All rights reserved.\r\nI will try also on other env at home.", "OK, at least it's not giving the `Invalid python library path` again.\r\nCan you do\r\n```\r\nexport PYTHON_BIN_PATH=I:/Anaconda3/python\r\nexport TF_NEED_CUDA=0\r\n```\r\nAnd then run `echo \"\" | ./configure` ?\r\n\r\n", "And adding `set -ex` at the beginning of `./configure` will give us more information to debug this. :)", "That is strange, but  after\r\nexport PYTHON_BIN_PATH=I:/Anaconda3/python\r\nexport TF_NEED_CUDA=0\r\nand \r\nexport BAZEL_VS=\"E:/Program Files (x86)/Microsoft Visual Studio 14.0\"\r\necho \"\" | ./configure\r\nconfigure command succeeded.\r\nBuild still failed\r\n\r\nbazel build tensorflow/tools/pip_package:build_pip_package\r\nExtracting Bazel installation...\r\nCannot terminate server process with PID 464576\r\n.\r\nINFO: Found 1 target...\r\nERROR: C:/tmp/_bazel_pavel/s7kGbK8z/external/grpc/BUILD:69:1: C++ compilation of rule '@grpc//:gpr' failed: gcc failed: error executing command C:/tools/msys64/usr/bin/gcc -MD -MF bazel-out/local-py3-fastbuild/bin/external/grpc/_objs/gpr/external/grpc/src/core/lib/support/wrap_memcpy.d -iquote external/grpc -iquote ... (remaining 19 argument(s) skipped): java.io.IOException: CreateProcess(): The system cannot find the file specified.\r\n.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 41.476s, Critical Path: 3.86s\r\n\r\nINFO: Found 1 target...\r\nERROR: C:/tmp/_bazel_pavel/s7kGbK8z/external/grpc/BUILD:69:1: C++ compilation of rule '@grpc//:gpr' failed: gcc failed: error executing command C:/tools/msys64/usr/bin/gcc -MD -MF bazel-out/local-py3-fastbuild/bin/external/grpc/_objs/gpr/external/grpc/src/core/lib/support/wrap_memcpy.d -iquote external/grpc -iquote ... (remaining 19 argument(s) skipped): java.io.IOException: CreateProcess(): The system cannot find the file specified.\r\n.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 41.476s, Critical Path: 3.86s", "It's good the configure command succeed.\r\n\r\nThe build failure is because you are not using MSVC toolchain.\r\nTry\r\n`bazel build -c opt  --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=\"/w\" --experimental_ui --verbose_failures tensorflow/tools/pip_package:build_pip_package`\r\nOr make sure your `~/.bazelrc` is:\r\n```\r\nbuild -c opt\r\nbuild --cpu=x64_windows_msvc\r\nbuild --host_cpu=x64_windows_msvc\r\nbuild --copt=\"/w\"                  # Suppress some warning messages\r\nbuild --experimental_ui            # Enable a nice UI\r\nbuild --verbose_failures\r\n\r\n```", "$ bazel -c opt --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=\"/w\" --experimental_ui --verbose_failures tens\r\norflow/tools/pip_package:build_pip_package\r\nUnknown Bazel startup option: '-c'.\r\n  For more info, run 'Bazel help startup_options'.\r\n\r\n$ bazel version\r\nBuild label: 0.4.1rc2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 23 08:59:10 2016 (1479891550)\r\nBuild timestamp: 1479891550\r\nBuild timestamp as int: 1479891550", "Sorry, I updated the comment, it's:\r\n`bazel build -c opt  --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=\"/w\" --experimental_ui --verbose_failures tensorflow/tools/pip_package:build_pip_package`", "That helped :)\r\n\r\nbuild still fails with\r\nERROR: C:/tmp/_bazel_pavel/s7kGbK8z/external/pcre/BUILD:5:1: C++ compilation of rule '@pcre//:pcre' failed: msvc_cl.bat failed: error executing command\r\n  cd C:/tmp/_bazel_pavel/s7kGbK8z/execroot/tensorflow\r\n  SET PATH=external/local_config_cc/wrapper/bin\r\n  external/local_config_cc/wrapper/bin/msvc_cl.bat -m64 /D__inline__=__inline /DOS_WINDOWS=OS_WINDOWS /DCOMPILER_MSVC /DNOGDI /DNOMINMAX /DPRAGMA_SUPPORTED /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /D_USE_MATH_DEFINES /nologo /bigobj /Zm500 /J /Gy /GF /W3 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /DNDEBUG /O2 -Xcompilation-mode=opt -g0 /Iexternal/pcre /Ibazel-out/host/genfiles/external/pcre /Iexternal/bazel_tools /Ibazel-out/host/genfiles/external/bazel_tools /Iexternal/pcre /Ibazel-out/host/genfiles/external/pcre /Iexternal/bazel_tools/tools/cpp/gcc3 /DEPENDENCY_FILE bazel-out/host/bin/external/pcre/_objs/pcre/external/pcre/pcre_study.d /c external/pcre/pcre_study.c /Fobazel-out/host/bin/external/pcre/_objs/pcre/external/pcre/pcre_study.o -DHAVE_BCOPY=1 -DHAVE_INTTYPES_H=1 -DHAVE_MEMMOVE=1 -DHAVE_STDINT_H=1 -DHAVE_STRERROR=1 -DHAVE_SYS_STAT_H=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_UNISTD_H=1 -DLINK_SIZE=2 -DMATCH_LIMIT=10000000 -DMATCH_LIMIT_RECURSION=1000 -DMAX_NAME_COUNT=10000 -DMAX_NAME_SIZE=32 -DNEWLINE=10 -DNO_RECURSE -DPARENS_NEST_LIMIT=50 -DPCRE_STATIC=1 -DPOSIX_MALLOC_THRESHOLD=10 -DSTDC_HEADERS=1 -DSUPPORT_UCP -DSUPPORT_UTF: com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2\r\nc:\\tmp\\_bazel_pavel\\s7kgbk8z\\execroot\\tensorflow\\external\\pcre\\pcre_internal.h(108): fatal error C1083: Cannot open include file: 'ctype.h': No such file or directory\r\nERROR: C:/tmp/_bazel_pavel/s7kGbK8z/external/pcre/BUILD:5:1: output 'external/pcre/_objs/pcre/external/pcre/pcre_newline.o' was not created\r\nERROR: C:/tmp/_bazel_pavel/s7kGbK8z/external/protobuf/BUILD:638:1: declared output 'external/protobuf/python/google/protobuf/empty.proto' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\r\nERROR: C:/tmp/_bazel_pavel/s7kGbK8z/external/protobuf/BUILD:638:1: declared output 'external/protobuf/python/google/protobuf/field_mask.proto' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\r\nERROR: C:/tmp/_bazel_pavel/s7kGbK8z/external/protobuf/BUILD:638:1: declared output 'external/protobuf/python/google/protobuf/source_context.proto' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\r\nERROR: C:/tmp/_bazel_pavel/s7kGbK8z/external/protobuf/BUILD:638:1: declared output 'external/protobuf/python/google/protobuf/struct.proto' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\r\nERROR: C:/tmp/_bazel_pavel/s7kGbK8z/external/protobuf/BUILD:638:1: declared output 'external/protobuf/python/google/protobuf/timestamp.proto' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\r\nERROR: C:/tmp/_bazel_pavel/s7kGbK8z/external/protobuf/BUILD:638:1: declared output 'external/protobuf/python/google/protobuf/type.proto' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\r\nERROR: C:/tmp/_bazel_pavel/s7kGbK8z/external/protobuf/BUILD:638:1: declared output 'external/protobuf/python/google/protobuf/wrappers.proto' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: C:/tensorflow/tensorflow/python/BUILD:1926:1 C++ compilation of rule '@pcre//:pcre' failed: msvc_cl.bat failed: error executing command\r\n  cd C:/tmp/_bazel_pavel/s7kGbK8z/execroot/tensorflow\r\n  SET PATH=external/local_config_cc/wrapper/bin\r\n  external/local_config_cc/wrapper/bin/msvc_cl.bat -m64 /D__inline__=__inline /DOS_WINDOWS=OS_WINDOWS /DCOMPILER_MSVC /DNOGDI /DNOMINMAX /DPRAGMA_SUPPORTED /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /D_USE_MATH_DEFINES /nologo /bigobj /Zm500 /J /Gy /GF /W3 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /DNDEBUG /O2 -Xcompilation-mode=opt -g0 /Iexternal/pcre /Ibazel-out/host/genfiles/external/pcre /Iexternal/bazel_tools /Ibazel-out/host/genfiles/external/bazel_tools /Iexternal/pcre /Ibazel-out/host/genfiles/external/pcre /Iexternal/bazel_tools/tools/cpp/gcc3 /DEPENDENCY_FILE bazel-out/host/bin/external/pcre/_objs/pcre/external/pcre/pcre_byte_order.d /c external/pcre/pcre_byte_order.c /Fobazel-out/host/bin/external/pcre/_objs/pcre/external/pcre/pcre_byte_order.o -DHAVE_BCOPY=1 -DHAVE_INTTYPES_H=1 -DHAVE_MEMMOVE=1 -DHAVE_STDINT_H=1 -DHAVE_STRERROR=1 -DHAVE_SYS_STAT_H=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_UNISTD_H=1 -DLINK_SIZE=2 -DMATCH_LIMIT=10000000 -DMATCH_LIMIT_RECURSION=1000 -DMAX_NAME_COUNT=10000 -DMAX_NAME_SIZE=32 -DNEWLINE=10 -DNO_RECURSE -DPARENS_NEST_LIMIT=50 -DPCRE_STATIC=1 -DPOSIX_MALLOC_THRESHOLD=10 -DSTDC_HEADERS=1 -DSUPPORT_UCP -DSUPPORT_UTF: com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2\r\nINFO: Elapsed time: 6.182s, Critical Path: 0.70s\r\nFAILED: Build did NOT complete successfully", "This is weird.. I am guessing maybe there is something wrong with your Visual Studio installation,\r\nI found [this error](https://social.msdn.microsoft.com/Forums/vstudio/en-US/0f580a73-6f11-4aa6-abbe-6bf1438734c2/standard-files-stddefh-stringh-missing-in-visual-studio-2015-rc?forum=vclanguage).\r\nOn my machine, `stddef.h` can be found at `C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.10240.0\\ucrt`\r\nCan you try to reinstall Visual Studio 2015?\r\n", "\r\n<On my machine, stddef.h can be found at C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.10240.0\\ucrt\r\nI also have stddef.h there.\r\n\r\nFrom [https://social.msdn.microsoft.com/Forums/vstudio/en-US/0f580a73-6f11-4aa6-abbe-6bf1438734c2/standard-files-stddefh-stringh-missing-in-visual-studio-2015-rc?forum=vclanguage](the topic you posted)\r\n\r\n> \"The headers, sources, and libraries are now distributed as part of a separate \r\n> Universal CRT SDK. This SDK is included with Visual Studio; it is installed by \r\n> default to C:\\Program Files (x86)\\Windows Kits\\10.\"\r\n\r\nI have installed Windows Kits v10 on my env. in the past, don't you have it? Visual Studio is ok on my env -except Tensorflow hundreds of different solution are building well there.\r\n\r\nI am familiar with visual studio and MSBuild pretty well and maybe can find the core of problem with stddef.h. Does bazel invoke VS/msbuild internally during build? Can I reproduce this error with msbuild command? \r\n\r\n\r\n\r\n---\r\n\r\nI have tried to run configure on other env. And got other unclear error:\r\n\r\n./configure\r\n./configure: line 2: $'\\r': command not found\r\n./configure: line 4: $'\\r': command not found\r\n./configure: line 30: syntax error near unexpected token `$'in\\r''\r\n'/configure: line 30: `  case $INPUT in", "This issue is quite old and hasn't had recent activity. Be sure to try the latest build instructions (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile) on the latest version of TensorFlow, and please create a new bug if it is still not working. Thank you."]}, {"number": 5747, "title": "Unable to use GPU with Tensorflow 11.0 (regression from 11.0rc2)", "body": "Cannot run convolutional network on GPU using TF 11.\r\nIt works fine under TF 11rc2. This looks like a regression.\r\n\r\nIssue #5555 indicated to install cudnn 5.1. I checked and that is what I appear to have. cudnn.h lists 5.1 as the major/minir version. I also compared the binary file libcudnn.5.dylib downloaded from NVIDIA with the one I have in /usr/local/cuda/lib and they are the same.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/5555\r\n\r\n### Environment info\r\nOperating System: MacOS\r\n\r\nInstalled version of CUDA and cuDNN:\r\nCUDA Driver Version: 8.0.51 \r\ncuDNN 5.1\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nlrwxr-xr-x  1 root  wheel     33 Oct 24 15:40 /usr/local/cuda/lib/libcuda.1.dylib -> /usr/local/cuda/lib/libcuda.dylib\r\n-rwxr-xr-x  1 root  wheel  13504 Oct 24 22:27 /usr/local/cuda/lib/libcuda.dylib\r\nlrwxr-xr-x@ 1 root  wheel     45 Sep 27 00:00 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a\r\nlrwxr-xr-x@ 1 root  wheel     50 Sep 27 00:00 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\nlrwxr-xr-x@ 1 root  wheel     46 Sep 27 00:00 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib\r\nlrwxr-xr-x@ 1 root  wheel     49 Sep 27 00:00 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a\r\nlrwxr-xr-x  1 root  wheel     47 Oct 24 15:34 /usr/local/cuda/lib/libcudnn.5.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5.dylib\r\nlrwxr-xr-x  1 root  wheel     45 Oct 24 15:34 /usr/local/cuda/lib/libcudnn.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.dylib\r\nlrwxr-xr-x  1 root  wheel     48 Oct 24 15:34 /usr/local/cuda/lib/libcudnn_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn_static.a\r\n\r\n1. A link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.11.0-py3-none-any.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.11.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nhttps://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_3.0_convolutional.py\r\n(run this file with python3 - you will need three additional files from the same directory, namely tensorflowvisu*. matplotlib is also required: \"pip3 matplotlib\")\r\n\r\n### What other attempted solutions have you tried?\r\nIt works under Tensorflow 11 rc2\r\n\r\n### Logs or other output that would be helpful\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \r\nname: GeForce GTX 980 Ti\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:c1:00.0\r\nTotal memory: 6.00GiB\r\nFree memory: 5.87GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:c1:00.0)\r\n**E tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded runtime CuDNN library: 5105 (compatibility version 5100) but source was compiled with 5005 (compatibility version 5000).  If using a binary install, upgrade your CuDNN library to match.**  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\nF tensorflow/core/kernels/conv_ops.cc:526] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\nAbort trap: 6\r\n\r\n", "comments": ["@yifeif @caisq Is it possible our GPU-mac testing fleet has inconsistent CuDNN versions installed?\r\nThat is the only explanation I can see.", "mac1-slave has CUDNN 5.0.5 (5005), as seen in in /usr/local/cuda/include/cudnn.h", "We should upgrade that to 5.1, and rebuild our pip packages with 5.1", "We have built and uploaded new pip packages for mac gpu. they should now all be built for cudnn 5.1\r\nPlease try redownloading and installing the pip packages.\r\nAlso, please reopen the issue if you still see problems.", "I confirm the fix worked! Thanks"]}, {"number": 5746, "title": "Optimization occurs after graph is partitioned across devices", "body": "The optimization (sub-expression, constant folding) of the graphs takes place after partitioning. While this is important, it prevents optimization of logic that is split across devices by an unfortunate placement decision.\r\n\r\nIn my simple example, constants are assigned to my device graph, and the downstream maths nodes are assigned to the CPU.  Consequently, parts of the graph where constant folding should occur are not eliminated, resulting in substantially larger graphs than is necessary.\r\n\r\nI propose that there should be an optmization pass before the graph is partitioned.  I realize that this is awkward, because the optimization takes place on a Graph, not a GraphDef.  ", "comments": ["This makes a lot of sense to me.  There are also a number of places where constant folding doesn't quite do what is expected (e.g. int32 tensor arithmetic on non-CPU devices).  ", "Closing because of inactivity."]}, {"number": 5745, "title": "Constants are placed on a device when they do nothing but feed off-device nodes", "body": "I have created a device that supports integer constant tensors (because I would like to be able to support the inputs to reshape etc...), but not the integer maths operators (because I'm not going to accelerate those initially).\r\n\r\nThe gradient generation logic creates trees of integer maths.  The constants for this maths are placed on the device, while the maths nodes themselves are placed on the CPU (as expected).\r\n\r\nThis isn't very sensible.  \r\n\r\nI think it would be better that constants are not placed on a device, unless they are feeding downstream nodes that are also on the same device.\r\n\r\nThere may be some classes of constant (ones with a specific memory placement, very large ones), which would end up on a different device to their downstream nodes, but I think that the default should be to keep them together.\r\n\r\n", "comments": ["There is some logic in the constant folder which treats int32 ops differently when assigned to non-CPU devices.  This can be a pain when adding new device types and should probably be improved.\r\n\r\nWe should also probably take a look at the interaction of constant folding (and other optimizations) with the the two graph partitioning stages.", "Thanks for the feedback.  For this particular issue, I would like to consider changing the placement algorithm so that constants are kept on the same device as the nodes that they feed.\r\n\r\nAn example of where this is important is as follows:\r\n\r\nI support INT constants on my device, but I do not support the 'Add' operation for integer inputs.  As it stands, the placer will put the constants on my device and the Add op on the CPU.  After elaboration, I will have the constants on the device, feeding _Send ops and nothing else.  This doesn't make sense.\r\n\r\nI will create a pull request for your evaluation.\r\n", "Hmm.  I will investigate the behaviour of the simple_placer more closely.  At first glance it appears to contain logic for this constant placement already.", "Ok.  I can see that you've already considered this.  There is code for doing what I would like, but it is failing to activate due to the simplicity of this 'IsGeneratorNode' function.  There is a comment that describes the solution already.  I will see about producing a pull request that solves this 'TODO'\r\n\r\n```\r\n// TODO(vrv): Currently this handles only nodes with one output, but\r\n// this could be extended to handle the case where a node has many\r\n// outputs that are connected to nodes in the same colocation group.\r\nbool IsGeneratorNode(const Node* node) {\r\n  return node->num_inputs() == 0 && node->num_outputs() == 1 &&\r\n         node->out_edges().size() == 1 && !IsRefType(node->output_type(0));\r\n}\r\n```", "Perhaps it is enough just to remove the requirement that there is a single downstream consumer node when deciding that a node is a Generator.   By doing this, the generator ends up on the same device as its first output.   As it stands, it can end up on a device which is neither of its outputs, as it is placed using the standard placement algorithm.\r\n\r\nA diff including a unit test is here\r\n\r\n\r\n[simple_placer_patch.txt](https://github.com/tensorflow/tensorflow/files/611256/simple_placer_patch.txt)\r\n", "@DavidNorman yeah this code contains only heuristics for placement in the absence of user code specifying placement -- it will never be foolproof, unfortunately.\r\n\r\nFor the time being, have you considered just using manual device placement for your device?", "(Also the reason for checking out_edges() == 1 is that if there are two downstream consumers of the op, one on CPU and one on GPU, which device should you put the op on?  This heuristic was meant to avoid any ambiguity).", "I seem to recall there is also some special case code for adding int32 `HostConstant`s on GPU devices, and logic to duplicate a `Const` node when used in two places.  (@yuanbyu probably knows more about this).  \r\n\r\n@DavidNorman wrt the integer maths in gradient code - a lot of this now disappears at run time due to constant folding if the shapes of ops are fully known.  We use this technique a lot with XLA. If you are writing a TF device to support a hardware accelerator this is probably the best route for you going forward. ", "Yeah, there is code in graph_partition.cc that looks at host memory attributes to know whether to add send/recv nodes, so some of this is handled already.\r\n\r\nI also agree that XLA would be a better long term bet for new devices -- to get a proof of concept, it probably makes sense to develop short-term workarounds rather than trying to do anything sophisticated with automatic device placement :)", "Thanks for the info.  The whole placement / constant folding doesn't work for me because I am not supporting integer maths ops, but I am supporting integer constants.  Because some of the constants feed 2 nodes, they are not placed with the maths ops on the CPU (even though both maths ops are on the CPU).  Consequently I have ops that do nothing but feed _Send nodes.\r\n\r\nI have worked around that though.\r\n\r\nThe other problem I had is when I was hoping to claim that I supported all integer ops on the card, and then hope that constant folding killed them off. This doesn't work because constant folding collapses constant trees, but does not kill off the nodes if there is a control edge holding the tree in place.  See the other issue as to why I think that this is wrong:\r\n\r\n[Constant folding / control edge issue](https://github.com/tensorflow/tensorflow/issues/5543)\r\n\r\nIt sounds like I should be aiming to use XLA instead of the normal system.  Is XLA going to take a normal graph, and somehow transform it into primitives that the device claims that it can support? Or will the user have to write the graph using a new set of primitives?\r\n\r\n\r\n\r\n", "Incidentally, while I agree that the \"out_edges() == 1\" check prevents ambiguity, it nevertheless introduces the situation where the generator node ends up on none of the devices that it feeds.\r\n\r\nI suppose that occasionally this might be the right thing to do (perhaps DevA->DevB, DevA->DevC are fast for IO, but DevB->DevC is slow, so putting a constant on A when the consumers are on B and C is good).\r\n\r\nHowever, in the case where a constant feeds two nodes, both on the same device, then surely the constant should be on the same device.   I will create a more sophisticated diff with that idea.", "Hi. This is a diff that ignores the number of outputs on the generator node, but allows freedom of placement if the consumer nodes are not all on the same device. When they are all on the same device, it will place the generator node there.\r\n\r\n[simple_placer_diff.txt](https://github.com/tensorflow/tensorflow/files/613027/simple_placer_diff.txt)\r\n", "That idea seems reasonable -- can you send us a PR?", "The PR is here: https://github.com/tensorflow/tensorflow/pull/5899\r\n\r\nI think that we have signed the CLA, but I will have to check. ", "merged - thanks :)"]}, {"number": 5744, "title": "Unable to run cifar10_multi_gpu_train as is in TensorFlow master", "body": "Ubuntu 14.04\r\ncommit used: a154b6a7826760f30e368e84f51048218002d376\r\n\r\nSimply changed num_gpus to 2. I have 2 GPUs. A 1060 and 970. Shows this error:\r\n\r\n`ValueError: Variable tower_1/tower_1/conv1/weight_loss/avg/biased does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?`\r\n\r\n", "comments": ["Please can you provide all of the information requested in the issues template.", "@prb12, @jkschin This is a duplicate of #5652 ", "Ok - closing as dup of #5652 "]}, {"number": 5743, "title": "seq2seq tutorial example not working for python 3.4/3.5 (#5118)", "body": "This is a fix for issue #5118", "comments": ["Can you sign the CLA please, @Invisibility ?", "@caisq I did sign the Google Individual CLA (based on the instructions at https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md) on Nov 20 with my github account name Invisibility. Let me know if I need to do anything extra.", "Jenkins, test this please.", "Macos tests fixed. \r\nJenkins, test this please.", "Jenkins, test this please.", "@tensorflow-jenkins , test this please.", "The failure of //tensorflow/core:gpu_tracer_test in the Linux GPU build is unrelated. Merging this PR now.\r\n\r\nThanks, @Invisibility "]}, {"number": 5742, "title": "TypeError: ones_initializer() got multiple values for keyword argument 'dtype' when execute the inception_train", "body": "## Exception:\r\n\r\n```\r\n...\r\nFile \"inception/slim/variables.py\", line 290, in variable\r\n  trainable=trainable, collections=collections)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1024, in get_variable\r\n  custom_getter=custom_getter)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 850, in get_variable\r\n  custom_getter=custom_getter)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 346, in get_variable\r\n  validate_shape=validate_shape)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 331, in _true_getter\r\n  caching_device=caching_device, validate_shape=validate_shape)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 677, in _get_single_variable\r\n  expected_shape=shape)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\r\n  expected_shape=expected_shape)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 327, in _init_from_args\r\n  initial_value(), name=\"initial_value\", dtype=dtype)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 665, in <lambda>\r\n  shape.as_list(), dtype=dtype, partition_info=partition_info)\r\nTypeError: ones_initializer() got multiple values for keyword argument 'dtype'\r\n```\r\n\r\nI think recently changes of tensorflow.python.init_ops influence to this issue.\r\n\r\n## [init_ops.py](https://github.com/tensorflow/tensorflow/commit/cfb2280d3f6ced298681bd1141479a59a06abde8)\r\n```diff\r\n< def ones_initializer(dtype=dtypes.float32, partition_info=None):\r\n---\r\n> def ones_initializer(shape, dtype=dtypes.float32, partition_info=None):\r\n```\r\n\r\n## [variable_scope.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L665)\r\n\r\n```python\r\n 655     # Create the tensor to initialize the variable.\r\n 656     if initializer is None:\r\n 657       initializer = init_ops.uniform_unit_scaling_initializer()\r\n 658     # Clear control dependencies while creating the initializer.\r\n 659     with ops.control_dependencies(None):\r\n 660       if initializing_from_value:\r\n 661         init_val = initializer\r\n 662         variable_dtype = None\r\n 663       else:\r\n 664         init_val = lambda: initializer(\r\n 665             shape.as_list(), dtype=dtype, partition_info=partition_info)\r\n 666         variable_dtype = dtype.base_dtype\r\n```\r\n\r\n`lambda:initiallizer` calls with three parameters. So, parameter missmatch occured.\r\n\r\nI think it needs to change `variable_scope.py` source code.\r\n", "comments": ["@masanao-miyamoto That's correct !! I am having same error too.. please [have a look here](https://github.com/tensorflow/models/issues/672)", "thanks for the code references, \r\nI could temporaily fix this issue by reverting 'ones_initializer' back to the previous version ", "This is not a TensorFlow bug, but a breaking change to the API which was introduced 11 days ago in this commit: https://github.com/tensorflow/tensorflow/commit/cfb2280d3f6ced298681bd1141479a59a06abde8\r\n\r\nI assume that you are using a nightly build of TensorFlow?\r\n\r\nThe inception model hasn't been updated to use the new API yet. @shlens \r\n\r\n\r\n", "The issue seems to be a duplicate of this [issue](https://github.com/tensorflow/models/issues/672) due to an API change. @itsmeolivia Would you be able to take a look?", "@prb12, @shlens thanks for your advise.\r\n\r\nI understood an API changes.\r\n\r\nI tried to fix [model/inception/inception/slim/ops.py](https://github.com/tensorflow/models/blob/master/inception/inception/slim/ops.py#L92) . And successfully execute inception training jobs. Like below\r\n\r\n```diff\r\n      gamma = variables.variable('gamma',\r\n                                 params_shape,\r\n-                                 initializer=tf.ones_initializer,\r\n+                                 initializer=tf.ones_initializer(),\r\n                                 trainable=trainable,\r\n                                 restore=restore)\r\n```", "only change one line above will fix this issue?", "@civilman628 I changed two lines to fix this issue.", "@civilman628 @panovr I changed two lines. \r\n\r\n``` diff \r\n 91     if scale:\r\n 92       gamma = variables.variable('gamma',\r\n 93                                  params_shape,\r\n- 94                                  initializer=tf.ones_initializer,\r\n+ 94                                  initializer=tf.ones_initializer(),\r\n 95                                  trainable=trainable,\r\n 96                                  restore=restore)\r\n 97     # Create moving_mean and moving_variance add them to\r\n 98     # GraphKeys.MOVING_AVERAGE_VARIABLES collections.\r\n 99     moving_collections = [moving_vars, tf.GraphKeys.MOVING_AVERAGE_VARIABLES]\r\n100     moving_mean = variables.variable('moving_mean',\r\n101                                      params_shape,\r\n102                                      initializer=tf.zeros_initializer,\r\n103                                      trainable=False,\r\n104                                      restore=restore,\r\n105                                      collections=moving_collections)\r\n106     moving_variance = variables.variable('moving_variance',\r\n107                                          params_shape,\r\n- 108                                          initializer=tf.ones_initializer,\r\n+ 108                                          initializer=tf.ones_initializer(),\r\n109                                          trainable=False,\r\n110                                          restore=restore,\r\n111                                          collections=moving_collections)\r\n```", "I think it's very counter-intuitive that `tf.zeros_initializer` and `tf.ones_initializer` has different APIs..", "The problem is still actual in `batch_normalization`:\r\n\r\n```\r\n        moving_mean = vs.variable(scope + 'moving_mean',\r\n                                  input_shape[-1:],\r\n                                  initializer=tf.zeros_initializer,\r\n                                  trainable=False,\r\n                                  restore=restore)\r\n        moving_variance = vs.variable(scope + 'moving_variance',\r\n                                      input_shape[-1:],\r\n                                      initializer=tf.ones_initializer,\r\n                                      trainable=False,\r\n                                      restore=restore)\r\n```\r\n\r\nAny workaround?", "Having this same error msg following: https://tensorflow.github.io/serving/serving_inception\r\nAt this point: \r\nroot@c97d8e820ced:/serving# bazel-bin/tensorflow_serving/example/inception_export --checkpoint_dir=inception-v3 --export_dir=inception-export\r\n\r\n\"\"\"    expected_shape=expected_shape)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variables.py\", line 322, in _init_from_args\r\n    initial_value(), name=\"initial_value\", dtype=dtype)\r\n  File \"/root/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 672, in <lambda>\r\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\r\nTypeError: zeros_initializer() got multiple values for keyword argument 'dtype'\"\"\"\r\n\r\n", "@luisbanjo try to change all `tf.zeros_initializer` to `tf.zeros_initializer()`, same as `tf.ones_initializer`.", "yup, that we did already, but it is not a permanent solution :)\n\nOn Wed, Jan 18, 2017 at 9:24 PM, Ziming Dong <notifications@github.com>\nwrote:\n\n> @luisbanjo <https://github.com/luisbanjo> try to change all\n> tf.zeros_initializer to tf.zeros_initializer(), same as\n> tf.ones_initializer.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5742#issuecomment-273685052>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWjHkYyGodxEk9IBogf9mBbKNjjVsKVQks5rTvODgaJpZM4K33n2>\n> .\n>\n", "I have recently updated to tensorflow 12.1 and installed tflearn.!\r\nI had the same problem when using batchnormalization from tflearn. \r\nI followed @suiyuan2009's suggestion and changed the `tf. ones_initializer` to `tf.ones_initializer()`\r\nand the error is gone (changing `tf.zeros_initializer `to `tf.zeros_initializer()` would generate error! so I left it as is)\r\nNow I'm getting \r\n`AttributeError: 'module' object has no attribute 'control_flow_ops'`\r\nany idea how to fix this? ", "I did a search on tensorflow for any zeros_initializer and ones_initializer instances that may have been missed and there are none.  tflearn is not a tensorflow supported library so you should report those issues to the actual tflearn repo on github.  Likewise, tensorflow_serving issues should be reported to the tensorflow_serving repo.", "Please update to TensorFlow 1.0. The initializer API should be consistent for all functions in 1.0."]}, {"number": 5741, "title": "Python API Docs broken at r0.11", "body": "#4969 isn't fixed at branch r0.11.\r\nhttps://www.tensorflow.org/versions/r0.11/api_docs/python/state_ops.html#constant_initializer\r\n", "comments": ["Closing as duplicate of #4969"]}, {"number": 5740, "title": "[Compression]Problem at Tensorflow/Model/Compression", "body": "\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/4772\r\ntensorflow/models/compression does not work,but his problem was not same to me.\r\n### Environment info\r\nOperating System:\r\n\r\nUbuntu 14.04\r\ntensorflow cpu-only version:0.8.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/common_shapes.py\", line 155, in get2d_conv_output_size\r\n    % (row_stride, col_stride, filter_height, filter_width))\r\nValueError: ('stride must be less than or equal to filter size', 'stride: [2x2] filter: [1x1]')\r\n\r\n\r\n### What other attempted solutions have you tried?\r\nI have open the common_shapes.py,but don't know how to repair.\r\n\r\n", "comments": ["Please can you report this issue in the tensorflow/models repository, clearly labelling the issue '[Compression]' and providing sufficient information to reproduce (e.g. at least OS version, TensorFlow version and all necessary command lines)"]}, {"number": 5739, "title": "[Windows/CMake] Fix some C++ tests", "body": "After recent changes, some enabled C++ tests do not build on Windows. This PR fixes that, plus has fixes to some other C++ tests to build under VC++. I have further fixes for other C++ tests too - will send them in subsequent PRs.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please."]}, {"number": 5738, "title": "tf.sub error message for bad types is confusing", "body": "tf.sub() displays a strange error message if the types don't match:\r\nTypeError: DataType uint8 for attr 'T' not in list of allowed values: float16, float32, float64, int32, int64, complex64, complex128\r\n\r\nIt's unclear what attr 'T' is.\r\n\r\nExpected behavior:\r\nfor tf.add(), tf.mul(), tf.div(), the message makes sense:\r\nTypeError: Input 'y' of 'Add' Op has type int64 that does not match type uint8 of argument 'x'.\r\n\r\nIt seems to me that tf.sub() should print a similar message.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nnone\r\n\r\n### Environment info\r\nOperating System:\r\nDarwin Kens-MacBook-Pro.local 16.1.0 Darwin Kernel Version 16.1.0: Thu Oct 13 21:26:57 PDT 2016; root:xnu-3789.21.3~60/RELEASE_X86_64 x86_64\r\n\r\nInstalled version of CUDA and cuDNN: none\r\n\r\n1. A link to the pip package you installed:\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0-py2-none-any.whl\"\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.11.0\r\n\r\n### If possible, provide a minimal reproducible example \r\n\r\nimport tensorflow as tf\r\nx0 = tf.constant(1, dtype=tf.uint8)\r\nx1 = tf.constant(255, dtype=tf.int64)\r\nx2 = tf.sub(x0, x1)\r\n", "comments": ["The error message isn't saying that the types don't match.  It is saying that the Sub operator doesn't accept uint8 for either operand.\r\n\r\nAdd supports these types: https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/ops/math_ops.cc#L518\r\nSub supports these:\r\nhttps://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/ops/math_ops.cc#L529\r\n\r\nI believe that this is the intended behavior (though I don't know exactly why Sub doesn't support this type  @josh11b Do you know why this is the case?)", "@martinwicke actually added the support for uint8 to Add/Mul/Div (in 109027800 \"Add convert_image op to convert between types for images (similar to OpenCV's cvtScale).\").  Not sure why he didn't also do it for Sub."]}, {"number": 5737, "title": "quantize_graph contrib tool cannot import name load_quantized_ops_so", "body": "Attempting to run quantize graph via \r\n`bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph --input=/path/to/graph --output=/path/to/quantized_graph --output_node_names=\"name\" --mode=eightbit`\r\n\r\nresults in \r\n\r\n`line 41, in <module>\r\n    from tensorflow.contrib.quantization import load_quantized_ops_so\r\nImportError: cannot import name load_quantized_ops_so\r\n`\r\n\r\n### Environment info\r\nOperating System:\r\nMac OS X 10.12.1\r\n\r\nInstalled version of CUDA and cuDNN: \r\n8.0\r\n\r\n`ls -l /usr/local/cuda/lib/libcud*\r\n-rwxr-xr-x  1 root  wheel  13504 Sep 26 17:59 /usr/local/cuda/lib/libcuda.1.dylib\r\nlrwxr-xr-x  1 root  wheel     45 Sep 26 18:00 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a\r\nlrwxr-xr-x  1 root  wheel     50 Sep 26 18:00 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\nlrwxr-xr-x  1 root  wheel     46 Sep 26 18:00 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib\r\nlrwxr-xr-x  1 root  wheel     49 Sep 26 18:00 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a\r\nlrwxr-xr-x  1 root  wheel     47 Nov 15 00:44 /usr/local/cuda/lib/libcudnn.5.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5.dylib\r\nlrwxr-xr-x  1 root  wheel     45 Nov 15 00:44 /usr/local/cuda/lib/libcudnn.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.dylib\r\nlrwxr-xr-x  1 root  wheel     48 Nov 15 00:44 /usr/local/cuda/lib/libcudnn_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn_static.a`\r\nIf installed from binary pip package, provide:\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n`282823b877f173e6a33bbc9d4b9ad7dd8413ada6 (0.11.0 tagged)`\r\n2. The output of `bazel version`\r\n`bazel version\r\nBuild label: 0.4.0-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 19:18:00 2016 (1478114280)\r\nBuild timestamp: 1478114280\r\nBuild timestamp as int: 1478114280\r\n`\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nN/a\r\n\r\n### What other attempted solutions have you tried?\r\nBazel clean and rebuild TF (results in same issue)\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["This appears fixed in master. Thanks - sorry for the noise."]}, {"number": 5736, "title": "Training Cifar10 on two GPUs crashed.", "body": "### Environment info\r\nOperating System:16.04.1 LTS (Xenial Xerus)\r\n\r\nInstalled version of CUDA and cuDNN: 8.0 and 5.0.5\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n-rw-r--r-- 1 root root   558720 Nov 19 10:33 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Nov 19 10:33 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\nInstalled from source:\r\n1. The commit hash: 8157ae2552f4ec031e9f3183e1dede66444320fd\r\n2. The output of `bazel version`:\r\nBuild label: 0.3.1\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\r\nBuild timestamp: 1469783392\r\nBuild timestamp as int: 1469783392\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\npython cifar10_multi_gpu_train.py --num_gpus=2\r\n(Both 'python cifar10_train.py' and 'python cifar10_multi_gpu_train.py --num_gpus=1' work well.)\r\n\r\n### Logs or other output that would be helpful\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:04:00.0)\r\nERROR:tensorflow:Exception in QueueRunner: Expected begin[0] in [0, 32], but got -6\r\n\t [[Node: tower_1/random_crop = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/Cast_1, tower_1/random_crop/mod, tower_1/random_crop/size)]]\r\n\t [[Node: tower_1/Div/_82 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:1\", send_device_incarnation=1, tensor_name=\"edge_131_tower_1/Div\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\nCaused by op u'tower_1/random_crop', defined at:\r\n  File \"cifar10_multi_gpu_train.py\", line 289, in <module>\r\n    tf.app.run()\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv))\r\n  File \"cifar10_multi_gpu_train.py\", line 285, in main\r\n    train()\r\n  File \"cifar10_multi_gpu_train.py\", line 189, in train\r\n    loss = tower_loss(scope)\r\n  File \"cifar10_multi_gpu_train.py\", line 76, in tower_loss\r\n    images, labels = cifar10.distorted_inputs()\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/models/image/cifar10/cifar10.py\", line 156, in distorted_inputs\r\n    batch_size=FLAGS.batch_size)\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/models/image/cifar10/cifar10_input.py\", line 172, in distorted_inputs\r\n    distorted_image = tf.random_crop(reshaped_image, [height, width, 3])\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/random_ops.py\", line 326, in random_crop\r\n    return array_ops.slice(value, offset, size, name=name)\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/array_ops.py\", line 328, in slice\r\n    return gen_array_ops._slice(input_, begin, size, name=name)\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/gen_array_ops.py\", line 2009, in _slice\r\n    name=name)\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/ops.py\", line 2322, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/ops.py\", line 1244, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nOther threads reported similar errors.\r\n\r\nPlease help! I'm looking forward to your reply.\r\nSincerely", "comments": ["This is most likely a compatibility issue between the cifar10 model and the latest source.  (there have been some API changes recently).\r\n\r\n@shlens Could you please take a look?", "Any progress here? I still cannot figure it out. :(", "@Hwhitetooth It's Thanksgiving and so I wouldn't expect any update for a little while.\r\nRather than using the 'live' tree which is in constant flux, you may want to try one of the binary installations.", "@prb12 Thanks for your reply!\r\nYes, I've tried using r0.11 binary. It reported similar errors.", "@shlens Are you working on this?", "I updated this example recently and it should work and actually about 6x faster :-)  Sorry it did not work in the past in this situation.  If you have issues with the latest version let me know.  I also know this issue is crazy old.  "]}, {"number": 5735, "title": "einsum: \"ij,ji->\" rasing error.", "body": "Hi I'm running Linux-64 bit os and I'm getting the following error after running:\r\n\r\n```\r\nimport tensorflow as tf\r\nI = tf.constant([[1,0,0],[0,1,0],[0,0,1]])\r\nII_tr = tf.einsum('ij,ji->',I,I)\r\nwith tf.Session() as sess:\r\n\tres = sess.run(II_tr)\r\n\tprint res\r\n```\r\n`AssertionError: Indices have incorrect format: ij,ji->` Clearly the result here should be 3. \r\n\r\n\r\n", "comments": ["It works in the current version. What version are you using?\n", "I'm using version 0.11.0\n\nI installed it via pip and https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\n", "`tf.einsum` is relatively recent and was not fully baked in `0.11`. Please try a newer version.\n", "Ok I have installed v0.11.0rc2-py27_0 using conda and its still giving me the same error. ", "`0.11rc2` is earlier than `0.11`, you need `0.12`.", "Is 0.12 released yet? I can't get it via anaconda. If not I can work around it. \r\n\r\nThanks", "My bad, not released yet. You could grab `tensorflow/python/ops/special_math_ops.py` from [HEAD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/special_math_ops.py#L96)."]}, {"number": 5734, "title": "Force precision to be float for the MNIST fully connected example.", "body": "Right now, when the example runs precision is always 0 because it computed by dividing two ints and the result is always less than 1.", "comments": ["Can one of the admins verify this patch?\n", "@tiriplicamihai, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @tensorflower-gardener and @keveman to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", " I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please.\n", "PR merged. Thank you, @tiriplicamihai "]}, {"number": 5733, "title": "Segmentation violation error after call to the go._Cfunc_TF_NewSession", "body": "The error repeats locally after manual build of the Tensorflow and in the docker container, based mostly on the [Dockerfile.devel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel)\r\n\r\n\r\n### Exception:\r\n\r\n```\r\nfatal error: unexpected signal during runtime execution                                \r\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x13 pc=0x7f37136ca51f]          \r\n                                                                                       \r\nruntime stack:                                                                         \r\nruntime.throw(0x770629, 0x2a)                                                          \r\n        /usr/local/go/src/runtime/panic.go:566 +0x95                                   \r\nruntime.sigpanic()                                                                     \r\n        /usr/local/go/src/runtime/sigpanic_unix.go:12 +0x2cc                           \r\n                                                                                       \r\ngoroutine 1 [syscall, locked to thread]:                                               \r\nruntime.cgocall(0x6b5900, 0xc420049aa8, 0x7f3700000000)                                \r\n        /usr/local/go/src/runtime/cgocall.go:131 +0x110 fp=0xc420049a78 sp=0xc420049a38\r\ngithub.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_NewSession(0x7f37000008c0, 0x7f3700130a80, 0x7f3700002be0, 0x0)                                                      \r\n        ??:0 +0x4e fp=0xc420049aa8 sp=0xc420049a78                                     \r\ngithub.com/tensorflow/tensorflow/tensorflow/go.NewSession(0xc4200fa010, 0xc420049c90, 0x1, 0x1, 0xc420494060)\r\n        /go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:51 +0x17e fp=0xc420049b30 sp=0xc420049aa8\r\nmain.recognize(0xc4200e4570, 0xc420130044, 0x449f, 0x44b7, 0xc4201344fa, 0x0, 0x1, 0x76aa86, 0x1b)\r\n        /go/src/Cerber/tensorflow.go:46 +0x55e fp=0xc420049d48 sp=0xc420049b30\r\nmain.HandleSendCommand()\r\n        /go/src/Cerber/sendCommand.go:9 +0x239 fp=0xc420049e48 sp=0xc420049d48\r\nmain.mainLoop(0x78f270)\r\n        /go/src/Cerber/main.go:44 +0x2b fp=0xc420049e98 sp=0xc420049e48\r\nmain.main()\r\n        /go/src/Cerber/main.go:28 +0x4a2 fp=0xc420049f48 sp=0xc420049e98\r\nruntime.main()\r\n        /usr/local/go/src/runtime/proc.go:183 +0x1f4 fp=0xc420049fa0 sp=0xc420049f48\r\nruntime.goexit()\r\n        /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1 fp=0xc420049fa8 sp=0xc420049fa0\r\n\r\ngoroutine 17 [syscall, locked to thread]:\r\nruntime.goexit()\r\n        /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1\r\n\r\ngoroutine 25 [IO wait]:\r\nnet.runtime_pollWait(0x7f37145e4ff8, 0x72, 0x3)\r\n        /usr/local/go/src/runtime/netpoll.go:160 +0x59\r\nnet.(*pollDesc).wait(0xc420057100, 0x72, 0xc420505758, 0xc420016160)\r\n        /usr/local/go/src/net/fd_poll_runtime.go:73 +0x38\r\nnet.(*pollDesc).waitRead(0xc420057100, 0xac3ee0, 0xc420016160)\r\n        /usr/local/go/src/net/fd_poll_runtime.go:78 +0x34\r\nnet.(*netFD).Read(0xc4200570a0, 0xc42052a000, 0x8000, 0x8000, 0x0, 0xac3ee0, 0xc420016160)\r\n        /usr/local/go/src/net/fd_unix.go:243 +0x1a1\r\nnet.(*conn).Read(0xc420028100, 0xc42052a000, 0x8000, 0x8000, 0x0, 0x0, 0x0)\r\n        /usr/local/go/src/net/net.go:173 +0x70\r\ncrypto/tls.(*block).readFromUntil(0xc42023a3f0, 0x7f37145e5218, 0xc420028100, 0x5, 0xc420028100, 0x1)\r\n        /usr/local/go/src/crypto/tls/conn.go:476 +0x91\r\ncrypto/tls.(*Conn).readRecord(0xc420073880, 0x78fa17, 0xc420073988, 0xc42002b800)\r\n        /usr/local/go/src/crypto/tls/conn.go:578 +0xc4\r\ncrypto/tls.(*Conn).Read(0xc420073880, 0xc4201e1000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\r\n        /usr/local/go/src/crypto/tls/conn.go:1113 +0x116\r\nbufio.(*Reader).fill(0xc4203186c0)\r\n        /usr/local/go/src/bufio/bufio.go:97 +0x10c\r\nbufio.(*Reader).Read(0xc4203186c0, 0xc4204fda78, 0x9, 0x9, 0xfef, 0x11, 0x0)\r\n        /usr/local/go/src/bufio/bufio.go:209 +0x1bc\r\nio.ReadAtLeast(0xac1da0, 0xc4203186c0, 0xc4204fda78, 0x9, 0x9, 0x9, 0xc420505c80, 0x50dbce, 0xc420073880)\r\n        /usr/local/go/src/io/io.go:307 +0xa4\r\nio.ReadFull(0xac1da0, 0xc4203186c0, 0xc4204fda78, 0x9, 0x9, 0xc420505c78, 0x457770, 0xc4200001a0)\r\n        /usr/local/go/src/io/io.go:325 +0x58\r\nnet/http.http2readFrameHeader(0xc4204fda78, 0x9, 0x9, 0xac1da0, 0xc4203186c0, 0x0, 0x0, 0x1884012202, 0xc4200f2320)\r\n        /usr/local/go/src/net/http/h2_bundle.go:779 +0x7b\r\nnet/http.(*http2Framer).ReadFrame(0xc4204fda40, 0xc42048c000, 0x0, 0x0, 0x0)\r\n        /usr/local/go/src/net/http/h2_bundle.go:1001 +0xa4\r\nnet/http.(*http2clientConnReadLoop).run(0xc420505f80, 0x78f470, 0xc420020f90)\r\n        /usr/local/go/src/net/http/h2_bundle.go:6004 +0xbb\r\nnet/http.(*http2ClientConn).readLoop(0xc420090ea0)\r\n        /usr/local/go/src/net/http/h2_bundle.go:5937 +0xa6\r\ncreated by net/http.(*http2Transport).newClientConn\r\n        /usr/local/go/src/net/http/h2_bundle.go:5314 +0x709\r\n```\r\n\r\n### Container details:\r\n```\r\n$ go env\r\nGOARCH=\"amd64\" GOBIN=\"\" GOEXE=\"\" GOHOSTARCH=\"amd64\" GOHOSTOS=\"linux\" GOOS=\"linux\" GOPATH=\"/go\" GORACE=\"\" GOROOT=\"/usr/local/go\" GOTOOLDIR=\"/usr/local/go/pkg/tool/linux_amd64\" CC=\"gcc\" GOGCCFLAGS=\"-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build477629842=/tmp/go-build -gno-record-gcc-switches\" CXX=\"g++\" CGO_ENABLED=\"1\"\r\n```\r\n\r\n```\r\n$ bazel version\r\nBuild label: 0.3.2 Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar Build time: Fri Oct 7 17:25:10 2016 (1475861110) Build timestamp: 1475861110 Build timestamp as int: 1475861110\r\n```\r\n\r\n```\r\n$ g++ --version\r\ng++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609 Copyright (C) 2015 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\n```\r\n$ g++ -v\r\nUsing built-in specs.\r\n\r\nCOLLECT_GCC=g++\r\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/5/lto-wrapper                        \r\nTarget: x86_64-linux-gnu                                                               \r\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 5.4.0-6ubuntu1~16.04.4' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-5-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-5-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-5-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu                 \r\nThread model: posix                                                                    \r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) \r\n```", "comments": ["@lukasz-pyrzyk : Could you provide some code snippet to reproduce this? \r\n\r\nLooking at the stacktrace, I don't see anything obvious in the C API or the Go code (since the Go code does seem to ensure that `TF_NewSession` gets all valid, non-NULL arguments. If that's the case, then the segfault would appear to trigger inside the C++ runtime due to some peculiarity of the input its currently getting. Any hints on reproducing this, or any more characteristics about the calling code will greatly help. For example, are you setting `SessionOptions.Target` and if so, what to?", "Also, if possible, can you run with `GOTRACEBACK=crash` (see https://golang.org/pkg/runtime/) so the core dump can be inspected in case reproducing it is hard.", "Hi @asimshankar,\r\n\r\nI tried to reproduce the issue today without success.. I think we can close the issue", "Thanks for following up."]}, {"number": 5732, "title": "infinite symlink expansion detected", "body": "Hi All,\r\njust starting to learn to build this thing, must be some noob error. Any advice would be appreciated.\r\n\r\n```\r\n____Downloading from https://github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz: 50KB\r\nERROR: infinite symlink expansion detected\r\n[start of symlink chain]\r\n/usr/bin/X11\r\n/usr/bin\r\n[end of symlink chain]\r\n.\r\n\r\n____Cloning https://github.com/antirez/linenoise.git: Receiving objects (268 / 393)\r\nERROR: /home/mdupont/.cache/bazel/_bazel_mdupont/cd1c359a0f897a12f2b0754121ddc2b1/external/local_config_cuda/cuda/BUILD:\\\r\n42:12: error globbing [**/*.h]: /home/mdupont/.cache/bazel/_bazel_mdupont/cd1c359a0f897a12f2b0754121ddc2b1/external/loca\\\r\nl_config_cuda/cuda/lib/x86_64-linux-gnu/hdf5/serial/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/\\\r\nlib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib (Too many levels of symbolic\\\r\n links).\r\nERROR: /home/mdupont/.cache/bazel/_bazel_mdupont/cd1c359a0f897a12f2b0754121ddc2b1/external/local_config_cuda/cuda/BUILD:\\\r\n146:12: error globbing [**/*.h]: /home/mdupont/.cache/bazel/_bazel_mdupont/cd1c359a0f897a12f2b0754121ddc2b1/external/loc\\\r\nal_config_cuda/cuda/lib/x86_64-linux-gnu/hdf5/serial/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib\\\r\n/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib/lib (Too many levels of symboli\\\r\nc links).\r\n\r\n____Loading package: @local_config_cuda//cuda\r\nERROR: infinite symlink expansion detected\r\n[start of symlink chain]\r\n/usr/lib/x86_64-linux-gnu/hdf5/serial/lib\r\n/usr/lib/x86_64-linux-gnu/hdf5/serial\r\n[end of symlink chain]\r\n.\r\n\r\n```", "comments": ["These were strange symlinks,\r\n\r\n 1106  ls -latr /usr/bin/X11\r\n 1107  sudo rm /usr/bin/X11\r\n```\r\nls -latr /usr/bin/X11\r\nlrwxrwxrwx 1 root root 1 Apr  1  2014 /usr/bin/X11 -> .\r\n```\r\n\r\n 1108  ls -latr /usr/lib/x86_64-linux-gnu/hdf5/serial/lib\r\n````\r\n ls -latr /usr/lib/x86_64-linux-gnu/hdf5/serial/lib\r\nlrwxrwxrwx 1 root root 1 Apr 23  2016 /usr/lib/x86_64-linux-gnu/hdf5/serial/lib -> .\r\n````\r\n 1109  sudo rm  /usr/lib/x86_64-linux-gnu/hdf5/serial/lib\r\n", "removing those the errors go away\r\n", "i'm getting the same error on my Xubuntu 16.04...i hope deleting the symlinks doesn't screw something else up, here it goes", "yup, deleting the /usr/bin/X11 symlink fixed it", "I also got \"infinite symlink expansion detected\" error. I was creating the build directory inside TF code tree. My problem got solved by moving the build directory out."]}, {"number": 5731, "title": "Hope intermediate state in dynamic_rnn", "body": "To implement a attention machine, I need the intermediate state to calculate the attention, but dynamic_rnn doesn't return the intermediate states but only final state.\r\nHope the dynamic_rnn could return intermediate states so that we can implement a attention machine with dynamic network. ", "comments": ["You can write a variant of the LSTMCell that returns both state tensors as part of the output, if you need both c and h state for each time step.  If you just need the h state, that's the output of reach time step.", "@ebrevdo Thanks for reply. Really helpful.", "@ebrevdo  May I ask a question? For each time step of LSTM there is a memory cell value **_c_**, a hidden state **_h_**, and an output **_y_**. So what you were suggesting is that the **_outputs_** returned by dynamic RNN contain **_h_** rather than _**y**_ for each step? Would that also mean the returned **_output_states_** is actually **_outputs_**[-1]? Thank you. ", "@WolfNiu I can answer your question.\r\nhttp://colah.github.io/posts/2015-08-Understanding-LSTMs/\r\nif you look into the structure of LSTM, you will see that the hidden state **h** and the output **y** is the same, they all come from the same line.\r\nSo actually, there are only two values at each step, h or y and c.", "@ebrevdo \r\n\r\nI tried your suggestion but I ran into such trouble doing it, can you take a look at my stackoverflow post?\r\n\r\nhttps://stackoverflow.com/questions/45528146/tensorflow-create-a-custom-sub-class-of-lstm-cell-with-a-dfferent-call-functi\r\n\r\nThanks", "@Sraw  am I able to assign specific values for lstm gates? for instance always 1 for input and...."]}, {"number": 5730, "title": "tf.nn.elu returns NaN", "body": "I am using a fairly straight-forward FFN with ELU activations, and getting all NaN from the network output.\r\n\r\nThis comes in the 4th training epoch so the data did not have any NaN.\r\n\r\n### Environment info\r\nOperating System: Mac OS X 10.12.1\r\n\r\nInstalled version of CUDA and cuDNN: N/A. I'm using CPU version.\r\n\r\ntensorflow version: 0.11.0rc2\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n    dense_output = tf.contrib.layers.stack(input, \r\n                                           tf.contrib.layers.fully_connected,\r\n                                           [config.hidden_size, config.hidden_size // 2, config.hidden_size // 3, target_size],\r\n                                           activation_fn=tf.nn.elu,\r\n                                           weights_initializer=tf.contrib.layers.xavier_initializer(),\r\n                                           scope='FFN')\r\n\r\n\r\n### Logs or other output that would be helpful\r\n\r\nhttps://gist.github.com/aht/5c719ce162158eaae9beb22555324f9d", "comments": ["I am using MSE loss and the targets do not have NaN. This is evidenced by the fact that the training went thru 3 epochs without any issues.\n\nRemoving `activation_fn=tf.nn.elu` makes the problem goes away. Replacing with tf.nn.relu also works.\n", "I did find that my training data has some NaN. I don't know why it worked for 3 epochs. But we can close this for now."]}, {"number": 5729, "title": "Feature request: Please release an official binary for Raspberry Pi", "body": "The Raspberry Pi is an excellent platform for robotics and automation. The native camera also makes a great combo for portable / embedded / robotics computer vision. Having TensorFlow available on this platform would really open up a lot of options in this context.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nnone\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nRaspbian Jessie on Raspberry Pi 3\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nhttps://github.com/samjabrahams/tensorflow-on-raspberry-pi\r\n\r\nIt only provides a binary for TensorFlow 0.10. I've had lots of issues with it running DNNs trained on 0.11. I need to train on 0.11 because the only fast GPU I have is an Nvidia Pascal chip, and 0.10 doesn't work well on Pascal (due to CUDA version issues).\r\n\r\nI've tried to compile 0.11 following the guide there, but there were a lot of errors.", "comments": ["To add an update to this, I have unofficial binaries through version 1.0.1 [available here](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/tag/v1.0.1).", "@itsmeolivia Any reasons you could provide for closing this issue?", "@petewarden agrees that some sort of official support would be good.  We're currently looking into options for automated testing and builds.", "Additional build options for RPi:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples\r\n\r\nand\r\n\r\n@petewarden just added frozen Mobilenet models here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/tutorials/image_retraining.md#other-model-architectures", "Hi, is there a (supported) way to compile the framework from the source for the RPi, including the Python bindings?", "@previ it would appear that the answer is soon!\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/11675\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/12190", "hey all,\r\n\r\nfirst off thanks for all the work here. I've been struggling to get TF setup on my Pi A+ (armv6), but with the recent work in #12190, i was able to build, install, and run TF. sweet!\r\n\r\ni was curious what type of benchmarks others were seeing though. I am running (through Keras) a very basic predict on a somewhat small CNN, and I am seeing anywhere from 10-40s predict times (not including loading the model, imports, etc). This seems extremely high to me, considering the fact that switching the Keras backend to Theano causes predictions to stick around 5s. (This is the first time i've seen Theano outperform TF)\r\n\r\nIs there perhaps someway to optimize the installation? I've tried overclocking and shrinking/expanding gpu memory, but everything is still 10+. As far as I know, it can't leverage the GPU but my next step is to try building the wheel with gpu support.", "Have you tried running the benchmark_model binary with your model?  The\npython bindings are good for prototyping but the best performance check for\ninference time is benchmark_model.\n\nOn Aug 19, 2017 12:07 AM, \"Matt Feury\" <notifications@github.com> wrote:\n\nhey all,\n\nfirst off thanks for all the work here. I've been struggling to get TF\nsetup on my Pi A+ (armv6), but with the recent work in #12190\n<https://github.com/tensorflow/tensorflow/pull/12190>, i was able to build,\ninstall, and run TF. sweet!\n\ni was curious what type of benchmarks others were seeing though. I am\nrunning (through Keras) a very basic predict on a somewhat small CNN, and I\nam seeing anywhere from 10-40s predict times (not including loading the\nmodel, imports, etc). This seems extremely high to me consider switching\nthe Keras backend to Theano causes predictions to stick around 5s.\n\nIs there perhaps someway to optimize the installation? I've tried\noverclocking and shrinking/expanding gpu memory, but everything is still\n10+. As far as I know, it can't leverage the GPU but my next step is to try\nbuilding the wheel with gpu support.\n\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/5729#issuecomment-323505961>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/ABtim7odRybLFXkJbW-3MSuEaGfHedepks5sZom3gaJpZM4K3eBk>\n.\n", "However we should definitely look into optimizing performance as well.  I'm\nassuming the pip package was built with -c opt?\n\nOn Aug 19, 2017 7:58 AM, \"Eugene Brevdo\" <ebrevdo@google.com> wrote:\n\n> Have you tried running the benchmark_model binary with your model?  The\n> python bindings are good for prototyping but the best performance check for\n> inference time is benchmark_model.\n>\n> On Aug 19, 2017 12:07 AM, \"Matt Feury\" <notifications@github.com> wrote:\n>\n> hey all,\n>\n> first off thanks for all the work here. I've been struggling to get TF\n> setup on my Pi A+ (armv6), but with the recent work in #12190\n> <https://github.com/tensorflow/tensorflow/pull/12190>, i was able to\n> build, install, and run TF. sweet!\n>\n> i was curious what type of benchmarks others were seeing though. I am\n> running (through Keras) a very basic predict on a somewhat small CNN, and I\n> am seeing anywhere from 10-40s predict times (not including loading the\n> model, imports, etc). This seems extremely high to me consider switching\n> the Keras backend to Theano causes predictions to stick around 5s.\n>\n> Is there perhaps someway to optimize the installation? I've tried\n> overclocking and shrinking/expanding gpu memory, but everything is still\n> 10+. As far as I know, it can't leverage the GPU but my next step is to try\n> building the wheel with gpu support.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5729#issuecomment-323505961>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7odRybLFXkJbW-3MSuEaGfHedepks5sZom3gaJpZM4K3eBk>\n> .\n>\n>\n>\n", "I will run those benchmarks later tonight and get back to you. in terms of flags i used to build, i used the `build_raspberry_pi.sh` directly, which translated into\r\n\r\n```\r\nbazel build -c opt --copt=-march=armv6 --copt=-mfpu=vfp \\\r\n  --copt=-funsafe-math-optimizations --copt=-ftree-vectorize \\\r\n  --copt=-fomit-frame-pointer --cpu=armeabi \\\r\n  --crosstool_top=@local_config_arm_compiler//:toolchain \\\r\n  --verbose_failures \\\r\n  //tensorflow/tools/benchmark:benchmark_model \\\r\n  //tensorflow/tools/pip_package:build_pip_package\r\n```", "It could be you should compile with neon optimizations, could be you need\nto configure the number of threads by passing the config object when\ncreating a session.\n\nOn Aug 19, 2017 12:28 PM, \"Matt Feury\" <notifications@github.com> wrote:\n\n> I will run those benchmarks later tonight and get back to you. in terms of\n> flags i used to build, i used the build_raspberry_pi.sh directly, which\n> translated into\n>\n> bazel build -c opt --copt=-march=armv6 --copt=-mfpu=vfp \\\n>   --copt=-funsafe-math-optimizations --copt=-ftree-vectorize \\\n>   --copt=-fomit-frame-pointer --cpu=armeabi \\\n>   --crosstool_top=@local_config_arm_compiler//:toolchain \\\n>   --verbose_failures \\\n>   //tensorflow/tools/benchmark:benchmark_model \\\n>   //tensorflow/tools/pip_package:build_pip_package\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5729#issuecomment-323543044>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_7Pbmt3mULB0L0E_tTQ0en4yOC7ks5sZzdXgaJpZM4K3eBk>\n> .\n>\n", "i will try neon optimizations, but it seems like those are explicitly disabled for armv6 architecture. probably for a decent reason. plan to try some new compilations today and will report back. ", "Indeed you may run into illegal instruction errors. Please report back.\n\nOn Aug 20, 2017 8:00 AM, \"Matt Feury\" <notifications@github.com> wrote:\n\n> i will try neon optimizations, but it seems like those are explicitly\n> disabled for armv6 architecture. probably for a decent reason. plan to try\n> some new compilations today and will report back.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5729#issuecomment-323590374>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8WwAaDWJLfb1eIZdwRQVMXsWidiks5saEn9gaJpZM4K3eBk>\n> .\n>\n", "yup \"illegal instruction\" when running the below :(\r\n\r\n```\r\n  PI_COPTS='--copt=-march=armv6 --copt=-mfpu=neon-vfpv4\r\n  --copt=-U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1\r\n  --copt=-U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2\r\n  --copt=-U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8'\r\n\r\nbazel build -c opt ${PI_COPTS} \\\r\n  --copt=-funsafe-math-optimizations --copt=-ftree-vectorize \\\r\n  --copt=-fomit-frame-pointer --cpu=armeabi \\\r\n  --crosstool_top=@local_config_arm_compiler//:toolchain \\\r\n  --local_resources 2048,.5,1.0 \\\r\n  --verbose_failures \\\r\n  //tensorflow/tools/benchmark:benchmark_model \\\r\n  //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\ni also tried configuring the number of threads, but to no avail (but since Pi A+ is single cpu i didn't expect much there).\r\n\r\nis there anyway to enable the pi gpu? i don't believe it's opencl or cuda compatible, but grasping at straws looking for ways to speed this guy up. not sure what makes theano so much quicker by comparison", "as a further information, I installed on a RPi 3 the framework (1.1) from the  python (2.7) wheel provided here: https://github.com/samjabrahams/tensorflow-on-raspberry-pi\r\n\r\nand I can confirm that python bindings work, but they are about 5x or more slower compared to the \"label_image\" c++ example for RPi (using the same graph and image).\r\n\r\nIt also appears that the framework uses just one core, even if   config=tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4) is passed as config param in tf.Session(), the total CPU usage never exceeds 25%.", "We may need to get c++ protobuf bindings compiled for rpi.  Also the single\nthread use looks like it may be a bug.  Will need Pete to look at that.\n\nOn Aug 20, 2017 2:37 PM, \"Roberto\" <notifications@github.com> wrote:\n\n> as a further information, I installed on a RPi 3 the framework (1.1) from\n> the python (2.7) wheel provided here: https://github.com/\n> samjabrahams/tensorflow-on-raspberry-pi\n>\n> and I can confirm that python bindings work, but they are about 5x or more\n> slower compared to the \"label_image\" c++ example for RPi (using the same\n> graph and image).\n>\n> It also appears that the framework uses just one core, even if\n> config=tf.ConfigProto(intra_op_parallelism_threads=4,\n> inter_op_parallelism_threads=4) is passed as config param in\n> tf.Session(), the total CPU usage never exceeds 25%.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5729#issuecomment-323613600>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6KDny5MQgZNPhjll3_sdOO9a7wuks5saKcLgaJpZM4K3eBk>\n> .\n>\n", "@mattfeury - Can you email me your model and the benchmark_model command line (or however you're running it?). My email is petewarden@google.com.\r\n\r\nI've done [some work with the Pi's GPU in the past](https://petewarden.com/2014/08/07/how-to-optimize-raspberry-pi-code-using-its-gpu/), and it's pretty old and funky. It doesn't support CUDA, OpenCL or any decent programming language, so it's not a great option unfortunately.\r\n\r\nWe do now have [automatic pip wheels being built every night](https://petewarden.com/2017/08/20/cross-compiling-tensorflow-for-the-raspberry-pi/), I'd be interested to get feedback on how well they work for everyone on this thread. For example @previ I believe the ones we create for the Pi 2/3 should be as fast as label_image, since they use NEON instructions, unlike the older versions. Let me know how you get on though.", "Thanks @petewarden. emailed you directly and am checking out the rest of your linked blog posts. i will update the thread here if I'm able to squeak out any performance improvements ", "@petewarden I installed tf from the official pip wheel on a RPi 3.\r\nIt actually works much faster than in my previous tests: I achieved about 8 fps using a retrained mobilenet model (0.25, 128, quantified) from python apis, which is very good.\r\nI noticed that the first couple of runs of the model are quite slow (about 10 times slower) but after that, tf optimizes the run and achieves very good performance, using all of the available resources (4 cores on the RPi).", "@samjabrahams @FlorinAndrei @petewarden I am also happy to contribute to this through my course (Data Science for IoT @Oxford uni). would you recommend a place to start so we can duplicate the existing implementation? I am also interested extending further with non image based use cases (ex sensor fusion, complex event processing etc on tensorflow with embedded devices esp on Pi). One more interest area is exploring synergies between Keras and TF embeded (most of the current course is structured on Keras with TF backend). many thanks for your help", "The latest documentation on cross-compiling for the Pi is here, along with\nlinks to the previous binaries:\nhttps://petewarden.com/2017/08/20/cross-compiling-tensorflow-for-the-raspberry-pi/\n\nWe'll be updating the main TensorFlow site soon, but let me know how you\nget on with these instructions. I've managed to get Keras running together\nwith TF on the Pi too using this approach.\n\nOn Sep 10, 2017 7:59 AM, \"DSIOT\" <notifications@github.com> wrote:\n\n> @samjabrahams <https://github.com/samjabrahams> @FlorinAndrei\n> <https://github.com/florinandrei> @petewarden\n> <https://github.com/petewarden> I am also happy to contribute to this\n> through my course (Data Science for IoT @oxford\n> <https://github.com/oxford> uni). would you recommend a place to start so\n> we can duplicate the existing implementation? I am also interested\n> extending further with non image based use cases (ex sensor fusion, complex\n> event processing etc on tensorflow with embedded devices esp on Pi). One\n> more interest area is exploring synergies between Keras and TF embeded\n> (most of the current course is structured on Keras with TF backend). many\n> thanks for your help\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5729#issuecomment-328324086>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAJ2s-lUaaPTofJXOv0rHw6EfJX4XqQUks5sg4jPgaJpZM4K3eBk>\n> .\n>\n", "@petewarden many thanks. This is very useful for my course as a base to start. shall share back also", "@petewarden another thought .. as I understand it .. TF embedded/ TF mobile is designed to work with a pre trained model and extend it(a form of transfer learning). This appears to be mostly for images. My qs is .. is the idea being explored to use other models in this way ex for non image? many thanks rgds ajit\r\n\r\nPS to elaborate(updated) applications like these which are mostly time series IoT sensors but with the main motivation that they do not send all the data to the cloud. This means mostly some form of LSTM in first guess. but was curious if the TF embedded community is exploring also non image based apps on the edge **Real-Time Data Reduction at the Network Edge of Internet-of-Things Systems**\r\nhttps://pdfs.semanticscholar.org/5186/642aeda8afad207b13fbf9fd52f23d73c039.pdf", "I am hoping to extend the simple speech recognition example here to use transfer learning:\r\nhttps://www.tensorflow.org/versions/master/tutorials/audio_recognition\r\n\r\nI have also been collaborating with Nic Lane (currently at UCL, but moving to Oxford next month) about getting some accelerometer models into TensorFlow, so I'd love to chat more about what's possible here.", "This is great Peter, I am based in London but I teach in Oxford .. happy to sync up with Nic, Shall send more offline, thanks for your help and insights :) ", "@petewarden I am currently, and happily, using the pip package for rpi, both for retraining and using the model. Are there any plans on delivering a python 3 (3.4) tensorflow pip distribution? Thanks!", "I've actually been experimenting with getting a Python3 version of the pip installation working. You can try the current version here:\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-pi-python3/\r\n\r\nIt's still a work in progress though, so I would expect some issues installing it, but let me know what errors you hit.", "Installation went fine, but when:\r\n```\r\n>>> import tensorflow\r\n```\r\nI get:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.4/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\nImportError: dynamic module does not define init function (PyInit__pywrap_tensorflow_internal)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.4/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\nImportError: dynamic module does not define init function (PyInit__pywrap_tensorflow_internal)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "Can you try with python2?\n\nOn Fri, Oct 6, 2017 at 3:08 PM, Roberto <notifications@github.com> wrote:\n\n> Installation went fine, but when:\n>\n> >>> import tensorflow\n>\n> I get:\n>\n> Traceback (most recent call last):\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n>     from tensorflow.python.pywrap_tensorflow_internal import *\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n>     _pywrap_tensorflow_internal = swig_import_helper()\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n>   File \"/usr/lib/python3.4/imp.py\", line 243, in load_module\n>     return load_dynamic(name, filename, file)\n> ImportError: dynamic module does not define init function (PyInit__pywrap_tensorflow_internal)\n>\n> During handling of the above exception, another exception occurred:\n>\n> Traceback (most recent call last):\n>   File \"<stdin>\", line 1, in <module>\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/__init__.py\", line 24, in <module>\n>     from tensorflow.python import *\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\n>     from tensorflow.python import pywrap_tensorflow\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\n>     raise ImportError(msg)\n> ImportError: Traceback (most recent call last):\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n>     from tensorflow.python.pywrap_tensorflow_internal import *\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n>     _pywrap_tensorflow_internal = swig_import_helper()\n>   File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n>   File \"/usr/lib/python3.4/imp.py\", line 243, in load_module\n>     return load_dynamic(name, filename, file)\n> ImportError: dynamic module does not define init function (PyInit__pywrap_tensorflow_internal)\n>\n>\n> Failed to load the native TensorFlow runtime.\n>\n> See https://www.tensorflow.org/install/install_sources#common_installation_problems\n>\n> for some common reasons and solutions.  Include the entire stack trace\n> above this error message when asking for help.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5729#issuecomment-334881574>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1UAHi-z5ur_x2zlQ_kEfrwAxV5-ks5spqTSgaJpZM4K3eBk>\n> .\n>\n", "python2 works.", "To be more specific, during installation I got lots of warnings and messages like this:\r\n```\r\n    In file included from /usr/include/python3.4m/pyport.h:328:0,\r\n                     from /usr/include/python3.4m/Python.h:50,\r\n                     from numpy/core/src/umath/scalarmath.c.src:12:\r\n    /usr/include/arm-linux-gnueabihf/bits/mathcalls.h:115:1: note: expected \u2018long double *\u2019 but argument is of type \u2018npy_longdouble *\u2019\r\n     __MATHCALL (modf,, (_Mdouble_ __x, _Mdouble_ *__iptr)) __nonnull ((2));\r\n     ^\r\n    arm-linux-gnueabihf-gcc: numpy/core/src/umath/umathmodule.c\r\n```\r\nor\r\n```\r\n     int casinhl (void);\r\n         ^\r\n    _configtest.c:2:5: warning: conflicting types for built-in function \u2018casinl\u2019\r\n```", "I think that we just don't compile the blackberry build for python3 right\nnow.  We'd need to have separate builds; and probably pete warden is\nlooking into that now.\n\nOn Fri, Oct 6, 2017 at 3:13 PM, Roberto <notifications@github.com> wrote:\n\n> To be more specific, during installation I got lots of warnings and\n> messages like this:\n>\n>     In file included from /usr/include/python3.4m/pyport.h:328:0,\n>                      from /usr/include/python3.4m/Python.h:50,\n>                      from numpy/core/src/umath/scalarmath.c.src:12:\n>     /usr/include/arm-linux-gnueabihf/bits/mathcalls.h:115:1: note: expected \u2018long double *\u2019 but argument is of type \u2018npy_longdouble *\u2019\n>      __MATHCALL (modf,, (_Mdouble_ __x, _Mdouble_ *__iptr)) __nonnull ((2));\n>      ^\n>     arm-linux-gnueabihf-gcc: numpy/core/src/umath/umathmodule.c\n>\n> or\n>\n>      int casinhl (void);\n>          ^\n>     _configtest.c:2:5: warning: conflicting types for built-in function \u2018casinl\u2019\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5729#issuecomment-334882646>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6z5eQKtLtkWNLhkGtgdmLzXlWlbks5spqYngaJpZM4K3eBk>\n> .\n>\n", "I am working on getting Python3 builds going, and you can see the latest versions here:\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-pi-python3/\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-pi-zero-python3/\r\n\r\nThese are still works-in-progress though.", "@petewarden I read your tutorial related to GPU accelerated image processing on pi , but is there any possibilities of utilizing GPU for tensorflow image classification? and if yes than how . Thanks for great article ", "It's been taking me about 2 days for each tensorflow raspberry build, thank you very much for this work :)", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this bug as fixed, since we now have fairly stable Raspberry Pi builds available from:\r\n\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-pi/\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-pi-zero/\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-pi-python3/\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-pi-zero-python3/\r\n\r\n", "@petewarden Assume you're new to TensorFlow. Look at the Install page on the main TF website https://www.tensorflow.org/install/ How would you figure out there is actually an officially supported release for RPi?\r\n\r\nThis issue was created in the hope that the RPi version would gain the same status as the other versions. This is not the case as of now. It seems to me like the definition of \"fixed\" is being stretched a little.\r\n\r\nBTW, I do appreciate that binary builds are available. I'm just worried that they will eventually disappear if they are left in this second-class citizen status for too long.", "Hi - thanks for creating builds for Raspberry Pi - I'm from RPF and have a few questions:\r\n\r\n- why are they tagged with the `any` platform rather than `linux_armv6l` and `linux_armv7l`?\r\n- are you able to support the Python 3 version that ships with Raspbian, i.e. 3.5?\r\n\r\nAlso, I maintain the [piwheels](https://www.piwheels.org/) project (we build and host Armv6/7 wheels for Raspberry Pi of everything on PyPI - this) - would you be able to send new releases to me for inclusion on there? Alternatively we could build ourselves, like we do for opencv, provided the instructions are kept up-to-date."]}, {"number": 5728, "title": "Get \"argv\" error when trying to run mnist_softmax.py example", "body": "Hi,\r\n\r\nI am trying to learn tensorflow from the tutorial on [MNIST](https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/index.html). I already have tensorflow installed on my system through conda and have been able to import tensorflow without any problems. Interaction with my GPU is fine when I ran code samples from research paper repositories.\r\n\r\nTo do so, I cloned the entire tensorflow repo into a local directory, and went into the tutorials/mnist directory. When I try to run the mnist_softmax.py example, I run into the error I have posted below.\r\n\r\n`$ python mnist_softmax.py `\r\n`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally`\r\n`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally`\r\n`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally`\r\n`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally`\r\n`I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally`\r\n`Traceback (most recent call last):`\r\n`  File \"mnist_softmax.py\", line 78, in <module>`\r\n`    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)`\r\n`TypeError: run() got an unexpected keyword argument 'argv'`\r\n\r\nSearching stackoverflow, I haven't found much helpful posts on this subject. The two posts I found were http://stackoverflow.com/questions/40513466/tensorflow-retrain-py-app-run-got-unexpected-keyword-argument-argv and http://stackoverflow.com/questions/40357548/inception-v3-guide-on-tensorflow-broken-for-c-and-python, but they are not really solutions to this problem.\r\n\r\nSo far I can't think of any reason why this would not work, except that maybe I need to compiler and build Tensorflow from source instead of just installing the binaries via conda.\r\n\r\nIf anyone could help with this, it would be really appreciated.\r\n\r\nThank you.\r\n", "comments": ["Try checking out the version of mnist_softmax.py at the branch of the version you have installed.  E.g., if you have r0.11 installed, check out the version of mnist_softmax.py in the r0.11 branch of git.  That should hopefully solve your problems.  Please ping this thread if you feel there's something more to this.\n", "Thank you very much @vrv , this solves it!\n"]}, {"number": 5727, "title": "Fix for the code in the tf.cond example.", "body": "Small problem, however without the fix it creates some confusion when copy/pasting code to the datalab Notebook (or any other place).", "comments": ["Can one of the admins verify this patch?\n", "@b0noI, thanks for your PR! By analyzing the history of the files in this pull request, we identified @yuanbyu, @keveman and @sherrym to be potential reviewers.\n", "Jenkins, test this please\n"]}, {"number": 5726, "title": "Don't imply top_k is nondifferentiable", "body": "`top_k` is in the [`Evaluation`](https://www.tensorflow.org/versions/master/api_docs/python/nn.html#evaluation) section of the documentation, which says\r\n\r\n> The evaluation ops are useful for measuring the performance of a network. Since they are\r\n> nondifferentiable, they are typically used at evaluation time.\r\n\r\nThis is confusing, since `top_k` is differentiable.  Pointed out by @nmduc: https://github.com/tensorflow/tensorflow/issues/288#issuecomment-261703608.", "comments": ["Hello, it looks like the latest docs (API r0.12) still has the old documentation that implies that `top_k` is non-differentiable.  See [docs](https://www.tensorflow.org/api_docs/python/nn/evaluation)", "They are fixed at HEAD: https://www.tensorflow.org/versions/master/api_docs/python/nn/evaluation."]}, {"number": 5725, "title": "add Pillow installation into Dockerfile for fixing scipy.misc.imread \u2026", "body": "@caisq this is a new PR related to [PR #5693 install Pillow in Dockerfile](https://github.com/tensorflow/tensorflow/pull/5693) and [PR #5711 add Pillow into docker build file](https://github.com/tensorflow/tensorflow/pull/5711). 2 changes are incorprated:\r\n1. add Pillow installation into tensorflow/tools/docker/Dockerfile\r\n2. add Pillow installation into tensorflow/tools/docker/Dockerfile.gpu\r\n\r\nthe following information for image size comparison:\r\n\r\n**gpu version image size drops, i guess latest-gpu tagged image is too old.**\r\n\r\n> REPOSITORY                     TAG                 IMAGE ID            CREATED             SIZE\r\n> gcr.io/tensorflow/tensorflow   gpu                 8ca00ec56643        8 minutes ago       2.658 GB\r\n> gcr.io/tensorflow/tensorflow   latest-gpu          dd645f420f1d        10 days ago         2.713 GB\r\n\r\n**cpu version image size increases about 22MB < 2.5% up**\r\n\r\n> REPOSITORY                     TAG                 IMAGE ID            CREATED             SIZE\r\n> gcr.io/tensorflow/tensorflow   Pillow              dff6ac3189bd        17 hours ago        981.6 MB\r\n> gcr.io/tensorflow/tensorflow   latest              5547120ff897        10 days ago         959.6 MB\r\n\r\nHao", "comments": ["@foreverfaint, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @caisq and @vrv to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "Thanks for doing this again, @foreverfaint \n", "PR merged. Thank you, @foreverfaint \n", ":) \n"]}]