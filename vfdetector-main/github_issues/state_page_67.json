[{"number": 26493, "title": "Be able to save a stateful operation", "body": "**System information**\r\n- TensorFlow version (you are using): tf 1.13\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrent behavior is the saver will crash trying to save a stateful operation.\r\nFeature behavior is the saver will have an additional option to save stateful operation with the known limitation that you will loose the state of your operation.\r\n\r\nThis could be useful if you want to save the state of your dataset iterator but your pipeline contains stateful operation (random crop for example).\r\n\r\n**Will this change the current api? How?**\r\nYes, by adding a new option?\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to save a stateful operation.\r\n\r\n**Any Other info.**\r\n", "comments": ["Sorry, not my department. Shivani, this sounds like an Iterator saving request. Are you the right person to triage those issues?"]}, {"number": 26447, "title": "Add SAMD21 or SAMD51 TensorFlow Lite for Microcontrollers", "body": "A widely used platform is the SAMD21 and SAMD51 family with a large number of development boards\r\nhttps://www.adafruit.com/product/4064\r\nhttps://www.adafruit.com/product/2772\r\nhttps://www.sparkfun.com/products/13672\r\n\r\nSupport:\r\n- C++11\r\n- Arduino\r\n- Circuitpython\r\n- Makecode\r\n- Rust\r\n\r\nthey should be considered to be ported\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Lite\r\n- Are you willing to contribute it (Yes/No): I can help with test\r\n\r\nThanks!\r\n", "comments": ["Adafruit have an example https://github.com/adafruit/Adafruit_TFLite_Micro_Speech", "Thanks for the pointer. I don't think that we have enough bandwidth to cover this board. Since Adafruit already their work open sourced. Are you interested in contributing this work to TF? Thanks,", "Moving to contributions welcome."]}, {"number": 26409, "title": "[TF 2.0] tf.summary should be easier to use with graphs", "body": "This feature request tracks improving the usability of tf.summary in TF 2.0 when used with graphs - specifically with `tf.function` and legacy graph mode.\r\n\r\nCurrently there are a number of interrelated limitations that make using tf.summary somewhat awkward and error-prone outside of eager mode:\r\n\r\n- In legacy graph mode, the writer must be configured in advance\r\n  - a writer resource handle must be created via `create_file_writer()` before any graph construction happens, or all summary-writing functions become no-ops\r\n  - note that resource initialization itself can be deferred (by calling `writer.init()` later), but all options to initialization, in particular the logdir, still must be passed earlier to `create_file_writer()`\r\n  - this means it's not possible to create a graph first, and then at execution time decide on the logdir it should emit summaries to; you need to define the logdir and then define the graph\r\n\r\n- In tf.functions it's a similar story\r\n  - the default writer is captured at graph construction time, aka the first function execution\r\n  - if later executions of the function use the same trace, they will still reflect the default writer from the first execution, rather than picking up any new default writer that may exist\r\n  - again, this means that you can't easily change the logdir in use at execution time; you would need to force the function to be re-traced to pick up the new default writer\r\n\r\n- tf.function has the additional complication that it can't own any state\r\n  - this means any writer created inside the function must be assigned a reference outside the function in order to still exist when the function is actually executed (since otherwise it's automatically deleted at the end of the trace and no longer exists at graph execution time)\r\n  - so it's not currently possible for a tf.function to have its own internal-only summary writer (akin to how they cannot currently have function local tf.Variables)\r\n  - furthermore, any mistakes here tend to generate opaque \"Resource not found\" errors that don't really communicate what the issue might be\r\n  - the safest approach is to always create the writer outside the tf.function and then just \"re-enter\" it within the tf.function via `with writer.as_default()`, and make sure the writer object exists as long as the tf.function is being used\r\n\r\n- The step and recording condition (`tf.summary.record_if()`) have milder but similar issues\r\n  - they are also captured at graph definition time\r\n  - however, they have a better workaround: set them to be either a `tf.Variable` or a placeholder (`tf.compat.v1.placeholder` for legacy graph mode, or a function argument for `tf.function`) and then set that value when executing the graph\r\n", "comments": ["@nfelt can you please guide I want to work on this.", "Unlike [`tf.summary.FileWriter`](https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter), TF 2.0's [`tf.summary.create_file_writer`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary/create_file_writer) doesn't accept a `Graph` object. Is there a different way of visualizing a custom model's graph in TensorBoard with TF 2.0? #1961 doesn't clear things up for me since it's only concerned with Keras models which seem to take care of passing the graph to TensorBoard automatically.\r\n\r\nSay I have a top-level `@tf.function`-decorated function that repeatedly constructs a Bayesian neural network and evaluates its performance for different choices of parameters, where would I add `tf.summary.trace_on(graph=True, profiler=False)` and `tf.summary.trace_export(name=\"bnn\", step=0)` and/or `@tf.function` or similar to ensure TensorBoard has access to the model's graph?\r\n\r\n```py\r\ndef build_bnn(weights_list, biases_list, activation=tf.nn.relu):\r\n    def model(X):\r\n        net = X\r\n        for (weights, biases) in zip(weights_list[:-1], biases_list[:-1]):\r\n            net = dense(net, weights, biases, activation)\r\n        # final linear layer\r\n        net = tf.matmul(net, weights_list[-1]) + biases_list[-1]\r\n        pred = net[:, 0]\r\n        std_dev = net[:, 1]\r\n        scale = tf.nn.softplus(std_dev) + 1e-6  # ensure scale is positive\r\n        return tfd.Normal(loc=pred, scale=scale)\r\n\r\n    return model\r\n```\r\n\r\nIf I try to insert `tf.summary.trace_on(graph=True, profiler=False)` anywhere in `build_bnn` I get a warning\r\n> W0423 13:13:04.606260 4439250368 tf_logging.py:161] Cannot enable trace inside a `tf.function`.", "Any progress on this?", "@nfelt We are checking to see if you still need help on this issue . could you please check with latest released `TF2.7` and let us know if the issue still persists in newer versions. Thanks!", "This is pretty much all still relevant in current TF, yes."]}, {"number": 26408, "title": "[TF 2.0] tf.summary built-in support in Estimator", "body": "This feature request tracks making the Estimator summary-writing logic provide built-in support for the TF 2.0 tf.summary.scalar(), image(), etc. ops.\r\n\r\nBackground: the TF 1.x summary ops are supported automatically in Estimator via use of the `tf.estimator.RunConfig` option for `save_summary_steps`, which uses `tf.estimator.SummarySaverHook` under the hood. This means that Estimator model code can invoke `tf.summary.scalar()` and it will Just Work - the framework takes care of collecting the summary at the correct step or time interval and writing it out to the model output directory.\r\n\r\nCurrently, to use TF 2.0 summary ops, one needs to set up the summary writer, step tracking, etc. logic by hand.", "comments": ["Hello @nfelt, \r\nIs it still the case in the freshly released TensorFlow 2.0?\r\nIf yes, do you have an example of a minimal estimator (or link to documentation) which saves custom scalar summary?\r\n\r\nThanks in advance", "@nfelt We are checking to see if you still need help on this issue . could you please check with latest released `TF2.7` and let us know if the issue still persists in newer versions. Thanks!", "@kumariko this is more of a feature request than a bug, so yes it will persist in all versions unless it's implemented."]}, {"number": 26407, "title": "[TF 2.0] tf.summary step argument should cast to int64", "body": "The TF 2.0 versions of tf.summary.scalar(), image(), etc now require a `step` argument at the callsite. This step argument should allow at least any integral type and probably also floating point types (at least if they have no fractional part), but it currently requires a python integer or a `tf.int64` tensor.\r\n\r\nThis results in errors like:\r\n\r\n```python\r\ntf.summary.scalar('foo', 22, step=tf.constant(1))\r\n...\r\nValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'tf.Tensor(1, shape=(), dtype=int32)'\r\n```\r\n\r\n```python\r\ntf.summary.scalar('foo', 22, step=1.0)\r\n...\r\nTypeError: Cannot convert provided value to EagerTensor. Provided value: 1.0 Requested dtype: int64\r\n```", "comments": ["please can i contribute to this @nfelt. Can you guide me", "Hi @nfelt! This issue seems to be resolved in latest stable version TF 2.6 , Attaching [Gist](https://colab.research.google.com/gist/mohantym/c38f275170bae273e96bfb235abdfb0d/github_26407.ipynb) for reference.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26407\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26407\">No</a>\n", "No, this isn't resolved. The error won't happen unless a summary is actually written, which requires that there be a summary writer enabled. I omitted that from the issue description for brevity, but here is a complete repro in TF 2.6:\r\n\r\n```\r\nimport tensorflow as tf\r\nwith tf.summary.create_file_writer(\"/tmp/foo\").as_default():\r\n    # Raises ValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: <tf.Tensor: shape=(), dtype=int32, numpy=1>\r\n    tf.summary.scalar('foo', 22, step=tf.constant(1))\r\nwith tf.summary.create_file_writer(\"/tmp/foo\").as_default():\r\n    # Raises TypeError: Cannot convert 1.0 to EagerTensor of dtype int64\r\n    tf.summary.scalar('foo', 22, step=1.0)\r\n```"]}, {"number": 26406, "title": "[TF 2.0] tf.summary centralized step management", "body": "This feature request tracks providing a way to centrally control the notion of the \"step\" of a given summary (used e.g. as the x-axis in TensorBoard scalar plots). In TF 1.x there was the global step, but in TF 2.0 it no longer exists and global state is generally discouraged.\r\n\r\nBackground: the TF 2.0 versions of tf.summary.scalar(), image(), etc now require a `step` argument at the callsite. This is because the new ops emit summaries immediately to the default file writer, so there's no subsequent `add_summary(merged_summaries, step=step)` call at which the step can be provided. This is inconvenient when working with nested code in which summaries are written from parts of the code that don't have easy access to a step variable, or when working with frameworks where the logical \"step\" isn't easily accessible to user code.\r\n\r\nNote that there is a provisional `tf.summary.experimental.set_step()` implementation that can be used to set a default step value or variable for all summary writing calls from that thread, but that may be changed or removed at any time because it's unclear if that's the right API for the long term.", "comments": ["Have you considered using a ContextManager for providing the default step to be used? Something very similar to `SummaryWriter.as_default`, in the lines of\r\n```python\r\n  with tf.summary.recording_step(step):\r\n    ...\r\n    tf.summary.scalar(...)\r\n    ...\r\n```\r\nThat seems useful enough (allowing client code to either reuse a \"current\" step, or provide an overload), and similar to `tf.summary.record_if` and the `as_default()`, which the users are already accustomed to.", "@foxik Thanks for the thoughtful input.  I did consider a context manager but in general the context managers make the API a bit cumbersome (since they force extra indentation and can't really be activated/deactivated flexibly), so since this `get/set_step()` API is very much provisional I opted to keep it simple.  (Note that's possible to implement a context manager API based on the get/set API, but hackier to go the other direction.)\r\n\r\nI'm actually planning on rethinking how the step management works a little more broadly, and making steps associated more directly with writers rather than a separate concept.  Do you have specific use cases for setting the step in just a local context, but not via passing it explicitly to e.g. `tf.summary.scalar(..., step=mystep)`?", "Thanks for the quick answer. Yes, I understand that implementing a context manager is more tricky, considering that also tracing should work nicely (currently I would assume the `{get,set}_step` is converted to reading/writing `global_step`).\r\n\r\nFor setting steps in local contexts, I sometimes report results in `optimizer.iterations` for training, but `epochs` for dev/val/test. If the metrics is computed by a function, it can be given the `step` and it will work nicely for both optimizer iterations and epochs, but explicitly threading the `step` arguments seems cumbersome (manually passing steps around seems similar to manually passing summary writers, which we do not require the user to do).\r\n\r\nBut the connection of step and summary writer seems like a good idea. I could imagine the `SummaryWriter.as_default(step=None)` method which can be given a default `step` value. In this case whoever wants to set default number of steps need to know the summary writer to which it should be applied, but it does not seem like a big restriction (and they can get the default SummaryWriter anyway).", "Any progress on this?\r\n\r\n> Background: the TF 2.0 versions of tf.summary.scalar(), image(), etc now require a step argument at the callsite. (...) This is inconvenient when working with nested code in which summaries are written from parts of the code that don't have easy access to a step variable, ...\r\n\r\nI'm in that exact situation and agree that `SummaryWriter.as_default(step=None)` seems like a good solution here.", "This is still the case in TF 2.1.", "What's still the case? You mean this isn't implemented yet?", "Yes, I meant nothing changed in TF 2.1 compared to TF 2.0 (as far as I know).\r\n\r\nPersonally I would really like an optional `step` argument to `SummaryWriter.as_default` and `set_as_default`. It would be also simple to implement. I will consider sending a pull request.", "I created a PR with the `step` argument of `SummaryWriter.as_default`. For symmetry, I added it also to `SummaryWriter.set_as_default`.", "With the #36839 merged, I believe this can be closed.\r\n\r\nCheers.", "@nfelt,\r\n\r\nCan you take a look at the above comment by @foxik  and confirm if we are good to close this issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "It's true that #36839 landed - thanks @foxik for the contribution - but there's still a couple points that seem unresolved to me:\r\n\r\n- The considerations discussed on the PR, i.e. https://github.com/tensorflow/tensorflow/pull/36839#issuecomment-652204270 - right now the added APIs still change the global step, but making the step truly per-writer would be IMO a better-encapsulated approach\r\n- We should resolve the status of the experimental get/set step APIs; either deprecating them in favor of the new ones, or promoting them to non-experimental and just having the writer-associated and non-writer-associated ways to set the step. This is somewhat dependent on the bullet above.\r\n\r\nSo I'd suggest leaving the issue open for those two follow-ups."]}, {"number": 26321, "title": "LookupError: No gradient defined for operation 'MatrixLogarithm'", "body": "**System information**\r\n- OS Platform and Distribution :Linux Ununtu 14.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0/7.4.2\r\n- GPU model and memory: GeForce GTX 1080/ 8G\r\n\r\n**Describe the current behavior**\r\nThere is no gradient defined for tf.linalg.logm(), therefore it could not be used in training procedures.\r\n", "comments": ["Hello\uff0c I wonder if this issue has already been solved?\r\nHowever, I found that if the input matrix is the symmetric matrix, then we could use the following way to replace tf.linalg.logm():\r\n\r\n```\r\ndef logm(A):\r\n    e, v = tf.linalg.eigh(A)\r\n    e = tf.linalg.diag(tf.log(e))\r\n    return tf.matmul(v, tf.matmul(e, tf.linalg.inv(v)))\r\n```\r\n\r\nand this operation is derivable.", "@ZhangXiao96,\r\nSorry for the delayed response. Can you please let us know if this issue is resolved so that we can close it? Thanks! ", "Not the OP, but tf 2.4.1 still lacks gradients for matrix logarithm:\r\n```\r\n>>> mat = tf.Variable([[5,2,3],\r\n...                    [2,9,4],\r\n...                    [3,2,6.]], dtype = tf.complex64)\r\n>>> with tf.GradientTape() as tape:\r\n...   loss = tf.reduce_sum(tf.linalg.logm(mat))\r\n... \r\n>>> grad = tape.gradient(loss, mat)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/evgeniizh/.conda/envs/lstf/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\", line 1080, in gradient\r\n    flat_grad = imperative_grad.imperative_grad(\r\n  File \"/home/evgeniizh/.conda/envs/lstf/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py\", line 71, in imperative_grad\r\n    return pywrap_tfe.TFE_Py_TapeGradient(\r\n  File \"/home/evgeniizh/.conda/envs/lstf/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\", line 151, in _gradient_function\r\n    grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access\r\n  File \"/home/evgeniizh/.conda/envs/lstf/lib/python3.9/site-packages/tensorflow/python/framework/registry.py\", line 98, in lookup\r\n    raise LookupError(\r\nLookupError: gradient registry has no entry for: MatrixLogarithm\r\n```\r\n", "Issue still persists in 2.6.0 and nightly. Here's the [gist](https://colab.research.google.com/gist/sanatmpa1/74dd60a85de8e6ecdc823e6bb99d4b9a/26321.ipynb)"]}, {"number": 26315, "title": "error: call of overloaded 'ResizeBilinearOpModel(<brace-enclosed initializer list>)' is ambiguous", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n-  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS - docker images tensorflow/tensorflow:latest and tensorflow/devel\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: commit a2bb5db1bf7931b0dc2cd08e53b8798489568198\r\n- Python version: Python 2.7.12\r\n- Installed using virtualenv? pip? conda?: sources\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen building tensorflow/lite/kernels:resize_bilinear_test error occurs because there's ambiguity in constructor arguments and as the result which constructor to be used.\r\n```\r\ntensorflow/lite/kernels/resize_bilinear_test.cc:403:58: error: call of overloaded 'ResizeBilinearOpModel(<brace-enclosed initializer list>)' is ambiguous\r\n   ResizeBilinearOpModel m({TensorType_INT8, {1, 2, 2, 2}});\r\n                                                          ^\r\ntensorflow/lite/kernels/resize_bilinear_test.cc:29:12: note: candidate: tflite::{anonymous}::ResizeBilinearOpModel::ResizeBilinearOpModel(const tflite::TensorData&, std::initializer_list<int>)\r\n   explicit ResizeBilinearOpModel(const TensorData& input,\r\n            ^\r\ntensorflow/lite/kernels/resize_bilinear_test.cc:27:7: note: candidate: tflite::{anonymous}::ResizeBilinearOpModel::ResizeBilinearOpModel(const tflite::{anonymous}::ResizeBilinearOpModel&) <deleted>\r\n class ResizeBilinearOpModel : public SingleOpModel {\r\n       ^\r\ntensorflow/lite/kernels/resize_bilinear_test.cc:27:7: note: candidate: tflite::{anonymous}::ResizeBilinearOpModel::ResizeBilinearOpModel(tflite::{anonymous}::ResizeBilinearOpModel&&) <deleted>\r\nTarget //tensorflow/lite/kernels:resize_bilinear_test failed to build\r\n```\r\nIt's explained on [stack overflow](https://stackoverflow.com/questions/48011854/deleting-overloaded-function-c11-call-of-overloaded-is-ambiguous/48012069#48012069) and [second point of the document](https://timsong-cpp.github.io/cppwp/n3337/dcl.fct.def.delete)\r\n\r\nIt can be solved by changing places like:\r\n```\r\n ResizeBilinearOpModel m({TensorType_FLOAT32, {1, 2, 1, 1}});\r\n```\r\nto \r\n```\r\n ResizeBilinearOpModel m(TensorData({TensorType_FLOAT32, {1, 2, 1, 1}}));\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel test --config=opt tensorflow/lite/kernels:resize_bilinear_test\r\n\r\n**Any other info / logs**\r\nBelow is a simplified code showing the same problem as occurring inside the test:\r\n```\r\nstruct Pair{\r\n\tint i;\r\n\tint j;\r\n\tPair(int i, int j = -1): i(i), j(j) {};\r\n};\r\n\r\nstruct B {\r\n\tB(const B&) = delete;\r\n\texplicit B(const Pair& p, int j = 0) {\r\n\t\tcout << p.i << \" \" << p.j << \" \" << j << endl;\r\n\t}\r\n};\r\n\r\nint main() {\r\n\tB b({1, 2});\r\n}\r\n```\r\nIt also causes:\r\n```\r\ndelete.cpp: In function \u2018int main()\u2019:\r\ndelete.cpp:32:12: error: call of overloaded \u2018B(<brace-enclosed initializer list>)\u2019 is ambiguous\r\n  B b({1, 2});\r\n            ^\r\ndelete.cpp:20:11: note: candidate: B::B(const Pair&, int)\r\n  explicit B(const Pair& p, int j = 0) {\r\n           ^\r\ndelete.cpp:19:2: note: candidate: B::B(const B&) <deleted>\r\n  B(const B&) = delete;\r\n  ^\r\n```\r\nIt doesn't matter that copy constructor is deleted as this resolution takes place later.", "comments": ["@shashishekhar you've recently worked on this code. Is it tested on some internal Google compiler which behaves differently form gcc on official TF docker image?\r\nIf yes are there any plans to apply in the future compiler compatible with gcc?"]}, {"number": 26278, "title": "tf.image.crop_and_resize() - weird alignment behavior?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):n/a\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):b'v1.13.0-rc2-0-gc865ec5621' 1.13.0-rc2\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version:n/a\r\n- GPU model and memory:n/a\r\n\r\nThis is a follow up on https://github.com/tensorflow/tensorflow/issues/6720#issuecomment-468830039 about `tf.image.crop_and_resize`. (cc @martinwicke )\r\n\r\nSuppose I have an image that looks like this:\r\n```\r\n[[ 0.  1.  2.  3.  4.]\r\n [ 5.  6.  7.  8.  9.]\r\n [10. 11. 12. 13. 14.]\r\n [15. 16. 17. 18. 19.]\r\n [20. 21. 22. 23. 24.]]\r\n```\r\nI wanted to crop the 2x2 patch that contains `[[6, 7], [11, 12]]`, and upsample it to 4x4. I expect to get the following outputs:\r\n```\r\n[[ 4.5  5.   5.5  6. ]\r\n [ 7.   7.5  8.   8.5]\r\n [ 9.5 10.  10.5 11. ]\r\n [12.  12.5 13.  13.5]]\r\n```\r\nI think this is a reasonable expectation. The above output, is also what I got if I do \"resize_and_crop\" instead of `tf.image.crop_and_resize`, __after the fix__ https://github.com/tensorflow/tensorflow/commit/371c96df55a7b23eb8d8496fb477b473fd137fcc yesterday that addressed the alignment issues in resize op.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.ops.image_ops_impl import resize_images_v2\r\narr = np.arange(25).astype('float32').reshape(5, 5)\r\ninput4D = tf.reshape(arr, [1, 5, 5, 1])\r\nresize = resize_images_v2(input4D, [10, 10], method='bilinear')[0,:,:,0]   # resize\r\nprint(resize[2:6,2:6])  # crop\r\n# print expected output\r\n```\r\nSee a Colab proof in https://colab.research.google.com/drive/1ojDErHyG_4v3vwdi3xYwpdtyxShYM9a6#scrollTo=-T1zLhI5uumV\r\n\r\nOK, what is the correct \"boxes\" I should provide for `crop_and_resize`, in order to get the above output?\r\nHere is what the document says:\r\n\r\n> boxes: A Tensor of type float32. A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values.\r\n\r\nIt turns out, that the correct \"boxes\" I should use, is: `[3/16, 3/16, 9/16, 9/16]`. If you cannot tell why it is 3/16 and 9/16 from the above documentation, you and I are on the same page:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\n\r\n# want to crop 2x2 out of a 5x5 image, and resize to 4x4\r\nimage = np.arange(25).astype('float32').reshape(5, 5)\r\ntarget = 4\r\nprint(tf.image.crop_and_resize(\r\n    image[None, :, :, None],\r\n    np.asarray([[3/16,3/16,9/16,9/16]]), [0], [target, target])[0][:, :, 0])\r\n# print expected output\r\n```\r\n\r\nThe crop_and_resize function has weird alignment issues like those fixed in #6720 . It's less of a problem than #6720 because at least we can provide some box coordinates to make it work as expected, and you can say it's just how this function is defined. \r\nThere is actually [a formula](https://github.com/tensorpack/tensorpack/blob/1cdd2e9efb6ffe3066a010e677ef9e76547faaa3/examples/FasterRCNN/modeling/model_box.py#L104-L135) that I use in my code to compute the coordinates in order to use this function. \r\n\r\nBut I do hope this function can have a better-defined behavior and fit reasonable expectation. In my experiments this ill-posed behavior actually hurt my models (which I believe also hurt other models like TF's object detection).", "comments": ["I am not sure, how upsampling\r\n```\r\n[[6, 7],\r\n [11, 12]]\r\n```\r\nwith interpolation would get you to\r\n```\r\n[[ 4.5  5.   5.5  6. ]\r\n [ 7.   7.5  8.   8.5]\r\n [ 9.5 10.  10.5 11. ]\r\n [12.  12.5 13.  13.5]]\r\n```\r\nwhere there are values outside the source values.", "It was not outside the source values because the source is the 5x5 image.", "Ah, thanks, I thought the crop happens before resize.", "@johnpjf The `crop_and_resize` op also has the half-pixel issue, which can be seen at https://github.com/tensorflow/tensorflow/blob/c95e5bcd98b35c2688577262038da645a2088de4/tensorflow/core/kernels/crop_and_resize_op.cc#L237-L248\r\n\r\nSince you fixed this issue in the resize ops, I'm wondering whether this op is on your radar and is there a plan to fix it soon?", "Hi @ppwwyyxx ,\r\n\r\nMay I ask why the expected output is\r\n```\r\n[[ 4.5  5.   5.5  6. ]\r\n[ 7.   7.5  8.   8.5]\r\n[ 9.5 10.  10.5 11. ]\r\n[12.  12.5 13.  13.5]]\r\n```\r\n\r\nI executed your code:\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\nfrom tensorflow.python.ops.image_ops_impl import resize_images_v2\r\narr = np.arange(25).astype('float32').reshape(5, 5)\r\ninput4D = tf.reshape(arr, [1, 5, 5, 1])\r\nresize = resize_images_v2(input4D, [10, 10], method='bilinear')[0,:,:,0]   # resize\r\nprint(resize[2:6,2:6])  # crop\r\n# print expected output\r\n```\r\n\r\nThe \"resize\" tensor is:\r\n```\r\n[[ 0.   0.5  1.   1.5  2.   2.5  3.   3.5  4.   4. ]\r\n [ 2.5  3.   3.5  4.   4.5  5.   5.5  6.   6.5  6.5]\r\n [ 5.   5.5  6.   6.5  7.   7.5  8.   8.5  9.   9. ]\r\n [ 7.5  8.   8.5  9.   9.5 10.  10.5 11.  11.5 11.5]\r\n [10.  10.5 11.  11.5 12.  12.5 13.  13.5 14.  14. ]\r\n [12.5 13.  13.5 14.  14.5 15.  15.5 16.  16.5 16.5]\r\n [15.  15.5 16.  16.5 17.  17.5 18.  18.5 19.  19. ]\r\n [17.5 18.  18.5 19.  19.5 20.  20.5 21.  21.5 21.5]\r\n [20.  20.5 21.  21.5 22.  22.5 23.  23.5 24.  24. ]\r\n [20.  20.5 21.  21.5 22.  22.5 23.  23.5 24.  24. ]]\r\n```\r\nSo the cropped tensor is:\r\n```\r\n[[ 6.   6.5  7.   7.5]\r\n [ 8.5  9.   9.5 10. ]\r\n [11.  11.5 12.  12.5]\r\n [13.5 14.  14.5 15. ]]\r\n```\r\n\r\nActually, the output above makes sense to me. I'm wondering why this is different from the expected result in your original post. Would you please give me a brief explanation on this?\r\n\r\nThanks so much and happy new year! ", "@ppwwyyxx,\r\nCan you please respond to the above comment? Thanks! ", "@rmothukuru  the above comment is incorrect.\r\n\r\n@lsy323 Unlike what you commented, the output of the code I provide, using a new enough tensorflow, is:\r\n```\r\n[[ 4.5  5.   5.5  6. ]\r\n [ 7.   7.5  8.   8.5]\r\n [ 9.5 10.  10.5 11. ]\r\n [12.  12.5 13.  13.5]]\r\n```\r\nwhich is shown in this colab: https://colab.research.google.com/drive/1ojDErHyG_4v3vwdi3xYwpdtyxShYM9a6?usp=sharing", "@ppwwyyxx I'm using your formula to determine the coordinates to use (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py#L105), however, I'm not able to reproduce your results by using `boxes = tf.convert_to_tensor([[1., 1., 2., 2.]])` to get the patch `[[6,7], [11,12]`.\r\n\r\nHas something changed in the meantime with regard to the alignment behaviour of `tf.image.crop_and_resize()`?", "It prints expected output:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# want to crop 2x2 out of a 5x5 image, and resize to 4x4\r\nimage = np.arange(25).astype('float32').reshape(5, 5)\r\ntarget = 4\r\n\r\ndef transform_fpcoor_for_tf(boxes, image_shape, crop_shape):\r\n    x0, y0, x1, y1 = tf.split(boxes, 4, axis=1)\r\n\r\n    spacing_w = (x1 - x0) / tf.cast(crop_shape[1], tf.float32)\r\n    spacing_h = (y1 - y0) / tf.cast(crop_shape[0], tf.float32)\r\n\r\n    imshape = [tf.cast(image_shape[0] - 1, tf.float32), tf.cast(image_shape[1] - 1, tf.float32)]\r\n    nx0 = (x0 + spacing_w / 2 - 0.5) / imshape[1]\r\n    ny0 = (y0 + spacing_h / 2 - 0.5) / imshape[0]\r\n\r\n    nw = spacing_w * tf.cast(crop_shape[1] - 1, tf.float32) / imshape[1]\r\n    nh = spacing_h * tf.cast(crop_shape[0] - 1, tf.float32) / imshape[0]\r\n\r\n    return tf.concat([ny0, nx0, ny0 + nh, nx0 + nw], axis=1)\r\n\r\nbox = tf.constant([1.0, 1, 3, 3])\r\nbox = tf.reshape(box, (1, 4))\r\nprint(tf.image.crop_and_resize(\r\n    image[None, :, :, None],\r\n    transform_fpcoor_for_tf(box, (5, 5), (4, 4)), [0], [target, target])[0][:, :, 0])\r\n# [[ 4.5  5.   5.5  6. ]\r\n#  [ 7.   7.5  8.   8.5]\r\n#  [ 9.5 10.  10.5 11. ]\r\n#  [12.  12.5 13.  13.5]]\r\n```", "Thanks for the example code!\r\nThe question now is, why the coordinates of the lower right point are defined as `(3, 3)`.\r\nThis seems to be that it should rather be `(2,2)`, if we use zero-indexed coordinates."]}, {"number": 26259, "title": "Any plan to add SoftClipping and SmoothMax", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\nYes.\r\n**Describe the feature and the current behavior/state.**\r\nSoftClipping for activation. [https://en.wikipedia.org/wiki/Activation_function]\r\nSmootMax for pooling. [https://en.wikipedia.org/wiki/Smooth_maximum]\r\n**Will this change the current api? How?**\r\nNo. This is new feature.\r\n**Who will benefit with this feature?**\r\nEverybody. \r\nSmoothClipping is smoot function and can not suffer from vanishing/exploding gradients. \r\nSo with this we will have faster learning. https://github.com/tiny-dnn/tiny-dnn/pull/1014.  Only 10 layers achieve 99.35% on MNIST set, Comparing to original 12 and 99.0%\r\n**Any Other info.**\r\n", "comments": ["@dmilos,\r\nSorry for the delayed response. [Relu Activation Function](https://arxiv.org/pdf/1505.00853.pdf%C3%A3%E2%82%AC%E2%80%9AReLU) seems to be good for handling the problem of **`Vanishing and Exploding Gradients`**.\r\n\r\nCan you please let us know how efficient are the **`activation functions`** that you proposed compared to the existing **`non-saturated activation functions`** like [Relu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu), [Elu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/elu), [Selu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/selu)? Thanks!", "SmoothClipping is smooth function and can not suffer from vanishing/exploding gradients.\r\nSo with this we will have faster learning. tiny-dnn/tiny-dnn#1014. Only 10 layers achieve 99.35% on MNIST set, Comparing to original 12 and 99.0%", "For more details in depth for  Soft Clipping see:\r\nNeural Network-Based Approach to Phase Space Integration\r\nM. D. Klimek1,2*, M. Perelstein1\r\n1 Laboratory for Elementary Particle Physics, Cornell University, Ithaca, NY, USA\r\n2 Department of Physics, Korea University, Seoul, Republic of Korea\r\n* klimek@cornell.edu\r\nAugust 18, 2020\r\n\r\n"]}, {"number": 26172, "title": "[Feature Request, INQ] Add functionality for INQ implementation", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nMy suggestion is related with the [article](https://arxiv.org/pdf/1702.03044.pdf) about Incremental Network Quantization (INQ) method of weights quantization.\r\n\r\nFirstly for INQ implementation it is necessary to create integer convolution operation and matrix multiplication operation with bit shift instead of multiplication. I also want to create an API for weights partitions with certain choices of elements (for example, the absolute value maximum).\r\n\r\nSuggested API will be low-level. High-level API will be a next step of my contributing. It is also interesting to add this feature to TFLite.\r\n\r\n**Will this change the current api? How?**\r\nYes, There will be added new functions for convolution and matmul with bit shift.\r\n\r\n**Who will benefit with this feature?**\r\nThe INQ method reduces the weight of the model, and also performs a bit shift instead of floating point multiplication.\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 26168, "title": "Tensorflow not working with vGPUs (when the machine is not in pass-through mode)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **Binary**\r\n- TensorFlow version: **1.12.0**\r\n- Python version: **3.6.8**\r\n- Installed using virtualenv? pip? conda?: **Conda**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: **CUDA 9.2; CuDNN 7.3**\r\n- GPU model and memory: **NVIDIA Tesla V100 (16 GB)**\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI was using TF for experimenting with seq2seq models on a VM that had access to a Tesla V100 GPU (in pass-through mode). Recently, that VM was restricted to use a vGPU (with 50% of the memory - Q8 variant of V100), and hence it was not is pass-through mode anymore. With this change, Tensorflow stopped working, and gave `UNKNOWN_CUDA_ERROR`.\r\n\r\nI would like to know if Tensorflow could be used with Virtual GPUs (not in pass-through mode).\r\nIf yes, any helpful resource for achieving the same would be much appreciated.\r\n\r\nThanks.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.test.is_gpu_available())\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nAttempting to fetch value instead of handling error\r\nInternal: failed initializing StreamExecutor for CUDA Device Ordinal 0\r\n```\r\n\r\n*(Currently, I do not have the full traceback unfortunately).*", "comments": ["I think this should be redirected to GPU or GPU performance team, seems like a low level GPU driver issue?", "@jvishnuvardhan @tatianashp Hey guys. Any update on this would be really useful."]}, {"number": 26133, "title": "Java Tensorflow save & restore without using filesystem", "body": "**System information**\r\n- TensorFlow version (you are using): Up to date\r\n- Are you willing to contribute it (Yes/No): \r\n\r\n\r\n**Describe the feature and the current behaviour/state.**\r\nI previously opened a feature ticket and asking for a **python** approach.\r\nhttps://github.com/tensorflow/tensorflow/issues/25839\r\n\r\nI wasn't aware there is **java tensorflow**.\r\n\r\nI would like to retrieve(instead of Save) training information and restore it to a Tensorflow session when necessary (without using a file system). I want to bypass filesystem.\r\n\r\n### I want to know a line of code or method that I can use to extract training data(as an object) and import it to (tf)session when necessary without me handling file path etc\r\n\r\nto elaborate =>\r\nI found that there is a way to retrieve updated weight values.\r\nhttps://www.quora.com/How-do-I-print-weights-of-a-fully-connected-neural-network-in-tensorflow\r\n//train your network here\r\nsess.run(optimizer, feed_dict={x:data, y:labels}\r\nprint(\"layer 1 weights:\", sess.run(W1))\r\nprint(\"layer 2 weights:\", sess.run(W2))\r\n\r\n**Rather than handling each individual tensor information, I want to extract the whole training information (as an object) and import it to (tf)session when necessary.**\r\n\r\nI have looked at the followings and searched but was not successful.\r\nhttps://www.tensorflow.org/guide/saved_model\r\nhttps://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model\r\n\r\nWhy does this feature not appear already present?\r\n\r\n**Will this change the current API? How?**\r\nno, I just need(or to know) an additional API.\r\n\r\n**Who will benefit with this feature?**\r\nIt will make the use of Tensorflow lib much more flexible.\r\n**Any Other info.**", "comments": ["@allenlavoie @jvishnuvardhan   Thank you for working on this feature! I really appreciate it \ud83d\udc4d "]}, {"number": 26096, "title": "Kenlm in tf.nn.ctc_beam_search_decoder", "body": "Please integrate Kenlm feature in tf.nn.ctc decoder. Because it is very hard to build tensorflow bindings from source. There are always issue with bazel version not matching with tf version.If every thing goes fine still there are issues such as missing libraries.So there should be some easy way to add a language model scoring with ctc_decoder.", "comments": ["@brevdo, If you don't mind, could I have a try for this? And hope you could give me some reference, Thanks.", "yes of course.\r\nMozilla Deep speech model have customized the tensorflow. This version supports building binaries for Kenlm language model scoring for ctc decoder.\r\nhttps://github.com/mozilla/DeepSpeech/blob/master/native_client/README.md\r\nSimilary, facebook wav2letter++ also have these binaries for c++ but they are hard to build.\r\nhttps://github.com/facebookresearch/wav2letter/tree/master/src/decoder\r\nlast\r\nNvidia OpenSeq2Seq toolkit also gives the building instructions for ctc_decoder with language model\r\nhttps://nvidia.github.io/OpenSeq2Seq/html/installation.html\r\n", "@SibtainRazaJamali, Thanks for all your reference, I'll talk with you if I met some problems."]}, {"number": 26060, "title": "tf-lite: allocateTensors never returns", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- I Have written custom code inspired from the tf-lite demo https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo. The code and model is here: https://github.com/BorisMansencal/tflite_test0\r\n- OS Platform and Distribution: I build with up-to-date Android Studio 3.3.1 on macOS 10.13.6\r\n- Mobile device: tested both on Nokia 7 Plus (TA-1046) with Android 9/API 28, and Motorola Moto G5S (XT1794) with Android 8.1.0/API 27.\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): I use tensorflow-lite:0.0.0-nightly\r\n\r\n**Describe the current behavior**\r\n\r\nMy code uses tf-lite for image segmentation.\r\nIt works correctly on Nokia 7 Plus (TA-1046) with Android 9/API 28.\r\nIt does not work on Motorola Moto G5S (XT1794) with Android 8.1.0/API 27. \r\n\r\n**Describe the expected behavior**\r\n\r\nMy code is doing image segmentation using a U-Net DNN architecture.  \r\nIt is inspired from the the tf-lite demo example. It returns an image, instead of a classification.\r\n\r\nThe code is available here: https://github.com/BorisMansencal/tflite_test0\r\nWhen run on on mobile device, you should see an image and a \"run\" button. You click on the \"run\" button and when the inference is finished, you should see an image : with white where the object was detected, black elsewhere.\r\n\r\nIt works correctly on Nokia 7 Plus (TA-1046) with Android 9/API 28, but not on Motorola Moto G5S (XT1794) with Android 8.1.0/API 27.\r\n\r\nIt should work on any device with API 26 Android version.\r\n\r\n**Code to reproduce the issue**\r\nhttps://github.com/BorisMansencal/tflite_test0\r\n\r\n**Other info / logs**\r\n\r\nWhen I run the application in the debugger, from Android Studio, on the Motorola Moto G5S , I can see that the call to the Interpreter constructor (called at ImageClassifier.java:59) never returns.\r\nIf I step into the Interpreter constructor, the problem seems to come from void NativeInterpreterWrapper::init(long errorHandle, long modelHandle, Options options): the call to allocateTensors(this.interpreterHandle, errorHandle) never returns.\r\n\r\n\r\n", "comments": ["I have also checked the same code on Motorola Moto G6 with Android 8.0.0/API 26, and I have the same issue.", "I have updated the code on the repository to be able to use it on Android 7/API 24.\r\nSo far, I have the following results:\r\n\r\n | smartphone model | android version/API level | result |\r\n | -------------------- | -------------------- | ------------- |\r\n | Nokia 7 Plus (TA-1046) | Android 8.1/API 27 | OK |\r\n | Nokia 7 Plus (TA-1046) | Android 9/API 28 | OK |\r\n | Nokia 6.1 (TA-1043) | Android 9/API 28 | OK |\r\n | Samsung Galaxy S7 Edge | Android 7/API 24 | OK |\r\n | Motorola Moto G5S (XT1794) | Android 8.1/API 27 | **NOK** |\r\n | Motorola Moto G6 | Android 8.0/API 26 | **NOK** |\r\n\r\n\r\n", "Sorry did you reach a resolution on this issue?"]}, {"number": 26033, "title": "[TF2.0] Change default types globally", "body": "Hello everyone,\r\n\r\nI made the same request a while ago at [tensorflow/community](https://github.com/tensorflow/community/issues/23). The similar question was raised before at [tensorflow/tensorflow#9781](https://github.com/tensorflow/tensorflow/issues/9781), where maintainers argued that GPU is much faster on float32, default type cannot (should not) be changed because of backwards compatibility reasons and cetera.\r\n\r\nThe thing is that the precision is very important for algorithms where operations like cholesky, solvers and etc. are used. This becomes very tedious to specify type everywhere, it gets even worse when you start using other frameworks or small libraries which follow the standard type settings and sometimes they become useless, just because type incompatibilities. The policy of **\"changing types locally solves your problems\"** becomes cumbersome.\r\n\r\nIt would be great to have methods `tf.set_default_float` and `tf.set_default_int` in TensorFlow 2.0 and I believe that such a small change will make TensorFlow more user friendly.\r\n\r\nKind regards,\r\nArtem Artemev", "comments": ["@reedwm @azaks2 I wonder if we can piggy back on the fp16 work to make it easier to use fp64 across the board?", "The fp16 API we are working on will only support Keras layers and models. Instead, perhaps [floatx](https://www.tensorflow.org/api_docs/python/tf/keras/backend/floatx) should be used as the default dtype for such algorithms, although I believe currently floatx only affects tf.keras.\r\n\r\n/CC @fchollet, what do you think about using floatx?", "@alextp , @reedwm Hello! By `tf.set_default_float`, I meant that the method will change default types for variables creation and tensors. E.g.:\r\n\r\n```python\r\ntensor1 = tf.Variable(v)\r\ntf.set_default_float(tf.float64)\r\ntensor2 = tf.Variable(v) \r\ntensor3 = tf.Variable(v, dtype=tf.float16)\r\n\r\n# tensor1 has fp32\r\n# tensor2 has fp64\r\n# tensor3 has fp16\r\n```\r\n\r\n\r\n\r\n", "@alextp, @reedwm Hello everyone! ooc, does anyone work on this feature, or it is postponed until better times? :)", "No current work that I am aware of.\n\nOn Thu, Apr 4, 2019 at 3:14 AM Artem Artemev <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp>, @reedwm <https://github.com/reedwm>\n> Hello everyone! ooc, does anyone work on this feature, or it is postponed\n> until better times? :)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26033#issuecomment-479836374>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxcFZOt_YW-lGXxNsF4ReoXpzERWxks5vddCUgaJpZM4bN3xC>\n> .\n>\n\n\n-- \n - Alex\n", "You're right, we cannot use floatx for backwards compatibility, since currently it does not affect the dtypes of `tf.Variable`s and such.\r\n\r\nThis proposal sounds reasonable. It would be convenient to be able to have all variables/constants in a user-chosen dtype without having to specify the `dtype` parameter for each. I do have several concerns however.\r\n\r\n1. It is confusing what the difference between `set_default_float` and `floatx` are. Floatx only affects `tf.keras`, while `set_default_float` would affect functions outside `tf.keras`. It is likely users will get them mixed up.\r\n2. It is confusing what exact `set_default_float` would affect. It probably should affect `tf.Variable` and `tf.constant` because they create tensors, and should probably not affect `tf.add`, etc. But what about functions like `tf.ones_like`? They arguably create a tensor from scratch, but they also take a tensor as input.\r\n3. We are currently working on a mixed precision API. Currently this only affects tf.keras, but nothing is set in stone at this point. The mixed precision API will create variables, but not necessarily constants, in float32, so it's unclear how it would interact with `set_default_float`. Since the mixed precision API fundamentally just changes the precision of tensors, it may be unclear to users what the difference between `set_default_float` and the mixed precision API are.\r\n\r\n@alextp what do you think? Maybe we should wait until the mixed precision API is further along, so at least we could address (3) with more certainty.", "I think this feature will be very useful for the upcoming tf.keras mixed precision API, so I will revisit this, at least for floating-point types.\r\n\r\nI think it makes sense for `tf.set_default_float` to only affect functions which produce `tf.Tensor` outputs without taking in `tf.Tensor` inputs, such as `tf.ones` and `tf.convert_to_tensor`. It would not affect ops like `tf.ones_like` which take Tensor inputs, unless they were pass non-Tensor inputs.\r\n\r\n @awav, in your example with variables, if `v` were a Tensor, `tf.Variable` would ignore the default float value and use the dtype of `v`. If you walk up `v`'s input chain, some op must have produced `v` without taking a Tensor input, and so that op would have used the default float. Due to dtype propogation, this would make `v` the default float type as well unless a cast were done. If `v` were a non-Tensor, like a float or numpy array, then it would use the default float value.\r\n\r\nAlso, IMO, `set_default_float` and `floatx` should be the same, so that setting one also sets the other. We would have to make `set_default_float` to be thread-local to be compatible with DistributionStrategy, so we would also have to change `floatx` to be thread-local.\r\n\r\n@alextp, @awav, any thoughts?", "Overall looks good. Ideally floatx can just call set_default_float for correctness.", "We would also be very interested in this features. \r\n\r\nSince we use TF to do likelihood fits requiring float64 ([zfit](https://github.com/zfit/zfit)) and allow users to specify their model with TF themselves, we often run into problems where users create (because of the default) a float32 tensor, conflicting with the other float64 tensors. Mostly for TF unaware users a quite annoying thing (and hard for us to catch and explicitly write what to change).\r\n\r\nCurrently, we even started wrapping some of the TF functions to avoid this problem, so a global fix would be highly appreciated (and we may can help implementing it oc).", "@mayou36 thank you for the feedback. I plan to start a design doc in the upcoming weeks.", "@reedwm , \r\n>  in your example with variables, if v were a Tensor, tf.Variable would ignore the default float value and use the dtype of v.\r\n\r\nI agree with it, this is absolutely logical.\r\nI look forward to this feature!", "@reedwm , any updates on that? ", "Not yet sorry! I am currently busy with other tasks, but I hope to have a design doc fairly soon.", "@alextp, @reedwm, it has been a while, no progress with this feature?", "Unfortunately still no update :(. I am working on some mixed precision tasks at the moment. Once the Keras mixed precision API is more complete I can get to this.", "@reedwm any news? I'd be very keen for this to make it into tensorflow - the lack of configurable default dtypes is one of our biggest pain points with tensorflow and makes life difficult and cumbersome for both the developers and users of downstream libraries (such as GPflow)!", "Has there been progress?", "Sorry no updates yet :(", "Any updates on this? I am very excited to see this feature in tensorflow.", "[tf.keras.backend.set_floatx](https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_floatx) seems to be doing this.", "> [tf.keras.backend.set_floatx](https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_floatx) seems to be doing this.\r\n\r\nNot exactly, it works for Keras only and doesn't have any effect on the rest of TensorFlow.", "This would be really helpful. Writing float16 code (if not keras) is filled with boilerplate crap.", "This could be quite helpful, but sorry to see no update", "Three years, and still waiting.", "@reedwm what is the status of this? Any progress, anything we can help with?", "I'd like to chime in as well that this would be a very useful feature. It can be very cumbersome to cast every tensor to the correct dtype manually, especially when using existing data. Let us know how we might be able to help, as a community!"]}, {"number": 26021, "title": "SplitPath should split on both forward slashes and backslashes on Windows", "body": "Currently, on Unix, SplitPath splits only on forward slashes; on\r\nWindows, it splits on forward slashes _unless_ there are no forward\r\nslashes in the string, in which case it splits on backslashes. This is\r\nconfusing, and inconsistent with platform APIs like [`_wmkdir`], which\r\ninterpret both `\\` and `/` as valid path delimiters.\r\n\r\nFor instance, `CreateRecursiveDirectory(r\"foo\\bar/baz\\quux\")` fails on\r\nWindows. This _should_ be broken into `[\"foo\", \"bar\", \"baz\", \"quux\"]`,\r\nbut actually becomes `[\"foo\", \"bar\", \"baz\\quux\"]`. When `CreateDir` is\r\ncalled on each of those components in sequence, it creates `foo`, then\r\ncreates `foo/bar`, then tries to create `foo/bar/baz\\quux`, which fails\r\nbecause the directory `foo/bar/baz` does not exist.\r\n\r\n(Googlers, see <http://b/125762969> for an example of this error.)\r\n\r\nThe whole situation is a bit less clean than one might hope, because the\r\nTF filesystem API prefers to use `/`-separators everywhere while people\r\nin Python-land prefer to use `os.path.join` to insert the proper\r\nplatform-specific path separator. But it seems clear that the current\r\nbehavior, where the delimiter used depends on what characters are in the\r\nstring, is undesirable, and that being consistent with native APIs is a\r\nbetter choice.\r\n\r\n[`_wmkdir`]: https://docs.microsoft.com/en-us/cpp/c-runtime-library/reference/mkdir-wmkdir?view=vs-2017\r\n\r\ncc @nfelt\r\n", "comments": []}, {"number": 25971, "title": "Requirements of SparseTensorDenseMatMul on GPU", "body": "Hi, I'm using SparseTensorDenseMatMul op on GPU. And I noticed that there are some requirements for the sparse matrix.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/9d508106b32eb6518912501d29a80ff9967dfe05/tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc#L96-L114\r\n\r\n\r\nIs this code snippet also unnecessary? Or I just can't use this op on big sparse matrix?\r\n", "comments": ["@ebrevdo Can you explain the reason?", "The reason is that the underlying computation kernel uses int32 indices to perform calculations for performance reasons.  We would need to bifurcate the kernel to have one that uses int32 indices and one that uses int64 indices in order to support bigger matrices.  Notice we use [regular ints](https://github.com/tensorflow/tensorflow/blob/4e15557ac4c92f219f51ed94883d1e47dca88417/tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc) for indexing throughout the GPU computation.  If you have indices larger than this, the computations would overflow.", "So is current best practice to split large sparse matrices into batches?  It's a little trying since indexing into sparse matrices isn't really a thing.  Would be easier if CSR matrices were supported...\r\n\r\nWhat do you recommend in practice?", "So here is what I'm doing right now, which seems goofy but I guess it works.  \r\n\r\n```\r\nclass SparseBinaryMatrixWrapper:\r\n    def __init__(self,Nc,Nt,row,col,batchsize):\r\n        self.Nc=Nc\r\n        self.Nt=Nt\r\n        self.row=row\r\n        self.col=col\r\n        self.batchsize=batchsize\r\n        self.batches=tf.convert_to_tensor(np.r_[0:len(row):batchsize,len(row)])\r\n\r\n    @tf.function\r\n    def matvec(self,vec):\r\n        result=tf.zeros(self.Nc,dtype=vec.dtype)\r\n        assert len(vec)==self.Nt\r\n        for i in range(len(self.batches)-1):\r\n            rows=self.row[self.batches[i]:self.batches[i+1]]\r\n            cols=self.col[self.batches[i]:self.batches[i+1]]\r\n            vals=tf.gather(vec,cols) # <-- 1d array\r\n            result=tf.tensor_scatter_nd_add(result,rows[:,None],vals)\r\n\r\n        return result\r\n\r\n    @tf.function\r\n    def matmul(self,mat):\r\n        result=tf.zeros((self.Nc,mat.shape[1]),dtype=mat.dtype)\r\n        assert mat.shape[0]==self.Nt\r\n        for i in range(len(self.batches)-1):\r\n            rows=self.row[self.batches[i]:self.batches[i+1]]\r\n            cols=self.col[self.batches[i]:self.batches[i+1]]\r\n            vals=tf.gather(mat,cols) # <-- nnz x mat.shape[1]\r\n            result=tf.tensor_scatter_nd_add(result,rows[:,None],vals)\r\n\r\n        return result\r\n\r\n    @tf.function\r\n    def matTmul(self,mat):\r\n        result=tf.zeros((self.Nt,mat.shape[1]),dtype=mat.dtype)\r\n        assert mat.shape[0]==self.Nc\r\n        for i in range(len(self.batches)-1):\r\n            rows=self.row[self.batches[i]:self.batches[i+1]]\r\n            cols=self.col[self.batches[i]:self.batches[i+1]]\r\n            vals=tf.gather(mat,rows) # <-- nnz x mat.shape[1]\r\n            result=tf.tensor_scatter_nd_add(result,cols[:,None],vals)\r\n\r\n        return result\r\n\r\n\r\n    @tf.function\r\n    def matTvec(self,vec):\r\n        result=tf.zeros(self.Nt,dtype=vec.dtype)\r\n        assert len(vec)==self.Nc\r\n        for i in range(len(self.batches)-1):\r\n            rows=self.row[self.batches[i]:self.batches[i+1]]\r\n            cols=self.col[self.batches[i]:self.batches[i+1]]\r\n            vals=tf.gather(vec,rows)\r\n            result=tf.tensor_scatter_nd_add(result,cols[:,None],vals)\r\n        return result\r\n\r\n    @tf.function\r\n    def matTvec(self,vec):\r\n        result=tf.zeros(self.Nt,dtype=vec.dtype)\r\n        assert len(vec)==self.Nc\r\n        for i in range(len(self.batches)-1):\r\n            rows=self.row[self.batches[i]:self.batches[i+1]]\r\n            cols=self.col[self.batches[i]:self.batches[i+1]]\r\n            vals=tf.gather(vec,rows)\r\n            result=tf.tensor_scatter_nd_add(result,cols[:,None],vals)\r\n        return result\r\n\r\n```\r\n\r\nI imagine that some masterful gurus out there will have a better proposal :)."]}, {"number": 25810, "title": "report_tensor_allocations_upon_oom and embedding_lookup together lead to memory leak", "body": "\r\n**Describe the current behavior**\r\nThe RunOptions report_tensor_allocations_upon_oom and embedding_lookup will lead to CPU memory leak.\r\nThis will only happen in tf 1.12, tf 1.10 didn't have this problem.\r\n\r\nI thought `tf.gather` have almost same functionality, so I test it and it seems no problem at all.\r\n\r\nI'm not sure if #21348 is related to this problem. My problem will only occur with runoptions.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nembeddings = tf.reshape(tf.range(1000), shape=(100,10))\r\nids = tf.reshape(tf.range(100), shape=(20,5))\r\n\r\nembedding_lookup = tf.nn.embedding_lookup(embeddings, ids)\r\nga = tf.gather(embeddings, ids)\r\n\r\nwith tf.Session() as sess:\r\n  sess.graph.finalize()\r\n  run_opts = tf.RunOptions(report_tensor_allocations_upon_oom=True)\r\n\r\n  for i in range(10000000):\r\n    sess.run(embedding_lookup, options=run_opts)\r\n    #sess.run(ga, options=run_opts)\r\n```\r\nThe code above leaks cpu memory quite rapidly ~3 Mb/s.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9.0/ 7.4.2\r\n- GPU model and memory: GTX 1070 with 8GB memory", "comments": []}, {"number": 25798, "title": "Speed of benchmark code in CPU windows is much slower than ubuntu", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):0.18\r\n- GCC/Compiler version (if compiling from source): MSVC 14 in windows \r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory:N?A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI have built the tensorflow v1.12. from source using bazel with the following configuration: \r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"C:/Python35/python.exe\"\r\nbuild --action_env PYTHON_LIB_PATH=\"C:/Python35/lib/site-packages\"\r\nbuild --python_path=\"C:/Python35/python.exe\"\r\nbuild:ignite --define with_ignite_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"0\"\r\nbuild --action_env TF_DOWNLOAD_CLANG=\"0\"\r\nbuild:opt --copt=/arch:AVX\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --config monolithic\r\nbuild --copt=-w --host_copt=-w\r\nbuild --verbose_failures\r\nbuild --distinct_host_configuration=false\r\nbuild --experimental_shortened_obj_file_path=true\r\nbuild --define=no_tensorflow_py_deps=true\r\nbuild:v2 --define=tf_api_version=2\r\n```\r\nAfter succesful build I ran the benchmark code provided in \"tensorflow/tools/benchmark/benchmark_model_main.cc\".  But I see the speed in windows is much slower(almost twice) than in ubuntu.In windows the inference time is on average ~25 ms and in ubuntu the timing is ~14ms. I am using the same machine in dual boot mode.\r\n\r\n**Code to reproduce the issue**\r\ntensorflow/tools/benchmark/benchmark_model_main.cc", "comments": ["I have tried to reproduce the results in my end and the timing difference is very much consistent. I have dual-booted my machine with Windows 10 and Ubuntu 16.0. I have listed down the timing that I found for a single forward pass of the network. Timings are calculated by averaging over 1000 iterations. I have used the example code given in the tensorflow repo(tensorflow/examples/label_image/label_image.py). I have used python 3.5 in each of the setting and use the command \"pip3 install --upgrade tensorflow==1.12\" to install tensorflow. I have used the model \"inception_v3_2016_08_28_frozen.pb\" mentioned in the example.\r\n![timing_diffence_tf](https://user-images.githubusercontent.com/16721547/53560237-ede40e00-3b71-11e9-9166-99bd473d53de.JPG)\r\n\r\n\r\nThe code for getting the inference time is as follows and the model can be found by executing following link:\r\nhttps://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport time\r\n\r\ndef load_graph(model_file):\r\n  graph = tf.Graph()\r\n  graph_def = tf.GraphDef()\r\n\r\n  with open(model_file, \"rb\") as f:\r\n    graph_def.ParseFromString(f.read())\r\n  with graph.as_default():\r\n    tf.import_graph_def(graph_def)\r\n\r\n  return graph\r\n\r\n\r\ndef read_tensor_from_image_file(file_name,\r\n                                input_height=299,\r\n                                input_width=299,\r\n                                input_mean=0,\r\n                                input_std=255):\r\n  input_name = \"file_reader\"\r\n  output_name = \"normalized\"\r\n  file_reader = tf.read_file(file_name, input_name)\r\n  if file_name.endswith(\".png\"):\r\n    image_reader = tf.image.decode_png(\r\n        file_reader, channels=3, name=\"png_reader\")\r\n  elif file_name.endswith(\".gif\"):\r\n    image_reader = tf.squeeze(\r\n        tf.image.decode_gif(file_reader, name=\"gif_reader\"))\r\n  elif file_name.endswith(\".bmp\"):\r\n    image_reader = tf.image.decode_bmp(file_reader, name=\"bmp_reader\")\r\n  else:\r\n    image_reader = tf.image.decode_jpeg(\r\n        file_reader, channels=3, name=\"jpeg_reader\")\r\n  float_caster = tf.cast(image_reader, tf.float32)\r\n  dims_expander = tf.expand_dims(float_caster, 0)\r\n  resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\r\n  normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\r\n  sess = tf.Session()\r\n  result = sess.run(normalized)\r\n\r\n  return result\r\n\r\n\r\ndef load_labels(label_file):\r\n  label = []\r\n  proto_as_ascii_lines = tf.gfile.GFile(label_file).readlines()\r\n  for l in proto_as_ascii_lines:\r\n    label.append(l.rstrip())\r\n  return label\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  file_name = \"grace_hopper.jpg\"\r\n  model_file = \\\r\n    \"inception_v3_2016_08_28_frozen.pb\"\r\n  label_file = \"imagenet_slim_labels.txt\"\r\n  input_height = 299\r\n  input_width = 299\r\n  input_mean = 0\r\n  input_std = 255\r\n  input_layer = \"input\"\r\n  output_layer = \"InceptionV3/Predictions/Reshape_1\"\r\n  max_run=1000\r\n\r\n  parser = argparse.ArgumentParser()\r\n  parser.add_argument(\"--image\", help=\"image to be processed\")\r\n  parser.add_argument(\"--graph\", help=\"graph/model to be executed\")\r\n  parser.add_argument(\"--labels\", help=\"name of file containing labels\")\r\n  parser.add_argument(\"--input_height\", type=int, help=\"input height\")\r\n  parser.add_argument(\"--input_width\", type=int, help=\"input width\")\r\n  parser.add_argument(\"--input_mean\", type=int, help=\"input mean\")\r\n  parser.add_argument(\"--input_std\", type=int, help=\"input std\")\r\n  parser.add_argument(\"--input_layer\", help=\"name of input layer\")\r\n  parser.add_argument(\"--output_layer\", help=\"name of output layer\")\r\n  args = parser.parse_args()\r\n\r\n  if args.graph:\r\n    model_file = args.graph\r\n  if args.image:\r\n    file_name = args.image\r\n  if args.labels:\r\n    label_file = args.labels\r\n  if args.input_height:\r\n    input_height = args.input_height\r\n  if args.input_width:\r\n    input_width = args.input_width\r\n  if args.input_mean:\r\n    input_mean = args.input_mean\r\n  if args.input_std:\r\n    input_std = args.input_std\r\n  if args.input_layer:\r\n    input_layer = args.input_layer\r\n  if args.output_layer:\r\n    output_layer = args.output_layer\r\n\r\n  graph = load_graph(model_file)\r\n  t = read_tensor_from_image_file(\r\n      file_name,\r\n      input_height=input_height,\r\n      input_width=input_width,\r\n      input_mean=input_mean,\r\n      input_std=input_std)\r\n\r\n  input_name = \"import/\" + input_layer\r\n  output_name = \"import/\" + output_layer\r\n  input_operation = graph.get_operation_by_name(input_name)\r\n  output_operation = graph.get_operation_by_name(output_name)\r\n  \r\n\r\n  #warm-up run  \r\n  config_proto = tf.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\r\n  sess = tf.Session(graph=graph, config=config_proto)\r\n  \r\n  results = sess.run(output_operation.outputs[0], {input_operation.outputs[0]: t})  \r\n\r\n  avg_inference_time=0.0\r\n  for iter in range(max_run):\r\n      start=time.time()\r\n      results = sess.run(output_operation.outputs[0], {input_operation.outputs[0]: t})  \r\n      inference_time = time.time() - start\r\n      avg_inference_time += inference_time\r\n      print('inference_time: '+str(inference_time))\r\n\r\n  avg_inference_time /= max_run\r\n  print('avg inference time: '+str(avg_inference_time))\r\n```\r\n", "Thank you for reporting and confirming the issue, and I'm sorry for my delay! I'll try to look into this next week!", "Hi, I am facing the same problem. Has anyone solved this?", "Not yet. I'm having my hands full with other issues. Will try to look at this soon or find other people  to do so. Sorry for the delay!"]}, {"number": 25661, "title": "NNAPI does not accept the reshape operation with the -1 parameter", "body": "**System information**\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n* Mobile device: Xiaomi Mi MIX 2S (Android 9, API 28)\r\n* TensorFlow Lite version on Android: 0.0.0-gpu-experimental\r\n\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\ntflite_convert \\\r\n--graph_def_file=tflite_graph.pb \\\r\n--output_file=detect.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='raw_outputs/class_predictions','raw_outputs/box_encodings' \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--mean_values=128 \\\r\n--std_dev_values=128 \\\r\n--change_concat_input_ranges=false \\\r\n--allow_custom_ops\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nI use SSD mobilenet V1 model from [zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md). When I try to use this model in an Android application using NNAPI, I get an error message. The problem is that the Reshape operation has the new shape [1, -1, 91]. Will it be possible to use Reshape with the -1 parameter in NNAPI. And is it possible to change this parameter to static without retraining the model?\r\n", "comments": ["Not sure this is still relevant, as I think this is pre-toco and pre having unknown shape in TF lite. At the least it should be checked."]}, {"number": 25590, "title": "Build Tensorflow version that detects CPU instruction set at runtime and lights-up/down", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12.0 CPU\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently Tensorflow cross-compiles for different instruction sets and will warn if the CPU supports instructions that the TF build does not use, and fail to load if the CPU does not support an instruction set that the TF build uses.  This makes it impossible to build an application that runs on a variety of hardware and ensure that it achieves optimal results for that hardware (or even runs at all).\r\n\r\nI understand that TF supports cross-compilation and developers can build their own library that works best for their hardware, but this doesn't solve the case where an application developer wants to ship an application that uses TF and runs on a variety of hardware.\r\n\r\nI understand that having multiple codepaths with runtime light-up could increase the size of TF, that could be dealt with by making this a separate flavor/configuration of TF, eg: \"portable\" build, and that could be published as a binary zip/tarball along side the current builds.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nApplications and libraries redistributing tensor flow binaries to run on a variety of hardware.\r\n\r\n", "comments": ["Maybe using MKL can help with this.\r\nhttps://github.com/tensorflow/community/blob/master/rfcs/20180604-dynamic-kernels.md will definitely be able to help.\r\nI am hoping to be able to land dynamic kernels this quarter.", "@ericstj,\r\nRFC mentioned in [this comment](https://github.com/tensorflow/community/blob/master/rfcs/20180604-dynamic-kernels.md) has been accepted and the support for **`MKL`** is specified in [Tensorflow Documentation](https://www.tensorflow.org/install/source#preconfigured_configurations). Can you please let us know if the feature you are looking for is implemented? Thanks!", "Is this flavor of TensorFlow produced and published as part of regular releases?  If you have a link to the binaries we could take a look.\r\n\r\nThat would enable projects like https://github.com/SciSharp/TensorFlow.NET  (used by https://github.com/dotnet/machinelearning) to redistribute these.  cc @Oceania2018 @michaelgsharp", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I responded, and asked a question.  I don't believe this should be stalled."]}, {"number": 25495, "title": "Inference speed slows down after using graph transform tool", "body": "**System information**\r\n- Have I written custom code: No, I used tensorflow's object detection api's officially released model `ssd-mobilenet-v1-fpn`.\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): built from source, with xla and mkl support, use nvcc as compiler.\r\n- TensorFlow version (use command below): r1.12.0\r\n- Python version: Python 3.6.4 |Anaconda\r\n- Bazel version (if compiling from source): 0.18.1\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: CUDA 9.0, cuDNN 7.4.2\r\n- GPU model and memory: 3*NVIDIA 1080Ti, \r\n\r\n**Describe the current behavior**\r\nI trained `ssd-mobilenet-v1-fpn` from tensorflow object detection api with default configurations. As its inference was pretty slow on CPU, I checked using tensorflow's profiler advise and found out the main time consumption was from batch normalization(took 140 ms in total).  Therefore, I tried to use [tensorflow's graph transform tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms) to make if faster.\r\n\r\nBut the inference speed actually slowed down after the graph transformation. To be specific, inference takes \r\n* 0.53 seconds for original inference graph\r\n* 0.56 seconds for optimized inference graph\r\n\r\nWhy is the case?\r\n\r\nAn additional information is that the size actually increases too: the original inference graph is 43MB and optimized inference graph is 44MB. Not sure it will be useful.\r\n\r\nHere is the code I used:\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=fpnssd/frozen_inference_graph.pb \\\r\n--out_graph=fpnssd/optimized_frozen_inference_graph.pb \\\r\n--inputs=\"image_tensor:0\" \\\r\n--outputs=\"detection_boxes:0,detection_scores:0,detection_classes:0,num_detections:0\" \\\r\n--transforms=\"strip_unused_nodes() fold_constants(ignore_errors=false) fold_batch_norms fold_old_batch_norms\"\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nI expect it to be quicker with optimized graph.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=fpnssd/frozen_inference_graph.pb \\\r\n--out_graph=fpnssd/optimized_frozen_inference_graph.pb \\\r\n--inputs=\"image_tensor:0\" \\\r\n--outputs=\"detection_boxes:0,detection_scores:0,detection_classes:0,num_detections:0\" \\\r\n--transforms=\"strip_unused_nodes() fold_constants(ignore_errors=false) fold_batch_norms fold_old_batch_norms\"\r\n```", "comments": ["Also used graph-transform on `deeplab-xception_65` and below is a comparison using tensorflow's benchmarking tool.\r\n\r\n|   | original graph  | optimized graph   |\r\n|---|---|---|\r\n|  Timings (microseconds) |avg=120329   |  avg=112269 |\r\n|  Memory (bytes)   |  avg=3.79e+09  |  curr=3789380632(all same)  |\r\n| nodes observed  |  1559  |  1337 |\r\n|  FLOPs estimate | 337.22B  | 337.22B   |\r\n|  FLOPs/second  | 3.35T   |  3.55T |\r\n\r\nSeems to work this time.  \r\n\r\nThe graph transform command I used is \r\n\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n  --in_graph=$GRAPH_PATH/frozen_inference_graph.pb \\\r\n  --out_graph=$GRAPH_PATH/optimized_inference_graph.pb \\\r\n  --inputs=\"ImageTensor:0\" \\\r\n  --outputs=\"SemanticPredictions:0\" \\\r\n  --transforms='remove_device strip_unused_nodes(type=uint8, shape=\"-1,401,531,3\") remove_nodes(op=CheckNumerics, op=Identity) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms'\r\n```", "@yoyolin  you may try to add 'fold_old_batch_norm' transform for xception_65, that will get kind of speed up, maybe 5~10%. However,  when it comes to ssd-mobilenet-fpn, the transform graph tool may broken the model, which lead to wrong result, and I'm working on it. "]}, {"number": 25474, "title": "Core dumped bfc allocator ", "body": "**System information**\r\n- Have I written custom code (yes):\r\n- OS Platform and Distribution (Linux Ubuntu 18):\r\n- TensorFlow installed from (binary):\r\n- TensorFlow version (1.12.0):\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: cuda9/ cudnn7\r\n- GPU model and memory: 1080Ti 11GB\r\n\r\n**Describe the current behavior**\r\nI get the following message during inference on mask rcnn model.\r\n```\r\nfreeMemory: 10.74GiB\r\n2019-02-03 22:26:45.730704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-02-03 22:26:45.988762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-03 22:26:45.988797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2019-02-03 22:26:45.988803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2019-02-03 22:26:45.989126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10363 MB memory) -> physical GPU (device: 0, name: 1080Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2019-02-03 22:32:17.296211: F tensorflow/core/common_runtime/bfc_allocator.cc:380] Check failed: h != kInvalidChunkHandle \r\nAborted (core dumped)\r\n```\r\n\r\n**Describe the expected behavior**\r\nExpected behaviour would have been to run without issues.\r\n\r\n**Code to reproduce the issue**\r\n1. Download project [here](https://github.com/matterport/Mask_RCNN)\r\n2. Run inference from live video feed with batch size = 1, i.e. one image at a time.\r\n\r\n**Other info / logs**\r\nStack trace from gdb\r\n```\r\n[New Thread 0x7fff6bd33700 (LWP 25492)]\r\n[New Thread 0x7fff63532700 (LWP 25493)]\r\n[New Thread 0x7fff6b532700 (LWP 25494)]\r\n[New Thread 0x7fff6ad31700 (LWP 25495)]\r\n[New Thread 0x7fff6a530700 (LWP 25496)]\r\n[New Thread 0x7fff69d2f700 (LWP 25497)]\r\n[New Thread 0x7fff6952e700 (LWP 25498)]\r\n[New Thread 0x7fff68d2d700 (LWP 25499)]\r\n[New Thread 0x7fff63fff700 (LWP 25500)]\r\n[New Thread 0x7fff62d31700 (LWP 25501)]\r\n[New Thread 0x7fff62530700 (LWP 25502)]\r\n[New Thread 0x7fff61d2f700 (LWP 25503)]\r\n[New Thread 0x7fff6152e700 (LWP 25504)]\r\n[New Thread 0x7fff60d2d700 (LWP 25505)]\r\n[New Thread 0x7fff33fff700 (LWP 25506)]\r\n[New Thread 0x7fff337fe700 (LWP 25507)]\r\n[New Thread 0x7fff32ffd700 (LWP 25508)]\r\n[New Thread 0x7fff327fc700 (LWP 25509)]\r\n[New Thread 0x7fff31ffb700 (LWP 25512)]\r\n[New Thread 0x7fff317fa700 (LWP 25513)]\r\n[New Thread 0x7fff30ff9700 (LWP 25514)]\r\n[New Thread 0x7fff11fff700 (LWP 25515)]\r\n2019-02-03 22:50:14.393995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:65:00.0\r\n2019-02-03 22:50:14.394052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-02-03 22:50:14.659094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-03 22:50:14.659133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2019-02-03 22:50:14.659139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2019-02-03 22:50:14.659452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10363 MB memory) -> physical GPU (device: 0, name: 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n[New Thread 0x7fff117fe700 (LWP 25516)]\r\n[New Thread 0x7fff10ffd700 (LWP 25517)]\r\n[New Thread 0x7ffef5fff700 (LWP 25518)]\r\n[New Thread 0x7ffef57fe700 (LWP 25519)]\r\n[New Thread 0x7ffef4ffd700 (LWP 25520)]\r\n[New Thread 0x7ffee1fff700 (LWP 25521)]\r\n[New Thread 0x7ffee17fe700 (LWP 25522)]\r\n[New Thread 0x7ffee0ffd700 (LWP 25523)]\r\n[New Thread 0x7ffebbfff700 (LWP 25524)]\r\n[New Thread 0x7ffebb7fe700 (LWP 25525)]\r\n[New Thread 0x7ffebaffd700 (LWP 25526)]\r\n[New Thread 0x7ffeba7fc700 (LWP 25527)]\r\n[New Thread 0x7ffeb9ffb700 (LWP 25528)]\r\n[New Thread 0x7ffeb97fa700 (LWP 25529)]\r\n[New Thread 0x7ffeb8ff9700 (LWP 25530)]\r\n[New Thread 0x7ffea3fff700 (LWP 25531)]\r\n[New Thread 0x7ffea37fe700 (LWP 25532)]\r\n[New Thread 0x7ffea2ffd700 (LWP 25533)]\r\n[New Thread 0x7ffea27fc700 (LWP 25534)]\r\n[Thread 0x7ffea27fc700 (LWP 25534) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25535)]\r\n[Thread 0x7ffea27fc700 (LWP 25535) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25536)]\r\n[Thread 0x7ffea27fc700 (LWP 25536) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25537)]\r\n[Thread 0x7ffea27fc700 (LWP 25537) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25538)]\r\n[New Thread 0x7ffea27fc700 (LWP 25539)]\r\n[Thread 0x7ffea27fc700 (LWP 25538) exited]\r\n[Thread 0x7ffea27fc700 (LWP 25539) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25540)]\r\n[Thread 0x7ffea27fc700 (LWP 25540) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25541)]\r\n[Thread 0x7ffea27fc700 (LWP 25541) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25560)]\r\n[Thread 0x7ffea27fc700 (LWP 25560) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25562)]\r\n[Thread 0x7ffea27fc700 (LWP 25562) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25563)]\r\n[Thread 0x7ffea27fc700 (LWP 25563) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25565)]\r\n[Thread 0x7ffea27fc700 (LWP 25565) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25566)]\r\n[Thread 0x7ffea27fc700 (LWP 25566) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25567)]\r\n[Thread 0x7ffea27fc700 (LWP 25567) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25568)]\r\n[Thread 0x7ffea27fc700 (LWP 25568) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25569)]\r\n[Thread 0x7ffea27fc700 (LWP 25569) exited]\r\n[New Thread 0x7ffea27fc700 (LWP 25570)]\r\n[New Thread 0x7fff731e6800 (LWP 25571)]\r\n[New Thread 0x7fff72de4880 (LWP 25572)]\r\n[New Thread 0x7fff729e2900 (LWP 25573)]\r\n[New Thread 0x7fff725e0980 (LWP 25574)]\r\n[New Thread 0x7fff721dea00 (LWP 25575)]\r\n[New Thread 0x7fff71ddca80 (LWP 25576)]\r\n[New Thread 0x7fff719db700 (LWP 25577)]\r\n[Thread 0x7fff719db700 (LWP 25577) exited]\r\n[New Thread 0x7fff719db700 (LWP 25578)]\r\n[Thread 0x7fff719db700 (LWP 25578) exited]\r\n[New Thread 0x7fff719db700 (LWP 25579)]\r\n[Thread 0x7fff719db700 (LWP 25579) exited]\r\n[New Thread 0x7fff719db700 (LWP 25580)]\r\n[Thread 0x7fff719db700 (LWP 25580) exited]\r\n[New Thread 0x7fff719db700 (LWP 25581)]\r\n[New Thread 0x7fff719db700 (LWP 25582)]\r\n[Thread 0x7fff719db700 (LWP 25581) exited]\r\n[Thread 0x7fff719db700 (LWP 25582) exited]\r\n[New Thread 0x7fff719db700 (LWP 25583)]\r\n[Thread 0x7fff719db700 (LWP 25583) exited]\r\n[New Thread 0x7fff719db700 (LWP 25584)]\r\n[Thread 0x7fff719db700 (LWP 25584) exited]\r\n[New Thread 0x7fff719db700 (LWP 25588)]\r\n[New Thread 0x7fff704f1700 (LWP 25589)]\r\n[New Thread 0x7fff6fcf0700 (LWP 25590)]\r\n[New Thread 0x7fff6f4ef700 (LWP 25591)]\r\n[New Thread 0x7fff6ecee700 (LWP 25592)]\r\n[New Thread 0x7fff6e4ed700 (LWP 25593)]\r\n[New Thread 0x7fff6dcec700 (LWP 25594)]\r\n[New Thread 0x7fff6d4eb700 (LWP 25595)]\r\n[New Thread 0x7fff6ccea700 (LWP 25596)]\r\n[New Thread 0x7ffea1ffb700 (LWP 25597)]\r\n[New Thread 0x7ffea17fa700 (LWP 25598)]\r\n[New Thread 0x7ffea0ff9700 (LWP 25599)]\r\n[New Thread 0x7ffe8dfff700 (LWP 25600)]\r\n[New Thread 0x7ffe8d7fe700 (LWP 25601)]\r\n[New Thread 0x7ffe8cffd700 (LWP 25602)]\r\n2019-02-03 22:57:34.463561: F tensorflow/core/common_runtime/bfc_allocator.cc:458] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum) \r\n\r\nThread 39 \"python\" received signal SIGABRT, Aborted.\r\n[Switching to Thread 0x7ffeb8ff9700 (LWP 25530)]\r\n0x00007ffff7b3fd7f in raise () from /usr/lib/libc.so.6\r\n```\r\n", "comments": ["@jlebar ", "@tatatodd does bfcallocator fall under your team's purview now?", "Check failed: c->in_use() && (c->bin_num == kInvalidBinNum)\r\n\r\nThis happens when spawning multiple TF process on same GPU,after running fine for few seconds, assigning .4 GPU memory fraction per process In configproto and using TF Object detection API with fasterRCNN.\r\nCan you suggest any fix for this?", "Any updates on this?", "@smit-hinsu will look into this after holidays.", "These failures definitely seem concerning.\r\n\r\nBut, I don't see any instructions for inference on the github page required to reproduce this issue.\r\nhttps://github.com/matterport/Mask_RCNN\r\n\r\nCould you try to come up with a [minimal, complete, verifiable example](https://stackoverflow.com/help/mcve) as described in https://stackoverflow.com/help/mcve to make it easier to reproduce the failure?", "@smit-hinsu There's a demo for inference [here](https://github.com/matterport/Mask_RCNN/blob/master/samples/demo.ipynb). All you have to do is change the `Run Object Detection` cell in order to run inference on live feed from web cam using opencv. \r\n\r\nHere's a simple example:\r\n```\r\ncc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\r\noutput = cv2.VideoWriter('output.avi', cc, 8, (640, 480))\r\nwhile cv2.VideoCapture(0).isOpened:\r\n      flag, frame = cv2.VideoCapture(0).read()\r\n      if not flag:\r\n         break\r\n      img = frame[:, :, ::-1]\r\n      prediction = model.detect([img], verbose=0)\r\n```\r\n", "Thanks for the instructions. These were not mentioned on the repository home page.\r\n\r\nIs it possible to reproduce the issue without any other dependency except TensorFlow? For example, by using fake data. Any other way to reduce the size of the code so that it is minimal and self sufficient which can be shared using https://gist.github.com/?", "It's still an issue, I've tested with no dependencies just importing blank numpy arrays same size as frame from live web cam feed and it doesn't dump the core.\r\nIt dumps the core only when retrieving live feed from web cam. I've also tried different versions of opencv and still the same thing.\r\nHere's a MWE:\r\n```\r\nimport os\r\nimport cv2\r\nimport sys\r\nimport numpy as np\r\nimport mrcnn.utils\r\nimport mrcnn.config\r\nfrom pathlib import Path\r\nfrom mrcnn.model import MaskRCNN\r\nfrom mrcnn import visualize\r\nfrom keras.utils import plot_model\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\r\n\r\n# COCO Class names\r\nclass_names = [\r\n    'BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\r\n    'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\r\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\r\n    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\r\n    'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\r\n    'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\r\n    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\r\n    'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',\r\n    'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant',\r\n    'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\r\n    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\r\n    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\r\n    'hair drier', 'toothbrush'\r\n]\r\n\r\ndetect_names = [\r\n    'backpack', 'umbrella', 'handbag',\r\n    'sports ball', 'bottle', 'cup', 'fork', 'knife', 'spoon',\r\n    'bowl', 'banana', 'apple', 'sandwich', 'orange',\r\n    'donut', 'cake', 'potted plant',\r\n    'mouse', 'keyboard', 'cell phone', 'book', 'clock',\r\n    'vase', 'scissors', 'toothbrush'\r\n]\r\n\r\n\r\n# Configuration that will be used by the Mask-RCNN library\r\nclass MaskRCNNConfig(mrcnn.config.Config):\r\n    NAME = \"coco_pretrained_model_config\"\r\n    IMAGES_PER_GPU = 1\r\n    GPU_COUNT = 1\r\n    NUM_CLASSES = 1 + 80  # COCO dataset has 80 classes + one background class\r\n    DETECTION_MIN_CONFIDENCE = 0.6\r\n\r\n\r\n# Root directory of the project\r\nROOT_DIR = Path(\".\")\r\n# sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))\r\n# import coco\r\n\r\n# Directory to save logs and trained model\r\nMODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\r\n\r\n# Local path to trained weights file\r\nCOCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\r\n\r\n# Download COCO trained weights from Releases if needed\r\nif not os.path.exists(COCO_MODEL_PATH):\r\n    mrcnn.utils.download_trained_weights(COCO_MODEL_PATH)\r\n\r\n# Directory of images to run detection on\r\nIMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\r\n\r\n# Video file or camera to process - set this to 0 to use your webcam instead of a video file\r\nVIDEO_SOURCE = 0\r\n\r\n# Create a Mask-RCNN model in inference mode\r\nmodel = MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=MaskRCNNConfig())\r\n\r\n# Load pre-trained model\r\nmodel.load_weights(COCO_MODEL_PATH, by_name=True)\r\n\r\nplot_model(model.keras_model, to_file='model_concat.png', expand_nested=False)\r\n\r\n# Load the video file we want to run detection on\r\nvideo_capture = cv2.VideoCapture(VIDEO_SOURCE)\r\n\r\n# saving video output\r\n# Define the codec and create VideoWriter object\r\nfourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\r\noutput_video = cv2.VideoWriter('output.avi', fourcc, 8, (640, 480))\r\n\r\n# Loop over each frame of video\r\nwhile video_capture.isOpened():\r\n    success, frame = video_capture.read()\r\n    if not success:\r\n        break\r\n\r\n    # Convert the image from BGR color (which OpenCV uses) to RGB color\r\n    rgb_image = frame[:, :, ::-1]\r\n    #rgb_image = np.random.randn(480, 640, 3)\r\n\r\n    # Run the image through the Mask R-CNN model to get results.\r\n    results = model.detect([rgb_image], verbose=0)\r\n\r\n    # Mask R-CNN assumes we are running detection on multiple images.\r\n    # We only passed in one image to detect, so only grab the first result.\r\n    r = results[0]\r\n\r\n    colors = visualize.random_colors(r['rois'].shape[0])\r\n    for bbox, mask, cls, score, color in zip(\r\n            r['rois'], r['masks'], r['class_ids'], r['scores'], colors):\r\n        y1, x1, y2, x2 = bbox\r\n      \r\n        if class_names[cls] in detect_names:\r\n            text = class_names[cls] + \" {:.2f}%\".format(score * 100)\r\n            cv2.rectangle(frame, (x1, y1), (x2, y2),\r\n                          tuple(map(lambda x: int(x * 255), color)), 1)\r\n            cv2.putText(frame, text, (10, y2), cv2.FONT_HERSHEY_DUPLEX, 3.0,\r\n                        (0, 255, 0), 2, cv2.FILLED)\r\n\r\n            if success is True:\r\n                output_video.write(frame)\r\n\r\n        cv2.imshow('Video', frame)\r\n\r\n    # Hit 'q' to quit\r\n    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n        break\r\n\r\n# Clean up everything when finished\r\nvideo_capture.release()\r\noutput_video.release()\r\ncv2.destroyAllWindows()\r\n```", "I have the same problem... using as well Mask_RCNN with similar code but not running on the webcam, just reader frames from a video", "I run into similar issues for the RetinaNet model with multiple scripts doing inferences. Each script is running on an independent GPU. The script runs for a while (~1h), then suddenly it runs into the error above.", "> I run into similar issues for the RetinaNet model with multiple scripts doing inferences.\r\n\r\nCan you share a reproducer that we can run without a webcam?  And does this reproduce on TF nightly?\r\n\r\nCC @poxvoculi "]}, {"number": 25432, "title": "2.0 Reference Models: NMT Model (TPU with dist strat and Keras)", "body": "**Sequence-to-sequence** (seq2seq) models ([Sutskever et al., 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf), [Cho et al., 2014](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)) have enjoyed great success in a variety of tasks such as machine translation, speech recognition, and text summarization. \r\n\r\nThis model will give TF 2.0 end users a full understanding of seq2seq models and show them how to build a competitive seq2seq model from scratch. We focus on the task of **Neural Machine Translation** (NMT), which was the very first [testbed for seq2seq models](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html).\r\n\r\nFor the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/nmt).\r\n\r\nThe purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.", "comments": ["Any updates? I find that the keras high level API is not flexible enough compared to the old `contrib.seq2seq`. ", "Any updates? +1", "We are revamping the official model garden: more details can be viewed in this RFC https://github.com/tensorflow/community/pull/130. We focus on supporting BERT and its variants for NLP tasks this quarter. We'd like to have NMT but don't have cycles now. Will try it include it in Q4."]}, {"number": 25400, "title": "Support SavedModel in WarmStartSettings initialization", "body": "Any reason not to support SavedModel in WarmStartSettings initialization? Currently only checkpoints are supported.\r\n\r\nEstimator's `warm_start_from` argument accepts a SavedModel, so it would be great to have parity with that for cases where a user needs finer-grained control via WarmStartSettings (e.g., only initializing a subset of variables for transfer learning/fine tuning).", "comments": ["Sounds reasonable to me. Although it should already be possible to point warm start to the `variables/` directory of the SavedModel, no? That's just a regular training checkpoint.", "Digging in, Estimators support SavedModels using the logic @allenlavoie mentions, so it is possible to just point WarmStartSettings at `[saved_model_dir]/variables/variables` (note the extra `variables`) (https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/estimator.py#L2285).\r\n\r\nWarmStartSettings makes an explicit call to checkpoint_utils.init_from_checkpoint (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/warm_starting_util.py#L475), so it seems like the above logic could replace that.", "I opened a small PR https://github.com/tensorflow/estimator/pull/53/files fixing the issue where WarmStartSettings needed to be initialized from `[saved_model_dir]/variables/variables` instead of `[saved_model_dir]/variables` like @allenlavoie suggested. I suspect this may be for v1 compatibility only at this point, but wanted to put it out there in the interest of closing this issue."]}, {"number": 25386, "title": "Add integer data types to IsTrainable for use with custom gradients", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, there are a limited number of DTypes usable with autograd (see `IsTrainable` and `_IsBackpropagatable` below) https://github.com/tensorflow/tensorflow/blob/6631cd5e0e43e672d2de3cf1f0e29da17b57824e/tensorflow/python/ops/gradients_impl.py#L300\r\n\r\nIn particular, nodes with integer tensors automatically generate gradients of None, which breaks any call to `tf.gradients`.  This is a reasonable assumption for usual practices in machine learning, however this prevents one from creating their own autograd system using `tf.custom_gradient`.  I'd contend that, as a general purpose automatic differentiation library, TF should enable experimenting with such uses of its autograd system.  This would be particularly useful when intending to perform automatic differentiation over rings or finite fields, which are often represented in computers as sets of consecutive integers (i.e. `Z_p`).\r\n\r\n**Will this change the current api? How?**\r\nThis change shouldn't affect any standard usage of TensorFlow -- all autograd on floats will be unchanged.  The only change will occur when calling autograd on graphs that compute integer arithmetic and similar.\r\n\r\nCurrently, performing `tf.gradients` on graphs with integer tensors will raise an unhandled error:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx_back = np.ones([2, 2])\r\ny_back = np.ones([2, 2])\r\nx = tf.Variable(x_back, dtype=tf.int32)\r\ny = tf.Variable(y_back, dtype=tf.int32)\r\nz = x + y\r\nvg = tf.gradients([z], [x, y])\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    out = sess.run(vg)\r\n    print(out)\r\n```\r\nReturns:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/jasonmancuso/dropout/research/customgrad/issue_min.py\", line 16, in <module>\r\n    out = sess.run(vg)\r\n  File \"/Users/jasonmancuso/anaconda/envs/tf-encrypted/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/jasonmancuso/anaconda/envs/tf-encrypted/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1137, in _run\r\n    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\r\n  File \"/Users/jasonmancuso/anaconda/envs/tf-encrypted/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 471, in __init__\r\n    self._fetch_mapper = _FetchMapper.for_fetch(fetches)\r\n  File \"/Users/jasonmancuso/anaconda/envs/tf-encrypted/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 261, in for_fetch\r\n    return _ListFetchMapper(fetch)\r\n  File \"/Users/jasonmancuso/anaconda/envs/tf-encrypted/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 370, in __init__\r\n    self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\r\n  File \"/Users/jasonmancuso/anaconda/envs/tf-encrypted/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 370, in <listcomp>\r\n    self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\r\n  File \"/Users/jasonmancuso/anaconda/envs/tf-encrypted/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 258, in for_fetch\r\n    type(fetch)))\r\nTypeError: Fetch argument None has invalid type <class 'NoneType'>\r\n```\r\nSee https://github.com/tensorflow/tensorflow/issues/783#issuecomment-175824168 for other cases in which Ops can generate a `None` gradient.\r\n\r\nAfter implementing this feature, we'd return the following:\r\n```\r\n[array([[1, 1], [1, 1]], dtype=int32),\r\n array([[1, 1], [1, 1]], dtype=int32)]\r\n```\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to perform automatic differentiation on integer data types, as would be the case when operating in integer rings or finite fields.\r\n\r\n**Any Other info.**\r\nThis request is motivated by the [`tf-encrypted`](https://github.com/mortendahl/tf-encrypted) project.", "comments": ["This is a cool use case, but unfortunately I think is too large a project to take on at this time. I think this would be a difficult feature to fully implement, because we'd have to make sure that every integer operation has a proper gradient function defined. Gradient functions are currently written with the assumption that we don't need to handle integers (e.g. we always return None for the indices param of a gather op). The result of this is, without careful auditing and/or testing, many gradients() calls over integers would silently return None or even the wrong answer. This is a big enough feature that we'd probably want close collaboration with someone on the TF team.\r\n\r\nIf there's enough demand in the future it might be worth the effort, but for now I don't think we can properly deliver on this.\r\n\r\ncc @alextp @ebrevdo @martinwicke -- maybe I'm missing something that makes this more tractable?", "I agree with @skye . It might be easier for you to bypass TF's gradient code entirely if you want to go this route, since our ops have gradients which don't behave well at all with integers.", "@skye to understand the problem better and out of curiosity, would you mind pointing to a place where the assumption on returning None is used?", "Here's one I happen to run into recently: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L410\r\nNote that we always return None for the indices grad of [gather](https://www.tensorflow.org/api_docs/python/tf/gather). This is a niche case, but if you were attempting to take the gradient w.r.t. a variable that determines the indices, this grad function would silently return the wrong answer (None corresponds to zero gradient more or less). There are likely more, part of the difficulty is combing through all the gradient functions and finding them :)", "I have this need for integer tensor and their gradients too ... it\u2019s disappointing nothing seems to be happening with this, I think this will restrict future innovation in some interesting areas"]}, {"number": 25352, "title": "Library Conversion: Tensor2Tensor", "body": "**Tensor2Tensor**, or [T2T](https://github.com/tensorflow/tensor2tensor) for short, is a library of deep learning models and datasets designed to make deep learning more accessible and [accelerate ML research](https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html). T2T is actively used and maintained by researchers and engineers within the [Google Brain team](https://research.google.com/teams/brain/) and a community of users. You can chat with the T2T team on [Gitter](https://gitter.im/tensor2tensor/Lobby) and join the [T2T Google Group](https://groups.google.com/forum/#!forum/tensor2tensor).\r\n\r\nThe purpose of this ticket will be to migrate Tensor2Tensor to the TF 2.0 API.\r\n\r\n- Convert all existing Tensor2Tensor code.\r\n- Run unit tests.\r\n- Ensure processes are well understood.", "comments": ["Hey @dynamicwebpaige @alextp! I would love to help out, I just need some guidance on where and how to start."]}, {"number": 25346, "title": "2.0 Reference Models: NCF Model (TPU with Keras)", "body": "This is an implementation of the **Neural Collaborative Filtering** (NCF) framework with Neural Matrix Factorization (NeuMF) model as described in the [Neural Collaborative Filtering](https://arxiv.org/abs/1708.05031) paper. Current implementation is based on the code from the authors' [NCF code](https://github.com/hexiangnan/neural_collaborative_filtering) and the Stanford implementation in the [MLPerf Repo](https://github.com/mlperf/reference/tree/master/recommendation/pytorch).\r\n\r\n**Neural Collaborative Filtering** is a general framework for collaborative filtering of recommendations in which a neural network architecture is used to model user-item interactions. Unlike traditional models, NCF does _not_ resort to Matrix Factorization (MF) with an inner product on latent features of users and items. It replaces the inner product with a multi-layer perceptron that can learn an arbitrary function from data.\r\n\r\nFor the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/models/blob/6518c1c7711ef1fdbe925b3c5c71e62910374e3e/official/recommendation/README.md).\r\n\r\nThe purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.", "comments": ["Hi guys, do you have a date for us in terms of when we can expect an NCF implementation in tensorflow 2.x? "]}, {"number": 25295, "title": "Data augmentation on 4-D or 5-D data", "body": "Hi, \r\n\r\nI would like to request a new feature.\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12, compiled with CUDA 10, NCCL, CuDNN7.\r\n- Are you willing to contribute it (Yes/No): Maybe, but as a student, time is a real constraint.\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI would like to have data augmentation functions that accept 4-D arrays with shape [width, height, depth, channels] or 5-D Tensor of shape [batch, width, height, depth, channels]. Current supported format is only 4-D [batch, height, width, channels] or 3-D Tensor of shape [height, width, channels]. \r\nHaving functions like random rotation, random shear, and/or random flip for this data shape would be useful.\r\n\r\n**Will this change the current api? How?** \r\nNo.\r\n\r\n**Who will benefit with this feature?** \r\nAny people that works like me in medical imaging and have to deal with 3D data.\r\n", "comments": ["We'd welcome contributions for this feature!", "Hi @mrry, I would like to contribute to this feature. Will it be okay for you?", "Sure, thanks for contributing!", "> Sure, thanks for contributing!\r\n\r\nGreat, I will let you know if I make some progress.", "Hi @pldelisle, I found out that there are three modules of tensorflow related to data/image augmentation, that is, \r\n\r\n> keras-preprocessing/keras_preprocessing/image/image_data_generator.py\r\n> tensorflow/tensorflow/python/ops/image_ops_impl.py\r\n> tensorflow/tensorflow/contrib/image/python/ops/image_ops.py\r\n\r\nOnly keras' can directly achieve random shear op, which is basically numpy implementation. Is the keras' function the one you want us to update? Or you can show me your data processing pipeline\r\n, so I know which function I should work on. \r\n", "@mrry,  could you help me pin down the function that I need to work on? As I said to @pldelisle,  \r\nthere are three possible modules of tensorflow related to data/image augmentation, \r\n\r\n> keras-preprocessing/keras_preprocessing/image/image_data_generator.py\r\n> tensorflow/tensorflow/python/ops/image_ops_impl.py\r\n> tensorflow/tensorflow/contrib/image/python/ops/image_ops.py\r\n\r\nwhich one should I extend to high rank data?\r\n\r\n", "Hi all. Sorry I was out for the weekend.\r\n\r\nMy data processing pipeline is, at the moment, in the following DataProvider class:\r\n\r\n```\r\nclass DataProvider(object):\r\n\r\n    def __init__(self, path, subset, config):\r\n        \"\"\"\r\n        Data provider for multimodal MRI.\r\n        :param tfrecord_path: path to the TFRecords\r\n        :param subset: the subset (train, validation, test) the data set will be instantiate with.\r\n        :param config: A JSON configuration file for hyper parameters.\r\n        \"\"\"\r\n\r\n        self._path = path\r\n        self._subset = subset\r\n        self._subject_batch_size = config.get(\"subject_batch_size\", 1)\r\n        self._shuffle = config.get(\"shuffle\", True)\r\n        self._use_fp16 = config.get(\"use_fp16\", False)\r\n        self._patch_shape = config.get(\"volume\")[\"patch_size\"]\r\n        self._width = config.get(\"width\", 240)\r\n        self._height = config.get(\"height\", 240)\r\n        self._depth = config.get(\"depth\", 155)\r\n        self._n_channels = config.get(\"n_channels\", 1)\r\n        self._batch_size = config.get(\"batch_size\", 32)\r\n        self._n_modalities = config.get(\"n_modalities\", 4)\r\n        self._use_weight_map = config.get(\"use_weight_map\", True)\r\n\r\n        if self._use_weight_map:\r\n            self._n_total_modalities = self._n_modalities + 2\r\n        else:\r\n            self._n_total_modalities = self._n_modalities + 1\r\n\r\n        # Declare a Sampler object here if required.\r\n\r\n    def _get_filename(self):\r\n        \"\"\"\r\n        Returns the file name associated to the data set's subset.\r\n        :return: A string containing the complete path and file name.\r\n        \"\"\"\r\n        return os.path.join(self._path, self._subset + \".tfrecords\")\r\n\r\n    def _training_parser(self, serialized_example):\r\n        \"\"\"\r\n        Parses a single tf.Example which contains multiple modalities and label tensors.\r\n        :param serialized_example: A TFRecord serialized example.\r\n        :return: A tuple containing all parsed image modalities, segmentation and weight_map with slice dimensions.\r\n        \"\"\"\r\n\r\n        # input format.\r\n        features = tf.parse_single_example(\r\n            serialized_example,\r\n            features={\r\n                \"flair\": tf.FixedLenFeature(self._width * self._height * self._depth * self._n_channels,\r\n                                            tf.int64),\r\n                \"t1\": tf.FixedLenFeature(self._width * self._height * self._depth * self._n_channels,\r\n                                         tf.int64),\r\n                \"t1ce\": tf.FixedLenFeature(self._width * self._height * self._depth * self._n_channels,\r\n                                           tf.int64),\r\n                \"t2\": tf.FixedLenFeature(self._width * self._height * self._depth * self._n_channels,\r\n                                         tf.int64),\r\n                \"segmentation\": tf.FixedLenFeature(self._width * self._height * self._depth * self._n_channels,\r\n                                                   tf.int64),\r\n                \"weight_map\": tf.FixedLenFeature(self._width * self._height * self._depth * self._n_channels,\r\n                                                 tf.float32),\r\n                \"original_shape\": tf.FixedLenFeature(4,\r\n                                                     tf.int64),\r\n                \"slices\": tf.FixedLenFeature(9,\r\n                                             tf.int64),\r\n\r\n            })\r\n\r\n        flair = features[\"flair\"]\r\n        t1 = features[\"t1\"]\r\n        t1ce = features[\"t1ce\"]\r\n        t2 = features[\"t2\"]\r\n        segmentation = features[\"segmentation\"]\r\n        weight_map = features[\"weight_map\"]\r\n        original_shape = features[\"original_shape\"]\r\n        slices = features[\"slices\"]\r\n\r\n        # Reshape from [depth * height * width] to [depth, height, width, channel] using serialized image shape.\r\n        flair = tf.reshape(flair, original_shape)\r\n        t1 = tf.reshape(t1, original_shape)\r\n        t1ce = tf.reshape(t1ce, original_shape)\r\n        t2 = tf.reshape(t2, original_shape)\r\n        segmentation = tf.reshape(segmentation, original_shape)\r\n        weight_map = tf.reshape(weight_map, original_shape)\r\n\r\n        if self._use_fp16:\r\n            flair = tf.cast(flair, dtype=tf.float16)\r\n            t1 = tf.cast(t1, dtype=tf.float16)\r\n            t1ce = tf.cast(t1ce, dtype=tf.float16)\r\n            t2 = tf.cast(t2, dtype=tf.float16)\r\n            segmentation = tf.cast(segmentation, dtype=tf.float16)  # Cast to FP16 because ongoing operations do not support mixed types (tf.Stack in random_crop).\r\n            weight_map = tf.cast(weight_map, dtype=tf.float16)\r\n\r\n        else:\r\n            flair = tf.cast(flair, dtype=tf.float32)\r\n            t1 = tf.cast(t1, dtype=tf.float32)\r\n            t1ce = tf.cast(t1ce, dtype=tf.float32)\r\n            t2 = tf.cast(t2, dtype=tf.float32)\r\n            segmentation = tf.cast(segmentation, dtype=tf.float32)  # Cast to FP32 because ongoing operations do not support mixed types (tf.Stack in random_crop).\r\n            weight_map = tf.cast(weight_map, dtype=tf.float32)\r\n\r\n        return flair, t1, t1ce, t2, segmentation, weight_map, slices\r\n\r\n    def _crop_image(self, flair, t1, t1ce, t2, segmentation, weight_map, slices):\r\n        \"\"\"\r\n        Crop modalities.\r\n        :param flair: FLAIR modality of an image.\r\n        :param t1: T1 modality of an image.\r\n        :param t1ce: T1ce modality of an image.\r\n        :param t2: T2 modality of an image.\r\n        :param segmentation: Labels of an image.\r\n        :param weight_map: Associated weight map of an image.\r\n        :return: Randomly cropped 3D patches from image's set.\r\n        \"\"\"\r\n\r\n        stack = tf.stack([flair, t1, t1ce, t2, segmentation, weight_map], axis=-1)\r\n\r\n        stack = stack[slices[0]:slices[2]:slices[1], slices[3]:slices[5]:slices[4], slices[6]:slices[8]:slices[7], :, :]\r\n\r\n        # Randomly crop a [self._patch_size, self._patch_size, self._patch_size, 1] section of the image using\r\n        # TensorFlow built-in function.\r\n        image = tf.random_crop(stack, [self._patch_shape, self._patch_shape, self._patch_shape, self._n_channels,\r\n                                       self._n_total_modalities])\r\n\r\n        [flair, t1, t1ce, t2, segmentation, weight_map] = tf.unstack(image, self._n_total_modalities, axis=-1)\r\n\r\n        return flair, t1, t1ce, t2, segmentation, weight_map\r\n\r\n    def _filter(self, flair, t1, t1ce, t2, segmentation, weight_map):\r\n\r\n        stack = tf.stack([flair, t1, t1ce, t2, segmentation, weight_map], axis=-1)\r\n\r\n        valid_idx = tf.where(tf.math.reduce_sum(stack, axis=(0, 1, 2, 3)) > 0)\r\n\r\n        stack = tf.boolean_mask(stack, valid_idx, axis=-1)\r\n\r\n        [flair, t1, t1ce, t2, segmentation, weight_map] = tf.unstack(stack, self._n_total_modalities, axis=-1)\r\n\r\n        return flair, t1, t1ce, t2, segmentation, weight_map\r\n\r\n\r\n    def input(self):\r\n        \"\"\"\r\n        Return a TensorFlow data set object containing inputs.\r\n        :return: dataset: TensorFlow data set object.\r\n        \"\"\"\r\n\r\n        # Get file names for this data set.\r\n        filename = self._get_filename()\r\n\r\n        with tf.name_scope(\"inputs\"):\r\n            # Instantiate a new data set based on a provided TFRecord file name. Generate a Dataset with raw records.\r\n            dataset = tf.data.TFRecordDataset(filename)\r\n\r\n            # Parse records. Returns 1 volume with all its modalities.\r\n            dataset = dataset.map(map_func=self._training_parser,\r\n                                  num_parallel_calls=1)\r\n\r\n            # Prefetch 10 subject. Adjust with available system RAM.\r\n            dataset = dataset.prefetch(10)\r\n\r\n            if self._subset == \"train\" or \"validation\":\r\n                min_queue_examples = int(\r\n                    DataProvider.num_examples_per_epoch(self._subset) * 0.10)\r\n\r\n                # Ensure that the capacity is sufficiently large to provide good random\r\n                # shuffling.\r\n                dataset = dataset.shuffle(buffer_size=min_queue_examples + 2 * self._batch_size)\r\n\r\n                # Repeat indefinitely.\r\n                dataset = dataset.repeat()\r\n\r\n                # Returns randomly cropped images.\r\n                dataset = dataset.map(self._crop_image, num_parallel_calls=self._batch_size)\r\n\r\n                # Filter inputs for only non-zero patches.\r\n                dataset = dataset.filter(self._filter)\r\n\r\n            # Batch 3D patches. Here, we want (training batch / number of modalities) samples per modality,\r\n            # which is usually 32 / 4 = 8 patches per modality.\r\n            dataset = dataset.batch(int(self._batch_size / self._n_modalities))\r\n\r\n            # Prepare for next iterations.\r\n            dataset = dataset.prefetch(10 * self._batch_size)\r\n\r\n            return dataset\r\n\r\n    @staticmethod\r\n    def num_examples_per_epoch(subset):\r\n        if subset == \"train\":\r\n            # Return the number of volumes in training dataset.\r\n            return int(230)\r\n        if subset == \"validation\":\r\n            # Return the number of volumes in validation dataset.\r\n            return int(29)\r\n        if subset == \"test\":\r\n            # Return the number of volumes in test dataset.\r\n            return int(26)\r\n```\r\n\r\nI would like to add data augmentation in it. I already do random crop of it after slicing the image to only non-zero voxels (these are brain MRI scans which has multiple modalities). Having random rotation or random flip would be great. Shearing in medical image doesn't necessary add value to the pipeline, so it wouldn't be a priority. \r\n\r\nSince Contrib is going to be removed with Tensorflow 2.0 (if I remember well), I think the `tensorflow/tensorflow/python/ops/image_ops_impl.py` or `keras-preprocessing/keras_preprocessing/image/image_data_generator.py` would be preferable. ", "Ok, @pldelisle, once I make some progress I will let you know.", "@musikisomorphie, could I have some help for this issue, I also have some experience of processing 3D medical  MRI images with Matlab.", "@a6802739, I already opened a PR for this issue. But you can leave some comments to my PR if you like.", "@musikisomorphie Could you please give me the link for your PR?", "@a6802739, https://github.com/tensorflow/tensorflow/pull/26269", "@pldelisle,\r\nYou can use `tf.keras.preprocessing.image.ImageDataGenerator()` to accomplish data Augmentation.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n#input and output\r\nx_train = np.random.normal(size=(128,128,128,3))\r\n \r\ndatagen = tf.keras.preprocessing.image.ImageDataGenerator(\r\n    featurewise_center=True,\r\n    featurewise_std_normalization=True,\r\n    rotation_range=20,\r\n    width_shift_range=0.2,\r\n    height_shift_range=0.2,\r\n    horizontal_flip=True,\r\n    validation_split=0.2)\r\ndatagen.fit(x_train)\r\n```"]}]