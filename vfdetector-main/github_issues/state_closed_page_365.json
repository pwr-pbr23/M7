[{"number": 43106, "title": "Enable integration of llvm/llvm-project@3a577f5", "body": "The function MemRefDescriptor::getElementType() was renamed upstream to\r\nMemRefDescriptor::getElementPtrType().", "comments": ["MLIR-HLO was already fixed with https://github.com/tensorflow/mlir-hlo/commit/b22f2f0eeae29ad5321783bdea7c0df854bc06a5."]}, {"number": 43105, "title": "[T.F 2.0 API Docs] Modifying documentation of set_optimizer in config.py", "body": "PR for issue #42450 \r\n\r\nMaking the modification to the existing functions:\r\n- `set_optimizer_experimental_options`", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43105) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.\r\nNot sure why it says that I haven't signed the CLA because when I log in to check it I can clearly see my information displayed. This isn't my first time contributing to TF docs either.", "@Harsh188, I see you have signed the CLA. \r\n\r\nBut the bot doesn't just check the user submitting the PR, it checks the email address in the commits.\r\n\r\nCan you probably fix it with a `git commit --amend --author=\"John Doe <john@doe.org>\"` and a `git push -f`.", "@Harsh188 Can you please make sure to use same GitHub username and email-id associated with it", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43105) for more info**.\n\n<!-- ok -->", "Looks like after reinstalling macOS on my computer I had my last name listed as _Mohan_ instead of _MohanKumar_ :P ", "Hi @jaingaurav, can you help me sort this out?\r\n\r\nIt looks like all of the default optimizers are being pushed onto the `optimizer` vector in [`meta_optimizers.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L228). This leads me to believe that all of the default optimizers are active. \r\n\r\nThis seems a bit confusing because when I run \r\n```python\r\n>>> tf.config.optimizer.get_experimental_options()\r\n```\r\nthe output I get is `{'disable_model_pruning': False, 'disable_meta_optimizer': False}`. \r\n\r\nIt doesn't list out all of the optimizers that are initialized to `True`. This is what initially lead me to believe that they were all set to `False`. \r\n\r\nThis can be observed in `context.py` where [`_optimizer_experimental_options`](https://github.com/tensorflow/tensorflow/blob/8b7d5b4842d9a6107ca0df319c488d4ac27d8f20/tensorflow/python/eager/context.py#L451) is initially set to an empty dictionary. This results in the state of the default optimizers not being added to the dictionary when [`get_optimizer_experimental_options`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/context.py#L1533) is called.", "@Harsh188: You are right, this is a problem. The API basically allows setting a value, but doesn't tell you the default that are already set. This is very confusing and largely a problem with the interplay between the python configuration interface and the C++ state. What we need is an API to the C++ layer that tells what optimizers are set once the context is initialized.", "I'm gonna open up another PR to fix that issue and then get back to this since I'm still unsure about what default optimizers are turned on."]}, {"number": 43104, "title": "TF lite fails conversion with post train int quantization, converts fine without: RuntimeError: Mismatch between number of weight maxs and channels", "body": "Hey there, I've managed to convert StyleGan2 to a tf lite model with your recent advice, however now upon trying to quantize the model I'm getting an error. The model converts **with no issues** if quantization is not used. It also converts fine with dynamic range quantization, only failing if a **representative dataset** is used. I have replicated the issue on a small part of the full model here.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): tried on tf 2.3, tf nightly \r\n\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n**Link to colab notebook with all commands and downloads:**\r\nhttps://colab.research.google.com/drive/1b-o3cPz_1er_EQC0Ezq-vsTmlH55kmAe?usp=sharing\r\n\r\n**Link to SavedModel tar**\r\nhttps://drive.google.com/file/d/1BdFA3CJ-uDIFrLWMTy9hAWPKTHWp9YCw/view?usp=sharing\r\n\r\n**Link to successfully converted tf lite model without post-train quantization**\r\nhttps://drive.google.com/file/d/157ujWxsnjAuR2tn5es5Oz3rRIYNwmiwO/view?usp=sharing\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nTakes in [1, 18, 512] tensor of random input:\r\n```python\r\nsamples = []\r\n\r\nfor i in range(10):\r\n    sample = np.random.randn(1, 18, 512)\r\n    sample = sample.astype(np.float32)\r\n    samples.append(sample)\r\n\r\ndef representative_data_gen():\r\n    for sample in samples:\r\n        yield [sample]\r\n```\r\n\r\nConverter code\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/content/synth_const')\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith tf.io.gfile.GFile('synth_const_opt.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-09-10 11:03:29.887482: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-10 11:03:30.014402: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-10 11:03:32.260056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.260451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.260987: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-09-10 11:03:32.261050: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-09-10 11:03:32.261693: I tensorflow/compiler/jit/xla_gpu_device.cc:161] Ignoring visible XLA_GPU_JIT device. Device number is 1, reason: Invalid argument: Invalid device ordinal value (1). Valid range is [0, 0].\r\n2020-09-10 11:03:32.319520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.319815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6325GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-09-10 11:03:32.319865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.320143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:05:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.392GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-09-10 11:03:32.320163: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-10 11:03:32.320177: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-10 11:03:32.320188: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-10 11:03:32.320198: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-10 11:03:32.320208: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-10 11:03:32.320218: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-10 11:03:32.320228: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-10 11:03:32.320263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.320548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.320844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.321124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.321394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1843] Ignoring visible gpu device (device: 1, name: GeForce GTX 1050 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1) with core count: 6. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\r\n2020-09-10 11:03:32.321401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-10 11:03:32.321419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-10 11:03:32.321424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 \r\n2020-09-10 11:03:32.321428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N \r\n2020-09-10 11:03:32.321432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N \r\n2020-09-10 11:03:32.321489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.321776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.322041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9842 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-09-10 11:03:32.366684: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-09-10 11:03:32.366717: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 88 nodes (84), 89 edges (86), time = 31.342ms.\r\n2020-09-10 11:03:32.366721: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.045ms.\r\n2020-09-10 11:03:32.479552: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2020-09-10 11:03:32.479576: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\n2020-09-10 11:03:32.498762: I tensorflow/compiler/jit/xla_gpu_device.cc:161] Ignoring visible XLA_GPU_JIT device. Device number is 1, reason: Invalid argument: Invalid device ordinal value (1). Valid range is [0, 0].\r\n2020-09-10 11:03:32.498945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.499265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6325GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-09-10 11:03:32.499316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.499602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:05:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.392GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-09-10 11:03:32.499622: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-10 11:03:32.499636: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-10 11:03:32.499645: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-10 11:03:32.499654: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-10 11:03:32.499663: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-10 11:03:32.499672: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-10 11:03:32.499681: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-10 11:03:32.499718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.500010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.500314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.500599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.500966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1843] Ignoring visible gpu device (device: 1, name: GeForce GTX 1050 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1) with core count: 6. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\r\n2020-09-10 11:03:32.500978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-10 11:03:32.501002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-10 11:03:32.501007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 \r\n2020-09-10 11:03:32.501011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N \r\n2020-09-10 11:03:32.501015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N \r\n2020-09-10 11:03:32.501084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.501389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-10 11:03:32.501696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9842 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"/home/y4tsu/PycharmProjects/sg2_nhwc_reduced_cleanup/test_pieces.py\", line 187, in <module>\r\n    main()\r\n  File \"/home/y4tsu/PycharmProjects/sg2_nhwc_reduced_cleanup/test_pieces.py\", line 174, in main\r\n    write_synth_const(g_params, convert)\r\n  File \"/home/y4tsu/PycharmProjects/sg2_nhwc_reduced_cleanup/test_pieces.py\", line 81, in write_synth_const\r\n    tflite_model = converter.convert()\r\n  File \"/home/y4tsu/anaconda3/envs/tf2_3/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 1076, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"/home/y4tsu/anaconda3/envs/tf2_3/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 899, in convert\r\n    return super(TFLiteFrozenGraphConverterV2,\r\n  File \"/home/y4tsu/anaconda3/envs/tf2_3/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 638, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"/home/y4tsu/anaconda3/envs/tf2_3/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 450, in _calibrate_quantize_model\r\n    return calibrate_quantize.calibrate_and_quantize(\r\n  File \"/home/y4tsu/anaconda3/envs/tf2_3/lib/python3.8/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 95, in calibrate_and_quantize\r\n    return self._calibrator.QuantizeModel(\r\nRuntimeError: Mismatch between number of weight maxs and channels: 1 vs 512\r\n```\r\n\r\nI think the Quantize op is expecting a shape that it's not receiving, but I've tried changing the dimensionality of tensors at various points and I haven't been able to identify the source of the issue. If you have any ideas please let me know!\r\n", "comments": ["I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/4fcbcbff9c163f9736ceb964702ce8fe/untitled412.ipynb).", "I met the same problem", "Do all ops in the model are in list of supported for quantization? E.g. Conv2D with variable weights? \r\n\r\nI am working on another model (can't share) and there is no issue with separate parts for inputs and weights but problem occurs after Conv2D. ", "@liufengdb do you have any update on this issue?", "The issue is fixed in the Tf Nightly 2.6 version, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/ced1a80dc4182d664e761aaf803d8c37/43104.ipynb). \r\nClosing the issue since the issue is fixed, feel free to reopen. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43104\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43104\">No</a>\n"]}, {"number": 43103, "title": "cuDNN error when passing all-masked sequences to RNN layers on GPU", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: CentOS 8\r\n- TensorFlow installed from: binary (pip)\r\n- TensorFlow version: 2.3.0 (v2.3.0-rc2-23-gb36436b087)\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.1 / 7\r\n- GPU model and memory: RTX 6000 (24GB)\r\n\r\n**Describe the current behavior**\r\n\r\nWhen passing a boolean mask together with data to a RNN layer running on GPU, a cuDNN error is raised if any row of the mask is made entirely of False values, _i.e._ if one of the batched input sequences is made entirely of padding values.\r\n\r\nThe raised error is the following:\r\n```\r\nUnknownError: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1521): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)' [Op:CudnnRNNV3]\r\n```\r\n\r\nThis may seem as an edge case, but can be encountered when reshaping a [batch, block, sequence, dimension] tensor with variable-size (therefore zero-padded) blocks of sequences representing e.g. a long text into a [..., sequence, dimension] tensor to be processed at once by a RNN and then reshaped back to [batch, block, rnn_dim].\r\n\r\nI was able to implement a work-around using a custom wrapper class, but this does not feel like a righteous solution.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect the RNN to output some default value (e.g. a vector or zeros) representing the all-padding sequence(s), as it does on CPU.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nMinimal code to reproduce the error:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ngru = tf.keras.layers.GRU(128)\r\nrng = tf.random.get_global_generator()\r\ninp = rng.normal((8, 64, 256))\r\nmsk = tf.concat([tf.ones((4, 64), dtype=tf.bool), tf.zeros((4, 64), dtype=tf.bool)], axis=0)\r\n\r\n# works\r\nwith tf.device('CPU:0'):\r\n    gru(inp, mask=msk)\r\n# works\r\nwith tf.device('GPU:0'):\r\n    gru(inp, mask=tf.ones_like(msk))\r\n# fails\r\nwith tf.device('GPU:0'):\r\n    gru(inp, mask=msk)\r\n```\r\n\r\nWorkaround I implemented, which \"unmasks\" all-padding sequences, thus triggering unrequired computations:\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass SafeRNN(tf.keras.layers.Wrapper):\r\n    \"\"\"Wrapper for keras RNN layers avoiding a mask-caused cuda error.\"\"\"\r\n\r\n    def call(self, inputs, mask=None, **kwargs):\r\n        \"\"\"Run inputs through the wrapped layer.\"\"\"\r\n        if mask is not None:\r\n            valid = tf.reduce_any(mask, axis=1, keepdims=True)\r\n            mask = tf.where(valid, mask, tf.ones_like(mask))\r\n        return self.layer(inputs, mask=mask, **kwargs)\r\n\r\n    def compute_mask(self, inputs, mask=None):\r\n        \"\"\"Return an output mask tensor.\"\"\"\r\n        if mask is None:\r\n            return None\r\n        return tf.reduce_any(mask, axis=1)\r\n\r\ngru = SafeRNN(tf.keras.layers.GRU(128))\r\nrng = tf.random.get_global_generator()\r\ninp = rng.normal((8, 64, 256))\r\nmsk = tf.concat([tf.ones((4, 64), dtype=tf.bool), tf.zeros((4, 64), dtype=tf.bool)], axis=0)\r\n\r\n# works\r\nwith tf.device('GPU:0'):\r\n    gru(inp, mask=msk)\r\n```\r\n\r\n**Other info**\r\n\r\nThis issue is not consistent from a system to the other; it does not trigger on my Linux Mint 19.1 system, with the same Python, TensorFlow and CUDA/cuDNN versions (but distinct GPU: Quadro P1000).", "comments": ["@pandrey-fr \r\n\r\nI have tried in colab with TF nightly version(`2.4.0-dev20200910`) and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/1b31cd0dc4889e6dcbb817e6517bb76d/untitled334.ipynb). Thanks!", "Indeed, the issue is solved in nightly. I will therefore use my workaround with the current stable release and discard it when I upgrade to 2.4 after a stable release is issued.\r\nIt is a bit tiring to constantly have to maintain / upgrade code to get such apparently-small bug fixes which are not back-ported to previous stable releases, but at least the bug seems fixed; thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43103\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43103\">No</a>\n"]}, {"number": 43102, "title": "Huge size difference of GPU delegate library between static and dynamic", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: RK3399\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: 10.4\r\n- GPU model and memory: GTX 1050\r\n**Describe the current behavior**\r\nSo I tried to use bazel to build the gpu delegate library as written in https://www.tensorflow.org/lite/performance/gpu_advanced\r\n`bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:delegate                           # for static library`\r\n`bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so  # for dynamic library`\r\n\r\nThe resulting dynamic library .so is 106 MB and static library .a is 1.1 MB. Why their size are so different? I need to use the dynamic library, however, 106 MB RAM will be used only for loading this dynamic library. Is there any way to reduce the size of the dynamic library?\r\n\r\nThanks!\r\n\r\n", "comments": ["Hi @terryheo, have you seen similar issues?", "You'd better provide \"--strip always\" option to the bazel command.\r\n\r\n```\r\n$ bazel build -c opt --config android_arm64 --strip always tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so\r\n```\r\n"]}, {"number": 43101, "title": "Graph mode failure with mask-fed RNN layers within a train_step loop on GPU.", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Mint 19.1 & CentOS 8\r\n- TensorFlow installed from: binary (pip)\r\n- TensorFlow version: 2.2.0 (v2.2.0-rc4-8-g2b96f3662b)\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1 / 7\r\n- GPU model and memory: Quadro P1000 (4GB) & RTX 6000 (24GB)\r\n\r\n**Describe the current behavior**\r\n\r\nModel training fails in graph mode when the custom `tf.keras.Model.train_step` involves both:\r\n* a loop (except if iterating over an int and not a tensor)\r\n* a `tf.keras.layers.RNN` inheriting layer\r\n* the passing of a `mask` to said layer (whether implicitly based on previous layer's `compute_mask` output or manually passing a deterministic mask, even an whole-true one)\r\n\r\nThe error raised has the following format:\r\n```\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  InstantiateOptions.input_devices must have the same length as the number of arguments: input_devices length = 49 number of arguments = 50\r\n\t [[{{node while/body/_1/StatefulPartitionedCall}}]]\r\n\t [[while/exit/_59/_46]]\r\n  (1) Invalid argument:  InstantiateOptions.input_devices must have the same length as the number of arguments: input_devices length = 49 number of arguments = 50\r\n\t [[{{node while/body/_1/StatefulPartitionedCall}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_<some_id_number>]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n```\r\n\r\nIt appears to be raised on the second call to `self._get_gradients`, _i.e._ when entering the loop within the custom `train_step` and passing inputs to the model for the second time.\r\n\r\nUnresolved, automatically-closed issue [#39827](https://github.com/tensorflow/tensorflow/issues/39827) appears to be similar to mine.\r\n\r\nI hereby provide a minimal example to reproduce this issue, which is arguably an edge-case but has caused me quite some trouble. In this example, I implement some gradient stacking as part of the custom training step, in a non-general way for the sake of simplicity. Note that in this example, since the number of substeps is fixed, the loop could be rewrote as a Python `for` loop over an `int`; then, the issue does not arise. In real life though, I am using a generalized gradient stacking implementation that requires iterating over tensors (hence my providing here a `tf.while_loop` based example).\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect all four tested cases to run without triggering an exception (as they do in 2.3 or on CPU), so that proper masking may be used with RNNs within gradient stacking training loops in graph mode, which is the most desirable setting in terms of both model correctness and code optimization.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThe following script will raise the issue if run on GPU with TF2.2.\r\nUsing a GRU layer and/or a Bidirectional wrapper does not alter the issue.\r\nNote that is will _not_ raise the issue if run on CPU or with TF2.3.\r\n\r\n```python\r\n# coding: utf-8\r\n\r\n\"\"\"Minimal example script for an RNN issue within custom train loops.\"\"\"\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass StackedModel(tf.keras.Model):\r\n    \"\"\"Minimal gradient stacking example Model subclass.\"\"\"\r\n\r\n    def train_step(self, data):\r\n        # NOTE: here we assume data is a single (x, y) tuple\r\n        #       in order to provide with a minimal example\r\n        inputs, y_true = data\r\n        size = tf.shape(inputs)[0] // 4\r\n        # Compute gradients on the batch's first quarter.\r\n        gradients = self._get_gradients(inputs[:size], y_true[:size])\r\n        # Define a process to compute and stack gradients.\r\n        def process_quarter(idx, gradients):\r\n            \"\"\"Compute gradients on a data sub-batch and stack them.\"\"\"\r\n            grads_loc = self._get_gradients(\r\n                inputs[idx * size:(idx + 1) * size],\r\n                y_true[idx * size:(idx + 1) * size]\r\n            )\r\n            gradients = [\r\n                self._add_gradients(a, b) for a, b in zip(gradients, grads_loc)\r\n            ]\r\n            return tf.add(idx, 1), gradients\r\n        # Iteratively process the remaining data quarters using the former.\r\n        _, gradients = tf.while_loop(\r\n            cond=lambda idx, _: tf.math.less(idx, 4),\r\n            body=process_quarter,\r\n            loop_vars=[tf.constant(1), gradients],\r\n            parallel_iterations=1\r\n        )\r\n        # Apply the aggregated gradients.\r\n        grads_and_vars = zip(gradients, self.trainable_variables)\r\n        self.optimizer.apply_gradients(grads_and_vars)\r\n        # Return the current values of the loss and metrics.\r\n        return {m.name: m.result() for m in self.metrics}\r\n\r\n    def _get_gradients(self, inputs, y_true):\r\n        \"\"\"Compute gradients for given (x, y) data.\"\"\"\r\n        with tf.GradientTape() as tape:\r\n            y_pred = self(inputs, training=True)\r\n            loss = self.compiled_loss(y_true, y_pred)\r\n        return tape.gradient(loss, self.trainable_variables)\r\n\r\n    @staticmethod\r\n    def _add_gradients(grad_a, grad_b):\r\n        \"\"\"Return the sum of two gradient objects (Tensor of IndexedSlices).\"\"\"\r\n        if not isinstance(grad_b, type(grad_a)):\r\n            raise TypeError(\"Trying to add objects of distinct types.\")\r\n        if isinstance(grad_a, tf.Tensor):\r\n            return tf.add(grad_a, grad_b)\r\n        if isinstance(grad_a, tf.IndexedSlices):\r\n            values = tf.concat([grad_a.values, grad_b.values], axis=0)\r\n            indices = tf.concat([grad_a.indices, grad_b.indices], axis=0)\r\n            return tf.IndexedSlices(values, indices, grad_a.dense_shape)\r\n\r\n\r\ndef build_example_model(run_eagerly, avoid_mask):\r\n    \"\"\"Return a keras Model for binary classification of tokens sequences.\r\n\r\n    This model expects an input batch of tokens, with zero values\r\n    being treated as padding, and thus masked. An embedding layer\r\n    encodes the tokens into vectors in R^{128}, then a LSTM layer\r\n    produces sequence-wise vectors in R^{128}, which are finally\r\n    transformed into binary probabilities by a dense layer.\r\n    \"\"\"\r\n    inputs = tf.keras.Input((None,), dtype=tf.int32)\r\n    emb = tf.keras.layers.Embedding(\r\n        input_dim=200,\r\n        output_dim=128,\r\n        mask_zero=True\r\n    )\r\n    rnn = tf.keras.layers.LSTM(128)\r\n    out = tf.keras.layers.Dense(2, 'softmax')\r\n    embedding = emb(inputs)\r\n    if avoid_mask:\r\n        embedding = rnn(embedding, mask=None)\r\n    else:\r\n        embedding = rnn(embedding)  # mask is passed implicitly\r\n    model = StackedModel(inputs, out(embedding))\r\n    model.compile(loss='binary_crossentropy', run_eagerly=run_eagerly)\r\n    return model\r\n\r\n\r\ndef build_example_dataset():\r\n    \"\"\"Return a tf.data.Dataset of batched right-padded tokens sequences.\"\"\"\r\n    # Define a random tokens sequences generator.\r\n    def generator():\r\n        \"\"\"Yield sequences of 8 to 32 random ints in (1, 200(, plus a label.\"\"\"\r\n        sizes = 8 + np.random.choice(24, size=640, replace=True)\r\n        for i in range(640):\r\n            seq = 1 + np.random.choice(199, size=sizes[i], replace=True)\r\n            lab = tf.one_hot(np.random.choice(2), depth=2)\r\n            yield (seq, lab)\r\n    # Set up and return a Dataset made of batches of 32 padded sequences.\r\n    dst = tf.data.Dataset.from_generator(\r\n        generator,\r\n        output_shapes=((None,), (2,)),\r\n        output_types=(tf.int32, tf.float32)\r\n    )\r\n    return dst.padded_batch(32, padded_shapes=((None,), (2,)))\r\n\r\n\r\ndef main():\r\n    \"\"\"Minimal demonstration script.\"\"\"\r\n    dst = build_example_dataset().repeat()\r\n    print('Running eagerly without masking at LSTM.')\r\n    model = build_example_model(run_eagerly=True, avoid_mask=True)\r\n    model.fit(dst, steps_per_epoch=20, epochs=3)\r\n    print('Running eagerly with masking at LSTM.')\r\n    model = build_example_model(run_eagerly=True, avoid_mask=False)\r\n    model.fit(dst, steps_per_epoch=20, epochs=3)\r\n    print('Running in graph mode without masking at LSTM.')\r\n    model = build_example_model(run_eagerly=False, avoid_mask=True)\r\n    model.fit(dst, steps_per_epoch=20, epochs=3)\r\n    print('Running in graph mode with masking at LSTM -- prepare for failure.')\r\n    model = build_example_model(run_eagerly=False, avoid_mask=False)\r\n    model.fit(dst, steps_per_epoch=20, epochs=3)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n**Other info**\r\n\r\nThe issue I raise here appears to have been fixed in TensorFlow 2.3.0; since I realized that after having spent a few hours tracking down the bug, revising my code and eventually implementing this test case, I still thought that it would be worth reporting. I would like to have someone confirm whether this has properly been fixed in 2.3, and if possible I would be interested in some insight as to the issue's initial cause and solving. I am also wondering whether the fix would be worth backporting to TF 2.2 (e.g. as a version 2.2.1) as updating custom code from 2.2 to 2.3 can require a limited yet non-anecdotal effort.", "comments": ["@pandrey-fr \r\nI ran the code shared on tf-nightly GPU \"2.4.0-dev20200909\" [also tf 2.3] and do not face any errors, please find [gist here](https://colab.research.google.com/gist/Saduf2019/6410bbf7dab4158a3dc3253d89520596/untitled410.ipynb), please confirm if the code shared is the code that creates the error reported. if possible please share a colab gist with the error reported.", "@Saduf2019 As I wrote in my issue report, the bug arises in 2.2, and not in later releases - and your attempt confirms it. It thus seems to have been fixed, but I would actually be curious to get a sense of why it used to occur.", "@pandrey-fr \r\nYes the issue is fixed in tf 2.3 and nightly, its hard to pin point, many bugs get addressed and performance is worked upon on newer versions.\r\nPlease feel free to move the issue to closed status as its resolved in later version. Thanks!", "This is a very disappointing, uninformative answer; I would have hoped for better, but fine.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43101\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43101\">No</a>\n"]}, {"number": 43100, "title": "Have similar default values for min_delta in ReduceLROnPlateau and EarlyStopping callbacks", "body": "The min_delta default values are 1e-4 for the ReduceLROnPlateau callback and 0 for the EarlyStopping callback. \r\n\r\nThis leads to cases in which training is stopped before the learning rate is reduced even though the patience parameter is higher for EarlyStopping, when the min_delta value is not set explicitly. \r\n\r\n", "comments": ["@FlorianMerkle \r\n\r\nRequest you to fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@ravikyram \r\nI was not sure which template is appropriate, as this is not a bug but I think the default values should be consistent.\r\n\r\nAlso, this is hard to reproduce but consider the toy model with the training procedure showcased in this gist:\r\nhttps://gist.github.com/FlorianMerkle/97895acf0159c45e2950b44c95d31b63#file-min_callbacks_example-py-L1\r\n\r\nIf the validation loss decreases in the patience period, but the decrease is less then 1e-4, training is stopped w/o decaying the LR first. \r\n\r\nI know this case is quite unlikely but I just encountered it and it took me some time to figure this out.\r\n\r\nReduceLROnPlateau:\r\nhttps://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/keras/callbacks.py#L2312\r\nEarlyStopping:\r\nhttps://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/keras/callbacks.py#L1612", "@FlorianMerkle \r\n\r\nI have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20200910`) .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/88de454f148bba811267d0d27be2dec2/untitled332.ipynb).You are also seeing the same behavior?\r\nThanks!", "@ravikyram \r\nThe behavior is not visible in the colab nb. This does only happen irregularly when the situation I specified in my last post occurs. \r\nAdditionally, I just realized that in contrast to TF 2.2, in TF 2.3 and 2.4 the learning rate is not printed during training time by default. Should have considered that in my gist. Sorry for that.", "@FlorianMerkle Is this still an issue for you? I ran your code and don't see any issue. If default `min_delta` is not working for your model, then you can pass them as arguments \r\n\r\n```\r\nearly_stopping = tf.keras.callbacks.EarlyStopping(\r\n      patience=4,\r\n      monitor='val_loss', ,min_delta=0.04 \r\n      )\r\n```\r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/61c833bd1a4a2e02ec29c3339cb53e25/untitled.ipynb). \r\n\r\nCan you please explain little more about the concern and/or your use-case. Thanks!\r\n", "@jvishnuvardhan not really an issue, I just thought it is counterintuitive to have different default values for min_delta in the two callbacks. When NOT specifying values for min_delta, it can rarely occur that training is stopped early, without reducing the Learning Rate first, which is an undesired behavior imo. See my explanation from my last post:\r\n\r\n> If the validation loss decreases in the patience period, but the decrease is less then 1e-4, training is stopped w/o decaying the LR first.\r\n> \r\n> I know this case is quite unlikely but I just encountered it and it took me some time to figure this out.\r\n\r\n\r\nI believe the default values should be 0 for both callbacks.\r\n\r\n\r\n", "@FlorianMerkle I think they set these numbers as default after analyzing lot of use cases. I am not sure how much back testing we need to do make those changes. One thing we can do is, add a note on the callback page about your observations so that others are informed about selection of `min_delta`. \r\n\r\nPlease feel free to raise a PR to update the docs. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43100\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43100\">No</a>\n"]}, {"number": 43098, "title": "[TFLite] Add int16x8 support for ABS operator", "body": "Hi,\r\n\r\nThis PR adds int16x8 support for the ABS operator in TensorFlow Lite.\r\n\r\nThibaut", "comments": ["@Tessil  Can you please resolve conflicts? Thanks!", "@gbaned I resolved the conflict, thanks.\r\n\r\nI just realised that I create a `toupstream/16x8_mean_operator` branch for this PR instead of `toupstream/16x8_abs_operator` and I can't change it without creating a new PR. As it's only the branch name, it's probably not too important.", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR."]}, {"number": 43097, "title": "Understanding the Difference between Normal Training vs eager mode training in TFOD API v2", "body": "Eager mode allowed you to see tensor values when TF 1 used sessions and graphs but since eager mode is the default in tf 2.0 so its a little confusing.\r\n\r\nI want to know the differences between the normal training vs Eager mode training.  One clear difference I can see is that the code is a lot different for both of them. Even the final data format is not in tfrecords in eager mode.  And the code is bigger.\r\n\r\n**I have the following questions:**\r\n\r\n- So is the Eager Mode Method Faster (Since it can train a really basic detector in just 5 minutes and the normal method takes just a few minutes to initialize)?\r\n\r\n- If the params of pipeline.config are not changed then will the final model have the same accuracy regardless of which method was used (normal or eager) and again will the training time be different? \r\n\r\n- What's the advantage of eager mode vs normal training, is it more flexibility in training?\r\n\r\n- Going forward what method will you recommend new users to use? and when will you prefer one method over the other?\r\n\r\n_Thanks, would really appreciate if these questions can be answered as I can't find any docs that address these questions._", "comments": ["If you are coming from TF 1.x I suggest you to give an overview to:\r\nhttps://medium.com/ai%C2%B3-theory-practice-business/tensorflow-1-0-vs-2-0-part-1-computational-graphs-4bb6e31c1a0f\r\nhttps://medium.com/ai%C2%B3-theory-practice-business/tensorflow-1-0-vs-2-0-part-2-eager-execution-and-autograph-47473ed8b817", "> \r\n> \r\n> If you are coming from TF 1.x I suggest you to give an overview to:\r\n> https://medium.com/ai%C2%B3-theory-practice-business/tensorflow-1-0-vs-2-0-part-1-computational-graphs-4bb6e31c1a0f\r\n> https://medium.com/ai%C2%B3-theory-practice-business/tensorflow-1-0-vs-2-0-part-2-eager-execution-and-autograph-47473ed8b817\r\n\r\n@bhack I read the articles above and also 2 other parts in the series. Now I do think that eager mode in TFOD API is more useful when you want to see the whole training process in detail and want to make debugging easier also in this mode no computational graphs will be used but I still have 2 questions above unanswered:\r\n\r\n\r\n1.  How is the eager mode training faster (Since it can train a really basic detector in just 5 minutes and the normal method takes just a few minutes to initialize)?  is it because of the initializations done for the computational graph?\r\n\r\n2.  If the params of pipeline.config are not changed then will the final model have the same accuracy and latency regardless of which method was used (normal or eager) and again will the training time be different?\r\n\r\nIf you're unable to answer these can you please mention someone in the tfod Dev team.\r\n", "@bhack Hi could really use some insight on the above 2 questions", "I don't know your specific model but eager is not generally faster:\r\nhttps://www.tensorflow.org/guide/intro_to_graphs#the_benefits_of_graphs\r\n\r\nPlease use [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) for this type of questions.\r\n", "> \r\n> \r\n> I don't know your specific model but eager is not generally faster:\r\n> https://www.tensorflow.org/guide/intro_to_graphs#the_benefits_of_graphs\r\n> \r\n> Please use [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) for this type of questions.\r\n\r\nThanks for replying, yeah but when the question is too specific It's really hard to get answers from there.", "Usually Eager mode is slower compared to graph mode, to make the best use of performance and optimization you can use Autograph in Tensorflow 2.x , once you implement part of your code with Autograph using @tf.function decorator, that part of code will be executed in graph mode. See more details [here](https://github.com/keras-team/keras/issues/15545).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43097\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43097\">No</a>\n"]}, {"number": 43096, "title": "dense_to_ragged_batch fails with map function implemented with py_function", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv2.3.0-rc2-23-gb36436b087 2.3.0\r\n\r\n**Describe the current behavior**\r\n\r\ndense_to_ragged_batch fails when I use a map function implemented with py_function.\r\n\r\n**Describe the expected behavior**\r\n\r\ndense_to_ragged_batch should work no matter which map_function I use.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\n#!/usr/bin/python3\r\n\r\nimport numpy as np;\r\nimport tensorflow as tf\r\n\r\ndef map_function(x):\r\n\r\n  image, bbox, label = tf.py_function(map_function_impl, inp = [x], Tout = [tf.float32, tf.float32, tf.int32]);\r\n  return image, bbox, label;\r\n\r\ndef map_function_impl(x):\r\n\r\n  image = np.random.normal(size = (416, 416, 3));\r\n  num_target = np.random.randint(low = 0, high = x, size = ());\r\n  bbox = np.random.normal(size = (num_target ,4));\r\n  label = np.random.randint(low = 0, high = 10, size = (num_target,));\r\n  return image, bbox, label;\r\n\r\ndef main():\r\n\r\n  dataset = tf.data.Dataset.from_tensor_slices(np.random.randint(low = 3, high = 10, size = (6,)));\r\n  dataset = dataset.map(map_function);\r\n  print(dataset.element_spec[0].shape);\r\n  print(dataset.element_spec[1].shape);\r\n  print(dataset.element_spec[2].shape);\r\n  dataset = dataset.apply(tf.data.experimental.dense_to_ragged_batch(batch_size = 2));\r\n  for batch in dataset:\r\n    print(batch);\r\n\r\nif __name__ == \"__main__\":\r\n\r\n  main();\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@breadbread1984 \r\n\r\nI have tried in colab with TF version 2.3, nightly versions(`2.4.0-dev20200909`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/238c82cf3caf6e38e80b3683d71767b4/untitled328.ipynb).You are also seeing the same behavior?\r\nThanks!", "@ravikyram there was some problem in the code reproducing the problem. I update the code. please try again. thx", "@breadbread1984 \r\n\r\nI tried with the updated code. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/69cedd1588405ae4ed7cfd2a387262f3/untitled329.ipynb).You are also getting the same error?\r\nThanks!", "yeah, I got the same error. batching ragged tensor fails ", "@breadbread1984 I think this is an implementation problem. Please take a look at this [issue](https://stackoverflow.com/questions/58529511/how-do-i-make-a-ragged-batch-in-tensorflow-2-0) and let me know if it helps. Thanks!", "the issue doesnt help. the problem occurs even before the tensors are batched into ragged tensor. please note the printed shape. after preprocess by the py_function. the output tensors' shape are total unknown. so the ragged tensor doesnt know which dimension is ragged.\r\npy_function should have some facilities to specify which dimension of the output tensor is ragged like how map_fn does with fn_output_signature parameter.", "This works as expected. TensorFlow does not implement shape inference for `py_function`-based computation and thus the shapes of tensors produced as the results of applying `py_function` will be completely unknown. If downstream computation requires the shapes to be (partially) known, you will need to use [set_shape](https://www.tensorflow.org/api_docs/python/tf/Tensor?version=nightly#set_shape) to assert the expected shape.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43096\">No</a>\n"]}, {"number": 43095, "title": "Input_c/Input_h shape in Cudnn LSTMP", "body": "Hi, I'm using Cudnn LSTMP as implemented here: [https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py#L1848](url).\r\n\r\nI have a question about the shapes of **input_h** and **input_c** in the `__call__` method. Why the shape is `[n_layers, batch_size, n_proj]` for both shapes. In LSTM cell, `h` is of shape `[n_layers, batch_size, n_proj]` while **c** is of shape `[n_layers, batch_size, n_units]`. I would really be appreciated if you could explain.", "comments": ["@YoPatapon Sorry for the late response. I cannot access the link you mentioned above. Can you please use recent TF version and let me know if you see any issue. \r\n\r\nPlease note that GitHub is mainly for Bug/performance related issue. If your issue is not related to bug/performance, please file an issue in Stackoverflow where there is larger community to support general questions. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43095\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43095\">No</a>\n"]}, {"number": 43094, "title": "batch training with model.fit not working for all batch_sizes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n    Yes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n    Windows\r\n\r\n- TensorFlow installed from (source or binary):\r\n    binary via anaconda\r\n\r\n- TensorFlow version (use command below): \r\n    2.1.0\r\n\r\n- Python version: 3.7.9\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nWhen the number of samples in the training set is not equal to a factor of the batch size, model.fit will throw an error. On 2.1.0, I get an error saying that the shapes of two operators are incompatible. Not sure if this is the expected behavior or not. \r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  Incompatible shapes: [32,1] vs. [4,1]\r\n         [[node mul_5 (defined at .\\classify.py:31) ]] [Op:__inference_distributed_function_435]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n```\r\n\r\nIt seems like when the function is creating the last batch in the epoch, it doesn't have enough samples remaining to create a full batch, as a result, it builds what it can then errors out. The size of my dataset is 1763, not exactly a neat number. The only other method I have to implement minibatch training is to split the dataset into batches myself and train manually, without model.fit. Like I said, if this is the expected behavior, ignore this probably, but it seems like a hassle. Especially if the size of someone's dataset was a prime number, in which case they would be restricted to training either with a batch size of 1 or the size of their full dataset, which seems inconvenient.\r\n\r\n\r\n**Describe the expected behavior**\r\nThe expected behavior for me would be for model.fit to split the data into batches without running out of room on the last batch.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Sequential, optimizers, losses\r\nfrom tensorflow.keras.layers import Input, Dense, Flatten\r\n\r\n\r\n\r\ndef make_model(input_shape, output_shape, batch_size=32):\r\n    model = Sequential()\r\n    model.add(Input(shape=input_shape, batch_size=batch_size, name='input'))\r\n    \r\n    model.add(Flatten())\r\n    model.add(Dense(output_shape))\r\n    model.build((batch_size, *input_shape))\r\n    return model\r\n    \r\n\r\nif __name__ == '__main__':\r\n    batch_size = 32\r\n    input_size = (5, 5)\r\n    output_size = 1\r\n    num_samples = 100\r\n    model = make_model(input_size, output_size, batch_size=batch_size)\r\n    model.compile(optimizer=optimizers.Adam(), loss=losses.SparseCategoricalCrossentropy(from_logits=True))       \r\n    X = np.ones((num_samples, *input_size))\r\n    y = np.zeros(num_samples)\r\n    model.fit(X, y, batch_size=batch_size, epochs=10)\r\n\r\n```\r\nHere's a link to a Colab notebook as well. It seems like in more recent version of tensorflow, the error is still there, but the exact error that comes up is slightly different. \r\nhttps://colab.research.google.com/drive/1RSOIKMYFAlg2jfPaljkosBpQyhaFbQNy?usp=sharing\r\n", "comments": ["Hi,\r\nMy guess is that the issue comes from your specifying a fixed batch size in the `Input` layer.\r\nCould you please change the line `model.add(Input(shape=input_shape, batch_size=batch_size, name='input'))` to `model.add(Input(shape=input_shape, batch_size=None, name='input'))` and run the script?", "Your fix works fantastically. Thank you. Although, in my opinion, this behavior is not immediately intuitive, it gets the job done.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43094\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43094\">No</a>\n", "Great!\r\nTo explain a bit: when you pass `batch_size=32` to the `Input` layer, the computational graph is built to support this, an only this, input batch size, which can result in some optimizations as compared with accepting a dynamic input size. If you wanted, you could use an option in `tf.data.Dataset.batch` to discard remaining samples that do not form a complete batch - another option could be to zero-pad the final batch, but then you have to use the masking system to ensure padding entries are not used when computing model updates.\r\nOn the other hand, passing `batch_size=None` to the `Input` layer triggers the support of any input batch size in your graph - but that means some operations may have to be recompiled on the fly to match the various input configurations. It is actually quite the same as when you use `None` to leave some other input dimensions unspecified; e.g. to build RNNs that receive variable-size sequences."]}, {"number": 43093, "title": "RaggedTensor support for model output", "body": "**System information**\r\n- TensorFlow version: 2.3.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Current behavior/state.**\r\nLoss function is implemented in a way that tries to convert `y_true` and `y_pred` to Tensor. That means the output of a model can't be a RaggedTensor, otherwise error shows up as RaggedTensor can't be converted to Tensor.\r\n\r\n**Will this change the current api? How?**\r\nLine 62 of `/tensorflow/python/ops/confusion_matrix.py`: `predictions = ops.convert_to_tensor(predictions)`.\r\nLine 63 of `/tensorflow/python/ops/confusion_matrix.py`: `labels = ops.convert_to_tensor(labels)`.\r\nThere should be an option to work with `labels` and `predictions` as RaggedTensor instead of just converting it to Tensor.\r\n\r\n**Who will benefit with this feature?**\r\nSequence-to-sequence autoencoders with RaggedTensor inputs and outputs or models with RaggedTensor outputs in general will benefit.\r\n", "comments": ["Note that supporting RaggedTensors as targets (e.g., in `model.fit`) is a frequent request, see #44988, #44112, #43591, #43093, #42320, #41810.\r\n\r\nSome partial progress is in pull requests #45060 and #45015, which will allow using custom losses in Keras model (but existing losses like MSE or (S)CE will still not work).", "Note that I created a new feature request #45403 to support RaggedTensors in standard Keras loss functions.", "@tungnat97,\r\nAs the issue, #45403 has been resolved, can you please confirm if we can close this issue? Thanks!"]}, {"number": 43092, "title": "OP_REQUIRES failed at reshape_op.h:57 : Invalid argument: Size 1 must be non-negative, not -9", "body": "1.environment: windows10, cuda10.1, cudnn7.6.5 and tensorflow2.2.0\r\n\r\n2.my problem:\r\ni wanted to save my total model with tf.saved_model.save , and i used tf.squeeze and tf.reshape together. Then, when i used `model.signatures[\"serving_default\"](inputs)` some bugs happened.\r\n**But, if i used tf.keras.models.load_model('./saved_model') to save model, everything is ok when predicting.**\r\n\r\n3.part of my codes:\r\n```python\r\n# save model\r\nraw_prediction = tf.squeeze(raw_prediction)\r\nshape = tf.shape(raw_prediction)\r\nheight = shape[0]\r\nwidth = shape[1]\r\nshape_float = tf.cast(shape[0:2], dtype=tf.float32)\r\nraw_prediction = keras.backend.reshape(raw_prediction, shape=(height, width, num_anchors, -1))\r\n\r\n# predict\r\nmodel = tf.saved_model.load('./saved_model')\r\nmodel.signatures[\"serving_default\"](inputs)  # bugs happen when i predicted\r\n```\r\n\r\n4.bugs:\r\n```\r\n OP_REQUIRES failed at reshape_op.h:57 : Invalid argument: Size 1 must be non-negative, not -9\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Size 1 must be non-negative, not -9\r\n\t [[{{node StatefulPartitionedCall/model_1/tf_op_layer_RealDiv_14/RealDiv_14-1-ReshapeNHWCToNCHW-LayoutOptimizer}}]]\r\n\t [[StatefulPartitionedCall/model_1/tf_op_layer_GatherV2_5/GatherV2_5/_18]]\r\n  (1) Invalid argument:  Size 1 must be non-negative, not -9\r\n\t [[{{node StatefulPartitionedCall/model_1/tf_op_layer_RealDiv_14/RealDiv_14-1-ReshapeNHWCToNCHW-LayoutOptimizer}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_signature_wrapper_10761]\r\n\r\nFunction call stack:\r\nsignature_wrapper -> signature_wrapper\r\n```", "comments": ["if i don't use tf.squeeze, codes are operating correctly.\r\n```python\r\n# save model\r\n# raw_prediction = tf.squeeze(raw_prediction)\r\nshape = tf.shape(raw_prediction)\r\nheight = shape[1]\r\nwidth = shape[2]\r\nshape_float = tf.cast(shape[1:3], dtype=tf.float32)\r\nraw_prediction = keras.backend.reshape(raw_prediction, shape=(height, width, num_anchors, -1))\r\n```", "@HirataYurina,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using.\r\n\r\nAlso, please update TensorFlow to v2.3 and check if you are still facing the same issue. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43091, "title": "[Intel MKL] Fixing mkl-eager-op-rewrite-test", "body": "This PR fixes a test failure caused by changes made to the build file in this commit 7bf13f9aae6b0bbdfbe4e01cb503f7988b2e8ce6", "comments": ["@alextp @jaingaurav can you please review this PR?"]}, {"number": 43090, "title": "LSTM Issue ", "body": "Hello, I am trying to implement an LSTM with input (123,45,4) and output (123,45,1) with a sequence of 4 integers as the input and a single number as the output. I am using Mac OS, Google Colab, and TF version 2.3.0. \r\n\r\nHere is my model:\r\n\r\n```\r\ndef define_models(n_input, n_output, n_units):\r\n\t# define training encoder\r\n\tencoder_inputs = Input(shape=(None, n_input))\r\n\tencoder = LSTM(n_units, return_state=True)\r\n\tencoder_outputs, state_h, state_c = encoder(encoder_inputs)\r\n\tencoder_states = [state_h, state_c]\r\n\t# define training decoder\r\n\tdecoder_inputs = Input(shape=(None, n_output))\r\n\tdecoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\r\n\tdecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\r\n\tdecoder_dense = Dense(n_output, activation='softmax')\r\n\tdecoder_outputs = decoder_dense(decoder_outputs)\r\n\tmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\r\n\t# define inference encoder\r\n\tencoder_model = Model(encoder_inputs, encoder_states)\r\n\t# define inference decoder\r\n\tdecoder_state_input_h = Input(shape=(n_units,))\r\n\tdecoder_state_input_c = Input(shape=(n_units,))\r\n\tdecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\r\n\tdecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\r\n\tdecoder_states = [state_h, state_c]\r\n\tdecoder_outputs = decoder_dense(decoder_outputs)\r\n\tdecoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\r\n\t# return all models\r\n\treturn model, encoder_model, decoder_model\r\n```\r\n\r\n\r\nWhen I try running the code: model.fit(x_train, y_train, epochs = 50) I get the error:     **AssertionError: Could not compute output Tensor(\"dense_2/truediv:0\", shape=(None, None, 1), dtype=float32).** Does anyone know how to fix this?\r\n\r\nHere is the code to reproduce the issue:\r\n\r\nLoad Data:\r\n```\r\nwith open(\"training_data_input.txt\") as fopen:\r\n  with open(\"training_data_output.txt\") as fopen2:\r\n    for line in fopen:\r\n      myList = line.strip().split()\r\n      myList[0] = myList[0].replace(\"[\",\"\")\r\n      if myList[0] == \"\":\r\n        myList = myList[1:]\r\n      if \"][\" in myList[3]:\r\n        j = 0\r\n        print(myList[3])\r\n        myList[3] = myList[3].replace(']][[',\"\")\r\n        if len(myList[3]) > 3:\r\n          myList[3] = (myList[3][:3])\r\n        myList = myList[:4]\r\n      myList[len(myList)-1] = myList[len(myList)-1].replace(\"]\",\"\")\r\n      x = np.empty((154,45,4),dtype=np.float32)\r\n      i = 0\r\n      j = 0\r\n      if j >=45:\r\n        j = 0\r\n      print(myList)\r\n      x[i][j] = myList\r\n      i+=1\r\n      j+=1\r\n    for line in fopen2:\r\n      myList = line.strip().split()\r\n      x_out = np.empty((154,45,1), dtype=np.float32)\r\n      myList[0] = myList[0].replace(\"[\",\"\")\r\n      if myList[0] == \"\":\r\n        myList = myList[1:]\r\n      if \"][\" in myList[0]:\r\n        j = 0\r\n        myList[0] = myList[0].replace(']][[',\"\")\r\n        if len(myList[0]) > 3:\r\n          myList[0] = (myList[0][:2])\r\n        myList = myList[:1]\r\n      myList[len(myList)-1] = myList[len(myList)-1].replace(\"]\",\"\")\r\n      i = 0\r\n      j = 0\r\n      if j >=45:\r\n        j = 0\r\n      x_out[i][j] = myList\r\n      i+=1\r\n      \r\nprint(x.shape)\r\nprint(x_out.shape)\r\n```\r\n\r\nTrain Model:\r\n\r\n```\r\n\r\nfrom sklearn.model_selection import train_test_split\r\nx_train, x_test, y_train, y_test = train_test_split(x, x_out, test_size = 0.2, random_state = 4)\r\nprint(x_train.shape)\r\nprint(y_train.shape)\r\n\r\nmodel.fit(x_train, y_train, epochs = 50)\r\n```\r\n\r\nThe input data:\r\n[training_data_input.txt](https://github.com/tensorflow/tensorflow/files/5198490/training_data_input.txt)\r\n[training_data_output.txt](https://github.com/tensorflow/tensorflow/files/5198491/training_data_output.txt)\r\n\r\n", "comments": ["Could you minimize a little bit your code to have a runnable standalone example/colab that reproduces your problem?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43090\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43090\">No</a>\n"]}, {"number": 43089, "title": "Link doesn't work.", "body": "Link for _TensorFlow graph optimization with Grappler_ from url https://www.tensorflow.org/guide/tf_numpy#performance_comparisons does not exist.\r\nThe link points to page https://www.tensorflow.org/guide/guide/graph_optimization which give Page not found error", "comments": ["Can I create a pull request to fix this issue?\r\n", "https://github.com/tensorflow/docs/pull/1667", "This is fixed."]}, {"number": 43088, "title": "Pywrap ops", "body": "@saxenasaurabh @alextp", "comments": ["You should be able to test this now that 7519f88d21138b4ad317c8b3dc688b39a1287f6a is merged.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43088) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43088) for more info**.\n\n<!-- ok -->", "@amturati Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on."]}, {"number": 43087, "title": "tflite_runtime Interpreter import error", "body": "OS: Ubuntu 16.04\r\n\r\nHi, there. I'm trying to use tflite_runtime Interpreter from [this docs](https://www.tensorflow.org/lite/guide/python) , but unfortunately I have an import error, when trying to use this package after installation.\r\nI have tested all \"supported\" versions of python from this doc, and installed tflite_runtime package for each versions of pythons from 3.5 to 3.8.\r\nMy code is:\r\n```python\r\nimport tflite_runtime.interpreter as tflite\r\nmodel = tflite.Interpreter(model_path=\"/path/to/model/my_model.tflite\")\r\nmodel.get_input_details()\r\n```\r\nIt only works at python 3.5. But it's unacceptable for me, because I need python 3.7 version.\r\nAll other versions of python, except 3.5 give me the same error.\r\nFor example:\r\nPython 3.7.4 error:\r\n```bash\r\nPython 3.7.4 (default, Aug 13 2019, 20:35:49) \r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tflite_runtime.interpreter as tflite\r\nTraceback (most recent call last):\r\n  File \"/home/bocharick/anaconda3/lib/python3.7/site-packages/tflite_runtime/interpreter_wrapper.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"/home/bocharick/anaconda3/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.27' not found (required by /home/bocharick/anaconda3/lib/python3.7/site-packages/tflite_runtime/_interpreter_wrapper.so)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/bocharick/anaconda3/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 46, in <module>\r\n    from tflite_runtime import interpreter_wrapper as _interpreter_wrapper\r\n  File \"/home/bocharick/anaconda3/lib/python3.7/site-packages/tflite_runtime/interpreter_wrapper.py\", line 17, in <module>\r\n    _interpreter_wrapper = swig_import_helper()\r\n  File \"/home/bocharick/anaconda3/lib/python3.7/site-packages/tflite_runtime/interpreter_wrapper.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_interpreter_wrapper')\r\n  File \"/home/bocharick/anaconda3/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_interpreter_wrapper'\r\n>>> \r\n```\r\n\r\nPython 3.6.12 error:\r\n```python\r\nPython 3.6.12 (default, Sep 10 2020, 03:15:05) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tflite_runtime.interpreter as tflite\r\nTraceback (most recent call last):\r\n  File \"/home/bocharick/.local/lib/python3.6/site-packages/tflite_runtime/interpreter_wrapper.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"/home/bocharick/Downloads/Python-3.6.12/Lib/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.27' not found (required by /home/bocharick/.local/lib/python3.6/site-packages/tflite_runtime/_interpreter_wrapper.so)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/bocharick/.local/lib/python3.6/site-packages/tflite_runtime/interpreter.py\", line 46, in <module>\r\n    from tflite_runtime import interpreter_wrapper as _interpreter_wrapper\r\n  File \"/home/bocharick/.local/lib/python3.6/site-packages/tflite_runtime/interpreter_wrapper.py\", line 17, in <module>\r\n    _interpreter_wrapper = swig_import_helper()\r\n  File \"/home/bocharick/.local/lib/python3.6/site-packages/tflite_runtime/interpreter_wrapper.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_interpreter_wrapper')\r\n  File \"/home/bocharick/Downloads/Python-3.6.12/Lib/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_interpreter_wrapper'\r\n>>>\r\n```\r\n\r\nSo as I see, it's `GLIBC_2.27`  problem. And what I must to do, if I have Ubuntu 16.04 with `GLIBC_2.23`, but I need to use python3.6+???", "comments": ["@Bocharick,\r\nTo resolve the `/lib/x86_64-linux-gnu/libm.so.6: version GLIBC_2.27 not found` error, please take a look at [this](https://stackoverflow.com/a/20560664) StackOverflow comment and let us know if it helps. Thanks!", "first of all those stackoverflow topic about older version of glibc (2.17).\r\nAnd my question is -  why different versions of same yours python package needs different versions of glibc?\r\nCould you make it with lowest glibc version, like at python3.5 version package of tflite_runtime?", "I have found this instruction: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package\r\nAnd built tflite_runtime package .whl.\r\nIt works now. BUT! It's very slow.\r\n\r\n1) Tensorflow 2.3.0 from pip\r\n2) tflite_runtime my build from tensorflow 2.3.0 sources\r\n\r\nSo\r\n```python\r\nimport tensorflow as tf\r\nmdl = tf.lite.Interpreter(model_path=\"/path/to/model.tflite\")\r\n\r\n-->cycle of invokes()\r\n```\r\n\r\nis ~2.3x faster then\r\n```python\r\nimport tflite_runtime.interpreter as tflite\r\nmodel = tflite.Interpreter(model_path=\"/path/to/model.tflite\")\r\n\r\n--> SAME cycle of invokes()\r\n```\r\n\r\nWhy? tensorflow pip package which have no native optimizations are fast, but tflite_runtime built with my own machine with -O3 optimization much slower?", "@terryheo would you mind taking a look at this one? It could be that we're not using a consistent set of build flags (or toolchains) between the PIP build and the source build.", "@Bocharick what's the build command you used to build tflite_runtime package wheel ?", "> @Bocharick what's the build command you used to build tflite_runtime package wheel ?\r\n\r\nI have tried two variants:\r\n\r\n1) tensorflow/lite/tools/pip_package/build_pip_package.sh (with preinstalled dependencies of course)\r\n2) make BASE_IMAGE=ubuntu:16.04 PYTHON=python3 TENSORFLOW_TARGET=native docker-build  \r\n\r\nboth packages have same performance in my case.", "build_pip_package.sh is using Makefile to build TFLite which doesn't support full features.\r\nCould you try build_pip_package_with_bazel.sh?\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package#alternative-build-with-bazel-experimental", "> build_pip_package.sh is using Makefile to build TFLite which doesn't support full features.\r\n> Could you try build_pip_package_with_bazel.sh?\r\n> https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package#alternative-build-with-bazel-experimental\r\n\r\nOk. I have built pip whl package with build_pip_package_with_bazel.sh. It was made from clean unzipped tf 2.3.0 sources.\r\nNow I have two different whl files:\r\n- **make built tflite_runtime**\r\n- **bazel built tflite_runtime**  \r\n\r\nAnd I make several tests.\r\nFirst of all I would like to tell, **tf2.2.0** and **tf2.3.0** has different tflite converted output models from same Keras model. Theese models has different indexes count, but same prediction (if it works) result. It was checked by me.\r\nSo I made four models from original Keras model:\r\n- **tf2.2.0.NoOptimizations.tflite**\r\n- **tf2.2.0.Optimize.DEFAULT.tflite**\r\n- **tf2.3.0.NoOptimizations.tflite**\r\n- **tf2.3.0.Optimize.DEFAULT.tflite**  \r\n\r\nAll of them were made with Python API, absolutely without errors.\r\n**BUT** tf2.3.0.Optimize.DEFAULT.tflite don't work at all.\r\n\r\nMy speed tests results (it pure only predict time, without import/etc/...):\r\n**Predictions count**: 10000\r\n\r\n**Original Keras model**:\r\n\r\n- \ttensorflow==2.3.0 - 135.380 seconds\r\n\r\n**TFLite converted from original Keras model with tensorflow==2.2.0, No Optimizations**:\r\n\r\n- \ttensorflow==2.3.0 tf.lite.Interpreter - 126.382 seconds\r\n- \tmake built tflite_runtime from tf 2.3.0 sources - 133.498 seconds\r\n- \tbazel built tflite_runtime from tf 2.3.0 sources - 137.244 seconds\r\n\r\n**TFLite converted from original Keras model with tensorflow==2.2.0, Optimize.DEFAULT**:\r\n\r\n- \ttensorflow==2.3.0 tf.lite.Interpreter - 100.528 seconds\r\n- \tmake built tflite_runtime from tf 2.3.0 sources - 193.405 seconds (yes, it's not and error, has checked several times)\r\n- \tbazel built tflite_runtime from tf 2.3.0 sources - 193.204 seconds (yes, it's not and error, has checked several times)\r\n\r\n**TFLite converted from original Keras model with tensorflow==2.3.0, No Optimizations**:\r\n\r\n- \ttensorflow==2.3.0 tf.lite.Interpreter - 125.875 seconds\r\n- \tmake built tflite_runtime from tf 2.3.0 sources - 135.651 seconds\r\n- \tbazel built tflite_runtime from tf 2.3.0 sources - 140.317 seconds\r\n\r\n**TFLite converted from original Keras model with tensorflow==2.3.0, Optimize.DEFAULT**:\r\n\r\n- \ttensorflow==2.3.0 tf.lite.Interpreter - FAIL (error*)\r\n- \tmake built tflite_runtime from tf 2.3.0 sources - FAIL (same error*)\r\n- \tbazel built tflite_runtime from tf 2.3.0 sources - FAIL (same error*)\r\n\r\n*error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"02_tflite_punc_test.py\", line 30, in <module>\r\n    model_interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\r\n  File \"/home/bocharick/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py\", line 204, in __init__\r\n    model_path, self._custom_op_registerers))\r\nValueError: Did not get operators, tensors, or buffers in subgraph 1.\r\n```\r\n\r\nAs you can see, make built and bazel built versions of tflite_runtime whl pip package has same (with a small error) performance\r\n\r\n**P.S.**: My Keras model summary:\r\n```\r\n***************************************************************************\r\nModel: \"decode_model\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\nx_input (InputLayer)            [(None, 50)]         0                                            \r\n__________________________________________________________________________________________________\r\nEmbed_ (Embedding)              (None, 50, 320)      64000640    x_input[0][0]                    \r\n__________________________________________________________________________________________________\r\nBGRU_0 (Bidirectional)          (None, 50, 512)      887808      Embed_[0][0]                     \r\n__________________________________________________________________________________________________\r\np_input (InputLayer)            [(None, 50, 1)]      0                                            \r\n__________________________________________________________________________________________________\r\nconcatenate (Concatenate)       (None, 50, 513)      0           BGRU_0[0][0]                     \r\n                                                                 p_input[0][0]                    \r\n__________________________________________________________________________________________________\r\nGRU_1 (GRU)                     (None, 50, 512)      1577472     concatenate[0][0]                \r\n__________________________________________________________________________________________________\r\ny_before (TimeDistributed)      (None, 50, 3)        1539        GRU_1[0][0]                      \r\n__________________________________________________________________________________________________\r\ny_after (TimeDistributed)       (None, 50, 7)        3591        GRU_1[0][0]                      \r\n==================================================================================================\r\nTotal params: 66,471,050\r\nTrainable params: 2,470,410\r\nNon-trainable params: 64,000,640\r\n__________________________________________________________________________________________________\r\nNone\r\n***************************************************************************\r\n\r\n```\r\n[tf.keras.utils.plot_model(keras_model) picture](https://drive.google.com/file/d/1FAVTgVxItzX7ESFlRusJiKUYIuViOxGg/view?usp=sharing)\r\n\r\n**P.P.S.**: tf.lite.Interpeter x2.3 faster than tflite_runtime.interpreter.Interpreter for me with another model. But I can't show it.", "Thanks for the detailed information.\r\nCould you try the following?\r\n\r\n```\r\n$ BAZEL_FLAGS=\"--copt=-O3 --copt=-march=native\" tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh\r\n```", "> Thanks for the detailed information.\r\n> Could you try the following?\r\n> \r\n> ```\r\n> $ BAZEL_FLAGS=\"--copt=-O3 --copt=-march=native\" tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh\r\n> ```\r\n\r\nYes. Finally it works as I expected. Thank you.\r\nYou need to add this to the docs, I think.\r\n\r\n**TFLite converted from original Keras model with tensorflow==2.2.0, No Optimizations:**  \r\n\r\n- bazel built with opts tflite_runtime from tensorflow 2.3.0 sources - 108.857 seconds\r\n\r\n**TFLite converted from original Keras model with tensorflow==2.2.0, Optimize.DEFAULT:**  \r\n\r\n- bazel built with opts tflite_runtime from tensorflow 2.3.0 sources - 94.416 seconds\r\n\r\n**TFLite converted from original Keras model with tensorflow==2.3.0, No Optimizations:**  \r\n\r\n- bazel built with opts tflite_runtime from tensorflow 2.3.0 sources - 106.098 seconds\r\n\r\n**TFLite converted from original Keras model with tensorflow==2.3.0, Optimize.DEFAULT:**  \r\n\r\n- bazel built with opts tflite_runtime from tensorflow 2.3.0 sources - FAIL (same error*)\r\n\r\n", "Thanks for the verification!\r\nLet me close this with script and doc update.", "> Thanks for the verification!\r\n> Let me close this with script and doc update.\r\n\r\nThanks. It's good for me."]}, {"number": 43086, "title": "Issue on Load Trax", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- Docker container: tensorflow/tensorflow:latest-devel-gpu (sha256:81b1629c567b638dbc197aa24aa7aaa22b84ae88d1bb6c963fa7e3d970cbe77e)\r\n- CUDA/cuDNN version: Cuda compilation tools, release 11.0, V11.0.221\r\nBuild cuda_11.0_bu.TC445_37.28845127_0\r\n- GPU model and memory: nvidia gtx 1060 6gb\r\n\r\n**Describe the current behavior**\r\nI have installed jupyter in the tensorflow/tensorflow:latest-devel-gpu  docker image, I'm running the following files:\r\n\r\n```\r\n!pip install --upgrade jax\r\n!pip install --upgrade jaxlib\r\n!pip install --upgrade trax\r\n\r\n!pip install --upgrade 'https://storage.googleapis.com/jax-releases/cuda110/jaxlib-0.1.51-cp36-none-manylinux2010_x86_64.whl'\r\n\r\n!pip install --upgrade jax\r\n```\r\n\r\n```\r\nfrom jax.lib import xla_bridge\r\nprint(xla_bridge.get_backend().platform)\r\n```\r\nBut when I run this code, I'm getting the following error (The GPU is not loading):\r\n`E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: UNKNOWN ERROR (-1)`\r\n\r\n**Describe the expected behavior**\r\nWhen I run the code with the docker image tensorflow/tensorflow:nightly-gpu-jupyter everything is working as expected.\r\n", "comments": ["Is `tensorflow/tensorflow:latest-devel-gpu` updated?", "@kujaomega \r\nPlease update as per above comment.\r\nplease refer to existing issues and let us know: #7653 #32623 [link](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Is `tensorflow/tensorflow:latest-devel-gpu` updated?\r\n\r\nYes, I have already test it today 2020-9-23 using `tensorflow/tensorflow:latest-devel-gpu` image updated without using cache.", "@kujaomega \r\nIs this still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43086\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43086\">No</a>\n"]}, {"number": 43085, "title": " TF 2.3 ImportError: DLL load failed: The specified module could not be found. ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source?\r\n- TensorFlow version: 2.3\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda? pip\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): na\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: cpu\r\n\r\nI run this code \r\n\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nand this error log appears\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Christian\\ML\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found. \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:/Christian/ML/main.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"D:\\Christian\\ML\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"D:\\Christian\\ML\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"D:\\Christian\\ML\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"D:\\Christian\\ML\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Christian\\ML\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Christian\\ML\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nFailed to load the native TensorFlow runtime.", "comments": ["Do you have installed  C++ redistributable? https://support.microsoft.com/it-it/help/2977003/the-latest-supported-visual-c-downloads", "@profzamorra \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website.](https://www.tensorflow.org/install/source_windows)\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issues #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "My cpu model is Intel Core i7-4771 3,5Ghz( 64 bit),  the latest version of C++ Redistributable is 2017 14.11.23325, installed 13.6.2020. I have installed anaconsa 2 (32bit) and 3 (32 bit) and Python 3.8 (64 bit). ", "@profzamorra \r\n\r\nAnaconda by default has a Python environment of its own, which comes with TensorFlow preinstalled.TensorFlow is tested and supported on 64-bit systems.So you'll need Anaconda 64 bit for this.\r\n\r\nAlternatively,if you want to install TensorFlow on Python (ie. without Anaconda), please check if you have all the compatible hardware and software requirements mentioned [here](https://www.tensorflow.org/install/pip?lang=python3#system-requirements).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43085\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43085\">No</a>\n"]}, {"number": 43084, "title": "\"Failed starting model allocation.\" while trying to allocate tensors", "body": "**System information**\r\nHello everyone,\r\nI am working on a project to implement a prediction algorithm, which is to be implemented for a microcontroller, based on an LSTM network. Therefore, I have not installed the complete library, instead I have downloaded the most recent repository and I have been selecting the files that are useful for my purpose(one by one). For the beginning I am building the c++ project on \"S32 Design Studio\" and running it on the laptop with Windows 10(haven't started to compile on microcontroller yet)\r\n\r\n**the tf keras model which was as below: **\r\n```\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Input(batch_input_shape=(1,6, 3), name='input'),\r\n    tf.keras.layers.LSTM(n_neurons, time_major=False, return_sequences=False),\r\n    tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid, name='output')\r\n])\r\nmodel.compile(loss='mean_squared_error', optimizer='adam')\r\n```\r\nand I have converted it without any problem. This is the output of conversion :\r\n```\r\nrun_model = tf.function(lambda x: model(x))\r\n# This is important, let's fix the input size.\r\nBATCH_SIZE = 1\r\nSTEPS = time_ev\r\nINPUT_SIZE = 3\r\nconcrete_func = run_model.get_concrete_function(\r\n    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\r\n\r\n# model directory.\r\nMODEL_DIR = \"keras_lstm\"\r\nmodel.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\r\ntflite_model = converter.convert()\r\n\r\n# Save the TF Lite model.\r\nwith tf.io.gfile.GFile('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\n```\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:109: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:109: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nINFO:tensorflow:Assets written to: keras_lstm/assets\r\n```\r\nThen I have downloaded the model as a .cc file because I could not use the function to read .tflite file: \r\n\r\n```\r\n# Define paths to model files\r\nimport os\r\n\r\nMODELS_DIR = 'models/'\r\nif not os.path.exists(MODELS_DIR):\r\n    os.mkdir(MODELS_DIR)\r\nMODEL_TF = MODELS_DIR + 'model.pb'\r\nMODEL_NO_QUANT_TFLITE = MODELS_DIR + 'model_no_quant.tflite'\r\nMODEL_TFLITE = MODELS_DIR + 'model.tflite'\r\nMODEL_TFLITE_MICRO = MODELS_DIR + 'model.cc'\r\n\r\n# # Save the model to disk\r\nopen(MODEL_NO_QUANT_TFLITE, \"wb\").write(tflite_model)\r\n\r\n# Install xxd if it is not available\r\n!apt-get update && apt-get -qq install xxd\r\n# Convert to a C source file\r\n!xxd -i {MODEL_NO_QUANT_TFLITE} > {MODEL_TFLITE_MICRO}\r\n# Update variable names\r\nREPLACE_TEXT = MODEL_NO_QUANT_TFLITE.replace('/', '_').replace('.', '_')\r\n!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}\r\n\r\nfiles.download(\"models/model.cc\") \r\n```\r\n\r\nFinally I have this small script just to import the model and build the interpreter:\r\n\r\n```\r\n#include <string.h>\r\n\r\n#include \"tensorflow/lite/micro/kernels/micro_ops.h\"\r\n#include \"tensorflow/lite/micro/micro_error_reporter.h\"\r\n#include \"tensorflow/lite/micro/micro_interpreter.h\"\r\n#include \"tensorflow/lite/micro/micro_mutable_op_resolver.h\"\r\n#include \"tensorflow/lite/micro/all_ops_resolver.h\"\r\n#include \"tensorflow/lite/version.h\"\r\n#include \"model.h\"\r\n#include \"tensorflow/lite/micro/micro_optional_debug_tools.h\"\r\n\r\n// Globals, used for compatibility with Arduino-style sketches.\r\nnamespace {\r\ntflite::ErrorReporter* error_reporter;\r\n//const tflite::Model* model ;\r\ntflite::MicroInterpreter* interpreter = nullptr;\r\n\r\n// Create an area of memory to use for input, output, and intermediate arrays.\r\n// Minimum arena size, at the time of writing. After allocating tensors\r\n// you can retrieve this value by invoking interpreter.arena_used_bytes().\r\nconstexpr size_t allocator_buffer_size = 2096 /* optimal arena size at the time of writting. */\r\n\t\t\t+ 16 /* alignment */ + 100 /* some headroom */;\r\n\r\nuint8_t allocator_buffer[allocator_buffer_size];\r\n} // namespace\r\n\r\nint main() {\r\n\r\n\tconst tflite::Model* model = ::tflite::GetModel(g_model);\r\n\r\n\tif (model->version() == TFLITE_SCHEMA_VERSION) {\r\n\t\tputs(\"Model provided is supported.\\n\");\r\n\t\t//fprintf(stderr, \"Model provided is supported.\\n\");\r\n\r\n\t}\r\n\r\n\ttflite::MicroErrorReporter micro_error_reporter;\r\n\terror_reporter = &micro_error_reporter;\r\n\r\n\tstatic tflite::AllOpsResolver resolver;\r\n\r\n         static tflite::MicroInterpreter static_interpreter(model, resolver, allocator_buffer, allocator_buffer_size, error_reporter);\r\n\r\n         interpreter = &static_interpreter;\r\n         interpreter->AllocateTensors();\r\n         printf(\"Interpreter has %d tensors and %d nodes\\n\",\r\n                 interpreter->tensors_size(), interpreter->operators_size());\r\n\treturn 0;\r\n\r\n}\r\n```\r\n\r\nWhen I run the code, I obtain this in the console : \r\n\r\n```\r\nModel provided is supported.\r\n\r\nInterpreter has 0 tensors and 4 nodes\r\n\r\nDidn't find op for builtin opcode 'UNIDIRECTIONAL_SEQUENCE_LSTM' version '1'\r\n\r\n\r\nFailed to get registration from op code UNIDIRECTIONAL_SEQUENCE_LSTM\r\n \r\n\r\nFailed starting model allocation.\r\n```\r\nCan anybody help me ? Is this network not supported ? ", "comments": ["Can you try with TF nightly?", "Hi @bertankursun As I check the op is not supported in TFlite Micro now.\r\nIf possible, you should tweak your model to only use supported op in: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/all_ops_resolver.cc", "> Can you try with TF nightly?\r\n\r\nI had done this at the beginning : \r\n```\r\n!pip install tf-nightly\r\n\r\nimport tensorflow.compat.v2 as tf\r\ntf.enable_v2_behavior()\r\n\r\n```\r\n\r\nthen I have changed to \"import tensorflow as tf\" and I had another error with the model : \r\nBecause now the subgraph size is 2 whereas it was 1 before...\r\n```\r\nOnly 1 subgraph is currently supported.\r\n```", "> Hi @bertankursun As I check the op is not supported in TFlite Micro now.\r\n> If possible, you should tweak your model to only use supported op in: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/all_ops_resolver.cc\r\n\r\nThanks for the suggestion. I am just confused because my model is very simple and I can not believe that LSTM layer is unsupported.", "@bertankursun take a look at https://github.com/tensorflow/tensorflow/issues/40574#issuecomment-647922464", "> @bertankursun take a look at [#40574 (comment)](https://github.com/tensorflow/tensorflow/issues/40574#issuecomment-647922464)\r\n\r\nThe case is a bit different. @bertankursun is trying to run the model on Micro controller using TFLite Micro.\r\nIssue#40574 is about running it on Android using TFLite.", "@thaink @bhack I have found the UNIDIRECTIONAL_SEQUENCE_LSTM op under tensorflow/lite/kernels. Do you think that can be used by adding to lite/micro/kernels? I started yesterday but it required many other files from the library and it got complicated and some references were not found in the end. ", "@thaink You are right. With just a quick overview I was confused by the `comp:lite` only label as generally we tag with the micro one.", "> @thaink @bhack I have found the UNIDIRECTIONAL_SEQUENCE_LSTM op under tensorflow/lite/kernels. Do you think that can be used by adding to lite/micro/kernels? I started yesterday but it required many other files from the library and it got complicated and some references were not found in the end.\r\n\r\nMost kernels are ported from TFLite to Micro so I think it is possible.\r\ncan some reference commits like https://github.com/tensorflow/tensorflow/commit/172647f58355ae0e87b9160a43da89f420151fd0 and https://github.com/tensorflow/tensorflow/commit/891e3dc6219bc886e5dbd22b268cd097ab2effcd help?", "And make sure the conversion follow https://www.tensorflow.org/lite/convert/rnn", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43084\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43084\">No</a>\n"]}, {"number": 43082, "title": "List of tensor names in graph in Tensorflow", "body": "I got a snippet of code for [List of tensor names in graph in Tensorflow](https://stackoverflow.com/questions/35336648/list-of-tensor-names-in-graph-in-tensorflow)\r\n\r\n```\r\nimport tensorflow as tf\r\na = tf.Variable(5)\r\nb = tf.Variable(6)\r\nc = tf.Variable(7)\r\nd = (a + b) * c\r\n\r\nfor i in tf.get_default_graph().get_operations():\r\n    print (i.name)\r\n```\r\n\r\nbut under tf2.x this ends with error:\r\n\r\n```\r\n2020-09-09 18:24:25.626596: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\nTraceback (most recent call last):\r\n  File \"c:/DATA/Projects/AITEC/unet/ops.py\", line 7, in <module>\r\n    for i in tf.get_default_graph().get_operations():\r\nAttributeError: module 'tensorflow' has no attribute 'get_default_graph'\r\n\r\n```\r\n\r\ncan you advise on this?", "comments": ["@peter197321 \r\n\r\nCan you please replace `tf.get_default_graph().get_operations()` with `tf.compat.v1.get_default_graph().get_operations()` in your code.I am not seeing any issue after replacing with `tf.compat.v1.get_default_graph().get_operations()`.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/2e53b15819b940d4be07158c7945784a/untitled327.ipynb).Thanks!", "under tf1.x, I see the output\r\n\r\nVariable/initial_value\r\nVariable\r\nVariable/Assign\r\nVariable/read\r\nVariable_1/initial_value\r\nVariable_1\r\nVariable_1/Assign\r\nVariable_1/read\r\nVariable_2/initial_value\r\nVariable_2\r\nVariable_2/Assign\r\nVariable_2/read\r\nadd\r\nmul\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/46971586/92631551-cd9e1f00-f2d1-11ea-93f9-fd7ea41ddc6e.png)\r\n\r\nbut under your modification under tf2.x, I see nothing out ... \r\n\r\n![image](https://user-images.githubusercontent.com/46971586/92632272-4a30fd80-f2d2-11ea-9c10-3010707e4ee0.png)\r\n", "TF2 Is in eager mode by default.\r\nAdd `tf.compat.v1.disable_eager_execution()` after your import.", "Now, it works :)\r\n\r\n![image](https://user-images.githubusercontent.com/46971586/92636129-28864500-f2d7-11ea-88a2-d5bd0cbda138.png)\r\n\r\nThank you."]}, {"number": 43081, "title": "[BodyPix] Improve body segmentation(flickering edges) updates for a video stream ", "body": "This is related to body segmentation using body-pix.\r\nbody segmentation has a flickering effect around the edges for a video stream.\r\nEven setting edgeBlurAmount to maximum for the bokeh effect still had flickering effect around the edges since we are continuously trying to predict, segment and update frames from a video. \r\nHad similar flickering around the edges when i get image data from the segmentation and write it to a canvas. Is there any way to minimize/reduce the flickering around the edges?\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tfjs@2.1.0, body-pix@2.0.5\r\n\r\n- Are you willing to contribute it (Yes/No): \r\nYes, I'm willing to help with whatever i can. But,  I'm not familiar with tensorflow models though.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThis is related to body segmentation using body-pix.\r\nbody segmentation has a flickering effect around the edges for a video stream.\r\nEven setting edgeBlurAmount to maximum for the bokeh effect still had flickering effect around the edges since we are continuously trying to predict, segment and update frames from a video. \r\nHad similar flickering around the edges when i get image data from the segmentation and write it to a canvas. Is there any way to minimize/reduce the flickering around the edges?\r\n\r\n**Will this change the current api? How?**\r\nNot sure. Might change current api if additional masks are added.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone in the community who uses body-pix model.\r\n\r\n**Any Other info.**\r\n", "comments": ["I think you need to open this at https://github.com/tensorflow/tfjs", "@mchandrra,\r\nAs mentioned by @bhack, TensorFlow.js issues are tracked in tensorflow/tfjs repo. Could you please submit a new issue from [this link](https://github.com/tensorflow/tfjs/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "Thank you @bhack and @amahendrakar. Submitted a new ticket at https://github.com/tensorflow/tfjs/issues/3902", "Since this issue will be tracked at https://github.com/tensorflow/tfjs/issues/3902, closing this ticket."]}, {"number": 43080, "title": "Support INT16 quantisation for RESIZE_NEAREST_NEIGHBOR", "body": "Per issue #43064 I'm requesting the RESIZE_NEAREST_NEIGHBOR of int16 implementation.\r\n\r\n`RuntimeError: Quantization to 16x8-bit not yet supported for op: 'RESIZE_NEAREST_NEIGHBOR'.` \r\n\r\nThanks.", "comments": ["@peter197321 \r\nplease share the code anf tf version or colab gist for us to analyse.", "Hi @Saduf2019 \r\nthis is the code we have been experimenting with https://gist.github.com/macenpav/404e65a23affc193a428d96a7260a2f4\r\n\r\nThere is a workaround using tf.lite.OpsSet.TFLITE_BUILTINS, but that creates quantize/dequantize nodes, i.e. f32 layer because RESIZE_NEAREST_NEIGHBOR is not supported (in the UpSampling layer).\r\n\r\nOn tensorflow==2.3.0 it prints just:\r\nRuntimeError: Quantization to 16x8-bit not yet supported for op: \r\n\r\nOn tf-nightly (tf_nightly-2.4.0.dev20200910-cp37-cp37m-win_amd64.whl) it shows which op is not supported:\r\nRuntimeError: Quantization to 16x8-bit not yet supported for op: \r\nRESIZE_NEAREST_NEIGHBOR", "Hi @peter197321 ! Above Runtime error is not replicating in the [2.8](https://colab.sandbox.google.com/gist/mohantym/6e25e54f77088017a30e6bb9a5d1f4f0/untitled51.ipynb#scrollTo=ih4AjcuXPSNQ) version now. Can we move this issue to closed status now?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43079, "title": "Added Tensor RuntimeShape check for Tanh kernel", "body": "The Tanh kernel unlike other linear op kernels does not have the RuntimeShape check but instead collects the size of the input as an int which is not checked against the output size. Moreso, it'd be troublesome to only check the size, so I made it use the MatchingFlatSize instead.\r\n\r\nThanks", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43078, "title": "Resolved type confusion on CreateQuantizedTensor test utility API", "body": "I was recently trying to use this API in the test suite and was briefly confused as to which is the input and output.\r\nIt'd be more obvious and more correct to make the input const since we're not modifying it.\r\n\r\nThanks", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43077, "title": "Add BUILD_TYPE=no_tf_lite_static_memory and -Wswitch.", "body": "This fixes #43076.\r\n\r\nManually tested this change on PR #42452 and confirmed that the following command results in an error:\r\n  ```\r\n  make -f tensorflow/lite/micro/tools/make/Makefile -j8 BUILD_TYPE=no_tf_lite_static_memory\r\n  ```", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 43076, "title": "Reproduce internal CI errors via TF Lite Micro Makefile", "body": "@tensorflow/micro\r\n\r\nSee [this comment](https://github.com/tensorflow/tensorflow/pull/42452#issuecomment-689199971) for an internal error, due to -Wswitch that we could not reproduce via the TFLM Makefile.\r\n\r\nThe underlying issue there was a missing -Wswitch in the makefile and an inability to build without -DTF_LITE_STATIC_MEMORY.", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43076\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43076\">No</a>\n"]}, {"number": 43075, "title": "install", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43075\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43075\">No</a>\n"]}]