[{"number": 50783, "title": "Getting clang errors while building tensorflow-lite using bazel on windows machine.", "body": "**System information**\r\n- OS Platform and Distribution : Windows 10 Home \r\n- TensorFlow installed from : source\r\n- TensorFlow version: Building from top of 926e8aaf98078d57175795025bf3f8a93db813bc\r\n- Python version: 3.9.6\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): MSVC 2019\r\n- CUDA/cuDNN version: NA\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm trying to compile tflite with custom operations using bazel. This is the command I'm using -  \r\n`.\\bazel build --cxxopt='--std=c++11' -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/lite/java:tensorflow-lite` but the build is failing every-time with the following errors - \r\n\r\n```\r\n C++ compilation of rule '@com_google_absl//absl/base:base' failed (Exit 1): clang failed: error executing command\r\n  cd C:/users/gojo/_bazel_gojo/o26qhjr4/execroot/org_tensorflow\r\n  SET ANDROID_BUILD_TOOLS_VERSION=30.0.3\r\n    SET ANDROID_NDK_API_LEVEL=21\r\n    SET ANDROID_NDK_HOME=C:/Users/gojo/AppData/Local/Android/Sdk/ndk/21.4.7075529\r\n    SET ANDROID_SDK_API_LEVEL=30\r\n    SET ANDROID_SDK_HOME=C:/Users/gojo/AppData/Local/Android/Sdk\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\Program Files\\jdk-11.0.2\\;C:\\Users\\gojo\\Downloads\\bazel.exe;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Git\\cmd;C:\\msys64\\usr\\bin;C:\\Users\\gojo\\AppData\\Roaming\\Python\\Python39\\Scripts;C:\\Users\\gojo\\Downloads\\bazel.exe;C:\\Users\\gojo\\AppData\\Local\\Programs\\Python\\Python39\\Scripts\\;C:\\Users\\gojo\\AppData\\Local\\Programs\\Python\\Python39\\;C:\\Users\\gojo\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\gojo\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/gojo/AppData/Local/Programs/Python/Python39/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/gojo/AppData/Local/Programs/Python/Python39/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -D__ANDROID_API__=21 -isystemexternal/androidndk/ndk/sysroot/usr/include/arm-linux-androideabi -target armv7-none-linux-androideabi -march=armv7-a -mfloat-abi=softfp -mfpu=vfpv3-d16 -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/windows-x86_64 -fpic -no-canonical-prefixes -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -funwind-tables -fstack-protector-strong -fno-addrsig -Werror=return-type -Werror=int-to-pointer-cast -Werror=pointer-to-int-cast -Werror=implicit-function-declaration -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/base/_objs/base/cycleclock.pic.d -frandom-seed=bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/base/_objs/base/cycleclock.pic.o -fPIC -iquote external/com_google_absl -iquote bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /std:c++14 --std=c++11 -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c external/com_google_absl/absl/base/internal/cycleclock.cc -o bazel-out/android-armeabi-v7a-opt/bin/external/com_google_absl/absl/base/_objs/base/cycleclock.pic.o\r\nExecution platform: @local_execution_config_platform//:platform\r\nclang: error: no such file or directory: '/W0'\r\nclang: error: no such file or directory: '/D_USE_MATH_DEFINES'\r\nclang: error: no such file or directory: '/experimental:preprocessor'\r\nclang: error: no such file or directory: '/d2ReducedOptimizeHugeFunctions'\r\nclang: error: no such file or directory: '/std:c++14'\r\nTarget //tensorflow/lite/java:tensorflow-lite failed to build\r\n```\r\n", "comments": ["Anybody there :) ???", "@Gojo1729 \r\nCould you please use stable version of tf and let us know if you still face the error, you may try with tf 2.4.1 or tf 2.5\r\nEnsure you have the system requirements verified [os/python 64 bits, avx support]", "@saberkun the issue was resolved after installing `Clang` ."]}, {"number": 50782, "title": "[TF-TRT] Removing TRT 5 & 6 from the code base - Large Cleanup", "body": "This PR removes TRT 5 & 6 from the codebase and performs a large cleanup / refactoring", "comments": ["Unittest results with TRT 8\r\n\r\n```bash\r\nINFO: 603 processes: 81 internal, 522 local.\r\nINFO: Build completed successfully, 603 total actions\r\n//tensorflow/compiler/tf2tensorrt:convert_graph_test                     PASSED in 9.1s\r\n//tensorflow/compiler/tf2tensorrt:convert_graph_test_gpu                 PASSED in 8.7s\r\n//tensorflow/compiler/tf2tensorrt:convert_nodes_test                     PASSED in 144.2s\r\n//tensorflow/compiler/tf2tensorrt:convert_nodes_test_gpu                 PASSED in 144.1s\r\n//tensorflow/compiler/tf2tensorrt:segment_test                           PASSED in 0.2s\r\n//tensorflow/compiler/tf2tensorrt:segment_test_gpu                       PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:tensorrt_test_cc                       PASSED in 4.1s\r\n//tensorflow/compiler/tf2tensorrt:tensorrt_test_cc_gpu                   PASSED in 4.2s\r\n//tensorflow/compiler/tf2tensorrt:trt_allocator_test                     PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_op_test                     PASSED in 2.9s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_op_test_gpu                 PASSED in 2.4s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test           PASSED in 2.4s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test_gpu       PASSED in 2.2s\r\n//tensorflow/compiler/tf2tensorrt:trt_lru_cache_test                     PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test   PASSED in 7.0s\r\n//tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test_gpu PASSED in 7.0s\r\n\r\nExecuted 16 out of 16 tests: 16 tests pass.\r\n\r\n# =============================================================================== #\r\n\r\nINFO: 3 processes: 1 internal, 2 local.\r\nINFO: Build completed successfully, 3 total actions\r\n//tensorflow/python/compiler/tensorrt:trt_convert_test                   PASSED in 56.8s\r\n//tensorflow/python/compiler/tensorrt:trt_convert_test_gpu               PASSED in 56.8s\r\n\r\nINFO: Build completed successfully, 3 total actions\r\n\r\n# =============================================================================== #\r\n\r\nRunning ./tensorflow/python/compiler/tensorrt/test/annotate_max_batch_sizes_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/base_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/batch_matmul_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/biasadd_matmul_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/binary_tensor_weight_broadcast_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/cast_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/combined_nms_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/concatenation_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/const_broadcast_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/conv2d_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/dynamic_input_shapes_test.py ... [IGNORED]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/identity_output_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/int32_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/lru_cache_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/memory_alignment_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/multi_connection_neighbor_engine_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/neighboring_engine_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/quantization_mnist_test.py ... [IGNORED]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/quantization_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/rank_two_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/reshape_transpose_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/tf_function_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/topk_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/trt_mode_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/unary_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/vgg_block_nchw_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/vgg_block_test.py ... [SUCCESS]\r\n```", "@bixia1 change requested applied - PR squashed"]}, {"number": 50781, "title": "tf.keras.preprocessing.timeseries_dataset_from_array function is broken in latest code", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- TensorFlow installed from (source or binary): latest source\r\n- TensorFlow version (use command below): latest source\r\n- Python version: 3.x.x\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nThe error says that there is a circular Keras reference. However , if i install last month's code, it works.\r\n\r\n**Describe the expected behavior**\r\n", "comments": ["@summa-code \r\n\r\nIs this the [link1](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array), or [link2](https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/preprocessing/timeseries.py#L26-L219)  you are talking about, if no Could you please share the broken  link.Thanks", "Yes, those are the links. Anything changed ?", "Should i get the latest code and test it ? Let me know... I can do testing part of it.", "@summa-code \r\n\r\nCould you please try [this](https://github.com/tensorflow/tensorflow/tree/v2.5.0/tensorflow/python/keras/preprocessing) and let us know if it helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Huh ? Yes, it is still a problem with latest code.\r\n\r\nhttps://www.tensorflow.org/tutorials/structured_data/time_series\r\n\r\nUse this example. This will throw the error. I pulled the code today and built. Same error.\r\n\r\n```from keras import __version__\r\nImportError: cannot import name '__version__' from partially initialized module 'keras' (most likely due to a circular import) (/home/xxxx/.virtualenvs/tensorflow/lib/python3.8/site-packages/keras/__init__.py)```", "I cleaned and reinstalled, there is some version compatibility. I am closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50781\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50781\">No</a>\n"]}, {"number": 50780, "title": "[TF-TRT] ActivationLayer inaccuracy in TRT 8 - Unittest fix", "body": "This PR fixes the unit test to workaround NVBug 3322482.\r\nAdds a small tolerance to the unit test to workaround a bug with TRT 8.", "comments": []}, {"number": 50779, "title": "when i use tf2.4, tf.image_resize takes more time than tf1.2", "body": "when i use tf.compact.v1 to run the tensorflow1.2 code , find that tf.image_resize takes 40% longer\r\nBesides\uff0cunpool and conv2d_transpose encountered the same phenomenon\r\nIs this a bug or a normal phenomenon?", "comments": ["No this is normal phenomenon, since Tensorflow 1 uses only graph execution and thus has higher performance. On the other hand \r\nTensorflow 2 uses Eager execution by default and is slower than Tensorflow 1, but easier to debug. You can increase your performance in Tendorflow 2 by using @tf.function above the function declaration to execute the piece of code as a graph. ", "@superlxt \r\nPlease refer to [this link](https://towardsdatascience.com/eager-execution-vs-graph-execution-which-is-better-38162ea4dbf6) and final notes for better understanding. [also [link1](https://stackoverflow.com/questions/58441514/why-is-tensorflow-2-much-slower-than-tensorflow-1])]\r\n", "> No this is normal phenomenon, since Tensorflow 1 uses only graph execution and thus has higher performance. On the other hand\r\n> Tensorflow 2 uses Eager execution by default and is slower than Tensorflow 1, but easier to debug. You can increase your performance in Tendorflow 2 by using @tf.function above the function declaration to execute the piece of code as a graph.\r\n\r\nSo you mean that although I use tf.compact.v1 to run the code on tf2.4, it is still executed by eager?\r\nBy the way, if I want to use tf.funcation to speed up, how should I implement it\u3002\r\nthanks", "At the same time, if I only use conv to build the network, tf1.2 and tf2.4 get the same speed. If tf.image.resize, unpool, conv_transpose are added after conv, the time consuming in tf2.4 will increase significantly\u3002This is why I think this is a bug\u3002", "@superlxt \r\nResizing image requires interpolation which are mathematically more expensive than matrix multiplication in case of convolution. As they are served into a graph the various tf.Operation objects are executed and are accelerated in case of graph execution while in Eager execution doesn't provide this opportunity of acceleration.\r\n ", "Also even if you use tf.compat.v1 it uses Tensorflow 2 as backend and executes Eager Execution which is default. \r\nYou can find examples of @tf.function [here](https://www.tensorflow.org/api_docs/python/tf/function?version=nightly)", "@superlxt \r\nKindly open this issue in discussion forum/stackoverflow and move it to closed status here.", "@Saduf2019 \r\nok,I will try to see if tf.function works, and I will close this question after I confirm", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50779\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50779\">No</a>\n"]}, {"number": 50778, "title": "Delete curl-to-sh", "body": null, "comments": []}, {"number": 50777, "title": "gast_archive URL error", "body": "There's an error in the mirrored URL for gast_archive:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e7017fd09cc249d0d522a2b77f0dcae4642dd04a/tensorflow/workspace2.bzl#L441\r\nThat URL gives an error, \"The specified key does not exist.\"  The URL should be `https://storage.googleapis.com/mirror.tensorflow.org/files.pythonhosted.org/packages/83/4a/07c7e59cef23fb147454663c3271c21da68ba2ab141427c20548ae5a8a4d/gast-0.4.0.tar.gz`\r\n\r\nWe've only whitelisted storage.googleapis.com in our firewall, so when the build tries to fall back to the official files.pythonhosted.org source, it fails.", "comments": ["@kevininderhees \r\nA pr is being created to update the changes, this issue will be closed once the [pr](https://github.com/tensorflow/tensorflow/pull/50882) is merged.", "Moving this to closed status pr is merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50777\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50777\">No</a>\n"]}, {"number": 50775, "title": "Fix build after previous cherrypick (#50773).", "body": "This is needed because of refactoring in 7c3f96bf324cebe443a7eb814ce771c66c073998 (on master). The cherrypick in #50773 includes code after this refactoring but for all releases before 2.6 we need this fix too.", "comments": []}, {"number": 50773, "title": "Fix segmentation fault in shape inference logic.", "body": "When running shape functions, some functions (such as `MutableHashTableShape`)\r\nproduce extra output information in the form of a `ShapeAndType` struct.  The\r\nshapes embedded in this struct are owned by an inference context that is\r\ncleaned up almost immediately; if the upstream code attempts to access this\r\nshape information, it can trigger a segfault.\r\n\r\n`ShapeRefiner` is mitigating this for normal output shapes by cloning them\r\n(and thus putting the newly created shape under ownership of an inference\r\ncontext that will not die), but we were not doing the same for shapes and\r\ntypes.  This commit fixes that by doing similar logic on output shapes and\r\ntypes.\r\n\r\nPiperOrigin-RevId: 384761124\r\nChange-Id: I07c0c42d29dfbb55bfa13ec1f09ef825fb0a1a1d", "comments": []}, {"number": 50772, "title": "r2.6 cherry-pick request: Update ACL and oneDNN versions in mkl_aarch64 build", "body": "This cherry-pick:\r\n* Ensures the build flags are consistent on AArch64 between TF2.5 and 2.6\r\n* Provides a number of bug fixes via ACL which impact the functioning of the convolutions under certain conditions.\r\n* Fixes an issue with scaling ACL to high core counts (cost of spinning up many threads has been reduced).\r\n* Keeps the oneDNN version consistent with x86.\r\n\r\nOriginal PR [#50757](https://github.com/tensorflow/tensorflow/pull/50757) was merged yesterday (Tue 7/13) but not in time for the nightly branch cut. So we'll have to wait for Wed 7/14 nightly tests.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50772) for more info**.\n\n<!-- need_author_consent -->", "Manually setting CLA to yes because this commit is from PR #50708, which is already merged into master."]}, {"number": 50771, "title": "Bump the JPEG dependency to 2.1.0", "body": "Handles [CVE-2020-17541](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-17541)\r\n\r\nPiperOrigin-RevId: 384722318\r\nChange-Id: I3f29c39acf3be527486f82d02c5b348f5ea4b153", "comments": ["As this breaks TFLite and TF 2.0.5 handles the CVE, we don't need this."]}, {"number": 50770, "title": "Bump curl dependency to 7.77.0", "body": "Handles the following CVEs:\r\n\r\n* [CVE-2021-22901](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22901)\r\n* [CVE-2021-22898](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22898)\r\n* [CVE-2021-22876](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22876)\r\n* [CVE-2021-22897](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22897)\r\n\r\nPiperOrigin-RevId: 384576784\r\nChange-Id: Iaf4f499736039ea957efb0af596d1a46f3062797", "comments": []}, {"number": 50769, "title": "apply_gradients() with Adam Optimizer gives error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: maOS 11.4\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.9.6\r\n- GPU model and memory: Apple M1 \r\n\r\n**Describe the current behavior**\r\nSGD and Adam gives error when using **apply_gradients()**, please see output and source code below \r\nWhen adam is used in **model.fit()** it works normally\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Other info / logs** \r\nInstalled TensorFlow after following instructions from [Getting Started with tensorflow-metal PluggableDevice](https://developer.apple.com/metal/tensorflow-plugin/)\r\n\r\nSource code for training: \r\n```\r\n'''---- Training -----'''\r\nmodel=tf.keras.Sequential() # model has 1 hidden layer\r\nmodel.add(tf.keras.layers.Dense(20,input_shape=(dimVectors,),activation='relu'))\r\nmodel.add(tf.keras.layers.Dense(5,activation='softmax')) # 5 is ouput shape\r\noptimizer=tf.keras.optimizers.Adam(\r\n    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\r\n    name='Adam'\r\n)\r\nepochs=10\r\n# without padding\r\nwordVectors=tf.Variable(tf.convert_to_tensor(wordVectors))\r\ntrainLabels=tf.keras.utils.to_categorical(trainLabels,5)\r\nfor epoch in range(epochs):\r\n    sent_count=0\r\n    print(\"---Epoch no:\",epoch,\"----\")\r\n    #one sentence in each batch\r\n    for sent in train_Sentences:\r\n        if((sent_count%10)==0):\r\n            print(\"Optimizing sentence number\",sent_count+1)\r\n        sent_count+=1\r\n        len_sent=len(sent)\r\n        one_hot=np.zeros((len_sent,total_words))\r\n        label=(trainLabels[sent_count-1]).reshape(-1,1)\r\n        label=tf.convert_to_tensor(label)\r\n        for i,word in enumerate(sent):\r\n            index=token_dict[word]\r\n            one_hot[i,index]=1\r\n        one_hot=tf.convert_to_tensor(one_hot)\r\n        with tf.GradientTape() as tape:\r\n            #tape.watch(wordVectors)\r\n            feature=tf.matmul(one_hot,wordVectors)\r\n            feature_sum=tf.math.reduce_sum(feature,axis=0,keepdims=True)\r\n            y_pred=model(feature_sum)\r\n            loss=tf.losses.MeanSquaredError()(y_pred,label)\r\n        gradients=tape.gradient(loss,[wordVectors]+model.trainable_variables)\r\n        if(((sent_count-1)%10)==0):\r\n                print(\"Loss: \",loss)\r\n        '''wordVectors.assign(wordVectors-learning_rate*gradients[0])\r\n        (model.trainable_variables[0]).assign(model.trainable_variables[0]-learning_rate*gradients[1])\r\n        (model.trainable_variables[1]).assign(model.trainable_variables[1]-learning_rate*gradients[2])\r\n        (model.trainable_variables[2]).assign(model.trainable_variables[2]-learning_rate*gradients[3])\r\n        (model.trainable_variables[3]).assign(model.trainable_variables[3]-learning_rate*gradients[4])'''\r\n        optimizer.apply_gradients(zip(gradients,[wordVectors]+model.trainable_variables))\r\n    \r\n```\r\nTraceback :\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/arpitjjw/Desktop/Training Program/Module 2/Sentiment_day1/model-tf.py\", line 152, in <module>\r\n    optimizer.apply_gradients(zip(gradients,[wordVectors]+model.trainable_variables))\r\n  File \"/Users/arpitjjw/miniforge3/envs/tf-m1/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 667, in apply_gradients\r\n    return self._distributed_apply(strategy, grads_and_vars, name,\r\n  File \"/Users/arpitjjw/miniforge3/envs/tf-m1/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 714, in _distributed_apply\r\n    update_op = distribution.extended.update(\r\n  File \"/Users/arpitjjw/miniforge3/envs/tf-m1/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 2576, in update\r\n    return self._update(var, fn, args, kwargs, group)\r\n  File \"/Users/arpitjjw/miniforge3/envs/tf-m1/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 3622, in _update\r\n    return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\r\n  File \"/Users/arpitjjw/miniforge3/envs/tf-m1/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 3628, in _update_non_slot\r\n    result = fn(*args, **kwargs)\r\n  File \"/Users/arpitjjw/miniforge3/envs/tf-m1/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 597, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/Users/arpitjjw/miniforge3/envs/tf-m1/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 697, in apply_grad_to_update_var\r\n    update_op = self._resource_apply_dense(grad, var, **apply_kwargs)\r\n  File \"/Users/arpitjjw/miniforge3/envs/tf-m1/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/adam.py\", line 172, in _resource_apply_dense\r\n    return gen_training_ops.ResourceApplyAdam(\r\n  File \"/Users/arpitjjw/miniforge3/envs/tf-m1/lib/python3.9/site-packages/tensorflow/python/util/tf_export.py\", line 404, in wrapper\r\n    return f(**kwargs)\r\n  File \"/Users/arpitjjw/miniforge3/envs/tf-m1/lib/python3.9/site-packages/tensorflow/python/ops/gen_training_ops.py\", line 1427, in resource_apply_adam\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/Users/arpitjjw/miniforge3/envs/tf-m1/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 6897, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'ResourceApplyAdam' OpKernel for 'GPU' devices compatible with node {{node ResourceApplyAdam}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_DOUBLE, use_locking=true, use_nesterov=false\r\n\t.  Registered:  device='GPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n [Op:ResourceApplyAdam]\r\n```\r\n", "comments": ["@arpitjjw  It looks like your Issue relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. As ([previously announced](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999/)), all future development of Keras is expected to happen in the [github.com/keras-team/keras](github.com/keras-team/keras) repository. If your issue lies with the TF-Core area please comment back with your explanation and we can look into it further. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50769\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50769\">No</a>\n"]}, {"number": 50768, "title": "Revert \"GPU Symmetric Fake Quantization\"", "body": "Reverts tensorflow/tensorflow#50519", "comments": ["@Xhark  I have noticed that no file changes in this PR? Shall I close it?", "@gbaned -- this PR effectively records the fact that commit https://github.com/tensorflow/tensorflow/commit/e4f883065 _already_ (otherwise silently) rolled back #50519 .  This empty PR should probably show as merged just for clarity of tracking on the history of the original PR.\r\n\r\nAt some point then we can sort out what test failure led to the rollback so @philipphack can resubmit the original PR accordingly; unless @Xhark 's test tolerance changes already addressed that?  And if so then we should (a) merge this bookkeeping PR _and_ (b) go ahead and resubmit the original PR.  @Xhark can comment on the status/next steps in this regard.", "Unfortunately, the CI infrastructure and the stack used to convert between GitHub and internal code does not work in this case.\r\n\r\nSo we'll have to close the PR unmerged. Sorry about this."]}, {"number": 50767, "title": "profiling: memory_info: add support for new mallinfo API", "body": "Starting with glibc 2.33 the mallinfo API is deprecated\r\nin favor of the newer mallinfo2 which returns the exact\r\nsame struct but with size_t elements instead of int.\r\n\r\nFor reference:\r\nhttps://man7.org/linux/man-pages/man3/mallinfo.3.html", "comments": ["Friendly ping? :smiley: "]}, {"number": 50766, "title": "Incomplete Installation of Tensorflow FreeBSD", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): FreeBSD 13\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: v2.1.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.29.0\r\n- GCC/Compiler version (if compiling from source):  clang version 11.0.1\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhile tensorflow will build on FreeBSD, the installation is incomplete and only provides lib/python3.8/site-packages/tensorflow_core with nothing else.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nCommands for building:\r\n`        @cd ${WRKSRC} && \\\r\n                bazel --bazelrc=\"${WRKDIR}/bazelrc\" ${BAZEL_BOOT} build ${BAZEL_COPT} --host_copt=\"-I${LOCALBASE}/include\" \\\r\n                --host_linkopt=\"-L${LOCALBASE}/lib\" --linkopt=\"-L${LOCALBASE}/lib\" --copt=\"-I${LOCALBASE}/include\" \\\r\n                --verbose_failures -s --incompatible_restrict_string_escapes=false \\\r\n                --distdir=${WORKDIR}/bazel-dist --linkopt='-fuse-ld=lld' \\\r\n                //tensorflow:libtensorflow_framework.so \\\r\n                //tensorflow:libtensorflow.so \\\r\n                //tensorflow:libtensorflow_cc.so \\\r\n                //tensorflow/tools/pip_package:build_pip_package\r\n`\r\nThe creation of the pip package:\r\n\r\n`\r\n        @cd ${WRKSRC} && ${SETENV} TMPDIR=${WRKDIR} \\\r\n                bazel-bin/tensorflow/tools/pip_package/build_pip_package \\\r\n                ${WRKDIR}/whl\r\n`\r\n\r\nThe following variables in the above command contain the following information:\r\n\r\nLOCALBASE=/usr/local\r\nBAZEL_BOOT= --output_user_root=${WRKDIR}/bazel_out\r\nBAZEL_COPT=\"-c opt --copt=-march=native --copt=-mfpmath=sse\"\r\n\r\n**Any other info / logs**\r\nI'm assuming the problem is just from the build files failing to default to a suitable platform and is falling back to default values. If I could be pointed into the right direction and files so that I could move forward and patch them it would be helpful.\r\n", "comments": ["@Amzo \r\n\r\nCould you please refer the similar [link1](https://github.com/tensorflow/tensorflow/issues/193) and [link2](https://github.com/tensorflow/tensorflow/issues/37345) and let us know if it helps.Thanks", "Nope. those seem to be build issues. I've remedied and patched them and tensorflow builds successfully. However, on the creation of the pip packages, there only seems to be usr/lib/python3.8/site-packages/tensorflow_core available. All other directories are not being installed during pip creation, even though I see they have been built. \r\n\r\nI'm not sure if there is some platform checks I've overlooked during build that I've failed to catch. ", "@Amzo \r\nCan you please try on virtual env and let us know. Also verify the prerequisites like python/os is 64 bits, avx is supported.", "I'll put this on hold for now, and see if I can get tensorflow 2.5.x to build on FreeBSD once I finish through the documentation to create a toolchain to allow it to build, as it currently errors out about no toolchains being found for now. Mostly due to there being no support for FreeBSD in bazel toolchain, and currently no docker support. \r\n\r\nI'll see if the problem still persists in the newer version of tensorflow. Might be easier to just run a virtual env in the linuxulator on FreeBSD and avoid a native port.", "I see on https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=250646 you said you have not submitted the fix, is there any reason why.\r\nI am trying to build photoprism (great google photo replacement), and it needs tensorflow :(", "The patch only fixes the import of tensorflow by a broken stage in the Makefile, a lot of stuff is still missing. photoprism only needs libtensorflow which the makefile builds, if you're on freebsd you can build tensorflow from the port and take the library files from the build directory until i finish working out issues with the port. Though this is off topic. i should have things fixed in a day or two. For your issue to build protoprism i can submit a patch tomorrow to package the tensorflow libraries in.", "Oh ok wow, thanks a bunch! and take your time. Silly me thought you fixed it but just haven't submitted it yet.  \r\nEdit: Got it to work, thanks a bunch!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50766\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50766\">No</a>\n", "@Amzo - Were you able to make progress?", "Yeah, i upgraded to 2.1.4 and fixed the packaging in the Makefile, and it works as intended on freebsd.", "@Amzo Appreciate the quick response!\r\n\r\nIs your updated code available in ports or was it something you just did locally on your m/c?\r\nIf you had locally compiled on your m/c how long did it take?\r\n\r\nAlso, can you point to any specifics that one to be aware of to compile TF on FreeBSD\r\n\r\n", "Code hasn't been merged yet, but this should work on FreeBSD, best to just email me about it or report on there instead of on here.\r\n\r\nhttps://github.com/Amzo/FreeBSD-Tensorflow/tree/master/science/py-tensorflow", "how much time did it take to compile TF?\r\n\r\n"]}, {"number": 50764, "title": "AssertionError occurred when initializing NLClassifier with TFlite model", "body": "\r\nI am using NLClassifier in my android application for a text classification problem\r\n\r\nDependency used - **implementation 'org.tensorflow:tensorflow-lite-task-text:0.2.0'**\r\n\r\nI am downloading the TFlite model from firebase using FirebaseModelDownloader with the following code\r\nDependency used - **implementation 'com.google.firebase:firebase-ml-modeldownloader-ktx:24.0.0'**\r\n\r\n```\r\nval conditions = CustomModelDownloadConditions.Builder().build()\r\n    FirebaseModelDownloader.getInstance()\r\n    .getModel(ADULT_FILTER_MODEL, DownloadType.LOCAL_MODEL_UPDATE_IN_BACKGROUND, conditions)\r\n    .addOnSuccessListener { model: CustomModel? ->\r\n        model?.file?.let { modelFile ->\r\n            try {\r\n                val classifier = NLClassifier.createFromFile(model)\r\n            } catch (e: IOException) {\r\n                e.printStackTrace()\r\n            }\r\n        }\r\n\r\n    }\r\n    .addOnFailureListener {\r\n        // failure handling\r\n    }\r\n```\r\n\r\n**Describe the current behavior**\r\nI am getting AssertionError while creating NLClassifier instance with given line of code irregularly\r\n`val classifier = NLClassifier.createFromFile(model)`\r\n\r\nStackTrace - \r\n_Fatal Exception: java.lang.AssertionError: Error occurred when initializing NLClassifier: The model is not a valid Flatbuffer buffer\r\n       at org.tensorflow.lite.task.text.nlclassifier.NLClassifier.initJniWithFileDescriptor(NLClassifier.java)\r\n       at org.tensorflow.lite.task.text.nlclassifier.NLClassifier.access$000(NLClassifier.java:67)\r\n       at org.tensorflow.lite.task.text.nlclassifier.NLClassifier$1.createHandle(NLClassifier.java:195)\r\n       at org.tensorflow.lite.task.core.TaskJniUtils.createHandleFromLibrary(TaskJniUtils.java:91)\r\n       at org.tensorflow.lite.task.text.nlclassifier.NLClassifier.createFromFileAndOptions(NLClassifier.java:191)\r\n       at org.tensorflow.lite.task.text.nlclassifier.NLClassifier.createFromFile(NLClassifier.java:161)_\r\n\r\n\r\n**Describe the expected behavior**\r\nI should not get the Error while creating NlClassifier on an irregular basis\r\n\r\nAny reason for this and a solution?\r\n\r\n", "comments": ["First, please make sure the model is a valid TFLite model. You can do so by using the [Python Interpreter](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python) and see if the model can be successfully loaded into Interpreter.\r\n\r\nIf the model is valid, please try the two answers in this [StackOverflow question](https://stackoverflow.com/questions/66958171/custom-machine-learning-code-to-import-in-the-android-studio). It seems to be a common issue from Firebase.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50764\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50764\">No</a>\n"]}, {"number": 50763, "title": "ModuleNotFounderror:no module named tensorflor_core.estimator ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@kalyani-o \r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50763\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50763\">No</a>\n"]}, {"number": 50761, "title": "Cannot login to jupyter notebook by Docker Tensorlfow", "body": "My docker version\r\n\r\n[![enter image description here][1]][1]\r\n\r\nI run `docker pull tensorflow/tensorflow:latest-jupyter`\r\n\r\n[![enter image description here][2]][2]\r\n\r\n\r\nlog\r\n\r\n[![enter image description here][3]][3]\r\n\r\n```\r\n[I 03:09:11.380 NotebookApp](B Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\r\n\r\njupyter_http_over_ws extension initialized. Listening on /http_over_websocket\r\n\r\n[I 03:09:11.626 NotebookApp](B Serving notebooks from local directory: /tf\r\n\r\n[I 03:09:11.626 NotebookApp](B Jupyter Notebook 6.3.0 is running at:\r\n\r\n[I 03:09:11.626 NotebookApp](B http://2271c0952dc1:8888/?token=43b3f33c0c9ff0fe3420c21ca3505865e9396dd613412ce7\r\n\r\n[I 03:09:11.626 NotebookApp](B or http://127.0.0.1:8888/?token=43b3f33c0c9ff0fe3420c21ca3505865e9396dd613412ce7\r\n\r\n[I 03:09:11.626 NotebookApp](B Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\n\r\n[C 03:09:11.632 NotebookApp](B\r\n\r\n\r\nTo access the notebook, open this file in a browser:\r\n\r\nfile:///root/.local/share/jupyter/runtime/nbserver-1-open.html\r\n\r\nOr copy and paste one of these URLs:\r\n\r\nhttp://2271c0952dc1:8888/?token=43b3f33c0c9ff0fe3420c21ca3505865e9396dd613412ce7\r\n\r\nor http://127.0.0.1:8888/?token=43b3f33c0c9ff0fe3420c21ca3505865e9396dd613412ce7\r\n\r\n[C 03:13:56.481 NotebookApp](B received signal 15, stopping\r\n\r\n[I 03:13:56.483 NotebookApp](B Shutting down 0 kernels\r\n\r\n[I 03:13:56.483 NotebookApp](B Shutting down 0 terminals\r\n\r\njupyter_http_over_ws extension initialized. Listening on /http_over_websocket\r\n\r\n[I 03:14:01.997 NotebookApp](B Serving notebooks from local directory: /tf\r\n\r\n[I 03:14:01.998 NotebookApp](B Jupyter Notebook 6.3.0 is running at:\r\n\r\n[I 03:14:01.998 NotebookApp](B http://2271c0952dc1:8888/?token=e52451166ea610fe5559c59eee0f3865e3af25b44c4966d6\r\n\r\n[I 03:14:01.998 NotebookApp](B or http://127.0.0.1:8888/?token=e52451166ea610fe5559c59eee0f3865e3af25b44c4966d6\r\n\r\n[I 03:14:01.998 NotebookApp](B Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\n\r\n[C 03:14:02.003 NotebookApp](B\r\n\r\n\r\nTo access the notebook, open this file in a browser:\r\n\r\nfile:///root/.local/share/jupyter/runtime/nbserver-1-open.html\r\n\r\nOr copy and paste one of these URLs:\r\n\r\nhttp://2271c0952dc1:8888/?token=e52451166ea610fe5559c59eee0f3865e3af25b44c4966d6\r\n\r\nor http://127.0.0.1:8888/?token=e52451166ea610fe5559c59eee0f3865e3af25b44c4966d6\r\n```\r\n[![enter image description here][4]][4]\r\n\r\nI enter token `e52451166ea610fe5559c59eee0f3865e3af25b44c4966d6` but cannot login.\r\n\r\nEven, when I use command line to set password, it still did not work\r\n\r\n[![enter image description here][5]][5]\r\n\r\nI also enter token then set password, then login, but not success. How to login success?\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/SaJZ8.png\r\n  [2]: https://i.stack.imgur.com/HnLkB.png\r\n  [3]: https://i.stack.imgur.com/JvTVB.png\r\n  [4]: https://i.stack.imgur.com/W8LDx.png\r\n  [5]: https://i.stack.imgur.com/8gGqm.png\r\n\r\n( https://stackoverflow.com/questions/68371609/how-to-login-to-jupyter-notebook-generated-by-tensorflow-docker )\r\n", "comments": ["@donhuvy \r\nPlease refer to [this link](https://www.programmersought.com/article/95383088204/) and let us know.", "thank you, please wait me.", "The link you have given to me is in 2015, it quite old.\r\n\r\nIMAGE ID: 1b40bdf7b060\r\n\r\n![image](https://user-images.githubusercontent.com/1328316/125712146-bb5dd33e-1469-4caa-94b8-361e232cbe5a.png)\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/1328316/125712005-6cd12767-0c91-4368-b746-6acbfa7a91c5.png)\r\n\r\nI did not see it.\r\n\r\n![image](https://user-images.githubusercontent.com/1328316/125711941-dbd2a436-7e7c-45a5-af75-10863bf34bac.png)\r\n\r\n```\r\n# cat /root/.jupyter_notebook_config.py\r\ncat: /root/.jupyter_notebook_config.py: No such file or directory\r\n# dir\r\ntensorflow-tutorials\r\n# pwd\r\n/tf\r\n# ls -la\r\ntotal 12\r\ndrwxrwxrwx 1 root root 4096 May 13 18:29 .\r\ndrwxr-xr-x 1 root root 4096 Jul 14 03:29 ..\r\ndrwxrwxrwx 1 root root 4096 May 13 18:29 tensorflow-tutorials\r\n# cd ..\r\n# ls -la\r\ntotal 84\r\ndrwxr-xr-x   1 root root 4096 Jul 14 03:29 .\r\ndrwxr-xr-x   1 root root 4096 Jul 14 03:29 ..\r\n-rwxr-xr-x   1 root root    0 Jul 14 03:29 .dockerenv\r\ndrwxrwxrwx   2 root root 4096 May 13 18:29 .local\r\ndrwxr-xr-x   2 root root 4096 Apr 16 14:54 bin\r\ndrwxr-xr-x   2 root root 4096 Apr 24  2018 boot\r\ndrwxr-xr-x   5 root root  360 Jul 15 01:04 dev\r\ndrwxr-xr-x   1 root root 4096 Jul 14 03:29 etc\r\ndrwxr-xr-x   2 root root 4096 Apr 24  2018 home\r\ndrwxr-xr-x   1 root root 4096 May 13 18:28 lib\r\ndrwxr-xr-x   2 root root 4096 Apr 16 14:53 lib64\r\ndrwxr-xr-x   2 root root 4096 Apr 16 14:45 media\r\ndrwxr-xr-x   2 root root 4096 Apr 16 14:45 mnt\r\ndrwxr-xr-x   2 root root 4096 Apr 16 14:45 opt\r\ndr-xr-xr-x 177 root root    0 Jul 15 01:04 proc\r\ndrwx------   1 root root 4096 Jul 14 03:29 root\r\ndrwxr-xr-x   1 root root 4096 Apr 23 22:21 run\r\ndrwxr-xr-x   1 root root 4096 Apr 23 22:21 sbin\r\ndrwxr-xr-x   2 root root 4096 Apr 16 14:45 srv\r\ndr-xr-xr-x  11 root root    0 Jul 15 01:04 sys\r\ndrwxrwxrwx   1 root root 4096 May 13 18:29 tf\r\ndrwxrwxrwt   1 root root 4096 May 13 18:29 tmp\r\ndrwxr-xr-x   1 root root 4096 Apr 16 14:45 usr\r\ndrwxr-xr-x   1 root root 4096 Apr 16 14:54 var\r\n# cd tf\r\n# ls -laf\r\ntensorflow-tutorials  ..  .\r\n# ls\r\ntensorflow-tutorials\r\n# tree\r\n/bin/sh: 10: tree: not found\r\n#\r\n```", "@donhuvy,\r\n\r\nCan you try following this [comment1](https://github.com/tensorflow/tensorflow/issues/47074#issuecomment-846574592), [comment2](https://github.com/tensorflow/tensorflow/issues/6351#issuecomment-268701445) and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50761\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50761\">No</a>\n"]}, {"number": 50760, "title": "[ROCm] Enable xlogy and xlog1py for HIP GPU.", "body": "Relax tolerance for xlogy for complex64 in gpu_binary_ops_test to enable that test to pass on HIP.", "comments": []}, {"number": 50759, "title": "[determinism] Fix line-wrap in r2.6 release notes", "body": "This PR is a follow-up to PR [50640](https://github.com/tensorflow/tensorflow/pull/50640). It removes the erroneously included 80-character line-breaks in the content added to `RELEASE.md`. This was prompted by noticing that the formatting was wrong in the [v2.6.0-rc1](https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0-rc1) release notes web page.", "comments": ["Tagging @mihaimaruseac"]}, {"number": 50757, "title": "Update of Compute Library and oneDNN versions in mkl_aarch64 build config", "body": "This PR brings the following changes:\r\n- the version of Compute Library for the Arm Architecture (ACL) is updated to v21.05. This release contains several bug fixes compared to the currently used v21.02 (Conv2D works correctly in the case of dilations and at high numbers of threads);\r\n- oneDNN version is updated to v2.3 to keep synchronisation with [PR#50708](https://github.com/tensorflow/tensorflow/pull/50708);\r\n- the macro `DNNL_AARCH64_USE_ACL` is now set automatically for `--config=mkl_aarch64`.", "comments": ["@penpornk @gbaned please have a look."]}, {"number": 50756, "title": "[XLA] Enable pointwise row vectorization for small row.", "body": "Follow up on : https://github.com/tensorflow/tensorflow/pull/48527\r\nThis speed up the same efficient net fusion but when the row size is small. That kernel happens for with many different size.\r\n\r\nThis enable row vectorization for small row. This lower the numbers of threads per block by 4x. This cause blocks that are too small and trigger speed regression in some cases due to the blocks being too small. To prevent those small blocks problems, this PR use blockIdx.y in addition to blockIdx.x. blockIdx.x continue to handle one row. But each elements in blockIdx.y handle a different consecutive row.\r\n\r\n@akuegel @cheshire \r\n\r\nI join 2 hlo that contain multiple kernels. \r\n[swish_fusion_5_160,190,190,48_simpler.txt](https://github.com/tensorflow/tensorflow/files/6858339/swish_fusion_5_160.190.190.48_simpler.txt)\r\n[swish_fusion_5_160,190,190,48.txt](https://github.com/tensorflow/tensorflow/files/6858340/swish_fusion_5_160.190.190.48.txt)\r\n\r\nThe kernels:\r\nfusion_5: The fusion I want to optimize. It come from EfficientNet.\r\nfusion_5_new: A new version of that if we mark kExp as cheap. This is what we want to enable. It does extra computation, but read less data. An exponential is duplicated to save bandwidth.\r\nfusion_5_simpler[_new]: Variation of above kernels that have less row inputs. They are synthetic HLO that doesn't exist in models.\r\n\r\nThe shape is a real shape from the EfficientNet model. Just one picked that have small row size that wasn't vectorized before.\r\n\r\nHere is the current sped result that I have.\r\n\r\nV100 | 72ac61dbed (upstream) | ...0_block | ...0_grid | fb71bc2388 (PR) | ...1_block | ...1_grid | af3beedbd24 PR+1commit | ...2_block | ...2_grid | 39f5185dc7 PR+2commit | ...3_block | ...3_grid\r\n-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --\r\nfusion_5 | 3059.776 | 256 | 270750 | 2730.784 | 132 | 525091 | 2928.48 | 132 | 32818 | 2868.544 | 264 | 16409\r\nfusion_5_new | 2623.712 | 256 | 270750 | 2539.392 | 132 | 32818 | 2546.208 | 132 | 32818 | 2337.664 | 264 | 16409\r\nfusion_5_simpler | 2676.832 | 256 | 270750 | **2711.008** | 132 | 525091 | **2867.552** | 132 | 32818 | **2880.832** | 264 | 16409\r\nfusion_5_simpler_new | 2450.368 | 256 | 270750 | **2533.536** | 132 | 32818 | **2541.536** | 132 | 32818 | 2350.048 | 264 | 16409\r\n\u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\r\nA100 | 72ac61dbed (upstream) | ...0_block | ...0_grid | fb71bc2388 (PR) | ...1_block | ...1_grid | af3beedbd24 PR+1commit | ...2_block | ...2_grid | 39f5185dc7 PR+2commit | ...3_block | ...3_grid\r\nfusion_5 | 2538.24 | 256 | 270750 | 2399.776 | 132 | 525091 | 2150.24 | 132 | 32818 | 1953.568 | 264 | 16409\r\nfusion_5_new | 3079.04 | 256 | 270750 | 2760.064 | 132 | 32818 | 2760.288 | 132 | 32818 | 2494.528 | 264 | 16409\r\nfusion_5_simpler | 2357.44 | 256 | 270750 | 2208.576 | 132 | 525091 | 2149.088 | 132 | 32818 | 2106.848 | 264 | 16409\r\nfusion_5_simpler_new | 2819.904 | 256 | 270750 | 2736.48 | 132 | 32818 | 2736.288 | 132 | 32818 | 2502.432 | 264 | 16409\r\n\r\nThe columns is the matching commit. PR mean the first version of the PR. I added 2 commits to limit the slowdown on V100. That also give a speed up on A100 at the same time.", "comments": ["@nouiz Can you please resolve conflicts? Thanks!", "I join an HLO with 2 fusion with input shapes of: 160x190x190x48.\r\nTheir time on A100 before optimization:\r\nfusion_5      2736.032\r\nfusion_5_new  3319.552\r\nAfter this PR:\r\nfusion_5      2581.344\r\nfusion_5_new  2972.384\r\n[swish_fusion_5_160,190,190,48.txt](https://github.com/tensorflow/tensorflow/files/6857735/swish_fusion_5_160.190.190.48.txt)\r\n", "would it be too hard to simply this input HLO and attach it to the commit description? Also is there a comparable speed-up on Volta, what is Ampere-specific here?", "@nouiz  Can you please check @cheshire's comments and keep us posted ? Thanks!", "I wasn't sure what he wanted. The same HLO repro but cleaner (no metadata...). Or a smaller HLO that repro the speed up. So I did both.\r\nThis worked well on A100. But the smaller repro was slower on V100.\r\nI started to investigate, but got busy with other things.\r\n\r\nOne quick fix would be to enable it only for A100. I can probably do that tomorrow.\r\nOtherwise it will take a few weeks before I can have a solution.\r\n\r\nEDIT: The slow down on the smaller repro is a synthetic kernel. So maybe it doesn't happens in real model. ", "Could you share that smaller repro, current performance as a percentage of rooftop, and performance with your patch as rooftop percentage after your change on Ampere and Volta?", "@cheshire I updated the description with the information you asked.\r\nI put in bold the number that have a slowdown vs TF upstream.", "@nouiz Thanks!\r\n\r\nThat's almost the information we want, but for this and similar PRs, could we go further?\r\nWe have some tooling to do this internally but sadly this was not ported to OSS yet (something --xla_hlo_profile used to do before it broke with MLIR migration).\r\n\r\nRaw timing numbers are hard to compare and understand without context.\r\nThe context is how far we are from the roofline performance.\r\n\r\nFor example, I have a Titan V card. It has a maximal bandwidth of ~650Gb/s.\r\nLet's say I'm doing a reduction `f32[1'500'000'000] -> f32[]`. This reads 6GB of data (the write is negligible here).\r\nLet's say this (hypothetically!) reduction runs in 10ms. Then I know it already runs at >90% of roofline throughput.\r\nOr if it runs in 20ms we are at 50% of roofline. So for these microbenchmarks, would it be possible to report both absolute and %% of roofline numbers?", "nsight-compute give me those number of the peak usage for the commit 39f5185dc7 \r\n| kernel name | % of Memory | % of SM |\r\n| ----- | ----- | ----- |\r\n| fusion_5 | 86% | 45% |\r\n| fusion_5_new | 79% | 77% |\r\n| fusion_5_simpler|85% | 44% |\r\n| fusion_5_simpler_new | 76% | 78% |\r\n", "is this on Volta? _new is with your change?", "This is on V100. A100 doesn't have regression.\r\n`fusion_5*new*` is the kernel if I add another change not in this PR. (consider kExp not expansive)\r\nBut this is something that will come later. I need this PR and the bitcast fusion lift to be good before the other one work well.", "@cheshire I rebased to solve the conflict.\r\nThis should be ready to merge.", "Sorry for being pedantic, could you expand the commit description to give a more high-level overview of what is going? E.g. what is different about small rows that we also want to utilize the Y-dimension of CUDA blocks? So that we can vectorize multiple subsequent blocks \"together\" as a group?", "> Sorry for being pedantic, could you expand the commit description to give a more high-level overview of what is going? E.g. what is different about small rows that we also want to utilize the Y-dimension of CUDA blocks? So that we can vectorize multiple subsequent blocks \"together\" as a group?\r\n\r\nI update the second paragraph of the description be explain why I combine small row vectorization and multiple row per block.", "Any idea why it didn't got merged?", "Currently the linter is failing:\r\n\r\n```\r\nCheckLint/0: Warning (//tensorflow/compiler/xla/METADATA:4:3)\r\nCheckLint found errors.\r\nYou don't need a ; after a }  [readability/braces] [4]\r\n\t//tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:170\r\n```", "> Currently the linter is failing:\r\n> \r\n> ```\r\n> CheckLint/0: Warning (//tensorflow/compiler/xla/METADATA:4:3)\r\n> CheckLint found errors.\r\n> You don't need a ; after a }  [readability/braces] [4]\r\n> \t//tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:170\r\n> ```\r\n\r\nFixed.", "The newly added test is failing to build, because you don't have a dependency for the header include\r\n\r\n#include \"tensorflow/compiler/xla/error_spec.h\"", "Updated. The public CI doesn't show that problem and I can't reproduce it locally. Hopefully I did the right fix."]}, {"number": 50755, "title": "InvalidArgumentError: 2 root error(s) found.", "body": "### System information\r\n\r\n- Colab with GPU\r\n- Latest tflite version\r\n\r\n[COLAB LINK](https://colab.research.google.com/drive/18HYg1-SLnNnYcGTIy4yQCgPGA0ZKyYx5?usp=sharing)\r\n\r\n### Failure after conversion\r\n\r\nINFO:tensorflow:Load image with size: 25002, num_label: 2, labels: Cat, Dog.\r\nINFO:tensorflow:Load image with size: 25002, num_label: 2, labels: Cat, Dog.\r\nINFO:tensorflow:Retraining the models...\r\nINFO:tensorflow:Retraining the models...\r\nWARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\r\nWARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\r\nModel: \"sequential_2\"\r\n\r\nTotal params: 3,415,586\r\nTrainable params: 2,562\r\nNon-trainable params: 3,413,024\r\n\r\n\r\n9 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/image_classifier.py in create(cls, train_data, model_spec, validation_data, batch_size, epochs, train_whole_model, dropout_rate, learning_rate, momentum, shuffle, use_augmentation, use_hub_library, warmup_steps, model_dir, do_train)\r\n    318     if do_train:\r\n    319       tf.compat.v1.logging.info('Retraining the models...')\r\n--> 320       image_classifier.train(train_data, validation_data)\r\n    321     else:\r\n    322       # Used in evaluation.\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/image_classifier.py in train(self, train_data, validation_data, hparams)\r\n    179       lib = train_image_classifier_lib\r\n    180     self.history = lib.train_model(self.model, hparams, train_data_and_size,\r\n--> 181                                    validation_data_and_size)\r\n    182     return self.history\r\n    183 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py in train_model(model, hparams, train_data_and_size, valid_data_and_size, log_dir)\r\n    249       validation_data=valid_data,\r\n    250       validation_steps=validation_steps,\r\n--> 251       callbacks=callbacks)\r\n    252 \r\n    253 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1181                 _r=1):\r\n   1182               callbacks.on_train_batch_begin(step)\r\n-> 1183               tmp_logs = self.train_function(iterator)\r\n   1184               if data_handler.should_sync:\r\n   1185                 context.async_wait()\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    887 \r\n    888       with OptionalXlaContext(self._jit_compile):\r\n--> 889         result = self._call(*args, **kwds)\r\n    890 \r\n    891       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    915       # In this case we have created variables on the first call, so we run the\r\n    916       # defunned version which is guaranteed to never create variables.\r\n--> 917       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    918     elif self._stateful_fn is not None:\r\n    919       # Release the lock early so that multiple threads can perform the call\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   3022        filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n   3023     return graph_function._call_flat(\r\n-> 3024         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n   3025 \r\n   3026   @property\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1959       # No tape is watching; skip to running the function.\r\n   1960       return self._build_call_outputs(self._inference_function.call(\r\n-> 1961           ctx, args, cancellation_manager=cancellation_manager))\r\n   1962     forward_backward = self._select_forward_and_backward_functions(\r\n   1963         args,\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    594               inputs=args,\r\n    595               attrs=attrs,\r\n--> 596               ctx=ctx)\r\n    597         else:\r\n    598           outputs = execute.execute_with_cancellation(\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) _Invalid argument:  Trying to decode BMP format using a wrong op. Use `decode_bmp` or `decode_image` instead. Op used: DecodePng_\r\n\t [[{{node cond/else/_1/cond/DecodePng}}]]\r\n\t [[IteratorGetNext]]\r\n\t [[IteratorGetNext/_2]]\r\n  (1) _Invalid argument:  Trying to decode BMP format using a wrong op. Use `decode_bmp` or `decode_image` instead. Op used: DecodePng_\r\n\t [[{{node cond/else/_1/cond/DecodePng}}]]\r\n\t [[IteratorGetNext]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_33212]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n", "comments": ["The model is failing at training step. Are you using images in jpeg format?\r\nAlso refer this guide if haven't already \r\nhttps://www.tensorflow.org/lite/tutorials/model_maker_image_classification\r\n", "Hello again sir, it only contains pictures in jpg format. Should the format be in jpeg format?\r\n![image](https://user-images.githubusercontent.com/27694294/125687778-205675d8-065c-4dfd-9556-0ba714b91611.png)\r\n![image](https://i.ibb.co/hZybtBY/2.png)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50755\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50755\">No</a>\n"]}, {"number": 50754, "title": "dockerfiles: pin jedi to allow ipython tab completion", "body": "Tab completion doesn't work on Jupyter because of an incompatibility\r\nissue. Using the web notebook, only an obscure error is printed to console\r\nbut in the case of ipython console, the whole REPL crashes.\r\n\r\nThis works around the issue by pinning the jedi library to the last\r\ncompatible version.\r\n\r\nSee: https://github.com/ipython/ipython/issues/12740\r\n", "comments": ["@angerson  Can you please review this PR ? Thanks!"]}, {"number": 50753, "title": "quantize.cpp:58 input->type == kTfLiteFloat32 || input->type == kTfLiteInt16 || inp was not true", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- Tensorflow version (commit SHA if source): 2.5.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arduino Nano 33 BLE Sense\r\n\r\n**Describe the problem**\r\n\r\nI have converted our CNN to TFLite using full integer quantization (in and out: uint8)\r\nAllocateTensors fails with quantize.cpp:58 input->type == kTfLiteFloat32 || input->type == kTfLiteInt16 || inp was not true\r\n\r\n![all_nano_33_ble_sense](https://user-images.githubusercontent.com/6291410/125519397-d10a8aa4-e1c9-4631-b896-a4f59b23aeee.png)\r\n\r\n", "comments": ["@AdamMiltonBarker ,\r\n\r\nThis issue is more suitable for TensorFlow Micro repo. Please post it on [tflite/micro](https://github.com/tensorflow/tflite-micro/issues) repo from here. Thanks!", "Did do thanks. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50753\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50753\">No</a>\n"]}, {"number": 50752, "title": "tf.keras.layers.TimeDistributed build method passes shapes as tuples instead of TensorShapes.", "body": "https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/layers/wrappers.py#L186\r\n\r\nThis causes inconsistencies in wrapped layer.", "comments": ["@iiwi It looks like your Issue relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. As ([previously announced](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999/)), all future development of Keras is expected to happen in the [github.com/keras-team/keras](github.com/keras-team/keras) repository. If your issue lies with the TF-Core area please comment back with your explanation and we can look into it further. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 50751, "title": "As (x*w+x1*w1+...xn*wn+b) checks if the obtained value is the difference between (x) and (y).", "body": "It would be interesting that at the end of the model.predict function the value obtained the difference between the X_test and the y_test, thus being necessary to multiply the test matrix (X_test) and the prediction matrix (y_preds) to obtain the final value (y_train), instead of needing to multiply the matrix I'm trying to predict (y_train) and the prediction matrix (y_preds) to get the matrix I'm trying to predict (y_train).\r\nThis would be useful in cases of future price forecasts of some stock on the stock exchange, because in a real use case, we would have the opening price of the day for example, and we would like to predict the closing price of that same day, using for this the opening price we have at hand.\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n2.4.1\r\n\r\n- Are you willing to contribute it (Yes/No):\r\nYes.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nBy default as this happens at the moment we depend on the closing price itself (y_train) to be multiplied by the forecast matrix (y_preds) to get the closing price itself (y_train), this doesn't make much sense in case of future forecasts, of which we do not yet have the future price in hand.\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nDevelopers.\r\n", "comments": ["@neiev What kind of model (TF/Keras) and what kind of layers (Dense/RNN) you are planning to use? Can you show us what is missing with the current implementation? Please provide a simple standalone code to demonstrate the feature.\r\n\r\nAs you mentioned that you are willing to contribute, you could raise a PR to update any code. Once you open a PR, you can connect this issue to the PR. Thanks!", "`model.predict(input)` does exactly this. It's just one line, there's no need to complicate TF API for this"]}, {"number": 50750, "title": "Error converting  mask_rcnn/inception_resnet_v2_1024x1024 Tensorflow model from TFHub to Tflite ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n\r\n### System information\r\n\r\n- Ubuntu 18.02\r\n- Tensorflow 2.5\r\n- WSL\r\n\r\n**Describe the current behavior**\r\nI have been trying to convert the [mask_rcnn/inception_resnet_v2_1024x1024](https://hub.tensorflow.google.cn/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1) model from TFHub to Tflite using the using the saved model conversion here https://www.tensorflow.org/lite/convert. The problem here is that for some reason when I run the sample code from Tensorflow the model does not convert correctly and gives me a segmentation fault or an error.\r\n\r\n**Describe the expected behavior**\r\nShould be able to have a successful conversion from saved_model to tflite model.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nThis is the source code I used for the conversion here taken from the Tensorflow website. All I did was download the model from Tfhub onto my Ubuntu.\r\n![image](https://user-images.githubusercontent.com/22630158/125495414-930090b6-20b4-4ac1-8bec-cde371f59edb.png)\r\n\r\nHere is the error when I try to convert the .pb file to a .tflite\r\n![image](https://user-images.githubusercontent.com/22630158/125495918-146443da-7041-480a-9cf9-393d005d44b4.png)\r\n\r\nMy files\r\n![image](https://user-images.githubusercontent.com/22630158/125496188-77a0b99f-8f0e-4c37-839f-c36ca51e4ebb.png)\r\n\r\n\r\n", "comments": ["@TranATT ,\r\n\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue.Also please provide the complete code to reproduce the issue.Thanks!", "> @TranATT ,\r\n> \r\n> We see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue.Also please provide the complete code to reproduce the issue.Thanks!\r\n\r\nHi @tilakrayal I have updated the template! Hope that helps.", "Is it possible to provide an end-to-end [colab](https://colab.research.google.com/) link with your code and also attach your model? We need to be able to reproduce the issue thoroughly on our end to debug this further. \r\n\r\n*Ensure that you provide read permissions for all the links you share* ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50750\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50750\">No</a>\n"]}, {"number": 50749, "title": "Custom metric fails with \"ValueError: Metric should be a callable\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 34\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Python version: Python 3.8.10\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nGiven the _minimal viable program_ below I get an error (also when defining `call` instead of `__call__`):\r\n```\r\nValueError: Metric should be a callable, found: <__main__.MyLoss object at 0xaddress>\r\n```\r\n\r\n**Describe the expected behavior**\r\nI expect the fitting to commence and complete without error.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow.keras as keras\r\nimport numpy as np\r\n\r\nclass MyLoss(keras.losses.Loss):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n    def __call__(self, x, y):\r\n        return x-y\r\n\r\ndef get_model():\r\n    i = keras.Input((1,))\r\n    m = keras.layers.Dense(10)(i)\r\n    m = keras.layers.Dense(1)(i)\r\n    return keras.Model(i,m)\r\n\r\nm = get_model()\r\nm.compile(loss='mse', metrics=[MyLoss()])\r\nm.fit(np.arange(100), np.arange(100))\r\n```\r\n\r\n**Other info / logs**\r\n[Full output](https://gist.github.com/obtu/763908fb4111b5da0f785cb13e1d7d34)\r\n", "comments": ["Was able to replicate the issue on colab. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/3d074af2b2a5dcdbc75116be890ae3e7/untitled112.ipynb). Thanks!", "@obtu It looks like your Issue relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. As ([previously announced](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999/)), all future development of Keras is expected to happen in the [github.com/keras-team/keras](github.com/keras-team/keras) repository. If your issue lies with the TF-Core area please comment back with your explanation and we can look into it further. Thanks!", "I have created a new issue at the keras repo: https://github.com/keras-team/keras/issues/14965", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@obtu  Closing the issue here since issue was created in keras repo.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50749\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50749\">No</a>\n"]}]