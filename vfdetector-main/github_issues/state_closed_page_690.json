[{"number": 32890, "title": "[TF 2.0 API Docs] tf.nn.batch_normalization", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/batch_normalization\r\n\r\n## Description of issue (what needs changing):\r\nDocumentation\r\n\r\n### Clear description\r\nYes\r\n### Correct links\r\nYes\r\nIs the link to the source code correct?\r\nYes\r\n### Parameters defined\r\nYes\r\nAre all parameters defined and formatted correctly?\r\nNo.\r\n\r\n- tf.nn.moments(..., keep_dims=True) \r\n\r\nIn TF2 version of tf.nn.moments, keep_dims keyword should be \"keepdims\" instead\r\n\r\n- Does not implement the equation as given, but the equation 11 in Algorithm 2 of the paper.\r\n![image](https://user-images.githubusercontent.com/1215029/65819369-429b5480-e239-11e9-98e7-07bbf34f18d9.png)\r\n\r\n\r\n### Returns defined\r\nAre return values defined?\r\nYes\r\n### Raises listed and defined\r\nNo\r\n### Usage example\r\nIs there a usage example?\r\nNo\r\n### Request visuals, if applicable\r\nNo\r\nAre there currently visuals? If not, will it clarify the content?\r\nNo\r\n### Submit a pull request?\r\nYes\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Closing this issue since the associated PR has been merged. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32890\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32890\">No</a>\n"]}, {"number": 32889, "title": "TypeError: An op outside of the function building code is being passed", "body": "I have realized a code to take previous lstm outputs as current inputs as a sequence generator. But when I want to pass states of lstm cells to next batch by take states as a member of the generator class, it rise an exception \"TypeError: An op outside of the function building code is being passed\". When add `@tf.function` to `main` function in my code, the exception became \"AttributeError: 'Tensor' object has no attribute '_numpy'\". I suspect this is about the compute graph, but I can't understand the reason. \r\n\r\nMy tensorflow version is 2.0.0-rc1.\r\n```\r\nIn [4]: tf.__version__                                                                                                                                \r\nOut[4]: '2.0.0-rc1'\r\n```\r\n\r\nI don't know whether it is a bug or not. Thank you for your attention.\r\n\r\nThe code is:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\n\r\nclass RNNGenerator(keras.layers.Layer):\r\n    def __init__(self, rnn_dim, rnn_layer_num, seq_len, seq_width, batch_size, **kwargs):\r\n        self.rnn_dim = rnn_dim              # lstm hidden dim\r\n        self.rnn_layer_num = rnn_layer_num  # number of lstm cell layers\r\n        self.seq_len = seq_len              # length of squence\r\n        self.seq_width = seq_width          # width of squence\r\n        self.batch_size = batch_size\r\n\r\n        self._cells = {}\r\n\r\n        super().__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        # input dense\r\n        self._dense_in = keras.layers.Dense(self.rnn_dim)\r\n\r\n        # many lstm cell layers\r\n        for i in range(self.rnn_layer_num):\r\n            cell = keras.layers.LSTMCell(units=self.rnn_dim)\r\n            self._cells[i] = cell\r\n\r\n        # output dense\r\n        self._dens_out = keras.layers.Dense(self.seq_width, activation=\"sigmoid\")\r\n\r\n        super().build(input_shape)\r\n\r\n    def call(self, inputs):\r\n        inputs = tf.squeeze(inputs, 1)\r\n\r\n        # init cells' states\r\n        states = getattr(self, 'states', None)\r\n        if states is None:\r\n            states = {}\r\n            for i, cell in self._cells.items():\r\n                init_cell_states = [tf.random.uniform([self.batch_size, self.rnn_dim]),\r\n                                    tf.random.uniform([self.batch_size, self.rnn_dim])]\r\n                states[i] = init_cell_states\r\n\r\n        # --- prev outputs as current inputs\r\n        rand_prev = tf.random.uniform([self.batch_size, self.seq_width], dtype=tf.float32)\r\n        prev_inputs = tf.concat([rand_prev, inputs], -1)\r\n        outputs = []\r\n        for _ in range(self.seq_len):\r\n            cell_inputs = self._dense_in(prev_inputs)\r\n            for i, cell in self._cells.items():\r\n                cell_outputs, states[i] = cell(cell_inputs, states[i])\r\n                cell_inputs = cell_outputs\r\n            step_outputs = self._dens_out(cell_outputs)\r\n            prev_inputs = tf.concat([step_outputs, inputs], -1)\r\n\r\n            outputs.append(tf.expand_dims(step_outputs, 1))\r\n\r\n        # reserve cells' state in current batch\r\n        self.states = states\r\n\r\n        return tf.concat(outputs, 1)\r\n\r\n# @tf.function\r\ndef main():\r\n    batch_size = 17\r\n    seq_width = 4\r\n    rand_input_dim = 3\r\n    inputs = keras.layers.Input(shape=[1, rand_input_dim])\r\n    outputs = RNNGenerator(rnn_dim=64, rnn_layer_num=2, seq_len=10, seq_width=seq_width, batch_size=batch_size)(inputs)\r\n    outputs = keras.layers.Flatten()(outputs)\r\n    outputs = keras.layers.Dense(1)(outputs)\r\n    outputs = tf.nn.sigmoid(outputs)\r\n    model = keras.models.Model(inputs=inputs, outputs=outputs)\r\n\r\n    # model.summary()\r\n\r\n    X = np.random.rand(batch_size, 1, rand_input_dim).astype(np.float32)\r\n    y = np.zeros([batch_size, 1])\r\n\r\n    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\r\n    model.fit(X, y, batch_size=32, epochs=10)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nRunning info without `@tf.function` before `main`:\r\n\r\n```bash\r\n2019-09-28 21:43:30.966344: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-28 21:43:30.980391: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f88282a2f60 executing computations on platform Host. Devices:\r\n2019-09-28 21:43:30.980415: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\nTrain on 17 samples\r\nEpoch 1/10\r\nWARNING:tensorflow:From /Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\n17/17 [==============================] - 2s 98ms/sample\r\nTraceback (most recent call last):\r\n  File \"/Users/xxxxxx/Desktop/GAN-seq/test.py\", line 85, in <module>\r\n    main()\r\n  File \"/Users/xxxxxx/Desktop/GAN-seq/test.py\", line 81, in main\r\n    model.fit(X, y, batch_size=32, epochs=10)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 76, in quick_execute\r\n    raise e\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 61, in quick_execute\r\n    num_outputs)\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: model/rnn_generator/lstm_cell_10/mul_2:0\r\n```\r\nRunning info with `@tf.function` before `main`:\r\n```bash\r\n2019-09-28 21:48:39.227708: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-28 21:48:39.242540: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fdda3d85620 executing computations on platform Host. Devices:\r\n2019-09-28 21:48:39.242575: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:From /Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nTrain on 17 samples\r\nEpoch 1/10\r\nTraceback (most recent call last):\r\n  File \"/Users/xxxxxx/Desktop/GAN-seq/test.py\", line 85, in <module>\r\n    main()\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 503, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/xxxxxx/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nAttributeError: in converted code:\r\n    relative to /Users/xxxxxx:\r\n\r\n    Desktop/GAN-seq/test.py:81 main  *\r\n        model.fit(X, y, batch_size=32, epochs=10)\r\n    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py:728 fit\r\n        use_multiprocessing=use_multiprocessing)\r\n    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py:674 fit\r\n        steps_name='steps_per_epoch')\r\n    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py:393 model_iteration\r\n        batch_outs = f(ins_batch)\r\n    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:3635 __call__\r\n        [x._numpy() for x in outputs],  # pylint: disable=protected-access\r\n    anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:3635 <listcomp>\r\n        [x._numpy() for x in outputs],  # pylint: disable=protected-access\r\n\r\n    AttributeError: 'Tensor' object has no attribute '_numpy'\r\n\r\n```", "comments": ["I have tried on colab with TF version 2.0.0-rc1,2.0.0-rc2 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/c3f66cbde5677fa47653892374f09871/untitled234.ipynb).Thanks!", "I am experiencing the exact same issue (just with different model) in TF version 2.0.0 and the error message is by no means suggestive of what may be done wrongly on the layer of abstraction I'm programming in to fix this (I'm using `tf.keras` Functional API to build the model and `tf.data.Dataset` to train with `model.fit()`). \r\n\r\n", "I think this issue is very likely related to the self.states field  captured in the call() context, this leaked reference is used under multiple training context, which could cause the issue. Let me see how to walk around the issue.", "Btw, you probably shouldn't annotation the main function with tf.function since keras.fit() already convert the model into graph execution under the hood. The tf.function annotation is only useful for training function that handles tensor inputs (your main function doesn't take any tensor as inputs).", "> Btw, you probably shouldn't annotation the main function with tf.function since keras.fit() already convert the model into graph execution under the hood. The tf.function annotation is only useful for training function that handles tensor inputs (your main function doesn't take any tensor as inputs).\r\n\r\nThank you very much. Your explanation is very useful to me. It would be grateful if the issue were solved.", "So there are few issues in your code, which is bit hard to diagnose:\r\n\r\n1. Creation of the state. Since the state is tracked and attached to the model, most likely you want to treat them like a data/eager tensor, rather than a symbolic tensor (graph tensor). Since the call() method is executed in the graph context, if you init the state in call(), they will just be graph tensor, and the final self.states = states will set the graph tensor as an attribute on the model, which will just causing issue in the when it proceed next batch. You will need to move the init of the state to build() method.\r\n\r\n2. Record the states at end of the batch. Currently your are doing \"self.states = states\", which just assign the self.states to a dict with symbolic tensors. The correct way is actually assign the value of the states to the individual state in the self.states.\r\n\r\nThe changed code is like below, I have just applied the change to how the states is handled, and it works fine.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\n\r\nclass RNNGenerator(keras.layers.Layer):\r\n  def __init__(self, rnn_dim, rnn_layer_num, seq_len, seq_width, batch_size, **kwargs):\r\n    self.rnn_dim = rnn_dim              # lstm hidden dim\r\n    self.rnn_layer_num = rnn_layer_num  # number of lstm cell layers\r\n    self.seq_len = seq_len              # length of squence\r\n    self.seq_width = seq_width          # width of squence\r\n    self.batch_size = batch_size\r\n\r\n    self._cells = {}\r\n\r\n    super().__init__(**kwargs)\r\n\r\n  def build(self, input_shape):\r\n    # input dense\r\n    self._dense_in = keras.layers.Dense(self.rnn_dim)\r\n\r\n    # many lstm cell layers\r\n    for i in range(self.rnn_layer_num):\r\n      cell = keras.layers.LSTMCell(units=self.rnn_dim)\r\n      self._cells[i] = cell\r\n\r\n    # output dense\r\n    self._dens_out = keras.layers.Dense(self.seq_width, activation=\"sigmoid\")\r\n\r\n    # init cells' states\r\n    states = getattr(self, 'states', None)\r\n    if states is None:\r\n      states = {}\r\n      for i, cell in self._cells.items():\r\n        init_cell_states = [tf.random.uniform([self.batch_size, self.rnn_dim]),\r\n                            tf.random.uniform([self.batch_size, self.rnn_dim])]\r\n        states[i] = init_cell_states\r\n      self.states = states\r\n\r\n    super().build(input_shape)\r\n\r\n  # @tf.function\r\n  def call(self, inputs):\r\n    inputs = tf.squeeze(inputs, 1)\r\n\r\n    states = self.states.copy()\r\n\r\n    # --- prev outputs as current inputs\r\n    rand_prev = tf.random.uniform([self.batch_size, self.seq_width], dtype=tf.float32)\r\n    prev_inputs = tf.concat([rand_prev, inputs], -1)\r\n    outputs = []\r\n    for _ in range(self.seq_len):\r\n      cell_inputs = self._dense_in(prev_inputs)\r\n      for i, cell in self._cells.items():\r\n        cell_outputs, states[i] = cell(cell_inputs, states[i])\r\n        cell_inputs = cell_outputs\r\n      step_outputs = self._dens_out(cell_outputs)\r\n      prev_inputs = tf.concat([step_outputs, inputs], -1)\r\n\r\n      outputs.append(tf.expand_dims(step_outputs, 1))\r\n\r\n    # reserve cells' state in current batch\r\n    for state_, state in zip(tf.nest.flatten(self.states), tf.nest.flatten(states)):\r\n      state_ = state\r\n\r\n    return tf.concat(outputs, 1)\r\n\r\n# @tf.function\r\ndef main():\r\n  batch_size = 17\r\n  seq_width = 4\r\n  rand_input_dim = 3\r\n  inputs = keras.layers.Input(shape=[1, rand_input_dim])\r\n  outputs = RNNGenerator(rnn_dim=64, rnn_layer_num=2, seq_len=10, seq_width=seq_width, batch_size=batch_size)(inputs)\r\n  outputs = keras.layers.Flatten()(outputs)\r\n  outputs = keras.layers.Dense(1)(outputs)\r\n  outputs = tf.nn.sigmoid(outputs)\r\n  model = keras.models.Model(inputs=inputs, outputs=outputs)\r\n\r\n  # model.summary()\r\n\r\n  X = np.random.rand(batch_size, 1, rand_input_dim).astype(np.float32)\r\n  y = np.zeros([batch_size, 1])\r\n\r\n  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\r\n  model.fit(X, y, batch_size=32, epochs=10)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  main()\r\n\r\n```\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32889\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32889\">No</a>\n", "This method is very nice! My problem has been solved. Thanks very much! @qlzh727 ", "Faced a similar issue. No more error raised using the solution given by @qlzh727. But my question is, how the below lines update the new state to self.states for the next batch? It looks like the `self.states` have never changed after initialized. Please correct me if I'm wrong.\r\n\r\n```\r\n# reserve cells' state in current batch\r\nfor state_, state in zip(tf.nest.flatten(self.states), tf.nest.flatten(states)):\r\n   state_ = state\r\n```", "```\r\nfrom keras import backend as K\r\nfrom keras.utils import conv_utils\r\nfrom keras.engine import InputSpec\r\nfrom keras.layers import Conv2D\r\n\r\n\r\nclass myPConv2D(Conv2D):\r\n\r\n    def __init__(self, last_layer=False, *args,  **kwargs):\r\n        print(\"-----args\", args, kwargs)\r\n        super().__init__(*args, **kwargs)\r\n        self.last_layer = last_layer\r\n        self.input_spec = [InputSpec(ndim=4), InputSpec(ndim=4)]\r\n\r\n    def build(self, input_shape):\r\n        \"\"\"\r\n        Adapted from original _Conv() layer of Keras.\r\n        Parameters\r\n            input_shape: list of dimensions for [img, mask].\r\n        \"\"\"\r\n        assert isinstance(input_shape, list)\r\n        assert self.data_format == 'channels_last', \"data format should be `channels_last`\"\r\n        channel_axis = -1\r\n\r\n        if input_shape[0][channel_axis] is None:\r\n            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')\r\n\r\n        self.input_dim = input_shape[0][channel_axis]\r\n\r\n        # Image kernel:\r\n        kernel_shape = self.kernel_size + (self.input_dim, self.filters)\r\n        self.kernel  = self.add_weight(shape=kernel_shape,\r\n                                      initializer=self.kernel_initializer,\r\n                                      name='img_kernel',\r\n                                      regularizer=self.kernel_regularizer,\r\n                                      constraint=self.kernel_constraint)\r\n        # Image bias:\r\n        if self.use_bias:\r\n            self.bias = self.add_weight(shape=(self.filters,),\r\n                                        initializer=self.bias_initializer,\r\n                                        name='bias',\r\n                                        regularizer=self.bias_regularizer,\r\n                                        constraint=self.bias_constraint)\r\n        else:\r\n            self.bias = None\r\n\r\n        # Mask kernel:\r\n        self.kernel_mask = K.ones(shape=self.kernel_size + (self.input_dim, self.filters))\r\n\r\n        self.built = True\r\n\r\n    def call(self, inputs):\r\n        assert isinstance(inputs, list) and len(inputs) == 2\r\n        #img, mask = inputs\r\n\r\n        # Masked convolution:\r\n        img_output = K.conv2d(inputs[0] * inputs[1],\r\n                              self.kernel,\r\n                              strides=self.strides,\r\n                              padding=self.padding,\r\n                              data_format=self.data_format)\r\n\r\n        # Image scaling:\r\n        sum_m = K.conv2d(inputs[1],\r\n                         self.kernel_mask,\r\n                         strides=self.strides,\r\n                         padding=self.padding,\r\n                         data_format=self.data_format)\r\n        # Note, sum_1 does not need to be created via conv2d (as sum_m), it can be generated straight away:\r\n        sum_1i = self.kernel_size[0] * self.kernel_size[1] * self.input_dim\r\n        sum_1 = sum_1i * K.ones(K.shape(sum_m))\r\n        # Prevent division by zero:\r\n        sum_m_clip = K.clip(sum_m, 1., None)\r\n        # Scale the image:\r\n        img_output = img_output * (sum_1 / sum_m_clip)\r\n\r\n        # Apply bias only to the image (if chosen to do so):\r\n        if self.use_bias:\r\n            img_output = K.bias_add(img_output,\r\n                                    self.bias,\r\n                                    data_format=self.data_format)\r\n\r\n        # Apply activation if needed. Note, in the paper, activation is applied after BatchNormalization.\r\n        if self.activation is not None:\r\n            img_output = self.activation(img_output)\r\n\r\n        if self.last_layer:\r\n            return img_output\r\n\r\n        # Update the mask:\r\n        mask_output = K.clip(sum_m, 0., 1.)\r\n\r\n        return [img_output, mask_output]\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        assert isinstance(input_shape, list)\r\n        assert self.data_format == 'channels_last'\r\n        space = input_shape[0][1:-1]\r\n        new_space = []\r\n        for i in range(len(space)):\r\n            new_dim = conv_utils.conv_output_length(\r\n                space[i],\r\n                self.kernel_size[i],\r\n                padding=self.padding,\r\n                stride=self.strides[i],\r\n                dilation=self.dilation_rate[i])\r\n            new_space.append(new_dim)\r\n        new_shape = (input_shape[0][0],) + tuple(new_space) + (self.filters,)\r\n        if self.last_layer:\r\n            return new_shape\r\n        return [new_shape, new_shape]\r\n\r\n```\r\nSame issue... Please help @qlzh727 \r\n\r\nComplete StackTrace error\r\n```\r\n2020-08-02 04:01:27.519864: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2020-08-02 04:01:27.541115: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2599990000 Hz\r\n2020-08-02 04:01:27.541850: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5578c71a0d30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-02 04:01:27.541891: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-02 04:01:27.541986: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2020-08-02 04:01:28.536401: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\r\nWARNING:tensorflow:From inpainter_main.py:47: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use Model.fit, which supports generators.\r\nTraceback (most recent call last):\r\n  File \"inpainter_main.py\", line 47, in <module>\r\n    model.fit_generator(\r\n  File \"/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1465, in fit_generator\r\n    return self.fit(\r\n  File \"/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 627, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 505, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2657, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in user code:\r\n\r\n    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    /home/bitsy-chuck/Downloads/PConv2D-2ndimp/inpainter_utils/pconv2d_layer.py:71 call  *\r\n        sum_1 = sum_1i * K.ones(K.shape(sum_m))\r\n    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:1365 ones  **\r\n        v = array_ops.ones(shape=shape, dtype=tf_dtype, name=name)\r\n    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2979 ones\r\n        output = fill(shape, constant(one, dtype=dtype), name=name)\r\n    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:234 fill\r\n        result = gen_array_ops.fill(dims, value, name=name)\r\n    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:3311 fill\r\n        return fill_eager_fallback(\r\n    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:3338 fill_eager_fallback\r\n        _result = _execute.execute(b\"Fill\", 1, inputs=_inputs_flat, attrs=_attrs,\r\n    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:75 quick_execute\r\n        raise e\r\n    /home/bitsy-chuck/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:59 quick_execute\r\n        tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n\r\n    TypeError: An op outside of the function building code is being passed\r\n    a \"Graph\" tensor. It is possible to have Graph tensors\r\n    leak out of the function building context by including a\r\n    tf.init_scope in your function building code.\r\n    For example, the following function will fail:\r\n      @tf.function\r\n      def has_init_scope():\r\n        my_constant = tf.constant(1.)\r\n        with tf.init_scope():\r\n          added = my_constant * 2\r\n    The graph tensor has name: model/pconv2d_enc_1/Shape:0\r\n```", "\"\"\"Capsule Network Layers.\r\n\"\"\"\r\n\r\nimport numpy as np\r\n\r\nfrom keras.engine.base_layer import Layer\r\nfrom keras.layers.convolutional import Convolution2D\r\nfrom keras import backend as K\r\nimport tensorflow as tf\r\n\r\nclass CapsuleLayer(Layer):\r\n    def __init__(self,\r\n                 capsules,\r\n                 capsule_dim,\r\n                 activation_caps,\r\n                 kernel_initializer='glorot_uniform',\r\n                 kernel_regularizer=None,\r\n                 kernel_constraint=None,\r\n                 **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.kernel_initializer = kernel_initializer\r\n        self.kernel_regularizer = kernel_regularizer\r\n        self.kernel_constraint = kernel_constraint\r\n\r\n        self.capsules = capsules\r\n        self.capsule_dim = capsule_dim\r\n        self.activation_caps = activation_caps\r\n        \r\nclass PrimaryCaps(CapsuleLayer, Convolution2D):\r\n    \"\"\"PrimaryCaps layer as described in Hinton's paper\"\"\"\r\n    def __init__(self, capsules, capsule_dim, activation_caps, **kwargs):\r\n        super().__init__(capsules, capsule_dim, activation_caps, filters=capsules*capsule_dim, **kwargs)\r\n    \r\n    def call(self, inputs):\r\n        # Apply convolution\r\n        outputs = Convolution2D.call(self, inputs)\r\n        outputs_shape = outputs.shape\r\n\r\n        # Reshape -> (None, -1, capsule_dim)\r\n        outputs = K.reshape(outputs, (K.shape(outputs)[0], outputs_shape[1]*outputs_shape[2]*self.capsules, self.capsule_dim))\r\n        \r\n        if self.activation_caps:\r\n            return self.activation_caps(outputs)\r\n        \r\n        return outputs\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        outputs_shape = Convolution2D.compute_output_shape(self, input_shape)\r\n        return (input_shape[0], outputs_shape[1]*outputs_shape[2]*self.capsules, self.capsule_dim)\r\n\r\nclass Caps(CapsuleLayer):\r\n    \"\"\"Regular capsule layer. Input must be a PrimaryCaps layer. For example, see CapsDigit in original paper.\"\"\"\r\n    def __init__(self, capsules, capsule_dim, routings, activation_caps, **kwargs):\r\n        super().__init__(capsules, capsule_dim, activation_caps, **kwargs)\r\n\r\n        self.routings = routings\r\n        \r\n\r\n    def build(self, input_shape):\r\n        assert len(input_shape) >= 3, \"The input Tensor (PrimaryCaps) should have shape=[None, num_capsules, dim_capsule]\"\r\n        self.input_capsule_dim = input_shape[-1]\r\n        self.input_capsules = input_shape[-2]\r\n\r\n        self.W = self.add_weight(shape=(self.capsules, self.input_capsules, self.capsule_dim, self.input_capsule_dim),\r\n                                  initializer=self.kernel_initializer,\r\n                                  name='W',\r\n                                  regularizer=self.kernel_regularizer,\r\n                                  constraint=self.kernel_constraint)\r\n\r\n        self.built = True\r\n\r\n\r\n    def call(self, inputs):\r\n        # Brodcast inputs to match capsules number\r\n        # inputs.shape:     batch_size, input_capsules, input_capsule_dim\r\n        # u.shape           batch_size, capsules, input_capsules, input_capsule_dim\r\n        u = K.expand_dims(inputs, 1)\r\n        u = K.tile(u, [1, self.capsules, 1, 1])\r\n\r\n        # Matrix multiplication along the last axis of u\r\n        # u_hat.shape:      batch_size, capsules, input_capsules, capsule_dim\r\n        u_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=u)\r\n\r\n        # Routing algorithm\r\n        # b.input_shape     batch_size, capsules, input_capsules\r\n        b = K.zeros(shape=(K.shape(u_hat)[0], self.capsules, self.input_capsules))\r\n\r\n        for r in range(self.routings):\r\n            c = K.softmax(b, axis=1)\r\n\r\n            # Weighted sum, and squash activation\r\n            # s.shape       batch_size, capsules, dim_capsule\r\n            s = K.batch_dot(c, u_hat, [2, 2])\r\n            v = self.activation_caps(s)\r\n\r\n            b += K.batch_dot(v, u_hat, axes=[2, 3])\r\n\r\n        return v\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return (input_shape[0], self.capsules, self.capsule_dim)\r\n    \r\n    \r\n    \r\nclass CapsCNN(Convolution2D):\r\n    def __init__(self, capsule_dim, routings, activation_caps, **kwargs):\r\n        kwargs.setdefault(\"use_bias\", False)\r\n        super().__init__(filters=capsule_dim, **kwargs)\r\n        \r\n        self.capsule_dim = capsule_dim\r\n        self.activation_caps = activation_caps\r\n        self.routings = routings\r\n        \r\n    def call(self, inputs):\r\n        KS = self.kernel_size[0]\r\n        C = K.shape(inputs)[-1]\r\n        x1, SO, x2, x3 = self.compute_output_shape(inputs.shape)\r\n        \r\n        \r\n        x_cols = tf.image.extract_patches(inputs,\r\n                                       sizes=[1, *self.kernel_size, 1],\r\n                                       strides=[1, *self.strides, 1],\r\n                                       rates=[1, *self.dilation_rate, 1],\r\n                                       padding=self.padding.upper()\r\n                                      )\r\n        \r\n        x_cols = K.reshape(x_cols, (-1, KS*KS, C))\r\n        w = K.reshape(self.kernel, (KS*KS, C, self.capsule_dim))\r\n        \r\n        u_hat = K.local_conv1d(x_cols, w, [1], [1])\r\n        u_hat = K.reshape(u_hat, (-1, SO, SO, KS*KS, self.capsule_dim))\r\n        \r\n        b = K.zeros(shape=(K.shape(u_hat)[0], SO, SO, KS*KS))\r\n        #b = K.zeros(shape=(K.shape(u_hat)[0], self.capsule_dim, KS*KS))\r\n        \r\n        for r in range(self.routings):\r\n            c = K.softmax(b, axis=-1)\r\n            \r\n            v = self.activation_caps(K.batch_dot(c, u_hat))\r\n            \r\n            if r < self.routings - 1:\r\n                b += K.batch_dot(v, u_hat, [3, 4])\r\n        \r\n        return v\r\n\r\nclass ClassesCaps(Layer):\r\n    \"\"\"Output of a capsule network, compute the norm of each capsule\"\"\"\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n\r\n    def call(self, inputs):\r\n        return K.sqrt(K.sum(K.square(inputs), axis=-1))\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape[:-1]\r\n\r\nclass Mask(Layer):\r\n    \"\"\"Mask all but correct capsule. Use for training with a decoder.\"\"\"\r\n\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n\r\n    def call(self, inputs, y_true):\r\n        mask = K.expand_dims(y_true, -1)\r\n\r\n        return K.batch_flatten(inputs * mask)\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return (input_shape[0], input_shape[1] * input_shape[2])\r\n\r\n\r\nTypeError: in user code:\r\n\r\n    <ipython-input-38-fd7159b7e222>:134 call  *\r\n        b = K.zeros(shape=(K.shape(u_hat)[0], SO, SO, KS*KS))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:1483 zeros  **\r\n        v = array_ops.zeros(shape=shape, dtype=tf_dtype, name=name)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:2819 wrapped\r\n        tensor = fun(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:2877 zeros\r\n        shape = ops.convert_to_tensor(shape, dtype=dtypes.int32)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py:163 wrapped\r\n        return func(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:1540 convert_to_tensor\r\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:1525 _autopacking_conversion_function\r\n        return _autopacking_helper(v, dtype, name or \"packed\")\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:1461 _autopacking_helper\r\n        return gen_array_ops.pack(elems_as_tensors, name=scope)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py:6385 pack\r\n        values, axis=axis, name=name, ctx=_ctx)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py:6425 pack_eager_fallback\r\n        ctx=ctx, name=name)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py:75 quick_execute\r\n        raise e\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py:60 quick_execute\r\n        inputs, attrs, num_outputs)\r\n\r\n    TypeError: An op outside of the function building code is being passed\r\n    a \"Graph\" tensor. It is possible to have Graph tensors\r\n    leak out of the function building context by including a\r\n    tf.init_scope in your function building code.\r\n    For example, the following function will fail:\r\n      @tf.function\r\n      def has_init_scope():\r\n        my_constant = tf.constant(1.)\r\n        with tf.init_scope():\r\n          added = my_constant * 2\r\n    The graph tensor has name: ConvolutionalCaps/strided_slice_26:0", "Issue @ <ipython-input-38-fd7159b7e222>:134 call  *\r\n    b = K.zeros(shape=(K.shape(u_hat)[0], SO, SO, KS*KS))", "You may need to enable Eager mode. When you compile your model, add this parameter:\r\n```\r\nmodel.compile(..., run_eagerly=True, ...)\r\n```"]}, {"number": 32888, "title": "RuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable or `var = var * value` to get a new Tensor object. When using tf2.0 rc2 and keras min_max_norm constraints for kernel and bias constraints in Google colab", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version GIT_VERSION: v2.0.0-rc1-51-g2646d23\r\n- TensorFlow version : v2.0.0-rc2\r\n\r\n```!python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"```\r\n```v2.0.0-rc1-51-g2646d23 2.0.0-rc2```\r\n\r\n**Code to reproduce the issue**\r\n```\r\ndef create_model(drop, init, embedd_out, kernal_size, padding):\r\n  DROP = drop\r\n  INIT = init\r\n  EMBEDD_OUT = embedd_out\r\n  KERNEL = kernal_size\r\n  PADDING = padding\r\n\r\n  inputs = Input(shape=(MAX_SEQ_LENGTH,), name='feature')\r\n  embedded = Embedding(input_dim=len(tokenizer.word_index) + 1, mask_zero=True, output_dim=EMBEDD_OUT,\r\n                       input_length=MAX_SEQ_LENGTH, name='seq_embedd') (inputs)\r\n\r\n  conv_filters = [64]\r\n  i = 0  \r\n  for u in conv_filters: \r\n    if i == 0:\r\n      conv_1 = Conv1D(u, kernel_size=KERNEL, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),\r\n                      padding=PADDING, activation='relu', kernel_initializer=INIT, name='conv_{}_{}'.format(u, i)) (embedded)\r\n    else:\r\n      conv_1 = Conv1D(u, kernel_size=KERNEL, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),\r\n                      padding=PADDING, activation='relu', kernel_initializer=INIT, name='conv_{}_{}'.format(u, i)) (conv_1)\r\n    conv_1 = MaxPool1D(KERNEL, padding=PADDING, name='max_{}_{}'.format(u, i)) (conv_1)\r\n    conv_1 = Dropout(DROP) (conv_1)\r\n    i += 1\r\n\r\n  conv_filters = [128]\r\n  i = 0  \r\n  for u in conv_filters: \r\n    if i == 0:\r\n      conv_2 = Conv1D(u, kernel_size=KERNEL, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),\r\n                      padding=PADDING, activation='relu', kernel_initializer=INIT, name='conv_{}_{}'.format(u, i)) (embedded)\r\n    else:\r\n      conv_2 = Conv1D(u, kernel_size=KERNEL, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),\r\n                      padding=PADDING, activation='relu', kernel_initializer=INIT, name='conv_{}_{}'.format(u, i)) (conv_2)\r\n    conv_2 = MaxPool1D(KERNEL, padding=PADDING, name='max_{}_{}'.format(u, i)) (conv_2)\r\n    conv_2 = Dropout(DROP) (conv_2)\r\n    i += 1\r\n\r\n  merge = keras.layers.concatenate([conv_1, conv_2])\r\n  reduce_1 = AvgPool1D(KERNEL, padding=PADDING, name='avg_reduce_1') (merge)\r\n\r\n  lstm_units = [64]\r\n  i = 0  \r\n  for u in lstm_units: \r\n    if i == 0:\r\n      lstm_1 = Bidirectional(LSTM(u, return_sequences=True, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),\r\n                                  dropout=DROP, recurrent_dropout=DROP, name='lstm_{}_{}'.format(u, i))) (reduce_1)\r\n    elif i == len(lstm_units):      \r\n      lstm_1 = Bidirectional(LSTM(u, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),\r\n                                  dropout=DROP, recurrent_dropout=DROP, name='lstm_{}_{}'.format(u, i))) (lstm_1)\r\n    else:\r\n      lstm_1 = Bidirectional(LSTM(u, return_sequences=True, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),\r\n                                  dropout=DROP, recurrent_dropout=DROP, name='lstm_{}_{}'.format(u, i))) (lstm_1)\r\n    i += 1\r\n\r\n  lstm_units = [128]\r\n  i = 0  \r\n  for u in lstm_units: \r\n    if i == 0:\r\n      lstm_2 = Bidirectional(LSTM(u, return_sequences=True, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),\r\n                                  dropout=DROP, recurrent_dropout=DROP, name='lstm_{}_{}'.format(u, i))) (reduce_1)\r\n    elif i == len(lstm_units):      \r\n      lstm_2 = Bidirectional(LSTM(u, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),\r\n                                  dropout=DROP, recurrent_dropout=DROP, name='lstm_{}_{}'.format(u, i))) (lstm_2)\r\n    else:\r\n      lstm_2 = Bidirectional(LSTM(u, return_sequences=True, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),\r\n                                  dropout=DROP, recurrent_dropout=DROP, name='lstm_{}_{}'.format(u, i))) (lstm_2)\r\n    i += 1\r\n\r\n  merge = keras.layers.concatenate([lstm_1, lstm_2])\r\n  reduce_2 = GlobalAveragePooling1D(name='g_avg_reduce') (merge)\r\n\r\n  flat = Flatten() (reduce_2)\r\n\r\n  dense_units = [512, 512]\r\n  i = 0  \r\n  for u in dense_units: \r\n    if i == 0:\r\n      dense = Dense(u, kernel_initializer=INIT, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),\r\n                    activation='relu', name='dense_{}_{}'.format(u, i)) (flat)  \r\n    else:\r\n      dense = Dense(u, kernel_initializer=INIT, kernel_constraint=min_max_norm(3), bias_constraint=min_max_norm(3),\r\n                    activation='relu', name='dense_{}_{}'.format(u, i)) (dense)\r\n    dense = Dropout(DROP) (dense)\r\n    i += 1\r\n\r\n  outputs = Dense(2, kernel_initializer=INIT, activation='softmax', name='label') (dense)\r\n  model = Model(inputs=[inputs], outputs=[outputs])\r\n  model.compile(optimizer=keras.optimizers.Adam(amsgrad=True), loss='sparse_categorical_crossentropy', metrics=['acc'])\r\n  return model\r\n```\r\n**Other info / logs**\r\nTrain for 3 steps, validate for 1 steps\r\nEpoch 1/500\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nWARNING:tensorflow:Can save best model only with val_loss available, skipping.\r\nWARNING:tensorflow:Can save best model only with val_acc available, skipping.\r\nWARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-7-65d8bf471350> in <module>()\r\n     13 \r\n     14 model.fit(training_set, epochs=500, validation_data=validation_set,\r\n---> 15           verbose=2, steps_per_epoch=X.shape[0]//BS, class_weight=class_weights, callbacks=callbacks)\r\n\r\n21 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py in __imul__(self, unused_other)\r\n   1227 \r\n   1228   def __imul__(self, unused_other):\r\n-> 1229     raise RuntimeError(\"Variable *= value not supported. Use \"\r\n   1230                        \"`var.assign(var * value)` to modify the variable or \"\r\n   1231                        \"`var = var * value` to get a new Tensor object.\")\r\n\r\nRuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable or `var = var * value` to get a new Tensor object.\r\n", "comments": ["My mistake I was importing from keras instead of tensorflow.keras"]}, {"number": 32887, "title": "[XLA:GPU] Thunk to cudnn fp16 for F16 batchnormalization", "body": "When using cuDNN for batchnormalization, xla currently thunks fp16 batchnorms to fp32 kernels by casting fp16 inputs and gradients to fp32 in the tf2xla bridge. This is non-optimal since cuDNN 7.6 has highly efficient `Half` kernels that can be leveraged. This PR modifies `cudnn_batchnorm_thunk` and introduces new translation units. To thunk F16 batchnorm to fp16 cudnn, converts introduced in the bridge need to be eliminated. This will be done in a subsequent PR.", "comments": ["Hi @sanjoy ... Does this look ok?", "Sorry for the delay Ayan, and thanks for pinging.  I'll review this by tomorrow.", "> This needs tests.\r\n\r\nI have tests. (See https://github.com/tensorflow/tensorflow/pull/32665) I will add them in the next PR since I need the rewriter changes for them to work.   ", "> only minor stuff\r\n\r\nDone."]}, {"number": 32886, "title": "image classification code sample error", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/beta/tutorials/images/classification\r\n\r\n## Description of issue (what needs changing):\r\n```\r\nacc = history.history['accuracy']\r\nval_acc = history.history['val_accuracy']\r\n```\r\nshould be\r\n```\r\nacc = history.history['acc']\r\nval_acc = history.history['val_acc']\r\n```\r\n### Clear description\r\nWhen I copy and paste the code and run it directly, the model is trained for a few minutes/a while but I get KeyError: 'accuracy' and then when I change that, get KeyError: 'val_accuracy'\r\n\r\nSomeone would use this to perform image recognition with code they take directly from the Tensorflow docs on this page.\r\n\r\n### Correct links\r\nIt is not correct yet. I am thinking of making a PR to the docs repo\r\n\r\n### Parameters defined\r\nn/a\r\n\r\n### Returns defined\r\n\r\nn/a\r\n\r\n### Raises listed and defined\r\n\r\nn/a\r\n\r\n### Usage example\r\n\r\nto train images\r\n\r\n### Request visuals, if applicable\r\n\r\nThere's a few.\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n\r\nI am!\r\n", "comments": ["@elizabethsiegle Thanks for reporting this issue. The code in the tutorial is correct for TF2.0. If you want to run it in TF1.x, you need to change those two lines from\r\n```\r\nacc = history.history['accuracy']\r\nval_acc = history.history['val_accuracy']\r\n```\r\nto\r\n```\r\nacc = history.history['acc']\r\nval_acc = history.history['val_acc']\r\n```\r\nThanks! Please close this issue if it was resolved? Thanks!", "I am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!"]}, {"number": 32885, "title": "When importing TensorFlow, error loading Hadoop", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux (like Debian)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-12014-gab20de6 2.0.0-dev20190927\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nImporting TensorFlow prints an error:\r\n\r\n> 2019-09-27 13:51:34.191065: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory\r\n\r\n**Describe the expected behavior**\r\n\r\nImporting TensorFlow should not print errors about Hadoop. I don\u2019t have\r\nHadoop installed and am not interested in using it.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\n$ cd \"$(mktemp -d)\"\r\n$ virtualenv -q -p python3.6 ./ve\r\n$ . ./ve/bin/activate\r\n(ve) $ pip install -q tf-nightly-2.0-preview==2.0.0.dev20190927\r\n(ve) $ python -c 'import tensorflow as tf; print(tf.__version__)'\r\n2019-09-27 13:51:21.263111: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory\r\n2.0.0-dev20190927\r\n```\r\n\r\n**Other info / logs**\r\n\r\nProbably relevant: <https://github.com/tensorflow/tensorflow/pull/32649>\r\n", "comments": ["Maybe the lazy load is a better solution", "Yes, Hadoop should probably be lazy-loaded. Even users who have it\r\ninstalled may well not want to use it in _every_ TensorFlow process;\r\nforcing them to pay the load cost doesn\u2019t make much sense to me.\r\n", "Yep, sending out a change to lazy load.", "I get the same error  as well ( in Tensorflow 2.0.0 and Python 3.5.2).\r\n\r\n> E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory \r\n\r\n\r\n\r\n", "@srinivasgln: @jhseu has a fix in the works; stay tuned.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32885\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32885\">No</a>\n", "Confirmed fixed in nightly; thanks!", "how to solve this error\r\n2020-01-18 19:44:59.740864: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory\r\n>>> \r\n", "@dhumal436 please refer to #36141 and if that doesn't solve your issue please open a new one."]}, {"number": 32884, "title": "tf.image.extract_patches bug - incorrect values", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\ndocker built from tensorflow/tensorflow:2.0.0rc0-gpu-py3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\ndocker built from tensorflow/tensorflow:2.0.0rc0-gpu-py3\r\n- TensorFlow version (use command below):\r\n2.0.0rc0\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n10\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI am using tf.image.extract_patches to get patches of an image read by cv2.\r\nI pad the image with zeros and so I noticed, that the last extracted patch (after reshaping the output) has unexpected values at the very end (last pixels of the last row). They look like coming from another part of the original image. All the other patches look quite all right.\r\nI also tried retyping the image to float32 before it goes to extract_patches - didn't help.\r\n\r\n**Code to reproduce the issue**\r\n```\r\ndef image_to_patches(image, patch_size, stride):                                                                                                                                                                         \r\n    target_width = ((image.shape[1] - patch_size) // stride + 1) * stride + patch_size + 1                                                                                                                  \r\n    target_height = ((image.shape[0] - patch_size) // stride + 1) * stride + patch_size + 1                                                                                                                 \r\n                                                                                                                                                                                                            \r\n    image = np.pad(image, ((0, target_height - image.shape[0]), (0, target_width - image.shape[1]), (0, 0)))                                                                                                \r\n    # here, the last row of `image` is all zeros                                                                                                                                                                      \r\n    batched_image = np.expand_dims(image, 0)                                                                                                                                                                \r\n    patches = tf.image.extract_patches(                                                                                                                                                                     \r\n        images=batched_image,                                                                                                                                                                               \r\n        sizes=[1, patch_size, patch_size, 1],                                                                                                                                                               \r\n        strides=[1, stride, stride, 1],                                                                                                                                                                     \r\n        rates=[1, 1, 1, 1],                                                                                                                                                                                 \r\n        padding='VALID',                                                                                                                                                                                    \r\n        name=None                                                                                                                                                                                           \r\n    )                                                                                                                                                                                                       \r\n    patches = np.array(patches)                                                                                                                                                                             \r\n    patches = np.resize(patches, (patches.shape[0] * patches.shape[1] * patches.shape[2], -1))                                                                                                              \r\n    patches = np.resize(patches, (patches.shape[0], patch_size, patch_size, image.shape[-1]))                                                                                                               \r\n    return patches                                  \r\n```\r\n", "comments": ["@adkoadko Can you please share a github gist of this issue as I am unable to reproduce it with the above code. Thanks!", "@adkoadko It would be better if you could share a complete reproducible code snippet that could expose the issue, as @gowthamkpr suggested to help finding the issue.\r\n\r\n> I am using tf.image.extract_patches to get patches of an image read by cv2.\r\n\r\nIf I remember correctly, OpenCV uses NCHW format which could be different from TensorFlow's layout. Wondering if you did the conversion before calling extract_patches?", "https://gist.github.com/adkoadko/59580391e636705b8684056f9a5482f4\r\n\r\nOpenCV uses NHWC by default, I also reproduced this with plain numpy and tensorflow 2.0.0 (see gist)", "@adkoadko Tried reproducing your issue, but was not able to. Please find my github gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/5fc22477c5e37930170941560319bf8e/untitled166.ipynb).", "I see, you are using an older version of numpy, where `mode` is a required argument. I updated the gist: https://gist.github.com/adkoadko/59580391e636705b8684056f9a5482f4\r\nAfter adding the argument, I was able to reproduce the issue in your notebook", "Was able to reproduce the issue. Please find the github gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/bbb1b6951c41e5337aa319e09e510780/untitled173.ipynb).", "in the repro, the patch size is 60 and stride size is 50, so there will be some overlap. just want to check whether this is what you want", "I think I found the problem, it is not about tf.image.extract_patches. It is about np.resize()\r\n\r\nI inserted print(xx.shape) to check the shapes of extracted patches, and the shape of np.resize, turns of the no.resize(), if you use -1,  the length of last dimension is reduced by 1. \r\nIf you fix it by using patches.shape[3], it prints the correct values as expected.\r\n\r\n    patches = np.array(patches)\r\n    print(patches.shape) .  #(1, 4, 2, 10800)\r\n    patches = np.resize(patches, (patches.shape[0] * patches.shape[1] * patches.shape[2], -1))\r\n    print(patches.shape) .  #(8, 10799)\r\n\r\nFix: \r\npatches = np.resize(patches, (patches.shape[0] * patches.shape[1] * patches.shape[2], **patches.shape[3]**))\r\n", "Or instead of using np.resize(), you could use np.reshape(), which interpret -1 as expected", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32884\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32884\">No</a>\n"]}, {"number": 32883, "title": "[ROCm] Change to pick up rocprim and hipcub libraries from ROCm install", "body": "In newer ROCm releases, rocprim and hipcub libraries will be installed on ROCm stack by default. Therefore it will no longer be necessary to pull them from source (as a third party package). This PR contains the changes requried to pick up rocprim and hipcub from ROCM install\r\n\r\nThis is essentially a cherry-pick this PR ROCmSoftwarePlatform/tensorflow-upstream#620  (authored by @whchung)\r\n\r\n----------------------------------------------------------------\r\n\r\n@whchung @parallelo @sunway513 @chsigg \r\n", "comments": ["@deven-amd would you help resolve the conflicts? thanks.", "> @deven-amd would you help resolve the conflicts? thanks.\r\n\r\ndone", "FYI, I'm working on getting this merged..."]}, {"number": 32882, "title": "Update commands to look for keyword controller in lspci log", "body": "Some GPU SKUs changed the lspci reading in ROCm2.8. \r\nBy looking for the keyword \"controller\" we would be able to cover all, hence this change in ROCm unit test script would ensure better coverage on AMD GPU coverage. \r\ncc @whchung @parallelo ", "comments": ["> Not agreed\r\n\r\nHi @Ishaan28malik , can you help elaborate your concern? \r\nThis change is specific to ROCm unit test script. ", "@sunway513 looks good to me, I have seen it thoroughly now.", "Thank you @Ishaan28malik.\r\nThe Ci failure in Android Demo App seems not related to the PR. @whchung @christisg what's the next step for this PR?", "I will try to get it merged manually."]}, {"number": 32881, "title": "Tensorflow device indexes mismatch with nvidia-smi", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0rc1\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: 10.0 / 7.6.0\r\n- GPU model and memory: RTX 2080 TI  11GB\r\n\r\n**Describe the current behavior**\r\n\r\nI have two gpus.\r\nMy code is designed to determine GPUs info and their indexes before import tensorflow by using nvidia-smi\r\nbut tensorflow device indexes mismatch nvidia-smi indexes.\r\n\r\ntensorflow log:\r\n```\r\n2019-09-27 21:10:02.247000: I tensorflow/stream_executor/platform/default/dso_lo\r\nader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-09-27 21:10:04.473000: I tensorflow/stream_executor/platform/default/dso_lo\r\nader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-09-27 21:10:04.550000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1\r\n\r\n618] Found device 0 with properties:\r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545\r\npciBusID: 0000:03:00.0\r\n2019-09-27 21:10:04.555000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1\r\n\r\n618] Found device 1 with properties:\r\nname: GeForce GT 730 major: 3 minor: 5 memoryClockRate(GHz): 0.9015\r\npciBusID: 0000:01:00.0\r\n\r\n2019-09-27 21:10:04.559000: I tensorflow/stream_executor/platform/default/dlopen\r\n_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-09-27 21:10:04.566000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1\r\n746] Adding visible gpu devices: 0, 1\r\n```\r\n\r\nnvidia-smi log:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 436.30       Driver Version: 436.30       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GT 730     WDDM  | 00000000:01:00.0 N/A |                  N/A |\r\n| 61%   69C    P0    N/A /  N/A |    959MiB /  2048MiB |     N/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 208... WDDM  | 00000000:03:00.0 Off |                  N/A |\r\n| 40%   78C    P2   143W / 250W |  10992MiB / 11264MiB |     89%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nexpected tensorflow match nvidia-smi device indexes\r\n\r\n**Code to reproduce the issue**\r\n\r\nno code required. Just launch tensorflow initialization to show the info of the devices.\r\n", "comments": ["it seems like nvcuda.dll device order differs from nvsmi device order", "@iperov CUDA tries to order the GPUs based on their computing power, nvidia-smi by pci bus id.\r\n\r\nTry running with `CUDA_DEVICE_ORDER=PCI_BUS_ID` to get the nvidia-smi indices.", "@iperov, Did you try the @ahtik's suggestion. Please let us know if that solves your issue. Thanks!", "I got rid of using pynvml.\r\nNow I am using cuda lib directly to determine devices and indexes that match tensorflow. Works windows and linux perfectly.\r\n\r\n```\r\ndef getDevices():\r\n    if Devices.devices is None:        \r\n        min_cc = int(os.environ.get(\"TF_MIN_REQ_CAP\", 35)) \r\n        libnames = ('libcuda.so', 'libcuda.dylib', 'nvcuda.dll')\r\n        for libname in libnames:\r\n            try:\r\n                cuda = ctypes.CDLL(libname)\r\n            except:\r\n                continue\r\n            else:\r\n                break\r\n        else:\r\n            return Devices([])\r\n        nGpus = ctypes.c_int()\r\n        name = b' ' * 200\r\n        cc_major = ctypes.c_int()\r\n        cc_minor = ctypes.c_int()\r\n        freeMem = ctypes.c_size_t()\r\n        totalMem = ctypes.c_size_t()\r\n        result = ctypes.c_int()\r\n        device = ctypes.c_int()\r\n        context = ctypes.c_void_p()\r\n        error_str = ctypes.c_char_p()\r\n        devices = []\r\n        if cuda.cuInit(0) == 0 and \\\r\n           cuda.cuDeviceGetCount(ctypes.byref(nGpus)) == 0:\r\n            for i in range(nGpus.value):\r\n                if cuda.cuDeviceGet(ctypes.byref(device), i) != 0 or \\\r\n                   cuda.cuDeviceGetName(ctypes.c_char_p(name), len(name), device) != 0 or \\\r\n                   cuda.cuDeviceComputeCapability(ctypes.byref(cc_major), ctypes.byref(cc_minor), device) != 0:\r\n                    continue\r\n                if cuda.cuCtxCreate_v2(ctypes.byref(context), 0, device) == 0:\r\n                    if cuda.cuMemGetInfo_v2(ctypes.byref(freeMem), ctypes.byref(totalMem)) == 0:\r\n                        cc = cc_major.value * 10 + cc_minor.value\r\n                        if cc >= min_cc:\r\n                            devices.append ( Device(index=i,\r\n                                                    name=name.split(b'\\0', 1)[0].decode(),                       \r\n                                                    total_mem=totalMem.value,\r\n                                                    free_mem=freeMem.value,\r\n                                                    cc=cc) )\r\n                    cuda.cuCtxDetach(context)\r\n        Devices.devices = Devices(devices)\r\n    return Devices.devices\r\n```", "@iperov I would be a bit cautious using your snuppet as it's likely not deterministic.\r\nHave a look at https://docs.nvidia.com/deploy/nvml-api/group__nvmlDeviceQueries.html#group__nvmlDeviceQueries_1g52677ecb45f937c5005124608780a3f4\r\n\r\n`The order in which NVML enumerates devices has no guarantees of consistency between reboots. For that reason it is recommended that devices be looked up by their PCI ids or UUID`\r\n\r\nBtw, nvidia-smi is also using nvml behind the scenes, but likely just orders by pci bus id internally by itself to ensure consistency.\r\n\r\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars has a bit more details on CUDA ordering with `CUDA_\u200bDEVICE_\u200bORDER`: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars Note that only the fastest is put at index 0 and the rest is not ordered.", "@ahtik  \r\n\r\nsometimes nvml does not come with video drivers on some notebook cards, even high-end cards like 1080. There is no ```NVIDIA Corporation\\NVSMI``` folder that contains nvml.dll on such systems.\r\n\r\nso I think using direct cuda lib is the best choice\r\nbecause\r\n1) indexes are always match tensorflow indexes\r\n2) works everywhere, tested on google colab.\r\n\r\n\r\n", "I see. If it works for you then great, but still: when running CUDA with `CUDA_\u200bDEVICE_\u200bORDER=FASTEST_FIRST` (default), only the device 0 is guaranteed to be the same (and 2nd, if you have two cards ;-) Rest are unspecified. AFAIK CUDA `FASTEST_FIRST` ordering code is not anywhere publicly available.\r\n\r\nBut depends on your use-case, looks like cuda lib call works great for you, especially if there are just 2 gpu-s or ordering must persist only within one process lifecycle.", "also I tried to set \r\n```\r\nos.environ['CUDA_\u200bDEVICE_\u200bORDER'] = 'PCI_BUS_ID'\r\n```\r\nbut it did not fixed the issue when using pynvml", "@iperov Is this still an issue or resolved? Please close the issue if it was resolved already. Also, please try latest `tf-nightly` and let us know whether the issue persists with the latest TF version. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 32880, "title": "deprecation warning when training a simple embedding with custom function", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-beta1\r\n- TensorFlow version (use command below): v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0\r\n- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\r\n\r\n**Describe the current behavior**\r\n\r\nThe following code trains a simple embedding on temporal indices, Z, to match a given time series X:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Model, losses, optimizers\r\nfrom tensorflow.keras.layers import Embedding\r\nimport numpy as np\r\n\r\n\r\ndef extract(indices, X): \r\n    # extract the lines of X given by the indices\r\n    # if indices.shape = 4\r\n    # and X.shape = (20, 5)\r\n    # then the output has shape (4, 5)\r\n    indices = tf.reshape(indices, [-1, 1])\r\n    return tf.gather_nd(X, indices)\r\n\r\n\r\nclass Test(Model):\r\n\r\n    def __init__(self, X, **kwargs):\r\n        super(Test, self).__init__(**kwargs)\r\n        self.X = X\r\n\r\n        self.optimizer = optimizers.Adam(learning_rate=1e-3)\r\n        self.loss = losses.CategoricalCrossentropy()\r\n\r\n        self.Z = Embedding(X.shape[0], X.shape[1])\r\n\r\n    def call(self, inputs, training=None):\r\n        # inputs are temporal indices\r\n        Z_t = self.Z(inputs)\r\n        return Z_t\r\n\r\n    def custom_train(self):\r\n\r\n        @tf.function\r\n        def _train_Step(index):\r\n            with tf.GradientTape() as tape:\r\n                Z_t = self(index, training=True)\r\n                X_t = extract(index, self.X)\r\n                loss = self.loss(X_t, Z_t)\r\n\r\n            grads = tape.gradient(loss, self.trainable_variables)\r\n            self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\r\n            return\r\n        # this is a toy exemple: we simply make one training step on the index 0\r\n        _train_Step(tf.constant([0]))\r\n        return \r\n    \r\nT = 20\r\nn = 5\r\nX = np.random.rand(T, n)\r\nmodel = Test(X=X)\r\nmodel.custom_train()\r\n```\r\n and I get the following warning about the use of a deprecated method:\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0927 19:04:48.957805 140735485318016 deprecation.py:323] From /path/to/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\n\r\n```\r\n\r\nThis reminds me about [an other issue](https://github.com/tensorflow/tensorflow/issues/29881) I opened with a lot of deprecation warnings arising when I use the API. This issue is closed but the warnings are still here (and, in fact, the release of the rc0 has brought a new one, see my last comment there).\r\n\r\n", "comments": ["Was able to reproduce the issue. The github gist is [here](https://colab.sandbox.google.com/gist/gowthamkpr/3b8b4b959ee44861ce8c20c3ff399559/untitled150.ipynb)", "The warning is not here in tf2.0.0 so I can close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32880\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32880\">No</a>\n"]}, {"number": 32879, "title": "I want a example code to use the embedding projector in tensorboard by tensorflow-2.0.", "body": "I want a example code to use the embedding projector in tensorboard by tensorflow-2.0.\r\n", "comments": ["@jiawei6636 \r\nCan you please go through below link and see if it helps you. Thanks!\r\nhttps://www.tensorflow.org/tutorials/text/word_embeddings", "> @jiawei6636\r\n> Can you please go through below link and see if it helps you. Thanks!\r\n> https://www.tensorflow.org/tutorials/text/word_embeddings\r\n\r\nI want to use the local tensorboard, not the online embedding projector. I can use the local tensorboard embedding projector in TF1.0, but I don't how to use it in TF2.0", "@jiawei6636,\r\nCan you please refer [this link](https://stackoverflow.com/a/41370610/11530462) for visualizing Embedding Projector Locally and let us know if it helps. Thanks!", "> @jiawei6636,\r\n> Can you please refer [this link](https://stackoverflow.com/a/41370610/11530462) for visualizing Embedding Projector Locally and let us know if it helps. Thanks!\r\n\r\nNo, it is TF1.0 code.", "@jiawei6636,\r\nCan you please check [this link](https://github.com/tensorflow/tensorboard/issues/2471)  and let us know if it is helpful. Thanks!", "> It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?\r\n\r\nYes, this is still an issue. But I can solve it by Pytorch([There](https://pytorch.org/docs/stable/tensorboard.html) is a convenient API in Pytorch). I think you should give me a  similar solution in Tensorflow-2.0. ", "> It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?\r\n\r\nYour answers are all perfunctory. Tensorboard is a good tool in Tensorflow's ecosystem, but the Pytorch uses it better than Tensorflow. \r\n : )", "> @jiawei6636\r\n> Can you please go through below link and see if it helps you. Thanks!\r\n> https://www.tensorflow.org/tutorials/text/word_embeddings\r\n\r\nI do not want to use http://projector.tensorflow.org/ but visualize my vectors/embeddings in board running locally. Can you share code for tf2.0. The issue is `from tensorflow.contrib.tensorboard.plugins import projector` command is invalid. :(", "The following works for me, though TF 1 vs 2 example updates would be much appreciated!\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorboard.plugins import projector\r\n\r\nhelp(projector)\r\n```\r\n\r\nThe help and examples need to be updated for 2.0.", "In another issue I have posted some examples on how to make Tensorboard Projector work in both TF2.0 and TF2.1 in both Non-Eager and Eager modes. Please check https://github.com/tensorflow/tensorboard/issues/2471#issuecomment-580423961. Hope this helps.", "We have updated the Projector documentation with a TF 2.0 example: https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin"]}, {"number": 32878, "title": "Iembedding projector ", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": []}, {"number": 32877, "title": "matmul / matvec / einsum bug when dim exceeds 2**16", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): git: **v1.12.1-9365-gff401a6** tf: **1.15.0-dev20190821**\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0 / 6.0.21  (also tested on 10.1 / 7.1.4 / GTX-10708GB\r\n- GPU model and memory: Quadro  2GB\r\n\r\n**Describe the current behavior**\r\nGPU float32 implementations of `tf.matmul`, `tf.matvec` and `tf.einsum` all fail when applied to tensors with a `dimension >= 2**16 = 65536` (though not necessarily the summation dimension). Could be related to [this](https://github.com/tensorflow/tensorflow/issues/31022).\r\n\r\nOf particular note:\r\n\r\n* The example below shows the bug as a result of summing over a dimension of size `2` and inputs of typical size (even `ones`), so is not a case of overflow of the individual entries.\r\n* The issue does not appear when using `tf.float64` or cpu implementation.\r\n* Results are **not deterministic** - at least, not when the large dimension is `2 ** 16 + 1` or more and using `tf.linalg.matvec` implementation, thoguh they do appear stable on at `2 ** 16`. The `tf.linalg.matvec` implementation seems to 'converge' to the the hacky `tf.matmul` version, thoguh that converged value is different to the `einsum` value.\r\n* The issue does not appear in this example when running in graph mode, though I believe it still does occur in other circumstances. The more complicated example I extracted this from is not resolved by running in graph mode, but I cannot produce a simpler example that exhibits failure in graph mode. This may be because of the non-determinism discussed above.\r\n\r\n**Describe the expected behavior**\r\nConsistency of implementation between CPU/GPU, correct values.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nnp.random.seed(123)\r\n\r\n\r\ndef matvec_prim_np(a, b):\r\n    \"\"\"numpy implementation of tf.linalg.matvec(a, b, transpose_a=True).\"\"\"\r\n    return np.sum(a * np.expand_dims(b, axis=-1), axis=-2)\r\n\r\n\r\ndef matvec_einsum_np(a, b):\r\n    return np.einsum('ijk,ij->ik', a, b)\r\n\r\n\r\ndef matvec_linalg(a, b):\r\n    return tf.linalg.matvec(a, b, transpose_a=True)\r\n\r\n\r\ndef matvec_matmul(a, b):\r\n    return tf.squeeze(tf.linalg.matmul(a,\r\n                                       tf.expand_dims(b, axis=-1),\r\n                                       transpose_a=True),\r\n                      axis=-1)\r\n\r\n\r\ndef matvec_prim(a, b):\r\n    return tf.reduce_sum(a * tf.expand_dims(b, axis=-1), axis=-2)\r\n\r\n\r\ndef matvec_einsum(a, b):\r\n    a = tf.convert_to_tensor(a)\r\n    b = tf.convert_to_tensor(b)\r\n    return tf.einsum('ijk,ij->ik', a, b)\r\n\r\n\r\ndef max_err(x, y):\r\n    if hasattr(x, 'numpy'):\r\n        x = x.numpy()\r\n    if hasattr(y, 'numpy'):\r\n        y = y.numpy()\r\n    return np.max(np.abs(x - y))\r\n\r\n\r\ndef report(N, values='ones', dtype=np.float64, device='/gpu:0'):\r\n    shapes = (N, 2, 2), (N, 2)\r\n    if values == 'ones':\r\n        a, b = (np.ones(s, dtype=dtype) for s in shapes)\r\n    elif values == 'uniform':\r\n        a, b = (np.random.uniform(size=s).astype(dtype=dtype) for s in shapes)\r\n    elif values == 'normal':\r\n        a, b = (np.random.normal(size=s).astype(dtype=dtype) for s in shapes)\r\n    else:\r\n        raise ValueError('values must be \"one\", \"uniform\" or \"normal\", '\r\n                         'got {}'.format(values))\r\n    a_extents = np.min(a), np.max(a)\r\n    b_extents = np.min(b), np.max(b)\r\n    base = matvec_prim_np(a, b)\r\n    names = ['matvec_einsum_np']\r\n    vals = [matvec_einsum_np(a, b)]\r\n    fns = (matvec_prim, matvec_linalg, matvec_matmul, matvec_einsum)\r\n    a = tf.constant(a, dtype=dtype)\r\n    b = tf.constant(b, dtype=dtype)\r\n    # call each function twice to demonstrate non-determinism of linalg.matvec\r\n    with tf.device(device):\r\n        tf_vals = tuple(fn(a, b) for fn in fns) + tuple(fn(a, b) for fn in fns)\r\n    names.extend(fn.__name__ for fn in fns)\r\n    names.extend(fn.__name__ for fn in fns)\r\n    if tf.executing_eagerly():\r\n        tf_vals = [v.numpy() for v in tf_vals]\r\n    else:\r\n        with tf.Session() as sess:\r\n            tf_vals = sess.run(tf_vals)\r\n    vals.extend(tf_vals)\r\n\r\n    print('##### {} - {} - {} - {} ####'.format(N, dtype.__name__, values,\r\n                                                device))\r\n    print('a extents: {}'.format(a_extents))\r\n    print('b extents: {}'.format(b_extents))\r\n    for name, value in zip(names, vals):\r\n        print('{:20s}: {}'.format(name, max_err(base, value)))\r\n\r\n\r\nworks = 65535\r\njust_too_big = 65536\r\neven_bigger = 65537\r\ndtype = np.float64\r\n\r\ntf.compat.v1.enable_eager_execution()\r\nfor device in '/cpu:0', '/gpu:0':\r\n    for dtype in (np.float64, np.float32):\r\n        for N in works, just_too_big, even_bigger:\r\n            for values in ('ones', 'uniform', 'normal'):\r\n                report(N, dtype=dtype, values=values, device=device)\r\n```\r\n\r\n**Other info / logs**\r\nAll results in the above are good except for `N >= 65536`, `dtype=tf.float32` and `device='/gpu:0'`. Those logs are below.\r\n\r\n```\r\n##### 65535 - float32 - normal - /gpu:0 ####\r\na extents: (-4.471484, 4.361648)\r\nb extents: (-5.123302, 4.7598076)\r\nmatvec_einsum_np    : 0.0\r\nmatvec_prim         : 0.0\r\nmatvec_linalg       : 0.0\r\nmatvec_matmul       : 0.0\r\nmatvec_einsum       : 0.0\r\nmatvec_prim         : 0.0\r\nmatvec_linalg       : 0.0\r\nmatvec_matmul       : 0.0\r\nmatvec_einsum       : 0.0\r\n##### 65536 - float32 - ones - /gpu:0 ####\r\na extents: (1.0, 1.0)\r\nb extents: (1.0, 1.0)\r\nmatvec_einsum_np    : 0.0\r\nmatvec_prim         : 0.0\r\nmatvec_linalg       : 1.0\r\nmatvec_matmul       : 2.0\r\nmatvec_einsum       : 1.0\r\nmatvec_prim         : 0.0\r\nmatvec_linalg       : 1.0\r\nmatvec_matmul       : 2.0\r\nmatvec_einsum       : 1.0\r\n##### 65536 - float32 - uniform - /gpu:0 ####\r\na extents: (1.0896997e-06, 0.9999953)\r\nb extents: (1.2711744e-06, 0.99999875)\r\nmatvec_einsum_np    : 0.0\r\nmatvec_prim         : 0.0\r\nmatvec_linalg       : 0.24248471856117249\r\nmatvec_matmul       : 0.3159205913543701\r\nmatvec_einsum       : 0.4631575047969818\r\nmatvec_prim         : 0.0\r\nmatvec_linalg       : 0.24248471856117249\r\nmatvec_matmul       : 0.3159205913543701\r\nmatvec_einsum       : 0.4631575047969818\r\n##### 65536 - float32 - normal - /gpu:0 ####\r\na extents: (-4.715384, 4.746153)\r\nb extents: (-4.523676, 4.59162)\r\nmatvec_einsum_np    : 0.0\r\nmatvec_prim         : 0.0\r\nmatvec_linalg       : 0.6585559248924255\r\nmatvec_matmul       : 0.709877073764801\r\nmatvec_einsum       : 0.4629881978034973\r\nmatvec_prim         : 0.0\r\nmatvec_linalg       : 0.6585559248924255\r\nmatvec_matmul       : 0.709877073764801\r\nmatvec_einsum       : 0.4629881978034973\r\n##### 65537 - float32 - ones - /gpu:0 ####\r\na extents: (1.0, 1.0)\r\nb extents: (1.0, 1.0)\r\nmatvec_einsum_np    : 0.0\r\nmatvec_prim         : 0.0\r\nmatvec_linalg       : 4.812917232513428\r\nmatvec_matmul       : 2.0\r\nmatvec_einsum       : 1.0\r\nmatvec_prim         : 0.0\r\nmatvec_linalg       : 2.0\r\nmatvec_matmul       : 2.0\r\nmatvec_einsum       : 1.0\r\n##### 65537 - float32 - uniform - /gpu:0 ####\r\na extents: (6.188006e-07, 0.9999941)\r\nb extents: (5.5208016e-06, 0.9999933)\r\nmatvec_einsum_np    : 0.0\r\nmatvec_prim         : 0.0\r\nmatvec_linalg       : 2.9982364177703857\r\nmatvec_matmul       : 0.4082830250263214\r\nmatvec_einsum       : 0.40019217133522034\r\nmatvec_prim         : 0.0\r\nmatvec_linalg       : 0.4082830250263214\r\nmatvec_matmul       : 0.4082830250263214\r\nmatvec_einsum       : 0.40019217133522034\r\n##### 65537 - float32 - normal - /gpu:0 ####\r\na extents: (-5.1086445, 4.5197787)\r\nb extents: (-4.8438015, 3.928705)\r\nmatvec_einsum_np    : 0.0\r\nmatvec_prim         : 0.0\r\nmatvec_linalg       : 4.819068908691406      <----------- same function, same inputs\r\nmatvec_matmul       : 2.1483089923858643\r\nmatvec_einsum       : 2.5970025062561035\r\nmatvec_prim         : 0.0\r\nmatvec_linalg       : 2.1483089923858643     <----------- same function, same inputs\r\nmatvec_matmul       : 2.1483089923858643\r\nmatvec_einsum       : 2.5970025062561035\r\n```\r\n", "comments": ["I have tried on colab with TF version 1.15.0-rc1 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/4025781d3a84432fc1badf78bf23fe98/untitled235.ipynb). Thanks!", "Related to #31166", "@reedwm, could you help to take a look? Thanks.", "This does not occurring when CUDA 10.1 is used (which requires building TF from source), so I'm guessing this is a CUDA 10.0 bug. This will be fixed when we switch the builds to CUDA 10.1.", "Confirmed that this is fixed when built with CUDA 10.1\r\n\r\nFor convenience, here's a wheel of tensorflow 1.15.0 that was built with CUDA 10.1, cuDNN 7.6.3. Built with compute compatability 6.1, 7.0, 7.5.  Built inside a docker container running nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04\r\n\r\nhttps://drive.google.com/open?id=1D4QGZql-ZDu6uYLKJljNJAV3zclmVmdV", "> Confirmed that this is fixed when built with CUDA 10.1\r\n\r\nClosing as per this comment.  Please reopen if there is something for us to do here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32877\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32877\">No</a>\n"]}, {"number": 32876, "title": "How to use estimator predict on multi machine, predict run on every worker?", "body": "", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 32875, "title": "tf.keras.metrics.MeanIoU have some conflicts with sparse_categorical_crossentropy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0 RC\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 12Gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n```\r\nmIOU = tf.keras.metrics.MeanIoU(num_classes=20)\r\nmodel.compile(optimizer='Adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=[\"accuracy\", mIOU])\r\n```\r\n\r\nHi, sparse_categorical_crossentropy will calculate the probabilities of 20 classes, which have the shape (None, 64, 1024, 20). However, the label has the shape (None, 64, 1024). Thus, the mIOU got unequal shape inputs. It got errors\r\n", "comments": ["Invalid argument:  Shapes of all inputs must match: values[0].shape = [131072] != values[1].shape = [2621440]", "@hagianga21 ,\r\nThank you for reporting the issue,Can you share a simple and standalone code to reproduce the issue?Thanks!\r\n", "I don't have any simple code right now. Do you have any suggestion?", "I also tried: \r\n\r\ndef mean_IOU(y_true, y_pred):\r\n    m = tf.keras.metrics.MeanIoU(num_classes=20)\r\n    m.update_state(y_true, tf.argmax(y_pred, 3))\r\n    return m.result()\r\nmodel.compile(optimizer= optimizers.SGD(lr=0.01, momentum=0.9), loss='sparse_categorical_crossentropy', metrics=['accuracy', mean_IOU])\r\n\r\nThe error I got: \r\n\r\nTrain for 2750.0 steps, validate for 151.0 steps\r\nEpoch 1/30\r\nWARNING:tensorflow:From c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nWARNING:tensorflow:From c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n   1/2750 [..............................] - ETA: 2:37:45\r\nEpoch 00001: saving model to checkpoints/squeezeSegV2_batch8_SGD/cp-0001.ckpt\r\n   1/2750 [..............................] - ETA: 3:06:50\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-9-5dd9eaf367ae> in <module>\r\n      2 tfdir = os.path.join(os.getcwd(), 'tflogs', Name)\r\n      3 tensorboard = tf.keras.callbacks.TensorBoard(log_dir=tfdir, update_freq = 'epoch')\r\n----> 4 history = model.fit(train_ds, validation_data=val_ds, epochs=30, steps_per_epoch=train_steps_per_epoch, validation_steps = val_steps_per_epoch, callbacks = [tensorboard, cp_callback])\r\n      5 #history = finalmodel.fit(train_ds, validation_data=val_ds, epochs=1, steps_per_epoch=11000, validation_steps = 601, callbacks = [tensorboard])\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    322                 mode=ModeKeys.TRAIN,\r\n    323                 training_context=training_context,\r\n--> 324                 total_epochs=epochs)\r\n    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    326 \r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    518         # Lifting succeeded, so variables are initialized and we can run the\r\n    519         # stateless function.\r\n--> 520         return self._stateless_fn(*args, **kwds)\r\n    521     else:\r\n    522       canon_args, canon_kwds = \\\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   1820   def __call__(self, *args, **kwargs):\r\n   1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n-> 1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n   1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1824 \r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in distributed_function(input_iterator)\r\n     71     strategy = distribution_strategy_context.get_strategy()\r\n     72     outputs = strategy.experimental_run_v2(\r\n---> 73         per_replica_function, args=(model, x, y, sample_weights))\r\n     74     # Out of PerReplica outputs reduce or pick values to return.\r\n     75     all_outputs = dist_utils.unwrap_output_dict(\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)\r\n    758       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\r\n    759                                 convert_by_default=False)\r\n--> 760       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    761 \r\n    762   def reduce(self, reduce_op, value, axis):\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\r\n   1785       kwargs = {}\r\n   1786     with self._container_strategy().scope():\r\n-> 1787       return self._call_for_each_replica(fn, args, kwargs)\r\n   1788 \r\n   1789   def _call_for_each_replica(self, fn, args, kwargs):\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)\r\n   2130         self._container_strategy(),\r\n   2131         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\r\n-> 2132       return fn(*args, **kwargs)\r\n   2133 \r\n   2134   def _reduce_to(self, reduce_op, value, destinations):\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py in wrapper(*args, **kwargs)\r\n    290   def wrapper(*args, **kwargs):\r\n    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 292       return func(*args, **kwargs)\r\n    293 \r\n    294   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)\r\n    262       y,\r\n    263       sample_weights=sample_weights,\r\n--> 264       output_loss_metrics=model._output_loss_metrics)\r\n    265 \r\n    266   if reset_metrics:\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py in train_on_batch(model, inputs, targets, sample_weights, output_loss_metrics)\r\n    313     outs = [outs]\r\n    314   metrics_results = _eager_metrics_fn(\r\n--> 315       model, outs, targets, sample_weights=sample_weights, masks=masks)\r\n    316   total_loss = nest.flatten(total_loss)\r\n    317   return {'total_loss': total_loss,\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py in _eager_metrics_fn(model, outputs, targets, sample_weights, masks)\r\n     72         masks=masks,\r\n     73         return_weighted_and_unweighted_metrics=True,\r\n---> 74         skip_target_masks=model._prepare_skip_target_masks())\r\n     75 \r\n     76   # Add metric results from the `add_metric` metrics.\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in _handle_metrics(self, outputs, targets, skip_target_masks, sample_weights, masks, return_weighted_metrics, return_weighted_and_unweighted_metrics)\r\n   2061           metric_results.extend(\r\n   2062               self._handle_per_output_metrics(self._per_output_metrics[i],\r\n-> 2063                                               target, output, output_mask))\r\n   2064         if return_weighted_and_unweighted_metrics or return_weighted_metrics:\r\n   2065           metric_results.extend(\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in _handle_per_output_metrics(self, metrics_dict, y_true, y_pred, mask, weights)\r\n   2012       with K.name_scope(metric_name):\r\n   2013         metric_result = training_utils.call_metric_function(\r\n-> 2014             metric_fn, y_true, y_pred, weights=weights, mask=mask)\r\n   2015         metric_results.append(metric_result)\r\n   2016     return metric_results\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py in call_metric_function(metric_fn, y_true, y_pred, weights, mask)\r\n   1065 \r\n   1066   if y_pred is not None:\r\n-> 1067     return metric_fn(y_true, y_pred, sample_weight=weights)\r\n   1068   # `Mean` metric only takes a single value.\r\n   1069   return metric_fn(y_true, sample_weight=weights)\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in __call__(self, *args, **kwargs)\r\n    191     from tensorflow.python.keras.distribute import distributed_training_utils  # pylint:disable=g-import-not-at-top\r\n    192     return distributed_training_utils.call_replica_local_fn(\r\n--> 193         replica_local_fn, *args, **kwargs)\r\n    194 \r\n    195   @property\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\distribute\\distributed_training_utils.py in call_replica_local_fn(fn, *args, **kwargs)\r\n   1133     with strategy.scope():\r\n   1134       return strategy.extended.call_for_each_replica(fn, args, kwargs)\r\n-> 1135   return fn(*args, **kwargs)\r\n   1136 \r\n   1137 \r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in replica_local_fn(*args, **kwargs)\r\n    174     def replica_local_fn(*args, **kwargs):\r\n    175       \"\"\"Updates the state of the metric in a replica-local context.\"\"\"\r\n--> 176       update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\r\n    177       with ops.control_dependencies([update_op]):\r\n    178         result_t = self.result()  # pylint: disable=not-callable\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\metrics_utils.py in decorated(metric_obj, *args, **kwargs)\r\n     73 \r\n     74     with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):\r\n---> 75       update_op = update_state_fn(*args, **kwargs)\r\n     76     if update_op is not None:  # update_op will be None in eager execution.\r\n     77       metric_obj.add_update(update_op)\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in update_state(self, y_true, y_pred, sample_weight)\r\n    579         y_pred, y_true)\r\n    580 \r\n--> 581     matches = self._fn(y_true, y_pred, **self._fn_kwargs)\r\n    582     return super(MeanMetricWrapper, self).update_state(\r\n    583         matches, sample_weight=sample_weight)\r\n\r\n<ipython-input-5-00bdd6491e85> in mean_IOU(y_true, y_pred)\r\n      1 mIOU = tf.keras.metrics.MeanIoU(num_classes=20)\r\n      2 def mean_IOU(y_true, y_pred):\r\n----> 3     m = tf.keras.metrics.MeanIoU(num_classes=20)\r\n      4     m.update_state(y_true, tf.argmax(y_pred, 3))\r\n      5     return m.result()\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in __init__(self, num_classes, name, dtype)\r\n   2308         shape=(num_classes, num_classes),\r\n   2309         initializer=init_ops.zeros_initializer,\r\n-> 2310         dtype=dtypes.float64)\r\n   2311 \r\n   2312   def update_state(self, y_true, y_pred, sample_weight=None):\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in add_weight(self, name, shape, aggregation, synchronization, initializer, dtype)\r\n    271         collections=[],\r\n    272         synchronization=synchronization,\r\n--> 273         aggregation=aggregation)\r\n    274 \r\n    275   ### End: For use by subclasses ###\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\r\n    520         collections=collections_arg,\r\n    521         synchronization=synchronization,\r\n--> 522         aggregation=aggregation)\r\n    523     backend.track_variable(variable)\r\n    524 \r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\r\n    742         dtype=dtype,\r\n    743         initializer=initializer,\r\n--> 744         **kwargs_for_getter)\r\n    745 \r\n    746     # If we set an initializer and the variable processed it, tracking will not\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer_utils.py in make_variable(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\r\n    137       synchronization=synchronization,\r\n    138       aggregation=aggregation,\r\n--> 139       shape=variable_shape if variable_shape else None)\r\n    140 \r\n    141 \r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py in __call__(cls, *args, **kwargs)\r\n    256   def __call__(cls, *args, **kwargs):\r\n    257     if cls is VariableV1:\r\n--> 258       return cls._variable_v1_call(*args, **kwargs)\r\n    259     elif cls is Variable:\r\n    260       return cls._variable_v2_call(*args, **kwargs)\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py in _variable_v1_call(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\r\n    217         synchronization=synchronization,\r\n    218         aggregation=aggregation,\r\n--> 219         shape=shape)\r\n    220 \r\n    221   def _variable_v2_call(cls,\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py in getter(**kwargs)\r\n     63 \r\n     64   def getter(**kwargs):\r\n---> 65     return captured_getter(captured_previous, **kwargs)\r\n     66 \r\n     67   return getter\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in creator(next_creator, *args, **kwargs)\r\n   2046     def creator(next_creator, *args, **kwargs):\r\n   2047       _require_strategy_scope_strategy(strategy)\r\n-> 2048       return next_creator(*args, **kwargs)\r\n   2049 \r\n   2050     self._var_creator_scope = variable_scope.variable_creator_scope(creator)\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py in getter(**kwargs)\r\n     63 \r\n     64   def getter(**kwargs):\r\n---> 65     return captured_getter(captured_previous, **kwargs)\r\n     66 \r\n     67   return getter\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in creator(next_creator, *args, **kwargs)\r\n   2046     def creator(next_creator, *args, **kwargs):\r\n   2047       _require_strategy_scope_strategy(strategy)\r\n-> 2048       return next_creator(*args, **kwargs)\r\n   2049 \r\n   2050     self._var_creator_scope = variable_scope.variable_creator_scope(creator)\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py in getter(**kwargs)\r\n     63 \r\n     64   def getter(**kwargs):\r\n---> 65     return captured_getter(captured_previous, **kwargs)\r\n     66 \r\n     67   return getter\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in creator(next_creator, *args, **kwargs)\r\n   2046     def creator(next_creator, *args, **kwargs):\r\n   2047       _require_strategy_scope_strategy(strategy)\r\n-> 2048       return next_creator(*args, **kwargs)\r\n   2049 \r\n   2050     self._var_creator_scope = variable_scope.variable_creator_scope(creator)\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py in getter(**kwargs)\r\n     63 \r\n     64   def getter(**kwargs):\r\n---> 65     return captured_getter(captured_previous, **kwargs)\r\n     66 \r\n     67   return getter\r\n\r\nc:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in invalid_creator_scope(*unused_args, **unused_kwds)\r\n    411       \"\"\"Disables variable creation.\"\"\"\r\n    412       raise ValueError(\r\n--> 413           \"tf.function-decorated function tried to create \"\r\n    414           \"variables on non-first call.\")\r\n    415 \r\n\r\nValueError: tf.function-decorated function tried to create variables on non-first call.", "> I also tried:\r\n> \r\n> def mean_IOU(y_true, y_pred):\r\n> m = tf.keras.metrics.MeanIoU(num_classes=20)\r\n> m.update_state(y_true, tf.argmax(y_pred, 3))\r\n> return m.result()\r\n> model.compile(optimizer= optimizers.SGD(lr=0.01, momentum=0.9), loss='sparse_categorical_crossentropy', metrics=['accuracy', mean_IOU])\r\n> \r\n> The error I got:\r\n> \r\n> ## Train for 2750.0 steps, validate for 151.0 steps\r\n> Epoch 1/30\r\n> WARNING:tensorflow:From c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use tf.where in 2.0, which has the same broadcast rule as np.where\r\n> WARNING:tensorflow:From c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.**init** (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> If using Keras pass *_constraint arguments to layers.\r\n> 1/2750 [..............................] - ETA: 2:37:45\r\n> Epoch 00001: saving model to checkpoints/squeezeSegV2_batch8_SGD/cp-0001.ckpt\r\n> 1/2750 [..............................] - ETA: 3:06:50\r\n> ValueError Traceback (most recent call last)\r\n> in\r\n> 2 tfdir = os.path.join(os.getcwd(), 'tflogs', Name)\r\n> 3 tensorboard = tf.keras.callbacks.TensorBoard(log_dir=tfdir, update_freq = 'epoch')\r\n> ----> 4 history = model.fit(train_ds, validation_data=val_ds, epochs=30, steps_per_epoch=train_steps_per_epoch, validation_steps = val_steps_per_epoch, callbacks = [tensorboard, cp_callback])\r\n> 5 #history = finalmodel.fit(train_ds, validation_data=val_ds, epochs=1, steps_per_epoch=11000, validation_steps = 601, callbacks = [tensorboard])\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n> 726 max_queue_size=max_queue_size,\r\n> 727 workers=workers,\r\n> --> 728 use_multiprocessing=use_multiprocessing)\r\n> 729\r\n> 730 def evaluate(self,\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n> 322 mode=ModeKeys.TRAIN,\r\n> 323 training_context=training_context,\r\n> --> 324 total_epochs=epochs)\r\n> 325 cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n> 326\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n> 121 step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n> 122 try:\r\n> --> 123 batch_outs = execution_function(iterator)\r\n> 124 except (StopIteration, errors.OutOfRangeError):\r\n> 125 # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n> 84 # `numpy` translates Tensors to values in Eager mode.\r\n> 85 return nest.map_structure(_non_none_constant_value,\r\n> ---> 86 distributed_function(input_fn))\r\n> 87\r\n> 88 return execution_function\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in **call**(self, *args, **kwds)\r\n> 455\r\n> 456 tracing_count = self._get_tracing_count()\r\n> --> 457 result = self._call(*args, **kwds)\r\n> 458 if tracing_count == self._get_tracing_count():\r\n> 459 self._call_counter.called_without_tracing()\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n> 518 # Lifting succeeded, so variables are initialized and we can run the\r\n> 519 # stateless function.\r\n> --> 520 return self._stateless_fn(*args, **kwds)\r\n> 521 else:\r\n> 522 canon_args, canon_kwds = \\\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in **call**(self, *args, **kwargs)\r\n> 1820 def **call**(self, *args, **kwargs):\r\n> 1821 \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n> -> 1822 graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n> 1823 return graph_function._filtered_call(args, kwargs) # pylint: disable=protected-access\r\n> 1824\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\r\n> 2148 graph_function = self._function_cache.primary.get(cache_key, None)\r\n> 2149 if graph_function is None:\r\n> -> 2150 graph_function = self._create_graph_function(args, kwargs)\r\n> 2151 self._function_cache.primary[cache_key] = graph_function\r\n> 2152 return graph_function, args, kwargs\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n> 2039 arg_names=arg_names,\r\n> 2040 override_flat_arg_shapes=override_flat_arg_shapes,\r\n> -> 2041 capture_by_value=self._capture_by_value),\r\n> 2042 self._function_attributes,\r\n> 2043 # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n> 913 converted_func)\r\n> 914\r\n> --> 915 func_outputs = python_func(*func_args, **func_kwargs)\r\n> 916\r\n> 917 # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in wrapped_fn(*args, **kwds)\r\n> 356 # **wrapped** allows AutoGraph to swap in a converted function. We give\r\n> 357 # the function a weak reference to itself to avoid a reference cycle.\r\n> --> 358 return weak_wrapped_fn().**wrapped**(*args, **kwds)\r\n> 359 weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n> 360\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in distributed_function(input_iterator)\r\n> 71 strategy = distribution_strategy_context.get_strategy()\r\n> 72 outputs = strategy.experimental_run_v2(\r\n> ---> 73 per_replica_function, args=(model, x, y, sample_weights))\r\n> 74 # Out of PerReplica outputs reduce or pick values to return.\r\n> 75 all_outputs = dist_utils.unwrap_output_dict(\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)\r\n> 758 fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\r\n> 759 convert_by_default=False)\r\n> --> 760 return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n> 761\r\n> 762 def reduce(self, reduce_op, value, axis):\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\r\n> 1785 kwargs = {}\r\n> 1786 with self._container_strategy().scope():\r\n> -> 1787 return self._call_for_each_replica(fn, args, kwargs)\r\n> 1788\r\n> 1789 def _call_for_each_replica(self, fn, args, kwargs):\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)\r\n> 2130 self._container_strategy(),\r\n> 2131 replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\r\n> -> 2132 return fn(*args, **kwargs)\r\n> 2133\r\n> 2134 def _reduce_to(self, reduce_op, value, destinations):\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py in wrapper(*args, **kwargs)\r\n> 290 def wrapper(*args, **kwargs):\r\n> 291 with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n> --> 292 return func(*args, **kwargs)\r\n> 293\r\n> 294 if inspect.isfunction(func) or inspect.ismethod(func):\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)\r\n> 262 y,\r\n> 263 sample_weights=sample_weights,\r\n> --> 264 output_loss_metrics=model._output_loss_metrics)\r\n> 265\r\n> 266 if reset_metrics:\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py in train_on_batch(model, inputs, targets, sample_weights, output_loss_metrics)\r\n> 313 outs = [outs]\r\n> 314 metrics_results = _eager_metrics_fn(\r\n> --> 315 model, outs, targets, sample_weights=sample_weights, masks=masks)\r\n> 316 total_loss = nest.flatten(total_loss)\r\n> 317 return {'total_loss': total_loss,\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py in _eager_metrics_fn(model, outputs, targets, sample_weights, masks)\r\n> 72 masks=masks,\r\n> 73 return_weighted_and_unweighted_metrics=True,\r\n> ---> 74 skip_target_masks=model._prepare_skip_target_masks())\r\n> 75\r\n> 76 # Add metric results from the `add_metric` metrics.\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in _handle_metrics(self, outputs, targets, skip_target_masks, sample_weights, masks, return_weighted_metrics, return_weighted_and_unweighted_metrics)\r\n> 2061 metric_results.extend(\r\n> 2062 self._handle_per_output_metrics(self._per_output_metrics[i],\r\n> -> 2063 target, output, output_mask))\r\n> 2064 if return_weighted_and_unweighted_metrics or return_weighted_metrics:\r\n> 2065 metric_results.extend(\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in _handle_per_output_metrics(self, metrics_dict, y_true, y_pred, mask, weights)\r\n> 2012 with K.name_scope(metric_name):\r\n> 2013 metric_result = training_utils.call_metric_function(\r\n> -> 2014 metric_fn, y_true, y_pred, weights=weights, mask=mask)\r\n> 2015 metric_results.append(metric_result)\r\n> 2016 return metric_results\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py in call_metric_function(metric_fn, y_true, y_pred, weights, mask)\r\n> 1065\r\n> 1066 if y_pred is not None:\r\n> -> 1067 return metric_fn(y_true, y_pred, sample_weight=weights)\r\n> 1068 # `Mean` metric only takes a single value.\r\n> 1069 return metric_fn(y_true, sample_weight=weights)\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in **call**(self, *args, **kwargs)\r\n> 191 from tensorflow.python.keras.distribute import distributed_training_utils # pylint:disable=g-import-not-at-top\r\n> 192 return distributed_training_utils.call_replica_local_fn(\r\n> --> 193 replica_local_fn, *args, **kwargs)\r\n> 194\r\n> 195 @Property\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\distribute\\distributed_training_utils.py in call_replica_local_fn(fn, *args, **kwargs)\r\n> 1133 with strategy.scope():\r\n> 1134 return strategy.extended.call_for_each_replica(fn, args, kwargs)\r\n> -> 1135 return fn(*args, **kwargs)\r\n> 1136\r\n> 1137\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in replica_local_fn(*args, **kwargs)\r\n> 174 def replica_local_fn(*args, **kwargs):\r\n> 175 \"\"\"Updates the state of the metric in a replica-local context.\"\"\"\r\n> --> 176 update_op = self.update_state(*args, **kwargs) # pylint: disable=not-callable\r\n> 177 with ops.control_dependencies([update_op]):\r\n> 178 result_t = self.result() # pylint: disable=not-callable\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\metrics_utils.py in decorated(metric_obj, *args, **kwargs)\r\n> 73\r\n> 74 with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):\r\n> ---> 75 update_op = update_state_fn(*args, **kwargs)\r\n> 76 if update_op is not None: # update_op will be None in eager execution.\r\n> 77 metric_obj.add_update(update_op)\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in update_state(self, y_true, y_pred, sample_weight)\r\n> 579 y_pred, y_true)\r\n> 580\r\n> --> 581 matches = self._fn(y_true, y_pred, **self._fn_kwargs)\r\n> 582 return super(MeanMetricWrapper, self).update_state(\r\n> 583 matches, sample_weight=sample_weight)\r\n> \r\n> in mean_IOU(y_true, y_pred)\r\n> 1 mIOU = tf.keras.metrics.MeanIoU(num_classes=20)\r\n> 2 def mean_IOU(y_true, y_pred):\r\n> ----> 3 m = tf.keras.metrics.MeanIoU(num_classes=20)\r\n> 4 m.update_state(y_true, tf.argmax(y_pred, 3))\r\n> 5 return m.result()\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in **init**(self, num_classes, name, dtype)\r\n> 2308 shape=(num_classes, num_classes),\r\n> 2309 initializer=init_ops.zeros_initializer,\r\n> -> 2310 dtype=dtypes.float64)\r\n> 2311\r\n> 2312 def update_state(self, y_true, y_pred, sample_weight=None):\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py in add_weight(self, name, shape, aggregation, synchronization, initializer, dtype)\r\n> 271 collections=[],\r\n> 272 synchronization=synchronization,\r\n> --> 273 aggregation=aggregation)\r\n> 274\r\n> 275 ### End: For use by subclasses ###\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\r\n> 520 collections=collections_arg,\r\n> 521 synchronization=synchronization,\r\n> --> 522 aggregation=aggregation)\r\n> 523 backend.track_variable(variable)\r\n> 524\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\r\n> 742 dtype=dtype,\r\n> 743 initializer=initializer,\r\n> --> 744 **kwargs_for_getter)\r\n> 745\r\n> 746 # If we set an initializer and the variable processed it, tracking will not\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer_utils.py in make_variable(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\r\n> 137 synchronization=synchronization,\r\n> 138 aggregation=aggregation,\r\n> --> 139 shape=variable_shape if variable_shape else None)\r\n> 140\r\n> 141\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py in **call**(cls, *args, **kwargs)\r\n> 256 def **call**(cls, *args, **kwargs):\r\n> 257 if cls is VariableV1:\r\n> --> 258 return cls._variable_v1_call(*args, **kwargs)\r\n> 259 elif cls is Variable:\r\n> 260 return cls._variable_v2_call(*args, **kwargs)\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py in _variable_v1_call(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\r\n> 217 synchronization=synchronization,\r\n> 218 aggregation=aggregation,\r\n> --> 219 shape=shape)\r\n> 220\r\n> 221 def _variable_v2_call(cls,\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py in getter(**kwargs)\r\n> 63\r\n> 64 def getter(**kwargs):\r\n> ---> 65 return captured_getter(captured_previous, **kwargs)\r\n> 66\r\n> 67 return getter\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in creator(next_creator, *args, **kwargs)\r\n> 2046 def creator(next_creator, *args, **kwargs):\r\n> 2047 _require_strategy_scope_strategy(strategy)\r\n> -> 2048 return next_creator(*args, **kwargs)\r\n> 2049\r\n> 2050 self._var_creator_scope = variable_scope.variable_creator_scope(creator)\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py in getter(**kwargs)\r\n> 63\r\n> 64 def getter(**kwargs):\r\n> ---> 65 return captured_getter(captured_previous, **kwargs)\r\n> 66\r\n> 67 return getter\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in creator(next_creator, *args, **kwargs)\r\n> 2046 def creator(next_creator, *args, **kwargs):\r\n> 2047 _require_strategy_scope_strategy(strategy)\r\n> -> 2048 return next_creator(*args, **kwargs)\r\n> 2049\r\n> 2050 self._var_creator_scope = variable_scope.variable_creator_scope(creator)\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py in getter(**kwargs)\r\n> 63\r\n> 64 def getter(**kwargs):\r\n> ---> 65 return captured_getter(captured_previous, **kwargs)\r\n> 66\r\n> 67 return getter\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in creator(next_creator, *args, **kwargs)\r\n> 2046 def creator(next_creator, *args, **kwargs):\r\n> 2047 _require_strategy_scope_strategy(strategy)\r\n> -> 2048 return next_creator(*args, **kwargs)\r\n> 2049\r\n> 2050 self._var_creator_scope = variable_scope.variable_creator_scope(creator)\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py in getter(**kwargs)\r\n> 63\r\n> 64 def getter(**kwargs):\r\n> ---> 65 return captured_getter(captured_previous, **kwargs)\r\n> 66\r\n> 67 return getter\r\n> \r\n> c:\\users\\giang\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in invalid_creator_scope(*unused_args, **unused_kwds)\r\n> 411 \"\"\"Disables variable creation.\"\"\"\r\n> 412 raise ValueError(\r\n> --> 413 \"tf.function-decorated function tried to create \"\r\n> 414 \"variables on non-first call.\")\r\n> 415\r\n> \r\n> ValueError: tf.function-decorated function tried to create variables on non-first call.\r\n\r\n@hagianga21 ,\r\nThank you for the info, looks like the complete code is not given. Can you provide a standalone code to replicate the issue ?Thanks!", "I believe this is the code which reproduces the issue:\r\n```\r\nimport tensorflow as tf\r\n\r\nNUM_CLASSES = 10\r\n\r\nX = tf.random.uniform(minval=-1, maxval=1, shape=(10, 10, 3), dtype=tf.float32)\r\nlabel = tf.random.uniform(minval=0,\r\n                          maxval=NUM_CLASSES-1,\r\n                          shape=(10, 10),\r\n                          dtype=tf.int32)\r\nds = tf.data.Dataset.from_tensors((X, label)).batch(1)\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Conv2D(NUM_CLASSES, (1, 1), padding='same'),\r\n    tf.keras.layers.Activation('softmax')\r\n])\r\n\r\nloss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\r\n\r\nmodel.compile(optimizer='sgd',\r\n              loss=loss_obj,\r\n              metrics=[tf.keras.metrics.MeanIoU(NUM_CLASSES, name='mIoU')])\r\n\r\nmodel.fit(ds, epochs=1, validation_data=ds)\r\n```\r\n\r\nRunning this throws the following exception: `InvalidArgumentError:  Shapes of all inputs must match: values[0].shape = [100] != values[1].shape = [1000]`", "@hagianga21 ,\r\nAny update on the issue ? thanks!", "In the meantime this should work:\r\n\r\n```python\r\nclass MeanIoU(tf.keras.metrics.MeanIoU):\r\n    def __call__(self, y_true, y_pred, sample_weight=None):\r\n        y_pred = tf.argmax(y_pred, axis=-1)\r\n        return super().__call__(y_true, y_pred, sample_weight=sample_weight)\r\n```", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "> In the meantime this should work:\r\n> \r\n> ```python\r\n> class MeanIoU(tf.keras.metrics.MeanIoU):\r\n>     def __call__(self, y_true, y_pred, sample_weight=None):\r\n>         y_pred = tf.argmax(y_pred, axis=-1)\r\n>         return super().__call__(y_true, y_pred, sample_weight=sample_weight)\r\n> ```\r\n\r\nThis worked for TF2.1 but broke again for TF2.2. ", "This issue is still persistent in the TF 2.3 version", "> > In the meantime this should work:\r\n> > ```python\r\n> > class MeanIoU(tf.keras.metrics.MeanIoU):\r\n> >     def __call__(self, y_true, y_pred, sample_weight=None):\r\n> >         y_pred = tf.argmax(y_pred, axis=-1)\r\n> >         return super().__call__(y_true, y_pred, sample_weight=sample_weight)\r\n> > ```\r\n> \r\n> This worked for TF2.1 but broke again for TF2.2.\r\n\r\nChange __call__ to update_states will work", "how\r\n\r\n> > > In the meantime this should work:\r\n> > > ```python\r\n> > > class MeanIoU(tf.keras.metrics.MeanIoU):\r\n> > >     def __call__(self, y_true, y_pred, sample_weight=None):\r\n> > >         y_pred = tf.argmax(y_pred, axis=-1)\r\n> > >         return super().__call__(y_true, y_pred, sample_weight=sample_weight)\r\n> > > ```\r\n> > \r\n> > \r\n> > This worked for TF2.1 but broke again for TF2.2.\r\n> \r\n> Change **call** to update_states will work\r\n\r\nhow to Change?", "```\r\nclass UpdatedMeanIoU(tf.keras.metrics.MeanIoU):\r\n  def __init__(self,\r\n               y_true=None,\r\n               y_pred=None,\r\n               num_classes=None,\r\n               name=None,\r\n               dtype=None):\r\n    super(UpdatedMeanIoU, self).__init__(num_classes = num_classes,name=name, dtype=dtype)\r\n\r\n  def update_state(self, y_true, y_pred, sample_weight=None):\r\n    y_pred = tf.math.argmax(y_pred, axis=-1)\r\n    return super().update_state(y_true, y_pred, sample_weight)\r\n```", "> ```\r\n> class UpdatedMeanIoU(tf.keras.metrics.MeanIoU):\r\n>   def __init__(self,\r\n>                y_true=None,\r\n>                y_pred=None,\r\n>                num_classes=None,\r\n>                name=None,\r\n>                dtype=None):\r\n>     super(UpdatedMeanIoU, self).__init__(num_classes = num_classes,name=name, dtype=dtype)\r\n> \r\n>   def update_state(self, y_true, y_pred, sample_weight=None):\r\n>     y_pred = tf.math.argmax(y_pred, axis=-1)\r\n>     return super().update_state(y_true, y_pred, sample_weight)\r\n> ```\r\n\r\nMany thanks! It works in TF2.3.", "Doesn't work on tf 2.2; i get the following error\r\nTraceback (most recent call last):\r\n  File \"/data/krishnan/pyenv/tf-gpu/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 2726, in zeros\r\n    shape = constant_op._tensor_shape_tensor_conversion_function(\r\n  File \"/data/krishnan/pyenv/tf-gpu/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 337, in _tensor_shape_tensor_conversion_function\r\n    raise ValueError(\r\nValueError: Cannot convert a partially known TensorShape to a Tensor: (None, None)\r\n", "I've had no issues with TF2.2 , TF2 3 and even TF2.4. Check your\narchitecture implementation.\n\n\n\nOn Fri, 1 Jan 2021, 10:31 Srikrishnan V, <notifications@github.com> wrote:\n\n> Doesn't work on tf 2.2; i get the following error\n> Traceback (most recent call last):\n> File\n> \"/data/krishnan/pyenv/tf-gpu/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\",\n> line 2726, in zeros\n> shape = constant_op._tensor_shape_tensor_conversion_function(\n> File\n> \"/data/krishnan/pyenv/tf-gpu/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\",\n> line 337, in _tensor_shape_tensor_conversion_function\n> raise ValueError(\n> ValueError: Cannot convert a partially known TensorShape to a Tensor:\n> (None, None)\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32875#issuecomment-753329807>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABUET3PFGHRKFR36HTEZBXDSXXTEJANCNFSM4I3FZILQ>\n> .\n>\n", "The simple architecture and example i have taken is from keras examples:\r\nhttps://keras.io/examples/vision/oxford_pets_image_segmentation/\r\n"]}, {"number": 32874, "title": "scientific research on the evaluation of Tensorflow", "body": "Hello, everyone!\r\nWe are students from Linkoping University, Sweden. We are doing research on evaluation of Tensorflow, so we need help!\r\nThere is a questionnaire we need you to answer! Just 1-2 minutes!\r\nThank you so much for your patience and kindness!\r\n\r\nhttps://docs.google.com/forms/d/e/1FAIpQLSfvqIUxxI5mm82dr_M8Ja7y_eGG0mqXzfCLQ0c6ehjqSjAQkA/viewform?usp=sf_link", "comments": ["The issue tracker is for bug reports, and I assume your post will not be widely read here. You can try posting to discuss@tensorflow.org.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32874\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32874\">No</a>\n"]}, {"number": 32873, "title": "XLA bug w/ Keras: \"Node name contains invalid characters\"", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tensorflow-gpu==1.14.0+nv\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: RTX 2080 Ti, NVIDIA Driver 418.43\r\n\r\n**Describe the current behavior**\r\n\r\nShows the warning at the predict time:\r\n```2019-09-27 11:02:42.377419: W tensorflow/core/common_runtime/process_function_library_runtime.cc:667] Ignoring multi-device function optimization failure: Invalid argument: Node '_arg_segments_ids_input_0_1_0_arg': Node name contains invalid characters```\r\n\r\nAnd doesn't enable XLA optimizations as I can see (performance doesn't improve)\r\n\r\n**Describe the expected behavior**\r\n\r\nThere should be no problem at all.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nOccured in a real-case keras-bert based code, but reproduces in the following code:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.backend import set_session\r\nfrom tensorflow.keras.layers import Input, Embedding, Add\r\nfrom tensorflow.keras.models import Model\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\nset_session(tf.Session(config=config))\r\n\r\nseq_len = 80\r\nbatch_size=5000\r\n\r\ntokens_ids_input = Input(shape=(seq_len, ), dtype='int32', name='tokens_ids_input')\r\nsegments_ids_input = Input(shape=(seq_len, ), dtype='int32', name='segments_ids_input')\r\n\r\noutput = Add()([Embedding(input_dim=5000, output_dim=16)(tokens_ids_input), Embedding(input_dim=2, output_dim=16)(segments_ids_input)])\r\n\r\nmodel = Model(inputs=[tokens_ids_input, segments_ids_input], outputs=[output])\r\n\r\nmodel.predict([np.zeros((batch_size, seq_len)), np.zeros((batch_size, seq_len))])\r\n```", "comments": ["@nsmetanin, I executed on Colab with Tf 1.14.0. But it didn't throw any error. Please take a look at [colab gist](https://colab.sandbox.google.com/gist/gadagashwini/57dd2eae878e43da89dbc146409c59a5/untitled169.ipynb) and confirm the expected behavior. Thanks! ", "I'm able to reproduce the bug in the \"tensorflow/tensorflow:1.14.0-gpu-py3\" docker container. Google Colab has K80 GPU while I have RTX 2080 Ti, so I think this bug could be related to the 2080 GPUs and/or CUDA 10.1 only.", "@nsmetanin, Can you try with with CUDA 10.0. And,  Let us know if issue still persists. Thanks!", "That reproduces both with CUDA 10.1 and 10.0 (as in tensorflow/tensorflow:1.14.0-gpu-py3 image)\r\n\r\nUPD: But \"CUDA driver API\" version is 10.1 in both cases (as it's the host's version and I'm running the code in docker containers)", "@nsmetanin Can you try recently released `TF1.15.0rc2` and let us know whether the issuer persists there? Thanks!", "The (mostly) same error in tensorflow/tensorflow:1.15.0rc2-gpu-py3:\r\n\r\n`2019-10-04 10:25:57.285825: W tensorflow/core/common_runtime/process_function_library_runtime.cc:688] Ignoring multi-device function optimization failure: Invalid argument: Node '_arg_tokens_ids_input_0_0_0_arg': Node name contains invalid characters`\r\n\r\nThe difference is that previously the problem was with \"_arg_segments_ids_input_0_1_0_arg\" node name and now with the \"_arg_tokens_ids_input_0_0_0_arg\". This error occurs because node names can't start with underscore, but for some reason in these cases they do.", "I don't think the log you're seeing signifies a hard error since it is a `LOG(WARNING)`.  But CC @iganichev who added the warning line.\r\n\r\nAs for why XLA does not provide a speedup, have you confirmed that XLA is kicking in?  You should see a `Compiled cluster using XLA!` log if it is kicking in.\r\n\r\nNote that speedups using XLA is expected but not guaranteed.", "That error is indeed non-fatal. It is warning that some optimizations could not be performed.\r\n\r\n@sanjoy maybe some Keras folks can take a look at who produces these badly named nodes.", "Yes, you're right, I can't really tell if it disables all optimizations made or only a part of them due to this error.", "I can confirm that there is a considerable speed difference between networks where this warning shows up and networks where it doesn't. I can also see from profiling that the former case splits the network into multiple XLA clusters and the latter turns it into a single cluster. I'm still trying to find which difference causes the ghost underscore node to show up. It is not in the graph_def!", "@nsmetanin \r\nIs this still an issue", "I am seeing this issue on one of my models:\r\n> 2020-05-12 18:32:58.336604: W tensorflow/core/common_runtime/process_function_library_runtime.cc:688] Ignoring multi-device function optimization failure: Invalid argument: Node '_arg_foo_input_bar_0_4_0_arg': Node name contains invalid characters\r\n\r\nThe model was created with TF Estimator on TF 1.15 w/ Feature Columns and Keras Layers.  The model is not compiled to TensorRT.  The model is being served with Nvidia's Triton with XLA enabled.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32873\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32873\">No</a>\n", "I am seeing the same issue with my model, Did someone find a solution or can we please reopen this issue?", "I'm still seeing the issue as well. I'm not sure if it is causing any\nperformance issues, though.\n\nOn Tue, Jun 2, 2020, 1:32 AM YanDavKMS <notifications@github.com> wrote:\n\n> I am seeing the same issue with my model, Did someone find a solution or\n> can we please reopen this issue?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32873#issuecomment-637308694>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACDUX2MDMAXN3XH46YAPOWTRUSMHVANCNFSM4I3FAVLA>\n> .\n>\n", "I'm not sure as well. From what @dmaniry said there is.\r\nAlso there is the initial delay on the first inference this error appears. for me it was of about 40 seconds", "I have got the same issue, both on GTX 2080ti and V100s. Does anyone have a solution?", "Did anyone find a solution to this? I have the same issue on GTX 1660 Ti.", "I have the same problem on T4. I found it was because my Node name has an underscore \"_\" at the beggining, in function:\r\nbool IsValidNodeName(StringPiece s, bool allow_internal_ops)\r\nwhen allow_internal_ops is false, it throws this error.\r\nIn my case, the allow_internal_ops depends on tensorflow::GraphConstructorOptions which is not set when calling ConvertGraphDefToGraph.\r\nI changed the opts to allow internal ops and then the warning disappeared."]}, {"number": 32872, "title": "In batching.map_and_batch, is map_fn for each sample build by random order?", "body": "Hi, \r\nI meet the difficulty to reproduce the result with fixed random_seed.\r\ntf.set_random_seed(0) is set before building a graph, for each run (rerun the script without modifying the codes), samples (after shuffle) are feed into network by the same order. But the output of `tf.image.random_flip_left_right` in `map_fn` in `batching.map_and_batch` still produce random flip image.  **In short, same input file, random flip result.**\r\n\r\nIf I set `seed=0` in `tf.image.random_flip_left_right`, the outputs are still random. So I think the order in which the graph is build is the key. The order may not be controlled by that global random seed. \r\nAny suggestion or reference for this problem, please? I use tf1.8 with horovod. \r\nThank you.\r\n\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "@Jar7 \r\nAny update please?.Is it possible for you to try latest TF versions and let us know whether the issue persists? There were lots of performance improvements in the latest versions. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 32871, "title": "Add Pinball Loss", "body": "# Summary\r\n\r\nThe pinball loss is a very simple modification of the MAE and allows (conditional) quantile regression.\r\n\r\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.470.9161&rep=rep1&type=pdf\r\nhttps://towardsdatascience.com/deep-quantile-regression-c85481548b5a\r\n\r\nThis is rather simple to code, and would be of interest for a large community.\r\n\r\n# Related Issues\r\n\r\n# Related PR\r\n\r\n   #32473\r\n\r\n# PR Overview\r\n\r\n    [ y] This PR requires new unit tests [y/n] (make sure tests are included)\r\n    [ n] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)\r\n    [ y] This PR is backwards compatible [y/n]\r\n    [ n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)\r\n", "comments": ["As the loss is also often use as a metric  (to quantify empirically the conditional  quality), and is registered in the API as a metric, should I add some tests in the metric module?", "Thank you for the PR! Can we add this to the losses in the add-ons repository and based on how widely it is being used we can may be move it to the core module later. "]}, {"number": 32870, "title": "No gradient defined for operation 'ExtractVolumePatches'", "body": "I saw it already been solved for images but it still hasn't been solved for 3D volumes.\r\n\r\nThe ExtractImagePatches solution:\r\nhttps://github.com/tensorflow/tensorflow/issues/2921\r\n\r\nI'm using tf 1.12 and get the following message:\r\nLookupError: No gradient defined for operation 'gpu_0/local_z_3d/EXTRACT_LOCAL_Z' (op type: ExtractVolumePatches)\r\n", "comments": ["@dahliau ,\r\nCan you please provide standalone code to reproduce the error mentioned here ?Thanks", "I've checked it again now in a simple standalone code and it had gradients. \r\nI'll try figure out later the problem in my original code.\r\n\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nDTYPE = tf.float32\r\ndef _weight_variable(name, shape):\r\n    return tf.get_variable(name, shape, DTYPE, tf.truncated_normal_initializer(stddev=0.1))\r\n\r\nx = tf.placeholder(dtype=tf.float32, shape=(4, 16,16,16,3))\r\ny_true = tf.placeholder(dtype=tf.float32, shape=(4, 1))\r\n\r\nin_filters=3\r\nout_filters=16\r\nkernel = _weight_variable('weights1', [3, 3, 3, in_filters, out_filters])\r\n\r\nnet = tf.nn.conv3d(x,kernel,[1,1,1,1,1],padding='SAME')\r\nnet = tf.extract_volume_patches(net,[1,3,3,3,1],[1,1,1,1,1],padding='SAME')\r\nprint(net)\r\n\r\nin_filters=net.shape[-1]\r\nout_filters=1\r\nkernel = _weight_variable('weights2', [3, 3, 3, in_filters, out_filters])\r\nnet = tf.nn.conv3d(net,kernel,[1,1,1,1,1],padding='SAME')\r\nprint(net)\r\nnet = tf.reshape(net[:,4,4,4,0],[4,-1,1])\r\nnet = tf.reduce_mean(net,1)\r\nprint(net)\r\ny_pred = net\r\n\r\nloss = tf.reduce_mean(tf.square(y_true-y_pred))\r\noptimizer = tf.train.GradientDescentOptimizer(0.01)\r\ntrain = optimizer.minimize(loss)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\n\r\nx_values = np.ones([4,16,16,16,3])\r\n\r\ny_values = np.array([[-1], [-1], [-1], [-1]])\r\nwith tf.Session() as sess:\r\n  sess.run(init)\r\n  for i in range(10):\r\n    print(i)\r\n    _, loss_value = sess.run((train, loss),\r\n                             feed_dict={x: x_values, y_true: y_values})\r\n    print(loss_value)\r\n\r\n\r\n  preds = sess.run(y_pred,\r\n                  feed_dict={x: x_values})\r\n  print(np.mean(preds))\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32870\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32870\">No</a>\n", "Have you solved this issue? I also have this problem. Thanks", "It look like it works on google colab (tf 1.14). \r\nI guess it depends on the tf version. I still haven't solve it in my work (tf 1.12).\r\n\r\nYou can check the stand alone code I've uploaded, it works on tf 1.14.", "Thanks\n\nGet Outlook for iOS<https://aka.ms/o0ukef>\n________________________________\nFrom: dahliau <notifications@github.com>\nSent: Friday, November 8, 2019 9:43:15 AM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Chen, Xiaoyang <xychen@email.unc.edu>; Comment <comment@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] No gradient defined for operation 'ExtractVolumePatches' (#32870)\n\n\nIt look like it works on google colab (tf 1.14).\nI guess it depends on the tf version. I still haven't solve it in my work (tf 1.12).\n\nYou can check the stand alone code I've uploaded, it works on tf 1.14.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/32870?email_source=notifications&email_token=AHVAPZZJLSLNDCJNERERQFDQSV3IHA5CNFSM4I3ED2LKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDSJLMQ#issuecomment-551851442>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AHVAPZ4IC5WI45FREYGRXFLQSV3IHANCNFSM4I3ED2LA>.\n"]}, {"number": 32869, "title": "sparse_categorical_crossentropy does not use underlying logits of tf.keras.layers.Softmax", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0-rc0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nAs per the comment in `sparse_categorical_crossentropy`, the function will use the underlying logits for better numerical stability if the last op is a softmax: https://github.com/tensorflow/tensorflow/blob/1ae01a07f5e4b2911218d5fc3419b3eaed48b75b/tensorflow/python/keras/backend.py#L4522-L4533\r\n\r\nBut since keras adds an identity op to the output of every layer, the last node of a layer will always be `Identity` so the condition in the above if statement will never fail for keras models and the underlying logits are never used: https://github.com/tensorflow/tensorflow/blob/b0aa37c3fdff00b5c69bce11c716c3e56f656dd4/tensorflow/python/keras/engine/base_layer.py#L851-L853\r\n\r\nUsing non-keras-layers like `tf.nn.softmax` will work, however.\r\n\r\n**Describe the expected behavior**\r\nInstead, `sparse_categorical_crossentropy` should check the first non-`Identity` node when determining whether the last node is a softmax node. I.e. it should first walk the chain of identity nodes.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.keras.layers.Input(shape=[10])\r\n\r\nsoftmax_keras = tf.keras.layers.Softmax()(x)\r\nsoftmax_tf = tf.nn.softmax(x)\r\n\r\ny_true = tf.keras.layers.Input(shape=[])\r\nloss_keras = tf.keras.losses.sparse_categorical_crossentropy(y_true, softmax_keras)\r\nloss_tf = tf.keras.losses.sparse_categorical_crossentropy(y_true, softmax_tf)\r\n\r\ndef transitive_inputs(tensor):\r\n    return set([tensor.op.type] + [inp for tensor in tensor.op.inputs for inp in transitive_inputs(tensor)])\r\n\r\nprint('Softmax' in transitive_inputs(loss_keras)) # prints \"True\"\r\nprint('Softmax' in transitive_inputs(loss_tf)) # prints \"False\"\r\n```", "comments": ["~~This seems to have an absolutely HUGE impact on my models. I *really* recommend anyone using `tf.keras.layers.Softmax` to use `tf.nn.softmax` instead until this issue is fixed.~~\r\n\r\n~~Edit: do be warned that using `tf.nn.softmax` directly will not propagate any masks you might have. You can hack masks back like this:~~\r\n\r\n\r\n```\r\n# Don't do this\r\noutput = tf.nn.softmax(x)\r\noutput._keras_mask = x._keras_mask\r\n```\r\n\r\nEdit: Do **not** do the above. It seems that this puts tensorflow with a weird state wrt. propagating masks where the mask is propagated during graph construction, but *not* during model fitting. This means that you might see a improvement in your models because it tries to predict masked outputs instead of real outputs!", "Tried replicating with given code for TF-2.0rc2, the same error was received. Kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/78bd4585819aa58fbc13155a1ab3871d/32869.ipynb) of colab.Thanks!", "@mpdn I think this was resolved in `tf-nightly`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/d30cfe2f0b6a5c3f6b72227378fbdad9/32869.ipynb).\r\n\r\nI am closing this issue as it was resolved. Please feel free to reopen if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32869\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32869\">No</a>\n"]}, {"number": 32868, "title": "Combo TPU/TFRecords for model.evaluate is not working", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: TPU Colab\r\n\r\n**Describe the current behavior**\r\nOn a TPU (colab) running model.evaluate on a tf.data.Dataset build with TFRecords throw :\r\nCompilation failure: Dynamic Spatial reduce window is not supported: %reduce-window.21 = f32[1,127,127,3]{3,2,1,0} reduce-window(f32[1,256,256,3]{3,2,1,0} %reshape.12, f32[] %constant.16), window={size=1x3x3x1 stride=1x2x2x1}, to_apply=%max_F32.17, metadata={op_type=\"MaxPool\" op_name=\"max_pooling2d_4/MaxPool\"}\r\n\tTPU compilation failed\r\n\r\nThe fit method work perfectly with the same dataset.\r\nEvaluation working perfectly if i rebuild the model and load the weights on a CPU/GPU instance.\r\n\r\nI don't have this issue on TPU if the tf.data.Dataset is not built from TFRecords\r\n\r\n**Describe the expected behavior**\r\nmodel.evaluate should work and provide a result close from the last fit iteration\r\n\r\n**Code to reproduce the issue**\r\n\r\nPut it on Colab and replace GOOGLE_BUCKET_TO_DEFINE by a real bucket (2 occurrences)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n#get a image as input data for model : tensorflow logo\r\n!curl https://avatars0.githubusercontent.com/u/15658638?s=256 --output tensor_logo.png\r\n\r\n#build 8 tfrecords based on the logo downloaded\r\ndef build_tf_records():\r\n  def serialize_example_pyfunction(image, label):\r\n    feature = {\r\n        'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.numpy()])),\r\n        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label.numpy()]))\r\n    }\r\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\r\n    return example_proto.SerializeToString()\r\n\r\n  def tf_serialize_example(image, label):\r\n    tf_string = tf.py_function(\r\n      serialize_example_pyfunction,\r\n      (image, label),  # pass these args to the above function.\r\n      tf.string)      # the return type is `tf.string`.\r\n    return tf.reshape(tf_string, ())# The result is a scalar\r\n\r\n  dataset = tf.data.Dataset.from_tensor_slices(['/content/tensor_logo.png' for x in range(8)])\r\n  dataset = dataset.map(lambda x : (tf.read_file(x), 0))\r\n  dataset = dataset.map(tf_serialize_example)\r\n  return dataset\r\n\r\n#write tf records to disc and move on a google bucket\r\nwith tf.Session() as sess:\r\n  filename = 'tfrecord.test'\r\n  writer = tf.data.experimental.TFRecordWriter(filename)\r\n  writting = writer.write(build_tf_records())\r\n  sess.run(writting)\r\n\r\n!ls\r\n\r\nfrom google.colab import auth\r\nauth.authenticate_user()\r\n\r\n!gsutil cp tfrecord.test gs://GOOGLE_BUCKET_TO_DEFINE/\r\n\r\n# the aim is to quickly build 8 items (x, y=0) with x of shape (256, 256, 3)\r\ndef train_input_fn_dummy_data():\r\n  dataset = tf.data.Dataset.from_tensor_slices(([0 for x in range(8)]))\r\n  dataset = dataset.map(lambda x : (tf.random.normal((256, 256, 3)), [0]))\r\n  dataset = dataset.batch(8)\r\n  return dataset\r\n  \r\n\r\ndef train_input_fn_from_tf_records():\r\n  # Create a description of the features.\r\n  feature_description = {\r\n    'image': tf.FixedLenFeature([], tf.string),\r\n    'label': tf.FixedLenFeature([], tf.int64, default_value=0)\r\n  }\r\n\r\n  def _parse_function(example_proto):\r\n    # Parse the input tf.Example proto using the dictionary above.\r\n    return tf.parse_single_example(example_proto, feature_description)\r\n\r\n  def _process_string_image(dic):\r\n    image_string = dic['image']\r\n    image_decoded = tf.image.decode_png(image_string, channels=3)\r\n    image_decoded = tf.cast(image_decoded, tf.float32)/255.\r\n    return image_decoded, tf.cast([dic['label']], tf.int32)\r\n\r\n  list_files = tf.data.Dataset.list_files('gs://GOOGLE_BUCKET_TO_DEFINE/tfrecord.test')\r\n  raw_tfrecords = tf.data.TFRecordDataset(list_files)\r\n  files_as_dict = raw_tfrecords.map(_parse_function)\r\n  files = files_as_dict.map(_process_string_image)\r\n  files = files.batch(8, drop_remainder=True)\r\n  \r\n  return files\r\n\r\n#basic check to compare train_input_fn_dummy_data, train_input_fn_from_tf_records\r\n#can't be run after TPU initialisation\r\nwith tf.Session() as sess:\r\n  batch = train_input_fn_dummy_data().make_one_shot_iterator().get_next()\r\n  while True:\r\n      try:\r\n          records = sess.run(batch)\r\n          print('shape of dummy items :', records[0].shape, records[1].shape)\r\n      except tf.errors.OutOfRangeError: break\r\n  batch = train_input_fn_from_tf_records().make_one_shot_iterator().get_next()\r\n  while True:\r\n      try:\r\n          records = sess.run(batch)\r\n          print('shape of tfrecords items :', records[0].shape, records[1].shape)\r\n      except tf.errors.OutOfRangeError: break\r\n\r\n##shape of dummy items : (8, 256, 256, 3) (8, 1)\r\n##shape of tfrecords items : (8, 256, 256, 3) (8, 1)\r\n\r\n\r\n#initialize tpu only once\r\nif not('strategy' in globals()):\r\n  resolver = tf.contrib.cluster_resolver.TPUClusterResolver()\r\n  tf.contrib.distribute.initialize_tpu_system(resolver)\r\n  strategy = tf.contrib.distribute.TPUStrategy(resolver)\r\n\r\n#build model and compile\r\nwith strategy.scope():\r\n  inputs = tf.keras.layers.Input(shape=(256, 256, 3))\r\n  x = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(inputs)\r\n  output = tf.keras.layers.GlobalAveragePooling2D()(x)\r\n  output = tf.keras.layers.Dense(1, activation = 'sigmoid')(output)\r\n  model = tf.keras.Model(inputs=inputs, outputs=output)\r\n  model.compile('adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\r\n\r\nmodel.summary()\r\n\r\n# fit the model, no issue\r\nprint('train dummy')\r\nmodel.fit(train_input_fn_dummy_data(), epochs= 1, steps_per_epoch=1)\r\nprint('train tfrecords')\r\nmodel.fit(train_input_fn_from_tf_records(), epochs= 1, steps_per_epoch=1)\r\nprint('evaluate dummy')\r\nmodel.evaluate(train_input_fn_dummy_data(), steps=1)\r\nprint('evaluate tfrecords')\r\nmodel.evaluate(train_input_fn_from_tf_records(), steps=1)\r\n\r\n##train dummy\r\n##WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. ##Please invoke `shuffle()` on input dataset.\r\n##WARNING:tensorflow:From /usr/local/lib/python3.6/dist-##packages/tensorflow/python/keras/engine/training_distributed.py:411: Variable.load (from ##tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n##Instructions for updating:\r\n##Prefer Variable.assign which has equivalent behavior in 2.X.\r\n##1/1 [==============================] - 0s 397ms/step - loss: 0.3180 - ##binary_accuracy: 1.0000\r\n##train tfrecords\r\n##1/1 [==============================] - 1s 786ms/step - loss: 0.4993 - ##binary_accuracy: 1.0000\r\n##evaluate dummy\r\n##1/1 [==============================] - 1s 1s/step\r\n##1/1 [==============================] - 1s 1s/step\r\n##evaluate tfrecords\r\n##---------------------------------------------------------------------------\r\n##UnimplementedError                        Traceback (most recent call last)\r\n##/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, ##fn, *args)\r\n##   1355     try:\r\n##-> 1356       return fn(*args)\r\n##   1357     except errors.OpError as e:\r\n##\r\n##10 frames\r\n##UnimplementedError: From /job:worker/replica:0/task:0:\r\n##Compilation failure: Dynamic Spatial reduce window is not supported: %reduce-window.21 ##= f32[1,127,127,3]{3,2,1,0} reduce-window(f32[1,256,256,3]{3,2,1,0} %reshape.12, f32[] ##%constant.16), window={size=1x3x3x1 stride=1x2x2x1}, to_apply=%max_F32.17, ##metadata=##{op_type=\"MaxPool\" op_name=\"max_pooling2d_4/MaxPool\"}\r\n##\tTPU compilation failed\r\n##\t [[{{node TPUReplicateMetadata_3}}]]\r\n##\r\n##During handling of the above exception, another exception occurred:\r\n\r\n\r\n```\r\n**Other info / logs**\r\n\r\nThe fit method work perfectly with the same dataset.\r\nEvaluation working perfectly if i rebuild the model and load the weights on a CPU/GPU instance.\r\n\r\nI don't have this issue on TPU if the tf.data.Dataset is not built from TFRecords\r\n\r\nFull StackTrace : \r\nUnimplementedError                        Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1355     try:\r\n-> 1356       return fn(*args)\r\n   1357     except errors.OpError as e:\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1340       return self._call_tf_sessionrun(\r\n-> 1341           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1342 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1428         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1429         run_metadata)\r\n   1430 \r\n\r\nUnimplementedError: From /job:worker/replica:0/task:0:\r\nCompilation failure: Dynamic Spatial reduce window is not supported: %reduce-window.21 = f32[1,127,127,3]{3,2,1,0} reduce-window(f32[1,256,256,3]{3,2,1,0} %reshape.12, f32[] %constant.16), window={size=1x3x3x1 stride=1x2x2x1}, to_apply=%max_F32.17, metadata={op_type=\"MaxPool\" op_name=\"max_pooling2d_4/MaxPool\"}\r\n\tTPU compilation failed\r\n\t [[{{node TPUReplicateMetadata_3}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nUnimplementedError                        Traceback (most recent call last)\r\n<ipython-input-8-22e1b9ed9dfe> in <module>()\r\n      6 model.evaluate(train_input_fn_dummy_data(), steps=1)\r\n      7 print('evaluate tfrecords')\r\n----> 8 model.evaluate(train_input_fn_from_tf_records(), steps=1)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in evaluate(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n    902             sample_weight=sample_weight,\r\n    903             steps=steps,\r\n--> 904             callbacks=callbacks)\r\n    905 \r\n    906     batch_size = self._validate_or_infer_batch_size(batch_size, steps, x)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py in evaluate_distributed(model, x, y, batch_size, verbose, sample_weight, steps, callbacks)\r\n    168   if distributed_training_utils.is_tpu_strategy(model._distribution_strategy):\r\n    169     return experimental_tpu_test_loop(\r\n--> 170         model, dataset, verbose=verbose, steps=steps, callbacks=callbacks)\r\n    171   else:\r\n    172     return training_arrays.test_loop(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py in experimental_tpu_test_loop(model, dataset, verbose, steps, callbacks)\r\n    562     callbacks._call_batch_hook(mode, 'begin', current_step, batch_logs)\r\n    563     try:\r\n--> 564       _, batch_outs = K.batch_get_value([test_op, output_tensors])\r\n    565     except errors.OutOfRangeError:\r\n    566       warning_msg = 'Make sure that your dataset can generate at least '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in batch_get_value(tensors)\r\n   3008     raise RuntimeError('Cannot get value inside Tensorflow graph function.')\r\n   3009   if tensors:\r\n-> 3010     return get_session(tensors).run(tensors)\r\n   3011   else:\r\n   3012     return []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    948     try:\r\n    949       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 950                          run_metadata_ptr)\r\n    951       if run_metadata:\r\n    952         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1171     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1172       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1173                              feed_dict_tensor, options, run_metadata)\r\n   1174     else:\r\n   1175       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1348     if handle is None:\r\n   1349       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1350                            run_metadata)\r\n   1351     else:\r\n   1352       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1368           pass\r\n   1369       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1370       raise type(e)(node_def, op, message)\r\n   1371 \r\n   1372   def _extend_graph(self):\r\n\r\nUnimplementedError: From /job:worker/replica:0/task:0:\r\nCompilation failure: Dynamic Spatial reduce window is not supported: %reduce-window.21 = f32[1,127,127,3]{3,2,1,0} reduce-window(f32[1,256,256,3]{3,2,1,0} %reshape.12, f32[] %constant.16), window={size=1x3x3x1 stride=1x2x2x1}, to_apply=%max_F32.17, metadata={op_type=\"MaxPool\" op_name=\"max_pooling2d_4/MaxPool\"}\r\n\tTPU compilation failed\r\n\t [[node TPUReplicateMetadata_3 (defined at <ipython-input-8-22e1b9ed9dfe>:8) ]]\r\n\r\nOriginal stack trace for 'TPUReplicateMetadata_3':\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 832, in start\r\n    self._run_callback(self._callbacks.popleft())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 605, in _run_callback\r\n    ret = callback()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\r\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-8-22e1b9ed9dfe>\", line 8, in <module>\r\n    model.evaluate(train_input_fn_from_tf_records(), steps=1)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 904, in evaluate\r\n    callbacks=callbacks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 170, in evaluate_distributed\r\n    model, dataset, verbose=verbose, steps=steps, callbacks=callbacks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 520, in experimental_tpu_test_loop\r\n    _test_step_fn, args=(test_input_data,))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py\", line 249, in experimental_run_v2\r\n    return _tpu_run(self, fn, args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py\", line 196, in _tpu_run\r\n    maximum_shapes=maximum_shapes)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py\", line 592, in replicate\r\n    maximum_shapes=maximum_shapes)[1]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py\", line 854, in split_compile_and_replicate\r\n    num_replicas=num_replicas, use_tpu=use_tpu, **metadata_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_tpu_ops.py\", line 6039, in tpu_replicate_metadata\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()", "comments": ["Could able to reproduce the issue on colab with TF 1.14.Please see the [gist here](https://colab.sandbox.google.com/gist/gadagashwini/b9ce05d1ae96ef8bc73e0124a674ea42/untitled168.ipynb). Thanks!", "Hello,\r\n\r\nIn your attempt to reproduce the issue, you used google drive but TFRecords/TPU are not compatible with google drive or local disk. A google bucket from google cloud service is mandatary.\r\n\r\nThen you will be able to have the error i got, currently, you have : \r\nUnimplementedError: From /job:worker/replica:0/task:0:\r\n**File system scheme '[local]' not implemented** (file: '/content/drive/My Drive/tfrecord.test')\r\n\t [[{{node list_files_3/MatchingFiles}}]]\r\n\r\nIndeed, TPU don't accept (yet) local source file.\r\n\r\nThe error i raised is also an UnimplementedError but with another root cause.", "@anhmeow Can you try `TF1.15.0rc1` and let us know if the issue persists? Thanks!", "@jvishnuvardhan I've added this line before executing my script :\r\n```\r\n!pip install tensorflow==1.15.0rc1\r\n```\r\n\r\nthe result is worst than in 1.14 as ithe first 'fit call' is failing now : \r\n```\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n11 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1347       # Ensure any changes to the graph are reflected in the runtime.\r\n-> 1348       self._extend_graph()\r\n   1349       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _extend_graph(self)\r\n   1387     with self._graph._session_run_lock():  # pylint: disable=protected-access\r\n-> 1388       tf_session.ExtendSession(self._session)\r\n   1389 \r\n\r\nNotFoundError: Op type not registered 'DatasetToTFRecord' in binary running on n-2aac8f7d-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-8-22e1b9ed9dfe> in <module>()\r\n      1 print('train dummy')\r\n----> 2 model.fit(train_input_fn_dummy_data(), epochs= 1, steps_per_epoch=1)\r\n      3 print('train tfrecords')\r\n      4 model.fit(train_input_fn_from_tf_records(), epochs= 1, steps_per_epoch=1)\r\n      5 print('evaluate dummy')\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    725         max_queue_size=max_queue_size,\r\n    726         workers=workers,\r\n--> 727         use_multiprocessing=use_multiprocessing)\r\n    728 \r\n    729   def evaluate(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    617         validation_split=validation_split,\r\n    618         shuffle=shuffle,\r\n--> 619         epochs=epochs)\r\n    620     if not dist_utils.is_distributing_by_cloning(model):\r\n    621       with model._distribution_strategy.scope():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _distribution_standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, validation_split, shuffle, epochs, allow_partial_batch)\r\n   2270         session = None\r\n   2271       else:\r\n-> 2272         session = K.get_session()\r\n   2273 \r\n   2274       first_x_value = nest.flatten(x)[0]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py in get_session(op_input_list)\r\n    484   if not _MANUAL_VAR_INIT:\r\n    485     with session.graph.as_default():\r\n--> 486       _initialize_variables(session)\r\n    487   return session\r\n    488 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py in _initialize_variables(session)\r\n    901     # marked as initialized.\r\n    902     is_initialized = session.run(\r\n--> 903         [variables_module.is_variable_initialized(v) for v in candidate_vars])\r\n    904     uninitialized_vars = []\r\n    905     for flag, v in zip(is_initialized, candidate_vars):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    954     try:\r\n    955       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 956                          run_metadata_ptr)\r\n    957       if run_metadata:\r\n    958         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1178     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1179       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1180                              feed_dict_tensor, options, run_metadata)\r\n   1181     else:\r\n   1182       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1357     if handle is None:\r\n   1358       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1359                            run_metadata)\r\n   1360     else:\r\n   1361       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nNotFoundError: Op type not registered 'DatasetToTFRecord' in binary running on n-2aac8f7d-w-0. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n```\r\n\r\n", "The TPU version must match the version of TensorFlow you are installing with `pip install`. Are you creating a TF with --version=1.15?", "@frankchn my initial problem is with TF 1.14 on collab which is in line with TPU, am i right?\r\ni think testing on 1.15 is finally worthless as long as TPU is expecting TF 1.14, do you agree or am i missing something?", "Yeah, in general you need to match the TF versions between the TPU and the Colab/VM instance that you are using. We don't support TF1.15 on Colab and 1.14 on the TPU (or any other combination).\r\n\r\nComing back to your original question, it looks like the XLA compiler doesn't support this feature right now: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/dynamic_dimension_inference.cc#L712\r\n\r\n@jvishnuvardhan Can you assign this to someone on the XLA team to see if they know more?", "This should have been fixed in our internal branch. @rxsang is there a way to disable dynamic shape for this case?", "You can call \"dataset = dataset.batch(8, drop_remainder=True)\" to disable the dynamic shape here. ", "@yunxing @rxsang In the code i provided to reproduce the issue, i've already put \r\n```\r\nfiles = files.batch(8, drop_remainder=True)\r\n```\r\nUnfortunately, it doesn't work for the evaluate method with TFRecords", "Facing a similar issue here. In tf1.15 on calling fit, the training step runs smoothly in the first epoch then breaks at the validation step. fit without validation runs smoothly all the way.\r\nIn tf2.2, the error pops up in fit method, with or without validation.", "What's the error popping up? I guess that may be a different issue than the previous comment, I'd like to know more details. Thanks!", "So it's a conv-net, with the batch size set to 128 and the drop_remainder argument set to True. The error I get is: \r\nCompilation failure: Dynamic Spatial Convolution is not supported: lhs shape is f32[16,<=262,<=262,3]\r\n\r\nIt's the same error in both the evaluate step in tf1.5 and train step in tf2.2. Since the last 3 dimensions are definitely okay, my intuition is that it's something to do with the dataset distribution among the tpu cores, but I don't know how to fix it without writing a custom training loop.", "It seems you get this error because your image size is dynamic (from the error message, the dimension which has an upper bound of 262), which we don't support it yet). Can you share your input pipeline code? If it is not intended that you have dynamic image size, maybe some dataset ops produce dynamic image size in your case.", "Dynamic image size is the next thing we are going to support. For now\nmaking images static will make the test pass.\n\nOn Tue, Apr 21, 2020, 6:27 PM rxsang <notifications@github.com> wrote:\n\n> It seems you get this error because your image size is dynamic (from the\n> error message, the dimension which has an upper bound of 262), which we\n> don't support it yet). Can you share your input pipeline code? If it is not\n> intended that you have dynamic image size, maybe some dataset ops produce\n> dynamic image size in your case.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32868#issuecomment-617469873>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAHYOWIQPHCPT4N546TAYELRNZBWRANCNFSM4I3DV4KQ>\n> .\n>\n", "That worked. There was a `tf.image.decode_jpeg` in the pipeline, so I added a `tf.reshape` after it. Gracias.\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32868\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32868\">No</a>\n"]}, {"number": 32867, "title": "Added CMSIS-NN specialization for fully connected op.", "body": "The optimized implementation requires temporary buffer space,\r\ntherefore a buffer was defined. The buffer is however a\r\ntemporary solution until there's a way to add this buffer to\r\nthe memory planner in the prepare phase of the op.\r\n\r\nChange-Id: I4faf9010b723e3ced6ef1a1af94f2a524df64c61", "comments": ["@wangtz , let me know what you think", "Can someone please re-run this tests. They should not be affected by this change.", "Hi! The \"Ubuntu CC\" seems broke. For all PR's, it seem to be in \"Expected \u2014 Waiting for status to be reported\" state."]}, {"number": 32866, "title": "The use of tflite Model of C3D Network in Android ", "body": "I use a model of C3D network training to convert to a tflite format; \r\nuse in the android project available at:\r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android\r\nThe following question arises: is android not currently supported Con3D? 5D data?\r\n\r\nE/tensorflow: CameraActivity: Exception!\r\n    java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/strided_slice.cc StridedSlice op only supports 1D-4D input arrays.\r\n    Node number 0 (STRIDED_SLICE) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:96)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:61)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)\r\n        at org.tensorflow.lite.examples.classification.tflite.Classifier.<init>(Classifier.java:189)\r\n        at org.tensorflow.lite.examples.classification.tflite.ClassifierFloatMobileNet.<init>(ClassifierFloatMobileNet.java:41)\r\n        at org.tensorflow.lite.examples.classification.tflite.Classifier.create(Classifier.java:100)\r\n        at org.tensorflow.lite.examples.classification.ClassifierActivity.recreateClassifier(ClassifierActivity.java:168)\r\n        at org.tensorflow.lite.examples.classification.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:71)\r\n        at org.tensorflow.lite.examples.classification.CameraActivity.onPreviewFrame(CameraActivity.java:232)\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1261)\r\n        at android.os.Handler.dispatchMessage(Handler.java:110)\r\n        at android.os.Looper.loop(Looper.java:203)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6406)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1113)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:974)\r\n\r\n", "comments": ["Many of our kernels are optimized for 4D cases, and may not support 5D. Quite frequently we see 5D models that really don't need 5D, and can be reduced to 4D when authoring. Can you provide a link to your model?", "@jdduke\r\nThank you for your reply. The download link for the FLOAT format of the tflite model is as follows\uff1a\r\nhttps://pan.baidu.com/s/1n4Gqg56PyO0tp1EjmmsruQ\r\n\r\n", "@jdduke I am also interested in using strided slice on the 5D tensors from Conv3D. TF Lite is able to convert models successfully but the interpreter fails to allocate tensors.\r\n```\r\n  File \"/Users/tkhot/anaconda3/envs/tf-nightly/lib/python3.6/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 106, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/strided_slice.cc StridedSlice op only supports 1D-4D input arrays.Node number 5 (STRIDED_SLICE) failed to prepare.\r\n```\r\nIs there a workaround for this?\r\n\r\n", "Hi all,\r\n\r\nsorry for the late response. (I just came back from vacation)\r\n\r\nCurrently 5-D strided_slice is not supported. conv_3d is not supported as well. :(\r\n\r\nWe would love to know your use cases as well: if conv 3d is necessary, maybe we can find some workaround methods.\r\n\r\ncheers,", "Thank you very much, when will you support the conv_3d operation?", "Hi,\r\n\r\nWonder if it's possible to share your usage and code snippet, so we can evaluate if c3d is necessary? maybe it's possible to avoid c3d? thanks!", "I use 3D convolution to solve the classification problem of continuous gestures, such as sliding to the left or sliding to the right; I refer to the following project to train the model\uff1a\r\nhttps://github.com/hx173149/C3D-tensorflow\r\nNow, I am trying to use the lstm method. Do you have any better suggestions?", "Hi,\n\nGreat. For LSTM support, you can refer here\n<https://www.tensorflow.org/lite/convert/rnn>.\n\nThanks,\n\nOn Tue, Oct 29, 2019 at 10:20 AM zxj11838 <notifications@github.com> wrote:\n\n> I use 3D convolution to solve the classification problem of continuous\n> gestures, such as sliding to the left or sliding to the right; I refer to\n> the following project to train the model\uff1a\n> https://github.com/hx173149/C3D-tensorflow\n> Now, I am trying to use the lstm method. Do you have any better\n> suggestions?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32866?email_source=notifications&email_token=AIURNGIGASRXWF2S3MMOMRTQQ6MYDA5CNFSM4I3DOHU2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECO7VFA#issuecomment-547224212>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIURNGIBLNXM23JVVGHXSHTQQ6MYDANCNFSM4I3DOHUQ>\n> .\n>\n\n\n-- \nRenjie Liu\n\nrenjieliu@google.com\n+1 (650) 253-4359\n", "Thai recently made a change to support > 4d dimension, can you retry with the selected ops?\r\n\r\nThanks", "@zxj11838  Con3D is not yet supported but StridedSlice is extended to 5D.", "@zxj11838 Could you please let us know if it  is still an issue in TF v2.6.0 ? Please refer to example [link](https://www.tensorflow.org/lite/examples) and let us know if it helps?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32866\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32866\">No</a>\n"]}, {"number": 32865, "title": "fit_generator validation steps running on CPU?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Nvidia L4T 32.2.1 (based on Ubuntu 18.04) running on Jetson Nano\r\n- TensorFlow installed from (source or binary): binary from Nvidia Jetson repo.\r\n- TensorFlow version (use command below): unknown 1.14.0 (Also seen on 1.13.0)\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: CUDA 10.0.326, cuDNN 7.5.0.56-1+cuda10.0\r\n- GPU model and memory: NVIDIA Jetson NANO/TX1, 4Gb (shared)\r\n\r\n**Describe the current behavior**\r\n\r\nTraining steps use 100% of GPU as reported by tegrastats; validation steps use 0% of GPU and 350% CPU. 3.4Gb of memory is used and 338k is free. \r\n\r\n**Describe the expected behavior**\r\n\r\nValidation steps should also be run on GPU.\r\n\r\n**Code to reproduce the issue**\r\n\r\nFull code at  https://github.com/simoncozens/atokern/blob/master/badkerndetector.py and https://github.com/simoncozens/atokern/blob/master/nntools.py . Relevant portion:\r\n\r\n```\r\n    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=output_dir+'/output/kernmodel-cp-val.hdf5', verbose=0, save_best_only=True, monitor=\"val_loss\")\r\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=lr_decay, patience=20, verbose=1, mode='auto', min_delta=1e-6, cooldown=100, min_lr=0)\r\n    tensorboard = tf.keras.callbacks.TensorBoard(log_dir=output_dir+\"/output\", histogram_freq=1,\r\n    write_graph=False, write_grads=False,batch_size=self.batch_size,update_freq='batch',\r\n    write_images=False,profile_batch=0 )\r\n\r\n    callback_list = [\r\n      checkpointer,\r\n      reduce_lr,\r\n      tensorboard,\r\n    ]\r\n\r\n    history = self.model.fit_generator(\r\n      generator = self.generator,\r\n      validation_data = self.validation_generator,\r\n      steps_per_epoch = steps_per_epoch,\r\n      validation_steps = validation_steps,\r\n      epochs=epochs, verbose=1, callbacks=callback_list,\r\n      max_queue_size=30\r\n    )\r\n```\r\n\r\n**Other info / logs**\r\n\r\nMy suspicion is that this is memory allocation related, as I've had to reduce the batch size - with larger batch sizes, the training will run all training steps and then OOM when it enters the validation stage.\r\n\r\nIs it possible that the validation step can't allocate something on the GPU and so uses CPU instead?", "comments": ["@simoncozens, Will it be possible to provide the simple standalone code to replicate the reported issue here. Thanks!", "@simoncozens, Provide us the standalone code snippet. It will indeed help us to move faster.", "Sure. I am trying to narrow it down and replicate on standard models/datasets, and will let you know when I manage it.", "OK, I think this is actually not a TF bug, but my problem with tuning the generator queues: the CPU was spending more time making data than the GPU spent consuming it. I don't know why this only happened on validation, but I think the training was actually running on GPU."]}, {"number": 32864, "title": "BatchNormalization update ops with gradient tape", "body": "how to update moving mean and moving var in each BatchNormalization layer in gradient tape mode?", "comments": ["solved:\r\n\r\nbatch normalization layer should be called with argument *training=True*\r\n\r\nso moving vars are propely updated\r\n\r\n```\r\n>>> model.layers[2].weights[2]\r\n<tf.Variable 'batch_normalization/moving_mean:0' shape=(3,) dtype=float32, numpy\r\n=array([-0.00062087,  0.00015137, -0.00013239], dtype=float32)>\r\n```"]}, {"number": 32863, "title": "save and recover problem in TensorFlow", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04.2\r\n- TensorFlow version:1.14.0\r\n- Python version:3.6.9\r\n- Installed using virtualenv? pip? conda?:conda\r\n- CUDA/cuDNN version:7/10\r\n\r\n**Describe the problem**\r\nI try to save the model by Saver.save(), and then use restore to recover the model, but with I get the same results acculated from the save file even though their inputs are different. I don't understand why I get the wrong answer, maybe because the output is stored?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nthe save code:\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport gzip\r\nimport os\r\nimport sys\r\nimport time\r\nimport joblib\r\nimport math\r\nimport numpy\r\nfrom six.moves import urllib\r\nfrom six.moves import xrange  \r\nfrom PIL import Image\r\nfrom sklearn.metrics import confusion_matrix as sk_confusion_matrix\r\nfrom sklearn.metrics import classification_report\r\nimport tensorflow as tf\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\r\nFLAGS = None\r\nIMAGE_HEIGHT = 128\r\nIMAGE_WEITH = 128\r\nNUM_CHANNELS = 1\r\nNUM_LABELS = 4\r\nSEED = 66478  # Set to None for random seed.\r\nBATCH_SIZE = 32\r\nEVAL_BATCH_SIZE = 32\r\nEVAL_FREQUENCY = 10  # Number of steps between evaluations.\r\n\r\ndef data_type():\r\n  \"\"\"Return the type of the activations, weights, and placeholder variables.\"\"\"\r\n  if FLAGS.use_fp16:\r\n    return tf.float16\r\n  else:\r\n    return tf.float32\r\n\r\n\r\ndef fake_data(num_images):\r\n  \"\"\"Generate a fake dataset that matches the dimensions of MNIST.\"\"\"\r\n  data = numpy.ndarray(\r\n      shape=(num_images, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS),\r\n      dtype=numpy.float32)\r\n  labels = numpy.zeros(shape=(num_images,), dtype=numpy.int64)\r\n  for image in xrange(num_images):\r\n    label = image % 2\r\n    data[image, :, :, 0] = label - 0.5\r\n    labels[image] = label\r\n  return data, labels\r\n\r\n\r\ndef error_rate(predictions, labels):\r\n  Confusion_matrix=sk_confusion_matrix(numpy.argmax(predictions, 1).tolist(), labels.tolist())\r\n  print('Confusion_matrix:')\r\n  print(Confusion_matrix)  \r\n\r\n  Se1 = Confusion_matrix[1,1]+Confusion_matrix[2,2]+Confusion_matrix[3,3]\r\n  Se2 = Confusion_matrix[1,1]+Confusion_matrix[1,0]+Confusion_matrix[1,2]+Confusion_matrix[1,3]+Confusion_matrix[2,2]+Confusion_matrix[2,0]+Confusion_matrix[2,1]+Confusion_matrix[2,3]+Confusion_matrix[3,3]+Confusion_matrix[3,0]+Confusion_matrix[3,1]+Confusion_matrix[3,2]\r\n  Se = Se1/Se2\r\n  Sp = Confusion_matrix[0,0]/(Confusion_matrix[0,0]+Confusion_matrix[0,1]+Confusion_matrix[0,2]+Confusion_matrix[0,3]) \r\n  Acc = (Se+Sp)*100/2\r\n\r\n  target_names = ['class 0', 'class 1', 'class 2', 'class 3']\r\n\r\n  print()\r\n  accuracy = 100.0-(100.0 *numpy.sum(numpy.argmax(predictions, 1) == labels)/predictions.shape[0])\r\n  \r\n  \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\r\n  return 100.0 - (\r\n      100.0 *\r\n      numpy.sum(numpy.argmax(predictions, 1) == labels) /\r\n      predictions.shape[0]), Acc\r\n\r\ndef GroupNorm(x, G, eps=1e-05):\r\n    # x: input features with shape [N,H,W,C]\r\n    # gamma, beta: scale and offset, with shape [1,C,1,1]\r\n    # G: number of groups for GN\r\n  N, H, W, C = x.shape\r\n  N = BATCH_SIZE\r\n  gamma = tf.ones([1, 1, 1, C])\r\n  beta = tf.zeros([1, 1, 1, C])\r\n  x = tf.reshape(x, [N, G, H, W, C // G])\r\n  mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)\r\n  x = (x-mean) / tf.sqrt(var + eps)\r\n  x = tf.reshape(x, [N, H, W, C])\r\n  return x * gamma + beta\r\n\r\nclass ResBlock(object):\r\n\r\n  def __init__(self, stride_num=1, downsample=False):\r\n    self.conv1_weights = tf.Variable(\r\n      tf.truncated_normal([1, 1, 64, 64],  # 1x1 filter, depth 64.\r\n                          stddev=0.1,\r\n                          seed=SEED, dtype=data_type()))  \r\n\r\n    self.conv2_weights = tf.Variable(\r\n      tf.truncated_normal([3, 3, 64, 64],  # 3x3 filter, depth 64.\r\n                          stddev=0.1,\r\n                          seed=SEED, dtype=data_type()))\r\n    self.conv3_weights = tf.Variable(\r\n      tf.truncated_normal([3, 3, 64, 64],  # 3x3 filter, depth 64.\r\n                          stddev=0.1,\r\n                          seed=SEED, dtype=data_type()))  \r\n    self.stride_num = stride_num\r\n    self.downsample = downsample\r\n\r\n  def forward(self, data):\r\n    with tf.name_scope('ResNet'):\r\n    # shortcut = x\r\n      shortcut = data\r\n      # out = self.relu(self.norm1(x))\r\n#      axis = list(range(len(data.get_shape()) - 1))\r\n      with tf.name_scope('BN1'):\r\n        out = GroupNorm(x=data, G=32)\r\n        #mean, variance = tf.nn.moments(data, axis)\r\n        #out = tf.nn.batch_normalization(data, mean, variance, 0, 1, 0.001)\r\n      with tf.name_scope('relu1'):\r\n        out = tf.nn.relu(out)\r\n      #  if self.downsample is not None:\r\n        #   shortcut = self.downsample(out)\r\n      with tf.name_scope('downsample'):\r\n        if self.downsample is True:\r\n          shortcut = tf.nn.conv2d(out,\r\n                                  self.conv1_weights,\r\n                                  strides=[1, self.stride_num, self.stride_num, 1],\r\n                                  padding='SAME')\r\n        #  out = self.conv1(out)\r\n      with tf.name_scope('conv1'):\r\n        out = tf.nn.conv2d(out,\r\n                          self.conv2_weights,\r\n                          strides=[1, self.stride_num, self.stride_num, 1],\r\n                          padding='SAME')\r\n      #  out = self.droupout(out)\r\n      #  out = self.norm2(out)\r\n      with tf.name_scope('BN2'):\r\n        out = GroupNorm(x=out, G=32)\r\n\r\n      #  out = self.relu(out) \r\n      with tf.name_scope('relu2'):\r\n        out = tf.nn.relu(out)   \r\n      #  out = self.conv2(out)\r\n      with tf.name_scope('conv2'):\r\n        out = tf.nn.conv2d(out,\r\n                          self.conv3_weights,\r\n                          strides=[1, 1, 1, 1],\r\n                          padding='SAME')\r\n    return shortcut+out\r\n\r\nclass BRN(object):\r\n\r\n  def __init__(self):\r\n    self.ResNet_0_0 = ResBlock(2, True)\r\n    self.ResNet_0_1 = ResBlock(2, True)\r\n    self.ResNet_1_0 = ResBlock(2, True)\r\n    self.ResNet_1_1 = ResBlock(2, True)\r\n    self.ResNet_0 = ResBlock(1, False)\r\n    self.ResNet_1 = ResBlock(1, False)\r\n    self.ResNet_2 = ResBlock(1, False)\r\n    self.ResNet_3 = ResBlock(1, False)\r\n    self.ResNet_4 = ResBlock(1, False)\r\n    self.ResNet_5 = ResBlock(1, False)\r\n    self.ResNet_6 = ResBlock(1, False)\r\n    self.ResNet_7 = ResBlock(1, False)\r\n    self.ResNet_8 = ResBlock(1, False)\r\n    self.ResNet_9 = ResBlock(1, False)\r\n    self.ResNet_10 = ResBlock(1, False)\r\n    self.ResNet_11 = ResBlock(1, False)\r\n    self.ResNet_12 = ResBlock(1, False)\r\n    self.ResNet_13 = ResBlock(1, False)\r\n    self.ResNet_14 = ResBlock(1, False)\r\n    self.ResNet_15 = ResBlock(1, False)\r\n    self.ResNet_16 = ResBlock(1, False)\r\n    self.ResNet_17 = ResBlock(1, False)\r\n    self.ResNet_18 = ResBlock(1, False)\r\n    self.ResNet_19 = ResBlock(1, False)\r\n    self.ResNet_20 = ResBlock(1, False)\r\n    self.ResNet_21 = ResBlock(1, False)\r\n    self.conv1_weights = tf.Variable(\r\n      tf.truncated_normal([3, 3, NUM_CHANNELS, 64],  \r\n                          stddev=0.1,\r\n                          seed=SEED, dtype=data_type()))  \r\n    self.conv2_weights = tf.Variable(\r\n      tf.truncated_normal([3, 3, NUM_CHANNELS, 64],  \r\n                          stddev=0.1,\r\n                          seed=SEED, dtype=data_type()))  \r\n    self.fc_weights = tf.Variable(tf.truncated_normal([64, NUM_LABELS],\r\n                                                stddev=0.1,\r\n                                                seed=SEED,\r\n                                                dtype=data_type()))\r\n\r\n    self.fc_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS], dtype=data_type()))\r\n\r\n  def forward(self, stft, mfcc):\r\n\r\n    with tf.name_scope('BRNcon1'):\r\n      out_s = tf.nn.conv2d(stft,\r\n                        self.conv1_weights,\r\n                        strides= [1, 1, 1, 1],\r\n                        padding='VALID')\r\n    with tf.name_scope('resnetblocks_1'):\r\n      out_s = self.ResNet_0_0.forward(out_s)\r\n    with tf.name_scope('resnetblocks_2'):\r\n      out_s = self.ResNet_0_1.forward(out_s)\r\n    with tf.name_scope('resnetblocks_3'):\r\n      out_s = self.ResNet_0.forward(out_s)\r\n    with tf.name_scope('resnetblocks_4'):\r\n      out_s = self.ResNet_2.forward(out_s)\r\n    with tf.name_scope('resnetblocks_5'):\r\n      out_s = self.ResNet_4.forward(out_s)\r\n    with tf.name_scope('resnetblocks_6'):\r\n      out_s = self.ResNet_6.forward(out_s)\r\n    with tf.name_scope('resnetblocks_7'):\r\n      out_s = self.ResNet_8.forward(out_s)\r\n    with tf.name_scope('resnetblocks_8'):\r\n      out_s = self.ResNet_10.forward(out_s)\r\n    with tf.name_scope('resnetblocks_9'):\r\n      out_s = self.ResNet_12.forward(out_s)\r\n    with tf.name_scope('resnetblocks_10'):\r\n      out_s = self.ResNet_14.forward(out_s)\r\n    with tf.name_scope('resnetblocks_11'):\r\n      out_s = self.ResNet_16.forward(out_s)\r\n    with tf.name_scope('resnetblocks_12'):\r\n      out_s = self.ResNet_18.forward(out_s)\r\n    with tf.name_scope('resnetblocks_13'):\r\n      out_s = self.ResNet_20.forward(out_s)\r\n    with tf.name_scope('brns_bn1'):\r\n      out_s = GroupNorm(x=out_s, G=32)\r\n\r\n    with tf.name_scope('brn_relu_1'):\r\n      out_s = tf.nn.relu(out_s)\r\n    with tf.name_scope('brn_pool1'):\r\n      out_s = tf.nn.avg_pool(out_s,\r\n                            ksize=[1,out_s.shape[2],out_s.shape[2],1],\r\n                            strides=[1, 1, 1, 1],\r\n                            padding='VALID')\r\n\r\n    with tf.name_scope('BRNcon2'):\r\n      out_m = tf.nn.conv2d(mfcc,\r\n                        self.conv2_weights,\r\n                        strides= [1, 1, 1, 1],\r\n                        padding='VALID')\r\n    with tf.name_scope('resnetblockm_1'):\r\n      out_m = self.ResNet_1_0.forward(out_m)\r\n    with tf.name_scope('resnetblockm_2'):\r\n      out_m = self.ResNet_1_1.forward(out_m)\r\n    with tf.name_scope('resnetblockm_3'):\r\n      out_m = self.ResNet_1.forward(out_m)\r\n    with tf.name_scope('resnetblockm_4'):\r\n      out_m = self.ResNet_3.forward(out_m)  \r\n    with tf.name_scope('resnetblockm_5'):\r\n      out_m = self.ResNet_5.forward(out_m)\r\n    with tf.name_scope('resnetblockm_6'):\r\n      out_m = self.ResNet_7.forward(out_m)\r\n    with tf.name_scope('resnetblockm_7'):\r\n      out_m = self.ResNet_9.forward(out_m)\r\n    with tf.name_scope('resnetblockm_8'):\r\n      out_m = self.ResNet_11.forward(out_m)\r\n    with tf.name_scope('resnetblockm_9'):\r\n      out_m = self.ResNet_13.forward(out_m)\r\n    with tf.name_scope('resnetblockm_10'):\r\n      out_m = self.ResNet_15.forward(out_m)\r\n    with tf.name_scope('resnetblockm_11'):\r\n      out_m = self.ResNet_17.forward(out_m)\r\n    with tf.name_scope('resnetblockm_12'):\r\n      out_m = self.ResNet_19.forward(out_m)\r\n    with tf.name_scope('resnetblockm_13'):\r\n      out_m = self.ResNet_21.forward(out_m)\r\n    with tf.name_scope('brnm_bn1'):\r\n      out_m = GroupNorm(x=out_m, G=32)\r\n\r\n    with tf.name_scope('brn_relu_2'):\r\n      out_m = tf.nn.relu(out_m)\r\n    with tf.name_scope('brn_pool2'):\r\n      out_m = tf.nn.avg_pool(out_m,\r\n                            ksize=[1,out_m.shape[2],out_m.shape[2],1],\r\n                            strides=[1, 1, 1, 1],\r\n                            padding='VALID')\r\n    with tf.name_scope('maumul'):\r\n    \r\n      out = tf.multiply(out_s,out_m)\r\n    with tf.name_scope('fc'):\r\n\r\n      out_shape = out.get_shape().as_list()\r\n      reshape = tf.reshape(\r\n          out,\r\n          [out_shape[0], out_shape[1] * out_shape[2] * out_shape[3]])    \r\n      out = tf.add(tf.matmul(reshape, self.fc_weights), self.fc_biases, name=\"logits_\")\r\n\r\n    return out\r\n\r\ndef main(_):\r\n\r\n  def loss_function(weight, logits, labels):\r\n    labels = tf.one_hot(labels,4)\r\n    labels = tf.cast(labels, tf.float32)\r\n    first = tf.reduce_sum(tf.multiply(-labels, logits),1)\r\n    second_0 = tf.add(tf.exp(logits[:,0]),tf.exp(logits[:,1]))\r\n    second_1 = tf.add(tf.exp(logits[:,2]),tf.exp(logits[:,3]))\r\n    log = tf.log(tf.add(second_1,second_0))\r\n    weight = tf.transpose(tf.reduce_sum(tf.multiply(labels, weight),1))\r\n    output = tf.multiply(weight,tf.add(first,log))\r\n\r\n    return output\r\n\r\n  def normalize(stft):\r\n    stft_1 = numpy.empty([stft.shape[0],128,128])\r\n    stft_2 = numpy.empty([stft_1.shape[0],stft_1.shape[1],stft_1.shape[2],1])\r\n    for i in range(stft_1.shape[0]):\r\n      image = Image.fromarray(stft[i,:,:])\r\n      image = image.resize([128,128])\r\n      stft_1[i,:,:] = numpy.array(image)\r\n\r\n      min = numpy.min(stft_1[i,:,:])\r\n      max = numpy.max(stft_1[i,:,:])\r\n      stft_1[i,:,:] = (stft_1[i,:,:]-min)/(max-min)\r\n      stft_2[i,:,:,:] = stft_1[i,:,:].reshape((stft_1.shape[1],stft_1.shape[2],1))\r\n    return stft_2  \r\n\r\n  if FLAGS.self_test:\r\n    \r\n    train_data, train_labels = fake_data(256)\r\n    validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)\r\n    test_data, test_labels = fake_data(EVAL_BATCH_SIZE)\r\n    num_epochs = 1\r\n  else:\r\n    # Get the data.\r\n    \r\n    stft_training, mfcc_training, labels_training = joblib.load(open(FLAGS.input, mode='rb'))\r\n    stft_test, mfcc_test, labels_test = joblib.load(open(FLAGS.test, mode='rb'))\r\n\r\n    stft_test = numpy.array(stft_test)\r\n    mfcc_test = numpy.array(mfcc_test)\r\n    labels_test = numpy.array(labels_test)\r\n    stft_test = normalize(stft_test)\r\n    mfcc_test = normalize(mfcc_test)\r\n\r\n    stft_training = numpy.array(stft_training)\r\n    mfcc_training = numpy.array(mfcc_training)\r\n    labels_training = numpy.array(labels_training)\r\n    stft_training = normalize(stft_training)\r\n    mfcc_training = normalize(mfcc_training)\r\n\r\n    stft_shape = stft_training.shape\r\n    stft_shape = (None, stft_shape[1], stft_shape[2], 1)\r\n\r\n    mfcc_shape = mfcc_training.shape\r\n    mfcc_shape = (None, mfcc_shape[1], mfcc_shape[2], 1)\r\n\r\n    labels_shape = labels_training.shape\r\n    labels_shape = (None)\r\n\r\n    stft_placeholder = tf.placeholder(stft_training.dtype, stft_shape)\r\n    labels_placeholder = tf.placeholder(labels_training.dtype, labels_shape)\r\n    mfcc_placeholder = tf.placeholder(mfcc_training.dtype, mfcc_shape)\r\n    \r\n    dataset_training = tf.data.Dataset.from_tensor_slices((stft_placeholder, mfcc_placeholder, labels_placeholder))\r\n    dataset_training  = dataset_training.apply(\r\n        tf.data.experimental.shuffle_and_repeat(len(stft_training), None))  \r\n    dataset_training  = dataset_training.batch(BATCH_SIZE)\r\n    dataset_training  = dataset_training.prefetch(1)\r\n    iterator_training = dataset_training.make_initializable_iterator()\r\n    next_element_training = iterator_training.get_next()\r\n    num_epochs = FLAGS.epochs\r\n\r\n  train_size = labels_training.shape[0]\r\n\r\n\r\n  stft_holder = tf.placeholder(\r\n        name=\"stft_holder\",\r\n        dtype=data_type(),\r\n        shape=(None, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))\r\n  mfcc_holder = tf.placeholder(\r\n        name=\"mfcc_holder\",\r\n        dtype=data_type(),\r\n        shape=(None, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))\r\n  labels = tf.placeholder(tf.int64, shape=(None,))\r\n\r\n  with tf.name_scope('test_input'):\r\n    stft_t = tf.placeholder(\r\n        data_type(),\r\n        shape=(EVAL_BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))\r\n    mfcc_t = tf.placeholder(\r\n        data_type(),\r\n        shape=(EVAL_BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))\r\n\r\n  model = BRN()\r\n  logits = model.forward(stft_holder, mfcc_holder)\r\n  out1 = tf.identity(logits, name=\"out\")\r\n  try:\r\n    scalar_summary = tf.scalar_summary\r\n    SummaryWrite = tf.train.SummaryWrite\r\n    merge_summary = tf.merge_summary\r\n  except:\r\n    scalar_summary = tf.summary.scalar\r\n    SummaryWrite = tf.summary.FileWriter\r\n    merge_summary = tf.summary.merge\r\n  with tf.name_scope('loss'):\r\n    weights = [1.0, 1.7, 4.1, 5.7]\r\n    mid = loss_function(weights, logits=logits, labels=labels)\r\n#    mid = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n#       labels=labels, logits=logits)\r\n\r\n    loss = tf.reduce_sum(mid)\r\n    \r\n    loss_summary = scalar_summary('loss', loss)\r\n\r\n    \r\n    # L2 regularization for the fully connected parameters.\r\n    regularizers = (tf.nn.l2_loss(model.conv1_weights) + tf.nn.l2_loss(model.conv2_weights) +\r\n                    tf.nn.l2_loss(model.fc_weights) + tf.nn.l2_loss(model.fc_biases))\r\n    # Add the regularization term to the loss.\r\n    loss += 0.02 * regularizers\r\n\r\n    batch = tf.Variable(0, dtype=data_type())\r\n  # Use simple momentum for the optimization.\r\n  with tf.name_scope('train'):\r\n\r\n    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\r\n\r\n  # Predictions for the current training minibatch.\r\n  train_prediction = tf.nn.softmax(logits)\r\n  eval_prediction = tf.nn.softmax(model.forward(stft_t, mfcc_t))\r\n\r\n  # Create a local session to run the training.\r\n  start_time = time.time()\r\n\r\n  def eval_in_batches(stft_data, mfcc_data, sess, type):\r\n    \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\r\n    size = stft_data.shape[0]\r\n    if size < EVAL_BATCH_SIZE:\r\n      raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\r\n    predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\r\n    for begin in xrange(0, size, EVAL_BATCH_SIZE):\r\n      end = begin + EVAL_BATCH_SIZE\r\n      if end <= size:\r\n        if type == 'train':\r\n          predictions[begin:end, :] = sess.run(\r\n              train_prediction,\r\n              feed_dict={stft_holder: stft_data[begin:end, ...], mfcc_holder: mfcc_data[begin:end, ...]})\r\n        else: \r\n          predictions[begin:end, :] = sess.run(\r\n              eval_prediction,\r\n              feed_dict={stft_t: stft_data[begin:end, ...], mfcc_t: mfcc_data[begin:end, ...]})\r\n      else:\r\n        if type == 'train':\r\n          batch_predictions = sess.run(\r\n              train_prediction,\r\n              feed_dict={stft_holder: stft_data[-EVAL_BATCH_SIZE:, ...], mfcc_holder: mfcc_data[-EVAL_BATCH_SIZE:, ...]})\r\n        else:\r\n           batch_predictions = sess.run(\r\n              eval_prediction,\r\n              feed_dict={stft_t: stft_data[-EVAL_BATCH_SIZE:, ...], mfcc_t: mfcc_data[-EVAL_BATCH_SIZE:, ...]})\r\n        predictions[begin:, :] = batch_predictions[begin - size:, :]\r\n    return predictions\r\n\r\n  saver = tf.train.Saver()\r\n  config = tf.ConfigProto()\r\n  config.gpu_options.allow_growth = True  \r\n\r\n  with tf.Session(config=config) as sess:\r\n    # Run all the initializers to prepare the trainable parameters.\r\n    tf.global_variables_initializer().run()\r\n\r\n    merged = tf.summary.merge_all()\r\n    writer = SummaryWrite(FLAGS.logs + 'train', sess.graph)\r\n    print('Initialized!')\r\n    sess.run(iterator_training.initializer, feed_dict={stft_placeholder:stft_training,\r\n                      mfcc_placeholder:mfcc_training,\r\n                      labels_placeholder:labels_training})\r\n\r\n    # Loop through training steps.\r\n    for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\r\n\r\n      batch_stft, batch_mfcc, batch_labels = sess.run(next_element_training)\r\n  \r\n      feed_dict = {stft_holder: batch_stft,\r\n                   mfcc_holder: batch_mfcc,\r\n                   labels: batch_labels}\r\n      # Run the optimizer to update weights.\r\n\r\n      sess.run(optimizer, feed_dict=feed_dict)\r\n      # print some extra information once reach the evaluation frequency\r\n      if step % EVAL_FREQUENCY == 0:\r\n        # fetch some extra nodes' data\r\n        summary, l = sess.run([merged, loss],\r\n                                      feed_dict=feed_dict)\r\n        writer.add_summary(summary, step)\r\n        elapsed_time = time.time() - start_time\r\n        start_time = time.time()\r\n        rate, acc = error_rate(eval_in_batches(stft_training, mfcc_training, sess, 'train'), labels_training)\r\n        acc_summary = scalar_summary('accuracy', acc)\r\n        print('Step %d (epoch %.2f), Minibatch loss: %.3f, Minibatch error: %.1f%%, Accuracy:%.4f' %\r\n              (step, float(step) * BATCH_SIZE / train_size,\r\n              l,rate, acc))\r\n\r\n        \r\n    # Finally print the result!\r\n        sys.stdout.flush()\r\n        test_error, test_acc = error_rate(eval_in_batches(stft_test, mfcc_test, sess, 'test'), labels_test)\r\n        print('Testset error: %.1f%%, Accuracy:%.4f' % (test_error, test_acc))\r\n\r\n    saver.save(sess, './local_ckpt')        \r\n    writer.close()\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n#  dev = '/gpu:0'\r\n  parser = argparse.ArgumentParser()\r\n  parser.add_argument(\r\n      '--use_fp16',\r\n      default=False,\r\n      help='Use half floats instead of full floats if True.',\r\n      action='store_true')\r\n  parser.add_argument(\r\n      '--self_test',\r\n      default=False,\r\n      action='store_true',\r\n      help='True if running a self test.')\r\n  parser.add_argument(\r\n      '--input',\r\n      default='wavelet_stft.p')\r\n  parser.add_argument(\r\n      '--test',\r\n      default='wavelet_stft_test.p')  \r\n  parser.add_argument(\r\n      '--epochs',\r\n      type=int,\r\n      default=1)  \r\n  parser.add_argument(\r\n      '--logs',\r\n      default='')  \r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  tf.enable_resource_variables()\r\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```\r\n\r\nand here is recover code:\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport gzip\r\nimport os\r\nimport sys\r\nimport time\r\nimport joblib\r\nimport math\r\nimport numpy\r\nfrom six.moves import urllib\r\nfrom six.moves import xrange  \r\nfrom PIL import Image\r\nfrom sklearn.metrics import confusion_matrix as sk_confusion_matrix\r\nfrom sklearn.metrics import classification_report\r\nimport tensorflow as tf\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\nFLAGS = None\r\nIMAGE_HEIGHT = 128\r\nIMAGE_WEITH = 128\r\nNUM_CHANNELS = 1\r\nNUM_LABELS = 4\r\nSEED = 66478  # Set to None for random seed.\r\nBATCH_SIZE = 2\r\nEVAL_BATCH_SIZE = 32\r\nEVAL_FREQUENCY = 10  # Number of steps between evaluations.\r\n\r\ntf.reset_default_graph()\r\n\r\ndef data_type():\r\n  \"\"\"Return the type of the activations, weights, and placeholder variables.\"\"\"\r\n  if FLAGS.use_fp16:\r\n    return tf.float16\r\n  else:\r\n    return tf.float32\r\n\r\n\r\ndef fake_data(num_images):\r\n  \"\"\"Generate a fake dataset that matches the dimensions of MNIST.\"\"\"\r\n  data = numpy.ndarray(\r\n      shape=(num_images, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS),\r\n      dtype=numpy.float32)\r\n  labels = numpy.zeros(shape=(num_images,), dtype=numpy.int64)\r\n  for image in xrange(num_images):\r\n    label = image % 2\r\n    data[image, :, :, 0] = label - 0.5\r\n    labels[image] = label\r\n  return data, labels\r\n\r\n\r\ndef error_rate(predictions, labels):\r\n  Confusion_matrix=sk_confusion_matrix(numpy.argmax(predictions, 1).tolist(), labels.tolist())\r\n  print('Confusion_matrix:')\r\n  print(Confusion_matrix)  \r\n\r\n  Se1 = Confusion_matrix[1,1]+Confusion_matrix[2,2]+Confusion_matrix[3,3]\r\n  Se2 = Confusion_matrix[1,1]+Confusion_matrix[1,0]+Confusion_matrix[1,2]+Confusion_matrix[1,3]+Confusion_matrix[2,2]+Confusion_matrix[2,0]+Confusion_matrix[2,1]+Confusion_matrix[2,3]+Confusion_matrix[3,3]+Confusion_matrix[3,0]+Confusion_matrix[3,1]+Confusion_matrix[3,2]\r\n  Se = Se1/Se2\r\n  Sp = Confusion_matrix[0,0]/(Confusion_matrix[0,0]+Confusion_matrix[0,1]+Confusion_matrix[0,2]+Confusion_matrix[0,3]) \r\n  Acc = (Se+Sp)*100/2\r\n\r\n  target_names = ['class 0', 'class 1', 'class 2', 'class 3']\r\n\r\n  print()\r\n  accuracy = 100.0-(100.0 *numpy.sum(numpy.argmax(predictions, 1) == labels)/predictions.shape[0])\r\n  \r\n  \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\r\n  return 100.0 - (\r\n      100.0 *\r\n      numpy.sum(numpy.argmax(predictions, 1) == labels) /\r\n      predictions.shape[0]), Acc\r\n\r\ndef GroupNorm(x, G, eps=1e-05):\r\n    # x: input features with shape [N,H,W,C]\r\n    # gamma, beta: scale and offset, with shape [1,C,1,1]\r\n    # G: number of groups for GN\r\n  N, H, W, C = x.shape\r\n  gamma = tf.ones([1, 1, 1, C])\r\n  beta = tf.zeros([1, 1, 1, C])\r\n  x = tf.reshape(x, [N, G, H, W, C // G])\r\n  mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)\r\n  x = (x-1) / tf.sqrt(1 + eps)\r\n  x = tf.reshape(x, [N, H, W, C])\r\n  return x * gamma + beta\r\n\r\nclass ResBlock(object):\r\n\r\n  def __init__(self, stride_num=1, downsample=False):\r\n    self.conv1_weights = tf.Variable(\r\n      tf.truncated_normal([1, 1, 64, 64],  # 1x1 filter, depth 64.\r\n                          stddev=0.1,\r\n                          seed=SEED, dtype=data_type()))  \r\n\r\n    self.conv2_weights = tf.Variable(\r\n      tf.truncated_normal([3, 3, 64, 64],  # 3x3 filter, depth 64.\r\n                          stddev=0.1,\r\n                          seed=SEED, dtype=data_type()))\r\n    self.conv3_weights = tf.Variable(\r\n      tf.truncated_normal([3, 3, 64, 64],  # 3x3 filter, depth 64.\r\n                          stddev=0.1,\r\n                          seed=SEED, dtype=data_type()))  \r\n    self.stride_num = stride_num\r\n    self.downsample = downsample\r\n\r\n  def forward(self, data):\r\n    with tf.name_scope('ResNet'):\r\n    # shortcut = x\r\n      shortcut = data\r\n      # out = self.relu(self.norm1(x))\r\n#      axis = list(range(len(data.get_shape()) - 1))\r\n      with tf.name_scope('BN1'):\r\n        out = GroupNorm(x=data, G=32)\r\n        #mean, variance = tf.nn.moments(data, axis)\r\n        #out = tf.nn.batch_normalization(data, mean, variance, 0, 1, 0.001)\r\n      with tf.name_scope('relu1'):\r\n        out = tf.nn.relu(out)\r\n      #  if self.downsample is not None:\r\n        #   shortcut = self.downsample(out)\r\n      with tf.name_scope('downsample'):\r\n        if self.downsample is True:\r\n          shortcut = tf.nn.conv2d(out,\r\n                                  self.conv1_weights,\r\n                                  strides=[1, self.stride_num, self.stride_num, 1],\r\n                                  padding='SAME')\r\n        #  out = self.conv1(out)\r\n      with tf.name_scope('conv1'):\r\n        out = tf.nn.conv2d(out,\r\n                          self.conv2_weights,\r\n                          strides=[1, self.stride_num, self.stride_num, 1],\r\n                          padding='SAME')\r\n      #  out = self.droupout(out)\r\n      #  out = self.norm2(out)\r\n      with tf.name_scope('BN2'):\r\n        out = GroupNorm(x=out, G=32)\r\n\r\n      with tf.name_scope('relu2'):\r\n        out = tf.nn.relu(out)   \r\n      #  out = self.conv2(out)\r\n      with tf.name_scope('conv2'):\r\n        out = tf.nn.conv2d(out,\r\n                          self.conv3_weights,\r\n                          strides=[1, 1, 1, 1],\r\n                          padding='SAME')\r\n    return shortcut+out\r\n\r\nclass BRN(object):\r\n\r\n  def __init__(self):\r\n    self.ResNet_0_0 = ResBlock(2, True)\r\n    self.ResNet_0_1 = ResBlock(2, True)\r\n    self.ResNet_1_0 = ResBlock(2, True)\r\n    self.ResNet_1_1 = ResBlock(2, True)\r\n    self.ResNet_0 = ResBlock(1, False)\r\n    self.ResNet_1 = ResBlock(1, False)\r\n    self.ResNet_2 = ResBlock(1, False)\r\n    self.ResNet_3 = ResBlock(1, False)\r\n    self.ResNet_4 = ResBlock(1, False)\r\n    self.ResNet_5 = ResBlock(1, False)\r\n    self.ResNet_6 = ResBlock(1, False)\r\n    self.ResNet_7 = ResBlock(1, False)\r\n    self.ResNet_8 = ResBlock(1, False)\r\n    self.ResNet_9 = ResBlock(1, False)\r\n    self.ResNet_10 = ResBlock(1, False)\r\n    self.ResNet_11 = ResBlock(1, False)\r\n    self.ResNet_12 = ResBlock(1, False)\r\n    self.ResNet_13 = ResBlock(1, False)\r\n    self.ResNet_14 = ResBlock(1, False)\r\n    self.ResNet_15 = ResBlock(1, False)\r\n    self.ResNet_16 = ResBlock(1, False)\r\n    self.ResNet_17 = ResBlock(1, False)\r\n    self.ResNet_18 = ResBlock(1, False)\r\n    self.ResNet_19 = ResBlock(1, False)\r\n    self.ResNet_20 = ResBlock(1, False)\r\n    self.ResNet_21 = ResBlock(1, False)\r\n    self.conv1_weights = tf.Variable(\r\n      tf.truncated_normal([3, 3, NUM_CHANNELS, 64],  \r\n                          stddev=0.1,\r\n                          seed=SEED, dtype=data_type()))  \r\n    self.conv2_weights = tf.Variable(\r\n      tf.truncated_normal([3, 3, NUM_CHANNELS, 64],  \r\n                          stddev=0.1,\r\n                          seed=SEED, dtype=data_type()))  \r\n    self.fc_weights = tf.Variable(tf.truncated_normal([64, NUM_LABELS],\r\n                                                stddev=0.1,\r\n                                                seed=SEED,\r\n                                                dtype=data_type()))\r\n\r\n    self.fc_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS], dtype=data_type()))\r\n\r\n  def forward(self, stft, mfcc):\r\n\r\n    with tf.name_scope('BRNcon1'):\r\n      out_s = tf.nn.conv2d(stft,\r\n                        self.conv1_weights,\r\n                        strides= [1, 1, 1, 1],\r\n                        padding='VALID')\r\n    with tf.name_scope('resnetblocks_1'):\r\n      out_s = self.ResNet_0_0.forward(out_s)\r\n    with tf.name_scope('resnetblocks_2'):\r\n      out_s = self.ResNet_0_1.forward(out_s)\r\n    with tf.name_scope('resnetblocks_3'):\r\n      out_s = self.ResNet_0.forward(out_s)\r\n    with tf.name_scope('resnetblocks_4'):\r\n      out_s = self.ResNet_2.forward(out_s)\r\n    with tf.name_scope('resnetblocks_5'):\r\n      out_s = self.ResNet_4.forward(out_s)\r\n    with tf.name_scope('resnetblocks_6'):\r\n      out_s = self.ResNet_6.forward(out_s)\r\n    with tf.name_scope('resnetblocks_7'):\r\n      out_s = self.ResNet_8.forward(out_s)\r\n    with tf.name_scope('resnetblocks_8'):\r\n      out_s = self.ResNet_10.forward(out_s)\r\n    with tf.name_scope('resnetblocks_9'):\r\n      out_s = self.ResNet_12.forward(out_s)\r\n    with tf.name_scope('resnetblocks_10'):\r\n      out_s = self.ResNet_14.forward(out_s)\r\n    with tf.name_scope('resnetblocks_11'):\r\n      out_s = self.ResNet_16.forward(out_s)\r\n    with tf.name_scope('resnetblocks_12'):\r\n      out_s = self.ResNet_18.forward(out_s)\r\n    with tf.name_scope('resnetblocks_13'):\r\n      out_s = self.ResNet_20.forward(out_s)\r\n    with tf.name_scope('brns_bn1'):\r\n      out_s = GroupNorm(x=out_s, G=32)\r\n\r\n    with tf.name_scope('brn_relu_1'):\r\n      out_s = tf.nn.relu(out_s)\r\n    with tf.name_scope('brn_pool1'):\r\n      out_s = tf.nn.avg_pool(out_s,\r\n                            ksize=[1,out_s.shape[2],out_s.shape[2],1],\r\n                            strides=[1, 1, 1, 1],\r\n                            padding='VALID')\r\n\r\n    with tf.name_scope('BRNcon2'):\r\n      out_m = tf.nn.conv2d(mfcc,\r\n                        self.conv2_weights,\r\n                        strides= [1, 1, 1, 1],\r\n                        padding='VALID')\r\n    with tf.name_scope('resnetblockm_1'):\r\n      out_m = self.ResNet_1_0.forward(out_m)\r\n    with tf.name_scope('resnetblockm_2'):\r\n      out_m = self.ResNet_1_1.forward(out_m)\r\n    with tf.name_scope('resnetblockm_3'):\r\n      out_m = self.ResNet_1.forward(out_m)\r\n    with tf.name_scope('resnetblockm_4'):\r\n      out_m = self.ResNet_3.forward(out_m)  \r\n    with tf.name_scope('resnetblockm_5'):\r\n      out_m = self.ResNet_5.forward(out_m)\r\n    with tf.name_scope('resnetblockm_6'):\r\n      out_m = self.ResNet_7.forward(out_m)\r\n    with tf.name_scope('resnetblockm_7'):\r\n      out_m = self.ResNet_9.forward(out_m)\r\n    with tf.name_scope('resnetblockm_8'):\r\n      out_m = self.ResNet_11.forward(out_m)\r\n    with tf.name_scope('resnetblockm_9'):\r\n      out_m = self.ResNet_13.forward(out_m)\r\n    with tf.name_scope('resnetblockm_10'):\r\n      out_m = self.ResNet_15.forward(out_m)\r\n    with tf.name_scope('resnetblockm_11'):\r\n      out_m = self.ResNet_17.forward(out_m)\r\n    with tf.name_scope('resnetblockm_12'):\r\n      out_m = self.ResNet_19.forward(out_m)\r\n    with tf.name_scope('resnetblockm_13'):\r\n      out_m = self.ResNet_21.forward(out_m)\r\n    with tf.name_scope('brnm_bn1'):\r\n      out_m = GroupNorm(x=out_m, G=32)\r\n\r\n    with tf.name_scope('brn_relu_2'):\r\n      out_m = tf.nn.relu(out_m)\r\n    with tf.name_scope('brn_pool2'):\r\n      out_m = tf.nn.avg_pool(out_m,\r\n                            ksize=[1,out_m.shape[2],out_m.shape[2],1],\r\n                            strides=[1, 1, 1, 1],\r\n                            padding='VALID')\r\n    with tf.name_scope('maumul'):\r\n    \r\n      out = tf.multiply(out_s,out_m)\r\n    with tf.name_scope('fc'):\r\n\r\n      out_shape = out.get_shape().as_list()\r\n      reshape = tf.reshape(\r\n          out,\r\n          [out_shape[0], out_shape[1] * out_shape[2] * out_shape[3]])    \r\n      out = tf.add(tf.matmul(reshape, self.fc_weights), self.fc_biases)\r\n\r\n    return out\r\n\r\ndef main(_):\r\n\r\n\r\n  def normalize(stft):\r\n    stft_1 = numpy.empty([stft.shape[0],128,128])\r\n    stft_2 = numpy.empty([stft_1.shape[0],stft_1.shape[1],stft_1.shape[2],1])\r\n    for i in range(stft_1.shape[0]):\r\n      image = Image.fromarray(stft[i,:,:])\r\n      image = image.resize([128,128])\r\n      stft_1[i,:,:] = numpy.array(image)\r\n\r\n      min = numpy.min(stft_1[i,:,:])\r\n      max = numpy.max(stft_1[i,:,:])\r\n      stft_1[i,:,:] = (stft_1[i,:,:]-min)/(max-min)\r\n      stft_2[i,:,:,:] = stft_1[i,:,:].reshape((stft_1.shape[1],stft_1.shape[2],1))\r\n    return stft_2  \r\n\r\n  if FLAGS.self_test:\r\n    \r\n    train_data, train_labels = fake_data(256)\r\n    validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)\r\n    test_data, test_labels = fake_data(EVAL_BATCH_SIZE)\r\n    num_epochs = 1\r\n  else:\r\n    # Get the data.\r\n    \r\n    stft_training, mfcc_training, labels_training = joblib.load(open(FLAGS.test, mode='rb'))\r\n    stft_test, mfcc_test, labels_test = joblib.load(open(FLAGS.test, mode='rb'))\r\n\r\n    stft_test = numpy.array(stft_test)\r\n    mfcc_test = numpy.array(mfcc_test)\r\n    labels_test = numpy.array(labels_test)\r\n    stft_test = normalize(stft_test)\r\n    mfcc_test = normalize(mfcc_test)\r\n\r\n    stft_training = numpy.array(stft_training)\r\n    mfcc_training = numpy.array(mfcc_training)\r\n    labels_training = numpy.array(labels_training)\r\n    stft_training = normalize(stft_training)\r\n    mfcc_training = normalize(mfcc_training)\r\n\r\n    stft_shape = stft_training.shape\r\n    stft_shape = (None, stft_shape[1], stft_shape[2], 1)\r\n\r\n    mfcc_shape = mfcc_training.shape\r\n    mfcc_shape = (None, mfcc_shape[1], mfcc_shape[2], 1)\r\n\r\n    labels_shape = labels_training.shape\r\n    labels_shape = (None)\r\n\r\n    stft_placeholder = tf.placeholder(stft_training.dtype, stft_shape)\r\n    labels_placeholder = tf.placeholder(labels_training.dtype, labels_shape)\r\n    mfcc_placeholder = tf.placeholder(mfcc_training.dtype, mfcc_shape)\r\n    \r\n    dataset_training = tf.data.Dataset.from_tensor_slices((stft_placeholder, mfcc_placeholder, labels_placeholder))\r\n    dataset_training  = dataset_training.apply(\r\n        tf.data.experimental.shuffle_and_repeat(len(stft_training), None))  \r\n    dataset_training  = dataset_training.batch(BATCH_SIZE)\r\n    dataset_training  = dataset_training.prefetch(1)\r\n    iterator_training = dataset_training.make_initializable_iterator()\r\n    next_element_training = iterator_training.get_next()\r\n    num_epochs = FLAGS.epochs\r\n\r\n  train_size = labels_training.shape[0]\r\n\r\n\r\n  stft_holder = tf.placeholder(\r\n          name=\"stft_holder\",\r\n          dtype=data_type(),\r\n          shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))\r\n  mfcc_holder = tf.placeholder(\r\n          name=\"mfcc_holder\",\r\n          dtype=data_type(),\r\n          shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))\r\n  labels = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\r\n\r\n  with tf.name_scope('test_input'):\r\n    stft_t = tf.placeholder(\r\n        data_type(),\r\n        shape=(EVAL_BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))\r\n    mfcc_t = tf.placeholder(\r\n        data_type(),\r\n        shape=(EVAL_BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))\r\n\r\n  model = BRN()\r\n  logits = model.forward(stft_holder, mfcc_holder)\r\n  out1 = tf.identity(logits, name=\"out\")\r\n\r\n  saver = tf.train.Saver()\r\n\r\n\r\n  config = tf.ConfigProto()\r\n  config.gpu_options.allow_growth = True  \r\n\r\n  with tf.Session(config=config) as sess:\r\n    # Run all the initializers to prepare the trainable parameters.\r\n\r\n    saver.restore(sess, \"./local_ckpt\")\r\n\r\n    stft_shape = (2,128,128,1)\r\n    batch_stft = numpy.array(numpy.random.random_sample(stft_shape), dtype=numpy.float32)\r\n    batch_mfcc = numpy.array(numpy.random.random_sample(stft_shape), dtype=numpy.float32)\r\n\r\n    feed_dict = {stft_holder: batch_stft,\r\n                mfcc_holder: batch_mfcc}\r\n      # Run the optimizer to update weights.\r\n\r\n    results = sess.run(out1, feed_dict=feed_dict)\r\n    print(results)\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n#  dev = '/gpu:0'\r\n  parser = argparse.ArgumentParser()\r\n  parser.add_argument(\r\n      '--use_fp16',\r\n      default=False,\r\n      help='Use half floats instead of full floats if True.',\r\n      action='store_true')\r\n  parser.add_argument(\r\n      '--self_test',\r\n      default=False,\r\n      action='store_true',\r\n      help='True if running a self test.')\r\n  parser.add_argument(\r\n      '--input',\r\n      default='wavelet_stft.p')\r\n  parser.add_argument(\r\n      '--test',\r\n      default='wavelet_stft_test.p')  \r\n  parser.add_argument(\r\n      '--epochs',\r\n      type = int,\r\n      default=1)  \r\n  parser.add_argument(\r\n      '--logs',\r\n      default='')  \r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  tf.enable_resource_variables()\r\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```\r\nand I get answer like this:\r\n```\r\n[[0.10859548 0.09478829 0.1094515  0.08498076]\r\n [0.10859548 0.09478829 0.1094515  0.08498076]]\r\n```\r\nbut they should be different\r\nThanks for your reading anaway", "comments": []}, {"number": 32862, "title": "Deprecate tf.test.is_gpu_available", "body": "The tf.config.experimental.list_physical_devices API is preferable as\r\nthe current API results in an initialization of the runtime which may be\r\nundesirable.\r\n\r\nPiperOrigin-RevId: 270812165\r\n(cherry picked from commit b8e6bc6a6980f79eae332ba9b01e722f571f2c05)", "comments": []}, {"number": 32861, "title": "Mark tf.keras.utils.multi_gpu_model as deprecated.", "body": "PiperOrigin-RevId: 271495434", "comments": []}]