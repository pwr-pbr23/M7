[{"number": 24070, "title": "How to restructure a tensor according to one dimension.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Centos 7\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:no\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.7.0\r\n- **Python version**:3.5.5\r\n- **Bazel version (if compiling from source)**:no\r\n- **GCC/Compiler version (if compiling from source)**:4.9.3\r\n- **CUDA/cuDNN version**:no\r\n- **GPU model and memory**:2 same GTX Titan X (Pascal), 12GB\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nHow to restructure a tensor according to one dimension specified by user?\r\n\r\nThe paper \"An End-to-End Deep Learning Architecture for Graph Classification\" published in AAAI-18([PDF](http://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf)) proposed a new novel neural network architecture for `graph` classification and a new layer called \"SortPooling\" as a key block of the network. The author of this paper implement the network by `torch`, i want to reproduce by tensorflow. One of the key operation is to restructure a tensor by the descend value according to some one dimension.\r\n\r\ne.g.\r\n\r\nassume the original tensor `a` is \r\n```python\r\n[[93, 69, 38, 46]\r\n[54, 5, 1, 13]\r\n[96, 45, 52, 50]]\r\n```\r\nand we want to restructure the tensor above according to the `a[:, 3]`, so output:\r\n```python\r\n[54, 5, 1, 13]\r\n[[93, 69, 38, 46]\r\n[96, 45, 52, 50]]\r\n```\r\nThen remove the last row(if two line already enough), so the final result i want is:\r\n```python\r\n[[54, 5, 1, 13]\r\n[93, 69, 38, 46]]\r\n```\r\n\r\nHow to achieve this by tensorflow? Any funciton exist? the `tf.nn.top_k` seem not enough for this.\r\n\r\n### Source code / logs\r\nno\r\n", "comments": ["Can you explain the reason why you want to do that operation?", "Maybe i can give you a brief explanation try my best. To do the graph(not image) classification, a more specialized architecture need to be designed, so `Graph Convolution layer` and `SortPooling Layer` are proposed to form GNN network. In my opinion, what the former do is extract features of each node from his neighbors by propagation, and as the author said, \"our graph convolution layer is a theoretically closer approximation to the Weisfeiler Lehman algorthm\", the WL algorithm is a graph labeling algorithm, what the latter do is to sort vertex by features(the value can be view as the importance of the node) instead of summing them up(the traditional method use), and feed the result into classic convolution layer.\r\nThe `Graph Convolution layer` is easy to code base on tensorflow, it just consist of some matrix operation(i think). but the `SortPooling layer` require sort and remove some dimension.  Maybe you have to dive into the paper if you want more detail and explanation.", "Can't you use a softmax layer or some kind of classification layer at the end to sort the tensors (which in this case, would be nodes) by importance? You could actually make a 2-network program: the first one gets the importance of the nodes, which you classify using some kind of softmax layer. Then, you can remove the nodes you don't want and feed the result in another NN as you see fit. I don't know if that is applicable to your problem AT ALL, but that's my best answer given my actual knowledge of your problem... ", "You could also probably find something interesting with these three basic functions, combined with your top_k: [slice](https://www.tensorflow.org/api_docs/python/tf/slice), [split](https://www.tensorflow.org/api_docs/python/tf/split) and [reshape](https://www.tensorflow.org/api_docs/python/tf/reshape).", "> You could also probably find something interesting with these three basic functions, combined with your top_k: [slice](https://www.tensorflow.org/api_docs/python/tf/slice), [split](https://www.tensorflow.org/api_docs/python/tf/split) and [reshape](https://www.tensorflow.org/api_docs/python/tf/reshape).\r\n\r\nthinks for your reply, it exactly give me great inspiration and different ideas.", "This requirement can be meet by `tf.gather()` combine `tf.nn.top_k()`.\r\n\r\n```python\r\nx = tf.constant(value=[[93, 69, 38, 46],[54, 5, 1, 13],[96, 45, 52, 50]])\r\nsort_op = tf.gather(x, tf.nn.top_k(-x[:, 3], 3).indices)[:2]\r\nwith tf.Session() as sess:\r\n    print(sess.run(sort_op))\r\n```"]}, {"number": 24069, "title": "TensorFlow Architecture low level intro link broken", "body": "Link:\r\nhttps://www.tensorflow.org/guide/extend/architecture\r\n\r\nIn the above link, an introduction link is provided. Its broken.\r\nhttps://www.tensorflow.org/guide/guide/low_level_intro\r\n\r\n", "comments": ["https://github.com/tensorflow/docs/blob/master/site/en/guide/extend/architecture.md is the file to change if you want to submit a PR to fix it. The other link in the paragraph is broken also\r\n\r\n```\r\nThis document describes the system architecture that makes this\r\ncombination of scale and flexibility possible. It assumes that you have basic familiarity\r\nwith TensorFlow programming concepts such as the computation graph, operations,\r\nand sessions. See [this document](../guide/low_level_intro.md) for an introduction to\r\nthese topics. Some familiarity with [distributed TensorFlow](../deploy/distributed.md)\r\nwill also be helpful.\r\n```\r\n\r\nI think the first link should be `../low_level_intro.md`\r\nand the 2nd link should be `../../deploy/distributed.md`", "@abhaygk , this was fixed last Friday. Can you verify and close?", "This was fixed, thanks"]}, {"number": 24068, "title": "devices: inform user that tensorflow wasn't compiled with CUDA support", "body": "The current behaviour can be confusing when no GPU are found on a computer that does have GPUs, so add a warning.", "comments": ["Nagging Reviewer @hawkinsp: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24067, "title": "max_pool_with_argmax: resulting indices dependent on data type", "body": "Hello!  \r\n\r\nWhen testing code using the function tf.nn.max_pool_with_argmax, I found that:\r\n- when the input tensor is tf.constant: the reported indices are correct  \r\n- when the input tensor is tf.Variable: the reported indices are calculated per batch-dimension (*wrong*)\r\n\r\n### Example code:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf \r\n\r\n# Create a tensor with KNOWN max-pooling output:\r\nA = np.zeros((4, 4), dtype = np.float32)\r\nA[0,0] = 1.1\r\nA[1,2] = 1.2\r\nA[2,1] = 1.4\r\nA[3,3] = 1.3\r\n\r\nB = np.zeros((4, 4), dtype = np.float32)\r\nB[1,0] = 1.9\r\nB[2,1] = 1.8\r\nB[0,2] = 1.7\r\nB[3,2] = 1.6\r\n\r\ntotal_stack = np.zeros((3, 4, 4, 2), dtype = np.float32 )\r\ntotal_stack[0,:,:,0] = A \r\ntotal_stack[0,:,:,1] = B \r\ntotal_stack[1,:,:,0] = A\r\ntotal_stack[1,:,:,1] = B\r\ntotal_stack[2,:,:,0] = A\r\ntotal_stack[2,:,:,1] = B \r\n\r\n# Wrap all in a two tensors with different data types:\r\ntest_constant = tf.constant( total_stack, dtype = tf.float32, name = 'constant'  )\r\ntest_variable = tf.Variable( total_stack, name = 'variable')\r\n\r\nconstant_values, constant_indices = tf.nn.max_pool_with_argmax( test_constant, [1, 2, 2, 1], [1, 2, 2, 1], padding = 'SAME')\r\nvariable_values, variable_indices = tf.nn.max_pool_with_argmax( test_variable, [1, 2, 2, 1], [1, 2, 2, 1], padding = 'SAME')\r\n\r\n# Setup a session and initialize variables\r\nmodel = tf.global_variables_initializer()\r\nsession = tf.InteractiveSession()\r\nsession.run( model )     \r\n\r\n# Print the results on screen\r\nprint(\"Constant -> CORRECT\")\r\nprint(session.run( constant_indices ))\r\nprint(\"Variable -> WRONG\")\r\nprint(session.run( variable_indices ))\r\n```\r\n\r\nThe output is: \r\n- In case of data type is tf.constant, the behavior is as expected:\r\n```\r\n[[[[ 0  9]\r\n   [12  5]]\r\n\r\n  [[18 19]\r\n   [30 29]]]\r\n\r\n\r\n [[[32 41]\r\n   [44 37]]\r\n\r\n  [[50 51]\r\n   [62 61]]]\r\n\r\n\r\n [[[64 73]\r\n   [76 69]]\r\n\r\n  [[82 83]\r\n   [94 93]]]]\r\n```\r\nNote that the indices are unique and go from 0 to 94 to span all dimensions.\r\n- In case the data type is tf.Variable, the results are *wrong*: indices are repeated per batch dimension: \r\n```\r\n[[[[ 0  9]\r\n   [12  5]]\r\n\r\n  [[18 19]\r\n   [30 29]]]\r\n\r\n\r\n [[[ 0  9]\r\n   [12  5]]\r\n\r\n  [[18 19]\r\n   [30 29]]]\r\n\r\n\r\n [[[ 0  9]\r\n   [12  5]]\r\n\r\n  [[18 19]\r\n   [30 29]]]]\r\n```\r\nNote that the sequence of indices are repeated 3 times, as the batch dimension is 3. This is *NOT* the expected behavior of the function.\r\n\r\n#### A fix would be highly appreciated....\r\n\r\nKind regards,\r\n\r\nTokin256\r\n\r\n\r\n| Configuration | Version |\r\n|----------------|----------|\r\n| Platform         |  Windows 7 (64) |\r\n| Python version | 3.6.4 (x64) |\r\n| IPython version | 6.2.1 |\r\n| TensorFlow | 1.10.0 (GPU) |\r\n| CUDA | V9.0.176|\r\n| GPU |  Nvidia M2000M GPU (2GB)|\r\n\r\n", "comments": ["@azaks2 this looks like an issue where constant folding (which runs on the CPU) gives different answers than the GPU kernel. Can someone on your team look at the GPU side of this?", "As @alextp said, this is caused by a difference between the CPU and the GPU implementations. The CPU implementation runs when the input is a constant, due to constant folding.\r\n\r\nAccording to [the documentation](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool_with_argmax) The CPU implementation is correct, and the GPU implementation is incorrect. However, the GPU behavior has been around since well before TensorFlow 1.0 while the CPU version was added more recently in #18145.  Also, the GPU behavior makes more sense IMO. So perhaps this is a bug in the documentation, although the buggy documentation has been around since well before TensorFlow 1.0 as well.\r\n\r\n/CC @rmlarsen , @martinwicke, do you think we should change the GPU version to match the CPU version and documentation, or change the CPU version and documentation to match the GPU version? I'm a bit worried about breaking existing code in either case.", "I think we should break the CPU version and documentation.\n\nOn Mon, Jan 7, 2019 at 12:58 PM Reed <notifications@github.com> wrote:\n\n> As @alextp <https://github.com/alextp> said, this is caused by a\n> difference between the CPU and the GPU implementations. The CPU\n> implementation runs when the input is a constant, due to constant folding.\n>\n> According to the documentation\n> <https://www.tensorflow.org/api_docs/python/tf/nn/max_pool_with_argmax>\n> The CPU implementation is correct, and the GPU implementation is incorrect.\n> However, the GPU behavior has been around since well before TensorFlow 1.0\n> while the CPU version was added more recently in #18145\n> <https://github.com/tensorflow/tensorflow/pull/18145>. Also, the GPU\n> behavior makes more sense IMO. So perhaps this is a bug in the\n> documentation, although the buggy documentation has been around since well\n> before TensorFlow 1.0 as well.\n>\n> /CC @rmlarsen <https://github.com/rmlarsen> , @martinwicke\n> <https://github.com/martinwicke>, do you think we should change the GPU\n> version to match the CPU version and documentation, or change the CPU\n> version and documentation to match the GPU version? I'm a bit worried about\n> breaking existing code in either case.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24067#issuecomment-452077464>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxZOQtRcM1HVA1Vp5XUwhFaduhyL1ks5vA7TogaJpZM4Y7bLB>\n> .\n>\n\n\n-- \n - Alex\n", "Should we close PR #23993, which breaks the GPU version?", "@reedwm wdyt? You have the most context here.", "@alextp, you think we should add an optional argument, say `include_batch_in_index`, that will revert the CPU version back to it's old behavior? That way if someone's model breaks because we changed the CPU version, they can pass the flag to go back to the old behavior.", "SGTM\n\nOn Fri, Jan 11, 2019 at 12:20 PM Reed <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp>, you think we should add an optional\n> argument, say include_batch_in_index, that will revert the CPU version\n> back to it's old behavior? That way if someone's model breaks because we\n> changed the CPU version, they can pass the flag to go back to the old\n> behavior.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24067#issuecomment-453644337>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxVHoFFPkyoVQkYUfqpNJOdvfAWRXks5vCPH-gaJpZM4Y7bLB>\n> .\n>\n\n\n-- \n - Alex\n", "Ok, someone should add an `include_batch_in_index` attribute which defaults to False, and implement both cases for the CPU version. Then @facaiy or someone else can modify #23993 to use the attribute.\r\n\r\n@facaiy, do you want to modify #23993 once support for the attribute is in? Also, if you want to, you can add the attribute as well.", "@reedwm I agree. And I'd like to split it into two parts as you suggested: The first PR is to add `include_batch_in_index` attribute, and implement both cases for CPU version; Then we modify #23993 to use the new attribute.\r\n\r\nI'll submit a new PR for the first part in the coming weeks :-)", "closed this in #26562 "]}, {"number": 24066, "title": "Android demo build fails with bazel", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version:\r\n1.12\r\n- Python version:\r\n2.7.15\r\n- Installed using virtualenv? pip? conda?:\r\npip\r\n- Bazel version (if compiling from source):\r\n0.15.1\r\n- GCC/Compiler version (if compiling from source):\r\n5.5\r\n- CUDA/cuDNN version:\r\n9.2 / 7.1.3\r\n- GPU model and memory:\r\nNvidia GTX 1080\r\n\r\n\r\n**Describe the problem**\r\nWhen trying to build android demo with bazel following the  [README]( https://github.com/tensorflow/tensorflow/tree/r1.12/tensorflow/examples/android) \r\nBuild command with bazel fails.\r\nThank you for your help. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n(cuda_env)  jean@pc:~/tensorflow$ bazel build //tensorflow/examples/android:tensorflow_demo\r\n(full details of output in logs)\r\n ...\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n11 warnings and 20 errors generated.\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\nINFO: Elapsed time: 78.831s, Critical Path: 18.32s\r\nINFO: 304 processes: 303 local, 1 worker.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n**Any other info / logs**\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/2632785/log.txt)\r\n\r\n\r\n", "comments": ["@Jeanbouvatt : I would recommend trying out Tensorflow Lite (see the vision demos [here](https://www.tensorflow.org/lite/demo_android)) as it's currently the recommended on-device inference solution.\r\n\r\nIt's unclear to me if we are still maintaining //tensorflow/examples/android:tensorflow_demo. Assigning shashishekhar@ to comment.", "@Jeanbouvatt : I am going to send a patch to fix the instructions, but the error message means that you need to pass --cxxopt='--std=c++11' to build the demo.\r\nSo the command should be the following\r\n\r\n```bash\r\nbazel build --cxxopt='--std=c++11' -c opt //tensorflow/examples/android:tensorflow_demo\r\n```", "Thank you for you time, I will try using tensorflow lite. "]}, {"number": 24065, "title": "InvalidArgumentError (see above for traceback): Got 88 frames, but animated gifs can only be decoded by tf.image.decode_gif or tf.image.decode_image", "body": "I am trying to train a new model. Here is my code!\r\n\r\npython /Users/tianchuangxin1/hub/examples/image_retraining/retrain.py --image_dir /Users/tianchuangxin1/tensorflow_image ----model_dir=/Users/tianchuangxin1/tensorflow_image --how_many_training_steps=6000 --flip_left_right Ture --testing_percentage=10 --train_batch_size=32 --validation_batch_size=-1 --random_brightness=30 --random_scale=30 --eval_step_interval=200\r\n\r\nThen , get the error:\r\n\r\n\r\nINFO:tensorflow:Looking for images in 'apple'\r\nINFO:tensorflow:Looking for images in 'computer'\r\nINFO:tensorflow:Looking for images in 'pants'\r\nINFO:tensorflow:Looking for images in 'shoes'\r\nINFO:tensorflow:Looking for images in 'watch'\r\nINFO:tensorflow:Using /var/folders/85/qbnsnmfn5vn0r3x3nvr09w790pxfq6/T/tfhub_modules to cache modules.\r\n2018-11-30 16:31:19.221881: W tensorflow/core/graph/graph_constructor.cc:1265] Importing a graph with a lower producer version 26 into an existing graph with producer version 27. Shape inference will have run different parts of the graph with different producer versions.\r\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\r\n2018-11-30 16:31:23.032847: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nWARNING:tensorflow:From /Users/tianchuangxin1/hub/examples/image_retraining/retrain.py:582: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.gfile.GFile.\r\nTraceback (most recent call last):\r\n  File \"/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Got 88 frames, but animated gifs can only be decoded by tf.image.decode_gif or tf.image.decode_image\r\n\t [[{{node DecodeJpeg_1}} = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_DistortJPGInput_0_0)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/tianchuangxin1/hub/examples/image_retraining/retrain.py\", line 1315, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/Users/tianchuangxin1/hub/examples/image_retraining/retrain.py\", line 1057, in main\r\n    distorted_image_tensor, resized_image_tensor, bottleneck_tensor)\r\n  File \"/Users/tianchuangxin1/hub/examples/image_retraining/retrain.py\", line 587, in get_random_distorted_bottlenecks\r\n    {input_jpeg_tensor: jpeg_data})\r\n  File \"/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Got 88 frames, but animated gifs can only be decoded by tf.image.decode_gif or tf.image.decode_image\r\n\t [[node DecodeJpeg_1 (defined at /Users/tianchuangxin1/hub/examples/image_retraining/retrain.py:671)  = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_DistortJPGInput_0_0)]]\r\n\r\nCaused by op 'DecodeJpeg_1', defined at:\r\n  File \"/Users/tianchuangxin1/hub/examples/image_retraining/retrain.py\", line 1315, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/Users/tianchuangxin1/hub/examples/image_retraining/retrain.py\", line 1024, in main\r\n    FLAGS.random_brightness, module_spec)\r\n  File \"/Users/tianchuangxin1/hub/examples/image_retraining/retrain.py\", line 671, in add_input_distortions\r\n    decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\r\n  File \"/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_image_ops.py\", line 954, in decode_jpeg\r\n    name=name)\r\n  File \"/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/tianchuangxin1/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Got 88 frames, but animated gifs can only be decoded by tf.image.decode_gif or tf.image.decode_image\r\n\t [[node DecodeJpeg_1 (defined at /Users/tianchuangxin1/hub/examples/image_retraining/retrain.py:671)  = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_DistortJPGInput_0_0)]]\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["You've probably got a \".gif\" file in your data folder which cannot actually be decoded as an image file. You can either remove the \".gif\" file from the folder or add this kind of a code to detect multiple file encodings:\r\n\r\nif imagePath.endswith(\".png\"):\r\n    imageDecoded = tf.image.decode_png(...)\r\n\r\nelif imagePath.endswith(\".jpg\"):\r\n    imageDecoded = tf.image.decode_jpeg(...)\r\n\r\nelif imagePath.endswith(\".gif\"):\r\n    imageDecoded = tf.image.decode_gif(...)\r\n\r\nThis needs to be modifier in the part of the code that converts your image files to tensors.", "This is where it should be modified:\r\n![image](https://user-images.githubusercontent.com/26579806/49293818-2e52c380-f47f-11e8-9af7-e5e5121aaba0.png)\r\n", "> You've probably got a \".gif\" file in your data folder which cannot actually be decoded as an image file. You can either remove the \".gif\" file from the folder or add this kind of a code to detect multiple file encodings:\r\n> \r\n> if imagePath.endswith(\".png\"):\r\n> imageDecoded = tf.image.decode_png(...)\r\n> \r\n> elif imagePath.endswith(\".jpg\"):\r\n> imageDecoded = tf.image.decode_jpeg(...)\r\n> \r\n> elif imagePath.endswith(\".gif\"):\r\n> imageDecoded = tf.image.decode_gif(...)\r\n> \r\n> This needs to be modifier in the part of the code that converts your image files to tensors.\r\n\r\n\r\nI have carefully checked the pictures I have trained, all the pictures are in JPG format. Very strange, I don't know if it is a version of my TensorFlow issue. After that, I deleted two more folders, just fine.", "This issue is more suitable on TensorFlow models repo. Please post it on TF models repo from [here](https://github.com/tensorflow/models/issues/new). Thanks!"]}, {"number": 24064, "title": "can't use tensorflow-gpu=1.12", "body": "I use anaconda and I both have cuda8 and cuda10 with the right cudnn. \r\nI tried 3.6.2 and 3.6.6. None of them works. \r\nNot only in the windows desk top. The tensorflow can't work on  linux cluster.\r\n\r\n(py36) C:\\Users\\GUANGYUAN>python\r\nPython 3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\GUANGYUAN\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\GUANGYUAN\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\GUANGYUAN\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\GUANGYUAN\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\GUANGYUAN\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\GUANGYUAN\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\GUANGYUAN\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\GUANGYUAN\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\GUANGYUAN\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\GUANGYUAN\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\GUANGYUAN\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\GUANGYUAN\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\GUANGYUAN\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["[Tensorflow GPU Requirements:](https://www.tensorflow.org/install/gpu#software_requirements)\r\nNvidia drivers >= 384.x\r\nCUDA 9.0 only\r\ncuDNN >= 7.2", "I highly suggest you try using CUDA v9.0 with cuDNN 7 as they are, from my experience, the most stable versions yet. I know that 10.0 and 8 are starting to be stable aswell, but as of right now, I still recommend the older versions. Also, make sure you have correctly installed tensorflow-gpu from pip with \"pip3 install tensorflow-gpu --upgrade\" or built it correctly from source following this [guide](https://www.tensorflow.org/install/source_windows). \r\n\r\nAlso, make sure you have correctly set your PATH environment variable following this [guide](https://www.tensorflow.org/install/gpu). You will also be able to see if you have the correct hardware requirements to use tensorflow GPU.", "Is there any way you can insstall on CUDA 11?\r\n", "CUDA 11 is currently not supported by tensorflow official. The only officially supported versions are CUDA 9 with cuDNN 7. Although, CUDA 10 with cuDNN 8 will soon be supported. There are no news for CUDA 11 yet.", "Closing this since explanation given by @FredVaugeois is correct. Feel free to reopen the issue if you are having trouble with cuda 9.0 TensorFlow build. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 24063, "title": "cann't import tensorflow-gpu 1.12 on windows", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["Have you followed all the instructions in this [guide](https://www.tensorflow.org/install/gpu)? You need to make sure your environment variables are correctly set and that you have correctly installed the correct pip package following this [guide](https://www.tensorflow.org/install/pip) -> \"pip3 install tensorflow-gpu --upgrade\". Also, make sure you have a [CUDA enabled GPU](https://developer.nvidia.com/cuda-gpus) (and that you have indeed installed CUDA/cuDNN - I suggest CUDA 9.0 and cuDNN 7).", "Duplicate of #24064 "]}, {"number": 24062, "title": "conv/weights/Assign: Input tensor Cannot convert a tensor of type float32 to an input of type float32_ref", "body": "When freezing a model, I keep encounter the problem represented in the title.\r\n\r\nAfter training's done, I use the following code to freeze the checkpoint to a pb file:\r\n```\r\nimport os, argparse\r\n\r\nimport tensorflow as tf\r\n\r\n# The original freeze_graph function\r\n# from tensorflow.python.tools.freeze_graph import freeze_graph \r\n\r\ndir = os.path.dirname(os.path.realpath(__file__))\r\n\r\ndef freeze_graph(model_dir, output_node_names):\r\n    \"\"\"Extract the sub graph defined by the output nodes and convert \r\n    all its variables into constant \r\n    Args:\r\n        model_dir: the root folder containing the checkpoint state file\r\n        output_node_names: a string, containing all the output node's names, \r\n                            comma separated\r\n    \"\"\"\r\n    if not tf.gfile.Exists(model_dir):\r\n        raise AssertionError(\r\n            \"Export directory doesn't exists. Please specify an export \"\r\n            \"directory: %s\" % model_dir)\r\n\r\n    if not output_node_names:\r\n        print(\"You need to supply the name of a node to --output_node_names.\")\r\n        return -1\r\n\r\n    # We retrieve our checkpoint fullpath\r\n    checkpoint = tf.train.get_checkpoint_state(model_dir)\r\n    input_checkpoint = checkpoint.model_checkpoint_path\r\n    \r\n    # We precise the file fullname of our freezed graph\r\n    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\r\n    output_graph = absolute_model_dir + \"/frozen_model.pb\"\r\n\r\n    # We clear devices to allow TensorFlow to control on which device it will load operations\r\n    clear_devices = True\r\n\r\n    # We start a session using a temporary fresh Graph\r\n    with tf.Session(graph=tf.Graph()) as sess:\r\n        # We import the meta graph in the current default Graph\r\n        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\r\n\r\n        # We restore the weights\r\n        saver.restore(sess, input_checkpoint)\r\n\r\n        # We use a built-in TF helper to export variables to constants\r\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n            sess, # The session is used to retrieve the weights\r\n            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \r\n            output_node_names.split(\",\") # The output node names are used to select the usefull nodes\r\n        ) \r\n\r\n        # Finally we serialize and dump the output graph to the filesystem\r\n        with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n\r\n    return output_graph_def\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model_dir\", type=str, default=\"\", help=\"Model folder to export\")\r\n    parser.add_argument(\"--output_node_names\", type=str, default=\"\", help=\"The name of the output nodes, comma separated.\")\r\n    args = parser.parse_args()\r\n\r\n    freeze_graph(args.model_dir, args.output_node_names)\r\n```\r\nAnd it looks fine:\r\n```\r\nConverted 603 variables to const ops.\r\n7851 ops in the final graph.\r\n```\r\nplus, the generated pb file is larger than the ckpt file but smaller than the size of meta plus ckpt together.\r\nThen, I tried to load the pb to inspect the ops:\r\n```\r\nimport argparse\r\nimport tensorflow as tf\r\n\r\ndef load_graph(frozen_graph_filename):\r\n    # We load the protobuf file from the disk and parse it to retrieve the\r\n    # unserialized graph def\r\n    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    # Then, we import the graph_def into a new Graph and return it\r\n    with tf.Graph().as_default() as graph:\r\n        # The name var will prefix every op/nodes in your graph\r\n        # Since we load everything in a new graph, this is not needed\r\n        tf.import_graph_def(graph_def, name='prefix')\r\n    return graph\r\n\r\nif __name__ == '__main__':\r\n    # Let's allow the user to pass the filename as an argument\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--frozen_model_filename', default='../experiments/ft-ht/frozen_model.pb', type=str, help='Frozen model file to import')\r\n    args = parser.parse_args()\r\n\r\n    # We use our \"load_graph\" function\r\n    graph = load_graph(args.frozen_model_filename)\r\n\r\n    # We can verify that we can access the list of operations in the graph\r\n    for op in graph.get_operations():\r\n        print(op.name)\r\n```\r\nAnd the code throws the error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"infer_pb.py\", line 25, in <module>\r\n    graph = load_graph(args.frozen_model_filename)\r\n  File \"infer_pb.py\", line 15, in load_graph\r\n    tf.import_graph_def(graph_def, name='prefix')\r\n  File \"/path/to/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 406, in import_graph_def\r\n    node, 'Input tensor %r %s' % (input_name, te)))\r\nValueError: graph_def is invalid at node u'resnet_v1_50/conv1/weights/Assign': Input tensor 'resnet_v1_50/conv1/weights:0' Cannot convert a tensor of type float32 to an input of type float32_ref.\r\n```\r\n\r\nAnyone could shed me some light? Thanks!", "comments": ["Hello, please refer to #3628 where the conversion issue is addressed.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24061, "title": "Investigate precomputing reciprocals for division operations in XLA", "body": "Today we optimize this elementwise division by a scalar:\r\n\r\n```\r\nHloModule DivReciprocal\r\n\r\nENTRY main {\r\n  x = f32[2000,200]{1,0} parameter(0)\r\n  y = f32[] parameter(1)\r\n  y.broadcast = f32[2000,200]{1,0} broadcast(y), dimensions={}\r\n  ROOT div = f32[2000,200]{1,0} divide(x, y.broadcast)\r\n}\r\n```\r\n\r\ninto the following:\r\n\r\n```\r\nHloModule DivReciprocal\r\n\r\nfused_computation {\r\n  param_0 = f32[2000,200]{1,0} parameter(0)\r\n  param_1.1 = f32[] parameter(1)\r\n  y.broadcast.1 = f32[2000,200]{1,0} broadcast(param_1.1), dimensions={}\r\n  ROOT div.1 = f32[2000,200]{1,0} divide(param_0, y.broadcast.1)\r\n}\r\n\r\nENTRY main {\r\n  x = f32[2000,200]{1,0} parameter(0)\r\n  y = f32[] parameter(1)\r\n  ROOT fusion = f32[2000,200]{1,0} fusion(x, y), kind=kLoop, calls=fused_computation\r\n}\r\n```\r\n\r\n`fusion` is ultimately lowered into the following PTX (via LLVM):\r\n\r\n```\r\n//\r\n// Generated by LLVM NVPTX Back-End\r\n//\r\n\r\n.version 6.0\r\n.target sm_60\r\n.address_size 64\r\n\r\n        // .globl       fusion\r\n\r\n.visible .entry fusion(\r\n        .param .u64 fusion_param_0,\r\n        .param .u64 fusion_param_1,\r\n        .param .u64 fusion_param_2\r\n)\r\n.reqntid 1024, 1, 1\r\n{\r\n        .reg .pred      %p<2>;\r\n        .reg .f32       %f<11>;\r\n        .reg .b32       %r<6>;\r\n        .reg .b64       %rd<10>;\r\n\r\n        mov.u32         %r2, %ctaid.x;\r\n        mov.u32         %r3, %tid.x;\r\n        shl.b32         %r4, %r2, 10;\r\n        or.b32          %r5, %r4, %r3;\r\n        setp.lt.u32     %p1, %r5, 100000;\r\n        @%p1 bra        LBB0_2;\r\n        bra.uni         LBB0_1;\r\nLBB0_2:\r\n        ld.param.u64    %rd4, [fusion_param_0];\r\n        ld.param.u64    %rd5, [fusion_param_2];\r\n        cvta.to.global.u64      %rd1, %rd5;\r\n        ld.param.u64    %rd6, [fusion_param_1];\r\n        cvta.to.global.u64      %rd2, %rd6;\r\n        cvta.to.global.u64      %rd3, %rd4;\r\n        shl.b32         %r1, %r5, 2;\r\n        mul.wide.u32    %rd7, %r1, 4;\r\n        add.s64         %rd8, %rd2, %rd7;\r\n        ld.global.nc.v4.f32     {%f1, %f2, %f3, %f4}, [%rd8];\r\n        ld.global.nc.f32        %f5, [%rd1];\r\n        rcp.approx.f32  %f6, %f5;\r\n        mul.f32         %f7, %f1, %f6;\r\n        add.s64         %rd9, %rd3, %rd7;\r\n        mul.f32         %f8, %f2, %f6;\r\n        mul.f32         %f9, %f3, %f6;\r\n        mul.f32         %f10, %f4, %f6;\r\n        st.global.v4.f32        [%rd9], {%f7, %f8, %f9, %f10};\r\nLBB0_1:\r\n        ret;\r\n\r\n}\r\n```\r\n\r\nIt is conceivable that in some cases computing the reciprocal for `x` once, before we launch the kernel for `fusion`, can be profitable.  We should investigate if (and when) precomputing the reciprocal helps, and if it does, implement it as an optimization in XLA.", "comments": ["Hi, as per the other bug report, would you be willing not to assign issues to me that I am not working on?", "It should be always profitable to hoist the reciprocal showing in the example, the problem is if the \r\nfloating point precision differences are acceptable. Many compilers do the transformation under options which allow such differences.", "> the problem is if the floating point precision differences are acceptable\r\n\r\nIndeed.\r\n\r\nWe already pass `-nvptx-prec-divf32=0` to LLVM, which lets it use PTX `div.approx`.  This is a large win on some benchmarks.\r\n\r\nBut even this optimization is observable.  For example, if `x == y`, then `x/y == 1` should be true.  But it's not with div.approx.  You can make this error arbitrarily large by e.g. doing `floor(x/y)` -- now this is 1 or 0, depending on whether or not there was an error.\r\n\r\nWe don't have a good framework for making decisions about whether or not these kinds of optimizations are allowable in XLA.  What we want for an ML compiler is different from traditional compiler \"fastmath\" assumptions.  We usually want to preserve infs and nans, maybe even signed zeroes.  We also *in general* don't want to arbitrarily decrease precision by reassociation (although we do this in some places today).\r\n\r\nInstead (I think) we want the freedom that comes from the assumption that the floating-point inputs to a program are effectively random numbers.  It should be OK for XLA to transform a program `f(x)` into a program `g(x) = f(x+epsilon)`, because that perturbation is \"within the noise\" anyway.  Under a model like that many approximations (not just approximate div, but say approximate tanh) may become acceptable.\r\n\r\nI dunno if that's a sound model, but it's my working idea.\r\n\r\nAnyway my hunch is that since we're already doing `div.approx`, precomputing reciprocals is probably ok.", "I am not sure if the original issue is about transforming the original code into the following \r\n(compute the reciprocal of parameter(1), and passed as a parameter to the fusion, and inside the\r\nfusion, the divide is replaced by multiply),\r\n\r\nHloModule DivReciprocal\r\n\r\nfused_computation {\r\n  param_0 = f32[2000,200]{1,0} parameter(0)\r\n  param_1.1 = f32[] parameter(1)\r\n  y.broadcast.1 = f32[2000,200]{1,0} broadcast(param_1.1), dimensions={}\r\n  ROOT div.1 = f32[2000,200]{1,0} multiply(param_0, y.broadcast.1)\r\n}\r\n\r\nENTRY main {\r\n  x = f32[2000,200]{1,0} parameter(0)\r\n  y = f32[] divide(1.0f/parameter(1))        \r\n  ROOT fusion = f32[2000,200]{1,0} fusion(x, y), kind=kLoop, calls=fused_computation\r\n}\r\n\r\n", "> I am not sure if the original issue is about transforming the original code into the following\r\n\r\nYes, exactly like you have it.  This is useful on the assumption -- now questioned! -- that computing div.approx is significantly more expensive than computing multiply.", "Regarding the single-precision floating point .approx PTX instructions (namely div.approx, rcp.approx, ex2.approx, lg2.approx, sqrt.approx, rsqrt.approx, sin.approx, and cos.approx),\r\n\r\n1.           All of them are implemented with a few straight-line hardware instructions (some of which are predicated).\r\n2.           The non-ftz versions have a few more instructions to handle denorms, thus are slower than the ftz versions.\r\n3.           div.approx is implemented as rcp.approx and mul.\r\n\r\nYou can verify the above using the `nvdisasm` tool shipped with the CUDA toolkit.\r\n\r\nThe throughput of these instructions is lower than that of mul, but in the same magnitude as mul compared with memory access. They generally should be considered \u2018cheap\u2019 in the cost function for fusion decisions, even if they are used in a broadcast.\r\n\r\nSince you\u2019re consistently lowering single precision HLO division to PTX\u2019s div.approx, we do not recommend pursuing the optimization suggested in this issue per the above analysis.\r\n\r\n"]}, {"number": 24060, "title": "Cpu tensorflow ImportError ", "body": "Dear All ,\r\n\r\nI am very new to Python and datascience , I have installed tensorflow cpu from anaconda navigator , but when I am importing tensorflow its showing the following error. also find the system information and conda information below \r\n\r\n\r\n**ERROR :** \r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Aashay.bane\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Aashay.bane\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Aashay.bane\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Aashay.bane\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Aashay.bane\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n************************************************************************************************************\r\n**!conda info**\r\n\r\n     active environment : base\r\n    active env location : C:\\Users\\Aashay.bane\\AppData\\Local\\Continuum\\anaconda3\r\n            shell level : 1\r\n       user config file : C:\\Users\\Aashay.bane\\.condarc\r\n populated config files : C:\\Users\\Aashay.bane\\.condarc\r\n          conda version : 4.5.11\r\n    conda-build version : 3.16.3\r\n         python version : 3.6.7.final.0\r\n       base environment : C:\\Users\\Aashay.bane\\AppData\\Local\\Continuum\\anaconda3  (writable)\r\n           channel URLs : https://repo.anaconda.com/pkgs/main/win-64\r\n                          https://repo.anaconda.com/pkgs/main/noarch\r\n                          https://repo.anaconda.com/pkgs/free/win-64\r\n                          https://repo.anaconda.com/pkgs/free/noarch\r\n                          https://repo.anaconda.com/pkgs/r/win-64\r\n                          https://repo.anaconda.com/pkgs/r/noarch\r\n                          https://repo.anaconda.com/pkgs/pro/win-64\r\n                          https://repo.anaconda.com/pkgs/pro/noarch\r\n                          https://repo.anaconda.com/pkgs/msys2/win-64\r\n                          https://repo.anaconda.com/pkgs/msys2/noarch\r\n          package cache : C:\\Users\\Aashay.bane\\AppData\\Local\\Continuum\\anaconda3\\pkgs\r\n                          C:\\Users\\Aashay.bane\\AppData\\Local\\conda\\conda\\pkgs\r\n       envs directories : C:\\Users\\Aashay.bane\\AppData\\Local\\Continuum\\anaconda3\\envs\r\n                          C:\\Users\\Aashay.bane\\AppData\\Local\\conda\\conda\\envs\r\n                          C:\\Users\\Aashay.bane\\.conda\\envs\r\n               platform : win-64\r\n             user-agent : conda/4.5.11 requests/2.20.1 CPython/3.6.7 Windows/8.1 Windows/6.3.9600\r\n          administrator : False\r\n             netrc file : None\r\n           offline mode : False\r\n***********************************************************************************************************\r\n!conda list --show-channel-urls\r\n# packages in environment at C:\\Users\\Aashay.bane\\AppData\\Local\\Continuum\\anaconda3:\r\n#\r\n# Name                    Version                   Build  Channel\r\n_ipyw_jlab_nb_ext_conf    0.1.0                    py36_0    defaults\r\nabsl-py                   0.6.1                    py36_0    defaults\r\nalabaster                 0.7.12                   py36_0    defaults\r\nanaconda                  5.3.0                    py37_0    defaults\r\nanaconda-client           1.7.2                    py36_0    defaults\r\nanaconda-navigator        1.9.2                    py36_0    defaults\r\nanaconda-project          0.8.2                    py36_0    defaults\r\nappdirs                   1.4.3            py36h28b3542_0    defaults\r\nasn1crypto                0.24.0                   py36_0    defaults\r\nastor                     0.7.1                    py36_0    defaults\r\nastroid                   2.1.0                    py36_0    defaults\r\nastropy                   3.0.5            py36he774522_0    defaults\r\natomicwrites              1.2.1                    py36_0    defaults\r\nattrs                     18.2.0           py36h28b3542_0    defaults\r\nautomat                   0.7.0                    py36_0    defaults\r\nbabel                     2.6.0                    py36_0    defaults\r\nbackcall                  0.1.0                    py36_0    defaults\r\nbackports                 1.0                      py36_1    defaults\r\nbackports.os              0.1.1                    py36_0    defaults\r\nbackports.shutil_get_terminal_size 1.0.0                    py36_2    defaults\r\nbeautifulsoup4            4.6.3                    py36_0    defaults\r\nbitarray                  0.8.3            py36hfa6e2cd_0    defaults\r\nbkcharts                  0.2              py36h7e685f7_0    defaults\r\nblas                      1.0                         mkl    defaults\r\nblaze                     0.11.3                   py36_0    defaults\r\nbleach                    3.0.2                    py36_0    defaults\r\nblosc                     1.14.4               he51fdeb_0    defaults\r\nbokeh                     1.0.2                    py36_0    defaults\r\nboto                      2.49.0                   py36_0    defaults\r\nbottleneck                1.2.1            py36h452e1ab_1    defaults\r\nbzip2                     1.0.6                hfa6e2cd_5    defaults\r\nca-certificates           2018.03.07                    0    defaults\r\ncertifi                   2018.10.15               py36_0    defaults\r\ncffi                      1.11.5           py36h74b6da3_1    defaults\r\nchardet                   3.0.4                    py36_1    defaults\r\nclick                     7.0                      py36_0    defaults\r\ncloudpickle               0.6.1                    py36_0    defaults\r\nclyent                    1.2.2                    py36_1    defaults\r\ncolorama                  0.4.0                    py36_0    defaults\r\ncomtypes                  1.1.7                    py36_0    defaults\r\nconda                     4.5.11                   py36_0    defaults\r\nconda-build               3.16.3                   py36_0    defaults\r\nconda-env                 2.6.0                h36134e3_1    defaults\r\nconsole_shortcut          0.1.1                         3    defaults\r\nconstantly                15.1.0           py36h28b3542_0    defaults\r\ncontextlib2               0.5.5            py36he5d52c0_0    defaults\r\ncryptography              2.3.1            py36h74b6da3_0    defaults\r\ncudatoolkit               9.0                           1    defaults\r\ncudnn                     7.1.4                 cuda9.0_0    defaults\r\ncurl                      7.61.0               h7602738_0    defaults\r\ncycler                    0.10.0           py36h009560c_0    defaults\r\ncython                    0.29             py36ha925a31_0    defaults\r\ncytoolz                   0.9.0.1          py36hfa6e2cd_1    defaults\r\ndask                      1.0.0                    py36_0    defaults\r\ndask-core                 1.0.0                    py36_0    defaults\r\ndatashape                 0.5.4                    py36_1    defaults\r\ndecorator                 4.3.0                    py36_0    defaults\r\ndefusedxml                0.5.0                    py36_1    defaults\r\ndistributed               1.25.0                   py36_0    defaults\r\ndocutils                  0.14             py36h6012d8f_0    defaults\r\nentrypoints               0.2.3                    py36_2    defaults\r\net_xmlfile                1.0.1            py36h3d2d736_0    defaults\r\nfastcache                 1.0.2            py36hfa6e2cd_2    defaults\r\nfilelock                  3.0.10                   py36_0    defaults\r\nflask                     1.0.2                    py36_1    defaults\r\nflask-cors                3.0.7                    py36_0    defaults\r\nfreetype                  2.9.1                ha9979f8_1    defaults\r\ngast                      0.2.0                    py36_0    defaults\r\nget_terminal_size         1.0.0                h38e98db_0    defaults\r\ngevent                    1.3.7            py36he774522_1    defaults\r\nglob2                     0.6                      py36_1    defaults\r\ngreenlet                  0.4.15           py36hfa6e2cd_0    defaults\r\ngrpcio                    1.14.1           py36h5c4b210_0    defaults\r\nh5py                      2.8.0            py36h3bdd7fb_2    defaults\r\nhdf5                      1.10.2               hac2f561_1    defaults\r\nheapdict                  1.0.0                    py36_2    defaults\r\nhtml5lib                  1.0.1                    py36_0    defaults\r\nhyperlink                 18.0.0                   py36_0    defaults\r\nicc_rt                    2017.0.4             h97af966_0    defaults\r\nicu                       58.2                 ha66f8fd_1    defaults\r\nidna                      2.7                      py36_0    defaults\r\nimageio                   2.4.1                    py36_0    defaults\r\nimagesize                 1.1.0                    py36_0    defaults\r\nimportlib_metadata        0.6                      py36_0    defaults\r\nincremental               17.5.0                   py36_0    defaults\r\nintel-openmp              2019.0                      118    defaults\r\nipykernel                 5.1.0            py36h39e3cac_0    defaults\r\nipython                   7.1.1            py36h39e3cac_0    defaults\r\nipython_genutils          0.2.0            py36h3c5d0ee_0    defaults\r\nipywidgets                7.4.2                    py36_0    defaults\r\nisort                     4.3.4                    py36_0    defaults\r\nitsdangerous              1.1.0                    py36_0    defaults\r\njdcal                     1.4                      py36_0    defaults\r\njedi                      0.13.1                   py36_0    defaults\r\njinja2                    2.10                     py36_0    defaults\r\njpeg                      9b                   hb83a4c4_2    defaults\r\njsonschema                2.6.0            py36h7636477_0    defaults\r\njupyter                   1.0.0                    py36_7    defaults\r\njupyter_client            5.2.3                    py36_0    defaults\r\njupyter_console           6.0.0                    py36_0    defaults\r\njupyter_core              4.4.0                    py36_0    defaults\r\njupyterlab                0.35.3                   py36_0    defaults\r\njupyterlab_launcher       0.13.1                   py36_0    defaults\r\njupyterlab_server         0.2.0                    py36_0    defaults\r\nkeras-applications        1.0.6                    py36_0    defaults\r\nkeras-preprocessing       1.0.5                    py36_0    defaults\r\nkeyring                   16.1.0                   py36_0    defaults\r\nkiwisolver                1.0.1            py36h6538335_0    defaults\r\nlazy-object-proxy         1.3.1            py36hfa6e2cd_2    defaults\r\nlibarchive                3.3.3                h798a506_1    defaults\r\nlibcurl                   7.61.0               h7602738_0    defaults\r\nlibiconv                  1.15                 h1df5818_7    defaults\r\nlibpng                    1.6.35               h2a8f88b_0    defaults\r\nlibprotobuf               3.6.1                h7bd577a_0    defaults\r\nlibsodium                 1.0.16               h9d3ae62_0    defaults\r\nlibssh2                   1.8.0                hd619d38_4    defaults\r\nlibtiff                   4.0.9                h36446d0_2    defaults\r\nlibxml2                   2.9.8                hadb2253_1    defaults\r\nlibxslt                   1.1.32               hf6f1972_0    defaults\r\nllvmlite                  0.26.0           py36ha925a31_0    defaults\r\nlocket                    0.2.0            py36hfed976d_1    defaults\r\nlxml                      4.2.5            py36hef2cd61_0    defaults\r\nlz4-c                     1.8.1.2              h2fa13f4_0    defaults\r\nlzo                       2.10                 h6df0209_2    defaults\r\nm2w64-gcc-libgfortran     5.3.0                         6    defaults\r\nm2w64-gcc-libs            5.3.0                         7    defaults\r\nm2w64-gcc-libs-core       5.3.0                         7    defaults\r\nm2w64-gmp                 6.1.0                         2    defaults\r\nm2w64-libwinpthread-git   5.0.0.4634.697f757               2    defaults\r\nMarkdown                  3.0.1                     <pip>\r\nmarkupsafe                1.1.0            py36he774522_0    defaults\r\nmatplotlib                3.0.1            py36hc8f65d3_0    defaults\r\nmccabe                    0.6.1                    py36_1    defaults\r\nmenuinst                  1.4.14           py36hfa6e2cd_0    defaults\r\nmistune                   0.8.4            py36he774522_0    defaults\r\nmkl                       2018.0.3                      1    defaults\r\nmkl-service               1.1.2            py36hb217b18_5    defaults\r\nmkl_fft                   1.0.6            py36hdbbee80_0    defaults\r\nmkl_random                1.0.1            py36h77b88f5_1    defaults\r\nmore-itertools            4.3.0                    py36_0    defaults\r\nmpmath                    1.0.0                    py36_2    defaults\r\nmsgpack-python            0.5.6            py36he980bc4_1    defaults\r\nmsys2-conda-epoch         20160418                      1    defaults\r\nmultipledispatch          0.6.0                    py36_0    defaults\r\nnavigator-updater         0.2.1                    py36_0    defaults\r\nnbconvert                 5.4.0                    py36_1    defaults\r\nnbformat                  4.4.0            py36h3a5bc1b_0    defaults\r\nnetworkx                  2.2                      py36_1    defaults\r\nnltk                      3.3.0                    py36_0    defaults\r\nnose                      1.3.7                    py36_2    defaults\r\nnotebook                  5.7.2                    py36_0    defaults\r\nnumba                     0.41.0           py36hf9181ef_0    defaults\r\nnumexpr                   2.6.8            py36h9ef55f4_0    defaults\r\nnumpy                     1.15.4           py36ha559c80_0    defaults\r\nnumpy-base                1.15.4           py36h8128ebf_0    defaults\r\nnumpydoc                  0.8.0                    py36_0    defaults\r\nodo                       0.5.1            py36h7560279_0    defaults\r\nolefile                   0.46                     py36_0    defaults\r\nopenpyxl                  2.5.9                    py36_0    defaults\r\nopenssl                   1.0.2p               hfa6e2cd_0    defaults\r\npackaging                 18.0                     py36_0    defaults\r\npandas                    0.23.4           py36h830ac7b_0    defaults\r\npandoc                    1.19.2.1             hb2460c7_1    defaults\r\npandocfilters             1.4.2                    py36_1    defaults\r\nparso                     0.3.1                    py36_0    defaults\r\npartd                     0.3.9                    py36_0    defaults\r\npath.py                   11.5.0                   py36_0    defaults\r\npathlib2                  2.3.2                    py36_0    defaults\r\npatsy                     0.5.1                    py36_0    defaults\r\npep8                      1.7.1                    py36_0    defaults\r\npickleshare               0.7.5                    py36_0    defaults\r\npillow                    5.3.0            py36hdc69c19_0    defaults\r\npip                       18.1                     py36_0    defaults\r\npkginfo                   1.4.2                    py36_1    defaults\r\npluggy                    0.8.0                    py36_0    defaults\r\nply                       3.11                     py36_0    defaults\r\nprometheus_client         0.4.2                    py36_0    defaults\r\nprompt_toolkit            2.0.7                    py36_0    defaults\r\nprotobuf                  3.6.1            py36h33f27b4_0    defaults\r\npsutil                    5.4.8            py36he774522_0    defaults\r\npy                        1.7.0                    py36_0    defaults\r\npyasn1                    0.4.4            py36h28b3542_0    defaults\r\npyasn1-modules            0.2.2                    py36_0    defaults\r\npycodestyle               2.4.0                    py36_0    defaults\r\npycosat                   0.6.3            py36hfa6e2cd_0    defaults\r\npycparser                 2.19                     py36_0    defaults\r\npycrypto                  2.6.1            py36hfa6e2cd_9    defaults\r\npycurl                    7.43.0.2         py36h74b6da3_0    defaults\r\npyflakes                  2.0.0                    py36_0    defaults\r\npygments                  2.2.0            py36hb010967_0    defaults\r\npyhamcrest                1.9.0                    py36_2    defaults\r\npylint                    2.1.1                    py36_0    defaults\r\npyodbc                    4.0.24           py36h6538335_0    defaults\r\npyopenssl                 18.0.0                   py36_0    defaults\r\npyparsing                 2.3.0                    py36_0    defaults\r\npyqt                      5.9.2            py36h6538335_2    defaults\r\npysocks                   1.6.8                    py36_0    defaults\r\npytables                  3.4.4            py36he6f6034_0    defaults\r\npytest                    4.0.0                    py36_0    defaults\r\npytest-arraydiff          0.2              py36h39e3cac_0    defaults\r\npytest-astropy            0.4.0                    py36_0    defaults\r\npytest-doctestplus        0.2.0                    py36_0    defaults\r\npytest-openfiles          0.3.0                    py36_0    defaults\r\npytest-remotedata         0.3.1                    py36_0    defaults\r\npython                    3.6.7                h33f27b4_1    defaults\r\npython-dateutil           2.7.5                    py36_0    defaults\r\npython-libarchive-c       2.8                      py36_6    defaults\r\npytz                      2018.7                   py36_0    defaults\r\npywavelets                1.0.1            py36h8c2d366_0    defaults\r\npywin32                   223              py36hfa6e2cd_1    defaults\r\npywinpty                  0.5.4                    py36_0    defaults\r\npyyaml                    3.13             py36hfa6e2cd_0    defaults\r\npyzmq                     17.1.2           py36hfa6e2cd_0    defaults\r\nqt                        5.9.6            vc14h1e9a669_2  [vc14]  defaults\r\nqtawesome                 0.5.3                    py36_0    defaults\r\nqtconsole                 4.4.2                    py36_0    defaults\r\nqtpy                      1.5.2                    py36_0    defaults\r\nrequests                  2.20.1                   py36_0    defaults\r\nrope                      0.11.0                   py36_0    defaults\r\nruamel_yaml               0.15.46          py36hfa6e2cd_0    defaults\r\nscikit-image              0.14.0           py36h6538335_1    defaults\r\nscikit-learn              0.20.1           py36hb854c30_0    defaults\r\nscipy                     1.1.0            py36h4f6bf74_1    defaults\r\nseaborn                   0.9.0                    py36_0    defaults\r\nsend2trash                1.5.0                    py36_0    defaults\r\nservice_identity          17.0.0           py36h28b3542_0    defaults\r\nsetuptools                40.6.2                   py36_0    defaults\r\nsimplegeneric             0.8.1                    py36_2    defaults\r\nsingledispatch            3.4.0.3          py36h17d0c80_0    defaults\r\nsip                       4.19.8           py36h6538335_0    defaults\r\nsix                       1.11.0                   py36_1    defaults\r\nsnappy                    1.1.7                h777316e_3    defaults\r\nsnowballstemmer           1.2.1            py36h763602f_0    defaults\r\nsortedcollections         1.0.1                    py36_0    defaults\r\nsortedcontainers          2.0.5                    py36_0    defaults\r\nsphinx                    1.8.2                    py36_0    defaults\r\nsphinxcontrib             1.0                      py36_1    defaults\r\nsphinxcontrib-websupport  1.1.0                    py36_1    defaults\r\nspyder                    3.3.2                    py36_0    defaults\r\nspyder-kernels            0.3.0                    py36_0    defaults\r\nsqlalchemy                1.2.14           py36he774522_0    defaults\r\nsqlite                    3.24.0               h7602738_0    defaults\r\nstatsmodels               0.9.0            py36h452e1ab_0    defaults\r\nsympy                     1.3                      py36_0    defaults\r\ntblib                     1.3.2            py36h30f5020_0    defaults\r\ntensorboard               1.12.0                    <pip>\r\ntensorflow-base           1.12.0          gpu_py36h6e53903_0    defaults\r\ntermcolor                 1.1.0                    py36_1    defaults\r\nterminado                 0.8.1                    py36_1    defaults\r\ntestpath                  0.4.2                    py36_0    defaults\r\ntextblob                  0.15.1                     py_0    conda-forge/label/gcc7\r\ntk                        8.6.8                hfa6e2cd_0    defaults\r\ntoolz                     0.9.0                    py36_0    defaults\r\ntornado                   5.1.1            py36hfa6e2cd_0    defaults\r\ntqdm                      4.28.1           py36h28b3542_0    defaults\r\ntraitlets                 4.3.2            py36h096827d_0    defaults\r\ntwisted                   18.9.0           py36he774522_0    defaults\r\ntyped-ast                 1.1.0            py36hfa6e2cd_0    defaults\r\nunicodecsv                0.14.1           py36h6450c06_0    defaults\r\nurllib3                   1.23                     py36_0    defaults\r\nvc                        14.1                 h0510ff6_4    defaults\r\nvs2015_runtime            14.15.26706          h3a45250_0    defaults\r\nwcwidth                   0.1.7            py36h3d5aa90_0    defaults\r\nwebencodings              0.5.1                    py36_1    defaults\r\nwerkzeug                  0.14.1                   py36_0    defaults\r\nwheel                     0.32.3                   py36_0    defaults\r\nwidgetsnbextension        3.4.2                    py36_0    defaults\r\nwin_inet_pton             1.0.1                    py36_1    defaults\r\nwin_unicode_console       0.5              py36hcdbd4b5_0    defaults\r\nwincertstore              0.2              py36h7fe50ca_0    defaults\r\nwinpty                    0.4.3                         4    defaults\r\nwrapt                     1.10.11          py36hfa6e2cd_2    defaults\r\nxlrd                      1.1.0                    py36_1    defaults\r\nxlsxwriter                1.1.2                    py36_0    defaults\r\nxlwings                   0.14.1                   py36_0    defaults\r\nxlwt                      1.3.0            py36h1a4751e_0    defaults\r\nxz                        5.2.4                h2fa13f4_4    defaults\r\nyaml                      0.1.7                hc54c509_2    defaults\r\nzeromq                    4.2.5                he025d50_1    defaults\r\nzict                      0.1.3                    py36_0    defaults\r\nzlib                      1.2.11               h8395fce_2    defaults\r\nzope                      1.0                      py36_1    defaults\r\nzope.interface            4.6.0            py36he774522_0    defaults\r\nzstd                      1.3.3                hfe6a214_0    defaults\r\n\r\n***********************************************************************************************************\r\nSYSTEM INFO\r\nWindows 8.1 pro\r\n\r\nprocessor : intel(R) core(TM)-i5-4590cpu @3.30GHz  3.30GHz\r\nRAM : 8gb\r\nSYSTEM TYPE: 64-bit OS, X64-based processor ", "comments": ["Have you tried building Tensorflow with pip using the --upgrade tag? \"pip3 install tensorflow --upgrade\". It might be related to that. If not, I'll dig deeper to find what your problem might be.", "@Aashaybane Did you get a chance to try this? Did it solve your issue?\r\n\r\n> Have you tried building Tensorflow with pip using the --upgrade tag? \"pip3 install tensorflow --upgrade\". It might be related to that. If not, I'll dig deeper to find what your problem might be.\r\n\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24059, "title": "The performance of transpose_conv op in TensorFlow Lite", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n**Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**Linux Ubuntu 14.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n**Meizu 16th**\r\n- TensorFlow installed from (source or binary):\r\n**benchmark_model is built form source**\r\n- TensorFlow version (use command below):\r\n**master**\r\n- Python version:\r\n**Python 3.5.3**\r\n- Bazel version (if compiling from source):\r\n**0.16.1**\r\n- GCC/Compiler version (if compiling from source):\r\n**4.9.4**\r\n- CUDA/cuDNN version:\r\n**No**\r\n- GPU model and memory:\r\n**N/A**\r\n\r\n**Describe the current behavior**\r\nI wanted to deploy a trained model on Android devices with TensorFlow Lite, but it was quite slow.\r\nThen, I profiled the model(.tflite) with the benchmark_model tool, and found that the transpose_conv op took too much time. The summary by node type shown below:\r\n![screenshot from 2018-11-30 11 11 48](https://user-images.githubusercontent.com/21071150/49267677-8f39c600-f496-11e8-9e91-1da5dd9f4603.png)\r\n\r\nWhen I profiled the same model(.pb) used to convert to .tflite, I found that the transpose_conv achieving fast inference speed. The summary by node type shown below:\r\n![screenshot from 2018-11-30 11 04 33](https://user-images.githubusercontent.com/21071150/49269045-d9727580-f49d-11e8-8d4c-84e965350420.png)\r\n\r\nIt seems like the transpose_conv op in TensorFlow Lite is much slower than that in TensorFlow Mobile?\r\n", "comments": ["This issue is also tracking accuracy and correctness: https://github.com/tensorflow/tensorflow/pull/24151", "Thanks for the report, can you provide the exact build options you used to build `benchmark_model` from source? I'm also assuming this was single-threaded execution?", "@jdduke I build benchmark_model according to the document TFLite Model Benchmark Tool.(https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark)\r\nbazel build -c opt \\\r\n  --config=android_arm \\\r\n  --cxxopt='--std=c++11' \\\r\n  --copt=-DTFLITE_PROFILING_ENABLED \\\r\n  tensorflow/lite/tools/benchmark:benchmark_model", "Thanks. More recently we've been recommending \"--config=android_arm64\" to get the most accurate performance numbers, but I suspect the delta remains the same. We'll take a look, though I should note that it looks like TensorFlow Lite is still ~15-20% faster overall for this model?\r\n\r\nIt would also help if you could either send me the model (or a minimal repro) directly, or at least give the general sizes used here. Thanks!", "@jdduke The models shown here:\r\n[models.zip](https://github.com/tensorflow/tensorflow/files/2719886/models.zip)\r\nAnd I also found that time cost was quite different with execution within the Android app (tflite.run(imgData, outputArray)).\r\n", "> And I also found that time cost was quite different with execution within the Android app (tflite.run(imgData, outputArray)).\r\n\r\nCan you add some more details about this? How did the performance differ relative to running the native benchmark_model? Is your imgData a ByteBuffer or multi-dimensional array?", "@jdduke The model used to test the difference:\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/2753865/model.zip)\r\nThe imgData is ByteBuffer, and the outputArray is multi-dimensional array, according to the TFLite Java Demo. The timecost to run model inference is about 170ms.\r\nBut I profiled the model with the benchmark_model tool, the summary by node type shown below:\r\n![screenshot from 2019-01-14 10 23 38](https://user-images.githubusercontent.com/21071150/51094314-bbab6680-17e6-11e9-9f05-035cef2c7d57.png)\r\nThe timecost is about 62ms.\r\nI also test the experimental Android APK wrapper for the benchmark model, and the timecost is about 62ms.\r\nMaybe there is bug in tensorflow-lite.aar or benchmark_model tool. Please have a look.", "Are you using the same shape with the Java demo and benchmark_model? I'm not able to repro the large discrepancy in latency. You might also check the Java demo you build to make sure it's using the 64-bit libraries.", "@jdduke In my environment, I confirmed TF Lite is x3 - x10 slower than TF mobile under same model including transpose_conv op, same in/out shape condition on Windows 64bit / MacOS X / Android arm64.\r\nI will show the detail of the benchmark result within days.", "Keep in mind that TF mobile enables threading by default, whereas TF lite does not. Please test TF mobile single-threaded to form an accurate comparison.", "Thanks. I've already checked both single and multiple threading with C++ benchmark model tool (not Java demo).", "> @jdduke I build benchmark_model according to the document TFLite Model Benchmark Tool.(https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark)\r\n> bazel build -c opt \r\n> --config=android_arm \r\n> --cxxopt='--std=c++11' \r\n> --copt=-DTFLITE_PROFILING_ENABLED \r\n> tensorflow/lite/tools/benchmark:benchmark_model\r\n\r\n@ljdang \r\nwhen I run benchmark_model, avg time cost displayed, but other is empty like below.\r\nwould you help on this ?\r\n\r\n============================== Run Order ==============================\r\n                     [node type]          [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n\r\n============================== Top by Computation Time ==============================\r\n                     [node type]          [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [times called]  [Name]\r\n\r\nNumber of nodes executed: 0\r\n============================== Summary by node type ==============================\r\n                     [Node type]          [count]         [avg ms]          [avg %]         [cdf %]       [mem KB]      [times called]\r\n\r\n"]}, {"number": 24058, "title": "Implement async TensorFromTransportOptions for GDR", "body": "Instead of blocking on completion of an RDMA op, RecvTensor client will now post a work request to the NIC send queue and return immediately. \r\n\r\nThe GDR background polling thread will handle the callback after the corresponding RDMA op is completed, i.e. polled from the completion queue on NIC. The old epoll based mechanism is removed to trade higher CPU usage for improved throughput and lower latencies for RDMA ops.\r\n\r\nThe maximum numbers of work request (WR) in the send/recv queues on NIC are increased to entertain the increased number of concurrent RDMA ops. The threshold of tensor size below which we pass the tensor content in metadata is also increased to reduce the pressure to send/recv queues on NIC.\r\n\r\nThis fixes #23933.\r\n\r\nSigned-off-by: Bairen Yi <byronyi@clustar.ai>", "comments": ["@poxvoculi I was blocked on #23933 for the past week, and I am glad I can fix it now.\r\n\r\nI will go back to refactoring of networking plugins in the next couple of days.", "I've rebased to latest master and cleanup the code.\r\n\r\nNote that the checksum part is removed, as it is not guaranteed that the async RDMA client will read the same tensor content through network compared with gRPC. It turns out that sometimes the worker will RDMA read weights of the next steps from PS after the checksum is transmitted though transport_options (which was computed with stale weights on RecvTensor server). I am not sure it is a bug or not, but the same trick of sharing backed tensor_buffer could be found in gRPC as well, so I'll just leave it there. The original purpose of checksumming (i.e. debugging) should be achieved via some other approaches (e.g. unit testing).", "@harshini-gadige Mind to kick off Kokoro testing again?", "> @harshini-gadige Mind to kick off Kokoro testing again?\r\n\r\nDone", "@harshini-gadige Ready to pull? It\u2019d be great if I could make it into the 1.13 source release.\r\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "@googlebot CLA shall be fine. Let me double check.", "CLAs look good, thanks!\n\n<!-- ok -->", "@harshini-gadige Ready to pull? It\u2019d be great if I could make it into the 1.13 source release.", "> @harshini-gadige Ready to pull? It\u2019d be great if I could make it into the 1.13 source release.\r\n\r\n\r\n\r\nDone"]}, {"number": 24057, "title": "[Intel MKL]Fix mkl_softmax integration error when the input tensor is mkl format", "body": "Bugs of master:\r\nThe method to get input_dims is wrong when input is mkl tensor.\r\nThe layout_type is wrong when input is mkl tensor.\r\n\r\nThis PR fix these two bugs", "comments": ["Please fix the clang format issue. See details in \r\nhttps://source.cloud.google.com/results/invocations/71e83fef-47ab-4c32-adf2-d4dce01aa616/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_clang_format_out/log", "clang format is  fixed. Thanks", "Hi @penpornk @tatianashp , is this PR ready to merge? Thanks.", "@mpjlu Hi, I was waiting on your adding explanatory comments to the code, per this conversation.\r\nhttps://github.com/tensorflow/tensorflow/pull/24057#discussion_r239206554", "Hi @penpornk , the comments are added. Thanks. "]}, {"number": 24056, "title": "[INTEL MKL]Enable reorder cache for MklSlice.", "body": "Enable reorder primitive's reuse for MklSlice, please review.", "comments": ["@penpornk Thank you for your reply! I have modified these comments.", "Hi @penpornk , code modified.", "@pandaoxin Thank you for the changes. Could you please answer my questions from two days ago?\r\nhttps://github.com/tensorflow/tensorflow/pull/24056#discussion_r238932019\r\nhttps://github.com/tensorflow/tensorflow/pull/24056#discussion_r238933893", "@penpornk Thank you, question answered.", "Hi @penpornk , in primitive cache we mainly decouple computation and data. During primitive creation time, we use 'dummyData' to create the primitive. Actually, this 'dummyData' will not be used in computation, because the real data will be filled before real execution (implemented by set_data_handle). For the reason to set it back after execution, we want to make primitive in cache pool stateless so that no data is bounded. We also consider it safer, since that if the result for previous iteration is kept, problems of current iteration won't be thrown immediately, and wrong data would be reused.", "@penpornk Sure~"]}, {"number": 24055, "title": "[Intel MKL] Enable MKL LeakyRelu OP", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@penpornk I will update my code quickly, but for the cla, I already have it. I can confuse with the cla:no label, do you have any suggestion? Or can you help to check it?", "@guizili0 Ah. Sorry about the bot acting funny. It turns to `no` because there are multiple authors in the same PR. In this case, I think the other author is me because you accepted my suggestions. To conform to the message, please post here that you are okay with your commits being contributed to this project. After that I'll manually set the `cla` to `yes`. I'll post mine too:\r\n\r\nI'm okay with my commits being contributed to this project.", "@penpornk Thank you for the update, I have update my code by your comments, please help to review it again, thanks.", "@penpornk thanks for the quick response, updated.", "I'm okay with Penporn's commits being contributed to this project. thanks @penpornk with this comments.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@penpornk fixed, and seems that you need add the cla again. sorry about this.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@guizili0 This got a merge conflict, could you please help resolve the conflict? Thank you!", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@penpornk conflict has been fixed.", "@penpornk seems that you need set the cla again, sorry about this inconvenience.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@penpornk do you have any suggestion about  the GPU Python3 failure? all test broken.", "@guizili0 The only three failures I see are `Windows Bazel`, `Windows Bazel GPU`, and `Mac OS Python2 and CC`. I think all are existing failures so we should be fine."]}, {"number": 24054, "title": "Improving S3 File System", "body": "**Usability:**\r\n- Adds an environment variable TF_S3_LOG_LEVEL to control the logging information from S3 operations. \r\n  - By default, this level is set to 1, which only logs fatal errors. This removes the flood of unnecessary logs that are thrown when using S3 currently which stem from improper usage of AWS_CPP_SDK logging levels. \r\n  - User can change the level when necessary using the following values: 0 (OFF), 1 (FATAL), 2 (ERROR), 3 (WARNING), 4 (INFO), 5 (DEBUG), 6 (TRACE)\r\n\r\n**Reliability**\r\nS3 supports an [eventual consistency model](https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel), which does not work like a regular file system. This means that many operations currently in the s3 file system crash now and then because the conditions it expects are not satisfied by S3. The following changes tries to address such crashes. \r\n\r\n- _Skip usage of temp file when writing to S3:_\r\n  When writing a file to S3, the file system currently writes a temp file locally and syncs at the end by creating a file on S3. This coupled with the fact that S3 provides atomic uploads of files means that we will never see incomplete data uploaded for a file. In this scenario, the current usage of temp files when writing anything to the s3 file system (such as for Checkpoints, SavedModels) are unnecessary. I've removed such usage of temp files for S3 as they not just are unnecessary but cause issues with S3's consistency model. (It may so happen that after writing to a temp file, the file is not visible while moving to final location because changes haven't propagated yet. )\r\n\r\n- _Skip usage of temp directory when saving Checkpoints and SavedModels to S3:_ \r\n  When saving Checkpoints and SavedModels, there is also the use of a temp directory where all files are uploaded and then moved to the final location. This is even more problematic given S3's eventual consistency model. This sequence of operations involves the writing to temp location, listing of files in temp location, copying each file from this temp location to final location, and deleting the temp location. There are multiple issues users have experienced because of this sequence. When listing files in temp directory, in some cases not all files show up in the list. This causes some files to not be copied. While deleting directory, there is another listing of all files in the directory. Again this list may be inconsistent and the deleting of directory fails. So the usage of such temp directory has been bypassed for S3 file system.\r\n\r\n- The above two changes have been implemented by introducing a NeedsTempLocation method for the filesystem and exposing it to python through gfile similar to how IsDirectory works. This method needs to be used multiple places both in python and CPP. By default for any file system other than S3, it returns False, which allows us to skip the usage of temp locations.\r\n\r\n- The above issues for Checkpointing and SavedModels have been addressed and verified for both code paths of Estimator based training, Session based training as well as Keras. \r\n\r\n- _Retry failed operations before crashing:_\r\n  It is recommended to use retries to handle exceptions in the case of unexpected errors due to the eventual consistency paradigm. Moved the class retrying_file_system from `platform/cloud` to `platform/` and reused that for S3 file system.\r\n\r\n**Reliability and Performance**\r\n- _Use TransferManager from the AWS CPP SDK to upload files to S3_ \r\nUsing PUT for uploads as is done currently has a few limitations. It doesn't allow the upload of files larger than 5GB which in our experience, users have run into. It also doesn't retry in the case of crashes. Using the TransferManager allows us to upload larger files, upload files faster by using multi-part upload, and retry only the failed parts in case of crashes.\r\n\r\n\r\n\u00a0 | Time to upload 2GB | Time to upload 4GB | Time to upload 5.5GB\r\n-- | -- | -- | --\r\nOfficial Tensorflow r1.12 | 103 sec | 116 sec | Crash: Entity too large\r\nTensorflow 1.12 with S3 patch | 9 sec | 20 sec | 28 sec\r\nSpeedup | 11x | 6x | \u00a0\r\n\r\n\r\nI request the maintainers to review, and provide feedback. ", "comments": ["Can u please rebase and resolve the merge conflict? Thanks", "Sure, am on it. Looks like there was a change to the v2 API recently. Integrating that.", "I rebased the changes onto master. Please review.", "@rahul003 can you please resolve conflicts ", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 24053, "title": "Why not using TensorFixedSize on most const TTypes?", "body": "Hi im not too sure if this is a bug but why in TTypes.h, most of the tensor types that are 'Const' are not using TensorFixedSize? only a few are.\r\n\r\nApologies if this was not a bug.", "comments": ["I realized this is just how eigen works"]}, {"number": 24052, "title": "Fix warning in tf.count_nonzero", "body": "While running tf.count_nonzero the following warning shown up:\r\n```\r\nroot@ubuntu:/v# python\r\nPython 2.7.15rc1 (default, Nov 12 2018, 14:31:15)\r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.count_nonzero([0, 1])\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:1590: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\n<tf.Tensor 'count_nonzero/Sum:0' shape=() dtype=int64>\r\n>>>\r\n```\r\n\r\nThis fix fixes the warning\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yifeif  \"Ready to pull\" is not creating a gerrit link. Could you PTAL.", "Closing this PR as the changes are already present in the master branch as per my discussion with yifeif."]}, {"number": 24051, "title": "Upgrade Dockerfile assembler system", "body": "This is a big upgrade to the Dockerfile assembler I wrote a couple of\nmonths ago. The spec has changed, the script has been rewritten, and\nthere are new features throughout:\n\n- The assembler can build and upload images to Docker Hub.\n- The assembler can also run tests (!), although the testing system is\n  extremely rudimentary. It could be expanded with parallelism later, if\n  execution time becomes a problem.\n- spec.yml is totally different, and now defines both dockerfiles and\n  images. It handles the combinatorial explosion of multiple optional features\n  without excessive duplication, unlike the previous spec format.\n- Partials are the same, but I dumped the extensive dockerfile\n  documentation support because I don't think anyone would have used it.\n- Dockerfiles are handled under the same kind of system as images, which\n  is neat. The new Dockerfiles aren't so duplicated.\n- I've upgraded the images with new tensorflow tutorial files (jupyter\n  only) and fixed some others that didn't actually work.\n- I've improved the development documentation by suggesting aliases.\n- Added \"static-dockerfiles\" directory to track independent Dockerfiles.\n\nThese changes should better support changes like #23194.", "comments": ["The full suite of versioned images are available at https://hub.docker.com/r/angersson/tensorflow/tags/, e.g.:\r\n\r\n```bash\r\ndocker run -it --rm --runtime=nvidia -u $(id -u):$(id -g) angersson/tensorflow:zeus-gpu bash\r\n```", "Thanks for the review. I fixed the problems you noted, plus a few more small issues.", "I created a singularity image using this command:\r\n\r\n> singularity build angersson_gpu.simg docker://angersson/tensorflow:zeus-gpu-py3\r\n\r\nBut when I launch this script:\r\n\r\n```\r\nimport tensorflow as tf\r\nwith tf.device('/gpu:0'):\r\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n    c = tf.matmul(a, b)\r\nwith tf.Session() as sess:\r\n    print (sess.run(c))\r\n```\r\n\r\nit gives me this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/volume/test_gpu.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\n```", "@maystroh Are you using the nvidia-docker2 runtime to launch the GPU images? I don't know how singularity works, but that looks like the same kind of failure that occurs when you don't use `--runtime=nvidia`. ", "It works when I add --nv to my singularity command. Thanks @angersson "]}, {"number": 24050, "title": "Feature Request: Consolidate tf.keras get_updates_for with existing tensorflow update_ops", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF 1.12\r\n- Are you willing to contribute it (Yes/No): Potentially (not sure how complicated this is)\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, if `tf.keras` layers have update_ops (notably BatchNormalization), they are not added to `tf.GraphKeys.UPDATE_OPS`. Instead, we have to directly access them via `tf.keras.Model.get_updates_for`. However, for normal tf.layers implementations like https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization, these ops are directly added to `tf.GraphKeys.UPDATE_OPS` and this is cleanly documented. \r\n\r\nIs there a reason this can't be done with `tf.keras` models too? Or at least be much better documented?\r\n\r\n**Will this change the current api? How?**\r\nNo, we can keep the current API. But, this will allow users migrating from older implementations to tf.keras based models to maintain their update_ops control dependencies.\r\n\r\n**Who will benefit with this feature?**\r\nDevelopers will face less technical load in migrating to `tf.keras`, especially since this was not clearly documented in the `tf.keras.layers.BatchNormalization` page.\r\n\r\n**Any Other info.**\r\n", "comments": ["After looking at source code and some Github issues, it looks like this was an intentional design choice. Can this, at least, be better documented in the tf.keras docs? Especially since there are now examples of using tf.keras layers in custom estimators, where these update ops need to be handled.", "The global collections are going away in TF2.0, so I'm not sure what we could do here. Agree better doc would be good"]}, {"number": 24049, "title": "Fixed Typo in GO Readme", "body": "Hey, I was reading the [golang readme file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.md) and noticed a little typo in `Linux of macOS` which should probably be `Linux or macOS`. So I just fixed it.", "comments": []}, {"number": 24048, "title": "Fix discrepancy between tf.placeholder and tf.sparse.placeholder", "body": "When working on sparse placeholder I noticed that if shape is presented with `None` in any dims, the shape information is remvoed from sparse.placeholder. So `(None, 4)` will be `(None, None)`.\r\n\r\nThis is different from tf.placeholder:\r\n```\r\nroot@ubuntu:/v# python\r\nPython 2.7.15rc1 (default, Nov 12 2018, 14:31:15)\r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> v1 = tf.placeholder(tf.float32, (None, 4))\r\n>>> v1.get_shape()\r\nTensorShape([Dimension(None), Dimension(4)])\r\n>>> v2 = tf.sparse.placeholder(tf.float32, (None, 4))\r\n>>> v2.get_shape()\r\nTensorShape([Dimension(None), Dimension(None)])\r\n>>>\r\n```\r\n\r\nThis fix adds the fix so that tf.placeholder and tf.sparse.placeholder\r\nbehaves the same.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang gentle ping! Can you please address the changes requested? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 24047, "title": "Major memory leak after calling `tf.estimator.DNNClassifier.train` with certain `tf.train` optimizers.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Reproduced on OS X and Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not sure.\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.11 and 1.12 tested\r\n- Python version: 3.6.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: (tested on CPU)\r\n- GPU model and memory: (tested on CPU)\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen using the pre-canned `DNNClassifier` together with `tf.train.RMSPropOptimizer`, I noticed that a `tf.Graph()` instance is left allocated in memory _after_ an `estimator.train()` call terminates. When running `estimator.train()` in a loop N times, there will be N `tf.Graph()` instances allocated in memory. Therefore, calling `estimator.train()` in a loop will lead to unbounded memory growth, and eventually, an OOM error.\r\n\r\nTo track down the root cause of this behaviour, I have used the [`objgraph`](https://mg.pov.lt/objgraph/) dependency together with the [`iris premade estimator`](https://github.com/tensorflow/models/blob/master/samples/core/get_started/premade_estimator.py) example. The script is attached in the code section. After 5 iterations of `train()`, the following is a visualization of all the `tf.Graph` instances that exist in memory (a total of 5):\r\n\r\n![iter-4](https://user-images.githubusercontent.com/2341691/49243825-2cc3c580-f3c3-11e8-8909-6ef4d41a1f41.png)\r\n\r\nYou can clearly see that the root cause is a `_slots` attribute on `tf.train.RMSPropOptimizer` that saves a reference to some variable associated with the graph every time `train()` runs. \r\n\r\nI have noticed this behaviour with `tf.train.AdamOptimizer`, `tf.train.RMSPropOptimizer`, and `tf.train.AdadeltaOptimizer`. I would be willing to bet that most provided optimizers that save some state in the optimization procedure run into this same issue. I have observed that this issue does _not_ occur with optimizers that _do not_ save state (see below).\r\n\r\nI would also be willing to bet that this issue is not specific to `DNNClassifier` and occurs with many different pre-canned and custom estimators. However, I have not yet investigated reproducing this issue on other types of estimators. \r\n\r\n**Describe the expected behavior**\r\nThe expected behaviour is that the `tf.Graph()` instance used by an invocation of `train()` is garbage collected by the Python GC after `train()` terminates. I've observed this expected behaviour when using `tf.train.GradientDescentOptimizer` and `tf.train.AdagradOptimizer`. \r\n\r\n**Code to reproduce the issue**\r\n[memory-leak-example.zip](https://github.com/tensorflow/tensorflow/files/2630449/memory-leak-example.zip)\r\n\r\nThe above code is a modified version of the [`iris premade estimator`](https://github.com/tensorflow/models/blob/master/samples/core/get_started/premade_estimator.py) example to loop over `train()` multiple times. It requires the [`objgraph`](https://mg.pov.lt/objgraph/) to show the peak memory growth every iteration and generate an visualization of in-memory `tf.Graph` instances. \r\n\r\nHere is an example log provided of an example invocation: `python premade_estimator.py --optimizer=rmsprop`\r\n\r\n[rmsprop.log](https://github.com/tensorflow/tensorflow/files/2630465/rmsprop.log)\r\n\r\nNote the object growth result after the 5th iteration. As can be seen, 5 `tf.Graph` instances remain in memory, along with all the `Tensor`, `TensorShape`, `Dimension`, and other objects that go along with it.\r\n```\r\nObject growth after iteration 4\r\ntuple                         82229    +14171\r\nlist                          21693     +2862\r\ndict                          24251     +2475\r\nTensorShape                    2806      +561\r\nTensor                         2775      +555\r\nOperation                      2745      +549\r\n_InputList                     2735      +547\r\nDimension                      2455      +491\r\ncell                           6487      +312\r\nfunction                      31199      +218\r\nweakref                        5083       +25\r\nVariable                         95       +19\r\nSaveSliceInfo                    90       +18\r\nset                            1409        +8\r\nCondContext                      30        +6\r\nPartitionedVariable              30        +6\r\ntype                           3077        +5\r\nbuiltin_function_or_method     3143        +4\r\nOrderedDict                      48        +1\r\nCondition                        10        +1\r\ndeque                            11        +1\r\n_local                            8        +1\r\nGraph                             5        +1\r\nGroupLock                         5        +1\r\nTraceableStack                    5        +1\r\nScopedTFGraph                     5        +1\r\n_VariableScopeStore               5        +1\r\n_VariableStore                    5        +1\r\nSaver                             5        +1\r\nTFShouldUseWarningWrapper         5        +1\r\nBulkSaverBuilder                  5        +1\r\nVariableScope                     5        +1\r\nGraph written to /var/folders/rc/_2lrng15575d3bsd9t75br840000gn/T/objgraph-sejl5tm3.dot (276 nodes)\r\nImage generated as /tmp/iter-4.png\r\n```\r\n\r\n", "comments": ["Hi. What is the reason that you have to call estimator.train in a loop? I didn't read the code snippet (it would be easier if you could post it on github snippet)", "```\r\n\"\"\"An Example of a DNNClassifier for the Iris dataset.\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport objgraph\r\nimport tensorflow as tf\r\n\r\nimport iris_data\r\n\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--batch_size', default=100, type=int, help='batch size')\r\nparser.add_argument('--train_steps', default=1000, type=int,\r\n                    help='number of training steps')\r\nparser.add_argument('--num_train', default=5, type=int)\r\nparser.add_argument('--optimizer', default=\"adam\", type=str)\r\n\r\n\r\ndef main(argv):\r\n    args = parser.parse_args(argv[1:])\r\n\r\n    # Fetch the data\r\n    (train_x, train_y), (test_x, test_y) = iris_data.load_data()\r\n\r\n    # Feature columns describe how to use the input.\r\n    my_feature_columns = []\r\n    for key in train_x.keys():\r\n        my_feature_columns.append(tf.feature_column.numeric_column(key=key))\r\n\r\n    if args.optimizer == 'adam':\r\n        optimizer = tf.train.AdamOptimizer()\r\n    elif args.optimizer == 'adadelta':\r\n        optimizer = tf.train.AdadeltaOptimizer()\r\n    elif args.optimizer == 'adagrad':\r\n        optimizer = tf.train.AdagradOptimizer(learning_rate=0.01)\r\n    elif args.optimizer == 'rmsprop':\r\n        optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01)\r\n    else:\r\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n\r\n    # Build 2 hidden layer DNN with 10, 10 units respectively.\r\n    classifier = tf.estimator.DNNClassifier(\r\n        feature_columns=my_feature_columns,\r\n        # Two hidden layers of 10 nodes each.\r\n        hidden_units=[10, 10],\r\n        # The model must choose between 3 classes.\r\n        n_classes=3,\r\n        optimizer=optimizer)\r\n\r\n    print('Initial objects')\r\n    objgraph.show_growth(limit=50)\r\n\r\n    # Train the Model.\r\n    for i in range(args.num_train):\r\n        classifier.train(\r\n            input_fn=lambda: iris_data.train_input_fn(train_x, train_y,\r\n                                                      args.batch_size),\r\n            steps=args.train_steps)\r\n\r\n        print('Object growth after iteration {}'.format(i))\r\n        objgraph.show_growth(limit=50)\r\n        objgraph.show_backrefs(\r\n            objgraph.by_type('Graph'),\r\n            max_depth=10,\r\n            filename='/tmp/iter-{}.png'.format(i))\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    tf.app.run(main)\r\n```\r\n\r\nAs I wrote in the issue description, the code is exactly the [iris premade estimator](https://github.com/tensorflow/models/blob/master/samples/core/get_started/premade_estimator.py) example, except that I call `train()` multiple times and swap out a few different optimizers. \r\n\r\nI don't think it's unreasonable to want to call `train()` multiple times throughout the lifetime of a process without having to pay the overhead of reinitializing an Estimator. For example, a system that needs to train model for an epoch -> stop to do some evaluation, checkpointing, or inference and then dynamically decide whether to proceed to the next epoch based on some intermediate result. \r\n\r\nRegardless of whether you need to call `train()` multiple times in a process, **there remains a root issue here that an an anonymous `tf.Graph` instance persists in memory after `train()` has terminated**. Even if you only call `train()` once in a script, you will have a (sometimes very large) memory footprint in the process hanging around until it terminates. It seems evident to me that the anonymous `tf.Graph()` instance [1] initialized during the course of a `train()` call is intended to be garbage collected after it's use. Indeed, my example demonstrates that it _is_ garbage collected unless you are using any Optimizer instances that save state.\r\n\r\n[1] https://github.com/tensorflow/estimator/blob/16c5c2108f3359d92d9f20f1a1490dbe24caa0fb/tensorflow_estimator/python/estimator/estimator.py#L1140", "@tanzhenyu -- this hopefully won't be an issue with the 2.0 optimizers? Calling train in a loop is fairly standard for multiple epochs, and the fact that it happens only with RMSProp implies it's a bug that should be resolvable. Assigning to you to test with the new optimizers.", "@karmel It happens with more than just RMSProp -- I've reproduced it with AdamOptimizer and AdadeltaOptimizer as well. Based on this, I would guess it happens with any optimizer that maintains some state variable during training (e.g. momentum).", "@yoavz Karmel was referring to optimizers that currently reside in /python/keras/optimizer_v2. I think they shouldn't create the same issue, but I will test it given some time next week. If you could contribute to the testing that'd be great as well!", "It seems that there currently no solution for this bug,however there is a simple workaround for this bug:\r\nBy leveraging tensoflow's `warm start` support, we can \"split\" the total epochs we expect to run into serveral 'parts'. For each 'part' we start a subprocess,the real training will be carried in this subprocess ,and these subprocesses will be started one after another. And, with the help of `warm start`,\r\neach subprocess's training will proceed on with the result(parameters etc,) of the previous subprocess.\r\n\r\nFor exmple, if we want to run epochs of 62, and suppose each subprocess will at most run 20 epochs(of the total epochs of 100), then we can:\r\nstart subprocess1(it run epoches of 20)\r\nsubprocess1 finishes\r\nstart subprocess2(it run epoches of 20)\r\nsubprocess2 finishes\r\nstart subprocess3(it run epoches of 20)\r\nsubprocess3 finishes\r\nstart subprocess3(it run epoches of 2)", "Can you try 1) upgrading to latest version, preferably nightly, 2) using tf.keras.optimizers.RMSProp?", "I've got the same problem.", "Hi @yoavz !We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24046, "title": "Error compiling iterator_ops.cc when building from source with MSVC 2017", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 \r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: latest from git - 1.12.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: NVIDIA GeForce 940MX 4096mb\r\n\r\n\r\n\r\n**When building tensorflow from source using bazel on windows 10 I encountered an error in iterator_ops.cc.**\r\nTo the best of my ability I think the error that stopped the build was \r\n`\"binary '*': no operator found which takes a left-hand operand of type 'const tensorflow::OpInputList::Iterator' (or there is no acceptable conversion)\"`\r\nBut I'm not familliar with tensorflow or the building process being used.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build :loader (loader being a barebones .cc file with tensorflow as a dependancy)\r\n\r\n**Any other info / logs**\r\nThe relevant build logs: (somewhat long error message)\r\nhttps://pastebin.com/akqRxB0P", "comments": ["TensorFlow is tested against the Visual Studio 2015 Update 3. Can you please install the Visual Studio 2015 and try again?", "I have done so and still recieved an error find the logs here\r\nIt appears to be a different issue, but its hard for me to be sure. These logs are pretty cryptic :)\r\nhttps://pastebin.com/mFq0mTwH", "After some experimentation, Including failing at using the cmake option as well, I tried again with python 3.7 rather than my initial 2.7. I also followed the steps here: https://www.tensorflow.org/guide/extend/cc \r\nWhen building using `bazel run -c opt //tensorflow/cc/example:example` I got almost entirely to the end of the build - excellent. On the penultimate task (presumably the cpp exmaple file and not tensorflow?)\r\nIt failed with the following error (quite short)\r\nhttps://pastebin.com/ZhjuRYmc\r\n\r\nSomething about linkers it looks like - Is the tutorial setup incorrect?", "Python 3.7 is not officially supported version for TF 1.12. Adding gunan. ", "I'll give it a go with Python 3.6 and see how that goes. It takes over an hour so I'll have to get back to you", "I have completed the attempt with python 3.6. Still no luck - failed again right at the end of the build.\r\nThis is still following the tensorflow guidance now from: https://www.tensorflow.org/guide/extend/cc\r\n\r\nIt does in fact fail on the penultimate task - this is listed as `linking tensorflow/cc/example/example.exe`\r\n\r\nThe logs of the error with python 3.6 can be found here:\r\nhttps://pastebin.com/14fsGd5k", "I'm still having trouble with this - it's consistently failing at the linking step.", "@gunan Sorry to ping you- I hate to be a nag. I've been blocked by this issue for several days now. It would be useful to get an ETA of when you can take a look at this.", "Sorry for missing this issue before.\r\nWhat is the reason for building from sources? Could you try installing from prebuilt binaries?\r\nFor CPU version, it should be very straightforward.", "Are there prebuit libraries for C++ libs? Especially for use with visual studio if possible. I was under the impression that there was not", "At the moment I have resorted to piping the data over a network to a python server to get predictions that way. It would be far nicer to have it working in native c++ however.\r\n\r\nThe model is already trained and in a h5 file (keras) and a .pb (tensorflow) file - these work well already. I had just hoped to be able to import one of these using native cpp and make predictions using it. I had tried to follow any tutorial I had found for the last week or so.", "@gunan I have found my own way around this issue by using python make predictions and recieve input data over networking.\r\n\r\nHowever, it would still be good to see this fixed for future work", "Hi @RugnirViking !\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24046\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24046\">No</a>\n"]}, {"number": 24045, "title": "Docker container support for RTX 2070", "body": "**System information**\r\n- OS Platform and Distribution : I've tried two with the same result: Linux Ubuntu 16.04 & Linux Ubuntu 18.04\r\n- TensorFlow installed from docker hub :  tensorflow/tensorflow:latest-gpu-py3\r\n- TensorFlow version: 1.12\r\n- Python version: 3\r\n- Installed using docker : tensorflow/tensorflow:latest-gpu-py3\r\n- CUDA/cuDNN version: \r\nnvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\r\nCuda compilation tools, **release 9.0, V9.0.176**\r\n\r\n- GPU model and memory: dual RTX 2070 8gb\r\n\r\nI'm trying to set up TF through a docker image and I'm having problems getting TF working with the GPUs.  I get a segfault shortly after TF allocates memory space on the GPUs.  I'm thinking the drivers are the issue at this point.\r\n\r\nI would like to clarify the pre-requisites for the host machine to be capable of running the TF docker image.  The info on the [site](https://www.tensorflow.org/install/docker) seems pretty straight forward, but when you look at the detail, it becomes confusing (to me at least) and slightly contradictory.  There appears to be a discrepancy between what is stated in instructions, and where you end up if you follow the links in the instructions.  \r\n\r\nThe [pre-requisites](https://www.tensorflow.org/install/docker) for the docker image state _only the NVIDIA\u00ae GPU driver is required on the host machine (the NVIDIA\u00ae CUDA\u00ae Toolkit does not need to be installed)_.\r\n\r\nHowever, if you follow the links, you will end up on a page which instructions you to install the CUDA toolkit and CUDA, by implication making these a pre-requisite for using the TR docker container.\r\n\r\nThe hyperlink for 'NVIDIA\u00ae GPU driver' links to an [NVIDIA FAQ](https://github.com/NVIDIA/nvidia-docker/wiki/Frequently-Asked-Questions#how-do-i-install-the-nvidia-driver) which recommends you use your package manager to install the NVidia drivers (which is what I have done, using the PPA repository ([Instructions here](http://www.linuxandubuntu.com/home/how-to-install-latest-nvidia-drivers-in-linux)).  However, if you click the link behind _package manager_, you end up [here](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-installation) \r\n an NVidia site which details how to install CUDA, not the NVidia graphics drivers! Also, if you check the pre-requisites of the CUDA installation guide you will see you need to install the CUDA toolkit, which seems to contradict the statement at the start:\r\n\r\n_only the NVIDIA\u00ae GPU driver is required on the host machine (the NVIDIA\u00ae CUDA\u00ae Toolkit does not need to be installed)_\r\n\r\nSo do we need to install CUDA toolkit and CUDA and graphics drivers on the host?\r\n\r\nConfusingly, there also seems to be something called a 'CUDA driver' which I assume is something different from the NVidia driver.  The [NVidia instructions ](https://github.com/NVIDIA/nvidia-docker/wiki/Frequently-Asked-Questions#how-do-i-install-the-nvidia-driver) are about installing the 'CUDA driver' on the host machine, not the NVidia driver??\r\n\r\n_The recommended way is to use your package manager and install the cuda-drivers package (or equivalent)_\r\n\r\nPlease can you advise how I should set up the host machine to make it work with the TF docker images?\r\n\r\nThanks!", "comments": ["CUDA driver and nvidia driver are the same thing, just some people call it different things.\r\n\r\nWhat you need on the host is the nvidia driver. The CUDA toolkit belongs in the docker container. \r\n\r\n1. Install the nvidia driver\r\n2. Reboot\r\n3. Verify with `nvidia-smi` the driver is working on the host\r\n4. Install docker\r\n5. Install nvidia-docker\r\n6. Verify with `docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi` that nvidia is working inside of the container.\r\n\r\nIf all this works, you should be able to run tensorflow inside of a container.", "I've discovered my setup appears to work with the TF container version 1.10, but not with the later containers: TF 1.11 or 1.12.0.\r\n\r\nI tested with nvidia-smi and running ./deviceQuery and ./bandwidthTest from the CUDA examples on the host & from inside the container.  I've found that even if these commands work, TF will still segfault.  I test TF by running the Jupyter notebook that comes with the image: 3_mnist_from_scratch.ipynb .  This segfaults on this command (16th cell)\r\n\r\n\\# Run the graph and fetch some of the nodes.\r\n_, l, lr, predictions = s.run(\r\n  [optimizer, loss, learning_rate, train_prediction],\r\n  feed_dict=feed_dict)\r\n\r\nI've tried the follow drivers (I'm using dual RTX 2070 cards)\r\n\r\nDriver source [PPA](https://launchpad.net/~graphics-drivers/+archive/ubuntu/ppa):\r\n396 - Don't recognise my RTX 2070 (this was expected, wrong architecture)\r\n410 - At time of writing, this installs driver version 410.78. nvidia-smi, and cuda samples succeed in both host and container.  Segfaults in container TF 1.12.0, 1.11.0.  Succeeds in container TF 1.10.0\r\n415 - nvidia-smi, and cuda samples work in host and container.  Segfaults on TF 1.12.0.  Not tested any other TF version.\r\n\r\nI did install CUDA 9.2 on the host, but I don't know if that helped or made no difference.", "Do you have the stack trace of the segfault? That may help track down if there is already an issue opened for the segfault problem. (Since you got docker working, you might want to re-title the issue if your allowed to.) ", "The issue might be because the RTX card requires the CUDA 10 toolkit , and the TensorFlow Dockerfile includes CUDA 9.0 at this time. You might have to build TensorFlow from source.\r\n\r\nSee these issues:\r\nhttps://github.com/tensorflow/tensorflow/issues/22900\r\nhttps://github.com/tensorflow/tensorflow/issues/22706\r\n\r\nAlthough I can't explain why 1.10 would have worked and later versions would not have.", "Thanks William!  \r\n\r\nThe fact that TF is expected CUDA 9 and the RTX 2070 wants CUDA 10 explains a lot.  Like you say the only weird thing is how TF 1.10 works at all.\r\n\r\nI'm happy to wait for TF to support CUDA 10 as it looks like it's going to happen soonish #22706 "]}, {"number": 24043, "title": "Pruning: Multi-GPU support", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): modified models/official/resnet/resnet_model.py to utilize Pruning's masked layers.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.10.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 16.1\r\n- GCC/Compiler version (if compiling from source): gcc\r\n- CUDA/cuDNN version: 8/6\r\n- GPU model and memory: x2 nvidia titan Xp\r\n\r\n**Describe the current behavior**\r\nMulti-GPU training session of modified tensorflow/models for pruning, the session won't boot.\r\nThe same modified Resnet model _does_ execute iff num_gpus=1.\r\n\r\nThe error is:\r\nValueError: You must specify an aggregation method to update a MirroredVariable in Tower Context.\r\nRaised from tensorflow/contrib/distribute/python/values.py\", line 336\r\n\r\n[out_batch64.log](https://github.com/tensorflow/tensorflow/files/2629288/out_batch64.log)", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Sure. Just clone my fork of models @ https://github.com/barrh/models\r\n\r\nFrom models directory, call official/resnet/imagenet_main.py.\r\nMake sure to include the following flags:\r\n--rv=2 --sparse --num_gpus=2", "> Sure. Just clone my fork of models @ https://github.com/barrh/models\r\n> \r\n> From models directory, call official/resnet/imagenet_main.py.\r\n> Make sure to include the following flags:\r\n> --rv=2 --sparse --num_gpus=2\r\n\r\n@azaks2  Any thoughts?", "I meet the same problem.hope for answer", "I have the same problem. Occurs in any situation where I use MirroredStrategy with num_gpus > 1. ", "I guess it caused by custom optimization which need to specify an aggregation method. Premade optimization will be ok ", "Looks like this is using model_pruning from contrib. We are not guaranteeing distribution strategy will work with everything in contrib. \r\n\r\nAs @bupt1409  mentioned, this looks like a variable is being created which doesn't have the any aggregation type specified. Specifically in your stack trace it seems like this variable: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/model_pruning/python/pruning_utils.py#L57\r\n\r\nIf this variable is going to be updated in replica mode, then you must specify an aggregation type when creating the variable. Feel free to send a pull request if this fixes your case. I don't think we will be able to prioritize this. \r\n\r\n", "As the Contrib folder has been depreciated this request is no longer valid. Please check this latest documentation for pruning here https://www.tensorflow.org/model_optimization/guide/pruning . Thank you "]}, {"number": 24042, "title": "ModelCheckpoint TypeError: get_config() missing 1 required positional argument: 'self'", "body": "[comment]\r\nEpoch 00001: loss improved from inf to 103.67410, saving model to /tmp/weights.hdf5\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-12-a8915ea6c3f5> in <module>()\r\n     27 \r\n     28 #validation_data=(X_test,y_test),\r\n---> 29 model.fit(X_train,y_train,epochs=2000,batch_size=128,callbacks=[checkpointer])\r\n     30 \r\n     31 return_eval = model.evaluate(X_test,y_test,batch_size=128)\r\n\r\n\r\n- [error ] as  error show:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/generic_utils.py in serialize_keras_object(instance)\r\n    134     return {\r\n    135         'class_name': instance.__class__.__name__,\r\n--> 136         'config': instance.get_config()\r\n    137     }\r\n    138   if hasattr(instance, '__name__'):\r\n\r\nTypeError: get_config() missing 1 required positional argument: 'self'\r\n\r\n\r\n- [ code]  as I use code below:\r\n\r\ncheckpointer = ModelCheckpoint(filepath=\"/tmp/weights.hdf5\", verbose=1, monitor='loss',save_best_only=True)\r\n\r\nmodel.compile(#metrics=[mean_pred],\r\n    loss=keras.losses.mean_squared_error,    \r\n    optimizer=keras.optimizers.Adam(lr=0.001))\r\n\r\n#validation_data=(X_test,y_test),\r\nmodel.fit(X_train,y_train,epochs=2000,batch_size=128,callbacks=[checkpointer])\r\n\r\nalso with validation_data & val_loss has issue, plz help to check it. thanks~", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation."]}, {"number": 24041, "title": "Tensorboard embedding is not visible when filename contains 'plugins'", "body": "Tensorboard embedding is not visible when filename contains 'plugins'.\r\nWhile it works well after changing 'plugins' to other names any.\r\nAnd, btw, graph is showing well even with 'plugins' file name.", "comments": ["I can not find tensorboard source code in tensorflow git.\r\nSo i raised code for fixing the issue in tensorboard git.\r\nhttps://github.com/tensorflow/tensorboard/pull/1635\r\n"]}, {"number": 24040, "title": "TensorForest: the base random seed takes no effects", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary, pip install tensorflow==1.12.0\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: No usage\r\n- GPU model and memory: No usage\r\n\r\n**Describe the current behavior**\r\n\r\nI found that setting the same base_random_seed for TensorForest generate different classification performance.\r\n\r\n**Describe the expected behavior**\r\n\r\nSetting the **same** base_random_seed for TensorForest should generate the **same** classification performance so that we can reproduce every experiment.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport sklearn.datasets\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nX, y = sklearn.datasets.load_digits(return_X_y=True)\r\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\r\nx_train = x_train.astype(np.float32)\r\nx_test = x_test.astype(np.float32)\r\n\r\nparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(\r\nnum_classes=10, num_features=64, regression=False,\r\nnum_trees=50, max_nodes=1000, base_random_seed=2)\r\n\r\nclassifier = tf.contrib.tensor_forest.client.random_forest.TensorForestEstimator(params)\r\nclassifier.fit(x=x_train, y=y_train)\r\ny_out = classifier.predict(x=x_test)\r\n\r\nyyy = [x for x in y_out]\r\n\r\ny_hat = np.array([x['classes'] for x in yyy])\r\nprint(accuracy_score(y_test, y_pred=y_hat))\r\n# first run:  0.9629629629629629\r\n# second run: 0.968013468013468\r\n# third run:  0.9646464646464646\r\n```\r\n\r\nThe base random seed is non-zero, it's supposed to produce the same result.\r\nThus, I wonder where else does the code have randomness?\r\n@yupbank ", "comments": ["The code snippet you provided looks incomplete. Can you please provide a complete repro code? Thanks!\r\n", "@ymodak more complete : )", "@yupbank hi, yupbank, can i ask you a question, is the TensorForest slow on small datasets? I trained on adult dataset (32561 samples, 113 features) and set 250 trees, max_nodes=10000, the trainning will cost 1200+ seconds. For this small dataset, I think this time cost is so large... especially compared to scikit-learn implementation. Was it my mis-use? \r\n\r\n```\r\nx_train, x_test, y_train, y_test = load_adult()\r\n\r\nstart_time = time.time()\r\nest_args = {'num_classes': 2, 'num_features': 113, 'regression': False,\r\n            'num_trees': 250, 'max_leaf_nodes': 10000,\r\n            'base_random_seed': 0}\r\n\r\nparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(**est_args)\r\nestimator = tf.contrib.tensor_forest.client.random_forest.TensorForestEstimator(params)\r\nestimator.fit(x_train, y_train)\r\nprint(\"Time Cost: {}\".format(time.time() - start_time))\r\n\r\n# Time Cost: 1282s\r\n```", "hey... so there is already a hyperparam control the deterministic, try turn on `collate_examples=True`  in `ForestHParams` and the result would be reproducible ", "@yupbank oh, thank you very much. But i have another question, may i get your help ?\r\nhttps://github.com/tensorflow/tensorflow/issues/24040#issuecomment-446980933, besides this time performance issue, I encountered another situation.\r\nWhen I trained a TensorForestEstimator with 250 trees and max_nodes 10000, on a classification problem with num_classes=10, num_examples=4032930. I encounted a **ResourceExhaustedError**. as follows\r\n```\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[4032930,250,10] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n```\r\nmy machine: 64GB RAM and 12 cores.\r\n\r\nThus, I wonder, isn't it an online algorithm ? it seems that the memory consumption of the TensorForest is relatively large? How can I improve it ? \r\nThanks : )", "thanks for the feedback, do you have a example code for me to reproduce the error? and you are right, it's a streaming algorithm, so there shouldn't be any memory problem.\r\n\r\nand yes, i can take a look at the problem, but do you mind to create separate issues about the problems? so it's easier for people to share different piece of your valuable problems\r\n\r\nand i think we can close this issue? feel free to reopen it..   ", "> hey... so there is already a hyperparam control the deterministic, try turn on `collate_examples=True` in `ForestHParams` and the result would be reproducible\r\n\r\noh, what is the meaning of the hyperparam `collate_examples` ?\r\nI set it to `True`, but it does not work ... 5555\r\n\r\nMy arguments are:\r\n```\r\ntf.set_random_seed(0)\r\nx_train, x_test, y_train, y_test = load_adult()\r\n\r\nstart_time = time.time()\r\nest_args = {'num_classes': 2, 'num_features': 113, 'regression': False,\r\n            'num_trees': 2, 'max_nodes': 1000, 'collate_examples': True,\r\n            'base_random_seed': 0}\r\nhparams = ForestHParams(**est_args)\r\nestimator = ...\r\nestimator.fit(x_train, y_trian)\r\ny_out = estimator.predict(x_test)\r\n```\r\nThe `y_out` differs every runs. The final loss for TensorForest also differs.\r\n@yupbank ", "Weird... are your using tf1.12 too? I can\u2019t reproduce it \n\nSent from my iPhone\n\n> On Dec 15, 2018, at 22:16, Qiu Hu <notifications@github.com> wrote:\n> \n> hey... so there is already a hyperparam control the deterministic, try turn on collate_examples=True in ForestHParams and the result would be reproducible\n> \n> oh, what is the meaning of the hyperparam collate_examples ?\n> I set it to True, but it does not work ... 5555\n> \n> My arguments are:\n> \n> tf.set_random_seed(0)\n> #tf.logging.set_verbosity(tf.logging.INFO)\n> x_train, x_test, y_train, y_test = load_adult()\n> #x_train = x_train.reshape((60000, -1))\n> \n> start_time = time.time()\n> est_args = {'num_classes': 2, 'num_features': 113, 'regression': False,\n>             'num_trees': 2, 'max_nodes': 1000, 'collate_examples': True,\n>             'base_random_seed': 0}\n> hparams = ForestHParams(**est_args)\n> estimator = ...\n> estimator.fit(x_train, y_trian)\n> y_out = estimator.predict(x_test)\n> The y_out differs every runs.\n> @yupbank\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "yes\uff0c1.12\n\n\n\n\n\n\n\n\n----------------------------------------------------------\nBest Wishes !\n\u80e1\u6c42\uff08Qiu Hu\uff09\n\n\n2018-12-16\n\nOn 12/16/2018 16:45, Peng Yu wrote:\nWeird... are your using tf1.12 too? I can\u2019t reproduce it\n\nSent from my iPhone\n\n> On Dec 15, 2018, at 22:16, Qiu Hu <notifications@github.com> wrote:\n>\n> hey... so there is already a hyperparam control the deterministic, try turn on collate_examples=True in ForestHParams and the result would be reproducible\n>\n> oh, what is the meaning of the hyperparam collate_examples ?\n> I set it to True, but it does not work ... 5555\n>\n> My arguments are:\n>\n> tf.set_random_seed(0)\n> #tf.logging.set_verbosity(tf.logging.INFO)\n> x_train, x_test, y_train, y_test = load_adult()\n> #x_train = x_train.reshape((60000, -1))\n>\n> start_time = time.time()\n> est_args = {'num_classes': 2, 'num_features': 113, 'regression': False,\n> 'num_trees': 2, 'max_nodes': 1000, 'collate_examples': True,\n> 'base_random_seed': 0}\n> hparams = ForestHParams(**est_args)\n> estimator = ...\n> estimator.fit(x_train, y_trian)\n> y_out = estimator.predict(x_test)\n> The y_out differs every runs.\n> @yupbank\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.", "ah, you are right, i can only reproduce the results because my set the `max_nodes=10`", "i think the pr i linked would fix the problem, thanks for your feedback !", "Hi @whatbeg!, \r\nYou seem to be using older versions of Tensorflow . We recommend that you upgrade your code base version to 2.x  as contrib is removed from 2.x versions and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24040\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24040\">No</a>\n"]}]