[{"number": 6603, "title": "Allow dynamic summary names in new summary interface", "body": "This is a feature request connected with pull request #5558.\r\n\r\nThe new summary interface only allows Python strings as summary names, while the old one could use string tensors.\r\n\r\nThe new behaviour is considerably less flexible. Consider the case where you have train/devel/test data split, and you want to report summaries (`accuracy` for example) for each dataset individually (i.e., having them in three different graphs in TensorBoard). With the old summary interface, this can be accomplished nicely:\r\n```python\r\n  dataset_name = tf.placeholder(tf.string, [])\r\n  summary = tf.scalar_summary(dataset_name+\"/accuracy\", accuracy)\r\n...\r\n  s = session.run([summary], {dataset_name:\"train\" / \"devel\" / \"test\"})\r\n```\r\n\r\nWith the new interface, there has to be three summary nodes in the Graph to accomplish this, which makes the code repetitive and more complicated to maintain (you can create the three nodes with different name scopes, but you have to store them somewhere [a dictionary?], and if you want to use these nodes after restoring a metagraph, things become unellegant).\r\n\r\nOne possible solution is #5558  -- that pull request adds a `prefix` to all summary operations. The `prefix` is added to all summary names and can be a string tensor. That would allow reporting summaries for different datasets, while being applicable to the \"new summary approach\" of naming the summary operations according to the summary names (i.e., with the new approach it is not possible for `name` to be string tensor, but a `prefix` is fine).\r\n\r\nThe change #5558 is opt-in, so nobody interested in dynamic summary names is affected, while allowing much more flexible names to those interested, and I would like to see it merged.\r\n\r\nHowever, there could be other approaches how the problem could be solved. Therefore, I am opening this issue for discussion and finding the best solution.", "comments": ["Also see #6150, which describes related issue -- summaries cannot be named as another existing op in the graph. So for example, if there is an op `loss` in the graph, it is not possible to create summary with the same name.\r\n\r\nThe proposal in #6150 is to optionally pass the whole name of the summary (not just a prefix) as a string tensor. That would also solve my problem.", "Migrating this to https://github.com/tensorflow/tensorboard/issues/59"]}, {"number": 6602, "title": "fatal error: tensorflow/stream_executor/lib/status.h: No such file or directory", "body": "I try to write my own op and I have installed TensorFlow 0.12.0 with GPU support on Linux.\r\n\r\nThis code fails:\r\n```\r\n#include \"tensorflow/core/platform/stream_executor.h\"\r\n```\r\nWith error:\r\n```\r\nfatal error: tensorflow/stream_executor/lib/status.h: No such file or directory\r\n```\r\n\r\nThat files does not exists.\r\nSome more include files seem to be missing. When grepping for `DeviceMemory` in the include path, the only file it finds is `include/tensorflow/core/util/stream_executor_util.h`.\r\n", "comments": ["AFAIK when adding new ops, you need to install from sources:\r\nhttps://www.tensorflow.org/how_tos/adding_an_op/\r\n\r\nFrom your description, I think you maybe installed the pip package?\r\nWhen I look at our repo, both master and r0.12 branch has the file:\r\nhttps://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/core/platform/stream_executor.h\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/stream_executor.h\r\n\r\nSo I am not sure what you installed, or what the reason might be.\r\nPlease try installing from sources and if the problem persists, include more debugging information, detailing each step you took to reproduce the problem.", "It's possible our stream executor headers are not packaged in pip. I can take a look tomorrow", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L159 it looks like we're missing tensorflow/stream_executor there\r\n\r\nI'm not sure if we're missing anything else, unfortunately, but I can add that change.", "This is not complete yet. When I try to include `stream_executor.h`:\r\n```\r\n#include \"tensorflow/core/platform/stream_executor.h\"\r\n```\r\nI get:\r\n`/u/zeyer/.local/lib/python2.7/site-packages/tensorflow/include/tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory`\r\n", "I get the same error with `stream_executor.h`, when compiling a custom op with `nvcc` and 1.3.0 binaries:\r\n```\r\nIn file included from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/platform/default/stream_executor.h:26:0,\r\n                 from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/platform/stream_executor.h:24,\r\n                 from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/util/cuda_kernel_helper.h:26,\r\n                 from roi_align_gpu.cu.cc:8:\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory\r\n```\r\n\r\n", "Same problem here with 1.3.0 compiled from source when I try to compile a custom op. Does anyone have a workaround?", "@pronobis it seems like you're suggesting a workaround yourself in #12860: \r\n\r\n   Copying cuda_config.h to /site-packages/tensorflow/include/tensorflow/stream_executor/cuda solves the problem.\r\n\r\nAre you asking for something else?", "I suppose #12860 is there now to track this problem. Generally, I'm looking for a better permanent solution, but again, that's what #12860 is for.", "I wonder what the state is here? Wouldn't it be possible to just add `cuda_config.h` to the pip package? Wouldn't that solve the problem?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@av8ramit could you check if we have all the dependencies of stream_executor.h as a part of the pip package, except for cuda headers?", "@gunan, the dependencies are in the pip package", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "All the dependencies are in the pip package. ", "Folding all the duplicates into #15002"]}, {"number": 6601, "title": "Added absolute path expansion of parent directory check in saver.py, \u2026", "body": "\u2026so as to correctly handle saving files in current directory", "comments": ["Can one of the admins verify this patch?", "Pull request in response to:\r\nhttps://github.com/tensorflow/tensorflow/pull/3695#r94327053", "Is it possible to add a test for the corner case this is trying to fix?", "@rohan100jain Yes, but the test case passes on my OSX without the fix too... I assume it would fail on Windows and pass with the new check added, but don't have a Windows machine to really confirm that...\r\nI can add the test case, but someone else would have to run it to see it really behaves as expected - especially that it fails with current code. If I add the test case here in comments, can someone run it to make sure it really fails on Windows? Or should I maybe first commit the test case alone without fix, make sure it fails when google bot runs tests and then add the fix and check that passes?", "If you can create the test as a separate commit, I can help you get it tested both with and without your fix.", "@gunan \r\nOk, this was a bit more complicated than I expected. First of all please ignore commits before 8a44d67, I made silly mistakes there.\r\nCommit 8a44d67 adds a test that saves a graph to a file in current directory, using a path without \"./\" prepended.\r\nI expect this to fail on Windows with parent not found error as per https://github.com/tensorflow/tensorflow/pull/3695#r94327053. But what I didn't expect was that it would fail on OSX too - however not on saving the graph, but on restoring it. Adding a simple path expansion to body of Saver.restore(), commit b968f6cd592dffbd6c099290a2fced379c31cc8d, fixed that issue.\r\n\r\nAnd commit 690b4ee (hopefully) fixes issue with not being able to save to current directory on Windows when no \"./\" is used in front of the path.\r\n\r\nOn a side note - it takes ~30min of rebuilding and re-running tests on my machine whenever I make a simple change to Tensorflow source code. Am I missing some caching settings, or is that a pain everyone here goes through? I could had solved the whole issue in 15 mins if not such slow rebuild + testing cycles...", "Is this fixed?", "No, I still was not able to verify with the tests. Please assign to me and\nblock on me.\nSorry for the delay.\n\nOn Wed, Jan 11, 2017 at 12:47 PM, drpngx <notifications@github.com> wrote:\n\n> Is this fixed?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6601#issuecomment-271989023>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOQNWIaYM899xH8qGyI4BTYoNS4-uks5rRT_mgaJpZM4LZFT5>\n> .\n>\n", "Can this move forward? I wonder if it's related to the problems that we've seen with \"./\" @rohan100jain ", "OK, working on it right now.", "I just confirmed that the test case added, without the changes in saver.py do fail in windows.\r\nI am now testing if I can get the test passing with saver.py changes.\r\nIn the meantime,\r\nJenkins, test this please.", "And with the fix the test passes.\r\nWill proceed with the review.", "The issues should be unrelated, and fixed.\r\nJenkins, test this please.", "More flakes,\r\nJenkins, test this please.", "This change breaks saving checkpoints to any non-local filesystem. Let's revert?", "Let's also discuss a plan for addressing the issue referenced by this PR.\r\nMaybe we can add a method to wrap os.path.abspath, which checks if the path is local or remote and selectively converts the path?", "Hmm, possibly one reasonable fix is to add our own version of AbsPath() here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/path.h\r\n\r\nAnd expose it in gfile:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/gfile.py\r\n\r\nWe can't really rely on os.path.abspath() unfortunately, because we support arbitrary filesystems and use URIs for paths."]}, {"number": 6600, "title": "Remove deprecated array_ops", "body": "", "comments": ["@ilblackdragon Feel free to close this if you've done so internally. I expect there will be a lot of clean-up recently. "]}, {"number": 6599, "title": "memory leak in tensorflow_gpu 0.12.1", "body": "**Hi, We use tensorflow for training our OCR system models. I simply train models in tensorflow 0.9 and former versions. after some upgrade in cuda and tensorflow, I see large memory leak in our server with 32GB RAM.**\r\n\r\n### I have tried all of suggestions in [How to debug a memory leak in TensorFlow](http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow#t=201612280142239281993) and some other github issues and stackoverflow posts. No of them worked.\r\n\r\ntrain code:\r\n\r\n```\r\n#!/bin/env python\r\nimport tensorflow as tf\r\n\r\nimport Config\r\nimport Utilities\r\nfrom Dataset import Dataset\r\nfrom Model import Model\r\n\r\ndataset = Dataset()\r\n\r\nsess_config = tf.ConfigProto()\r\nsess_config.gpu_options.allow_growth = True\r\nsess = tf.Session(config=sess_config)\r\nimages, labels = dataset.train_images_labels()\r\nmodel = Model(images, labels, training=True)\r\ntf.train.start_queue_runners(sess=sess)\r\n\r\n\r\ndef main(global_step=0):\r\n    if global_step == 0:\r\n        init_op = tf.global_variables_initializer()\r\n        sess.run(init_op)\r\n    else:\r\n        checkpoint_path = Utilities.get_checkpoint_path(global_step)\r\n        model.saver.restore(sess, checkpoint_path)\r\n        Utilities.log_checkpoint_load(checkpoint_path)\r\n\r\n    loss = 0\r\n    train_iterations = global_step\r\n    while train_iterations < Config.train_max_iterations:\r\n        # train\r\n        l = model.train(sess)\r\n        loss += l\r\n        train_iterations += 1\r\n\r\n        # show train loss\r\n        if train_iterations % Config.display_intervals == 0:\r\n            loss /= float(Config.display_intervals)\r\n            Utilities.log_train_loss(train_iterations, loss)\r\n            loss = 0\r\n\r\n        # save checkpoint\r\n        if train_iterations % Config.checkpoint_intervals == 0:\r\n            checkpoint_path = model.saver.save(sess, Utilities.get_checkpoint_path(train_iterations))\r\n            Utilities.log_checkpoint_save(checkpoint_path)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\nimages, labels are queues from tf.train.shuffle_batch function. I used Graph.finalize() and I am sure no new operation added to graph in training because the size of stored models' files are equal. I guess the reason of leak is tensorflow queues.\r\n\r\n\r\n### Environment info\r\nUbuntu Server 14.04.5 LTS \r\n\r\nInstalled version of CUDA and cuDNN: \r\nCUDA V8.0.44, cuDNN V5.1.5.\r\n\r\ninstalled tensorflow from PyPI.\r\n1. sudo pip install --upgrade tensorflow_gpu.\r\n2. tensorflow 0.12.1.", "comments": ["It can be useful to do a memory profile using google [performance tools](http://goog-perftools.sourceforge.net/) ", "@yaroslavvb I use google-pprof and heap profile save in /tmp folder. it takes too long time to train so the profile only contain near 150 itrations. the files uploaded [here](https://www.dropbox.com/s/2r2pqs4l0bjar9h/tmp.zip?dl=1)", "if you combine it with pprof, you can get a graph and track down which function is responsible for allocating extra memory", "It doesn't show any function, this is the output of top30 command in pprof:\r\n```\r\n(pprof) top30\r\nTotal: 658.2 MB\r\n   194.3  29.5%  29.5%    194.3  29.5% 00007f8a8afae90e\r\n   172.2  26.2%  55.7%    172.2  26.2% 00007f8a8aedbae3\r\n    84.1  12.8%  68.5%     84.1  12.8% Eigen::internal::TensorExecutor::run::{lambda#1}@266d900\r\n    50.2   7.6%  76.1%     50.2   7.6% 00007f8a8aec2ffe\r\n    25.9   3.9%  80.0%     25.9   3.9% 00007f8a8af5377d\r\n    23.2   3.5%  83.5%     23.4   3.6% PyDict_Clear\r\n    14.2   2.2%  85.7%     14.2   2.2% 00007f8a8aec2778\r\n    10.5   1.6%  87.3%     10.5   1.6% 00007f8a8adef4c8\r\n     8.1   1.2%  88.5%      8.1   1.2% 00007f8a8af826d2\r\n     6.8   1.0%  89.6%      6.8   1.0% 00007f8a8af542a3\r\n     6.8   1.0%  90.6%      6.8   1.0% 00007f8a8af52fed\r\n     5.7   0.9%  91.4%    596.5  90.6% extra_lbits\r\n     4.2   0.6%  92.1%      4.2   0.6% 00007f8a8afad614\r\n     3.7   0.6%  92.7%      3.7   0.6% 00007f8ace0b7249\r\n     3.1   0.5%  93.1%      6.1   0.9% PyMem_Malloc\r\n     3.1   0.5%  93.6%      4.1   0.6% ecp_nistz256_precomputed\r\n     2.8   0.4%  94.0%      9.1   1.4% PyEval_EvalFrameEx\r\n     2.8   0.4%  94.4%      8.6   1.3% std::_Maybe_get_result_type@26ca280\r\n     2.4   0.4%  94.8%      2.5   0.4% PyBuffer_FillInfo\r\n     2.1   0.3%  95.1%      2.1   0.3% __GI___strdup\r\n     2.1   0.3%  95.4%      2.1   0.3% 00007f8a8adee7a7\r\n     2.0   0.3%  95.7%      2.3   0.4% __FRAME_END__\r\n     1.6   0.2%  96.0%      1.6   0.2% 00007f8a8af53762\r\n     1.3   0.2%  96.2%      1.3   0.2% std::_Bind@26bb1c0\r\n     1.2   0.2%  96.4%      1.2   0.2% 00007f8a8af433a1\r\n     1.2   0.2%  96.6%      1.3   0.2% std::_Bind@26c08c0\r\n     1.1   0.2%  96.7%      1.1   0.2% PyObject_RichCompareBool\r\n     1.0   0.2%  96.9%      1.8   0.3% std::_Weak_result_type@26a88a0\r\n     1.0   0.2%  97.0%      1.0   0.2% 00007f8a8aff9f73\r\n     1.0   0.2%  97.2%      1.0   0.2% 00007f8a8b01a2ea\r\n```\r\ngraph file uploaded [here](https://www.dropbox.com/s/mjo4kc17micbfw6/pprof3105.0.ps?dl=1), unfortunately I cant find a function is responsible for allocating extra memory", "That usage doesn't seem very high, why do you think there's a memory leak in queue? Memory usage can increase between versions without having a leak", "because I see 32 GB memory usage after 700000 iterations. I think the cause is data, I use tfrecord and tensorflow queue for data handling. I am sure that I hadn't leak in version 0.9.", "@zffchen78 do you have any idea why google perf tools shows \"29.5% 00007f8a8afae90e\" instead of symbol name in the memory dump? Does it mean this memory was allocated outside of tensorflow?", "@yaroslavvb the hex address shows up when the symbol lookup failed. Address to symbol name mappings are always very brittle. You can try to fiddle with build options to see if it helps. You need the frame pointers for instance, IIRC. If you gdb attach to it, then info symbol, then you might be able to reverse map it.", "I got exactly the same problem as you, I also use TFRecord and Queue to process data, and my main memory keeps growing until the system goes down...\r\nBy the way, I use Tensorflow 0.12.1, CUDA 8.0.44, CUDNN 5.1, CentOS 7", "Main memory as in CPU? Could you build with tcmalloc, then get a heap profile?", "To enable tcmalloc\r\n\r\n```\r\nsudo apt-get install google-perftools\r\nexport LD_PRELOAD=\"/usr/lib/libtcmalloc_and_profiler.so.4\"\r\n\r\n```", "Can this problem be solved in the next release?", "@carltonwang Could you give a reproducible example and profile as a picture/graph?\r\ncc @ebrevdo in case he has ideas", "Are you using dynamic or static rnn?", "I use static rnn.", "This is the simplified version of my code and data (in TFRecord), please run it on GPU...\r\nMy system is: CentOS 7, GTX 1070, CUDA 8.0.44, CUDNN 5.1, NVIDIA Driver 375.26\r\n\r\n[test.zip](https://github.com/tensorflow/tensorflow/files/752806/test.zip)\r\n", "Any idea about this problem?", "@carltonwang can you isolate your problem a bit more? IE, does it happen with latest version? Does it happen with tcmalloc? Can you run it with memory profiler and figure out who allocates all the extra memory?", "It happens on the latest version, with or without tcmalloc... ", "I implemented an encoding algorithm in my code. Today I just encapsulated this encoding method as an RNN cell, and then the memory leak disappeared!!!", "Closing since the problem is gone.  Happy to reopen if there is still an underlying bug to be fixed in TensorFlow, but it sounds like everything so far is inconclusive.", "@Mahdizade I met similar problem. \r\nMy program is similar to yours and after several epoch, 32G memory exhausted. I debugged the program and found each time the saver.save() function is run, the python process got a little bigger. I recommend you try this and see if the same thing happens. \r\n", "I have the same issue here. It goes overflowing after couple of iterations on version 1.0.1", "My issue is already exists in TF 1.0. @benwu232 Thanks for your comment. I will test your solution but I see the leak on my validation code. till now I cant find the problem. in my validation code leak happened only after the first load of graph parameters (checkpoint), after validate one checkpoint and load another checkpoint, I don't see any leak in htop. I will investigate more on this problem. I don't thing the saver is the cause, I see the steady leak on training, I save the checkpoint every 5000 iteration but leak exists in this intervals.", "Look at this code:\r\n`result = tf.contrib.layers.bias_add(inputs=tf.matmul(a=left, b=right), activation_fn=tf.nn.softmax)`\r\nIf I use \"softmax\" as the activation function, there will be a memory leak, but if I change the activation function to \"relu\" or \"softplus\" or \"sigmoid\", the memory leak will disappear.\r\nBy the way, this \"softmax\" is not the one used at the last layer for classification.\r\n\r\nPlatform: CentOS 7, x86_64\r\nTensorFlow version: 1.0_gpu", "Could you provide a complete test case that exhibits this behavior so we can reproduce this?", "After I update from TF 1.0.0 to TF 1.0.1, this problem disappear...", "looks like this was resolved with 1.0 release. Closing the issue."]}, {"number": 6598, "title": "Type Error", "body": "i have encounter an type error issue, and tried `import sys; reload(sys); sys.setdefaultencoding('UTF8')`.but i didn't work.\r\nhere is the detail\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so.8.0 locally\r\nE tensorflow/core/framework/op_kernel.cc:925] OpKernel ('op: \"NegTrain\" device_type: \"CPU\"') for unknown op: NegTrain\r\nE tensorflow/core/framework/op_kernel.cc:925] OpKernel ('op: \"Skipgram\" device_type: \"CPU\"') for unknown op: Skipgram\r\nTraceback (most recent call last):\r\n  File \"Model_Multi_Gpu.py\", line 371, in <module>\r\n    tf.app.run()\r\n  File \"/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"Model_Multi_Gpu.py\", line 355, in main\r\n    model = Model(train_scope,4)\r\n  File \"Model_Multi_Gpu.py\", line 85, in __init__\r\n    avaraged_grads = self.avg_grads()\r\n  File \"Model_Multi_Gpu.py\", line 91, in avg_grads\r\n    averaged_grads.append(tf.reduce_mean(grads_per_var))\r\n  File \"/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1329, in reduce_mean\r\n    _ReductionDims(input_tensor, axis, reduction_indices),\r\n  File \"/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1178, in _ReductionDims\r\n    return range(0, array_ops.rank(x))\r\n  File \"/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 342, in rank\r\n    return rank_internal(input, name, optimize=True)\r\n  File \"/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 362, in rank_internal\r\n    input_tensor = ops.convert_to_tensor(input)\r\n  File \"/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 645, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 710, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 441, in make_tensor_proto\r\n    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\r\n  File \"/home/rootuser/.virtualenvs/tfrc1py2/local/lib/python2.7/site-packages/tensorflow/python/util/compat.py\", line 65, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got <tensorflow.python.framework.ops.IndexedSlices object at 0x7fa82ab1cf50>\r\n\r\nwhat should i do to correct it? thanks a lot", "comments": ["What you have is a TensorFlow op, so maybe you can run it and get the value.", "This doesn't seem like a feature request or a bug in TensorFlow, could you repost this on stackoverflow instead\u00a0with tensorflow tag? That tag is monitored by TF team and many volunteers"]}, {"number": 6597, "title": "About the implementation of attention seq2seq decoder function", "body": "I am very pleased to hear the news that the attention decoder function is added recently to the master branch.\r\n\r\nHowever, the documentation of the function does not match with the actual implementation now.\r\n\r\n1) Clarification on 'luong' option\r\nInspecting the code briefly, I guess the paper that the 'luong' option referencing is the following paper:\r\nhttp://www.aclweb.org/anthology/D15-1166.\r\n- When we do not use the 'input feeding' approach, then the attention decoder is unneeded, since attention vectors depend only on the current hidden states. Thus, we can use the simple decoder and compute attention vectors afterwards.\r\n- When we use 'input feeding' approach, a computed attention vector should be fed to the next time-step's input. This is done by the added attention decoder with 'luong' option.\r\nI think the above information should be stated to avoid confusion.\r\n\r\n2) A possible bug on 'bahdanau' option\r\nWhen we use 'bahdanau' option, _init_attention function just creates a zero vector as an attention vector. However, according to the referencing paper (https://arxiv.org/pdf/1409.0473.pdf) an attention vector should be taken into consideration (i.e. non-zero) when computing the first hidden state. Thus, I think _init_attention should output an attention vector if attention option is designated as 'bahdanau'.\r\n\r\nPlease tell me if I understood wrong. Thanks!", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly."]}, {"number": 6596, "title": "[ Bug ] The 2nd Saver fails to recognize its Checkpoint State file.", "body": "**Operating System:** macOS Sierra\r\n\r\n**Steps to Reproduce:**\r\n1. In a session, create two Savers.\r\n2. Let one of the Saver save the variables.\r\n3. Let the other Saver save the variables in a different directory from the first Saver's destination directory.\r\n\r\n**Result:**\r\nAn Info message is displayed indicating that a Checkpoint State file does not exist (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L738). And TensorFlow creates a new Checkpoint State file. This happens every time the 2nd Saver saves the variables.\r\n\r\n**Expected Result:**\r\nThe 2nd Saver's Checkpoint State file should be recognized.\r\n\r\n**Code to Reproduce:**\r\n```\r\nwith tf.Session(graph=graph) as sess:\r\n    ...\r\n    saver_best = tf.train.Saver()\r\n    saver_hourly = tf.train.Saver(max_to_keep=None)  \r\n    ...\r\n    for i in range(max_step):\r\n        ...\r\n        if last_hourly_save + datetime.timedelta(hours=1) < datetime.datetime.now():                     \r\n            path_checkpoint_file = saver_hourly.save(sess, 'checkpoint_directory/hourly/model', global_step=i, latest_filename='hourly_checkpoint')\r\n        ...\r\n        if best:\r\n            path_checkpoint_file = saver_best.save(sess, 'checkpoint_directory/best/model', global_step=i, latest_filename='best_checkpoint')\r\n        ...\r\n```", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 6595, "title": "update license to 2017", "body": "Happy New Year!", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 6594, "title": "http://zlib.net/zlib-1.2.8.tar.gz no longer available", "body": "I'm attempting to install tensorflow 0.12.1 from the r0.12 branch from source.\r\n\r\nzlib has been updated from 1.2.8 to 1.2.9 so it appears the link must be updated from:\r\nhttp://zlib.net/zlib-1.2.8.tar.gz\r\n\r\n\r\n**edit**: per later comments, this has been moved to http://zlib.net/fossils/zlib-1.2.8.tar.gz\r\n\r\n~~one solution may be to update to:~~\r\n~~http://zlib.net/zlib-1.2.9.tar.gz~~\r\n\r\nhere is the error I'm getting:\r\n```\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n........\r\nERROR: /home/ahundt/src/tensorflow/tensorflow/core/BUILD:970:1: no such package '@zlib_archive//': Error downloading [http://zlib.net/zlib-1.2.8.tar.gz] to /home/ahundt/.cache/bazel/_bazel_ahundt/beca172f341045bf57b6baf5296669b3/external/zlib_archive/zlib-1.2.8.tar.gz: GET returned 404 Not Found and referenced by '//tensorflow/core:lib_internal'.\r\nERROR: /home/ahundt/src/tensorflow/tensorflow/core/BUILD:970:1: no such package '@zlib_archive//': Error downloading [http://zlib.net/zlib-1.2.8.tar.gz] to /home/ahundt/.cache/bazel/_bazel_ahundt/beca172f341045bf57b6baf5296669b3/external/zlib_archive/zlib-1.2.8.tar.gz: GET returned 404 Not Found and referenced by '//tensorflow/core:lib_internal'.\r\nERROR: Evaluation of query \"deps((//tensorflow/... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\r\n```\r\n\r\nIn addition to the breakage fix, perhaps it would also make sense to make a change that would prevent future breakage of this sort?", "comments": ["For those wanderers with the same problem it can be fixed changing `tensorflow/tensorflow/workspace.bzl` from \r\n\r\n```\r\nnative.new_http_archive(\r\n      name = \"zlib_archive\",\r\n      url = \"http://zlib.net/zlib-1.2.8.tar.gz\",\r\n      sha256 = \"36658cb768a54c1d4dec43c3116c27ed893e88b02ecfcb44f2166f9c0b7f2a0d\",\r\n      strip_prefix = \"zlib-1.2.8\",\r\n      build_file = str(Label(\"//third_party:zlib.BUILD\")),\r\n)\r\n```\r\n\r\nTo\r\n\r\n```\r\nnative.new_http_archive(\r\n      name = \"zlib_archive\",\r\n      url = \"http://zlib.net/zlib-1.2.9.tar.gz\",\r\n      sha256 = \"73ab302ef31ed1e74895d2af56f52f5853f26b0370f3ef21954347acec5eaa21\",\r\n      strip_prefix = \"zlib-1.2.9\",\r\n      build_file = str(Label(\"//third_party:zlib.BUILD\")),\r\n)\r\n```", "This fix does not work for me. Instead, both zlib-1.2.8.tar.gz and zlib-1.2.9.tar.gz are moved under http://zlib.net/fossils/  So it needs to be \r\n\r\n```\r\nnative.new_http_archive(\r\n      name = \"zlib_archive\",\r\n      url = \"http://zlib.net/fossils/zlib-1.2.8.tar.gz\",\r\n      sha256 = \"36658cb768a54c1d4dec43c3116c27ed893e88b02ecfcb44f2166f9c0b7f2a0d\",\r\n      strip_prefix = \"zlib-1.2.8\",\r\n      build_file = str(Label(\"//third_party:zlib.BUILD\")),\r\n)\r\n```\r\n", "Suggested options for a permanent fix: \r\n\r\n1. The github release could be used for the relevant files, which @madler of zlib himself conveniently provides https://github.com/madler/zlib/releases, perhaps releases here can be consistently expected in the future?\r\n2. propose to zlib that they have a permalink available on zlib.net (or find/switch to it if it already exists)\r\n3. As the stewards of tensorflow, I assume google has, and they may want to use their own googley options and may want to use them. :-)\r\n4. Enable or make use of existing support for check more than one url location for redundancy which should be safe if the hash is correct and secure (i.e. provide both the normal http://zlib.net/zlib-1.2.8.tar.gz and fossils http://zlib.net/fossils/zlib-1.2.8.tar.gz path, and maybe even the github https://github.com/madler/zlib/archive/v1.2.8.tar.gz one too)\r\n5.  option to be avoided: increment releases automatically (would likely eventually cause many people  pain similar to this issue if there was an unexpected regression or breakage)\r\n6. ?\r\n7. Profit!\r\n\r\n:-) ", "There is already a permanent link for the current version: http://zlib.net/current/zlib.tar.gz .", "Linking to the current version means that builds might not be repeatable.  (Of course, unexpected 404s lead to non-repeatable builds, too.)", "I think this has been fixed in the master branch already:\r\n`urls = [\r\n          \"http://bazel-mirror.storage.googleapis.com/zlib.net/zlib-1.2.8.tar.gz\",\r\n           \"http://zlib.net/zlib-1.2.8.tar.gz\",\r\n      ],`\r\n(although the second url is now dead).  ", "@madler thanks for your reply! Unfortunately, a permanent link to the ever changing current version would make it difficult for a user to return to a version they previously had installed in an automated fashion if the link downloads different things now from what it downloads in the future. Plus bazel (and many other build tools) also require a hash so I think with the link you specified the breakage in this github issue would just recur the next time the version changed.\r\n\r\n@ParkerLowrey ah if the bazel-mirror link is in master then I bet if a google developer can cherry-pick that change to the r0.12 branch it would be fixed!", "As already noted, all of the versions are in http://zlib.net/fossils/ , including the current version.", "@madler perfect! Thanks again for taking the time to give feedback.\r\n\r\nNow with the bazel-mirror and knowing that the fossils links are permalinks I believe #6612 will resolve this if merged into r0.12.", "By the way, the current version is 1.2.10.", "Are any of the code suggestions working for anyone? I tried the [code suggested by seungryulchoisc](https://github.com/tensorflow/tensorflow/issues/6594#issuecomment-270007654) and the code in [ahundt's  commit](https://github.com/ahundt/tensorflow/commit/d1e6fec4a07a873c8c939a8b5991726c9b3f2a30?diff=split)\r\n\r\nBut i still get the following errors when trying to compile from source on an Amazon P2 instance using `TF_UNOFFICIAL_SETTING=1 ./configure`\r\n\r\n```\r\nERROR: /home/ubuntu/tensorflow/tensorflow/core/BUILD:970:1: no such package '@zlib_archive//': BUILD file not found on package path and referenced by '//tensorflow/core:lib_internal'.\r\nERROR: /home/ubuntu/tensorflow/tensorflow/core/BUILD:970:1: no such package '@zlib_archive//': BUILD file not found on package path and referenced by '//tensorflow/core:lib_internal'.\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/png_archive/BUILD:6:1: no such package '@zlib_archive//': BUILD file not found on package path and referenced by '@png_archive//:png'.\r\nERROR: Evaluation of query \"deps((//tensorflow/... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\r\n```\r\n\r\nI have tried using different versions of Bazel, 0.4.3, 0.4.2, and 0.3.2 (0.3.2 doesn't like \"urls\", so i made it a single \"url\" when testing with 0.3.2). \r\n\r\n", "Simple fix that has been working for me for days:\r\n\r\n- edit `tensorflow/workspace.bzl`\r\n- replace `zlib-1.2.8` with `zlib-1.2.10`\r\n- remove the `sha256` line\r\n\r\nor use this patch:\r\n```\r\ndiff --git a/tensorflow/workspace.bzl b/tensorflow/workspace.bzl\r\nindex 06e16cd..7c7b44c 100644\r\n--- a/tensorflow/workspace.bzl\r\n+++ b/tensorflow/workspace.bzl\r\n@@ -228,9 +228,8 @@ def tf_workspace(path_prefix = \"\", tf_repo_name = \"\"):\r\n \r\n   native.new_http_archive(\r\n     name = \"zlib_archive\",\r\n-    url = \"http://zlib.net/zlib-1.2.8.tar.gz\",\r\n-    sha256 = \"36658cb768a54c1d4dec43c3116c27ed893e88b02ecfcb44f2166f9c0b7f2a0d\",\r\n-    strip_prefix = \"zlib-1.2.8\",\r\n+    url = \"http://zlib.net/zlib-1.2.10.tar.gz\",\r\n+    strip_prefix = \"zlib-1.2.10\",\r\n     build_file = str(Label(\"//:zlib.BUILD\")),\r\n   )\r\n```", "Patch from https://github.com/adamcrume/tensorflow-rust/commit/84cc47389188e6fa2cabf3b27c886514632918d5 helped me to configure for 0.11 build.", "I'm still getting this error after the changes from @beniz \r\n\r\nI get the error when following along with this tutorial https://www.tensorflow.org/how_tos/image_retraining/ . I'm using the `TFAMI.v3 - ami-0e969619` AMI\r\n\r\n> ubuntu@ip-172-31-35-11:~/tensorflow$ bazel build tensorflow/examples/image_retraining:retrain\r\n> ERROR: /home/ubuntu/tensorflow/tensorflow/core/BUILD:1017:1: no such package '@zlib_archive//': Error downloading [http://zlib.net/zlib-1.2.10.tar.gz] to /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/zlib_archive/zlib-1.2.10.tar.gz: GET returned 404 Not Found and referenced by '//tensorflow/core:lib_internal'.\r\n> ERROR: Analysis of target '//tensorflow/examples/image_retraining:retrain' failed; build aborted.\r\n> INFO: Elapsed time: 5.315s\r\n\r\nMy release is 0.12.0\r\n\r\n```\r\nubuntu@ip-172-31-35-11:~/tensorflow$ grep \"Release\" -m 1 RELEASE.md\r\n# Release 0.12.0\r\n```\r\n\r\nIn `tensorflow/workspace.bzl` I've placed the following:\r\n```\r\n\r\n  native.new_http_archive(\r\n    name = \"zlib_archive\",\r\n    url = \"http://zlib.net/zlib-1.2.10.tar.gz\",\r\n    strip_prefix = \"zlib-1.2.10\",\r\n    build_file = str(Label(\"//:zlib.BUILD\")),\r\n  )\r\n```\r\n\r\n\r\n\r\n\r\n\r\n", "The below works. Thanks @ahundt for updating the **edit**\r\n\r\n\r\n```\r\n  native.new_http_archive(\r\n    name = \"zlib_archive\",\r\n    url = \"http://zlib.net/fossils/zlib-1.2.10.tar.gz\",\r\n    strip_prefix = \"zlib-1.2.8\",\r\n    build_file = str(Label(\"//:zlib.BUILD\")),\r\n  )\r\n```", "Thanks @Omnipresent\r\n\r\nAfter applying your edit to workspace.bzl,I got:\r\n**Prefix zlib-1.2.8 was given, but not found in the archive and referenced by '//tensorflow/core:lib_internal'.**\r\n\r\nChanging the entry as follows worked for me:\r\n```  \r\n    native.new_http_archive(\r\n        name = \"zlib_archive\",\r\n        url = \"http://zlib.net/fossils/zlib-1.2.10.tar.gz\",\r\n        strip_prefix = \"zlib-1.2.10\",\r\n        build_file = str(Label(\"//:zlib.BUILD\")),\r\n    )\r\n```\r\nI am using TFAMI.v3 (ami-52bb0c32)", "@skullkey I have modified workspace.bzl as per your changes, now I get the below error\r\n\"ERROR: /tensorflow/core/BUILD:853:1: no such package '@zlib_archive//': Unable to load package for //:zlib.BUILD: not found. and referenced by '//tensorflow/core:lib_internal'.\"\r\n\r\nI am using bazel-0.4.0 binaries build from sources.\r\n\r\nAny idea how to fix it?\r\n\r\nThanks\r\nMadhu\r\n"]}, {"number": 6593, "title": "MonitoredSession Implementation [awaiting response]", "body": "I am trying to solve simple arithmetic operations on 4 separate devices which I was successful in doing. Now, I want to implement fault tolerance in my program in the event that one of the node fails. I am using the tf.train.MonitoredTrainingSession class to achieve the desired results. I am not confident about the implementation in my program as I am not getting a favorable output. Below is the program as far as I could get:-\r\n\r\nPlease help\r\n```\r\nimport tensorflow as tf\r\n\r\nglobal_step_tensor = tf.Variable(10, trainable=False, name='global_step')\r\n\r\ncluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\",\"localhost:2224\", \"localhost:2225\"]})\r\nx = tf.constant(2)\r\n\r\nwith tf.device(\"/job:local/task:0\"):\r\n    y1 = x + 300\r\n\r\nwith tf.device(\"/job:local/task:1\"):\r\n    y2 = x**2\r\n\r\nwith tf.device(\"/job:local/task:2\"):\r\n    y3 = 5*x\r\n\r\nwith tf.device(\"/job:local/task:3\"):\r\n    y0 = x - 66\r\n    y = y0 + y1 + y2 + y3\r\n\r\nChiefSessionCreator = tf.train.ChiefSessionCreator(scaffold=None, master='localhost:2222', config='grpc://localhost:2222', checkpoint_dir='/home/chaitanya/tensorflow/codes/checkpoints')\r\nsaver_hook = tf.train.CheckpointSaverHook(checkpoint_dir='/home/chaitanya/tensorflow/codes/checkpoints', save_secs=10, save_steps=None, saver=y, checkpoint_basename='model.ckpt', scaffold=None)\r\nsummary_hook = tf.train.SummarySaverHook(save_steps=None, save_secs=10, output_dir='/home/chaitanya/tensorflow/codes/savepoints', summary_writer=None, scaffold=None, summary_op=y)\r\n\r\nwith tf.train.MonitoredTrainingSession(master='localhost:2222', is_chief=True, checkpoint_dir='/home/chaitanya/tensorflow/codes/checkpoints', \r\n    scaffold=None, hooks=[saver_hook, summary_hook], chief_only_hooks=None, save_checkpoint_secs=10, save_summaries_steps=None, config='grpc://localhost:2222') as sess:\r\n\r\n    while not sess.should_stop():\r\n        sess.run(model)\r\n\r\n    while not sess.should_stop():\r\n        print(sess.run(y0))\r\n        print('\\n')\r\n\r\n    while not sess.should_stop():\r\n        print(sess.run(y1))\r\n        print('\\n')\r\n\r\n    while not sess.should_stop():\r\n        print(sess.run(y2))\r\n        print('\\n')\r\n\r\n    while not sess.should_stop():\r\n        print(sess.run(y3))\r\n        print('\\n')\r\n\r\n    while not sess.should_stop():\r\n        result = sess.run(y)\r\n        print(result)\r\n```\r\nI have also posted this on stackoverflow:-\r\n\r\nhttps://stackoverflow.com/questions/41478027/tf-train-monitoredtrainingsession-arguments", "comments": ["Would ` RPCOptions rpc_options = 13` in [config.proto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto) help me in anyway? How can I implement it?\r\n\r\n", "The master config syntax should be `master='grpc://localhost:2222'` instead of `master='localhost:2222'`\r\n\r\nCurrent error to tackle:- \r\n`TypeError: Using a tf.Tensor as a Python bool is not allowed. Use if t is not None: instead of if t: to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n`", "Closing here: stackoverflow is the right place for this discussion."]}, {"number": 6592, "title": "About the implementation of recently added attentional seq2seq decoder function", "body": "Hello, I heard a good news that the attentional seq2seq decoder function is recently added in the master branch.\r\nIt is a very pleasing news to me, since nowadays I have been struggling to implement the feature.\r\n\r\nHowever, while reviewing the detailed implementation, I found some unclear points.\r\n\r\n~~Firstly, in line 395-397 of attention_decoder_fn.py,\r\n`attention_query` and `context` are concatenated and passed to a fully connected layer to be projected to a vector whose size is `num_units`.\r\nAs far as I know, the current implementations of RNN cells already do this procedure.\r\nFor example, the current implementation of `LSTMCell` (line 337-338 in core_rnn_cell_impl.py) concatenates `inputs` and `m_prev`, and multiplies the concatenated vector by `(input_dim + num_units, 4 * num_units)` weight matrix.\r\nThus, to avoid multiplying `inputs` with weight matrices two times, I think line 396-397 should be removed.~~\r\nReading codes again, I found I understood them wrongly.\r\nAnd, I also think `concat_v2` function should be used to concatenate two vectors.\r\n\r\n~~Secondly, in line 122-123 and 288-289, I see that `cell_output` is used as `attention_query` parameter of `attention_construct_fn`.\r\nThis is correct if we choose 'Luong-style' alignment calculation, where the computation path is \"h_t -> a_t -> c_t -> tilde h_t\", however this seems to be problematic if we choose 'Bahdanau-style' alignment calculation, where the computation path is \"h_(t-1) -> a_t -> c_t -> h_t\".\r\nFor now I can't think of a simple way to correct this behavior since the style of attention mechanism is unknown in decoder_fn's scope, but I think it should be dealt with anyway.~~\r\n\r\nPlease tell me if I read the codes wrongly! :)\r\n\r\nI am a bit confused with the exact meaning of luong and bahdanau method. The both options use h_(t-1), unlike the referencing paper.\r\nI close this, and will make a new issue after organizing my brain! ", "comments": []}, {"number": 6591, "title": "WideNet in examples/learn  throws error on UBUNTU 16.04 LTS", "body": "Am running TensorFlow (0.11.0rc0) On Ubuntu 16.04 LTS installed via pip \r\n\r\nI was going through the guides at  https://www.tensorflow.org/tutorials/wide_and_deep/  I thought that my code had an error, but then running the example script at  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py \r\n\r\nAlso throws the same error is the problem my environment? the script or Ubuntu 16.04 I haven't tried it in 14.04.\r\n\r\n`Training data is downloaded to /tmp/tmp1SYKCv\r\nTest data is downloaded to /tmp/tmpWhX5FI\r\nmodel directory = /tmp/tmp9TzI1B\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"mean\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:Change warning: default value of `enable_centered_bias` will change after 2016-10-09. It will be disabled by default.Instructions for keeping existing behaviour:\r\nExplicitly set `enable_centered_bias` to 'True' if you want to keep existing behaviour.\r\nWARNING:tensorflow:Using default config.\r\nTraceback (most recent call last):\r\n  File \"widenet.py\", line 208, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"widenet.py\", line 204, in main\r\n    train_and_eval()\r\n  File \"widenet.py\", line 197, in train_and_eval\r\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 333, in fit\r\n    max_steps=max_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 660, in _train_model\r\n    features, targets = input_fn()\r\n  File \"widenet.py\", line 197, in <lambda>\r\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\r\n  File \"widenet.py\", line 159, in input_fn\r\n    for k in CATEGORICAL_COLUMNS}\r\n  File \"widenet.py\", line 159, in <dictcomp>\r\n    for k in CATEGORICAL_COLUMNS}\r\nTypeError: __init__() got an unexpected keyword argument 'dense_shape'`", "comments": ["Could you see if the same problem exists in 0.12.1?", "Yeah it works.. thanx..", "Sorry, issue persists in 0.12.1", "Hi, as  commented at https://github.com/tensorflow/tensorflow/issues/6648#issuecomment-270806057, please try example file on v0.12 branch.", "works..  thanx @nagachika "]}, {"number": 6590, "title": "tf.nn.softmax fails with dim > 0", "body": "If I do this:\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n    session = tf.InteractiveSession()\r\n    logits = np.array([[1.0,2.0,4.0],[1.0, 1.0, 1.0]])\r\n\r\n    tf.nn.softmax(logits,dim=1).eval()\r\n\r\nI get:\r\n`InvalidArgumentError (see above for traceback): Requires start <= limit when delta > 0: 2/1\r\n\t [[Node: range_9 = Range[Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](range_9/start, Sub_6, range_9/delta)]]`\r\n\r\nBut both of these work:\r\n\r\n    tf.nn.softmax(logits.T, dim=0).eval().T\r\n    tf.nn.softmax(logits, dim=-1).eval()\r\n\r\nThose are what I would expect to get from the first one with `dim=1`.\r\n\r\nI am using '0.12.0-rc1'\r\n", "comments": ["stick to what works.", "Documentation is confusing. It says the default of `dim=-1` means `dim` is the `last dimension`. So  would expect `dim=1` for a tensor with shape=(2,) to be the same as default, but it gives different result. @yuefengz can you comment?", "came across with exactly the same problem. tf.__version__ = 1.0.1", "This is already fixed."]}, {"number": 6589, "title": "tensorflow.python.framework.errors_impl.NotFoundError: logs", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone\r\n\r\n### Environment info\r\nOperating System:\r\nMac OS X El Capitan\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\nUsed steps shown here with docker\r\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#3\r\n\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI am following all the steps in this tutorial\r\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#3 but I am getting the following error\r\n\r\n`COMPUTER_NAME:tf_files USERNAME$ docker run -it -v $HOME/tf_files:/tf_files  gcr.io/tensorflow/tensorflow:latest-devel\r\nroot@e6a49799067c:~# cd /tensorflow/\r\nroot@e6a49799067c:/tensorflow# git pull\r\nremote: Counting objects: 1444, done.\r\nremote: Compressing objects: 100% (719/719), done.\r\nremote: Total 1444 (delta 693), reused 490 (delta 490), pack-reused 221\r\nReceiving objects: 100% (1444/1444), 1.36 MiB | 469.00 KiB/s, done.\r\nResolving deltas: 100% (753/753), completed with 239 local objects.\r\nFrom https://github.com/tensorflow/tensorflow\r\n * [new branch]      fix-dbpedia -> origin/fix-dbpedia\r\n   8308ecd..fa4ba83  master     -> origin/master\r\n   9381242..1092f91  r1.0       -> origin/r1.0\r\n * [new tag]         0.12.1     -> 0.12.1\r\nAlready up-to-date.\r\nroot@e6a49799067c:/tensorflow# python tensorflow/examples/image_retraining/retrain.py \\ --bottleneck_dir=/tf_files/bottlenecks \\ --how_many_training_steps 500 \\ --model_dir=/tf_files/inception \\ --output_graph=/tf_files/retrained_graph.pb \\ --output_labels=/tf_files/retrained_labels.txt \\ --image_dir /tf_files/flower_photos\r\n>> Downloading inception-2015-12-05.tgz 100.0%\r\nSuccessfully downloaded inception-2015-12-05.tgz 88931400 bytes.\r\nLooking for images in '.git'\r\nNo files found\r\nLooking for images in 'logs'\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 1012, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 757, in main\r\n    FLAGS.validation_percentage)\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 148, in create_image_lists\r\n    file_list.extend(gfile.Glob(file_glob))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 269, in get_matching_files\r\n    compat.as_bytes(filename), status)]\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: logs`\r\n\r\n### What other attempted solutions have you tried?\r\nTried install tensorflow 0.11 in docker but that didn't help\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["The issue seemed to have been with my retrain statement. I changed it to this and it worked\r\n\r\n`python tensorflow/examples/image_retraining/retrain.py --bottleneck_dir=/tf_files/bottlenecks --model_dir=/tf_files/inception --output_graph=/tf_files/retrained_graph.pb --output_labels=/tf_files/retrained_labels.txt --image_dir /tf_files/flower_photos`", "python -c \"import tensorflow; print(tensorflow.__version__)\"\r\n0.12.1", "Looking for images in '.git'\r\nNo files found\r\nLooking for images in 'refs'\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 1012, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 757, in main\r\n    FLAGS.validation_percentage)\r\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 148, in create_image_lists\r\n    file_list.extend(gfile.Glob(file_glob))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 269, in get_matching_files\r\n    compat.as_bytes(filename), status)]\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: refs", "^Getting a similar error. Retrain command seems fine. Have you managed to resolve this?", "Getting same error.", "It was months ago I got this. Run me through what you're trying to do, maybe I can help. @avi-jain @TonySnark ", "Thanks easye. I have a feeling it's a file structure problem...\r\n\r\nI cd to a models-master dir. In there, I have a tf_files dir with my image folders.\r\n\r\nI run Retrain.py from models-master, and it works fine. Spits out /bottlenecks /inception /retrained_labels.txt and /retrained_graph.pb in the tf_files folder.\r\n\r\nAgain from models-master, I run label_image.py as it is on \"tensor flow for poets\" tutorial:\r\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets/?utm_campaign=chrome_series_machinelearning_063016&utm_source=gdev&utm_medium=yt-desc#5\r\n\r\nerrors below:\r\n`(tensorflow) Benjamins-Mac-Pro:models-master tonysnark$ python3 label_image.py /Users/tonysnark/Downloads/20158069_10104845333160530_2508081536327318535_o.jpg\r\nTraceback (most recent call last):\r\n  File \"label_image.py\", line 15, in <module>\r\n    in tf.gfile.GFile(\"retrained_labels.txt\")]\r\n  File \"label_image.py\", line 14, in <listcomp>\r\n    label_lines = [line.rstrip() for line \r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 213, in __next__\r\n    return self.next()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 207, in next\r\n    retval = self.readline()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 176, in readline\r\n    self._preread_check()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 78, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512, status)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: retrained_labels.txt\r\n`", "even I am facing same issue:\r\n File \"object_detection/create_flower_tf_record.py\", line 216, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"object_detection/create_flower_tf_record.py\", line 195, in main\r\n    examples_list = dataset_util.read_examples_list(examples_path)\r\n  File \"/home/rashmi/models/object_detection/utils/dataset_util.py\", line 59, in read_examples_list\r\n    lines = fid.readlines()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 181, in readlines\r\n    self._preread_check()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 78, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512, status)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: annotations/trainval.txt\r\n", "Can someone help me out with this problem?\r\n\r\nTraceback (most recent call last):\r\n  File \"export_inference_graph.py\", line 106, in <module>\r\n    tf.app.run()\r\n  File \"/home/foss/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"export_inference_graph.py\", line 99, in main\r\n    text_format.Merge(f.read(), pipeline_config)\r\n  File \"/home/foss/anaconda3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 118, in read\r\n    self._preread_check()\r\n  File \"/home/foss/anaconda3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 78, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512, status)\r\n  File \"/home/foss/anaconda3/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/home/foss/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: training/to/ssd_inception_v2.config\r\n", "I am facing same issue, any update ?", "I am facing same issue, any update ?", "Guys, it has been months. Has anyone found a solution or quit tensorflow?", "@advancedits @EAZYE9000 @avi-jain @TonySnark @drashmi29 ", "@hamzamk Can you share the error which you are getting? It will be easy to figure it out.", "@onlyak \r\n**What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?**\r\nhttps://github.com/tensorflow/models/issues/2246\r\n\r\n**Environment info**\r\n\r\nEnvironment:\r\nUbuntu 16.04\r\nCUDA 8.0 and 6.0, TF 1.4.0, Python 3.5\r\n\r\nI am using conda virtual environment, below are my paths from the bashrc file:\r\n#added by me for protobuf\r\nexport PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\r\n\r\n#To the Ob folder \r\nexport MODELS=/home/hamza/anaconda3/envs/tensorflow/models/research\r\nexport PYTHONPATH=$MODELS:$MODELS/slim\r\nexport OBJ_DET=$MODELS/object_detection\r\n\r\n#further i add for the custom training of mobilenet\r\nexport PYTHONPATH=$OBJ_DET\r\nexport PYTHONPATH=$OBJ_DET/training\r\n\r\n\r\nErrors are the follow:\r\n\r\nwhen i enter the command\r\npython3 train.py --logtostderr --train_dir=training/ --pipeline_config_path=~/Desktop/ssd_mobilenet_v1_pets.config\r\n\r\nor \r\n\r\npython $OBJ_DET/train.py --logtostderr --train_dir=training --pipeline_config_path=training/ssd_mobilenet_v1_pets.config\r\ni get the following error:\r\n\r\ntensorflow.python.framework.errors_impl.NotFoundError: ~/Desktop/ssd_mobilenet_v1_pets.config; No such file or directory\r\n\r\nthe file ssd_mobilenet_v1_pets.config is in training folder\r\n", "I am facing same issue...please any one help me to solve this issue...", "Hi Everyone,\r\n\r\nWith regards to :- \r\n\r\ntensorflow.python.framework.errors_impl.NotFoundError: training/to/ssd_inception_v2.config\r\n\r\nMake sure in your terminal you are not in /training path as if you are already in /training folder than edit path to /to/ssd_inception_v2.config\r\n\r\nThanks\r\n\r\n\r\n\r\n", "> Hi Everyone,\r\n> \r\n> With regards to :-\r\n> \r\n> tensorflow.python.framework.errors_impl.NotFoundError: training/to/ssd_inception_v2.config\r\n> \r\n> Make sure in your terminal you are not in /training path as if you are already in /training folder than edit path to /to/ssd_inception_v2.config\r\n> \r\n> Thanks\r\n\r\nI don't understand you description on this issue. It is not really clear to anyone"]}, {"number": 6588, "title": "Fixes #6549: Added missing keep_checkpoint_every_n_hours flag", "body": "Fixes #6549", "comments": []}, {"number": 6587, "title": "1% -> 5% in comment", "body": "The download reports every 5% not every 1%.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it! ", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks @mbasilyan!"]}, {"number": 6586, "title": "Crash in  Jupyter Notebook with conv2d - stream->parent()->GetConvolveAlgorithms(&algorithms)", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI have googled for the issue on several fronts and found nothing close since the Windows release.\r\n\r\n### Environment info\r\nOperating System:\r\nWindows 7-64bit.  Intel i3-6100 (64-bit), 8 gb DDR4, nVidia GTX 750 ti 2gb, \r\n\r\nnVidia GPU driver: 369.30\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nCUDA v8.0\r\ncuDNN 5.1 \r\n\r\njupyter notebook messages when importing tensorflow:\r\nsuccessfully loaded:\r\ncublas64_80.dll\r\ncudnn64_5.dll\r\ncufft64_80.dll\r\nnvcuda.dll\r\ncurand64_80.dll\r\n\r\nFound device 0 with properties:\r\nname: GeForce GTX 750 Ti\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.1105\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.65GiB\r\nDMA:0\r\n0: Y\r\nCreating TensorFlow device ... \r\n\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nI used the default: pip install tensorflow as described on their website.\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.12.0-rc0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI built and tested this jupyter notebook WITHOUT gpu support.  It already builds and runs correctly (as in, have trained and optimized a network).  Now I'm trying to get it to work with gpu to speed it up.\r\n\r\nHere's the step in the notebook that causes the issue.  Prior to this, tensorFlow is imported successfully as tf.  This is the first session it's running.  the 'training_operation' uses conv2d.\r\n`\r\n##run Training\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    num_examples = len(X_train)\r\n    \r\n    print(\"Training...\")\r\n    print()\r\n    for i in range(EPOCHS):\r\n        X_train, y_train = shuffle(X_train, y_train)\r\n        for offset in range(0, num_examples, BATCH_SIZE):\r\n            end = offset + BATCH_SIZE\r\n            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\r\n            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\r\n            \r\n        validation_accuracy = evaluate(X_validation, y_validation)\r\n        print(\"EPOCH {} ...\".format(i+1))\r\n        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\r\n        print()\r\n        \r\n    saver.save(sess, './lenetTrafficSign')\r\n    print(\"Model saved\")`\r\n\r\n\r\n### What other attempted solutions have you tried?\r\nI saw a similar issue in a pre-windows version that suggested it had something to do with memory.\r\nI tried adding the following before the tensorflow session:\r\n`config = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())`\r\n\r\nIt did not resolve the issue.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nThe error is as follows, when trying to run the session:\r\n![image](https://cloud.githubusercontent.com/assets/7866171/21579570/f25e4a0c-cf7d-11e6-8136-1f8b681e3dcd.png)\r\n\r\n\r\nThe plain text at the last three lines are:\r\ncuda_event.cc:49]Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED\r\ngpu_event_mgr.cc:190] Unexpected Event Status: 1\r\ncuda_dnn.cc:305] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\ncuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nconv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n\r\nA few seconds after the script crashes, the screen goes black and the video driver resets.\r\n", "comments": ["Can you provide a reproducible example? (ie, the model) This error can happen when you give 0 dimensions to conv2d", "I'm sure it's not the issue.  The code works fine on Windows using CPU.  Since I've posted this, I've installed Ubuntu and the same code, with no changes, runs just fine on tensorflow with GPU support.", "I have the exact same problem on a MacBook using its internal GPU (macOS 10.12, Xcode 8.2 overridden by Command Line Tools 7.3.1). I have CUDA 8.0, cuDNN 5.1, and Tensorflow 0.12.1 installed via `pip install tensorflow-gpu`. \r\n\r\nThe first error is either `CUDNN_STATUS_INTERNAL_ERROR` or `CUDNN_STATUS_NOT_INITIALIZED`. In the latter case, an additional message `error retrieving driver version: Invalid argument: expected %d.%d or %d.%d.%d form for driver version; got \"\"` is output. Tensorflow is imported normally, but Python crashes when I run a session that uses convolutional neural networks (CNN). \r\n\r\nAdditionally, I tried compiling Tensorflow from the source, but the same error persists. I also tried uninstalling CUDA 8.0 and installing CUDA 7.5, and reinstalled cuDNN and Tensorflow. The problem persists. This is very annoying.", "@mrry this seems to be Windows-specific. Any ideas?", "Can you try upgrading TensorFlow to the latest release version (0.12.1) and upgrading your drivers to the latest version (376.33, as far as I can tell), then see if the problem persists?\r\n\r\n", "@ymfa It sounds like you might be experiencing a different problem with the Mac OS GPU support. Can you please open a separate issue? ", "Is it possible this is the same issue as #6509 ?", "This seems to be related to the issue as #6509 as @gunan mentioned. \r\n\r\n1) I had the same problem then I changed the **tf.one_hot()** to a custom solution as mentioned by @name-name in #6509, the problem went away. May be it might work for this.", "You can also upgrade your TF version to 1.0.0rc1 to resolve the problem.", "It's tied to the tf.one_hot() issue.", "> It's tied to the tf.one_hot() issue.\r\n\r\n@danbergeland did you solve the problem ? ", "@ricardobnjunior  I believe it was resolved with release version 1.0.  I no longer have this setup to verify, as this was 2 years ago.  "]}, {"number": 6585, "title": "Thank You TensorFlow Team! ", "body": "Special thanks to all TensorFlow engineers for your incredible work this year. Maintaining and developing so successfully a project of this magnitude isn't an easy work. I've personally always seen great attention to the issues and contributions made with an also prompt, helpful and nice attitude and believe the community is very grateful for all you've done this year and the excellence of your work. Can't wait for what is to come in 2017!\r\n\r\n\r\n", "comments": ["I just want to know what's the time of the v1.0 release and what kind of changes made to it"]}, {"number": 6584, "title": "SKCompat gives error when used with DNNClassifier", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nWas not able to find any that apply.\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nMac\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nTried on:\r\n\r\n0.12.1\r\n0.12.0-rc0\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nI would like to use SKCompat with a DNN classifier, but it generates an error.  A real easy way to see this is to run: \r\n\r\nhttps://www.tensorflow.org/tutorials/tflearn/#load_the_iris_csv_data_to_tensorflow\r\n\r\nIf I run the above example, exactly as it is, I get this warning:\r\n\r\n```\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\n```\r\n\r\nSo I follow the advice, and add this line of code, just after the DNNClassifier is created.\r\n\r\n`classifier = tf.contrib.learn.SKCompat(classifier)`\r\n\r\nThis results in the following error.  How do I use SKCompat with this example?  I would like to use the standard sklearn interface for this application.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test3.py\", line 35, in <module>\r\n    steps=2000)\r\n  File \"/Users/jeff/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1126, in fit\r\n    loss = self._estimator._train_model(\r\nAttributeError: 'DNNClassifier' object has no attribute '_train_model'\r\n\r\n```\r\n### What other attempted solutions have you tried?\r\n\r\nExamined source code for tensorflow.  It appears that SKCompat is trying to call a function () that DNNClassifier does not have.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["@ispirmustafa  You are reworking currently all this things. Probably good to add tests that DNNClassifier and other estimators work nicely with SKCompat.", "@ilblackdragon @ispirmustafa Are you working on removing the deprecation warnings? ", "@ilblackdragon @ispirmustafa @michaelisard Is this issue fixed. Kindly comment.", "@ilblackdragon Just ran into this \"AttributeError\" today. Tried to upgrade the tensorflow package but seems like the issue still exists... If anyone knows any work around and would like to share, it will be highly appreciated. Thanks in advance.", "Same problem for me today (with a version of tf installed 3 days ago using conda).\r\nAny update?", "Also still running into this problem.", "Same here", "Same issue here when replicating the \"cnn_mnist.py\" tutorial example :\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py", "I've tested with DNNClassifier/Regressor. It's working with the head. Could you please try with latest and provide me what is not working? The original issue was resolved with the later releases.", "See: https://github.com/tensorflow/tensorflow/blob/7e9f2dc91f724e807d849a4f8fdd6aa6dacbd4d8/tensorflow/docs_src/get_started/tflearn.md", "That link is not related with SKCompat. I'm closing this thread. If you see SKCompat related issues, please re-open it. \r\nThanks", "I tried with the latest and no longer see this issue.", "use tf.contrib.learn.DNNClassifier"]}, {"number": 6583, "title": "Branch 143288671", "body": "", "comments": []}, {"number": 6582, "title": "Incompatible state size between LSTMCell and LSTMBlockCell", "body": "Hello,\r\n\r\nI heard that the implementation of LSTMBlockCell is faster than that of LSTMCell, so I am trying to convert my codes using LSTMCell to use LSTMBlockCell.\r\nHowever, I found one weird point: the types of state_size of the two are not compatible.\r\nSpecifically, LSTMCell's state_size is of type LSTMStateTuple and LSTMBlockCell's state_size is of type tuple.\r\nAs far as I know, tuple and LSTMStateTuple are quite similar but the latter handles an internal memory state and a hidden state in a more abstract way.\r\n\r\nI know the separated use of tuple and LSTMStateTuple is quite useful; for example when I have to detect whether a cell is MultiRNNCell (where state_size is of type tuple) or a cell is LSTMCell (where state_size is of type LSTMStateTuple).\r\nThus, I think the current behavior of LSTMBlockCell should be corrected to use LSTMStateTuple as a state type.\r\nIt can be easily modified if there is no backward compatibility issue, and I will send a pull request if nobody is working on this now.", "comments": ["@ebrevdo what do you think?", "Looks like an oversight.  The block implementation should use the same tuple object.  Will try to fix next week."]}, {"number": 6581, "title": "Bazel\uff1acommand not found", "body": "I have install the bazel v0.3.2 per the instruction and it works well on the terminal.but when I was running ./configure, it comes with a result of \"Bazel :command not found\"", "comments": ["COuld you post more information about what is the operating system, how was bazel installed, also run these commands from your terminal and post the outputs:\r\n\r\n```\r\npython -c \"import tensorflow; print(tensorflow.__version__)\"\r\ngit rev-parse HEAD\r\nbazel version\r\n```\r\n\r\nIf you are using Linux or Mac OS, also post this:\r\n\r\n```\r\necho  ${PATH}\r\n./configure\r\n```\r\n\r\nI am not sure what is the equivalent for `PATH` in Windows", "This looks like a problem with your installation of bazel.\r\nNot really a TensorFlow issue.\r\n\r\nPlease make sure in the same terminal you are running `./configure`, you can run `bazel version`.\r\nYou might need to add the location of bazel binary to your path.\r\n\r\nSince this is a bazel installation issue, I am closing it. Please make sure to follow bazel installation documentation to make sure it is installed properly."]}, {"number": 6580, "title": "`models` directory disappeared in the latest commit", "body": "The documentation refers to `models/image/mnist/convolutional.py`, however it doesn't exist anymore. It exists in `r0.12`, but not `master`. In fact, the whole `models` directory is missing. If restructuring is going on, I think the documentation has to be updated.\r\n\r\n[Here is the documentation that refers to it](https://github.com/tensorflow/tensorflow/blob/2e22f1b20/tensorflow/g3doc/get_started/os_setup.md#train-your-first-tensorflow-neural-net-model)", "comments": ["Forgot to clarify, the `master` under question is 2e22f1b20", "All models have been moved to `github.com/tensorflow/models` repository.\r\nSome documentation is still left to be fixed, but we are working on them\r\n\r\n@nealwu ", "So is it a separate repo now?\r\n\r\nIf there is a push towards abstraction and modularization, shouldn't other folders become separate entities? \r\n\r\nFor example\r\n - [Tensorboard](https://github.com/tensorflow/tensorflow/tree/2e22f1b20fdfa77b1332c518617391dc32359c5b/tensorflow/tensorboard) could be a submodule, because although it is related, it more of a tool FOR TF rather than an integral part of it\r\n- [Examples](https://github.com/tensorflow/tensorflow/tree/2e22f1b20fdfa77b1332c518617391dc32359c5b/tensorflow/examples) this is also not necessary for correct operation of TF, and could be moved out (same as `models`).\r\n- [Contrib](https://github.com/tensorflow/tensorflow/tree/2e22f1b20fdfa77b1332c518617391dc32359c5b/tensorflow/contrib) could be a separate project with its own documentation (aka SciKit)\r\n\r\nAny ideas on which direction it is headed?", "We moved the models/ directory to https://github.com/tensorflow/models because that makes more sense. We will likely do more such moves as you describe: we will likely keep a small number of small examples, but possibly less than we have. contrib is definitely a target for moving to its own repo, but we have to figure out a bunch of tooling to make that work well. As you say, tensorboard is also not required in the main repo, but we have no immediate plans to move it.\r\n\r\nThese changes will make it easier for us to maintain these components. Please bear with us, I know these changes can be disruptive.", "Thanks for reporting this @zafartahirov. The documentation you linked to though instructs you to clone the [TensorFlow models repo](https://github.com/tensorflow/models), so it looks fine to me. Let us know if you find other outdated documentation.", "@nealwu Oh yeah, sorry -- I meant to link to the [official docs](https://www.tensorflow.org/get_started/os_setup#run_a_tensorflow_demo_model)", "OK got it. The reason for this is that the current default version of the docs is r0.12 (which should also be the version you get if you use pip to install tensorflow).\r\n\r\nIf you look at the [master version of the docs](https://www.tensorflow.org/versions/master/get_started/os_setup) instead, it should be up to date with the master version of the code."]}, {"number": 6579, "title": "Just some Grammar", "body": "\r\n\r\n![untitled](https://cloud.githubusercontent.com/assets/7543641/21576381/ac92b3a8-cf52-11e6-8662-8b72a36a7caf.png)\r\n\r\n\r\n\r\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["I'm assuming this is the retraining tutorial, the grammar seems to be fixed at head -- https://github.com/tensorflow/tensorflow/blob/4172ca2cd7ac34be67bda2600944d284e8907b95/tensorflow/g3doc/how_tos/image_retraining/index.md"]}, {"number": 6578, "title": "tf.rank not working. Increments each call", "body": "When running this line:\r\n```tf.rank(tf.placeholder(tf.int32))```\r\nthe output increments each time:\r\n```<tf.Tensor 'Rank:0' shape=() dtype=int32>```\r\n```<tf.Tensor 'Rank_1:0' shape=() dtype=int32>```\r\n```<tf.Tensor 'Rank_2:0' shape=() dtype=int32>```\r\n\r\nUbuntu 14.04\r\nLatest install from pip\r\nBoth gpu and cpu\r\n0.12.0", "comments": ["Each call to `tf.rank()` is creating a new `tf.Tensor` object, and the number you see incrementing is a suffix added to operation names to make them unique. The actual value of the rank depends on what value you feed to the placeholder (since the placeholder has unknown rank) and it will be evaluated when you pass it to `tf.Session.run()`. (Note that the line of code you posted is a bit strange, because you don't capture the `tf.placeholder()` in a variable, so you won't easily be able to feed it.)\r\n\r\n(This kind of query is better handled on Stack Overflow, since it's neither a bug nor a feature request. Feel free to post a question there to continue the discussion.)\r\n", "Oh! So dumb! Sorry.", "import tensorflow as tf\r\nx = tf.placeholder(tf.int32,[1,2,5])\r\nrank = tf.rank(x)\r\nwith tf.Session() as sess:\r\n    rank_out = sess.run(rank)\r\n    print(rank_out)"]}, {"number": 6577, "title": "[Java] Bulk data transfer for Tensor class", "body": "Closes #6576\r\n\r\nIntroduces bulk data-access pattern to `Tensor`, with tests.\r\n- `Tensor::create(DataType dt, long[] shape, Buffer data)` method\r\n- `Tensor::getDataByteSize()` method\r\n- `Tensor::readData(Buffer data)` method\r\n\r\nDetails:\r\n- rename `allocate` to `allocateNDArray` since it is designed for the\r\nNDarray scenario.\r\n- introduce a native `allocate` method that accepts the data size in\r\nbytes, rather than computing the size with shape information (which\r\nisn\u2019t always sufficient, e.g. TF_STRING).\r\n- introduce a private `buffer` accessor to access the TF data as a\r\ndirect `ByteBuffer`.\r\n- implement `create` and `readData` methods to read and write data\r\nusing a variety of `Buffer` variants.   Supports typed buffers and\r\ndirect buffers.", "comments": ["Can one of the admins verify this patch?", "@asimshankar please have a look, thanks.  I think that the NIO buffer classes are the most efficient way to provide a general bulk-access pattern while minimizing new JNI code.  I listed various scenarios in #6576 but my primary goal was efficient serialization.\r\n\r\nAn important design goal was to avoid sharing the native tensor's buffer address while leveraging the optimized `put` methods on the typed `Buffer` classes.\r\n", "Thanks for the PR! Took a quick look, and at a very high level this looks great. Will make a detailed pass a little later (apologies for the slow responses, I've been traveling but will be back next week and faster to respond thereafter :)\r\n\r\nOne thing to keep in mind: Since part of the intention here is for serialization, this isn't quite accurate. Turns out that the in-memory representation and serialized form aren't always the same (even if you ignore endian-ness).  Ideally we'd always use what is filled in [`TensorProto`](https://github.com/tensorflow/tensorflow/blob/9c41d5f64efedf44d30e5cc21f900ea07097395c/tensorflow/core/framework/tensor.proto#L36) as the serialized form, which is produced by [`Tensor::AsProtoTensorContent`](https://www.tensorflow.org/api_docs/cc/class/tensorflow/tensor#asprototensorcontent) in C++. \r\n\r\nSo I would avoid committing to the `ByteBuffer` over the wire. Do keep that in mind. Though, at this stage I'm mostly interested in getting the API right, not having a comprehensive implementation in the short term is fine (For example, we did that with [Go](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/tensor.go#L196)).\r\n\r\n(CC @jhseu )", "I agree that TensorProto would be ideal for serialization, and spent some time on it.  But became discouraged when I realized that the generated classes don't use primitive arrays. For many scenarios, Buffers provide the ideal performance and flexibility.", "Any luck with this @EronWright ?", "@drpngx ETA tomorrow.", "No hurry, just checking. Nice contribution!", "@asimshankar Just to be sure - is that good to go?", "A comment on why the typed `create` methods take a `DataType` parameter, it is to support unsigned values in the future.   i.e.:\r\n```java\r\nIntBuffer buffer = ...\r\nTensor t = Tensor.create(DataType.UINT32, shape, buffer);\r\n```\r\n\r\nNote the JDK 8 support for unsigned values ([reference](https://blogs.oracle.com/darcy/entry/unsigned_api)).", "I plan to work on these changes this weekend.", "@EronWright : Any updates? We're looking forward to this :)", "@asimshankar sorry for the delay, I need a few more days.", "@asimshankar ready for you to have another look.  Thanks again.", "Jenkins, test this please.", "This fails in `//tensorflow/python:coordinator_test`. @rmlarsen, feel free to merge if this is a known unrelated failure.", "@asimshankar I believe GitHub now lets you edit PRs, so you can go ahead and make small typo changes yourself :)", "@tensorflow-jenkins test this please", "It appears that the test failures are unrelated. Merging."]}, {"number": 6576, "title": "[Java] Bulk data transfer for Tensor class", "body": "\r\nEnhance the `Tensor` class with bulk data-access methods, to support the following scenarios:\r\n- serialization of tensors in managed code (regardless of tensor datatype)\r\n- efficient transfer of large blocks of multidimensional data\r\n- support for direct transfers (i.e. using direct `ByteBuffer`s)\r\n- support for typed `Buffer`s\r\n", "comments": []}, {"number": 6575, "title": "does not work on my dataset", "body": "Linux Unbuntu\r\nPython 2.7\r\n\r\nI did run word2vec_basic.py example successfully but as i change dataset from text8.zip to mydata.zip, i get following error\r\n\r\nTraceback (most recent call last):\r\n  File \"../../statistics/word2vec/word2vec_basic.py\", line 218, in <module>\r\n    close_word = reverse_dictionary[nearest[k]]\r\nKeyError: 21049\r\n\r\ni have checked, my data is loading successfully in data array\r\n", "comments": ["Closing as it doesn't seem to be a bug in TensorFlow or feature request. Posting on StackOverflow with tag:tensorflow may elicit some help how to modify the example to work on custom datasets"]}, {"number": 6574, "title": "C drive space usage", "body": "# **EDIT:  MY MITAKE** \r\n**sorry guys**\r\n\r\n//I ran this ..\r\ndocker run -it -v /$(pwd)/tf_files-local:/tf_files-container gcr.io/tensorflow/tensorflow:latest-devel\r\n\r\nthen started retrain.py to train my model. Suddenly it took around 60GB of my C drive. I tried to clean using CC Cleaner, Windows DiskCleaner, Uninstalled TensorFlow ,Deleted every file i created including bottleneck files, but still its taking up that space. \r\nHelp Needed.//", "comments": []}]