[{"number": 41556, "title": "Fix setting out_mtypes in MemoryTypesForNode", "body": "Clear hostmem_attr vector before populating it with indices from \"_output_hostmem\" attribute so that it doesn't contain indices from \"_input_hostmem\" attribute.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41556) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41556) for more info**.\n\n<!-- ok -->", "Is the claim that `TryGetNodeAttr` mutates `hostmem_attr` but still returns a bad status?\r\n\r\nI don't see why it would do that. Is there a bug this was causing / can we write a unit test for this?", "The problematic case is when both `_input_hostmem` and `_output_hostmem` attributes are specified. In such case first `TryGetNodeAttr` is called and `hostmem_attr` is populated with host memory input indices, then `TryGetNodeAttr` is called again and host memory output indices are added to the `hostmem_attr` vector, because `TryGetNodeAttr` doesn't clear the vector, but rather simply appends values to it. And so we end up with `out_mtypes` being set based on the indices from both the `_input_hostmem` and `_output_hostmem` attributes.\r\nSure, I will try to prepare a unit test for this bug.", "Ah, that makes sense, thanks. Not sure why I was mentally inserting a return.\r\n\r\nWith a unit test this looks like a good change to me then.", "@lahead Can you please check @allenlavoie's comments and keep us posted ? Thanks!", "I'm working on a unit test, please hold on.", "Unit test extended to cover this case, please review.", "No problem!\r\nI see some CI build failures, but the failures don't seem to be related to my change. May this be some environment issue?"]}, {"number": 41555, "title": "TFLite Metal Delegate failed to build on macOS from v2.3.0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.3.0-rc2\r\n- Python version: 3.6.1\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): Apple clang version 11.0.3 (clang-1103.0.32.62)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nTFLite Metal Delegate for macOS failed to build on v2.3.0-rc2 (bezel 3.1.0)\r\nI could build on v2.2.0 with the same command. (bezel v2.0.0)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\nbazel build -c opt --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=default --linkopt -s --strip always --cxxopt=-std=c++14 --apple_platform_type=macos //tensorflow/lite/delegates/gpu:tensorflow_lite_gpu_dylib\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThe command\r\n\r\n`bazel build -c opt --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=default --linkopt -s --strip always --cxxopt=-std=c++14 --apple_platform_type=macos //tensorflow/lite/delegates/gpu:tensorflow_lite_gpu_dylib`\r\n\r\nBazel Logs\r\n\r\n```\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=159\r\nINFO: Reading rc options for 'build' from /Users/ibu/Projects/tf-unity/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/ibu/Projects/tf-unity/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /Users/ibu/Projects/tf-unity/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/Users/ibu/.pyenv/shims/python --action_env PYTHON_LIB_PATH=/Users/ibu/.pyenv/versions/3.6.1/lib/python3.6/site-packages --python_path=/Users/ibu/.pyenv/shims/python --config=xla --action_env TF_CONFIGURE_IOS=1\r\nINFO: Found applicable config definition build:v2 in file /Users/ibu/Projects/tf-unity/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /Users/ibu/Projects/tf-unity/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:macos in file /Users/ibu/Projects/tf-unity/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /private/var/tmp/_bazel_ibu/7695e3c21358fe2941153284a4ed3f79/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nERROR: /private/var/tmp/_bazel_ibu/7695e3c21358fe2941153284a4ed3f79/external/cpuinfo/BUILD.bazel:96:1: Configurable attribute \"srcs\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n @cpuinfo//:linux_x86_64\r\n @cpuinfo//:linux_arm\r\n @cpuinfo//:linux_armhf\r\n @cpuinfo//:linux_armv7a\r\n @cpuinfo//:linux_armeabi\r\n @cpuinfo//:linux_aarch64\r\n @cpuinfo//:macos_x86_64\r\n @cpuinfo//:windows_x86_64\r\n @cpuinfo//:android_armv7\r\n @cpuinfo//:android_arm64\r\n @cpuinfo//:android_x86\r\n @cpuinfo//:android_x86_64\r\n @cpuinfo//:ios_x86_64\r\n @cpuinfo//:ios_x86\r\n @cpuinfo//:ios_armv7\r\n @cpuinfo//:ios_arm64\r\n @cpuinfo//:ios_arm64e\r\n @cpuinfo//:watchos_x86_64\r\n @cpuinfo//:watchos_x86\r\n @cpuinfo//:watchos_armv7k\r\n @cpuinfo//:watchos_arm64_32\r\n @cpuinfo//:tvos_x86_64\r\n @cpuinfo//:tvos_arm64\r\nINFO: Repository eigen_archive instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule tf_http_archive defined at:\r\n  /Users/ibu/Projects/tf-unity/tensorflow/third_party/repo.bzl:134:19: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/lite/delegates/gpu:tensorflow_lite_gpu_dylib' failed; build aborted:\r\n\r\n/private/var/tmp/_bazel_ibu/7695e3c21358fe2941153284a4ed3f79/external/cpuinfo/BUILD.bazel:96:1: Configurable attribute \"srcs\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n @cpuinfo//:linux_x86_64\r\n @cpuinfo//:linux_arm\r\n @cpuinfo//:linux_armhf\r\n @cpuinfo//:linux_armv7a\r\n @cpuinfo//:linux_armeabi\r\n @cpuinfo//:linux_aarch64\r\n @cpuinfo//:macos_x86_64\r\n @cpuinfo//:windows_x86_64\r\n @cpuinfo//:android_armv7\r\n @cpuinfo//:android_arm64\r\n @cpuinfo//:android_x86\r\n @cpuinfo//:android_x86_64\r\n @cpuinfo//:ios_x86_64\r\n @cpuinfo//:ios_x86\r\n @cpuinfo//:ios_armv7\r\n @cpuinfo//:ios_arm64\r\n @cpuinfo//:ios_arm64e\r\n @cpuinfo//:watchos_x86_64\r\n @cpuinfo//:watchos_x86\r\n @cpuinfo//:watchos_armv7k\r\n @cpuinfo//:watchos_arm64_32\r\n @cpuinfo//:tvos_x86_64\r\n @cpuinfo//:tvos_arm64\r\nINFO: Elapsed time: 0.569s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (13 packages loaded, 395 targets configured)\r\n```\r\n\r\nsimilar issue #37472\r\n", "comments": ["duplicated with #41039", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41555\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41555\">No</a>\n"]}, {"number": 41554, "title": "ObjectTracking is returning null", "body": "I implemented tensorflow/example/android.../tracking into my android app and when I am trying to activate and disable ObjectDetection, then I am getting an error : \r\n'CHECK FAILED (object_tracker != NULL): null object tracker!' \r\n\r\nIn ObjectTracker.java there is a documentation like this:\r\n**\r\n * True object detector/tracker class that tracks objects across consecutive preview frames.\r\n * It provides a simplified Java interface to the analogous native object defined by\r\n * jni/client_vision/tracking/object_tracker.*.\r\n *\r\n * Currently, the ObjectTracker is a singleton due to native code restrictions, and so must\r\n * be allocated by ObjectTracker.getInstance(). **In addition, release() should be called**\r\n * as soon as the ObjectTracker is no longer needed, **and before a new one is created.**\r\n *\r\n * nextFrame() should be called as new frames become available, preferably as often as possible.\r\n *\r\n * After allocation, new TrackedObjects may be instantiated via trackObject(). TrackedObjects\r\n * are associated with the ObjectTracker that created them, and are only valid while that\r\n * ObjectTracker still exists.\r\n */\r\n\r\nI put a Log in release() method and I saw that this method gets called only once when ObjectTracker is no longer needed but not when a new one is created, \r\nwhere should I call the release() method before new one is created?  \r\n\r\n", "comments": ["@ArtanBerisha1 \r\n\r\nRequest you to fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease provide details about what platform you are using (operating system, architecture).\r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41553, "title": "keras.Model.save_weights is overwriting all_model_checkpoint_paths", "body": "**System information**\r\n- Tensorflow 2.2.0\r\n\r\n**Describe the current behavior**\r\n\r\n`keras.Model.save_weights` does not respect existing checkpoints.\r\n\r\nUsing `save_weights()` from a Keras model seems to overwrite the `checkpoint` file without preserving existing checkpoints (`all_model_checkpoint_paths`) of that file.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/network.py#L1173\r\n\r\n**Describe the expected behavior**\r\n\r\n`keras.Model.save_weights` should respect existing checkpoints.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\n\r\ndef main():\r\n    nb_checkpoints = 3\r\n\r\n    inputs = layers.Input(shape=(8,))\r\n    x = layers.Dense(2)(inputs)\r\n    outputs = layers.Dense(2)(x)\r\n    model = keras.Model(inputs=[inputs], outputs=[outputs])\r\n\r\n    model_dir = '/tmp/save-weights'\r\n    for i in range(nb_checkpoints):\r\n        model.save_weights(os.path.join(model_dir, 'ckpt-%04d' % (i + 1)))\r\n\r\n    state = tf.train.get_checkpoint_state(model_dir)\r\n    print('')\r\n    print(state.all_model_checkpoint_paths)\r\n\r\n    checkpoint_fp = os.path.join(model_dir, 'checkpoint')\r\n    print('\\nContent of %s' % checkpoint_fp)\r\n    with open(checkpoint_fp) as f:\r\n        print(f.read())\r\n\r\n    assert state.all_model_checkpoint_paths == nb_checkpoints, \\\r\n        'Expected %d checkpoints got %d' % (nb_checkpoints, len(state.all_model_checkpoint_paths))\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nOutput:\r\n\r\n```none\r\n['/tmp/save-weights/ckpt-0003']\r\n\r\nContent of /tmp/save-weights/checkpoint\r\nmodel_checkpoint_path: \"ckpt-0003\"\r\nall_model_checkpoint_paths: \"ckpt-0003\"\r\n\r\nTraceback (most recent call last):\r\n    ...\r\n    assert state.all_model_checkpoint_paths == nb_checkpoints, \\\r\nAssertionError: Expected 3 checkpoints got 1\r\n```\r\n", "comments": ["@stefan-falk,\r\nCould you please try setting the `overwrite` argument as `False` and check if you are facing the same issue. Thanks!\r\n\r\nDocumentation of [save_weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save_weights) for reference. ", "@amahendrakar I just tried it out. Using `overwrite=False` has the same effect.\r\n\r\nIt should be the line I posted in my first comment that causes this:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/network.py#L1173\r\n\r\n", "Hi @amahendrakar , I am interested in working on this.  @gowthamkpr please let me know if you haven't already started and if it's okay if I work on this. ", "Was able to reproduce the issue in TF 2.6.0-dev20210530,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/0e402dfc620207ca74534413626bcc7d/untitled115.ipynb#scrollTo=9rXLNxQgPe87)..Thanks !", "The code which you are referring to is no longer exists in Master brach, update the details with the recent version and post the issue in keras-team/keras repo.\r\n\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41553\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41553\">No</a>\n"]}, {"number": 41552, "title": "CategoryEncoding not working when loading model again", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 20.04 LTS** \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **nightly binary (tf-nightly-gpu)**\r\n- TensorFlow version (use command below): **2.4.0-dev20200719**\r\n- Python version: **3.8.2**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **V10.1.243**\r\n- GPU model and memory: **NVIDIA RTX2070s 8GB VRAM**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen running **[this example](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)** but saving the model at the end and moving the predict code to another file (predict.py), I get the following error while loading the model: **RuntimeError: If you construct a `CategoryEncoding` layer with `max_tokens=None`, you need to call `adapt()` on it before using it**. \r\n\r\n**Describe the expected behavior**\r\nI don't expect the error, but just being able to load the model and call predict on the model.\r\n\r\n**Standalone code to reproduce the issue**\r\n/\r\n\r\n**Other info / logs** \r\n```\r\n2020-07-19 19:11:26.244328: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nInvalid MIT-MAGIC-COOKIE-1 key2020-07-19 19:11:27.161486: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-07-19 19:11:27.164500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-19 19:11:27.164743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce RTX 2070 SUPER computeCapability: 7.5\r\ncoreClock: 1.8GHz coreCount: 40 deviceMemorySize: 7.77GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-07-19 19:11:27.164756: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-19 19:11:27.165592: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-07-19 19:11:27.166145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-07-19 19:11:27.166487: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-07-19 19:11:27.167819: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-19 19:11:27.168359: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-19 19:11:27.168437: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n2020-07-19 19:11:27.168457: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-07-19 19:11:27.168681: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-07-19 19:11:27.172281: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3600000000 Hz\r\n2020-07-19 19:11:27.172519: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4b78d40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-19 19:11:27.172531: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-19 19:11:27.175397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-19 19:11:27.175415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      \r\nTraceback (most recent call last):\r\n  File \"predict.py\", line 5, in <module>\r\n    model = load_model(\"models/model\")\r\n  File \"/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\", line 187, in load_model\r\n    return saved_model_load.load(filepath, compile, options)\r\n  File \"/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 120, in load\r\n    model = tf_load.load_internal(\r\n  File \"/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 632, in load_internal\r\n    loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\r\n  File \"/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 194, in __init__\r\n    super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n  File \"/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 130, in __init__\r\n    self._load_all()\r\n  File \"/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 221, in _load_all\r\n    self._finalize_objects()\r\n  File \"/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 530, in _finalize_objects\r\n    self._reconstruct_all_models()\r\n  File \"/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 548, in _reconstruct_all_models\r\n    self._reconstruct_model(model_id, model, layers)\r\n  File \"/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 588, in _reconstruct_model\r\n    created_layers) = functional_lib.reconstruct_from_config(\r\n  File \"/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\", line 1210, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\", line 1158, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 919, in __call__\r\n    return self._functional_construction_call(inputs, args, kwargs,\r\n  File \"/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1111, in _functional_construction_call\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/home/jeroen/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/preprocessing/category_encoding.py\", line 282, in call\r\n    raise RuntimeError(\r\nRuntimeError: If you construct a `CategoryEncoding` layer with `max_tokens=None`, you need to call `adapt()` on it before using it\r\n```\r\n\r\n", "comments": ["Same problem", "same here", "got the same issue. Happens both when saving/loading under a .h5 format, or tensorflow SavedModel", "Workaround is to explicitly set the max_tokens and re-train->save->load, even if you call `adapt`. ", "@jclaessens97 @scd75 @lukashondrich @IaroslavElistratov As mentioned in the above comment, the workaround is to specify max_tokens in CategoryEncoding. Please find the [gist](https://colab.research.google.com/gist/gowthamkpr/93d57b30ed262aa7e5388a34a092be64/structured_data_classification_from_scratch.ipynb)\r\n\r\nAlso, yes if you leave max_tokens = None, then we run into this error. Please find my gist [here](https://colab.research.google.com/gist/gowthamkpr/0bc64433e84fb9ef6e4de95b1797e355/structured_data_classification_from_scratch.ipynb)", "Any processing ?", "same here", "same - and the work-around is bit nonsense isn't it? The point of the preprocessing layer is to preprocess. If I have to do the same preprocessing... Okay so my workaround is going to be to make two preprocessing layers, adapt one with max_tokens=None, get the size of the vocabulary, adapt again on the second layer but with max_tokens=len(first_vectorizer.get_vocabulary()). Only return the second layer. \r\n\r\nReally dumb but at least I can write the same code twice (and wait twice as long for the adapt step) instead of having to write separate code that figures out what max_tokens should be.\r\n", "An alternative workaround is setting the max_tokens after adapt\r\n\r\n\r\n```python\r\n    ...\r\n    # Learn the space of possible indices\r\n    encoder.adapt(feature_ds)\r\n\r\n    # WORKAROUND for tensorflow/tensorflow#41552\r\n    # Lock the max_tokens to allow load_model\r\n    encoder.max_tokens = encoder._combiner.max_tokens = len(index.get_vocabulary())\r\n\r\n    # Apply one-hot encoding to our indices\r\n    encoded_feature = encoder(encoded_feature)\r\n    return encoded_feature\r\n```\r\n\r\nStill, would be nice to \"freeze\" the max_tokens automatically when saving the model.\r\n", "@johnpaulett \r\n```python\r\nlen(index.get_vocabulary())\r\n```\r\nDon't you mean `encoder.get_vocabulary()`? Otherwise you still have to adapt twice.", "Running the second example from @gowthamkpr with `max_tokens = None` seems to be working in current TF Nightly. Please update this thread with an example if you're still seeing this error in nightly.", "Installing tensorflow version 2.5.0 will solve the problem\r\n\r\n! pip install tensorflow==2.5.0 \r\n", "@jclaessens97 \r\nAs the issue does not appear on tf 2.5, could you please upgrade and let us know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41552\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41552\">No</a>\n"]}, {"number": 41551, "title": "S3 simple copy", "body": "@mihaimaruseac \r\nA small PR for `CopyFile`. `MultipartCopy` will come in the next PR since it is the hardest part of S3 in my opinion. I am expecting that we will finish S3 by the end of this week ( There will be 4 more PRs so a PR merged / day should be ok )", "comments": []}, {"number": 41550, "title": "tensorflow lite for android a bug: Node number 213 (TfLiteGpuDelegateV2) failed to invoke.", "body": "2020-07-20 13:21:08.699 24598-24632/org.gddi.ai.camera E/AndroidRuntime: FATAL EXCEPTION: Thread-5\r\n    Process: org.gddi.ai.camera, PID: 24598\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: TfLiteGpuDelegate Invoke: Failed to read data from GPU (clEnqueueReadBuffer) - Execution status error for events in wait list\r\n    Node number 213 (TfLiteGpuDelegateV2) failed to invoke.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:158)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:343)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:304)\r\n        at com.ncnn.demo.tflite.TfLiteFaceRecognitionModel.recognizeImage(TfLiteFaceRecognitionModel.java:77)\r\n        at com.ncnn.demo.loopTask.FaceComparisonLoopTask.run(FaceComparisonLoopTask.java:103)\r\n        at java.lang.Thread.run(Thread.java:764)", "comments": ["Can you please provide more information like what device you are trying this on? What TF version you are using?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41550\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41550\">No</a>\n"]}, {"number": 41549, "title": "Incorrect processing in tf.image.decode_gif for multiple-frame image", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): n/a\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS or Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0 and 2.3.0rc1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n\r\nWhile working with `tf.image.decode_gif` for mult-frame image, I noticed the returned value is incorrect after the first image.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nAll frames should be handled correctly.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\n\r\nprint(tf.version.VERSION)\r\n\r\n!curl -OL https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Animated_GIF_cheloVechek.gif/440px-Animated_GIF_cheloVechek.gif\r\n\r\nimage = tf.image.decode_gif(tf.io.read_file('440px-Animated_GIF_cheloVechek.gif'))\r\nfor i in range(image.shape[0]):\r\n  plt.imshow(image[i])\r\n  plt.figure()\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nThe image is downloaded from WIKI page:\r\nhttps://en.wikipedia.org/wiki/GIF#Animated_GIF\r\n\r\n\r\nOriginal picture (gif):\r\n\r\n![cradle](https://user-images.githubusercontent.com/6932348/87897007-da537480-c9fe-11ea-848e-258add6a3dd5.gif)\r\n\r\nFirst frame extracted:\r\n\r\n![test_0](https://user-images.githubusercontent.com/6932348/87897022-e4757300-c9fe-11ea-9020-9c979e5216db.png)\r\nSecond frame extracted:\r\n\r\n![test_1](https://user-images.githubusercontent.com/6932348/87897023-e7706380-c9fe-11ea-9cc6-b3e2991a26d5.png)\r\n\r\n", "comments": ["Can you also check with 2.1.0 please? To detect if there was a regression.", "Thanks @mihaimaruseac. I tested with 2.1.0 and the result remains the same.", "Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/1bb8d8abb8ac6eec6eccd48f32735fbc/41549-2-1.ipynb), [TF v2.2](https://colab.research.google.com/gist/amahendrakar/f0bdede175086ce81f5830f577a65389/41549-2-2.ipynb), [TF v2.3.0rc2](https://colab.research.google.com/gist/amahendrakar/4a078bcb9f0dab11ac12d037a084de65/41549-2-3.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/ed185d4223655cce1ce94cb3ec2817ad/41549-tf-nightly.ipynb). Please find the attached gist. Thanks!", "This should be fixed with: https://github.com/tensorflow/tensorflow/commit/3741a7bda7131f7ba26c840d9906a5824bbbc3c3\r\n\r\nHere is a [colab](https://colab.research.google.com/drive/1VtcWpG82O4dl4zbC2cyOvDy0hMp6RKQ_?usp=sharing) verifying the fix with a tf-nightly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41549\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41549\">No</a>\n"]}, {"number": 41548, "title": "Enable reciprocal GPU kernel on complex number", "body": "Fixes #41546. \r\n- Enable GPU reciprocal kernel for complex number\r\n- Add associated tests", "comments": []}, {"number": 41547, "title": "TensorFlow Lite currently doesn't support control flow ops", "body": "TensorFlow Lite currently doesn't support control flow ops: Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, ADD_N, ARG_MAX, CAST, CONCATENATION, CONV_2D, EXPAND_DIMS, FILL, FLOOR, FULLY_CONNECTED, GATHER_ND, GREATER_EQUAL, LESS, LOGICAL_AND, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, RANGE, REDUCE_MAX, RESHAPE, RESIZE_BILINEAR, REVERSE_SEQUENCE, SELECT, SHAPE, SLICE, SPARSE_TO_DENSE, SPLIT, SQUEEZE, STRIDED_SLICE, SUB, TANH, TILE, TRANSPOSE, UNPACK. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, RandomUniform.", "comments": ["Currently, TFLite does not support v1 control flow ops. Please try the conversion with the following option or you can upgrade the old control flow ops with v2 control flow ops.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2", "As Jaesung mentioned, you will need to enable Control Flow v2\r\nPlease reopen if you're having issues and provide details and reproduce steps.\r\n\r\nThanks", "Hi, \r\nI'm having the same error. \"Exception: TensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch.\"\r\nThe problem is that my model has some custom layers implemented in tf1.x.\r\nBesides, The solution @abattery proposed is only possible when using tf2.x, So that for tf1.x the error still remains. \r\nIn the following I attach the code that I used for loading and conversion:(Please Note that I have saved the model weights, and in tf1.x, it is only possible to save in tflite format from session not keras model(when having custom layers)):\r\n`\r\ndef convert_h5_2TFLite():\r\n    # DOES NOT WORK !!!!!!!!!!!!!!!\r\n    # load model\r\n    model_path = 'keras_model/model_weights.h5'\r\n    K.set_learning_phase(0)  # make sure its testing mode\r\n    \r\n    with tf.Graph().as_default():\r\n\r\n        with tf.Session() as sess:\r\n    \r\n            K.set_session(sess)\r\n            model = create_model_obj(image_size, num_classes)()\r\n            model.load_weights(model_path)\r\n\r\n            # convert keras model to tflite\r\n            tfLite_Path = 'tfLite_model/model_Lite.tflite'\r\n\r\n            # Convert the model.\r\n            converter = tf.lite.TFLiteConverter.from_session(sess, model.inputs,\r\n                                                         model.outputs)\r\n            converter.allow_custom_ops=True\r\n            tflite_model = converter.convert()\r\n            open(tfLite_Path, \"wb\").write(tflite_model)`"]}, {"number": 41546, "title": "Reciprocal is not executed on GPU for complex number", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: colab GPU\r\n- GPU model and memory: colab GPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nReciprocal is not executed on GPU for complex number.\r\n\r\n```python3\r\nimport tensorflow as tf\r\n\r\ntf.debugging.set_log_device_placement(True)\r\nwith tf.device(\"gpu\"):\r\n  tf.math.reciprocal(tf.complex(1.0, 0.0))\r\n\r\n\"\"\"\r\nExecuting op Complex in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op Reciprocal in device /job:localhost/replica:0/task:0/device:CPU:0\r\n\"\"\"\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe reciprocal op can be executed on GPU for complex number\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1x0NX11ZBjA95KSeafLg0_LRvVe8L2uyD?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried in colab with TF version 2.2, 2.3-rc1,nightly versions(`2.4.0-dev20200719`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/adcddd8a7a1b1f8280c3f44685d57064/untitled151.ipynb).Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41546\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41546\">No</a>\n"]}, {"number": 41545, "title": "model.export(export_dir='.')  does not export labels nor model.", "body": "**System information**\r\n- OSx\r\n- TensorFlow installed from source\r\n-  tf.__version__  ='2.4.0-dev20200719'\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nRunning the code in [this notebook](https://colab.research.google.com/drive/1_LZxdWIfYFpZorpUkzgx_MPZzdzi8Hhy?usp=sharing), the export is fine. The issue arises when running the same code locally in a virtualenv with py36 and the same packages as per `!pip install git+https://github.com/tensorflow/examples.git#egg=tensorflow-examples[model_maker]`   \r\n\r\n```\r\nmodel.export(export_dir='.')\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nWARNING:tensorflow:From /Users/akiva111admin/code/security/venv_tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From /Users/akiva111admin/code/security/venv_tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n2020-07-19 22:21:59.233332: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:From /Users/akiva111admin/code/security/venv_tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From /Users/akiva111admin/code/security/venv_tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n2020-07-19 22:22:11.638306: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\r\n2020-07-19 22:22:11.638403: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-07-19 22:22:11.689811: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-07-19 22:22:11.689845: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 912 nodes (656), 920 edges (664), time = 32.473ms.\r\n2020-07-19 22:22:11.689850: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 1.228ms.\r\n2020-07-19 22:22:14.652608: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:315] Ignored output_format.\r\n2020-07-19 22:22:14.652634: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:318] Ignored drop_control_dependency.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nipdb> model.summary()                                                                                                                                                                \r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nhub_keras_layer_v1v2 (HubKer (None, 1280)              3413024   \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 1280)              0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 2)                 2562      \r\n=================================================================\r\nTotal params: 3,415,586\r\nTrainable params: 2,562\r\nNon-trainable params: 3,413,024\r\n_________________________________________________________________\r\n\r\ntype(model)                                                                                                                                                                    \r\n<class 'tensorflow_examples.lite.model_maker.core.task.image_classifier.ImageClassifier'>\r\n\r\nmodel.hparams                                                                                                                                                                  \r\nHParams(train_epochs=5, do_fine_tuning=False, batch_size=32, learning_rate=0.005, momentum=0.9, dropout_rate=0.2)\r\n\r\nmodel.model_spec.name                                                                                                                                                          \r\n'efficientnet_lite0'\r\n\r\nmodel.num_classes                                                                                                                                                              \r\n2\r\n\r\n```\r\n\r\n**Failure details**\r\n\r\nExpected output as per the [example here](https://www.tensorflow.org/lite/tutorials/model_maker_image_classification#step_3_evaluate_the_customized_model) looks like: \r\n```INFO:tensorflow:Saving labels in ./labels.txt\r\nINFO:tensorflow:Saving labels in ./labels.txt.\r\nINFO:tensorflow:Assets written to: /tmp/tmp886peoe2/assets\r\nINFO:tensorflow:Assets written to: /tmp/tmp886peoe2/assets```\r\n\r\n\r\n\r\n", "comments": ["@amgsharma,\r\nI do not have the access to view the Colab notebook you have linked. Could you please grant the required permissions or upload the script here. Thanks! ", "Its the colab notebook found [here](https://www.tensorflow.org/lite/tutorials/model_maker_image_classification):  ", "Was able to reproduce the issue, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/d7039fea469616765d3ba58d61191cdc/41545.ipynb). Thanks!", "I have this problem, too. (saving labels.txt to the export_dir - don\u00b4t want to search for it in the tmp folder,\r\n`\r\nmodel.export(export_dir='./model-files/', tflite_filename='model.tflite', label_filename='labels.txt'` \r\ndoes not work)\r\n\r\nDo I understand this right (from the TF 2.3 label) that this issue will be solved with the next version or has this been dropped, as the labels are embedded in the tflite file?", "@lintian06 @amahendrakar is there an update to this issue ? Thanks! ", "The syntax changed and some parts of the documentation/tutorials still use the old syntax. For more recent versions the solution is here:\r\n\r\n> labels are embedded in tflite metadata.\r\n> use `model.export(export_dir='.', with_metadata=False)` instead of `model.export(export_dir='.')` to get labels.txt\r\n> Check the source code for why, https://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/core/task/custom_model.py#L130-L177\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/43463#issuecomment-697167439\r\n\r\nI tested this with the Flower_Classification_with_TFLite_Model_Maker demo and it worked. https://github.com/tensorflow/tensorflow/issues/43463", "> The syntax changed and some parts of the documentation/tutorials still use the old syntax.\r\n\r\nCan this please be added to the documentation? As the documentation stands, it's not at all clear that this is what's needed to have `model.export()` generate `labels.txt`, and it's painful to have to find this information by searching through GitHub issues.", "@rringham,\r\n\r\nCan you point out the link for documentation, which needs to be modified? Thanks!", "@amgsharma,\r\n\r\nCan you take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/41545#issuecomment-746653192) from @natowi and let us know if it helps in resolving your issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41545\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41545\">No</a>\n"]}, {"number": 41544, "title": "How to load tensorflow 2.0 savedbundle file  by using Java", "body": "From Tensorflow 1.x version, Java can handle a pb file after using freeze_graph.pb.\r\nBut, from Tensorflow 2, since the saved pb structure is different, I don't think a regular way to export file does not work.\r\n\r\nI was wondering how to load Tensorflow 2 saved files by using Java.", "comments": ["I searched several explanations and found out tensorflow 2.0 only supports Andriod app using tflite. So, it seems like there is no way we can use tensorflow 2.0 model file to use Java, right?", "It's possible to do it using tensorflow 2.2.0 by copying the graph and loading the weights:\r\n```\r\nfrom tensorflow.keras.models import clone_model\r\n    w = tf_model.get_weights()\r\n    with tf.Graph().as_default():\r\n        # clone model in new graph and set weights\r\n        _model = clone_model(tf_model)\r\n        _model.set_weights(w)\r\n```\r\nthen save `_model` as usual. I tried the same with tensorflow 2.3.0 and it didn't work for me. Is there any reason?\r\n\r\nAlso, is there any plan to update the Java API to Tensorflow 2?", "@ryanshin712,\r\n\r\nCan you take a look at this [library](https://github.com/deepjavalibrary/djl) and see if it helps? You can refer to this [SO thread](https://stackoverflow.com/questions/56859956/how-to-load-tensorflow-pb-model-in-java-exported-from-keras) which has a similar question. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41544\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41544\">No</a>\n"]}, {"number": 41543, "title": "subclassed model load_model ValueError?", "body": "I am using tf==2.2.0\r\nTried the collaborative filtering model\r\nhttps://keras.io/examples/structured_data/collaborative_filtering_movielens/\r\n\r\nfor saving and loading the model, these worked\r\n```\r\ntf.keras.models.save_model(model,'./saved_model')\r\nloaded_model = tf.keras.models.load_model('./saved_model')\r\n```\r\nBut after loading the model while prediction, I get value error\r\n```\r\nratings = model.predict(user_movie_array).flatten()\r\n\r\n    ValueError: Python inputs incompatible with input_signature:\r\n      inputs: (\r\n        Tensor(\"IteratorGetNext:0\", shape=(None, 2), dtype=int32))\r\n      input_signature: (\r\n        TensorSpec(shape=(None, 2), dtype=tf.int64, name='input_1'))\r\n```\r\nMay i know how to fix this?\r\n\r\n", "comments": ["I tried saving as\r\n```\r\ntf.keras.models.save_model(model,'./saved_model',\r\n                           overwrite=True, \r\n                           include_optimizer=True, \r\n                           signatures=model.call.get_concrete_function(\r\n                               tf.TensorSpec(shape=(None, 2), dtype=tf.int64, name='input_1')), \r\n                           save_format='tf')\r\n```\r\nstill not working", "I have tried\r\n```\r\nmodel.save(\"NameOfYourModel\", save_format='tf')\r\n```\r\nand loaded back with \r\n```\r\nloaded_tfkmodel = tf.keras.models.load_model('./NameOfYourModel')\r\n```\r\nstill i get \r\n```\r\n    ValueError: Python inputs incompatible with input_signature:\r\n      inputs: (\r\n        Tensor(\"IteratorGetNext:0\", shape=(None, 2), dtype=int32))\r\n      input_signature: (\r\n        TensorSpec(shape=(None, 2), dtype=tf.int64, name='input_1'))\r\n```\r\n", "Found it, Thanks\r\nActually recreating the model with\r\n```\r\nkeras.models.load_model('path_to_my_model')\r\n```\r\ndidn't work for me\r\n\r\nFirst we have to save_weights from the built model\r\n```\r\nmodel.save_weights('model_weights', save_format='tf')\r\n```\r\nThen\r\nwe have to initiate a new instance for the subclass Model then compile and train_on_batch with one record and load_weights of built model\r\n```\r\nloaded_model = ClassifierModel(parameters)\r\nloaded_model.compile(parameters)\r\nloaded_model.train_on_batch(x_train[:1], y_train[:1])\r\nloaded_model.load_weights('model_weights')\r\n```\r\nThis work perfectly in TensorFlow==2.2.0\r\n", "@hanzigs \r\nGlad to know it worked.Here is the document for your [reference](https://www.tensorflow.org/tutorials/keras/save_and_load).Please, close this thread if your issue was resolved.Thanks!", "Thanks"]}, {"number": 41542, "title": "tf.io.TFRecordWriter cannot work with gs:// or ram:// at Google Colab TPU", "body": "**System information**\r\n- TensorFlow version (use command below): 2.2.0 (v2.2.0-0-g2b96f3662b)\r\n- Python version: 3.6.9\r\n- GPU model and memory: Google Colab TPU\r\n\r\n**Current behavior**\r\n\r\nAt first, I gave TPU permission to create files on my cloud bucket:\r\n`!gsutil iam ch serviceAccount:service-495559152420@cloud-tpu.iam.gserviceaccount.com:roles/storage.objectCreator gs://oleg-zyablov/`\r\n\r\nNow I want to create file and tf.io.write_file does it successfully. Standalone code (except bucket name):\r\n```\r\npath = 'gs://oleg-zyablov/car-classification/train_tfrecords/256x256_square/test'\r\n    tf.io.write_file(path, tf.constant('', dtype = tf.string))\r\n```\r\n\r\nBut TFRecordWriter fails:\r\n```\r\nwith tf.io.TFRecordWriter(path) as out_file:\r\n  out_file.write(tf.constant('', dtype = tf.string))\r\n```\r\n\r\n```---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-340-960a1cb6ee89> in <module>()\r\n      5 with tf.io.TFRecordWriter(path) as out_file:\r\n----> 6   out_file.write(tf.constant('', dtype = tf.string))\r\n\r\n1 frames\r\nTypeError: write(): incompatible function arguments. The following argument types are supported:\r\n    1. (self: tensorflow.python._pywrap_record_io.RecordWriter, record: str) -> None\r\n\r\nInvoked with: <tensorflow.python.lib.io.tf_record.TFRecordWriter object at 0x7feba977aa40>, <tf.Tensor: shape=(), dtype=string, numpy=b''>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nPermissionDeniedError                     Traceback (most recent call last)\r\n<ipython-input-340-960a1cb6ee89> in <module>()\r\n      4 \r\n      5 with tf.io.TFRecordWriter(path) as out_file:\r\n----> 6   out_file.write(tf.constant('', dtype = tf.string))\r\n\r\nPermissionDeniedError: Error executing an HTTP request: HTTP response code 401 with body '{\r\n  \"error\": {\r\n    \"code\": 401,\r\n    \"message\": \"Anonymous caller does not have storage.objects.create access to oleg-zyablov/car-classification/train_tfrecords/256x256_square/test.\",\r\n    \"errors\": [\r\n      {\r\n        \"message\": \"Anonymous caller does not have storage.objects.create access to oleg-zyablov/car-classification/train_tfrecords/256x256_square/test.\",\r\n        \"domain\": \"global\",\r\n        \"reason\": \"required\",\r\n        \"locationType\": \"header\",\r\n        \"location\": \"Authorization\"\r\n      }\r\n  '\r\n\t when initiating an upload to gs://oleg-zyablov/car-classification/train_tfrecords/256x256_square/test\r\n```\r\n\r\nWhen I want to write file to temporary storage (path = 'ram://testfile'), tf.io.write_file also does it successfully, but TFRecordWriter fails again with message \"File system scheme 'ram' not implemented\".\r\n\r\n**Expected behavior**\r\n\r\nI expected that TFRecordWriter could write file to gs:// just like tf.io.write_file does it. But seems like it does not use proper autentification.", "comments": ["CPU and GPU on local machines have same issue.\r\n```tf.io.gfile``` APIs have similar bugs too.", "The bug seems to have been fixed on nightly.", "@sedol1339,\r\nPlease take a look at @fsx950223's comment and check if you are facing the same issue with TF-nightly. Thanks!", "@amahendrakar unfortunately I can't even work with TPU on nightly build so I cannot check this problem. I do the following actions on Colab and get exception:\r\n```\r\n!pip install tf-nightly\r\n```\r\n```\r\nimport tensorflow as tf\r\ntf.__version__\r\n```\r\nprints 2.4.0-dev20200720\r\n```\r\n!gcloud auth login\r\n```\r\n```\r\nimport os\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n```\r\n```\r\nINFO:tensorflow:Initializing the TPU system: grpc://10.6.181.82:8470\r\nINFO:tensorflow:Initializing the TPU system: grpc://10.6.181.82:8470\r\nINFO:tensorflow:Clearing out eager caches\r\nINFO:tensorflow:Clearing out eager caches\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-dd6f07db7b76> in <module>()\r\n      3 resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n      4 tf.config.experimental_connect_to_cluster(resolver)\r\n----> 5 tf.tpu.experimental.initialize_tpu_system(resolver)\r\n      6 strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system(cluster_resolver)\r\n    109     context.context()._clear_caches()  # pylint: disable=protected-access\r\n    110 \r\n--> 111     serialized_topology = output.numpy()\r\n    112 \r\n    113     # TODO(b/134094971): Remove this when lazy tensor copy in multi-device\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self)\r\n   1062     \"\"\"\r\n   1063     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n-> 1064     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n   1065     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n   1066 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)\r\n   1030       return self._numpy_internal()\r\n   1031     except core._NotOkStatusException as e:  # pylint: disable=protected-access\r\n-> 1032       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n   1033 \r\n   1034   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op<name=_Send; signature=tensor:T -> ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>; NodeDef: {{node _Send}}\r\n```", "Also tf.io.gfile.glob() cannot read files from private google bucket at version 2.2, as mentioned above. It says that \"Anonymous caller does not have storage.objects.list access\" even when I am logged in.", "Any updates on this? ", "So the main problem is that Tensorflow nightly build does not work with Google TPU. After `!pip install tf-nightly` i still get the following error when initalizing:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nprint(tf.__version__)\r\nprint(tf.__git_version__)\r\n\r\ntry:\r\n    tf.get_logger().propagate = False\r\n    if 'COLAB_TPU_ADDR' in os.environ: #colab\r\n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n    else: #kaggle\r\n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n    tf.config.experimental_connect_to_cluster(resolver)\r\n    tf.tpu.experimental.initialize_tpu_system(resolver)\r\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n    print('TPU initialized, num of accelerators:', strategy.num_replicas_in_sync)\r\nexcept (ValueError, KeyError):\r\n    print('TPU not found, using CPU/GPU')\r\n    strategy = tf.distribute.get_strategy()\r\n```\r\n```\r\n2.4.0-dev20200729\r\nv1.12.1-37840-ga95897414e\r\nINFO:tensorflow:Initializing the TPU system: grpc://10.64.154.82:8470\r\nINFO:tensorflow:Clearing out eager caches\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-6-cd53b4a25d16> in <module>()\r\n     11         resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n     12     tf.config.experimental_connect_to_cluster(resolver)\r\n---> 13     tf.tpu.experimental.initialize_tpu_system(resolver)\r\n     14     strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n     15     print('TPU initialized, num of accelerators:', strategy.num_replicas_in_sync)\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system(cluster_resolver)\r\n    109     context.context()._clear_caches()  # pylint: disable=protected-access\r\n    110 \r\n--> 111     serialized_topology = output.numpy()\r\n    112 \r\n    113     # TODO(b/134094971): Remove this when lazy tensor copy in multi-device\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self)\r\n   1063     \"\"\"\r\n   1064     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n-> 1065     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n   1066     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n   1067 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)\r\n   1031       return self._numpy_internal()\r\n   1032     except core._NotOkStatusException as e:  # pylint: disable=protected-access\r\n-> 1033       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n   1034 \r\n   1035   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op<name=_Send; signature=tensor:T -> ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>; NodeDef: {{node _Send}}\r\n```", "@sedol1339,\r\nSorry for the delayed response. The code mentioned in this comment could be executed without any error in **`Colab`** with **`TPU Runtime`** and **`Tensorflow Version 2.4.1`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/58492a16f72a7c8e2613844259602914/gh_41542.ipynb) of working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41542\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41542\">No</a>\n"]}, {"number": 41541, "title": "tensorflow_gpu-1.4.0 requires libcudart.so.10.0 rather than libcudart.so.8.0", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Mobile device if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.4.0\r\n- Python version: 3.6.10\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 8.0 / 6\r\n- GPU model and memory: Tesla V100-SXM2-16GB\r\n\r\n**Describe the problem**\r\nI tried to use openai's [blocksparse](https://github.com/openai/blocksparse), which requires CUDA 8 and Tensorflow 1.4.0. When I `import blocksparse`, the error `tensorflow.python.framework.errors_impl.NotFoundError: libcudart.so.10.0: cannot open shared object file: No such file or directory` is raised. Since tensorflow_gpu-1.4.0 is compatible with CUDA 8, I wonder why `libcudart.so.10.0` is required rather than `libcudart.so.8.0`. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nFirst I created a conda environment and activated it.\r\n```\r\nconda create -n py36-bs python=3.6\r\nconda activate py36-bs\r\n```\r\nThen I installed TensorFlow 1.4.0.\r\n```\r\npip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.0-cp36-cp36m-linux_x86_64.whl\r\n```\r\nNext I installed [blocksparse](https://github.com/openai/blocksparse).\r\n```\r\npip install blocksparse\r\n```\r\nWhen I tried to `import bloacksparse`, an error occurred.\r\n```\r\nPython 3.6.10 |Anaconda, Inc.| (default, May  8 2020, 02:54:21) \r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import blocksparse\r\n/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/blocksparse/__init__.py\", line 3, in <module>\r\n    from blocksparse.utils import (\r\n  File \"/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/blocksparse/utils.py\", line 16, in <module>\r\n    _op_module = tf.load_op_library(os.path.join(data_files_path, 'blocksparse_ops.so'))\r\n  File \"/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \"/home/ubuntu/anaconda3/envs/py36-bs/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: libcudart.so.10.0: cannot open shared object file: No such file or directory\r\n>>>\r\n```\r\n\r\n**Any other info / logs**\r\nI checked several things related to CUDA.\r\n```\r\n(py36-bs) ubuntu@xxx:~$ ls -l /usr/local/cuda\r\nlrwxrwxrwx 1 root root 19 Jul 19 13:13 /usr/local/cuda -> /usr/local/cuda-8.0\r\n```\r\n```\r\n(py36-bs) ubuntu@xxx:~$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2016 NVIDIA Corporation\r\nBuilt on Tue_Jan_10_13:22:03_CST_2017\r\nCuda compilation tools, release 8.0, V8.0.61\r\n```\r\n```\r\n(py36-bs) ubuntu@xxx:~$ echo $PATH\r\n/home/ubuntu/anaconda3/envs/py36-bs/bin:/home/ubuntu/anaconda3/bin:/home/ubuntu/anaconda3/bin:/home/ubuntu/anaconda3/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/home/ubuntu/.dl_binaries/bin:/usr/local/mpi/bin:/opt/aws/neuron/bin:/home/ubuntu/.vscode-server/bin/5763d909d5f12fe19f215cbfdd29a91c0fa9208a/bin:/home/ubuntu/anaconda3/bin:/home/ubuntu/bin:/home/ubuntu/.local/bin:/home/ubuntu/anaconda3/condabin:/home/ubuntu/anaconda3/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/home/ubuntu/.dl_binaries/bin:/usr/local/mpi/bin:/opt/aws/neuron/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/home/ubuntu/.vscode-server/bin/5763d909d5f12fe19f215cbfdd29a91c0fa9208a/bin:/home/ubuntu/anaconda3/bin:/home/ubuntu/bin:/home/ubuntu/.local/bin:/home/ubuntu/anaconda3/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/home/ubuntu/.dl_binaries/bin:/usr/local/mpi/bin:/opt/aws/neuron/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\r\n```\r\n```\r\n(py36-bs) ubuntu@xxx:~$ echo $LD_LIBRARY_PATH\r\n/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:\r\n```\r\n`libcudart.so.8.0` exists but TensorFlow asked for `libcudart.so.10.0`. \r\n```\r\n(py36-bs) ubuntu@xxx:~$ ls -l /usr/local/cuda/lib64/libcudart.so.8.0\r\nlrwxrwxrwx 1 root root 19 Jun 17 01:36 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n```\r\nBesides, there exist multiple versions of CUDA on my machine.\r\n```\r\n(py36-bs) ubuntu@xxx:~$ ls /usr/local/\r\nbin  cuda  cuda-10.0  cuda-10.1  cuda-10.2  cuda-8.0  cuda-9.0  cuda-9.2  etc  games  include  init  lib  man  mpi  sbin  share  src\r\n```\r\nIs the problem related to GPU driver? When I entered `nvidia-smi`, `CUDA Version` is `10.2`.\r\n\r\nSince I met some wired problems when I tried to run [blocksparse](https://github.com/openai/blocksparse) with CUDA 10.0, I want to use CUDA 8.0, which is the version authors of [blocksparse](https://github.com/openai/blocksparse) use. Could anyone help me with the error? Thanks a lot!\r\n\r\ntensorflow_gpu-1.4.0 requires libcudart.so.10.0 rather than libcudart.so.8.0\r\n", "comments": ["@xuyifangreeneyes I guess this issue is more suitable to be posted on [blocksparse](https://github.com/openai/blocksparse/issues) repo. Please post your issue there as this repo is related to core Tensorflow. \r\n\r\n\r\nI am closing this issue. Please feel free to reopen if I am mistaken. Thanks!"]}, {"number": 41540, "title": "Train a tensorflow model using 1 Terabyte swap file", "body": "Hi there,\r\ni want to train a tensorflow model that is huge and require alot of memory.\r\nSo i increased the swap file size and swapiness, but in training it also uses the ram so it fails.\r\nIs there a way to train tensorflow with using swap file as memory? ", "comments": ["@deepseek \r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "@ravikyram  i already did,\r\nBut this is a very serious question to answer, when dealing which large scale data, and i mean real big data that requires Terabytes of RAM. Corporations trying to build a huge model at that scale, it's vital to shift training from using the actual RAM, and instead use the swap. Note that my storage is fast M.2\r\nAnswering this question is a serious matter\r\n", "There are several tricks you can try, including decreasing batch size and looking into [mixed precision](https://www.tensorflow.org/guide/mixed_precision), but the community on Stack Overflow is better prepared to answer and advise. If there are specific bugs or feature requests that come out of this, please open a new Issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41540\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41540\">No</a>\n", "@karmel it's not about mixed precision, it's about using the system storage as memory instead of ram."]}, {"number": 41539, "title": "nccl_ops.all_sum does not correctly reduce gradients", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): v2.2.0-rc3-33-g70087ab4f4 2.2.0-rc4\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version:10.1/7.6.5\r\n- GPU model and memory:P100, V100\r\n\r\n**Describe the current behavior**\r\nThe allreduce operation `nccl_ops.all_sum` does not correctly sum gradients. The results are __incorrect__.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\n#!/usr/bin/env python\r\nimport argparse\r\nfrom tensorflow.compat import v1 as tf\r\nimport tqdm\r\n\r\ndef split_grad_list(grad_list):\r\n    g = []\r\n    v = []\r\n    for tower in grad_list:\r\n        g.append([x[0] for x in tower])\r\n        v.append([x[1] for x in tower])\r\n    return g, v\r\n\r\ndef allreduce_grads(all_grads):\r\n    # reduce gradients for N variables on K devices\r\n    from tensorflow.python.ops import nccl_ops as nccl\r\n    nr_tower = len(all_grads)\r\n    assert nr_tower > 1\r\n    new_all_grads = []  # N x K\r\n    for grads in zip(*all_grads):\r\n        # k grads\r\n        summed = nccl.all_sum(grads)\r\n\r\n        grads_for_devices = []  # K\r\n        true_sum = tf.add_n(grads)\r\n        for g in summed:\r\n            diff = tf.abs(true_sum - g)\r\n            eql = diff < 1e-4\r\n            nccl_res_correct = tf.reduce_all(eql, name=\"corr_\" + grads[0].op.name)\r\n\r\n            def flat(x):\r\n                x = tf.reshape(x, [-1])\r\n                x = tf.slice(x, [0], [tf.minimum(tf.size(x), 200)])\r\n                return x\r\n\r\n            assert_op = tf.debugging.Assert(nccl_res_correct, [\r\n                tf.reduce_max(diff), flat(true_sum), flat(g)], summarize=1000,\r\n                name='assert_' + grads[0].op.name)\r\n            with tf.control_dependencies([assert_op]):\r\n                g = tf.identity(g)\r\n            grads_for_devices.append(g)\r\n        new_all_grads.append(grads_for_devices)\r\n    # transpose to K x N\r\n    ret = list(zip(*new_all_grads))\r\n    return ret\r\n\r\ndef build_graph(image, label, idx):\r\n    v1 = tf.get_variable('aaa/W', shape=[3, 3, 3, 64], trainable=True)\r\n    v2 = tf.get_variable('bbb/W', shape=[3, 3, 3, 64], trainable=True)\r\n    v = v1 if idx == 0 else v2\r\n    image = tf.nn.conv2d(image, v, 1, padding='SAME', data_format='NCHW')\r\n\r\n    def conv(name, x, chan, stride=1):\r\n        with tf.variable_scope(name):\r\n            in_chan = x.shape[1]\r\n            W = tf.get_variable('W', [3, 3, in_chan, chan])\r\n            ret = tf.nn.conv2d(x, W, strides=stride, padding=\"SAME\", data_format=\"NCHW\")\r\n            return tf.nn.relu(ret)\r\n\r\n    x = conv('conv1', image, 64)\r\n    x = conv('conv2', x, 64)\r\n    x = conv('conv3', x, 1280, stride=2)\r\n    x = conv('conv4', x, 1280, stride=2)\r\n    x = conv('conv5', x, 10)\r\n    logits = tf.reduce_mean(x, axis=[2, 3])\r\n    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\r\n    cost = tf.reduce_mean(cost, name='cross_entropy_loss')\r\n    return cost\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--gpu', type=int)\r\n    args = parser.parse_args()\r\n    num_gpu = args.gpu\r\n\r\n    with tf.Graph().as_default():\r\n        opt = tf.train.GradientDescentOptimizer(0.001)\r\n\r\n        grad_list = []\r\n        for k in range(num_gpu):\r\n            with tf.device(\"/gpu:{}\".format(k)), tf.variable_scope(\"tower{}\".format(k)):\r\n                print(\"Building {} ...\".format(k))\r\n                image = tf.random.uniform([32, 3, 30, 30])\r\n                label = tf.random.uniform([32], maxval=9, dtype=tf.int32)\r\n                cost = build_graph(image, label, k)\r\n                varlist = [x for x in tf.trainable_variables() if x.name.startswith(\"tower{}\".format(k))]\r\n                print(\"Varlist for tower {}: \".format(k), [x.name for x in varlist])\r\n                wd_cost = [tf.reduce_sum(x) * 1e-3 for x in varlist]\r\n                cost = tf.add_n([cost] + wd_cost)\r\n                grads = opt.compute_gradients(cost, var_list=varlist)\r\n                grad_list.append(grads)\r\n\r\n        all_grads, all_vars = split_grad_list(grad_list)\r\n        all_grads = allreduce_grads(all_grads)\r\n        grad_list = [list(zip(gs, vs)) for gs, vs in zip(all_grads, all_vars)]\r\n\r\n        train_ops = []\r\n        for idx, grad_and_vars in enumerate(grad_list):\r\n            with tf.device('/gpu:{}'.format(idx)):\r\n                train_ops.append(opt.apply_gradients(\r\n                    grad_and_vars, name='apply_grad_{}'.format(idx)))\r\n        train_op = tf.group(*train_ops)\r\n\r\n        sess = tf.Session()\r\n        sess.run(tf.global_variables_initializer())\r\n        print(\"Training ...\")\r\n        for k in tqdm.trange(5000):\r\n            sess.run(train_op)\r\n```\r\n\r\nThe above code trains a toy network on random data, and allreduce the gradients using `nccl_ops.all_sum`. It checks the allreduce results against the sum of gradients computed by a naive `add_n`, and asserts that the difference is reasonably small. However, the difference can be quite large sometimes and the assertion usually fails within 100 steps of training.\r\n\r\nThe code above (written in TF1 style) can be run on a machine with >=2 GPUs using\r\n```\r\n$ TF2_BEHAVIOR=0 python a.py --gpu 2\r\nBuilding 0 ...\r\n Varlist for tower 0:  ['tower0/aaa/W:0', 'tower0/bbb/W:0', 'tower0/conv1/W:0', 'tower0/conv2/W:0', 'tower0/conv3/W:0', 'tower0/conv4/W:0', 'tower0/conv5/W:0']                                      \r\nBuilding 1 ...  \r\nVarlist for tower 1:  ['tower1/aaa/W:0', 'tower1/bbb/W:0', 'tower1/conv1/W:0', 'tower1/conv2/W:0', 'tower1/conv3/W:0', 'tower1/conv4/W:0', 'tower1/conv5/W:0'] \r\n1%|\u2589                                                                    | 71/5000 [00:06<07:39, 10.73it/s]    \r\nTraceback (most recent call last):                                                                                                                                                                  \r\n  File \"/private/home/yuxinwu/env/py37-tf2.2v2/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1365, in _do_call                                                             \r\n    return fn(*args)                                                                                                                                                                                \r\n  File \"/private/home/yuxinwu/env/py37-tf2.2v2/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _run_fn                                                              \r\n    target_list, run_metadata)                                                                                                                                                                      \r\n  File \"/private/home/yuxinwu/env/py37-tf2.2v2/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1443, in _call_tf_sessionrun                                                  \r\n    run_metadata)                                                                                                                                                                                   \r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.                                                                                                                \r\n  (0) Invalid argument: assertion failed: [0.00100000016] [0.00234295963 0.00230941921 0.00176228327 0.00197261758 0.00213356828 0.00188576151 0.00211580051 0.00221353304 \r\n```\r\n\r\nMy initial investigation suggests (no proof, just a guess) that the bug might appear because the gradients are computed on each GPU in different order.\r\n\r\nThe bug was found to exist in TF 1.15 as well. Have not tested earlier versions.\r\nThe bug rarely triggers itself if I revert https://github.com/tensorflow/tensorflow/pull/31481, which is a PR that make allreduce ops scheduled as early as possible. \r\n`collective_ops.all_reduce` with the ring implementation does not seem to have similar issue, but it significantly slows down my training.\r\n\r\ncc @dubey @yuefengz @chsigg  who may have context on this issue.", "comments": ["Hi @ppwwyyxx, I ran this script with 2.2.0 and 2 GPUs and it completed training without error. I also tried with 4 GPUs and saw no error. I was running it on GCP with the Deep Learning VM Image, so possibly some slight (relevant) differences in the versions.\r\n\r\nAm I understanding correctly that if PR #31481 is reverted, then you are able to run the script without error. But occasionally the script will fail?", "Thanks for the response. I just did a fresh reinstall of TF2.2 from `pip install tensorflow==2.2`. Then I found that:\r\n* I **can** reliably (3 out of 3 times all within 500 steps) reproduce the issue on a machine with only 2 P100s using all 2 GPUs\r\n* I **cannot** reproduce it on a machine with 8 V100s (to be precise, this is V100-SXM2-32GB), using first 2 or first 4 of the 8 GPUs\r\n* I **can** reliably (3 out of 3 times all within 1000 steps) reproduce it on a machine with 8 V100s (V100-SXM2-32GB) using all 8 GPUs (run with `--gpu 8`)\r\n* I **can** reliably reproduce it on a machine with 2 V100s (Quadro GV100) using all 2 GPUs.\r\n\r\nIt looks like it may require using all GPUs on a machine to reproduce.\r\nSince it's likely a concurrency issue, it's very possible that hardware and other softwares (cuda, cudnn, etc) both affect it. I hope the above details help find the right environment. If not, I may try narrow it down further (e.g. provide a docker?).\r\n\r\nIf I revert the PR, I stopped seeing the above simple script failing, but my original training (to which I added a similar check) still can fail, but fails much more infrequently.", "@nikitamaia any progress in reproducing the error?", "I was able to reproduce the error but without much consistency. have 4 P100s on my machine and running  `TF2_BEHAVIOR=0 python a.py --gpu 4` doesn't fail every time. When I did see the error it was just after step 500. So far running with 2/4 GPUs hasn't produced the error.\r\n", "Thanks. At least it proves the bug exists. I'm looking forward to a fix given the seriousness of the issue.", "Hi @ppwwyyxx can you confirm that you run `collective_ops.all_reduce` with NCCL and not the default ring implementation?  It isn't really expected that `collective_ops.all_reduce` is slower than `nccl_ops.all_sum`.", "Oh, I was not clear about that. I was trying `collective_ops.all_reduce` with the default `communiation_hint`, so it's likely not using nccl, which is probably why it does not have the same bug. I have not tried letting `collective_ops.all_reduce` use nccl.", "I can reproduce the bug using `collective_ops.all_reduce(communication_hint='nccl')`. I've not used it before so I might not be using it correctly. The updated script looks like this:\r\n<details>\r\n<summary>click</summary>\r\n\r\n```python\r\n#!/usr/bin/env python\r\nimport argparse\r\nimport threading\r\nfrom tensorflow.compat import v1 as tf\r\nimport tqdm\r\n\r\ndef split_grad_list(grad_list):\r\n    g = []\r\n    v = []\r\n    for tower in grad_list:\r\n        g.append([x[0] for x in tower])\r\n        v.append([x[1] for x in tower])\r\n    return g, v\r\n\r\n_module_lock = threading.Lock()\r\n_shared_cnt_counter = 0\r\n\r\n\r\ndef _get_shared_cnt():\r\n    global _shared_cnt_counter\r\n\r\n    with _module_lock:\r\n        val = _shared_cnt_counter\r\n        _shared_cnt_counter += 1\r\n    return val\r\n\r\ndef allreduce_grads(all_grads):\r\n    # reduce gradients for N variables on K devices\r\n    from tensorflow.python.ops import nccl_ops as nccl\r\n    nr_tower = len(all_grads)\r\n    assert nr_tower > 1\r\n    new_all_grads = []  # N x K\r\n    for grads in zip(*all_grads):\r\n        USE_NCCL_OPS = True\r\n        # k grads\r\n        if USE_NCCL_OPS:\r\n            summed = nccl.all_sum(grads)\r\n        else:\r\n            from tensorflow.python.ops import collective_ops\r\n            summed = []\r\n            shared_cnt = _get_shared_cnt()\r\n            for t in grads:\r\n                with tf.device(t.device):\r\n                    t = collective_ops.all_reduce(\r\n                        t, len(grads), 3, shared_cnt + 300,\r\n                        'Add', 'Id', communication_hint='nccl')\r\n                    summed.append(t)\r\n\r\n        grads_for_devices = []  # K\r\n        true_sum = tf.add_n(grads)\r\n        for g in summed:\r\n            diff = tf.abs(true_sum - g)\r\n            eql = diff < 1e-4\r\n            nccl_res_correct = tf.reduce_all(eql, name=\"corr_\" + grads[0].op.name)\r\n\r\n            def flat(x):\r\n                x = tf.reshape(x, [-1])\r\n                x = tf.slice(x, [0], [tf.minimum(tf.size(x), 200)])\r\n                return x\r\n\r\n            assert_op = tf.debugging.Assert(nccl_res_correct, [\r\n                tf.reduce_max(diff), flat(true_sum), flat(g)], summarize=1000,\r\n                name='assert_' + grads[0].op.name)\r\n            with tf.control_dependencies([assert_op]):\r\n                g = tf.identity(g)\r\n            grads_for_devices.append(g)\r\n        new_all_grads.append(grads_for_devices)\r\n    # transpose to K x N\r\n    ret = list(zip(*new_all_grads))\r\n    return ret\r\n\r\ndef build_graph(image, label, idx):\r\n    v1 = tf.get_variable('aaa/W', shape=[3, 3, 3, 64], trainable=True)\r\n    v2 = tf.get_variable('bbb/W', shape=[3, 3, 3, 64], trainable=True)\r\n    v = v1 if idx == 0 else v2\r\n    image = tf.nn.conv2d(image, v, 1, padding='SAME', data_format='NCHW')\r\n\r\n    def conv(name, x, chan, stride=1):\r\n        with tf.variable_scope(name):\r\n            in_chan = x.shape[1]\r\n            W = tf.get_variable('W', [3, 3, in_chan, chan])\r\n            ret = tf.nn.conv2d(x, W, strides=stride, padding=\"SAME\", data_format=\"NCHW\")\r\n            return tf.nn.relu(ret)\r\n\r\n    x = conv('conv1', image, 64)\r\n    x = conv('conv2', x, 64)\r\n    x = conv('conv3', x, 1280, stride=2)\r\n    x = conv('conv4', x, 1280, stride=2)\r\n    x = conv('conv5', x, 10)\r\n    logits = tf.reduce_mean(x, axis=[2, 3])\r\n    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\r\n    cost = tf.reduce_mean(cost, name='cross_entropy_loss')\r\n    return cost\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--gpu', type=int)\r\n    args = parser.parse_args()\r\n    num_gpu = args.gpu\r\n\r\n    with tf.Graph().as_default():\r\n        opt = tf.train.GradientDescentOptimizer(0.001)\r\n\r\n        grad_list = []\r\n        for k in range(num_gpu):\r\n            with tf.device(\"/gpu:{}\".format(k)), tf.variable_scope(\"tower{}\".format(k)):\r\n                print(\"Building {} ...\".format(k))\r\n                image = tf.random.uniform([32, 3, 30, 30])\r\n                label = tf.random.uniform([32], maxval=9, dtype=tf.int32)\r\n                cost = build_graph(image, label, k)\r\n                varlist = [x for x in tf.trainable_variables() if x.name.startswith(\"tower{}\".format(k))]\r\n                print(\"Varlist for tower {}: \".format(k), [x.name for x in varlist])\r\n                wd_cost = [tf.reduce_sum(x) * 1e-3 for x in varlist]\r\n                cost = tf.add_n([cost] + wd_cost)\r\n                grads = opt.compute_gradients(cost, var_list=varlist)\r\n                grad_list.append(grads)\r\n\r\n        all_grads, all_vars = split_grad_list(grad_list)\r\n        all_grads = allreduce_grads(all_grads)\r\n        grad_list = [list(zip(gs, vs)) for gs, vs in zip(all_grads, all_vars)]\r\n\r\n        train_ops = []\r\n        for idx, grad_and_vars in enumerate(grad_list):\r\n            with tf.device('/gpu:{}'.format(idx)):\r\n                train_ops.append(opt.apply_gradients(\r\n                    grad_and_vars, name='apply_grad_{}'.format(idx)))\r\n        train_op = tf.group(*train_ops)\r\n\r\n        sess = tf.Session()\r\n        sess.run(tf.global_variables_initializer())\r\n        print(\"Training ...\")\r\n        for k in tqdm.trange(5000):\r\n            sess.run(train_op)\r\n```\r\n</details>", "Going through your code, my best guess is there is a problem (race condition?) with the way `true_sum` (the source of truth) is calculated in the code. I changed your code by ensuring the `true_sum` is calculated at the same time as `tf.all_sum`:\r\n```\r\ntrue_sum = tf.add_n(grads)\r\nwith tf.control_dependencies([true_sum.op]):\r\n    summed = nccl.all_sum(grads)\r\n```\r\nOn my setup, the assert errors are gone. Please let me know if this fixes your problem as well.\r\n\r\nOn the reason why `collective_ops.all_reduce` does not cause this issue, my best explanation is since the ring algorithm is significantly slower than nccl native implementation, the`true_sum` calculation always happens to run before `all_sum`. ", "This change does make the error disappear. However, I don't think this supports the hypothesis that `true_sum` is incorrect. I think the error probably disappears in the same way why reverting #31481 can make the error disappear: reverting the PR will delay the execution of NCCL allreduce. And my guess is that there are issues in Tensorflow or NCCL that can be triggered unless they are explictly delayed. Also, assuming your hypothesis is correct, it still means that there is a serious bug in tensorflow.\r\n\r\nI believe the result of `nccl.all_sum` is incorrect because this is how I found the bug and made the above repro: a model that I regularly train with NCCL started to diverge after #31481 was added. And if I use either `tf.add_n` to aggregate gradients, or if I revert #31481, it can converge.\r\n\r\n> Please let me know if this fixes your problem as well.\r\n\r\nThis does not fix the problem because in a real training I expect to use only NCCL (not `tf.add_n`) for its speed, and I believe its results are incorrect.", "I also have another guess for which I don't have much evidence: I think NCCL allreduce can more likely produce wrong results when different workers compute gradients in different orders, although this situation is part of the consideration of `NcclManager`. \r\n\r\nThis is completely a guess from intuition. This guess leads me to create the above example, in which I explicitly let each worker use the variables differently so that they will compute the gradients w.r.t `aaa/w` and `bbb/W` in a different order: some workers use them at the beginning of the model, while some other workers use them only for regularization loss.", "Any updates after a month?", "Any updates after a month?", "TensorFlow is reproducibly giving __wrong gradients__, but there are no updates to address it in >2 months. This is very surprising.", "@ppwwyyxx Thank you for your follow up. After the [fix to your repro code](https://github.com/tensorflow/tensorflow/issues/41539#issuecomment-667348808) the error could not reproduced. You mentioned you have other instances which experience the same error. Could you possibly share a repro code?", "I would not call it a \"fix\" because the original code does not contain user-side bugs (at least not pointed out so far).\r\n\r\nThe \"fix\" is in fact a __different way to write the code__ that no longer shows the bug. But as I mentioned in https://github.com/tensorflow/tensorflow/issues/41539#issuecomment-667358562, this alternative way does not apply in a real training, because in reality it would be dumb and extremely inefficient to compute the sum twice.", "As you mentioned, the original code indeed shows an error. What it does not reproduce is what the title suggests, \"nccl_ops.all_sum does not correctly reduce gradients\". The \"fix\" just makes sure the source of truth (true_sum) is calculated at the same time as \"nccl_ops.all_sum\". The \"fix\" is in no way a suggestion to fix the original error (whatever it may be).\r\n", "Nevertheless, I will escalate this bug. Hopefully it is fixed soon.", "My model with batch size = 8 and [sync bn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/normalization_v2.py) worked very well at TPU, but when use 2\u00d7V100, the result is  1-2% lower\r\nI use `TPUStrategy` for TPU and  `MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.RING)` for GPU.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41539\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41539\">No</a>\n", "We have reverted #31481.  Please reopen if this is still an issue.", "Why does #31481 cause this issue?", "We have reverted #31481 to unblock users, but we don't have a good handle on the root cause.  We will continue to investigate the root cause.", "Sounds good! Thanks for investigating. Shall we keep the issue open then, just like #41980 ?"]}, {"number": 41538, "title": "  The memory required for the training model is equivalent to the size of \u201cmodel.ckpt-0.data\u201d\uff1f", "body": "I want to allocate enough memory space in advance for my model.\r\nThe size of \"model.ckpt-0.data\" is 1GB,\r\nDoes it mean that the memory consumed during training is about 1GB.", "comments": ["You could try [this](https://stackoverflow.com/questions/36331419/tensorflow-how-to-measure-how-much-gpu-memory-each-tensor-takes) on colab before allocating memory locally.", "@AI-Friend \r\nIf you want to use colab, you may select gpu by changing runtime type and run your code.", "Thank you\uff0cI see!  \r\nI use \"Hook\" to save the last five models by default. Now I want to save only one model to save space, \r\nbut I can't find parameters like \u201ctf.train.Saver :max_to_keep in \"SummarySaverHook\" or \"ProfilerHook\".\r\nDoes hook have similar parameters\uff1f\r\n\r\nThanks!\r\n\r\n\r\n\r\n", "If you can use estimators then it provides a handy api [tf.estimator.BestExporter](https://www.tensorflow.org/api_docs/python/tf/estimator/BestExporter) which can be useful in your case.\r\nYou can also see external contributors https://github.com/vonclites/checkmate wrapper for `tf.train.Saver` api.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41537, "title": "From Keras 2.3.1 to 2.4: Input 0 of layer fc1 is incompatible with the layer", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): It's taken from a deep learning book.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.5, 2.0, 2.2, 2.3\r\n- Python version: 3.6, 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: GTX 1060 6Go\r\n\r\n1. Local machine: v1.15.2-30-g4386a66 1.15.3\r\n2. Google Colab: v2.2.0-0-g2b96f3662b 2.2.0\r\n\r\n**Describe the current behavior**\r\nI change the input shape of the VGG19 model.but w\r\nOn Keras 2.3.1 it works well using Keras imports. When changing to Tensorflow imports, and Keras 2.4.0, I get an error with the same code.\r\n\r\n**Describe the expected behavior**\r\nI expect the input shape of VGG19 to change with the new shape as it works on Keras 2.3.1\r\n\r\n**Standalone code to reproduce the issue**\r\nGoogle Colab: https://colab.research.google.com/drive/1IT9e2aYEodW5tO-Z8fEeszgVHA41R4WY?usp=sharing\r\n\r\nWith Keras 2.4\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import Input\r\nfrom tensorflow.keras.applications import VGG19\r\nfrom tensorflow.keras.models import Model\r\n\r\ninput_shape = (256, 256, 3)\r\n# Load a pre-trained VGG19 model trained on 'Imagenet' dataset\r\nvgg = VGG19(weights=\"imagenet\")\r\nvgg.outputs = [vgg.layers[9].output]\r\n\r\ninput_layer = Input(shape=input_shape)\r\n\r\n# Extract features\r\nfeatures = vgg(input_layer)\r\n\r\n# Create a Keras model\r\nmodel = Model(inputs=[input_layer], outputs=[features])\r\n```\r\nOn the `features = vgg(input_layer)` line I get the error:\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-9b16ae481ea8> in <module>\r\n      1 # Extract features\r\n----> 2 features = vgg(input_layer)\r\n\r\n~/anaconda3/envs/srgan/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    852                     outputs = base_layer_utils.mark_as_return(outputs, acd)\r\n    853                 else:\r\n--> 854                   outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    855 \r\n    856             except errors.OperatorNotAllowedInGraphError as e:\r\n\r\n~/anaconda3/envs/srgan/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in call(self, inputs, training, mask)\r\n    693                                 ' implement a `call` method.')\r\n    694 \r\n--> 695     return self._run_internal_graph(inputs, training=training, mask=mask)\r\n    696 \r\n    697   def compute_output_shape(self, input_shape):\r\n\r\n~/anaconda3/envs/srgan/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)\r\n    842 \r\n    843           # Compute outputs.\r\n--> 844           output_tensors = layer(computed_tensors, **kwargs)\r\n    845 \r\n    846           # Update tensor_dict.\r\n\r\n~/anaconda3/envs/srgan/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    817         # are casted, not before.\r\n    818         input_spec.assert_input_compatibility(self.input_spec, inputs,\r\n--> 819                                               self.name)\r\n    820         graph = backend.get_graph()\r\n    821         with graph.as_default(), backend.name_scope(self._name_scope()):\r\n\r\n~/anaconda3/envs/srgan/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/input_spec.py in assert_input_compatibility(input_spec, inputs, layer_name)\r\n    211                 ' incompatible with the layer: expected axis ' + str(axis) +\r\n    212                 ' of input shape to have value ' + str(value) +\r\n--> 213                 ' but received input with shape ' + str(shape))\r\n    214     # Check shape.\r\n    215     if spec.shape is not None:\r\n\r\nValueError: Input 0 of layer fc1 is incompatible with the layer: expected axis -1 of input shape to have value 25088 but received input with shape [None, 32768]\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nWorking code with Keras 2.3.1:\r\n\r\n```\r\nfrom tensorflow import keras\r\nfrom keras import Input\r\nfrom keras.applications import VGG19\r\nfrom keras.models import Model\r\n\r\ninput_shape = (256, 256, 3)\r\n\r\n# Load a pre-trained VGG19 model trained on 'Imagenet' dataset\r\nvgg = VGG19(weights=\"imagenet\")\r\nvgg.outputs = [vgg.layers[9].output]\r\n\r\ninput_layer = Input(shape=input_shape)\r\n\r\n# Extract features\r\nfeatures = vgg(input_layer)\r\n\r\n# Create a Keras model\r\nmodel = Model(inputs=[input_layer], outputs=[features])\r\n```\r\n_Note_: code is the same in both cases, just the imports change.\r\n\r\n\r\n\r\n", "comments": ["I have tried in colab with TF version 2.2, 3.2-rc1, nightly versions(`2.4.0-dev20200719`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/39ed7b1b24f174af015af8e2af2a7b07/untitled150.ipynb).Thanks!", "Should this 2.4.0 code have the same behavior as the 2.3.1 code?\r\n\r\n```\r\nfrom tensorflow.keras.applications import VGG19\r\nfrom tensorflow.keras.models import Model\r\n\r\nvgg = VGG19(weights=\"imagenet\",include_top=False, input_shape=(256, 256, 3))\r\nmodel = Model(inputs=vgg.inputs, outputs=vgg.layers[9].output)\r\n```", "@houseofai The behaviour is the same in tensorflow 2.3.0 and tf-nightly. Please find the gist [here](https://colab.research.google.com/gist/gowthamkpr/f8dd32e7a894589ba5b8916955021330/untitled314.ipynb)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41537\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41537\">No</a>\n"]}, {"number": 41536, "title": "Extending ctc_batch_cost to handle RaggedTensors", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nThis is with respect to https://github.com/tensorflow/tensorflow/issues/41276. Basically, I am trying to extend this [official Keras example on OCR](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/captcha_ocr.ipynb) to the [IAM dataset](http://www.fki.inf.unibe.ch/DBs/iamDB/iLogin/index.php) which consists of images of handwritten characters. Here are some samples from the dataset:\r\n\r\n![image](https://user-images.githubusercontent.com/22957388/87866184-23261180-c99c-11ea-8c98-1f9e4b6269c5.png)\r\n\r\n\r\n In order for the CTC loss to work (calculated with `ctc_batch_cost`) the ground truth labels and the targets should not be variable-length sequences (`RaggedTensors`s) which is not the case here ([reference](https://github.com/tensorflow/tensorflow/issues/41276#issuecomment-659466273)). So my question is how to extend `ctc_batch_cost` in this case. \r\n\r\n### Source code / logs\r\n[Colab Notebook](https://colab.research.google.com/gist/amahendrakar/71ce949e24f2ac3a4c8ad6ad71590a04/41276-2-3.ipynb)\r\n\r\nCc: @alextp \r\n", "comments": ["@nikitamaia any updates on this?", "@nikitamaia @sayakpaul \r\n\r\nAny update on this ?\r\n\r\nThanks", "@menonnsl here's how I tackled it - \r\n\r\n* Your inputs can have dynamic shapes. The catch is within a batch all the samples should be of the same length. \r\n* So, while processing each batch I took the highest length of the sample from the current batch and padded the other samples (belonging to the current batch) to that length. You could also compute the global maximum sequence length and pad the other samples to that length. \r\n\r\nIf you process your data in this way, you should hopefully be able to start the training. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41536\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41536\">No</a>\n"]}, {"number": 41535, "title": "KeyError: 'name' when try to load saved model with tf.keras.models.load_model()", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: None\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: NVIDIA GeForce GTX 750 Ti 10GB (2GB Dedicated Memory; 8GB Shared Memory)\r\n\r\n**Describe the current behavior**\r\nI keep getting an `KeyError: 'name'` when trying to load a saved model (.pb) with `tf.keras.models.load_model(path)`\r\n\r\n**Describe the expected behavior**\r\nLoad the model successfully.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSnippet where the error happens. \r\n```python\r\nimport tensorflow as tf \r\n\r\n# Load model\r\nmodel = tf.keras.models.load_model(\"models/bs-32-ep-10-1595106737.3691123/\") # Error happens here\r\n\r\n# Do stuff\r\n...\r\n```\r\n\r\n**Other info / logs**\r\n\r\nMy model is being saved within a class that inherits from `tf.keras.models.Sequential()`.\r\nI haven't got any errors while training or saving my model.\r\n\r\n```python\r\ndef train(self):\r\n        self.compile(\r\n            loss=self.loss,\r\n            optimizer=self.optimizer(lr=0.0001, decay=1e-6),\r\n            metrics=[*self.mtcs]\r\n        )\r\n\r\n        self.fit(\r\n            x=self.train_data[0],\r\n            y=self.train_data[1],\r\n            batch_size=self.batch_size,\r\n            epochs=self.epochs,\r\n            validation_data=(*self.test_data,),\r\n            callbacks=[*self.callbacks]\r\n        )\r\n\r\n        if self.save_after_training:\r\n            self.save(\"models/{}\".format(self.NAME)) # Save Model\r\n```\r\nThe Error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"RNN.py\", line 63, in <module>\r\n    model = tf.keras.models.load_model(\"models/bs-32-ep-10-1595106737.3691123/\")\r\n  File \"C:\\Users\\EWC\\github\\GLAI_\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\", line 190, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"C:\\Users\\EWC\\github\\GLAI_\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 116, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"C:\\Users\\EWC\\github\\GLAI_\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 604, in load_internal\r\n    export_dir)\r\n  File \"C:\\Users\\EWC\\github\\GLAI_\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 188, in __init__\r\n    super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n  File \"C:\\Users\\EWC\\github\\GLAI_\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 123, in __init__\r\n    self._load_all()\r\n  File \"C:\\Users\\EWC\\github\\GLAI_\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 215, in _load_all\r\n    self._finalize_objects()\r\n  File \"C:\\Users\\EWC\\github\\GLAI_\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 510, in _finalize_objects\r\n    self._reconstruct_all_models()\r\n  File \"C:\\Users\\EWC\\github\\GLAI_\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 528, in _reconstruct_all_models\r\n    self._reconstruct_model(model_id, model, layers)\r\n  File \"C:\\Users\\EWC\\github\\GLAI_\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 564, in _reconstruct_model\r\n    config, created_layers={layer.name: layer for layer in layers})\r\n  File \"C:\\Users\\EWC\\github\\GLAI_\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 2019, in reconstruct_from_config\r\n    process_layer(layer_data)\r\n  File \"C:\\Users\\EWC\\github\\GLAI_\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1993, in process_layer\r\n    layer_name = layer_data['name']\r\nKeyError: 'name'\r\n```\r\n\r\nIf more code is required, please let me know. Thank you", "comments": ["Could you link the whole code snippet, so I can check it out?", "[Link to colab](https://colab.research.google.com/drive/1Wytmka9mPZZhQvC2KmqOfDL17hW9Reyp?usp=sharing). ", "Have you found the solution?", "@gustavo-dev \r\nPlease follow [this link]( https://www.tensorflow.org/tutorials/keras/save_and_load?hl=en) as you have not saved the model, and let us know", "I have saved\r\n![image](https://user-images.githubusercontent.com/42685889/89104269-6ea1dc00-d3ee-11ea-891e-806f7e8b87e7.png)\r\n\r\n", "This error also occurs in 2.3.0", "Was able to reproduce the error with tf 2.3.0 and tf-nightly-gpu. Heres the [gist for tf 2.3](https://colab.research.google.com/gist/gowthamkpr/4ad5f0b4baf46e07e8c9db4e187eaf8c/load_model-error.ipynb)\r\n and [tf-nightly](https://colab.research.google.com/gist/gowthamkpr/e61577cc56e804471b524c5dcf0cb9f6/load_model-error.ipynb)\r\n", "Same with 2.3.0 with models reloaded from  https://github.com/chrisfcarroll/ml-faster/tree/2020-Oct-16\r\nAlso with models in a class inheriting from Sequential", "Encountered when saving and loading in 2.2.0. Module is a class inherited from Sequential.", "My experience with `KeyError: 'name'` with `tf.keras.models.load_model()` was because of the `get_config()` implementation of a custom layer that did not include the base config from the parent (tested with tensorflow 2.2.0).  \r\nMore details: https://github.com/tensorflow/models/issues/8692#issuecomment-727033061", "I went digging and found that when keras tries to reload a model which is a subclass of sequential, it attempts to reload it as a functional model instead of a sequential model.\r\n\r\nDigging into the load/save code I found [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/saved_model/load.py#L472) - which seems to be perhaps a proximal cause.\r\n\r\nAn subclass of sequential will not have the class name `Sequential` which forces saving and loading as a Functional model. I'm not sure why this causes the 'name' KeyError, but at the very least there is a work around which is to avoid sub-classing `keras.models.Sequential`\r\n", "Was able to replicate the issue in TF 2.6.0-dev20210530,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/27155855058c112a3749601aeb449055/untitled116.ipynb#scrollTo=GmOlrSwMQakH)..Thanks !", "Hi @gustavo-dev ! I think issue is getting resolved in [2.8 ](https://colab.sandbox.google.com/gist/mohantym/39284ae747126ba7b4b29de05d590ad1/untitled116.ipynb#scrollTo=r8pcFteDQ7fm)version.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41535\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41535\">No</a>\n", "@google-ml-butler This is still an issue. @gustavo-dev can you please reopen.", "Hi @rickstaa ! This issue replicated in[ 2.3](https://github.com/tensorflow/tensorflow/issues/41535#issuecomment-682250730) even after saving the loading of the model. The same pattern was not observed in 2.8. \r\n\r\nAnyway , I **removed the self.save_after_training condition from below snippet** in the train block and added a print line to show the model path name too. I was able to run without error then. (Refresh from folder in Colab once before you load the model)\r\n\r\n```\r\n if self.save_after_training:\r\n            self.save(\"models/{}\".format(self.NAME)) # Save Model .\r\n```\r\n\r\nAttaching fresh [gist ](https://colab.sandbox.google.com/gist/mohantym/fa1f343fbe4f6b9d7b2e19fef5e4dd47/load_model-error.ipynb#scrollTo=1FqIwG5wGncE)for reference. \r\n\r\nPlease raise a fresh ticket in Keras repo if you are still looking for assistance. Thanks!"]}, {"number": 41534, "title": "TensorFlow imports into python3 correctly, but I receive unexpected messages", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NVIDIA Jetpack4.3 Ubuntu 18.0.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Jetson Nano\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.1.0+nv20.3\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI followed these instruction to install TensorFlow 2.1.0 on Jetpack4.3 on my Jetson Nano: https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html except that I installed TensorFlow in a virtual environment (py3cv4) per Section 3 Installing TensorFlow/3.1 Installing Multiple Versions.\r\n\r\nWhen I am in my virtual environment if I start python3 and import tensorflow, I get the following response before the command line prompt returns:\r\n\r\n(py3cv4) thomas@thomas-desktop:~$ python3\r\nPython 3.6.9 (default, Apr 18 2020, 01:56:04)\r\n[GCC 8.4.0] on linux\r\nType \u201chelp\u201d, \u201ccopyright\u201d, \u201ccredits\u201d or \u201clicense\u201d for more information.\r\n\r\nimport tensorflow\r\n2020-07-18 17:28:48.819958: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-07-18 17:28:51.385750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2020-07-18 17:28:51.387403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\n\r\nIs this an issue I should be concerned about or did I miss installing a particular TensorFlow dependency?\r\n\r\nRegards,\r\nTCIII", "comments": ["No you shouldn't be, these are just loader messages that CUDA libs have been communicated with.", "@lordtt13 ,\r\n\r\nThanks for the response, much appreciated.\r\nRegards,\r\nTCIII", "If that's resolved your issue, please close it.", "@lordtt13,\r\n\r\nI will close it as soon as I get concurrence from the NVIDIA Developer Forum as this is their version of Tensorflow for the Jetson Nano Jetpack4.3.\r\nHowever I will not let this issue drag on very long if I do not hear from them.\r\nRegards,\r\nTCIII", "@TCIII \r\n\r\nThe following NVIDIA\u00ae software must be installed on your system:\r\n\r\nNVIDIA\u00ae GPU drivers \u2014CUDA 10.1 requires 418.x or higher.\r\nCUDA\u00ae Toolkit \u2014TensorFlow supports CUDA 10.1 (TensorFlow >= 2.1.0)\r\nCUPTI ships with the CUDA Toolkit.\r\ncuDNN SDK (>= 7.6)\r\n(Optional) TensorRT 6.0 to improve latency and throughput for inference on some models.\r\nYou can find a tutorial here: https://medium.com/@ardianumam/installing-tensorrt-in-ubuntu-dekstop-1c7307e1dcf6", "The NVIDIA developers forum concurs that this is normal and there is no problem with my installation.\r\nRegards,\r\nTCIII", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41534\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41534\">No</a>\n"]}, {"number": 41533, "title": "Can Keras be installed to work with the TensorFlow 1.15.0 for C++ binary or should I build TensorFlow 2.0 for C++ from source to get Keras?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS Oracle VM Virtualbox inside Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): Binary https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.15.0.tar.gz\r\n- TensorFlow version: 1.15.0\r\n- Python version: Actually I'm using the C++ 0.29.0 extension in Visual Studio Code 1.47.1, but my Python version is Python 3.8.2\r\n- Installed using virtualenv? pip? conda?: No installed from TensorFlow C++ binary libtensorflow-gpu-linux-x86_64-1.15.0.tar.gz\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a?\r\n- GPU model and memory: NVIDIA GeForce GTS 965M with 2,048 MB GDDR5 memory. 32 GB RAM.\r\n\r\n**Describe the problem**\r\n\r\nI do not know how to install Keras in TensorFlow 1.15.0 for C++ even though I have previously installed and used Keras in Python.  The description of Keras on their website suggests that Keras only works in Python and cannot be used in C++, while on the other hand I heard that TensorFlow 2.0 has Keras inside so maybe I could build that from source to work in C++?\r\n\r\n> Keras is a deep learning API written in **Python**, running on top of the machine learning platform TensorFlow.\r\n\r\nhttps://keras.io/about/\r\n\r\nOn the other hand I heard that TensorFlow 2.0 has Keras inside:\r\n\r\n> Now with version 2, TensorFlow includes Keras built it.\r\n\r\nhttps://towardsdatascience.com/creating-a-tensorflow-cnn-in-c-part-2-eea0de9dcada\r\n\r\nSo would it be possible to build TensorFlow 2.0 alpha in C++ to get Keras that way?\r\n\r\n> Build TensorFlow 2.0 Alpha from source\r\n\r\nhttps://itnext.io/how-to-use-your-c-muscle-using-tensorflow-2-0-and-xcode-without-using-bazel-builds-9dc82d5e7f80\r\n\r\nIs this where the TensorFlow 2.0 source is?\r\nhttps://github.com/tensorflow/tensorflow\r\n\r\nI want to use Keras in C++ because I am doing a Udemy.com C++ online course capstone project to use a Convolutional Neural Network for image classification.  I have successfully installed TensorFlow 1.15.0 for C++ by following the directions here: https://www.tensorflow.org/install/lang_c and some C++ TensorFlow test code worked.\r\n\r\nSo the installation instructions for Keras say do this but the problem is I think that's for python only not C++:\r\n```\r\nsudo apt install python3-pip\r\npip install --upgrade pip\r\npip install keras\r\npip install --upgrade keras\r\n```\r\nhttps://linuxize.com/post/how-to-install-pip-on-ubuntu-18.04/\r\nhttps://www.liquidweb.com/kb/how-to-install-keras/\r\n\r\nThe alternative is to not use Keras to do the Convolutional Neural Network project and maybe this tutorial would show me how to do that, though they say that Keras is more efficient than doing it without Keras:\r\nhttps://itnext.io/how-to-use-your-c-muscle-using-tensorflow-2-0-and-xcode-without-using-bazel-builds-9dc82d5e7f80\r\nhttps://itnext.io/creating-a-tensorflow-dnn-in-c-part-1-54ce69bbd586\r\nhttps://towardsdatascience.com/creating-a-tensorflow-cnn-in-c-part-2-eea0de9dcada\r\nhttps://towardsdatascience.com/creating-a-tensorflow-cnn-in-c-part-3-freezing-the-model-and-augmenting-it-59a07c7c4ec6\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@ProfHariSeldon tensorflow 2.x comes with Keras built in. However you can't find much documentation using it. You can follow this [example](https://towardsdatascience.com/creating-a-tensorflow-cnn-in-c-part-2-eea0de9dcada#:~:text=Now%20with%20version%202%2C%20TensorFlow,and%20documentation%20are%20in%20Python.) for implementing Convolution neural network in C++", "> @ProfHariSeldon tensorflow 2.x comes with Keras built in. However you can't find much documentation using it. You can follow this [example](https://towardsdatascience.com/creating-a-tensorflow-cnn-in-c-part-2-eea0de9dcada#:~:text=Now%20with%20version%202%2C%20TensorFlow,and%20documentation%20are%20in%20Python.) for implementing Convolution neural network in C++\r\n\r\nOne more question: If I build tensorflow 2.x from source will it work with C++?  Or NOT work with C++ and only work with Python?\r\n\r\nThe instructions online (https://www.tensorflow.org/install/source) seem to imply that tensorflow 2.x from source only works with Python because it has a whole \"Python version\" column in the table and NO \"C++ version\" column in the table.", "@gowthamkpr I got TensorFlow 2.2.0 to compile but Visual Studio Code does not know how to #include the files from the 2.2.0 compiled source.  I got Visual Studio Code to #include and run test C++ code when I installed the 1.15.0 binary, but that binary does not have keras, which I want because I am trying to do a capstone coding project for an online C++ course and keras is more efficient and possibly more convenient than doing a C++ Convolutional Neural Network image classifier manually without helpful packages.\r\n\r\nI followed these install instructions:\r\nhttps://gist.github.com/philwo/f3a8144e46168f23e40f291ffe92e63c\r\n\r\nThose instructions are nice because they tell me what to do from start to finish in full detail and is for Ubuntu not Mac like your example, however I worry that they actually tell me how to install TensorFlow for Python not C++ because they use bazel to do this:\r\n```\r\n$ ./configure\r\n$ bazel build //tensorflow/tools/pip_package:build_pip_package\r\n$ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n$ pip3 install --user /tmp/tensorflow_pkg/tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl\r\n```\r\n\r\nAnd your example does this instead with bazel (I tried it but it fails to compile, possibly because it is for Mac not Ubuntu):\r\n```\r\nbazel build -c opt \u2014 verbose_failures //tensorflow:libtensorflow_cc.so\r\nbazel build -c opt \u2014 verbose_failures //tensorflow:libtensorflow_framework.so\r\nbazel build -c opt \u2014 copt=-mavx \u2014 copt=-mavx2 \u2014 copt=-mfma \u2014 copt=-msse4.2 \u2014 verbose_failures //tensorflow:libtensorflow_framework.so\r\ntensorflow/contrib/makefile/download_dependencies.sh\r\ncd tensorflow/contrib/makefile/downloads/protobuf/\r\n./autogen.sh\r\n./configure\r\nmake\r\nmake install\r\n```\r\n\r\nI am worried about your example's instructions because they use libtensorflow in TensorFlow 2.0 and:\r\n\"Note: There is no libtensorflow support for TensorFlow 2 yet. It is expected in the 2.3 release. In the meantime please use nightly libtensorflow releases.\"\r\nhttps://www.tensorflow.org/install/lang_c\r\n\r\nSo I followed https://gist.github.com/philwo/f3a8144e46168f23e40f291ffe92e63c with these changes:\r\n```\r\nsudo curl -Lo /usr/local/bin/bazel https://github.com/bazelbuild/bazelisk/releases/download/v1.5.0/bazelisk-linux-amd64\r\n\r\ncurl -LO https://github.com/tensorflow/tensorflow/archive/v2.2.0.tar.gz\r\ntar xvfz v2.2.0.tar.gz\r\nrm v2.2.0.tar.gz\r\ncd tensorflow-2.2.0\r\n\r\n# ./configure.py:_TF_MAX_BAZEL_VERSION = '2.0.0'\r\n\r\necho \"2.0.0\" > .bazelversion\r\n```\r\n\r\nWhen I did \"bazel build //tensorflow/tools/pip_package:build_pip_package\" I got a successful message:\r\n```\r\n...\r\nINFO: From Executing genrule //tensorflow/lite/python/testdata:permute_float:\r\n2020-07-22 11:05:51.444193: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1 operators, 3 arrays (0 quantized)\r\n2020-07-22 11:05:51.444364: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1 operators, 3 arrays (0 quantized)\r\n2020-07-22 11:05:51.444434: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1 operators, 4 arrays (0 quantized)\r\n2020-07-22 11:05:51.444460: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 1 operators, 4 arrays (0 quantized)\r\n2020-07-22 11:05:51.444476: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 1 operators, 4 arrays (0 quantized)\r\n2020-07-22 11:05:51.444489: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Identify nearest upsample.: 1 operators, 4 arrays (0 quantized)\r\n2020-07-22 11:05:51.444507: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\r\n2020-07-22 11:05:51.444522: I tensorflow/lite/toco/toco_tooling.cc:456] Estimated count of arithmetic ops: 36 ops, equivalently 18 MACs\r\n2020-07-22 11:05:51.444531: I tensorflow/lite/toco/toco_tooling.cc:471] Number of parameters: 20\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 76329.124s, Critical Path: 260.54s\r\nINFO: 14283 processes: 14283 local.\r\nINFO: Build completed successfully, 15315 total actions\r\n```\r\n\r\nWhen I did the next step \"./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\" I got warnings:\r\n```\r\ntlroot@tlroot-VirtualBox:~/tensorflow-2.2.0$ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nWed 22 Jul 2020 11:49:17 AM EDT : === Preparing sources in dir: /tmp/tmp.8j7NtK55u8\r\n~/tensorflow-2.2.0 ~/tensorflow-2.2.0\r\n~/tensorflow-2.2.0\r\n~/tensorflow-2.2.0/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow ~/tensorflow-2.2.0\r\n~/tensorflow-2.2.0\r\n/tmp/tmp.8j7NtK55u8/tensorflow/include ~/tensorflow-2.2.0\r\n~/tensorflow-2.2.0\r\nWed 22 Jul 2020 11:49:37 AM EDT : === Building wheel\r\nwarning: no files found matching 'README'\r\nwarning: no files found matching '*.pyd' under directory '*'\r\nwarning: no files found matching '*.pyi' under directory '*'\r\nwarning: no files found matching '*.pd' under directory '*'\r\nwarning: no files found matching '*.dylib' under directory '*'\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nwarning: no files found matching '*.csv' under directory '*'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*.proto' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\nWed 22 Jul 2020 11:50:14 AM EDT : === Output wheel file is in: /tmp/tensorflow_pkg\r\ntlroot@tlroot-VirtualBox:~/tensorflow-2.2.0$\r\n```\r\n\r\nWhen I tried to run my test code I got errors:\r\n```\r\n#include <stdio.h>\r\n#include <tensorflow/c/c_api.h>\r\n\r\n#include \"/home/tlroot/tensorflow-2.2.0/tensorflow/c/c_api.h\"\r\n\r\nint main() {\r\n  printf(\"Hello from TensorFlow C library version %s\\n\", TF_Version());\r\n  return 0;\r\n}\r\n```\r\n\r\n```\r\n#include <tensorflow/c/c_api.h>\r\n```\r\ncannot open source file \"tensorflow/c/c_api.h\"C/C++(1696)\r\n\r\n```\r\n#include \"/home/tlroot/tensorflow-2.2.0/tensorflow/c/c_api.h\"\r\n```\r\n#include errors detected. Please update your includePath. Squiggles are disabled for this translation unit (/home/tlroot/Documents/C++/Capstone/CppND-Capstone-Hello-World/src/hello_tf.c).C/C++(1696)\r\ncannot open source file \"tensorflow/c/tf_attrtype.h\" (dependency of \"/home/tlroot/tensorflow-2.2.0/tensorflow/c/c_api.h\")C/C++(1696)\r\n\r\nI notice that the binary TensorFlow install placed files in /usr/local/include and /usr/local/lib which caused <tensorflow/c/c_api.h> to find that file because that file was in one of those usr directories.  The compiled TensorFlow source code does not put files there it puts them here:\r\n```\r\ntlroot@tlroot-VirtualBox:~/tensorflow-2.2.0$ find / -name c_api.h -print 2>/dev/null\r\n/home/tlroot/.cache/bazel/_bazel_tlroot/dec4da563955f967838996e74fa578f5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/c/c_api.h\r\n/home/tlroot/.cache/bazel/_bazel_tlroot/dec4da563955f967838996e74fa578f5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/c/eager/c_api.h\r\n/home/tlroot/tensorflow-2.2.0/tensorflow/lite/c/c_api.h\r\n/home/tlroot/tensorflow-2.2.0/tensorflow/c/c_api.h\r\n/home/tlroot/tensorflow-2.2.0/tensorflow/c/eager/c_api.h\r\n```\r\n\r\nI also tried \"tlroot@tlroot-VirtualBox:~/tensorflow-2.2.0$ sudo ldconfig\" to add the tensorflow-2.2.0 source code to the PATH, so that \"#include \"/home/tlroot/tensorflow-2.2.0/tensorflow/c/c_api.h\"\" might work but that did not solve the PATH error.\r\n\r\nIn case the TensorFlow build from source only installed for python3 not c++ I tried this and got an error:\r\n```\r\ntlroot@tlroot-VirtualBox:/home$ python3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n2020-07-22 15:30:10.496416: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2\r\n2020-07-22 15:30:10.506637: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2591995000 Hz\r\n2020-07-22 15:30:10.510569: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3d71670 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-22 15:30:10.510648: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\ntf.Tensor(535.93054, shape=(), dtype=float32)\r\n```", "@ProfHariSeldon Tensorflow 2.3.0 is now released. You can start using it", "Thanks that might solve it.  I also found out I can use OpenCV to solve this problem:\r\nhttps://answers.opencv.org/question/232940/help-updating-10-lines-of-deprecated-opencv-code-in-c/\r\n\r\nSpecifically by using cats.vs.dogs.cpp:\r\nhttps://gist.github.com/berak/70bcf5e8240c4af4426f9eff3f42121c", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41533\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41533\">No</a>\n"]}, {"number": 41532, "title": "2.3.0-rc2: Check failed: ret == 0 (11 vs. 0) Thread creation via pthread_create() failed. Aborted", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Red Hat Enterprise Linux Server release 7.6 (Maipo)`\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `v2.3.0-rc1-15-gbb3c460114 2.3.0-rc2`\r\n- Python version: `3.7.5`\r\n- CUDA/cuDNN version: `10.1.1/7.6.5.32`\r\n- GPU model and memory: `Tesla V100-PCIE-16GB`\r\n\r\n**Describe the current behavior**\r\n\r\nTensorFlow crashes:\r\n\r\n```\r\n2020-07-18 21:29:32.310794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:41:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\r\ncoreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\r\n2020-07-18 21:29:32.310851: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-18 21:29:32.310887: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-07-18 21:29:32.310898: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-07-18 21:29:32.310908: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-07-18 21:29:32.310918: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-18 21:29:32.310929: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-18 21:29:32.310943: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-18 21:29:32.311936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-07-18 21:29:32.311989: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-18 21:29:33.680712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-18 21:29:33.680781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-07-18 21:29:33.680813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-07-18 21:29:33.682225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14757 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:41:00.0, compute capability: 7.0)\r\n2020-07-18 21:29:33.694136: F tensorflow/core/platform/default/env.cc:72] Check failed: ret == 0 (11 vs. 0)Thread creation via pthread_create() failed.\r\nAborted\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nJust this:\r\n\r\n```\r\nimport tensorflow as tf\r\nx = tf.constant([1, 2]) \r\n```\r\n\r\n", "comments": ["@netw0rkf10w \r\nI ran the above shared code on nightly and on tf 2.2 and do not face any such issues, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/8d04cbff31eeb441d1108190e73c78ba/untitled284.ipynb).", "Please refer to [this issue](https://github.com/tensorflow/tensorflow/issues/18652 ) with similar error ", "@Saduf2019 Yes it seems related to that issue. What should I do then? It seems to have something to do with the hardware... Thanks.", "Did you try using a virtual environment ?", "@ymodak Thanks. The issue seems to come from the server I'm using.", "in my case, it occurs during training.  I have trained my model hundreds of epochs, this issue occurs", "@netw0rkf10w\r\nWhich problem was that? I faced same issue, and every time I try tensorflow function same error message is shown.\r\n(Check failed: ret == 0 (11 vs. 0)Thread creation via pthread_create() failed..) \r\nIs it related to the version of library? Or is it a problem of virtual environment?\r\n\r\n", "Hi @linaria-7726, @cl886699 \r\n\r\naccording to my own research, this error is generated by the pthread library, particularly its status code is 11 and its name is EAGAIN [1];\r\n\r\n_Insufficient resources to create another thread, or a system-imposed limit on the number of threads was encountered. The latter case may occur in two ways: the RLIMIT_NPROC soft resource limit (set via setrlimit(2)), which limits the number of process for a real user ID, was reached; or the kernel's system-wide limit on the number of threads, /proc/sys/kernel/threads-max, was reached._ \r\n\r\nSo if you are not running out of RAM, then it is likely that you need to increase the max number of threads of your system.\r\n\r\nNote that I am assuming that the problem is not related with the library version.\r\n\r\n[1] https://linux.die.net/man/3/pthread_create\r\n", "@xehartnort \r\n\r\nI am experiencing the same problem.\r\n\r\nI checked my system-imposed limit: RLIMIT_NPROC from /etc/security/limits.conf and number of threads from /proc/sys/kernel/threads-max. The former is 300 while the latter is 512198. I'm not 100% sure but at first thought both of them are large enough to create a new thread. The RAM quota is 4GB and the memory on GPU is 24GB.\r\n\r\nSo please someone let me know what the minimum required number of these things are. If my configuration all exceeds the minimum, I have to assume that the problem is related to GPU driver version and CUDA version."]}, {"number": 41531, "title": "tf.ragged.stack issue", "body": "tf.ragged.stack(input) return tf.Tensor if the first dim of the input is of length 1 and tf.RaggedTensor if not.\r\nThis is a problem as those types don't share the same interface and if the length of the input is not know in advance (as is often the case when using ragged tensors) one must have in \"if\" condition inside the graph to check the output type.\r\n\r\n```\r\ntf.ragged.stack([[1,2,3]]) => tf.Tensor([[1 2 3]], shape=(1, 3), dtype=int32)\r\ntf.ragged.stack([[1,2,3],[1,2,3]]) =>  <tf.RaggedTensor [[1, 2, 3], [1, 2, 3]]>\r\n\r\n```", "comments": ["I have tried in colab with TF version 2.2, 2.3-rc1, nightly version(`2.4.0-dev20200719`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/ca64601372cfa6fe3938590894ae5d51/untitled149.ipynb).Thanks!", "`tf.ragged.stack`'s first argument is a list of tensors, not a single tensor.  And the length of that list is always known in advance (i.e., at graph-construction time).  So you don't need any conditional **inside** the graph here.  \r\n\r\nIn general, `RaggedTensor` operations return a `tf.Tensor` if it can statically determine that the result has no ragged dimensions.  This has some advantages in terms of efficiency (`Tensor` operations are sometimes more efficient than `RaggedTensor` ops) and flexibility (`tf.Tensor` supports more operations than `RaggedTensor`).  So this is working as intended.\r\n\r\nThat said, I agree that it's annoying to have an API that may return either a `Tensor` or a `RaggedTensor` since, as you point out, they have somewhat different interfaces.  This is a side-effect of the current `RaggedTensor` design, where ragged tensors are defined recursively, and you can't create a `RaggedTensor` with `rank<2`.  We have some thoughts about how `RaggedTensor` could be refactored to avoid this (so ragged ops and ops that take ragged inputs would always return a `RaggedTensor` object, even if the result has no ragged dimensions); but it's somewhat difficult to change in a backwards-compatible way.\r\n\r\nFor now, I think the best solution for your case may be to write something like the following:\r\n\r\n```\r\nx = tf.ragged.stack([[1,2,3]])\r\nif isinstance(x, tf.Tensor):\r\n  x = tf.RaggedTensor.from_tensor(x)\r\n```\r\n\r\n(Note that this will fail with a shape error if `x.shape.rank<2`, since `RaggedTensor` can currently only be used if `rank>=2`.  Also, depending on what you're doing downstream of this operation, you may want to explicitly specify `ragged_rank` when calling `from_tensor`.)\r\n\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41531\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41531\">No</a>\n"]}, {"number": 41530, "title": "Dataset not repeating when ignoring errors and shuffling before indefinite repetition", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.2.0\r\n- Python version: Python 3.x\r\n- CUDA/cuDNN version: not using GPU\r\n- GPU model and memory: not using GPU\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using `tf.data.experimental.ignore_errors()`, indefinite repetition does not work if an error occurs during getting the elements to fill the shuffle buffer initially. I think it's a easier to see in code:\r\n\r\n```\r\ndef assert_greater_0(x):\r\n  tf.debugging.assert_greater(x, tf.convert_to_tensor(0))\r\n  return x\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 0, 3, 4])\r\ndataset = dataset.map(assert_greater_0)\r\ndataset = dataset.shuffle(buffer_size=3)\r\ndataset = dataset.repeat()\r\ndataset = tf.data.experimental.ignore_errors()(dataset)\r\n```\r\n\r\nyields a dataset that has 4 elements whereas adjusting the numbers in the initial list slightly\r\n\r\n```\r\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 0, 4])\r\ndataset = dataset.map(assert_greater_0)\r\ndataset = dataset.shuffle(buffer_size=3)\r\ndataset = dataset.repeat()\r\ndataset = tf.data.experimental.ignore_errors()(dataset)\r\n```\r\n\r\nyields an infinitely repeating dataset.\r\n\r\nMore data:\r\n* removing the shuffle produces infinite datasets in both cases.\r\n* specifying the number of repetitions as 2 produces the same result (8 elements) in both cases,\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect that the two code snippets both produce infinite datasets.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1quYzXz3inEiG6D7tfUhoGjapZbO8-AAA?usp=sharing", "comments": ["So I don't know why but the issue is with the ```assert_greater_0``` function, I tried it out using lambda in map and got infinite datasets, please check it out [here](https://gist.github.com/lordtt13/e79538a69ef69bb66fcfeeaa3a6701c7)", "You are right in the sense that the problem appears only with this function and the reason is that for one of the elements in the dataset it triggers a (in this case forced) error. The snippet is the distillation of a much larger pipeline with different errors and I simply tried to have the smallest example showing the behavior I'm referring to.\r\n\r\nThe ticket is about what happens when an error (violated assertion and I assume it happens for other errors as well) happens in a pipeline that specifies `tf.data.experimental.ignore_errors()`, which is about ignoring exactly those errors. My observation is that when you use this functionality provided by tf, then indefinite dataset repetition after shuffling does not work if the error in the error occurs at a position that is smaller than the shuffle buffer size. If the error happens later, then `tf.data.experimental.ignore_errors()` correctly ignores the errors and the dataset is repeated just fine.", "@chhensel While running the code with TF -Nightly  I am getting different output. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/b341f0057575b19cca8c2698cbb28231/copy-of-repeat-error.ipynb).Thanks!", "When I open the gist, it still shows the version that is not working (with tf 2.2.0), but I can confirm that using tf-nightly indeed shows the desired behavior, thanks!", "@chhensel Thank you for the update and marking this issue as closed since it is fixed in tf -nightly. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41530\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41530\">No</a>\n"]}, {"number": 41529, "title": "Account for possibility of pyc extension in TFLite check.", "body": "Closes https://github.com/tensorflow/tensorflow/issues/41352\r\n\r\nThis PR fixes the runtime check for whether TFLite is running as a separate library or as a part of the full TensorFlow package.", "comments": ["@MeghnaNatraj, it seems I'm not able to add reviewers to this PR myself so I'm flagging this manually for you to take a look. Thanks!", "I approve the changes in this PR. @rthadur Is it possible to merge this?", "There are two tests failing but when I look at them I haven't figured out how they're related to the change in this PR. The only message is `exited with error code 2`, both on `//tensorflow/compiler/xla/tests:convolution_test_1d_cpu`.", "> I approve the changes in this PR. @rthadur Is it possible to merge this?\r\n\r\nsure working on it , thank you"]}, {"number": 41528, "title": "No gradients provided for any variable. TF 2.1.0", "body": "I am trying to build an implicit quantile network. I build a custom loss function but do not get it working. I get the error 'no gradients available' but I belief I only use functions that should provide gradients, like tf.tile and stuff. I dont explicityly cast something in my loss_kv_iq() function.\r\n\r\nBelow I provide the code for my custom layer ( IQNlayer ) , the network I use (IQN),  and my custom loss function. Also a small piece of code in the __main__  that should be able to reproduce the error. \r\n\r\nTF version: 2.1.0\r\n\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport numpy as np\r\n  \r\nclass IQN(keras.Model):\r\n    def __init__(self, quantile_dims, fc_dims, n_actions, n_quantiles):\r\n        super(IQN, self).__init__()\r\n        self.n_quantiles = n_quantiles\r\n                \r\n        initializer = keras.initializers.he_uniform()\r\n    \r\n        self.iq = IQNlayer(quantile_dims, n_quantiles)\r\n        self.dense = keras.layers.Dense(fc_dims, activation='relu', kernel_initializer = initializer)\r\n        self.out = keras.layers.Dense(n_actions, activation = None)\r\n    \r\n    def call(self, state, tau):\r\n        batch_size, state_size = state.shape\r\n        \r\n        x = self.iq(state, tau)\r\n        x = self.dense(x)\r\n        x = self.out(x)\r\n        \r\n        x = tf.transpose(tf.split(x, batch_size, axis=0), perm=[0, 2, 1])\r\n        return x\r\n    \r\n      \r\nclass IQNlayer(keras.layers.Layer):\r\n    def __init__(self, quantile_dims, n_quantiles):\r\n        super(IQNlayer, self).__init__()\r\n        self.quantile_dims = quantile_dims\r\n        self.n_quantiles = n_quantiles\r\n        \r\n        self.fc1 = keras.layers.Dense(self.quantile_dims, activation = tf.nn.selu)\r\n        self.fc2 = keras.layers.Dense(self.quantile_dims, activation = tf.nn.relu)\r\n        \r\n    def call(self, state, tau):\r\n        batch_size, state_size = state.shape\r\n        \r\n        state_tile = tf.tile(state, [1, self.n_quantiles])\r\n        state_reshape = tf.reshape(state_tile, [-1, state_size])\r\n        state_net = self.fc1(state_reshape)\r\n        \r\n        tau = tf.reshape(tau, [-1, 1])\r\n        pi_mtx = tf.constant(np.expand_dims(np.pi * np.arange(0, 64), axis=0), dtype=tf.float32)\r\n        cos_tau = tf.cos(tf.matmul(tau, pi_mtx))\r\n        phi = self.fc2(cos_tau)\r\n        \r\n        net = tf.multiply(state_net, phi)\r\n        return net\r\n    \r\n\r\ndef loss_kv_iq(x, tau, action_hot, theta_target):\r\n    expand_dim_action = tf.expand_dims(action_hot, -1)\r\n    main_support = tf.reduce_sum(x * expand_dim_action, axis=1)\r\n\r\n    theta_loss_tile = tf.tile(tf.expand_dims(main_support, axis=2), [1, 1, N_QUANTILES])\r\n    logit_valid_tile = tf.tile(tf.expand_dims(theta_target, axis=1), [1, N_QUANTILES, 1])\r\n    Huber_loss = hloss(logit_valid_tile, theta_loss_tile)\r\n    \r\n    inv_tau = 1 - tau\r\n    tau = tf.tile(tf.expand_dims(tau, axis=1), [1, N_QUANTILES, 1])\r\n    inv_tau = tf.tile(tf.expand_dims(inv_tau, axis=1), [1, N_QUANTILES, 1])\r\n    error_loss = logit_valid_tile - theta_loss_tile\r\n\r\n    Loss = tf.where(tf.less(error_loss, 0.0), inv_tau * Huber_loss, tau * Huber_loss)\r\n    loss = tf.reduce_mean(tf.reduce_sum(tf.reduce_mean(Loss, axis=2), axis=1))\r\n    return loss\r\n        \r\nif __name__ == '__main__':\r\n    hloss = tf.keras.losses.Huber(reduction = tf.keras.losses.Reduction.NONE)\r\n    \r\n    N_QUANTILES = 10\r\n    BATCH_SIZE = 2\r\n    ACTION_SIZE = 5\r\n    STATE_SIZE = 16\r\n    \r\n    # FOR EXAMPLE: RANDOM BATCH\r\n    cs = np.random.rand(BATCH_SIZE,STATE_SIZE)\r\n    a = np.random.randint(0,5,size=(2))\r\n    r = np.random.randint(0,500,size=(2))\r\n    ns = np.random.rand(BATCH_SIZE,STATE_SIZE)\r\n    \r\n    tau = np.random.uniform(size=(BATCH_SIZE, N_QUANTILES))\r\n    tau = tau.astype('float32')    \r\n    iq = IQN(128,128,ACTION_SIZE,N_QUANTILES)\r\n    \r\n    action_hot = np.zeros((BATCH_SIZE,ACTION_SIZE), dtype = np.float32)\r\n    action_hot[np.arange(BATCH_SIZE), a] = 1\r\n    \r\n    Q = iq(ns, tau)\r\n    theta_target = np.random.rand(BATCH_SIZE,N_QUANTILES)\r\n    theta_target = theta_target.astype('float32')\r\n    \r\n    optimizer = tf.keras.optimizers.Adam(lr = 1e-3)\r\n    \r\n    with tf.GradientTape() as tape:\r\n        loss = loss_kv_iq(Q, tau, action_hot, theta_target)\r\n        grads = tape.gradient(loss, iq.trainable_weights)\r\n        optimizer.apply_gradients(zip(grads,iq.trainable_weights))\r\n```\r\n\r\nError:\r\n\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\rensj\\.spyder-py3\\Thesis\\test.py\", line 106, in <module>\r\n    optimizer.apply_gradients(zip(grads,iq.trainable_weights))\r\n\r\n  File \"C:\\Users\\rensj\\Anaconda3\\envs\\tfnew\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 426, in apply_gradients\r\n    grads_and_vars = _filter_grads(grads_and_vars)\r\n\r\n  File \"C:\\Users\\rensj\\Anaconda3\\envs\\tfnew\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 1039, in _filter_grads\r\n    ([v.name for _, v in grads_and_vars],))\r\n\r\nValueError: No gradients provided for any variable: ['iqn_4/iq_nlayer_4/dense_16/kernel:0', 'iqn_4/iq_nlayer_4/dense_16/bias:0', 'iqn_4/iq_nlayer_4/dense_17/kernel:0', 'iqn_4/iq_nlayer_4/dense_17/bias:0', 'iqn_4/dense_18/kernel:0', 'iqn_4/dense_18/bias:0', 'iqn_4/dense_19/kernel:0', 'iqn_4/dense_19/bias:0'].\r\n```\r\n\r\nEDIT: Someone pointed out that I used numpy operation in the following line:\r\n```\r\n pi_mtx = tf.constant(np.expand_dims(np.pi * np.arange(0, 64), axis=0), dtype=tf.float32)\r\n```\r\n\r\nI changed this line to \r\n```\r\npi_mtx = tf.constant(tf.expand_dims(tf.constant(np.pi) * tf.range(0, 64, dtype=tf.float32), axis=0), dtype=tf.float32)\r\n   ```\r\nBut keep getting the same error\r\n\r\n--\r\nCORRELATED TO:\r\nhttps://github.com/tensorflow/tensorflow/issues/1511", "comments": ["@rensjochemsen \r\nCan you please refer to [this link](https://stackoverflow.com/questions/41689451/valueerror-no-gradients-provided-for-any-variable) with same error reported and let us know if it helps.\r\n[link](https://stackoverflow.com/questions/49289930/tensorflow-no-gradients-provided-for-any-variable?rq=1) [link1](https://www.reddit.com/r/tensorflow/comments/ghcwcj/user_error_or_bug_valueerror_no_gradients/) ", "@Saduf2019  Thanks for your answer and sorry for my late reply. I looked at the topics you mention but it did not help me solve my problem. ", "@rensjochemsen I am noticing a size incompatibility error. Please check the [gist](https://colab.research.google.com/gist/jvishnuvardhan/3ceea259576d2b01750ab7a8a953e305/untitled.ipynb) and update if it is required. Specifically, shape of `Huber_loss` is breaking the code.\r\n\r\n```\r\n    print(error_loss.shape) # this is (2, 10, 10)\r\n    print(inv_tau.shape)    # this is (2, 10, 10)\r\n    print(tau.shape)        # this is (2, 10, 10)\r\n    print(Huber_loss.shape) # this is (2, 10)\r\n```\r\n\r\nPlease check whether this is the error you are noticing? Thanks", "@jvishnuvardhan Thanks for your help and answer, although I am quite sure the loss function is correct.\r\n\r\nI did solve my error/problem. I had to calculate the 'predictions' within the scope of the tf.GradientTape():\r\n\r\nOLD CODE:\r\n```\r\n## .... stufff\r\nQ = iq(ns, tau)\r\n## .... stuff\r\nwith tf.GradientTape() as tape:\r\n        loss = loss_kv_iq(Q, tau, action_hot, theta_target)\r\n        grads = tape.gradient(loss, iq.trainable_weights)\r\n        optimizer.apply_gradients(zip(grads,iq.trainable_weights))\r\n\r\n```\r\n\r\n\r\nNEW, WORKING CODE:\r\n```\r\n## .. stuff\r\nwith tf.GradientTape() as tape:\r\n        Q = iq(ns, tau)\r\n        loss = loss_kv_iq(Q, tau, action_hot, theta_target)\r\n        grads = tape.gradient(loss, iq.trainable_weights)\r\n        optimizer.apply_gradients(zip(grads,iq.trainable_weights))\r\n\r\n\r\n\r\n```\r\nso it was merely moving one line of code \r\n\r\nI'll close the topic as my error is solved. Thanks for the help. If you have questions about the solution feel free to open the topic again ofc.\r\n\r\n\r\nEDIT:: @jvishnuvardhan . I was curious and checked your code. I do not understand why it returns these dimension on your machine. I added the print statements to the loss function and as far as I can tell they have their shape in common, (2, 10, 10)"]}, {"number": 41527, "title": "Export frozen inference graph with custom eager mode in tf 2.0", "body": "hello, I am training my custom object detection API with eager mode which is totally working fine but now I want to convert this eager mode model into a frozen inference graph so i can use it in opencv can u guyz help me out with this. I tried the traditional approach for exporting frozen graph but this won't work in our case\r\nhere's the link of colab notebook you can try this by your own too\r\nhttps://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb#scrollTo=YBD6l-E4N71y", "comments": ["@pardeep-kesnani1234 \r\n\r\nI think this is more related to models repo.Please, raise an issue in models repo by filling issue template from [here](https://github.com/tensorflow/models/issues/new/choose).Also, please share related code so it helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]