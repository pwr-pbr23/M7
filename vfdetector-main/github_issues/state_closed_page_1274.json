[{"number": 14912, "title": "golang: ~15x speedup for decodeTensor()", "body": "Make decodeTensor() faster by running binary.Read() for the whole slice in last dimension.\r\n\r\nSimilar to https://github.com/tensorflow/tensorflow/pull/14427\r\n\r\nbefore:\r\n\r\n$ go test -bench=.\r\ngoos: linux\r\ngoarch: amd64\r\npkg: github.com/tensorflow/tensorflow/tensorflow/go\r\nBenchmarkNewTensor/[150528]-8                200           7459717 ns/op\r\nBenchmarkDecodeTensor/[150528]-8             100          11205557 ns/op\r\nPASS\r\nok      github.com/tensorflow/tensorflow/tensorflow/go  3.447s\r\n\r\n\r\nafter:\r\n\r\n$ go test -bench=.\r\ngoos: linux\r\ngoarch: amd64\r\npkg: github.com/tensorflow/tensorflow/tensorflow/go\r\nBenchmarkNewTensor/[150528]-8                200           7009254 ns/op\r\nBenchmarkDecodeTensor/[150528]-8            2000            747224 ns/op\r\nPASS\r\nok      github.com/tensorflow/tensorflow/tensorflow/go  3.793s\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 14911, "title": "add extra document to parameter:num_epochs", "body": "Add extra documents to DatasetDataProvider. I forget to call  `tf.local_variables_initializer` when I set num_epochs to 1. I trace source code to find out that I need to do that.  I think put some extra documentation will help others to avoid this mistake.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 14910, "title": "Replace sys.maxint with sys.maxsize in learning.py", "body": "use sys.maxsize because sys.maxint doesn't exist in Python 3", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "@macmonac do you have multiple emails on your github account. You might want to sign the CLA with the same email as the commit. Can you verify that so that this code can be reviewed", "Hello,\r\nI tried to change but i can't signed with my github mail and i can't change le commit address.\r\nRegards", "CLAs look good, thanks!\n\n<!-- ok -->", "Ok, i add the address like secondary. It's seem ok. Regards", "Jenkins, test this please."]}, {"number": 14909, "title": "Crossentropy loss function with weights by sample and by category and outcome", "body": "In my loss function I would like to weight each sample differently and in each sample, each category should be weighted differently as well depending on the outcome. Meaning if in a cross entropy the one_hot is correctly specified, a different weight needs to be applied than when the output is incorrect. So I would need two weights per category. A tensor with rank 3. One dimension for the samples, a second dimension for the amount of classes, and a third dimension that differentiates between correct and incorrect match.\r\n\r\nI have seen that with sparse_softmax_cross_entropy it is possible to pass in a weight, that serves as a coefficient for positive examples. This is a good start, but I would need to pass in a tensor instead, to treat each sample differently. weighted_cross_entropy_with_logits seems to work in a very similar way but doesn't offer that functionality.\r\n\r\nIs this a feature that could be added?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there.\r\n\r\nNote that after discussion on StackOverflow, if you believe a feature request is warranted, feel free to file a specific feature request here in github issues.\r\n\r\nThanks!"]}, {"number": 14908, "title": "TensorBoard Modifications for Word2Vec Example", "body": "TensorBoard modifications are added to Word2Vec example for visualizing the loss graphic and embeddings with proper words in TensorBoard.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "I signed CLA.\r\nThanks.", "CLAs look good, thanks!\n\n<!-- ok -->", "I'm not the right person to review this since I haven't worked much with this code. Going to unassign myself.", "Re-routing, @jart feel free to bounce.", "@jart WDYT?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Looks like @ozgyal made the last requested changes. Setting as good to go.", "Jenkins, test this please."]}, {"number": 14907, "title": "Create new_file", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "hello world"]}, {"number": 14906, "title": "Functionality Available?: Dataset Input perform slicing along time axis", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7\r\n- **TensorFlow installed from (source or binary)**: from pip\r\n- **TensorFlow version (use command below)**: 1.3 \r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0, 6.1\r\n- **GPU model and memory**: k2200\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nCurrently, as many suggested, when we want to train LSTM to predict the values of next time step, we would better slice all the samples along time axis to tuples of (lookback numbers of values, target predicted values) as (x, y), then store all these in a variable in RAM or as data file in disk. Then we build dataset to point to either form. However, this method makes LSTM stateless. Currently, our scheme to stateful LSTM is as follows:\r\nfor an input signal [1, 2, 3, 4, 5, 6, ..., 9, 10, 11]\r\nwe initialize LSTM and set state to 0.\r\nfeed ([1, 2, 3], 4) to train (`tf.nn.dynamic_rnn`), then feed ([2, 3, 4], 5) ....... ([8, 9, 10], 11), until end of this sequence.\r\nIn this batch, we have a number (Batch size, like 32) of signals like this and they will be processed in parallel in GPU by TF.\r\nIn the next batch of new signal samples, we reset LSTM with initial state being 0. And then repeat this process.\r\n\r\nIn this way, we think that given [8, 9, 10] to the model to predict next value (as 11), it is helpful for the model to choose whether to utilize the information of its current state (whether it is at beginning of a series of signal, zero initial; or it is at middle of a signal sequence).\r\n\r\nCurrently, before version 1.4, we built two generators, one (batch generator) is to generate a batch of signals. The other (time slicer) is to generate (x, y) [shape of (batch size, max time step, number of features) ] along time axis by using the data yield by the batch generator.\r\n\r\nIn version 1.4, we find Dataset, Estimator and Experiment pipeline powerful. Is there a way to implement the same idea using such pipeline?\r\nThanks!\r\n", "comments": ["Currently the `tf.data` APIs all treat the outermost dimension as the batch dimension, both for slicing into elements and for batching. Can you use `tf.tranpose()` or `np.transpose()` to reorder the dimensions so that this works for your data?", "Thank you! @mrry \r\nOur data now is in shape [batch, time, features]. If we do a transpose into [time, batch, features], this is not what we want to have. \r\n\r\nOtherwise, for a batch of two signals of time series, for example, for simplicity number of features == 1, so of shape [2, 6, 1]\r\n[ \r\n  [[1], [2], [3], [4], [5], [6]],\r\n  [[7], [8], [9], [10], [11], [12]]\r\n]\r\nWhat we want to pass as input of x is\r\n[\r\n [[1], [2], [3]],\r\n [[2], [3], [4]],\r\n [[3], [4], [5]],\r\n [[7], [8], [9]],\r\n [[8], [9], [10]],\r\n [[9], [10], [11]]\r\n]\r\nso of shape [batch * (max time - lookback), max time, num features], as [6, 6, 1]\r\nHow could we achieve this by using Dataset API? Thanks!", "Ah, I misunderstood what you were asking for. Yes, you can do this using `Dataset.flat_map()`. For example, the following program produces the result you describe (although it doesn't use a `lookback` variable, so you might need to change it still):\r\n\r\n```python\r\nmax_time = 3\r\n\r\ndef get_slices(x):\r\n  num_slices = tf.shape(x, out_type=tf.int64)[0] - max_time\r\n  return tf.data.Dataset.range(num_slices).map(lambda i: x[i:i + max_time])\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices([\r\n    [[1], [2], [3], [4], [5], [6]],\r\n    [[7], [8], [9], [10], [11], [12]]]).flat_map(get_slices).batch(6)\r\n```", "@mrry Thanks! This works in splitting them into chunks along time axis. However, if the lengths of the original signals are different, how could we know when to reset the state of LSTM to 0?\r\nFor example, if we have two batches and each batch size is 1. \r\n[\r\n[[[1], [2], [3], [4], [5], [6]]],\r\n[[[7], [8], [9], [10], [11], [12], [13]]]\r\n]\r\nThe only one sample in the first batch is of length 6 while the other one is of length 7. We want reset the state of LSTM to 0 after slicing over the first sample, to make the LSTM state-ful.\r\n"]}, {"number": 14905, "title": "Running label_wav.py from Simple Audio Recognition on Windows 7/64 is generating errors: CRITICAL:tensorflow:Audio file does not exist CRITICAL:tensorflow:Labels file does not exist CRITICAL:tensorflow:Graph file does not exist", "body": "Windows 7/64, GPU Nvidia M2000M, Python 3.5.4, tensorflow 1.5.0-dev20171120.\r\n\r\nI was following the Simple Audio Tutorial. After retraining and freezing the model I was trying to run the script label_wav.py. The script produces an error: CRITICAL:tensorflow:Audio file does not exist CRITICAL:tensorflow:Labels file does not exist CRITICAL:tensorflow:Graph file does not exist:\r\n\r\nC:\\Users\\bbb738>python tensorflow/tensorflow/examples/speech_commands/label_wav.py \\--graph=/tmp/my_\r\nfrozen_graph.pb \\--labels=/tmp/speech_commands_train/conv_labels.txt \\--wav=/tmp/speech_dataset/left\r\n/a5d485dc_nohash_0.wav\r\nCRITICAL:tensorflow:Audio file does not exist\r\nCRITICAL:tensorflow:Labels file does not exist\r\nCRITICAL:tensorflow:Graph file does not exist\r\nTraceback (most recent call last):\r\n  File \"tensorflow/tensorflow/examples/speech_commands/label_wav.py\", line 135, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\bbb738\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\p\r\nlatform\\app.py\", line 129, in run\r\n    _sys.exit(main(argv))\r\n  File \"tensorflow/tensorflow/examples/speech_commands/label_wav.py\", line 107, in main\r\n    FLAGS.output_name, FLAGS.how_many_labels)\r\n  File \"tensorflow/tensorflow/examples/speech_commands/label_wav.py\", line 93, in label_wav\r\n    labels_list = load_labels(labels)\r\n  File \"tensorflow/tensorflow/examples/speech_commands/label_wav.py\", line 58, in load_labels\r\n    return [line.rstrip() for line in tf.gfile.GFile(filename)]\r\n  File \"tensorflow/tensorflow/examples/speech_commands/label_wav.py\", line 58, in <listcomp>\r\n    return [line.rstrip() for line in tf.gfile.GFile(filename)]\r\n  File \"C:\\Users\\bbb738\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\l\r\nib\\io\\file_io.py\", line 214, in __next__\r\n    return self.next()\r\n  File \"C:\\Users\\bbb738\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\l\r\nib\\io\\file_io.py\", line 208, in next\r\n    retval = self.readline()\r\n  File \"C:\\Users\\bbb738\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\l\r\nib\\io\\file_io.py\", line 177, in readline\r\n    self._preread_check()\r\n  File \"C:\\Users\\bbb738\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\l\r\nib\\io\\file_io.py\", line 79, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512, status)\r\n  File \"C:\\Users\\bbb738\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\f\r\nramework\\errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile failed to Create/Open:  :\r\n The system cannot find the path specified.\r\n; No such process", "comments": ["Do those files exist? They don't look like Windows paths....", "Yes, they do exist according exactly as it is stated in the command bellow. So  the **my_frozen_graph.pb** in c:/tmp, **conv_labels.txt** is in c:/tmp/speech_commands_train/ and **a5d485dc_nohash_0.wav** is in c:/tmp/speech_dataset/left/.\r\n\r\n\r\nAccording to the code in label_wav.py the program probably cannot find the tf.gfile...:\r\n\r\ndef label_wav(wav, labels, graph, input_name, output_name, how_many_labels):\r\n  \"\"\"Loads the model and labels, and runs the inference to print predictions.\"\"\"\r\n  if not wav or not tf.gfile.Exists(wav):\r\n    tf.logging.fatal('Audio file does not exist %s', wav)\r\n\r\n  if not labels or not tf.gfile.Exists(labels):\r\n    tf.logging.fatal('Labels file does not exist %s', labels)\r\n\r\n  if not graph or not tf.gfile.Exists(graph):\r\n    tf.logging.fatal('Graph file does not exist %s', graph)\r\n\r\n\r\npython tensorflow/examples/speech_commands/label_wav.py \\\r\n--graph=/tmp/my_frozen_graph.pb \\\r\n--labels=/tmp/speech_commands_train/conv_labels.txt \\\r\n--wav=/tmp/speech_dataset/left/a5d485dc_nohash_0.wav", "Does it work if you explicitly include `C:/` at the beginning of the paths?", "No, adding c:/   does not.  Still the same  failure.\r\nI even copied all these three files to the same folder where I am starting the script and I called the script like this:\r\n\r\n_C:\\Users\\bbb738\\tensorflow>python tensorflow/examples/speech_commands/label_wav.py \\--graph=my_frozen_graph.pb \\--labels=conv_labels.txt \\--wav=a5d485dc_nohash_0.wav_\r\n\r\nI still see the same failure:\r\n\r\n_CRITICAL:tensorflow:Audio file does not exist\r\nCRITICAL:tensorflow:Labels file does not exist\r\nCRITICAL:tensorflow:Graph file does not exist__", "Can you try running a small script that does the following?\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.gfile.Exists(\"/tmp/my_frozen_graph.pb\"))\r\nprint(tf.gfile.Exists(\"/tmp/speech_commands_train/conv_labels.txt\"))\r\nprint(tf.gfile.Exists(\"/tmp/speech_dataset/left/a5d485dc_nohash_0.wav\"))\r\n```\r\n\r\nFrom the error message, it looks like the arguments `wav`, `labels`, and `graph` are empty strings. Notice that the filenames do not appear in those \"CRITICAL\" error messages. Perhaps something is going wrong in the command-line argument parsing...", "3x true\r\n\r\nC:\\Users\\bbb738\\tensorflow>python tensorflow/examples/speech_commands/test_rf.py\r\nTrue\r\nTrue\r\nTrue", "Not sure if it helps, but I googled this issue \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/9027\r\n\r\n", "I believe that that old issue has been fixed, and the problem seems to be in how `argparse.ArgumentParser()` is working in this code:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/7726333292b9e3d97a033617ee53099f6c4fedd5/tensorflow/examples/speech_commands/label_wav.py#L109-L130\r\n\r\nCan you try modifying `label_wav.py` to print out both `FLAGS` and `unparsed` after it calls `parse_known_args()`?", "I added a few prints after definition of label_wav like this (arround line 85):\r\n\r\n_def label_wav(wav, labels, graph, input_name, output_name, how_many_labels):\r\n  \"\"\"Loads the model and labels, and runs the inference to print predictions.\"\"\"\r\n  \r\n  **print(FLAGS.wav)\r\n  print(FLAGS.labels)\r\n  print(FLAGS.graph)**\r\n\r\n  \r\n  if not wav or not tf.gfile.Exists(wav):\r\n    tf.logging.fatal('Audio file does not exist %s', wav)_\r\n\r\nand the result is:\r\n\r\n   _**wav_data:0 labels_softmax:0 3**\r\n\r\n\r\nCRITICAL:tensorflow:Audio file does not exist\r\nCRITICAL:tensorflow:Labels file does not exist\r\nCRITICAL:tensorflow:Graph file does not exist_", "OK, it definitely looks like `parser.parse_known_args()` isn't doing the expected thing, because that looks like all the flags have their default values.\r\n\r\nWhat print statement is printing \"_wav_data:0 labels_softmax:0 3\"?", "Q: What print statement is printing \"_wav_data:0 labels_softmax:0 3\"?\r\nA: three prints - I added to the code yesterday. pls see my post above, however it is not important anymore\r\n\r\nToday I  have change the code  to print **unparsed** and I got the following result:\r\n\r\n**check unparsed=  ['\\\\--graph=/tmp/my_frozen_graph.pb', '\\\\--labels=/tmp/speech_commands_train/conv_l\r\nabels.txt', '\\\\--wav=/tmp/speech_dataset/left/a5d485dc_nohash_0.wav']**\r\n\r\nIt means that the parser is wrong in interpreting the backslash in the command line ??\r\n\r\n![image](https://user-images.githubusercontent.com/2536392/33306495-19c01c34-d413-11e7-8b63-420aa0f08035.png)\r\n\r\n\r\n\r\n\r\nWhen I deleted the backslash and left only  **--**  then the problem was solved.\r\nNow the command to run the **label_wav.py** for Windows looks like this:\r\n\r\n**C:\\Users\\brm738\\tensorflow>python tensorflow/examples/speech_commands/label_wav.py --graph=/tmp/my_frozen_graph.pb --labels=/tmp/speech_commands_train/conv_labels.txt --wav=/tmp/speech_dataset/left/a5d485dc_nohash_0.wav**\r\n\r\nand I get my testing results as expected:\r\n**left (score = 0.87401)\r\nright (score = 0.07143)\r\n_unknown_ (score = 0.03335)**\r\n\r\nThank you Derek for your help!\r\n\r\n", "Argh, now that you mention it, `^` (rather than `\\`) is the line continuation character in the Windows command prompt. \r\n\r\nThanks for persevering with the example!"]}, {"number": 14904, "title": "Fix typos", "body": "memory_intialized -> memory_initialized\r\nset_memory_intialized -> set_memory_initialized\r\n\r\ninstrucion -> instruction\r\n\r\nelment -> element\r\n\r\ntensroflow -> tensorflow\r\n\r\ninterpretted -> interpreted", "comments": ["Can one of the admins verify this patch?", "In general we avoid renaming fields in a proto so it might not be a safe change.", "+1 all the changes apart from the proto field name change look good.\r\n\r\nAFAICT, the `ExecProfile.memory_intialized` (sic) field isn't used anywhere else in the codebase, but @panyx0718 might be able to comment on whether that proto is part of any public APIs that might be broken by the change.", "The change passed api_compatibility_test. So it's not part of public API.", "OK, then I take that to be an approval then.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 14903, "title": "Non-deterministic result using  a pre-trained word embedding  in TensorFlow", "body": "I use pre-trained word embedding to initialize embedding in tensorflow, but got a non-deterministic result. However, if I fix the seed and use random initialization, the result is almost the same.What's the problem here? Thanks.\r\nThe code is here:\r\n\r\n```\r\nwith tf.variable_scope(\"embedding\"):\r\n    if init_embedding is None:\r\n        self.embedding = tf.get_variable(name='embedding', shape=[vocab_size, word_dim],\r\n                                              dtype=np.float32)\r\n    else:\r\n        self.embedding = tf.get_variable(name=\"embedding\", shape=init_embedding.shape,\r\n                                         initializer=tf.constant_initializer(init_embedding), `trainable=True)`\r\n```\r\n\r\ninit_embedding is the pre-trained word embedding\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14902, "title": "tensorflow for python 3.6 will cause Jupyter notebook not executing properly", "body": "Details refer to this issue I originally posted. -->https://github.com/jupyter/notebook/issues/3084\r\n", "comments": ["@gunan, could you please take a look?", "I do not have much experience with Jupyter myself.\r\nMaybe @caisq or @random-forests may know, or may know who to redirect this to?", "yomoko@ wrote in a separate GH issue and I copy it here:\r\n> @caisq The lastest version. I used this pip install --ignore-installed --upgrade tensorflow to download.\r\n> I downgraded my python to 3.5 already, and I am using tensor flow 3.5 now. Since 3.6 is not working properly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 14901, "title": "fix typos", "body": "intialized -> initialized", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14900, "title": "How to control the thread number on Tensorflow?", "body": "Hi all,\r\n\r\nDo we have any options to control the number of threads in TF-Slim both in training and evaluation processes?\r\n\r\nSpecifically, I use [this network](https://github.com/pudae/tensorflow-densenet) for my classification problem. I changed the evaluation part in a way that runs train and evaluation in parallel like [this code](https://github.com/mnuke/tf-slim-mnist). I can run it on my own CPU without any problem. But I can't execute them on a supercomputer. It seems that it is related to the very large number of threads which are being created by Tensorflow. If the number of threads exceeds the maximum number of threads pre-set in SLURM (= 28) then the job will fail. Since it's unable to create new threads it will end up with error \"resource temporarily unavailable\".\r\n\r\nThis error provided when the code tries to restore parameters from checkpoints. If there is no limitation on the number of threads (like on my pc) it works fine:\r\n```\r\nINFO:tensorflow:Restoring parameters from ./model.ckpt-0\r\nINFO:tensorflow:Starting evaluation at\r\nI tensorflow/core/kernels/logging_ops.cc:79] eval/Accuracy[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] eval/Recall_5[0]\r\nINFO:tensorflow:Evaluation [1/60]\r\n```\r\n\r\nHowever, when there is a limitation on the number of threads (like SLURM job submission on supercomputers) we get:\r\n```\r\nINFO:tensorflow:Restoring parameters from ./model.ckpt-0\r\nterminate called after throwing an instance of 'std::system_error'\r\n  what():  Resource temporarily unavailable\r\n```\r\n\r\nI tried to limit the number of CPU threads used by Tensorflow to 1 by creating config like:\r\n\r\n     FLAGS.num_preprocessing_threads=1\r\n      config = tf.ConfigProto()\r\n      config.intra_op_parallelism_threads = FLAGS.num_preprocessing_threads\r\n      config.inter_op_parallelism_threads = FLAGS.num_preprocessing_threads\r\n    \r\n        slim.evaluation.evaluation_loop(\r\n            master=FLAGS.master,\r\n            checkpoint_path=each_ckpt,\r\n            logdir=FLAGS.eval_dir,\r\n            num_evals=num_batches,\r\n            eval_op=list(names_to_updates.values()) + print_ops,\r\n            variables_to_restore=variables_to_restore,\r\n            session_config=config)\r\nBut unfortunately, that didn't help. In my opinion, the main problem we are having here is the fact that we are not able to control the number of threads here. Although we set it to 1 with various TF options you can actually see that this job is creating many more threads on the node:\r\n\r\n\r\nslurm_script\u2500\u252c\u2500python\u2500\u2500\u2500128*[{python}]\r\n             \u2514\u2500python\u2500\u2500\u25008*[{python}]\r\n\r\nTraining script is creating 128 threads and evaluation script is creating 8 (both numbers vary over time).  \r\n\r\nAny idea on the way to control the thread numbers will be highly appreciated because I do need to fix this issue urgently.\r\nEllie\r\n\r\nP.S. I'm using Python 2.7.13 and Tensorflow 1.3.0.", "comments": ["@mrry, do you have any ideas on this? I'd imagine python creates many threads, but it's strange that slurm actually limits the number of threads. A lot of tensorflow threads are IO threads that don't really correspond to an allocation of CPU resources. Perhaps you can talk to your SLURM sysadmin to increase this limit.\r\n", "The main additional source of threads would be from the Slim input pipeline, which uses Python queue runners. From your code fragment I can't tell how many threads would be created, but you might be able to set a smaller number of threads when creating the pipeline.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Hi @aselle \r\nUnfortunately, SLURM sysadmin can't increase the limits for me.", "Hi @mrry \r\nThanks for your answer. I mentioned the way that I tried to set a smaller number of threads, but it doesn't help.\r\nDo you have any suggestion for me to do this?\r\nAs @pudae mentioned [here](https://github.com/pudae/tensorflow-densenet/issues/7#issuecomment-351054685) \r\n\"It seems that there is no way to reduce number of threads with using slim.\r\nI tested evaluation scripts with num_preprocessing_threads=1,\r\nthen I found that slim create 6 threads for input pipeline and 1 thread for event logger.\"\r\n\r\n> \r\n\r\n> ", "You could try switching your code to use `tf.data` for the input pipeline and `tf.train.MonitoredSession` for the session. These classes create far fewer threads than the Slim-based training code, and should be able to fit in the limit of 28 threads if you also limit the `inter_op_parallelism_threads` and `intra_op_parallelism_threads`.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Closing this due to lack of activity. Hopefully it worked and you're happily training with a smaller number of threads!"]}, {"number": 14899, "title": "Update tf_core_framework.cmake", "body": "To resolve this [issue](https://github.com/tensorflow/tensorflow/issues/14896)\r\n\r\nReplace gpu_tracer.cc with device_tracer.cc", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it. \ud83d\ude04 ", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please."]}, {"number": 14898, "title": "r1.4 build failed constexpr constructor calls non-constexpr function", "body": "BLUF:\r\nBuild fails on OSX.\r\n./tensorflow/core/framework/variant.h(343): error: constexpr constructor calls non-constexpr function \"std::__1::unique_ptr<_Tp, _Dp>::unique_ptr() [with _Tp=tensorflow::Variant::ValueInterface, _Dp=std::__1::default_delete<tensorflow::Variant::ValueInterface>]\"\r\n\r\n1 error detected in the compilation of \"/var/folders/96/nq247q_92n99r8hkfvlxg_6w0000gn/T//tmpxft_00004cf5_00000000-7_beam_search_ops_gpu.cu.cpp1.ii\".\r\nERROR: /Users/joa23/projects/tensorflow/tensorflow/contrib/seq2seq/BUILD:51:1: output 'tensorflow/contrib/seq2seq/_objs/python/ops/_beam_search_ops_gpu/tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.pic.o' was not created.\r\nERROR: /Users/joa23/projects/tensorflow/tensorflow/contrib/seq2seq/BUILD:51:1: not all outputs were created or valid.\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n10.13.1 (17B48)\r\n- **TensorFlow installed from (source or binary)**:\r\nTrying to compile with sources. \r\n- **TensorFlow version (use command below)**:\r\nr1.4 branch\r\n- **Python version**: \r\nPython 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n0.7.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\nConfigured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 8.0.0 (clang-800.0.42.1)\r\nTarget: x86_64-apple-darwin17.2.0\r\nThread model: posix\r\n- **CUDA/cuDNN version**:\r\nCUDA Driver Version: 8.0.61\r\ncuDNN 6 April 2017\r\n- **GPU model and memory**:\r\nNVIDIA GTX TITAN X, 12 GB, DVI, HDMI, 3 DP (3072 CUDA cores)\r\n- **Exact command to reproduce**:\r\n bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\n### Describe the problem\r\nFailed to build with\r\n./tensorflow/core/framework/variant.h(343): error: constexpr constructor calls non-constexpr function \"std::__1::unique_ptr<_Tp, _Dp>::unique_ptr() [with _Tp=tensorflow::Variant::ValueInterface, _Dp=std::__1::default_delete<tensorflow::Variant::ValueInterface>]\"\r\n\r\n1 error detected in the compilation of \"/var/folders/96/nq247q_92n99r8hkfvlxg_6w0000gn/T//tmpxft_00004cf5_00000000-7_beam_search_ops_gpu.cu.cpp1.ii\".\r\nERROR: /Users/joa23/projects/tensorflow/tensorflow/contrib/seq2seq/BUILD:51:1: output 'tensorflow/contrib/seq2seq/_objs/python/ops/_beam_search_ops_gpu/tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.pic.o' was not created.\r\nERROR: /Users/joa23/projects/tensorflow/tensorflow/contrib/seq2seq/BUILD:51:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n\r\n\r\n", "comments": ["I can build the last commit before the last changes to variant.h which is [9380e3a832](https://github.com/tensorflow/tensorflow/commit/9380e3a8327e801bee5cf3ab34b98dcbd3c72bf7)\r\n\r\nThe changes at the following commit break the build for me so is probably a good place to start [105dd2aa22c7aaac401d2d8ed3131b5d81fdc227](https://github.com/tensorflow/tensorflow/commit/105dd2aa22c7aaac401d2d8ed3131b5d81fdc227#diff-62365b8836772b553669e96e9d5c86d0) although I'm not familiar with the Tensorflow code.\r\n\r\nLike you I'm on CUDA 8 cuDNN 6 and am using OSX with an eGPU (1080TI). \r\n", "@danbarnes333, \r\nThanks! Good to know someone else has the same setup. I was afraid it was hardware related. \r\nDoes the 9380e3a832 build than work for you? I managed to the build r1.3 branch but any Tensorflow hello world just hang. No stack trace. No Error message. ", "Yes I can build and run 1.3, no hangs. 9380e3a832 will build into a wheel but throws some errors when trying to import it which maybe isnt suprising as its not a release version. \r\n\r\nI can build r1.4 head with GPU support (see below) and use in OSX by changing tensorflow/core/framework/variant.h(343)\r\n`std::unique_ptr<ValueInterface> value_;`\r\nto\r\n`std::shared_ptr<ValueInterface> value_;`\r\nBut I cant really advise this without spending time going through the code it affects.\r\n\r\n@ebrevdo you made the 105dd2aa22c7aaac401d2d8ed3131b5d81fdc227 commit. Do you have thoughts if the above change is ok?\r\n\r\nThanks for any thoughts / advice you have!\r\n\r\n\r\n```\r\nIn [1]: import tensorflow as tf\r\nIn [2]: tf.__version__\r\nOut[2]: '1.4.1'\r\n\r\nIn [3]: from mrg_beta.tf.utils import LocalDevices\r\n\r\nIn [4]: LocalDevices()\r\n2017-11-28 10:59:31.406129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:856] OS X does not support NUMA - returning NUMA node zero\r\n2017-11-28 10:59:31.406316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:4f:00.0\r\ntotalMemory: 11.00GiB freeMemory: 2.31GiB\r\n2017-11-28 10:59:31.406346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:4f:00.0, compute capability: 6.1)\r\n\r\n\r\nIn [5]: import platform\r\nIn [6]: platform.platform()\r\nOut[6]: 'Darwin-16.6.0-x86_64-i386-64bit'\r\n```", "Can you provide the full compile time error?\n\nOn Tue, Nov 28, 2017 at 3:02 AM, Dan Barnes <notifications@github.com>\nwrote:\n\n> Yes I can build and run 1.3, no hangs. 9380e3a\n> <https://github.com/tensorflow/tensorflow/commit/9380e3a8327e801bee5cf3ab34b98dcbd3c72bf7>\n> will build into a wheel but throws some errors when trying to import it\n> which maybe isnt suprising as its not a release version.\n>\n> I can build r1.4 head with GPU support (see below) and use in OSX by\n> changing tensorflow/core/framework/variant.h(343)\n> std::unique_ptr<ValueInterface> value_;\n> to\n> std::shared_ptr<ValueInterface> value_;\n> But I cant really advise this without spending time going through the code\n> it affects.\n>\n> @ebrevdo <https://github.com/ebrevdo> you made the 105dd2a\n> <https://github.com/tensorflow/tensorflow/commit/105dd2aa22c7aaac401d2d8ed3131b5d81fdc227>\n> commit. Do you have thoughts if the above change is ok?\n>\n> Thanks for any thoughts / advice you have!\n>\n> In [1]: import tensorflow as tf\n> In [2]: tf.__version__\n> Out[2]: '1.4.1'\n>\n> In [3]: from mrg_beta.tf.utils import LocalDevices\n>\n> In [4]: LocalDevices()\n> 2017-11-28 10:59:31.406129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:856] OS X does not support NUMA - returning NUMA node zero\n> 2017-11-28 10:59:31.406316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\n> name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\n> pciBusID: 0000:4f:00.0\n> totalMemory: 11.00GiB freeMemory: 2.31GiB\n> 2017-11-28 10:59:31.406346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:4f:00.0, compute capability: 6.1)\n>\n> In [5]:\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14898#issuecomment-347488587>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-GuDfyZCm7BaT70eFt4GL6Mjcdmks5s6-g7gaJpZM4QrEsY>\n> .\n>\n", "I cant seem to get any more verbose than the following\r\n\r\nThanks\r\n\r\n```\r\n./tensorflow/core/framework/variant.h(343): error: constexpr constructor calls non-constexpr function \"std::__1::unique_ptr<_Tp, _Dp>::unique_ptr() [with _Tp=tensorflow::Variant::ValueInterface, _Dp=std::__1::default_delete<tensorflow::Variant::ValueInterface>]\"", "what version of clang are you using?  are you using the cmake build?", "I used gcc and followed this tutorial step by step:\r\nhttps://metakermit.com/2017/compiling-tensorflow-with-gpu-support-on-a-macbook-pro/", "Not using cmake build, using the normal configure script and clang (with nvcc as the CUDA compiler).\r\n \r\n@joa23 I dont believe on OSX you can actually build with gcc without some changes to the build files. Despite the question on at the end of the configure pointing to gcc thats by default a symlink back to clang. I tried separately to build with a version of gcc from homebrew, but it threw up a lot of errors because it was expecting clang.\r\n\r\n```\r\nApple LLVM version 8.0.0 (clang-800.0.38)\r\nTarget: x86_64-apple-darwin16.6.0\r\n\r\nxcode-select version 2347.", "Unfortunately, Apple support for NVIDIA CUDA is not strong, so it is difficult for us to support TensorFlow CUDA on Mac OS X. ", "Confirmed that @danbarnes333 fix of changing line 343 of variant.h to \r\nstd::shared_ptr<ValueInterface> value_; works in getting TF 1.4 with cuDNN 6 and CUDA 8 installed and working using bazel 0.8 on OS X 10.12.6 however I also needed to comment out the line linkopts = [\u201c-lgomp\u201d] in tensorflow/third_party/gpus/cuda/BUILD.tpl", "Yes. I linked against libgomp, but your way probably makes more sense. ", "Closing this issue due to staleness. Please use the latest version of TensorFlow and build again.\r\nFeel free to report any issues you encounter with latest TensorFlow. Thanks!"]}, {"number": 14897, "title": "A bug in tensorflow r1.4 when applying  MultiRNNCell", "body": "\r\n### System information\r\n- **TensorFlow installed from source :\r\n- **TensorFlow version: r1.4\r\n- **Python version: 3.5.4\r\n- **Bazel version: 0.5.4\r\n- **GCC/Compiler version 5.4.0\r\n- **CUDA/cuDNN version: 9.0 &5.0\r\n- **GPU model and memory*: GeForce GTX 1080\r\n\r\n### Describe the problem\r\nwhen applying the MultiRNNCell as below, an error occurs. The code went well in tensorflow r1.3\r\n# Source code\r\ninput_list is a list of tensor with shape[None, 8]\r\nn_hidden = 32\r\nlstm = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\r\nstacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm]*2)\r\noutputs, states = tf.nn.static_rnn(stacked_lstm, input_list, dtype=tf.float32)\r\n# error\r\nValueError: Dimensions must be equal, but are 64 and 40 for 'rnn/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [?,64], [40,128].\r\n\r\nHowever when only applying one single lstm, i works well.\r\n\r\nopinions:\r\nwhen calculating, the basiclstmcell will be called, where a class named Linear will be initialized, as an example, in my case, the variable self.weight in this class will be initialized as [40,128]([32+8,32*4]). \r\n*** code from rnn_cell_impl.py***\r\nif self._linear is None:\r\n    self._linear = _Linear([inputs, h], 4 * self._num_units, True)\r\n\r\nBut, when MultiRNNCell is the case, for example,  a 2 layers lstm. in the second layer, the weight should be [64,128]('h' in last layer (32)+'o' in  last layer(32)). Disappointingly, the weight will only be initialized once and stay with the shape [40,128] due to the sentence \"if self._linear is None:\". So that the reason why such error occurs.\r\n\r\ni try to comment out this sentence, but since share variable mechanism is related. it dosen't work, and induces other problem.\r\n\r\nValueError: Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, but specified shape (64, 128) and found shape (40, 128).\r\n\r\nAny idea how to solve this problem efficiently?\r\n\r\n\r\n\r\n", "comments": ["```\r\n def get_a_cell(lstm_size, keep_prob):\r\n            lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\r\n            drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\r\n            return drop\r\n        \r\n with tf.name_scope('lstm'):\r\n     cell = tf.nn.rnn_cell.MultiRNNCell(\r\n     [get_a_cell(self.lstm_size, self.keep_prob) for _ in range(self.num_layers)]\r\n     )\r\n```", "@arixlin  Thank you so much,  it works out", "@arixlin thank you!", "It does not work for me, though. Because I add `tf.get_variable_scope().reuse_variables()` before it. Then it breaks again with the same error...", "@HelloSeeing tf.get_variable_scope().reuse_variables() works in version 1.0.0", "@pangzhan27  Why does it work out? I am still confused and have the same problem with you.", "Anybody explain why it has to be modified in the form of @arixlin 's code?", "@GuitarmonYz  you can run tensorflow 1.1.0, the version has explanation.", "@GuitarmonYz  Actually it's not a bug of TensorFlow but a feature of Python. `[lstm]*2` may not behave as you expect. The list it creates contains two elements referring to the **same** lstm cell instance instead of two separate ones. Here is an example:\r\n```shell\r\n>>> l = [{'a': 1}] * 4\r\n>>> l\r\n[{'a': 1}, {'a': 1}, {'a': 1}, {'a': 1}]\r\n>>> l[0]['a'] = 2\r\n>>> l\r\n[{'a': 2}, {'a': 2}, {'a': 2}, {'a': 2}]\r\n>>> ll = [{'a': 1} for _ in range(4)]\r\n>>> ll\r\n[{'a': 1}, {'a': 1}, {'a': 1}, {'a': 1}]\r\n>>> ll[0]['a'] = 2\r\n>>> ll\r\n[{'a': 2}, {'a': 1}, {'a': 1}, {'a': 1}]\r\n```\r\nSo using list comprehension or for loop is the correct way to create separate RNN instances in  `MultiRNNCell`.  ", "> ```\r\n>  def get_a_cell(lstm_size, keep_prob):\r\n>             lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\r\n>             drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\r\n>             return drop\r\n>         \r\n>  with tf.name_scope('lstm'):\r\n>      cell = tf.nn.rnn_cell.MultiRNNCell(\r\n>      [get_a_cell(self.lstm_size, self.keep_prob) for _ in range(self.num_layers)]\r\n>      )\r\n> ```\r\nA genius answer!!!!\r\n", "really helpful!!!"]}, {"number": 14896, "title": "[bug report] CMakeList config error", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nWhen using cmake config to build tensorflow under windows, after enabling gpu, following error appears:\r\n\r\n```\r\nCMake Error at tf_core_framework.cmake:215 (list):\r\n  list sub-command REMOVE_ITEM requires two or more arguments.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:385 (include)\r\n```\r\nafter checking tf_core_framwork.cmake, it is requesting to remove \"${tensorflow_source_dir}/tensorflow/core/platform/default/gpu_tracer.cc\" from core resources. However, this file is missing from the latest tensorflow. Commenting this line help to finish cmake config but I don't think this is a good practice.\r\n\r\nWould the development team consider to update the cmake file?\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Met the same issue, to cheat cmake, either comment the line for deleting gpu_tracer.cc or create a fake empty file, but still I don't think it's a good practice. ", "Seems gpu_tracer.cc is renamed to device_trace.cc, changing the filename in .cmake file help to solve the problem but the git repository need to be updated", "Thanks for the fix @concerttttt. This should resolve your problem @jackyko1991 ."]}, {"number": 14895, "title": "Add `tf.unravel_index` as an equivalent of `np.unravel_index`", "body": "This fix tries to address the issue raised in #2075 where there was no implementation of  `tf.unravel_index`.\r\n\r\nThe `tf.unravel_index` could be quite useful in many places.\r\n\r\nThis fix adds the `tf.unravel_index` in CPU kernel. Note `order` in `np.unravel_index` has not been added yet.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @martinwicke for the review. The PR has been updated.", "@yongtang could you take a look at the test failures?", "Thanks @josh11b for the review. The PR has been updated.", "@martinwicke good to go?", "Damn, I'm sorry I slacked off on this one for so long. I assume the changes in API definition since then will have made this fail tests. I'll trigger them, we'll see.\r\n\r\nThe change itself looks good to me.", "Thanks @martinwicke. I updated the PR with API defs and golden changes. The CI tests should pass now.", "Thank you!"]}, {"number": 14894, "title": "'No gradients provided for any variable, check your graph for ops that do not support gradients'", "body": "I write the code like following\r\n```\r\nimport tensorflow as tf\r\ninput1=tf.Variable([1.0,2.0,3.0,4.0,5.0,6.0],name='input1')\r\ninput2=tf.Variable([2.0,3.0,4.0,6.0,8.0,9.0],name='input2')\r\nvalues_range = tf.constant([0., 10.], dtype = tf.float32)\r\nsource_hist = tf.histogram_fixed_width(tf.to_float(input1), values_range, 11)\r\ntemplate_hist = tf.histogram_fixed_width(tf.to_float(input2), values_range, 11)\r\nsource_hist=tf.cast(source_hist,tf.float32)\r\ntemplate_hist=tf.cast(template_hist,tf.float32)\r\nloss=2*tf.nn.l2_loss(source_hist-template_hist)\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    writer=tf.summary.FileWriter('./',sess.graph)\r\n    train_step=tf.train.AdamOptimizer(0.001).minimize(loss)\r\n    for i in range(0,10000,1):\r\n        sess.run(train_step)\r\n        print('input1_value',input1.eval())\r\n        print('input2_value',input2.eval())\r\n    writer.close()\r\n```\r\nTensorflow throws an error and shows \r\n''ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'input1:0' shape=(6,) dtype=float32_ref>\", \"<tf.Variable 'input2:0' shape=(6,) dtype=float32_ref>\"] and loss Tensor(\"mul:0\", shape=(), dtype=float32).''\r\n\r\n  I know the op  tf.histogram_fixed_width doesn't support backpropagation and is not differentiable. While \r\nthe op tf.floor has the same attribute as  tf.histogram_fixed_width. And the code below can run  without any error which surprises me a lot.\r\n\r\n```\r\nimport tensorflow as tf\r\ncst=tf.constant([1.2,1.4,2.8,4.6,6.8], dtype=tf.float32)\r\ninput=tf.Variable(cst)\r\nnew=tf.floor(input)\r\nloss=2*tf.nn.l2_loss(input-new)\r\ntrain_step=tf.train.AdamOptimizer(0.001).minimize(loss)\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    for i in range(0,10000,1):\r\n        sess.run(train_step)\r\n        print('input_value',input.eval())\r\n        print('new_value',new.eval())\r\n```\r\nI could not figure it out for days, please help", "comments": ["I have no idea what you are trying to accomplish.\r\n\r\nYour loss is defined in terms of histogram_fixed_width, so your loss is not differentiable in terms of the variables, so you can't optimize this.  You seem to understand that based on \"I know the tf.histogram_Fixed_width\"...\r\n\r\nPerhaps you want to try defining a gradient for histogram_fixed_width that is [None] like Floor has \r\nSee e.g. https://github.com/tensorflow/tensorflow/issues/897 \r\n\r\n\r\n", "Thanks so much for your reply. In fact my task is to implement the histogram loss mentioned in the paper [Stable and Controllable Neural Texture Synthesis and Style Transfer Using Histogram Losses](https://arxiv.org/abs/1701.08893), which involves histogram match technique in the process of computing the loss function. According to the paper we can regard the data flows through the ops that are not differentiable as constants. \r\n\r\nWhile I do not find the op Histogram_fixed_width in  'python/ops/math_grad.py' which mentioned in [#897](https://github.com/tensorflow/tensorflow/issues/897). So I do not konw where I can  add the function below. \r\n```\r\n@ops.RegisterGradient(\"Histogram_fixed_width\")\r\ndef _Histogram_fixed_widthGrad(_, grad):\r\n    return [None] \r\n```", "You can edit the source code and recompile tensorflow yourself, or else you can use the gradient override map\r\nhttps://uoguelph-mlrg.github.io/tensorflow_gradients/\r\nI would recommend asking this question in StackOverflow as others might have implemented this in tensorflow already.", "Thanks for your advice\uff01", "@uptodiff were you able to find a solution? Currently facing this same issue.", "@axelsly Did you find an answer? I'm also having the same issue."]}, {"number": 14893, "title": "[XLA] FIX XLA/tfcompile on OSX. #if Guard AVX, SSE and NEON instructions", "body": "This fixes XLA / tfcompile on OSX. \r\n\r\nOn OSX you currently run into linker errors because unsupported\r\ninstructions are registered. Add ifdefs to register only the\r\nsupported instructions.\r\n\r\nAlso include PR#14137 changes for missing __sincos/__sincosf in\r\nXLA on macOS since it was closed without a merge. \r\n\r\nTEST=Build tensorflow/compiler/aot/tests:tfcompile builds\r\nsuccessfully on OSX (10.13.2)", "comments": ["Can one of the admins verify this patch?", "@hawkinsp any guidance on this PR", "Sanjoy: could you please review this?", "I have addressed all the review comments and updated the review branch. \r\n\r\n", "@sanjoy @hawkinsp can we please merge this ? There is one more new compile breakage tracked in https://github.com/tensorflow/tensorflow/issues/15196 \r\nThx", "@tensorflow-jenkins  test this please.", "@tensorflow-jenkins test this please.", "Is this good to merge ? Thanks", "I am working on XLA/tfcompile for Windows, part of my patch is similar to this PR. Can this PR gets merged soon?", "@tensorflow-jenkins test this please"]}, {"number": 14892, "title": "R1.4", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@sidb007 was this PR intentional? I'm closing this for now. Let me know if otherwise."]}, {"number": 14891, "title": "`panic: runtime error: cgo argument has Go pointer to Go pointer` when using FIFOQueueV2 with non scalar shapes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: NA, using go bindings\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: NA, using CPU\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen using `op.FIFOQueueV2()` from the go bindings and passing it only scalar shapes in `op.FIFOQueueV2Shapes`, the OP works as expected. However when using multi dimensional shapes, it panics with `panic: runtime error: cgo argument has Go pointer to Go pointer`.\r\n\r\n### Source code / logs\r\nFor a working example with scalar shapes, replace the `dataShapes` and `data` lines with the commented versions below them.\r\n```\r\npackage main\r\n\r\nimport (\r\n\t\"fmt\"\r\n\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n\t\"github.com/tensorflow/tensorflow/tensorflow/go/op\"\r\n)\r\n\r\nfunc main() {\r\n\ts := op.NewScope()\r\n\tdataType := []tf.DataType{tf.Int32}\r\n\r\n\tdataShapes := []tf.Shape{tf.MakeShape(2)} // Panics\r\n\t//dataShapes := []tf.Shape{tf.ScalarShape()} // Works\r\n\r\n\tdata := op.Const(s, []int32{3, 4}) // Panics\r\n\t//data := op.Const(s, int32(3)) // Works\r\n\r\n\tqueue := op.FIFOQueueV2(s, dataType, op.FIFOQueueV2Shapes(dataShapes))\r\n\tenqueue := op.QueueEnqueueV2(s, queue, []tf.Output{data})\r\n\tcomponents := op.QueueDequeueV2(s, queue, dataType)\r\n\tgraph, err := s.Finalize()\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tsess, err := tf.NewSession(graph, nil)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\t_, err = sess.Run(nil, nil, []*tf.Operation{enqueue})\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tresults, err := sess.Run(nil, components, nil)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tfmt.Println(results[0].Value())\r\n}\r\n```\r\n```\r\n[isaac@d6-arch tfes]$ go run queue_shape_error.go \r\n2017-11-26 14:51:13.523481: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\npanic: runtime error: cgo argument has Go pointer to Go pointer\r\n\r\ngoroutine 1 [running]:\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.setAttr.func21(0xc42000e040, 0x1, 0x1, 0xe4a9c0, 0xc714a0, 0xc42000e040, 0xc42001614c, 0x1)\r\n\t/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:308 +0x100\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.setAttr(0xe4a9c0, 0xc42000e038, 0x4dc10a, 0x6, 0x4b6e00, 0xc42000c100, 0x0, 0x0)\r\n\t/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:309 +0x9b0\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.(*Graph).AddOperation(0xc42000e028, 0x4dcaa4, 0xb, 0x4dcaa4, 0xb, 0x0, 0x0, 0x0, 0xc42007c1e0, 0xc42008e1b8, ...)\r\n\t/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/graph.go:176 +0x4a0\r\ngithub.com/tensorflow/tensorflow/tensorflow/go/op.(*Scope).AddOperation(0xc42007c180, 0x4dcaa4, 0xb, 0x4dcaa4, 0xb, 0x0, 0x0, 0x0, 0xc42007c1e0, 0x7f05e202e000)\r\n\t/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/scope.go:83 +0xa0\r\ngithub.com/tensorflow/tensorflow/tensorflow/go/op.FIFOQueueV2(0xc42007c180, 0xc4200160e8, 0x1, 0x1, 0xc420057f40, 0x1, 0x1, 0x0, 0x7f05e20322f8)\r\n\t/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/wrappers.go:5136 +0x1ea\r\nmain.main()\r\n\t/home/isaac/go/src/github.com/is8ac/tfes/queue_shape_error.go:20 +0x238\r\nexit status 2\r\n```", "comments": ["@asimshankar might have some ideas here.", "Thanks for the report, this is indeed a bug, will send out a fix."]}, {"number": 14890, "title": "Tensorflow installation for python version 3.6", "body": "I am trying to install tensorflow framework on my Windows 10 system and the framework is showing me the similar error of \r\n![image](https://user-images.githubusercontent.com/31855851/33243068-c272b944-d2ac-11e7-97fc-327b8b18bafa.png)\r\nI have checked my system architecture which is 32 bit. I have tried it installing using the conda - 3.5 python version fix but then again same error arises. Please help me resolve this issue as I am on a fix for last few days.", "comments": ["Hi Preeti,\r\n\r\nKindly uninstall python 3.6 version and install python 3.5 version\r\n\r\nTo install TensorFlow, start a terminal. Then issue the appropriate pip3 install command in that terminal. \r\n\r\nTo install the CPU-only version of TensorFlow, enter the following command:\r\n\r\n(C:\\> pip3 install --upgrade tensorflow)\r\n", "@preetivyas , does this suggestion resolve your issue?", "So after reading all such blogs I decided to install python version 3.5 and 2.7 too. I was installing the tensorflow using conda installation as given here.\r\n![image](https://user-images.githubusercontent.com/31855851/33348787-ae8620dc-d465-11e7-83fc-f6a954a11344.png)\r\nI installed it successfully but after installation when I went inside my python terminal and tested tensorflow it shows me this.\r\n![image](https://user-images.githubusercontent.com/31855851/33348934-13a13826-d466-11e7-94d6-4a2e72dfd657.png)\r\n\r\nBut after a while I thought of following this way which gave me an exception in the end.\r\n![image](https://user-images.githubusercontent.com/31855851/33348966-32f707fa-d466-11e7-9b23-d47f84a5278d.png)\r\n\r\nBut, now it runs tensorflow!\r\n![image](https://user-images.githubusercontent.com/31855851/33349040-8d649dba-d466-11e7-8085-7f6ce92d214e.png)\r\nBravo! ", "cool", "Glad it worked. If your issue is resolved, please feel free to close it."]}, {"number": 14889, "title": "Add collection parameter into built-in runners", "body": "To use built-in queue runners in fine-grained manner within a single graph,\r\neach thread shouled be managed in different collection, not default collection.\r\n\r\nThreads in different user-defined collections can be started and stopped,\r\n\tseparately.\r\n\r\nSigned-off-by: Taeksang Kim <voidbag@gmail.com>", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks for the contribution. We are in the process of deprecating the `QueueRunner`-based input pipeline API, and encouraging users to move to `tf.data`, which supports creating multiple different pipelines in the same program (the use case for this change) as well as giving users the ability to restart pipelines, and improving overall performance.\r\n\r\nSince there is already a substitute for this change in `tf.data`, and we are no longer devoting resources to enhancing the `QueueRunner`-based pipelines, I'm going to close this PR. If you need help to convert an existing pipeline to use `tf.data`, see the [Programmer's Guide](https://www.tensorflow.org/programmers_guide/datasets) for an introduction to the new API."]}, {"number": 14888, "title": "Is it possible to extend normal operators like add/minus with convolution-like operating?", "body": "This is a feature request, and so far I haven't got any solution from tensorflow source code or websites like stackoverflow. \r\nIt may be confusing to state the problem like my title, so, I am going to give it a demo:\r\n1. The input matrix A has shape of [5,5], and the operation matrix B has shape of [3,3];\r\n2. In convolution manner, tf.nn.conv2d(A, B, padding='VALID') will compute like this:\r\n     create a sliding window C with the same size of the filter matrix B on matrix A, and apply \r\n     computation DOT(C, B) for all possible position of C on A. All obtained product of B and C \r\n     form the matrix of convolution result,\r\n3. Here, if we enables the users to replace the DOT(C, B) with other element-wise operators \r\n   like ADD(C, B) or user defined ones, it will enable tons of more creative layer designs to \r\n   explore the power of AI. \r\n   A more flexiable interface will help users to avoid building his own operators by hacking the ops lib.\r\nSorry for interruption. If this request get passed, I hope I can help implementing it.", "comments": ["This is an interesting feature request. For now, we would probably prefer not to add this functionality, since the implementation effort would be high. However, you can experiment with this idea using `tf.extract_image_patches()`. This gets the values that tf.conv (or others) would use to produce an output at that tensor index. You can use this to implement custom sliding window  operations. \r\n"]}, {"number": 14887, "title": "feature request - decode_compressed", "body": "It will be great if you could add **tf.decode_compressed** to be used in **situations that tfrecords cannot be used**.\r\n\r\nCurrently, only **tf.decode_raw()** can be used in such situations, which becomes a big issue with massive amount/size of files.\r\n\r\n\r\n\r\n", "comments": ["@mrry might have some thoughts on this.", "What the signature of such an op be?", "same signature as decode_raw, I don't currently see any reason for additional arguments.\r\nso basically\r\n\r\n```python\r\ndecode_compressed(\r\n    bytes,\r\n    out_type,\r\n    little_endian=True,\r\n    name=None\r\n)\r\n```\r\n\r\n", "Did you have a particular compression algorithm in mind?", "[zlib](https://zlib.net/) which uses [DEFLATE](http://www.gzip.org/algorithm.txt) sounds like a reasonable option.\r\n\r\nVery standard nowadays, easy to compress/decompress using python/c++ (and other programming languages as well)\r\n\r\n\r\n\r\n\r\n\r\n", "Also, few more thoughts about the API.\r\nFor a first phase, having only the mentioned decode_compressed() operation will be great and completely usable.\r\n\r\nThinking about how it may advance in the future, the following could be helpful as well:\r\n\r\n* A matching \"compress\" tensorflow operation. Possibly with a parameter describing the compression method (which may only allow compression_method=tf.compression.DEFLATE for a long time)\r\n\r\n* In case there's support for more than one compression format, I believe that decode_compressed(...) should read the file header and deduce the compression format by itself, but in case it's not feasible due to some reason, then we'll need to provide compression_method=tf.compression.DEFLATE as an additional argument.\r\n\r\n", "I am wondering if it makes sense to just add an optional `compression` flag to `decode_raw`?\r\n\r\nAt the moment compression are supported in  some of the classes and there are some PRs pending,  like #14599, #12369, #14342. There is no dedicated `compressed format` process. Rather compression are treated as an optional layer on top of  `TFRecord`, `TextLineRecord`, etc.\r\n\r\nMaybe we could just do the same so that we have\r\n```\r\ndecode_raw(\r\n    bytes,\r\n    out_type,\r\n    little_endian=True,\r\n    name=None,\r\n    compression=\"NONE\"\r\n)\r\n\r\n```\r\nwith compression = `\"NONE\"`/`\"ZLIB\"`/`\"GZIP\"`/etc supported?", "Usage wise it sounds good.\r\nFunction name wise it might be a bit misleading due to the \"raw\" in the function name.\r\n\r\n", "@YoelShoshan I created a PR #15132 for further discussion. After looking through decode_raw, I think it might make sense to have `decode_compressed` take a string Tensor input and generate a string Tensor output. In this way, it might potentially be used in many other places (not limited to decode_raw).\r\n\r\nTo apply `decode_compressed` to another op (e.g., decode_raw):\r\n```\r\nin_bytes = tf.placeholder(tf.string, shape=[None])\r\ndecompressed = tf.decode_compressed(in_bytes, compression_type=\"ZLIB\")\r\ndecode = tf.decode_raw(decompressed, out_type=tf.int16)\r\n```\r\n\r\nPlease take a look", "@martinwicke WDYT?", "I like it. We can figure out details, if any, on the PR. ", "@yongtang I agree - good to have it for other possible usage as well.\r\nAdded code inline comments/questions on PR #15132"]}, {"number": 14886, "title": "avg_pool ignores channel stride dimension, but max_pool does not", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary (conda-tensorflow-gpu)\r\n- **TensorFlow version (use command below)**: `b'unknown' 1.3.0`\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9/7\r\n- **GPU model and memory**: GeForce GTX 1050 Ti\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.get_variable('x', shape=(100, 32, 32, 64),\r\n        initializer=tf.constant_initializer(5), dtype=tf.float32)\r\nksize = (1, 2, 2, 2)\r\nstrides = (1, 2, 2, 2)\r\nmax_pool = tf.nn.max_pool(x, ksize, strides, padding='SAME')\r\navg_pool = tf.nn.avg_pool(x, ksize, strides, padding='SAME')\r\nprint(max_pool.shape)\r\nprint(avg_pool.shape)\r\n``` \r\n\r\nThe unexpected output is\r\n```\r\n(100, 16, 16, 32)\r\n(100, 16, 16, 64)\r\n```\r\n\r\nIt says [here](https://github.com/Hvass-Labs/TensorFlow-Tutorials/issues/19#issuecomment-274249942) that first and last stride dimension must be 1, but apparently it isn't implemented like this. If this is a feature, there should be consistent behaviour and documentation.\r\n\r\nLink to StackOverflow question: https://stackoverflow.com/q/47423172/2397253", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "Not compiling from source, so setting to N/A.", "@martinwicke WDYT?", "I see no documentation about this\u00a0behavior. @themightyoarfish, where did you see \"that first and last stride dimension must be 1\"? I cannot find anything like that. \r\n\r\nSo I think this is a bug. Fixing avgpool to respect the channels stride would be ideal. Or we can clearly document the expectations.", "i forgot to add the link to my issue; it says here (https://github.com/Hvass-Labs/TensorFlow-Tutorials/issues/19#issuecomment-274249942) that first and last should be 1, but it's not official documentation.", "Yeah, IMO that's pointing out a bug. Let's fix this.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Hi, I read the implementation of pool functions, and find that:\r\n+ `MaxPool`: \r\n   - supports pooing across either depth (`DepthwiseMaxPool`) or width/height (`SpatialMaxPool`).\r\n   - only supports NHWC for CPU.\r\n+ `AvgPool`:\r\n   - only supports pooling accross widht/height (`SpatialAvgPool`).\r\n   - only supports NHWC for CPU.\r\n\r\nFor more details to see the script and logs at the bottom.\r\n\r\nUnfortunately, all those checking are placed in c++ side, and the corresponding exception will be triggered at runtime. That's to say, both `ksize` and `strides` above are invalid for `max_pool` and `avg_pool`.\r\n\r\nI think we should also check those conditions on python side to warn user as soon as possible, or update document for the details.\r\n\r\n### test script\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.fill((100, 32, 32, 64), 5.0)\r\nksize = (1, 2, 2, 2)\r\nstrides = (1, 2, 2, 2)\r\nmax_pool = tf.nn.max_pool(x, ksize, strides, padding='VALID')\r\navg_pool = tf.nn.avg_pool(x, ksize, strides, padding='VALID')\r\nprint(max_pool.shape)\r\nprint(avg_pool.shape)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run([max_pool, avg_pool]))\r\n```\r\n\r\n### logs\r\n```python\r\n~/Downloads \u276f\u276f\u276f python test.py\r\n(100, 16, 16, 32)\r\n(100, 16, 16, 64)\r\n2018-01-17 13:17:48.363973: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnimplementedError: MaxPooling supports exactly one of pooling across depth or pooling across width/height.\r\n         [[Node: MaxPool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 2, 2], padding=\"VALID\", strides=[1, 2, 2, 2], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Fill)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 12, in <module>\r\n    print(sess.run([max_pool, avg_pool]))\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnimplementedError: MaxPooling supports exactly one of pooling across depth or pooling across width/height.\r\n         [[Node: MaxPool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 2, 2], padding=\"VALID\", strides=[1, 2, 2, 2], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Fill)]]\r\n\r\nCaused by op 'MaxPool', defined at:\r\n  File \"test.py\", line 6, in <module>\r\n    max_pool = tf.nn.max_pool(x, ksize, strides, padding='VALID')\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 1958, in max_pool\r\n    name=name)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2806, in _max_pool\r\n    data_format=data_format, name=name)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nUnimplementedError (see above for traceback): MaxPooling supports exactly one of pooling across depth or pooling across width/height.\r\n         [[Node: MaxPool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 2, 2], padding=\"VALID\", strides=[1, 2, 2, 2], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Fill)]]\r\n```", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "You seem to be using older version(1.x) of Tensorflow which is not supported anymore. Please try the latest [Tensorflow version](https://www.tensorflow.org/install/pip) and let us know if the problem still persists. \r\nAlso, check the `avg_pool` in the Tesorflow 2 which could address your issue. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/14886\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/14886\">No</a>\n"]}, {"number": 14885, "title": "tensorflow-terminate called after throwing an instance of 'std::system_error'   ", "body": "Hi\r\nI am training a resNet50 with tensorflow, using a shared server with these properties:\r\n\r\nubuntu 16.04\r\n3 gtx 1080 gpus\r\ntensorflow 1.3\r\npython 2.7\r\nCUDA 8.0.4\r\nCUDNN 6\r\nbut always after two epochs, and during the third epoch, I encounter this error:\r\n`terminate called after throwing an instance of 'std::system_error' what():\r\nResource temporarily unavailable\r\nAborted (core dumped)`\r\nwith adding some print in my code, I have found where is the problem:\r\nthis is convert tfrecord to dataset:\r\n`filenames = [\"balanced_t.tfrecords\"]\r\ndataset = tf.contrib.data.TFRecordDataset(filenames)\r\n    def parser(record):\r\n    keys_to_features = {\r\n        # \"label\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\r\n        \"mhot_label_raw\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\r\n        \"mel_spec_raw\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\r\n    }\r\n    parsed = tf.parse_single_example(record, keys_to_features)\r\n\r\n    mel_spec1d = tf.decode_raw(parsed['mel_spec_raw'], tf.float64)\r\n    # label = tf.cast(parsed[\"label\"], tf.string)\r\n    mhot_label = tf.decode_raw(parsed['mhot_label_raw'], tf.float64)\r\n    mel_spec = tf.reshape(mel_spec1d, [96, 64])\r\n    # aa=mel_spec\r\n    return {\"mel_data\": mel_spec}, mhot_label\r\n    dataset = dataset.map(parser)\r\n    dataset = dataset.batch(batch_size)\r\n    dataset = dataset.repeat(3)\r\n    iterator = dataset.make_one_shot_iterator()`\r\nand this is my input pipline:\r\n`while True:\r\n            try:\r\n               (features, labels) = sess.run(iterator.get_next())\r\n            except tf.errors.OutOfRangeError:\r\n               print(\"end of training dataset\")\r\n`\r\ndue to my prints output,the error is for this line:\r\n`(features, labels) = sess.run(iterator.get_next())`\r\nbut I dont see any problem,can you help me now?\r\n\r\nreproduce:replace any tfrecord with mine\r\n\r\n", "comments": ["I posted a (possible) answer on your [Stack Overflow question](https://stackoverflow.com/q/47499138/3574081). Feel free to reopen this issue if that doesn't solve the problem.", "I observed the same error originating from the same problem as @mrry explained on stack overflow. I thought it noteworthy though that this only ever occurred when I built TensorFlow from source. Using the pip installed version this problem did not occur. ", "I just run \"tf.test.is_gpu_available()\" and get the same error, can anyone help me?", "I solved this problem by increasing the max user processes.", "@theonegis Where did you increase the max user processes?", "You can use the `ulimit` command (Linux/UNIX) to check and change related configurations.\n\n> \u5728 2019\u5e743\u670825\u65e5\uff0c\u4e0a\u534810:49\uff0c||Dhoomketu|| <notifications@github.com> \u5199\u9053\uff1a\n> \n> @theonegis <https://github.com/theonegis> Where did you increase the max user processes?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/14885#issuecomment-476228729>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AZB-DXlpGFnSmgOkjAF_6VQWKXdMx93Rks5vaOIMgaJpZM4QqqNZ>.\n> \n\n", "@theonegis I ran the ulimit command. The output was unlimited. Btw I am on a shared cluster with no root access.", "If your max user processes is unlimited, then there may be something else causes this error.\n\n> \u5728 2019\u5e743\u670825\u65e5\uff0c\u4e0a\u534811:03\uff0c||Dhoomketu|| <notifications@github.com> \u5199\u9053\uff1a\n> \n> @theonegis <https://github.com/theonegis> I ran the ulimit command. The output was unlimited. Btw I am on a shared cluster with no root access.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/14885#issuecomment-476234554>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AZB-DdWzcPSbhxTabDrN1eMUm-frlpPKks5vaOUkgaJpZM4QqqNZ>.\n> \n\n", "I also have the same error. I do not have a graph growing out of memory as suggested by @mrry. \r\nI do have `ncpu` processes running tensorflow where `ncpu` is the number of cores with affinity. Perhaps worthwhile to note, I'm also using py_function a lot in each computation graph of each process. Does this produce extra threads? My limits are:\r\n``` bash\r\n$ ulimit -a\r\ncore file size          (blocks, -c) 0\r\ndata seg size           (kbytes, -d) unlimited\r\nscheduling priority             (-e) 0\r\nfile size               (blocks, -f) unlimited\r\npending signals                 (-i) 1539218\r\nmax locked memory       (kbytes, -l) unlimited\r\nmax memory size         (kbytes, -m) unlimited\r\nopen files                      (-n) 131072\r\npipe size            (512 bytes, -p) 8\r\nPOSIX message queues     (bytes, -q) 819200\r\nreal-time priority              (-r) 0\r\nstack size              (kbytes, -s) unlimited\r\ncpu time               (seconds, -t) unlimited\r\nmax user processes              (-u) 4096\r\nvirtual memory          (kbytes, -v) unlimited\r\nfile locks                      (-x) unlimited\r\n```\r\n\r\nIf `max user processes = 4096` and I'm running 24 processes with default intra and inter process config for tensorflow, then does that means I might create 24*24 threads << 4096.\r\n\r\nEdit: this doesn't seem to be deterministic, as sometimes it succeeds, thus it's very troublesome for stability.", "I have same problem, and also I work on shared cluster without root privilegies.\r\nFor me help reach limit to hard limit (see ulimit -H -a) by ulimit -u 1029439.", "@mrry Could you reopen please?", "@Joshuaalbert It sounds like you're running into a different problem, so please open a new issue with details of how to reproduce the problem. Thanks!"]}, {"number": 14884, "title": "Object detection works on Linux but not Mac", "body": "------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution **: Linux 16.04 and Mac OS 10.12.6\r\n- **TensorFlow installed from (source or binary)**: Mac \u2013 Binary, Linux \u2013 Source\r\n- **TensorFlow version (use command below)**: Mac\u00a0\u2013 ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\nLinux \u2013 ('v1.3.0-rc1-4003-g1f582aa', '1.4.0-rc0')\r\n- **Python version**: Mac \u2013 2.7.14, Linux \u2013 2.7.12\r\n- **Bazel version (if compiling from source)**: Linux \u2013 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: Linux GCC 5.4.0\r\n- **CUDA/cuDNN version**: Linux CUDA 9, cuDNN 7\r\n- **GPU model and memory**: Linux TitanXp\r\n\r\n### Describe the problem\r\n\r\nI trained a model using the tensor flow object detection api with faster_rcnn_resnet101. I then exported the model using the provided export_inference_graph.py. The model works on Linux, but does not work on Mac. Both platforms are using tensor flow 1.3.0. I've provided the crash log.\r\n\r\n### Source code / logs\r\n2017-11-25 20:39:12.847344: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; NodeDef: ClipToWindow/Where = Where[T=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ClipToWindow/Greater). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: ClipToWindow/Where = Where[T=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ClipToWindow/Greater)]]\r\nTraceback (most recent call last):\r\n  File \"/Documents/detect.py\", line 13, in <module>\r\n    model.detect(image)\r\n  File \"/Documents/object_detector.py\", line 71, in detect\r\n    feed_dict={self.image_tensor: image_np_expanded})\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; NodeDef: ClipToWindow/Where = Where[T=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ClipToWindow/Greater). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: ClipToWindow/Where = Where[T=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ClipToWindow/Greater)]]\r\n\r\nCaused by op u'ClipToWindow/Where', defined at:\r\n  File \"/Documents/detect.py\", line 7, in <module>\r\n    limbs = det.object_detector(\"/Documents/graph.pbtxt\",\"/Documents/graph.pb\", 2)\r\n  File \"/Documents/object_detector.py\", line 49, in __init__\r\n    tf.import_graph_def(od_graph_def, name='')\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): NodeDef mentions attr 'T' not in Op<name=Where; signature=input:bool -> index:int64>; NodeDef: ClipToWindow/Where = Where[T=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ClipToWindow/Greater). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: ClipToWindow/Where = Where[T=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ClipToWindow/Greater)]]\r\n", "comments": ["I suspect you are using an op that is only available on the GPU. It seems like maybe that the indices in the CPU don't support 64 bit indices. @ebrevdo, could you take a quick look?", "The problem seems to be with the new type T.  This was introduced in TF 1.4; you may want to upgrade your mac os version to using TF 1.4 as well.\r\n\r\nAs a long-term solution, we probably also want to add forwards compatibility-type default attr stripping in `import_scoped_meta_graph`.  @josh11b for comments.", "You are right, I exported the model using export_inference_graph.py on Mac OS with CPU and the frozen graph now works. Thanks for the help!", "Hi @teaglin. I'm having a somehow similar issue. Could you explain how you specify you want to export the model using export_inference_graph.py but with CPU. Thanks.", "@juandes I used the CPU only version of tensorflow for Mac. So it automatically defaults to that. I didn\u2019t specify anything in the export inference graph. \r\n\r\nSome what related to this I can\u2019t get either of these models to work on iOS. They complain about a similar issue. ", "@teaglin thanks for the quick reply.", "Hi,\r\n\r\nI trained ssd mobilenet model. When I use the trained model fo rinferecne on ain image, I get error as:\r\n\r\nValueError: cannot reshape array of size X into shape (a,b,c)\r\nX,a,b,c are different values for different test images.\r\n\r\nSO link is:\r\nhttps://stackoverflow.com/questions/47699181/tensorflow-object-detectionvalueerror-cannot-reshape-array\r\n\r\nI am on Windows 10.\r\n\r\nPlease help me here.", "Issue is solved. My image has 4 channels, while program expects image with 3 channel.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Have the same issue on Ubuntu.\r\n\r\n**I have one setup that works and the second one that doesn't both are Ubuntu 16.04.**\r\n\r\n> 2018-01-17 13:34:33.500813: E tensorflow/core/common_runtime/executor.cc:643] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: conv/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Mul, conv/conv2d_params). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n> \t [[Node: conv/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Mul, conv/conv2d_params)]]\r\n> Traceback (most recent call last):\r\n>   File \"label_image.py\", line 246, in <module>\r\n>     tf.app.run(main=main, argv=sys.argv[:1]+unparsed)\r\n>   File \"/home/master/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n>     _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n>   File \"label_image.py\", line 224, in main\r\n>     FLAGS.num_top_predictions)\r\n>   File \"label_image.py\", line 188, in run_on_images\r\n>     num_top_predictions)\r\n>   File \"label_image.py\", line 160, in predict_image\r\n>     temp = run_graph_label_only(image, labels, input_layer_name, output_layer_name, num_top_predictions)\r\n>   File \"label_image.py\", line 133, in run_graph_label_only\r\n>     predictions, = sess.run(softmax_tensor, {input_layer_name: image_data})\r\n>   File \"/home/master/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n>     run_metadata_ptr)\r\n>   File \"/home/master/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n>     feed_dict_tensor, options, run_metadata)\r\n>   File \"/home/master/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n>     options, run_metadata)\r\n>   File \"/home/master/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n>     raise type(e)(node_def, op, message)\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: conv/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Mul, conv/conv2d_params). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n> \t [[Node: conv/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Mul, conv/conv2d_params)]]\r\n> \r\n> Caused by op u'conv/Conv2D', defined at:\r\n>   File \"label_image.py\", line 246, in <module>\r\n>     tf.app.run(main=main, argv=sys.argv[:1]+unparsed)\r\n>   File \"/home/master/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n>     _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n>   File \"label_image.py\", line 221, in main\r\n>     load_graph(FLAGS.graph)\r\n>   File \"label_image.py\", line 99, in load_graph\r\n>     tf.import_graph_def(graph_def, name='')\r\n>   File \"/home/master/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\r\n>     op_def=op_def)\r\n>   File \"/home/master/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n>     op_def=op_def)\r\n>   File \"/home/master/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n>     self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n> \r\n> InvalidArgumentError (see above for traceback): NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: conv/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Mul, conv/conv2d_params). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n> \t [[Node: conv/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Mul, conv/conv2d_params)]]\r\n\r\nI have tries to update the tensorflow but getting the same error. The Retrain part working without problem. only when I'm trying with use the retrained model. I'm thinking that maybe it some how related to Conda or Cuda\r\n ", "I am facing the same error.... Using tensorflow 1.3 on ubuntu with cuda 8\r\n ", "same error!", "You can now use the \"strip_default_attrs=True\" option when exporting your model to increase the chances that it will work with an older version of TensorFlow. However this is only likely to really work when the exporting version of TensorFlow is not much newer than the importing version, so updating TensorFlow to a more recent version may also be necessary.", "Ran into the same thing -- glad to find this thread! GPU training, running on CPU. Will try fixes above.  May I suggest exposing a note about this somewhere in the TF model export docs?", "@juandes Thanks! I'm updating Tensorflow to recent version==1.6.0. Iit's working", "@josh11b But, which step for \"\"strip_default_attrs=True\"? freeze_graph? or optimize_graph? or quantize_graph?", "For anyone who'd like to take models from newer TF and transform them into models compatible with TF 1.3, here's my piece of code:\r\n\r\n```\r\ndef fix_graph_def(graph_def):\r\n        # fix nodes\r\n        for node in graph_def.node:\r\n            if node.op == 'RefSwitch':\r\n                node.op = 'Switch'\r\n                for index in range(len(node.input)):\r\n                    if 'moving_' in node.input[index]:\r\n                        node.input[index] = node.input[index] + '/read'\r\n            elif node.op == 'AssignSub':\r\n                node.op = 'Sub'\r\n                if 'use_locking' in node.attr:\r\n                    del node.attr['use_locking']\r\n            if \"dilations\" in node.attr:\r\n                del node.attr[\"dilations\"]\r\n            if \"index_type\" in node.attr:\r\n                del node.attr[\"index_type\"]\r\n```\r\n\r\nIt's probably not a complete list of all new attributes that have to be deleted, but in a few iterations you can make it strip everything you need.", "@peci1 hello, I meet sameproblem with TF 1.3. Does the code you mentioned work?", "@enningxie For my networks, it does. It depends on which ops you are using.", "@peci1  the `del node.attr[\"dilations\"]` doesn't work for me.", "@enningxie can you be more verbose?", "@peci1 I had retrained a model using TF1.7 HUB. I want to deploy it with TF 1.3. But meet errors above, so I would try your func to transform them into models compatible with TF 1.3, while it do nothing. By the       way, it couldn't del the `node.attr[\"dilations\"]`.", "Paste the error here.", "2018-06-01 10:03:31.606643: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](module_apply_default/hub_input/Sub, module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D/ReadVariableOp). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](module_apply_default/hub_input/Sub, module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D/ReadVariableOp)]]\r\nTraceback (most recent call last):\r\n  File \"/home/enningxie/anaconda3/envs/tf13/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"/home/enningxie/anaconda3/envs/tf13/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1306, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/enningxie/anaconda3/envs/tf13/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/enningxie/anaconda3/envs/tf13/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](module_apply_default/hub_input/Sub, module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D/ReadVariableOp). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](module_apply_default/hub_input/Sub, module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D/ReadVariableOp)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/enningxie/Codes_03/example_code/label_image_13.py\", line 230, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/enningxie/anaconda3/envs/tf13/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/enningxie/Codes_03/example_code/label_image_13.py\", line 206, in main\r\n    predictions, = sess.run(softmax_tensor, {FLAGS.input_layer: t})\r\n  File \"/home/enningxie/anaconda3/envs/tf13/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/home/enningxie/anaconda3/envs/tf13/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/enningxie/anaconda3/envs/tf13/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/enningxie/anaconda3/envs/tf13/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](module_apply_default/hub_input/Sub, module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D/ReadVariableOp). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](module_apply_default/hub_input/Sub, module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D/ReadVariableOp)]]\r\n\r\nCaused by op 'module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D', defined at:\r\n  File \"/home/enningxie/Codes_03/example_code/label_image_13.py\", line 230, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/enningxie/anaconda3/envs/tf13/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/enningxie/Codes_03/example_code/label_image_13.py\", line 185, in main\r\n    load_graph(FLAGS.graph)\r\n  File \"/home/enningxie/Codes_03/example_code/label_image_13.py\", line 114, in load_graph\r\n    tf.import_graph_def(graph_def, name='')\r\n  File \"/home/enningxie/anaconda3/envs/tf13/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/home/enningxie/anaconda3/envs/tf13/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/enningxie/anaconda3/envs/tf13/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](module_apply_default/hub_input/Sub, module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D/ReadVariableOp). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](module_apply_default/hub_input/Sub, module_apply_default/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D/ReadVariableOp)]]\r\n", "I forgot to mention the code I posted (probably) only works for .pb models with variables converted to constants. I've created a gist where you'll find how I build, save and execute the models: https://gist.github.com/peci1/80cf0dd79986db83b4c99d0714ddf2ff .\r\n\r\nIf the model you have is in a different format, first load it in newer TF, save it with save_pb_model.py from the gist, and then the fix_graph_def should work. You can even call fix_graph_def before saving the model, and then you wouldn't need to call it during execution.", "@peci1 Thanks!"]}, {"number": 14883, "title": "Add uint32 and uint64 support for `bitwise_and/or/xor`", "body": "In `tensorflow/core/ops/bitwise_ops.cc`, uint32 and uint64 have been enabled for bitwise operations `and/or/xor/left_shift/right_shift`.\r\n\r\nHowever, the kernels of `and/or/xor` have no support of uint32 and uint64.\r\n\r\nThis is in comparision to `left_shift/right_shift` which have the uint32/uint64 support, and, is tested in `bitwise_ops_test.py`.\r\n\r\nThis fix adds uint32 and uint64 to bitwise `and/or/xor` kernels.\r\n\r\nThis fix also adds relevant test cases in `bitwise_ops_test.py`, to bring `and/or/xor` as `left_shift/right_shift`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}]