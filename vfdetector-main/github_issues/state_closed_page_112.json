[{"number": 51684, "title": "Can't find GPUs after updating tensorflow to 2.6.0 (with pip) in conda environment ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Linux/Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.4.1, trying to upgrade to 2.6.0\r\n- Python version: 3.9.6\r\n- Installed using virtualenv? pip? conda?: initially conda, then upgraded tensorflow and tensorflow-gpu with pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: Tesla V100-SXM / 32510MiB\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to use a GPU for my tensorflow modeling. I've downloaded anaconda and created a new conda environment which includes the packages t=tensorflow verion 2.4.1, tensorflow-gpu 2.4.1). I can initially see GPU devices available when running a python script find_gpu.py (which just outputs the result of tf.config.experimental.list_physical_devices('GPU')).\r\n\r\nI need to update tensorflow to version 2.6.0 for my jupyter notebooks to run. However, after I upgrade either tensorflow or tensorflow-gpu to version 2.6.0 using pip install --upgrade, I can no longer see the GPU devices. I tried installing cudatoolkit with conda, but the issue still persists. A complicating factor is that I don't have sudo permissions on this machine.\r\n\r\nThis is my first tensorflow issue on Github, please do let me know if you need any additional information. Thank you so much for your help, I really appreciate it! \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n>> sh Anaconda3-2021.05-Linux-x86_64.sh\r\n>> eval \"$(/storage/mkhambet/anaconda3/bin/conda shell.bash hook)\"\r\n>> conda create -n tf-gpu tensorflow-gpu\r\n>> conda activate tf-gpu\r\n>> conda install cudatoolkit\r\n>> export CUDA_VISIBLE_DEVICES=0,1\r\n>> python find_gpu.py # This is successful and outputs the first log\r\n>> pip install --upgrade tensorflow\r\n>> pip install --upgrade tensorflow-gpu\r\n>> python find_gpu.py # This fails and outputs the second log\r\n\r\n**Any other info / logs**\r\n\r\nFirst log (successful run): \r\n\r\n2021-08-25 13:09:21.685015: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\n2021-08-25 13:09:22.517016: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-08-25 13:09:22.517772: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-08-25 13:09:27.746672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:1b:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\r\ncoreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\r\n2021-08-25 13:09:27.748023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \r\npciBusID: 0000:1c:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\r\ncoreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\r\n2021-08-25 13:09:27.748047: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\n2021-08-25 13:09:27.750010: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\r\n2021-08-25 13:09:27.750081: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\r\n2021-08-25 13:09:27.751989: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-08-25 13:09:27.752289: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-08-25 13:09:27.754163: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-08-25 13:09:27.755219: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\r\n2021-08-25 13:09:27.759360: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\r\n2021-08-25 13:09:27.764501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\r\n\r\n\r\nSecond log (failed run):\r\n\r\n2021-08-25 14:07:55.083234: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /storage/mkhambet/anaconda3/envs/tf-gpu/lib/libcudart.so.10.1\r\n2021-08-25 14:07:55.083263: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2021-08-25 14:08:00.970050: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /storage/mkhambet/anaconda3/envs/tf-gpu/lib/libcudart.so.10.1\r\n2021-08-25 14:08:00.970123: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /storage/mkhambet/anaconda3/envs/tf-gpu/lib/libcudart.so.10.1\r\n2021-08-25 14:08:00.970172: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /storage/mkhambet/anaconda3/envs/tf-gpu/lib/libcudart.so.10.1\r\n2021-08-25 14:08:00.972063: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /storage/mkhambet/anaconda3/envs/tf-gpu/lib/libcudart.so.10.1\r\n2021-08-25 14:08:00.972119: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /storage/mkhambet/anaconda3/envs/tf-gpu/lib/libcudart.so.10.1\r\n2021-08-25 14:08:00.972167: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /storage/mkhambet/anaconda3/envs/tf-gpu/lib/libcudart.so.10.1\r\n2021-08-25 14:08:00.972181: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n[]\r\n", "comments": ["@MIHIRKHAMBETE ,\r\nCan you please try to install the tf2.6 with tested build configurations.Please find the [link](https://www.tensorflow.org/install/source#gpu) for the configurations.Also for Installation issues within the Anaconda environment are tracked in the Anaconda repo.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Solved by running `conda install pip` before pip installing tensorflow 2.6.0 when making the virtual environment.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51684\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51684\">No</a>\n"]}, {"number": 51682, "title": "Inconsistency in Gradients Calculation of max_pooling and reduce_max", "body": "Hi, I found that the gradients calculations of `max_pooling` and `reduce_max`  are inconsistent in tensorflow, which make me really confused. The inconsistency exists when there are **multiple max elements**.\r\n\r\nIn `max_pooling`, only one of the max elements will get the gradients; while in `reduce_max`, all the max elements will divide the gradients equally. I want to know which algorithm is correct?\r\n\r\nHere is an example code of the gradients calculation of `max_pooling` using tensorflow2.6.0:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nwith tf.GradientTape() as tape:\r\n    x = tf.Variable([[[0.6],\r\n                      [0.6],\r\n                      [0.3]]])\r\n\r\n    max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=3)\r\n    y = max_pool_1d(x)\r\n\r\ng = tape.gradient(y, x)\r\n\r\n\r\nprint(\"gradients of max: \", g.numpy())\r\n```\r\nThe result is:\r\n```\r\ngradients of max:  [[[1.]\r\n                     [0.]\r\n                     [0.]]]\r\n```\r\nAnd this is the code of `reduce_max`:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nwith tf.GradientTape() as tape:\r\n    x = tf.Variable([[[0.6],\r\n                      [0.6],\r\n                      [0.3]]])\r\n\r\n    y = tf.reduce_max(x)\r\ng = tape.gradient(y, x)\r\n\r\n\r\nprint(\"gradients of max: \", g.numpy())\r\n```\r\nThe result is:\r\n```\r\ngradients of max:  [[[0.5]\r\n                     [0.5]\r\n                     [0. ]]]\r\n```\r\n\r\nBesides, I also found the gradients calculation of `max_pooling` in theano is different from either of the above methods. In theano's implementation, each of the max elements get the whole gradients, that is, the result of theano is:\r\n```\r\ngradients of max:  [[[1.]\r\n                     [1.]\r\n                     [0.]]]\r\n```\r\nIs it correct?\r\n\r\nAny replies will be appreciated.\r\nThanks.", "comments": ["@Saduf2019 Was able to replicate the issue on Colab using TF v2.5,2.6,tf-nightly,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/d44ab2c9579a40cbfb6df10eb74829ea/untitled410.ipynb) for reference.Thanks!", "@River861 \r\nmax_pooling is localized and and reduce_max is not  so they will not have same gradients.\r\nYou may open this issue at discussion forum as there is a larger community to respond there and as this is not a performance issue or bug.\r\n", "@Saduf2019,\r\nI think I get that.  Thank you!", "@River861 \r\nCan you please move this to close status as it answers your question.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51682\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51682\">No</a>\n"]}, {"number": 51680, "title": "Activity Regularizer not working with quantization aware training (QAT)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (or github SHA if from source): TF 2.3\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n  1/120 [..............................] - ETA: 0s - loss: 2.3153 - accuracy: 0.1040WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\r\n120/120 [==============================] - 1s 11ms/step - loss: 2.2161 - accuracy: 0.3372\r\nQuantizing model\r\n[<tf.Tensor 'conv2d/ActivityRegularizer_2/truediv:0' shape=() dtype=float32>, <tf.Tensor: shape=(), dtype=float32, numpy=0.00021372623>, <tf.Tensor 'conv2d_1/ActivityRegularizer_2/truediv:0' shape=() dtype=float32>, <tf.Tensor: shape=(), dtype=float32, numpy=0.004322933>]\r\nTraceback (most recent call last):\r\n  File \"<path to python file>/tempTrain.py\", line 95, in <module>\r\n    print(quantized_model.losses.numpy())\r\nAttributeError: 'list' object has no attribute 'numpy'\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow_model_optimization.python.core.quantization.keras import quantize\r\nfrom tensorflow.python import keras\r\nl = keras.layers\r\n\r\ntf.config.run_functions_eagerly(True)\r\n\r\ndef layers_list():\r\n  return [\r\n      l.Conv2D(32, 5, padding='same', activation='relu',\r\n               input_shape=image_input_shape(), activity_regularizer=tf.keras.regularizers.l2(l=0.0001), kernel_regularizer=tf.keras.regularizers.l2(l=0.0001)),\r\n      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\r\n      # TODO(pulkitb): Add BatchNorm when transformations are ready.\r\n      # l.BatchNormalization(),\r\n      l.Conv2D(64, 5, padding='same', activation='relu', activity_regularizer=tf.keras.regularizers.l2(l=0.0001), kernel_regularizer=tf.keras.regularizers.l2(l=0.0001)),\r\n      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\r\n      l.Flatten(),\r\n      l.Dense(1024, activation='relu'),\r\n      l.Dropout(0.4),\r\n      l.Dense(10, activation='softmax')\r\n  ]\r\n\r\n\r\ndef sequential_model():\r\n  return keras.Sequential(layers_list())\r\n\r\n\r\ndef functional_model():\r\n  \"\"\"Builds an MNIST functional model.\"\"\"\r\n  inp = keras.Input(image_input_shape())\r\n  x = l.Conv2D(32, 5, padding='same', activation='relu', activity_regularizer=tf.keras.regularizers.l2(l=0.0001), kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(inp)\r\n  x = l.MaxPooling2D((2, 2), (2, 2), padding='same')(x)\r\n  # TODO(pulkitb): Add BatchNorm when transformations are ready.\r\n  # x = l.BatchNormalization()(x)\r\n  x = l.Conv2D(64, 5, padding='same', activation='relu', activity_regularizer=tf.keras.regularizers.l2(l=0.0001), kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(x)\r\n  x = l.MaxPooling2D((2, 2), (2, 2), padding='same')(x)\r\n  x = l.Flatten()(x)\r\n  x = l.Dense(1024, activation='relu')(x)\r\n  x = l.Dropout(0.4)(x)\r\n  out = l.Dense(10, activation='softmax')(x)\r\n\r\n  return keras.models.Model([inp], [out])\r\n\r\n\r\ndef image_input_shape(img_rows=28, img_cols=28):\r\n  if tf.keras.backend.image_data_format() == 'channels_first':\r\n    return 1, img_rows, img_cols\r\n  else:\r\n    return img_rows, img_cols, 1\r\n\r\ndef preprocessed_data(img_rows=28,\r\n                      img_cols=28,\r\n                      num_classes=10):\r\n  \"\"\"Get data for mnist training and evaluation.\"\"\"\r\n  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\n  if tf.keras.backend.image_data_format() == 'channels_first':\r\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n  else:\r\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n\r\n  x_train = x_train.astype('float32')\r\n  x_test = x_test.astype('float32')\r\n  x_train /= 255\r\n  x_test /= 255\r\n\r\n  # convert class vectors to binary class matrices\r\n  y_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\n  y_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\n\r\n  return x_train, y_train, x_test, y_test\r\n\r\n\r\n\r\nmodel = functional_model() #sequential_model()\r\nmodel.summary()\r\nx_train, y_train, x_test, y_test = preprocessed_data()\r\n\r\nmodel.compile(\r\n    loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=500)\r\n_, model_accuracy = model.evaluate(x_test, y_test, verbose=0)\r\n\r\nprint(\"Quantizing model\")\r\n\r\nquantized_model = quantize.quantize_model(model)\r\nprint(quantized_model.losses)\r\nquantized_model.compile(\r\n    loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\nprint(quantized_model.losses.numpy())\r\n\r\nquantized_model.fit(x_train, y_train, batch_size=500)\r\n_, quantized_model_accuracy = quantized_model.evaluate(\r\n    x_test, y_test, verbose=0)\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["Hi @bayesian-mind , replicated and resolved in TF 2.3 by replacing the `print(quantized_model.losses.numpy())` with `print(np.array(quantized_model.losses)) ` ,providing [gist](https://colab.research.google.com/gist/mohantym/9af2df552104b132395030bce77c1128/github_51680.ipynb#scrollTo=sXyeFD-KIbEV) for reference.\r\n\r\nFor further queries ,Please raise an issue in [model_optimization ](https://github.com/tensorflow/model-optimization)repo.\r\n", "Hello @mohantym  that was not the problem I can print the `model.losses` as a list, the problem is since I am using 2 regularization i.e. activity and kernel you can see that the activity regularizer tensor is inaccessible i.e. it is not a eager tensor but kernel tensor values are accessible and it is eager tensor. However this issue does not happen when you don't make a QAT model. I already raised an issue with model-optimization folks. I will close this for now until they respond with something. Link to the issue - https://github.com/tensorflow/model-optimization/issues/802", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51680\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51680\">No</a>\n"]}, {"number": 51679, "title": "Mask R-CNN on TFLite (Input and Output tensors Help)", "body": "Hello everyone,\r\nthe past couple of days I spent trying to get tflite running with select tf ops on my raspberry pi. How I did that you can find here:\r\ntensorflow/tensorflow#51657\r\nNow it works, great. I am using Mask R CNN from Leekunhee (https://github.com/leekunhee/Mask_RCNN)\r\nThen I thought, there is so much logic going on behind this great opensource project, and the instance segmentation masks are not going to appear by themselves as they do when running on regular tensorflow.\r\nI hope it is nothing that cannot be fixed. I counted 3 input tensors and 7 output tensors of which I do not have the slightest clue of what they are doing except for maybe the 512x512 input tensor for the image.\r\n\r\nBtw, the conversion went fine, no issues there. I am only concerned how to use the outputs to gain useful information from the model.\r\nCurrently I am using tensorflow lite 2.5 on a 64 bit raspberry pi 4 (1GB of ram), with python 3.7.3.\r\n\r\nSource code:\r\n\r\n```\r\nimport numpy as np\r\nimport tflite_runtime.interpreter as tflite\r\n#Download tflite_runtime here https://github.com/PINTO0309/TensorflowLite-bin\r\nimport cv2\r\nimport os\r\nimport sys\r\nimport numpy as np\r\n\r\ndef loadImage():\r\n    image = '/home/pi/robotproject/13.08.2021_at_15-17-17.png'\r\n\r\n    #img = io.imread(image_paths[0])\r\n    img = cv2.imread(image)\r\n    img_arr = np.array(img)\r\n    \r\n    #Obviously convert to float 32\r\n    img_arr = np.float32(img_arr)\r\n    img_arr = cv2.resize(img_arr, (512, 512), interpolation = cv2.INTER_AREA)\r\n\r\n    #Create a batch of size 1 from image\r\n    img_arr = np.concatenate([img_arr[np.newaxis, :, :]]*1)\r\n        \r\n    return img_arr\r\n\r\nprint(\"Before loading the interpreter\")\r\ninterpreter = tflite.Interpreter(model_path=\"/home/pi/robotproject/model.tflite\")\r\nprint(\"Afterloading interpreter\")\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\n\r\ninterpreter.resize_tensor_input(input_details[0]['index'], (1, 512, 512, 3))\r\n\r\ninterpreter.allocate_tensors()\r\n\r\n# Test the model on random input data.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\nimage_np = loadImage()\r\n\r\nfor i in range(0, len(input_details)):\r\n    print(\"Mask R CNN input shape \" + str(input_details[i]['shape']))\r\n\r\nfor i in range(0, len(output_details)):\r\n    print(\"Mask R CNN output shape \" + str(output_details[i]['shape']))\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], image_np)\r\n\r\nprint(\"Before invoke\")\r\ninterpreter.invoke()\r\nprint(\"After invoke\")\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = []\r\nfor i in range(0, len(output_details)-1):\r\n    output_data.append([interpreter.get_tensor(output_details[i]['index']).copy()])\r\n```\r\n\r\nAnd here is the ouput:\r\n\r\n> Before loading the interpreter\r\n> INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n> INFO: Created TensorFlow Lite delegate for select TF ops.\r\n> INFO: TfLiteFlexDelegate delegate: 13 nodes delegated out of 480 nodes with 6 partitions.\r\n> \r\n> INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\n> \r\n> INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 18 nodes with 2 partitions.\r\n> \r\n> Afterloading interpreter\r\n> Beyond input details\r\n> Mask R CNN input shape [ 1 512 512 3]\r\n> Mask R CNN input shape [ 1 14]\r\n> Mask R CNN input shape [1 1 4]\r\n> Mask R CNN output shape [ 1 100 6]\r\n> Mask R CNN output shape [ 1 500 2]\r\n> Mask R CNN output shape [ 1 500 2 4]\r\n> Mask R CNN output shape [ 1 100 28 28 2]\r\n> Mask R CNN output shape [1 1 1]\r\n> Mask R CNN output shape [1 1 2]\r\n> Mask R CNN output shape [1 1 4]\r\n> Before invoke\r\n> 2021-08-25 17:58:07.981717: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 25088000 exceeds 10% of free system memory.\r\n> After invoke\r\n\r\nYep, no errors. A reason to feel good? Not yet, I really need to figure out how to use this model now. Please help me with ideas on how to format inputs and use outputs so that one actually has masks for each instances in the end.\r\n\r\nI do realize, this is a tensorflow repository and not one for mask r cnn. There is so little documentation on the internet on running this model with tflite, that I just hope someone who has some experience might stumble over this post and could help.", "comments": ["The TensorFlow Lite model has the same interface with the original TensorFlow model. Please check out the details of the original model and you can use the same input and output names with TF model signature via https://www.tensorflow.org/lite/guide/signatures", "@CodeMonkey3435 Could you please refer to the [comment](https://github.com/tensorflow/tensorflow/issues/51679#issuecomment-905841298) and let us know if it helps?Thanks!", "Solved it by using this method:\r\n\r\n```\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\nprint(input_details)\r\nprint(\"\\n\")\r\nprint(output_details)\r\n```\r\n\r\nThe suggested method is not available in TF 2.5, so this is the workaround. Thank you all.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51679\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51679\">No</a>\n"]}, {"number": 51678, "title": "build mlir pip", "body": null, "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51678) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 51676, "title": "TF-Nightly Broke for Multi GPU training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): Latest Git\r\n- Python version: 3.8.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.4 & 8.2.2.26\r\n- GPU model and memory: 2070 & 3080ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nGetting this error half way through training.\r\n```\r\ntensorflow.python.framework.errors_impl.CancelledError:  [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node div_no_nan_1/ReadVariableOp_3/_40}}]] [Op:__inference_test_function_29082]\r\n```\r\n\r\n\r\n", "comments": ["Hi @summa-code In order to expedite the trouble-shooting process, please provide a stand alone code  to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51676\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51676\">No</a>\n"]}, {"number": 51675, "title": "A Suspected Bug in binary_crossentropy", "body": "Hi, I found that the the calculation result of `binary_crossentropy` loss function is different from other deep learning libraries, such as theano.\r\n\r\nHere is an example code using tensorflow2.6.0:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ny_true = np.array([0., 1., 0.], dtype=np.float32)\r\ny_pred = np.array([0.9999999, 0.9999999, 0.0000001], dtype=np.float32)\r\n\r\nres = tf.keras.backend.binary_crossentropy(y_true, y_pred)\r\n\r\nprint(\"loss = \", res.numpy())\r\n```\r\nThe result is:\r\n```\r\nloss =  [15.333239 -0.       -0.      ]\r\n```\r\n\r\nAnd this is the code using theano1.0.4:\r\n```python\r\nimport numpy as np\r\nfrom theano import tensor as T\r\n\r\ny_true = np.array([0., 1., 0.], dtype=np.float32)\r\ny_pred = np.array([0.9999999, 0.9999999, 0.0000001], dtype=np.float32)\r\n\r\nres = T.nnet.binary_crossentropy(y_pred, y_true)\r\n\r\nprint(\"loss = \", res.eval())\r\n```\r\nThe result is:\r\n```\r\nloss =  [1.5942385e+01 1.1920930e-07 1.1920930e-07]\r\n```\r\n\r\nI then found the cause of the inconsistency is that, tensorflow use the `epsilon`  to caculate the loss value,  which I think is redundant because the output has been clipped using `epsilon` eariler. Here is the source code location:\r\nhttps://github.com/tensorflow/tensorflow/blob/fbd7286aba58ba180a2e3c8a280ed5379ee5435d/tensorflow/python/keras/backend.py#L5047-L5052\r\n\r\nBesides, I found that the `categorical_crossentropy` doesn't use the `epsilon` to caculate the loss value. This makes me more suspicious of the usage of `epsilon` in `binary_crossentropy`:\r\nhttps://github.com/tensorflow/tensorflow/blob/fbd7286aba58ba180a2e3c8a280ed5379ee5435d/tensorflow/python/keras/backend.py#L4906-L4908\r\n\r\nThanks.", "comments": ["Duplicate of #https://github.com/keras-team/keras/issues/15249\r\n\r\n@River861 This is more related to keras-team/keras. We will close here and monitor the progress in the repo.  Thanks!"]}, {"number": 51674, "title": "does lstm only running on CPU? Can it delegated to Apple Neural Engine(ANE) by CoreML delegate ?", "body": "Such as title. \r\nAnd I have another confuse. When I split my model into two pieces, the second piece consists of two nodes(Dense and Softmax) can not be delegate to CoreML anymore.\r\n\r\nwho can tell me something about this? Any reply will be appreciate.", "comments": ["@xunbing ,\r\n\r\nCan you please refer this link [1](https://developer.apple.com/documentation/accelerate/bnns/using_long_short-term_memory_layers_lstm) and [2](https://www.mlfairy.com/blog/2020/01/31/lstm-coreml/) for more information on LSTM on ANE.It helps.Thanks!", "@tilakrayal  thanks a lot! In these days, I'm trying my best to figure it out. The two blogs are very helpful to me. "]}, {"number": 51673, "title": "failed to alloc X bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory", "body": "\r\n\r\nI am trying to run a tensorflow project and I am encountering memory problems on the university HPC cluster. I have to run a prediction job for hundreds of inputs, with differing length. We have GPU nodes with different amount of vmem, so I am trying to set up the scripts in a way that will not crash in any combination of GPU node - input length.\r\n\r\nAfter searching the net for solutions, I played around with TF_FORCE_UNIFIED_MEMORY, XLA_PYTHON_CLIENT_MEM_FRACTION, XLA_PYTHON_CLIENT_PREALLOCATE and TF_FORCE_GPU_ALLOW_GROWTH, and also with tensorflow's set_memory_growth. As I understood, with unified memory, I should be able to use more memory than a GPU has in itself.\r\n\r\nThis was my final solution (only relevant parts)\r\n\r\n```\r\nos.environ['TF_FORCE_UNIFIED_MEMORY']='1'\r\nos.environ['XLA_PYTHON_CLIENT_MEM_FRACTION']='2.0'\r\n#os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false'\r\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH ']='true' # as I understood, this is redundant with the set_memory_growth part\r\n```\r\n\r\n```\r\nimport tensorflow as tf    \r\ngpus = tf.config.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      print(gpu)\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)\r\n\r\n```\r\nand I submit it on the cluster with` --mem=30G` (slurm job scheduler) and `--gres=gpu:1.`\r\n\r\nAnd this is the error my code crashes with. As I understand, it does try to use the unified memory but is failing for some reason.\r\n\r\n```\r\nCreated TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5582 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:02:00.0, compute capability: 3.5)\r\n2021-08-24 09:22:02.053935: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 12758286336 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2021-08-24 09:22:03.738635: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 11482457088 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2021-08-24 09:22:05.418059: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 10334211072 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2021-08-24 09:22:07.102411: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 9300789248 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2021-08-24 09:22:08.784349: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 8370710016 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2021-08-24 09:22:10.468644: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 7533638656 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2021-08-24 09:22:12.150588: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 6780274688 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2021-08-24 09:23:10.326528: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.33GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"scripts/script.py\", line 654, in <module>\r\n    prediction_result, (r, t) = cf.to(model_runner.predict(processed_feature_dict, random_seed=seed), \"cpu\")\r\n  File \"env/lib/python3.7/site-packages/alphafold/model/model.py\", line 134, in predict\r\n    result, recycles = self.apply(self.params, jax.random.PRNGKey(random_seed), feat)\r\n  File \"env/lib/python3.7/site-packages/jax/_src/traceback_util.py\", line 183, in reraise_with_filtered_traceback\r\n    return fun(*args, **kwargs)\r\n  File \"env/lib/python3.7/site-packages/jax/_src/api.py\", line 402, in cache_miss\r\n    donated_invars=donated_invars, inline=inline)\r\n  File \"env/lib/python3.7/site-packages/jax/core.py\", line 1561, in bind\r\n    return call_bind(self, fun, *args, **params)\r\n  File \"env/lib/python3.7/site-packages/jax/core.py\", line 1552, in call_bind\r\n    outs = primitive.process(top_trace, fun, tracers, params)\r\n  File \"env/lib/python3.7/site-packages/jax/core.py\", line 1564, in process\r\n    return trace.process_call(self, fun, tracers, params)\r\n  File \"env/lib/python3.7/site-packages/jax/core.py\", line 607, in process_call\r\n    return primitive.impl(f, *tracers, **params)\r\n  File \"env/lib/python3.7/site-packages/jax/interpreters/xla.py\", line 608, in _xla_call_impl\r\n    *unsafe_map(arg_spec, args))\r\n  File \"env/lib/python3.7/site-packages/jax/linear_util.py\", line 262, in memoized_fun\r\n    ans = call(fun, *args)\r\n  File \"env/lib/python3.7/site-packages/jax/interpreters/xla.py\", line 758, in _xla_callable\r\n    compiled = compile_or_get_cached(backend, built, options)\r\n  File \"env/lib/python3.7/site-packages/jax/interpreters/xla.py\", line 76, in compile_or_get_cached\r\n    return backend_compile(backend, computation, compile_options)\r\n  File \"env/lib/python3.7/site-packages/jax/interpreters/xla.py\", line 373, in backend_compile\r\n    return backend.compile(built_c, compile_options=options)\r\njax._src.traceback_util.UnfilteredStackTrace: RuntimeError: Resource exhausted: Out of memory while trying to allocate 4649385984 bytes.\r\n\r\nThe stack trace below excludes JAX-internal frames.\r\nThe preceding is the original exception that occurred, unmodified.\r\n\r\n--------------------\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"scripts/script.py\", line 654, in <module>\r\n    prediction_result, (r, t) = cf.to(model_runner.predict(processed_feature_dict, random_seed=seed), \"cpu\")\r\n  File \"env/lib/python3.7/site-packages/alphafold/model/model.py\", line 134, in predict\r\n    result, recycles = self.apply(self.params, jax.random.PRNGKey(random_seed), feat)\r\n  File \"env/lib/python3.7/site-packages/jax/interpreters/xla.py\", line 373, in backend_compile\r\n    return backend.compile(built_c, compile_options=options)\r\nRuntimeError: Resource exhausted: Out of memory while trying to allocate 4649385984 bytes.\r\n```\r\n\r\nI would be glad for any ideas how to get it work and use all the available memory.\r\n\r\nThank you!\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): debian 5781\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: Python 3.7.3\r\n- Bazel version (if compiling from source): - \r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 11\r\n- GPU model and memory: several GPUs on the cluster (Tesla M60, vmem:8G ; RTX 2080Ti, vmem:10G; Quadro RTX 6000, vmem: 24G)\r\n\r\n\r\nI would be glad for any ideas how to get it work and use all the available memory.\r\n\r\nThank you!\r\n", "comments": ["Hi @gezmi \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51673\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51673\">No</a>\n"]}, {"number": 51672, "title": "Refactor Slice reference op for port to TFLM", "body": "Solves second part of issue #51669, PR2:  Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences\r\n\r\nChange-Id: I1c8e2a4550d9c6fd792b2d5ad0361579b9d437dd", "comments": ["@advaitjain Thank you for reviewing, I'm currently working on fixing the Ubuntu Sanity build fail. Did you suggest any other changes for this patch? I am unable to find it in the changes view, therefore I wanted to double-check that I'm not missing anything. BR Annie", "> @advaitjain Thank you for reviewing, I'm currently working on fixing the Ubuntu Sanity build fail. Did you suggest any other changes for this patch? I am unable to find it in the changes view, therefore I wanted to double-check that I'm not missing anything. BR Annie\r\n\r\nApologies, looks like my previous comments got lost. Added a comment this time to IWYU.\r\n\r\nThe Ubuntu sanity build can sometimes fail for unrelated reasons. Once I approve the PR, it will be imported internally and that is a place where more exhaustive tests run.", "> > @advaitjain Thank you for reviewing, I'm currently working on fixing the Ubuntu Sanity build fail. Did you suggest any other changes for this patch? I am unable to find it in the changes view, therefore I wanted to double-check that I'm not missing anything. BR Annie\r\n> \r\n> Apologies, looks like my previous comments got lost. Added a comment this time to IWYU.\r\n> \r\n> The Ubuntu sanity build can sometimes fail for unrelated reasons. Once I approve the PR, it will be imported internally and that is a place where more exhaustive tests run.\r\n\r\nThank you for the helpful reply and for your comments! ", "apologies for the delay. [`5a6b185` (#51672)](https://github.com/tensorflow/tensorflow/pull/51672/commits/5a6b185bab38c888a9d487cee9c773d2e7b759f9) fixes a lint error about not having a newline at the end of the file. This PR should be ready to go now."]}, {"number": 51671, "title": "Error EOF", "body": "\r\n![1](https://user-images.githubusercontent.com/26819449/130785201-768b1e8d-6897-40f2-9501-149ea3eca737.JPG)\r\n\r\n![2](https://user-images.githubusercontent.com/26819449/130785213-978c460b-247b-4f41-96b8-447ec1c6e15c.JPG)\r\n\r\nHello,\r\nActually, I am trying to run a library from the Github repo.\r\nAll modules of the application are running Except this one. I haven't found any syntax error.\r\nand code was not altered.\r\nCan you tell me what's wrong?\r\nThankyou.", "comments": ["@starboyvarun In order to reproduce the issue reported here, could you please provide the complete code and the dataset , tensorflow version you are using. Thanks!", "I have already shared the complete code.\r\ntf version =2.2\r\nthis is the implementation of tutorials you told me to do of janggu.\r\n", "can you please assign this to someone else as you can't already solve this?\r\nIt will safe both of our time.\r\nThank you. ", "@sanatmpa1 Was able to reproduce this issue on colab using TF v2.2 & 2.6 please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/87b5a39e99b67b44adf41d218f79c29a/untitled408.ipynb#scrollTo=-ve9mZ-aQgTN) for reference. The original issue ticket is #51651 . Thank you!", "@starboyvarun,\r\n\r\nThe straight forward issue that I see in the code is missing parantheses `)` in the `model.fit` line.\r\n\r\nReplace the `model.fit(DNA, ReduceDim(LABELS, epochs=100)` with `model.fit(DNA, ReduceDim(LABELS, epochs=100))` and let me know if it works. Thanks!", "ok thankyou.\r\n"]}, {"number": 51670, "title": "Flatbuffer_conversion changes for Slice", "body": "Solves first part of issue #51669, PR1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver\r\nChange-Id: I6afc247cac3fd3cbf5837f8702c7cc630ab1fdbc", "comments": []}, {"number": 51669, "title": "micro: port op SLICE from lite", "body": "This issue tracks my work porting operator SLICE from lite to micro. My work will be supervised by @mansnils. Proposed labels: comp:lite comp:micro:arm comp:micro\r\n\r\nThe port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:\r\n\r\nPR 1 #51670: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver\r\nPR 2 #51672: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences\r\n\r\nThe remaining three PRs will instead be opened in tensorflow/tflite-micro, but are included here for clarity:\r\nPR 3: Copy operator from lite to micro without making any changes or including in the build \r\nPR 4: Delete extra code from the micro copy of the operator\r\nPR 5: Port micro copy of operator as necessary and add a corresponding test", "comments": ["Closing this one nii favor of https://github.com/tensorflow/tflite-micro/issues/467\r\n\r\n"]}, {"number": 51668, "title": "Stateful LSTM can't be converted to TF Lite with Integer Quantization", "body": "That is what I'm doing:\r\n\r\n```\r\nonnx_model = onnx.load(path_to_onnx)\r\nmodel = prepare(onnx_model, 'CPU')\r\nmodel.export_graph(path) \r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(path)\r\nconverter.experimental_enable_resource_variables = True\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS,\r\n  tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\ndef representative_dataset():\r\n    with open('dataset.p', 'rb') as fp:\r\n        tensors_list = pickle.load(fp)\r\n    for tensor in tensors_list:\r\n        tensor = np.asarray(tensor, dtype = np.float32)\r\n        h_tensor = np.zeros((1,1,512), dtype = np.float32)\r\n        c_tensor = np.zeros((1,1,512), dtype = np.float32)\r\n        yield [h_tensor, c_tensor, tensor]\r\n\r\nconverter.representative_dataset = representative_dataset\r\ntflite_model = converter.convert()\r\n```\r\n\r\nAnd I get:\r\n```\r\n  File \"...\\lib\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py\", line 78, in __init__\r\n    raise ValueError(\"Failed to parse the model: %s.\" % e)\r\nValueError: Failed to parse the model: Op FlexVarHandleOp missing inputs.\r\n```\r\n\r\nI'm trying to do it with the simplest stateful LSTM layer, but it has the issue.\r\n\r\nThanks,\r\nAnastasiia\r\n\r\n_Originally posted by @AnastGerus in https://github.com/tensorflow/tensorflow/issues/35194#issuecomment-903722201_", "comments": ["Two additional moments:\r\n\r\n1) Unfortunately \"select_tf_ops\" can't be removed for LSTM, because (even without any optimizations) you will get an error:\r\n```\r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\nTF Select ops: AssignVariableOp, ReadVariableOp, VarHandleOp\r\nDetails:\r\n\ttf.AssignVariableOp(tensor<!tf.resource<tensor<2048xf32>>>, tensor<2048xf32>) -> ()\r\n\ttf.AssignVariableOp(tensor<!tf.resource<tensor<2048xf32>>>, tensor<2048xf32>) -> () : {device = \"\"}\r\n\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<1024x2048xf32>) -> () : {device = \"\"}\r\n\ttf.AssignVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>, tensor<1x1xf32>) -> ()\r\n\ttf.ReadVariableOp(tensor<!tf.resource<tensor<2048xf32>>>) -> (tensor<2048xf32>) : {device = \"\"}\r\n\ttf.ReadVariableOp(tensor<!tf.resource<tensor<?x?xf32>>>) -> (tensor<?x?xf32>) : {device = \"\"}\r\n\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<2048xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_bias_lstm_7\"}\r\n\ttf.VarHandleOp() -> (tensor<!tf.resource<tensor<?x?xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_kernel_lstm_7\"}\r\n```\r\nBut the possibility to remove \"select_tf_ops\" for LSTM also would be a good solution, because \"select_tf_ops\" makes TFLite library binary size increase drastically.\r\n\r\n2) I have reproduced this issue using only the Keras LSTM layer (without ONNX->TF conversion process). \r\n`keras.layers.LSTM(64, stateful=True)`\r\nAssignVariableOp, ReadVariableOp, VarHandleOp are needed when you use stateful=True for LSTM layer (that is very important option for \"infinite\" data (e.g. audio stream)).\r\n\r\nPlease let me know if you need another information.\r\n", "Hi! @AnastGerus !We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks", "I've created an issue https://github.com/tensorflow/tensorflow/issues/51683 to complete the template, so I'll close this one.\r\nThanks, \r\nAnastasiia"]}, {"number": 51666, "title": "Syntax Error: unexpected EOF while parsing", "body": "\r\n![1](https://user-images.githubusercontent.com/26819449/130758213-9adecd0f-f20c-4bae-b581-2fd9ac565b60.JPG)\r\n![2](https://user-images.githubusercontent.com/26819449/130758218-c12b6128-2f02-4cef-9b22-e2bca37ad38f.JPG)\r\nHello,\r\nActually, I am trying to run a library from the Github repo.\r\nAll modules of the application are running Except this one. I haven't found any syntax error.\r\nand code was not altered.\r\nCan you tell me what's wrong?\r\nThankyou.\r\n\r\n", "comments": ["@starboyvarun ,\r\nThe SyntaxError: unexpected EOF while parsing means that the end of your source code was reached before all code blocks were completed. \r\nAlso, this can simply mean you are missing or have too many parenthesis. For example this has too many, and will result in unexpected EOF. Please try to close all the opened parenthesis.Thanks"]}, {"number": 51665, "title": "[tf2.6] Model saving error for a customized model and loss function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n-- Dockerfile: \r\n```\r\nFROM python:3.8\r\n\r\nWORKDIR /usr/src/app\r\n\r\nCOPY requirements.txt ./\r\nRUN pip install --no-cache-dir -r requirements.txt\r\n\r\nCOPY . .\r\n\r\nCMD [ \"python\", \"./test.py\" ]\r\n```\r\n-- requirements.txt:\r\n```\r\nnumpy\r\ntensorflow\r\n```\r\nand\r\n`docker build -t python-docker .`\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested\r\n- TensorFlow installed from (source or binary): Docker on Mac mini (2018)\r\n- TensorFlow version (use command below): TF2.6\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): Not tested, unlikely to be related to this issue\r\n- GCC/Compiler version (if compiling from source): Not tested, unlikely to be related to this issue\r\n- CUDA/cuDNN version: No CUDA on this Mac mini\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nTyping `docker run -it --rm -v \"$PWD\":/usr/src/app -w /usr/src/app python-docker python test.py` in my environment will end up getting the following error, **if the `self.compiled_loss(targets, logits)` and `model.compile(optimizer=Adam(lr_schedule), loss=mean_huber_loss)` are used.**\r\n\r\n```\r\nKeyError: \"Failed to add concrete function b'__inference_train_step_1086' to object based saved model as it captures tensor tf.Tensor(<unprintable>, shape=(), dtype=resource) which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).\"\r\n```\r\n\r\n**If instead I use `loss = self.criterion(targets, logits)` and `model.compile(optimizer=Adam(lr_schedule))` everything works perfectly.**\r\n\r\n**Describe the expected behavior**\r\nModel should be properly saved to the assigned folder regardless of the two different combinations above.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nBATCH_SIZE = 2 ** 7\r\nNUM_ACTION = 11\r\nSTATE_DIM = 1\r\n\r\ndef _huber_loss(y_true, y_pred, max_grad=1.):\r\n    a = tf.abs(y_true - y_pred)\r\n    less_than_max = 0.5 * tf.square(a)\r\n    greater_than_max = max_grad * (a - 0.5 * max_grad)\r\n    return tf.where(a <= max_grad, x=less_than_max, y=greater_than_max)\r\n\r\ndef mean_huber_loss(y_true, y_pred):\r\n    return tf.reduce_mean(_huber_loss(y_true, y_pred))\r\n\r\nclass NonDistributionalModel(keras.Model):\r\n    def __init__(self, inputs, outputs):\r\n        super(NonDistributionalModel, self).__init__(inputs=inputs, outputs=outputs)\r\n\r\n        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\r\n        self.abs_metric = keras.metrics.MeanTensor(name=\"abs\") # Returns a tensor with the same shape of the input tensors\r\n\r\n        self.criterion = mean_huber_loss\r\n\r\n    @tf.function\r\n    def train_step(self, data):\r\n        states, targets = data\r\n\r\n        with tf.GradientTape() as tape:\r\n            logits = self(states, training=True)\r\n            loss = self.compiled_loss(targets, logits)\r\n            # loss = self.criterion(targets, logits)\r\n\r\n        trainable_vars = self.trainable_variables\r\n        gradients = tape.gradient(loss, trainable_vars)\r\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n\r\n        self.loss_tracker.update_state(loss)\r\n        self.abs_metric.update_state(tf.reduce_mean(tf.math.abs(targets - logits), axis=-1))\r\n        \r\n        return {\"loss\": self.loss_tracker.result(), \"abs\": self.abs_metric.result()}     \r\n\r\n    @property\r\n    def metrics(self):\r\n        # We list our `Metric` objects here so that `reset_states()` can be\r\n        # called automatically at the start of each epoch\r\n        # or at the start of `evaluate()`.\r\n        # If you don't implement this property, you have to call\r\n        # `reset_states()` yourself at the time of your choosing.\r\n        return [self.loss_tracker, self.abs_metric]   \r\n\r\ninputs = keras.Input(shape=(STATE_DIM,))\r\noutputs = keras.layers.Dense(NUM_ACTION)(inputs)\r\nmodel = NonDistributionalModel(inputs, outputs)\r\n\r\nlr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=0.1,\r\n                                                                       first_decay_steps=1000)\r\nmodel.compile(optimizer=Adam(lr_schedule), loss=mean_huber_loss)\r\n# model.compile(optimizer=Adam(lr_schedule))\r\n\r\nx = np.random.random((BATCH_SIZE, 1))\r\ny = np.random.random((BATCH_SIZE, NUM_ACTION))\r\nmodel.fit(x, y, batch_size=BATCH_SIZE, epochs=1)\r\n\r\nmodel.save('model')\r\n\r\n```\r\n\r\n**Other info / logs** \r\n```\r\n2021-08-25 08:06:24.613536: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-08-25 08:06:24.613609: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nMemory usage: 0.2776603698730469 GB\r\nTotal processing time: 1.830595807s\r\n2021-08-25 08:06:26.613800: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2021-08-25 08:06:26.613940: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-08-25 08:06:26.613981: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (b9da2514de51): /proc/driver/nvidia/version does not exist\r\n2021-08-25 08:06:26.614187: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-08-25 08:06:26.669676: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\r\n1/1 [==============================] - 1s 567ms/step - loss: 0.1817 - abs: 0.5033\r\n2021-08-25 08:06:27.407536: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/function_serialization.py\", line 65, in serialize_concrete_function\r\n    bound_inputs.append(node_ids[capture])\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/util/object_identity.py\", line 139, in __getitem__\r\n    return self._storage[self._wrap_key(key)]\r\nKeyError: <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test1.py\", line 238, in <module>\r\n    model.save('model')\r\n  File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 2145, in save\r\n    save.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n  File \"/usr/local/lib/python3.8/site-packages/keras/saving/save.py\", line 149, in save_model\r\n    saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n  File \"/usr/local/lib/python3.8/site-packages/keras/saving/saved_model/save.py\", line 90, in save\r\n    saved_nodes, node_paths = save_lib.save_and_return_nodes(\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\", line 1228, in save_and_return_nodes\r\n    _build_meta_graph(obj, signatures, options, meta_graph_def))\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\", line 1399, in _build_meta_graph\r\n    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\", line 1362, in _build_meta_graph_impl\r\n    object_graph_proto = _serialize_object_graph(\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\", line 936, in _serialize_object_graph\r\n    serialized = function_serialization.serialize_concrete_function(\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/function_serialization.py\", line 67, in serialize_concrete_function\r\n    raise KeyError(\r\nKeyError: \"Failed to add concrete function b'__inference_train_step_1086' to object based saved model as it captures tensor tf.Tensor(<unprintable>, shape=(), dtype=resource) which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).\"\r\n```\r\n\r\nAny ideas? Thank you for your time.", "comments": ["Hi @OniReimu , was able to replicate and resolve in TF 2.6 , providing [gist ](https://colab.research.google.com/gist/mohantym/aedce04fa6d0e00f4557d9ed5507e232/github_51565.ipynb)for reference.\r\n\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) for further help\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "ok! @OniReimu ,Could you close this issue here?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51665\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51665\">No</a>\n"]}, {"number": 51664, "title": "TFLite Converter fails for models with embedding layer", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.0.0\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n#### Option B: Paste your code here or provide a link to a custom end-to-end colab\r\n\r\nThis Colab notebook creates the TF Model as well as tries to it convert to a TF Lite Model.\r\n\r\nhttps://colab.research.google.com/drive/1mmqmPvq9FMh47q1iYxUnDt2OUKcfbzkV?usp=sharing\r\n\r\nConversion fails with error \r\n```\r\nValueError: Input 0 of node StatefulPartitionedCall/model_2/embedding_2/embedding_lookup was passed float from Func/StatefulPartitionedCall/input/_2:0 incompatible with expected resource.\r\n```\r\n\r\n### 5. (optional) Any other info / logs\r\n\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, op_dict, producer_op_list)\r\n    500         results = c_api.TF_GraphImportGraphDefWithResults(\r\n--> 501             graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n    502         results = c_api_util.ScopedTFImportGraphDefResults(results)\r\n\r\nInvalidArgumentError: Input 0 of node StatefulPartitionedCall/model_2/embedding_2/embedding_lookup was passed float from Func/StatefulPartitionedCall/input/_2:0 incompatible with expected resource.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n12 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, op_dict, producer_op_list)\r\n    503       except errors.InvalidArgumentError as e:\r\n    504         # Convert to ValueError for backwards compatibility.\r\n--> 505         raise ValueError(str(e))\r\n    506 \r\n    507     # Create _DefinedFunctions for any imported functions.\r\n\r\nValueError: Input 0 of node StatefulPartitionedCall/model_2/embedding_2/embedding_lookup was passed float from Func/StatefulPartitionedCall/input/_2:0 incompatible with expected resource.\r\n```\r\n", "comments": ["Please use the recent TensorFlow version and enable the following flag:\r\n\r\n```\r\nconverter.experimental_enable_resource_variables = True\r\n```", "Worth looking here as well #https://github.com/tensorflow/tensorflow/issues/43833", "@dhakrasp Could you please try as per the comment [above](https://github.com/tensorflow/tensorflow/issues/51664#issuecomment-905151738) using latest stable version of TF 2.6 and let us know if the issue still persists ?", "@sushreebarsa I tried with TF v2.6.0 and it worked. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51664\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51664\">No</a>\n"]}, {"number": 51663, "title": "Fix `DeprecationWarning: the imp module is deprecated in favour of importlib`", "body": "This PR tries to fix the following warning:\r\n```\r\n  /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py:22: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n    import imp\r\n```\r\n\r\nNote: After the fix, the flatbuffer still generates a warning:\r\n```\r\n../usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19\r\n  /usr/local/lib/python3.8/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n    import imp\r\n```\r\n\r\nHowever, this PR at least removes the warnings within tensorflow.\r\n\r\nThis PR fixes #31412.\r\n\r\nThis PR fixes #51631.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 51662, "title": "[TF:TRT]  Enable dynamic shape for Slice and StridedSlice operations", "body": "Enables dynamic shape support for conversion of \"slice\" and \"strided slice\" Tensorflow operations to the TensorRT ISliceLayer in TF:TRT.", "comments": ["Replaced with PR #51662 "]}, {"number": 51661, "title": "[Intel oneDNN] update curl version to 7.78.0", "body": "Update Curl version to 7.78.0", "comments": []}, {"number": 51660, "title": "Fix typo `tpu_system_medata`", "body": "Fix typo in docs: tpu_system_medata --> tpu_system_metadata", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51660) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 51658, "title": "Fix crash of tf.image.extract_glimpse with negative input", "body": "This PR tries to fix the issue raised in #51618 where\r\ntf.image.extract_glimpse will crash in case of negative input.\r\n\r\nThis PR adds additional checking to allow graceful error message.\r\n\r\nThis PR fixes #51618.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Here are the internal errors, @yongtang can you please verify ?  Thanks!\r\n\r\nTraceback (most recent call last):\r\n  File \"/tensorflow/python/framework/ops.py\", line 1926, in _create_c_op\r\n    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\r\ngoogle3.third_party.tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 2 but is rank 1 for '{{node ExtractGlimpseV2}} = ExtractGlimpseV2[centered=false, noise=\"uniform\", normalized=false, uniform_noise=false](ExtractGlimpseV2/input, ExtractGlimpseV2/size, ExtractGlimpseV2/offsets)' with input shapes: [1,3,3,1], [2], [2].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/tensorflow/python/kernel_tests/attention_ops_test.py\", line 314, in testGlimpseNegativeInput\r\n    normalized=False)\r\n  File \"/tensorflow/python/util/traceback_utils.py\", line 141, in error_handler\r\n    return fn(*args, **kwargs)\r\n  File \"/tensorflow/python/util/dispatch.py\", line 1048, in op_dispatch_handler\r\n    return dispatch_target(*args, **kwargs)\r\n  File \"/tensorflow/python/ops/image_ops_impl.py\", line 4945, in extract_glimpse_v2\r\n    name=name)\r\n  File \"/tensorflow/python/ops/gen_image_ops.py\", line 2015, in extract_glimpse_v2\r\n    name=name)\r\n  File \"/tensorflow/python/framework/op_def_library.py\", line 751, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n  File \"/tensorflow/python/framework/ops.py\", line 3695, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/tensorflow/python/framework/ops.py\", line 2088, in __init__\r\n    control_input_ops, op_def)\r\n  File \"/tensorflow/python/util/traceback_utils.py\", line 141, in error_handler\r\n    return fn(*args, **kwargs)\r\n  File \"/tensorflow/python/framework/ops.py\", line 1929, in _create_c_op\r\n    raise ValueError(e.message)\r\nValueError: Shape must be rank 2 but is rank 1 for '{{node ExtractGlimpseV2}} = ExtractGlimpseV2[centered=false, noise=\"uniform\", normalized=false, uniform_noise=false](ExtractGlimpseV2/input, ExtractGlimpseV2/size, ExtractGlimpseV2/offsets)' with input shapes: [1,3,3,1], [2], [2].\r\n\r\n", "Thanks @mihaimaruseac @gbaned, I have updated the PR with additional exceptions captured. Please give it a try."]}, {"number": 51657, "title": "Compile TFLite wheel file for Raspberry Pi 4. (Tensorflow operators problem)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Raspbian(Buster) (64 bit version)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4 (1GB RAM, but 2GB Swap memory I think)\r\n- TensorFlow installed from (source or binary): trying to install\r\n- TensorFlow version: 2.5.0, also TFLite 2..5.0\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: virtualenv, pip3\r\n- Bazel version (if compiling from source): Build label: 4.2.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- CUDA/cuDNN version: 11.2/8.1 on ubuntu 20.04\r\n- GPU model and memory: GTX 1070 8GB\r\n\r\nSo I started with a well working model on my computer and wanted to transfer it to the Raspberry Pi. My plan is to use TFLite on the RPI with verson 2.5.0(tflite), as the regular Tensorfow is also running on 2.5.0. My first instinct was to install the wheel file that was downloadable from the tensorflow website. (Here https://github.com/google-coral/pycoral/releases/)\r\nI used this:  tflite_runtime-2.5.0.post1-cp37-cp37m-linux_aarch64.whl\r\nIt installed fine with pip3 in my venv.\r\n\r\nSadly, I get the following error:\r\n\r\n>     interpreter.invoke()\r\n>   File \"/home/pi/.local/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 833, in invoke\r\n>     self._interpreter.Invoke()\r\n> RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 275 (FlexCropAndResize) failed to prepare.\r\n\r\nI understand, that I am using tensorflow operators that are not included in the tflite library. This is unfortunate but I believe there is one other option: To build the wheel file myself.\r\n\r\nI found a readme file deep down in tensorflow, located here: tensorflow/tensorflow/lite/tools/pip_package/README.md\r\n\r\nI used this part:\r\n\r\n> ### Cross build with Flex for armhf Python 3.7\r\n> \r\n> ```sh\r\n> CI_DOCKER_EXTRA_PARAMS=\"-e CUSTOM_BAZEL_FLAGS=--define=tflite_pip_with_flex=true\" \\\r\n>   tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\\r\n>   tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh armhf\r\n> ```\r\n\r\nAll I changed was, \"armhf\" to \"aarch64\" because of 64 bit OS and I added a flag \"--jobs=2\" to keep my computer from freezing up during the process.\r\n\r\nThe process ended with exit 0, so I thought everything should be fine, but then I get this error message when running the same script as with the first error message:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"tfliteTest.py\", line 2, in <module>\r\n>     import tflite_runtime.interpreter as tflite\r\n>   File \"/home/pi/.local/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 42, in <module>\r\n>     from tflite_runtime import metrics_portable as metrics\r\n> ImportError: cannot import name 'metrics_portable' from 'tflite_runtime' (/home/pi/.local/lib/python3.7/site-packages/tflite_runtime/__init__.py)\r\n\r\nPlease help me, and thank you already for reading this. Also reply if you have another way around this problem, even if I have to take another route.\r\n\r\n\r\n", "comments": ["@terryheo could you take a look?", "Okay, so I have made some progress this morning, but it still does not look very good. I focused on the last error message of my previous post, with the file metrics_portable. I searched through the tensorflow lite directory from github and simply found the missing files (metrics_interface.py and metrics_portable.py) and moved them to the installed tflite_runtime. This stopped the import error.\r\nNevertheless I am back to the select ops problem. Take a look at this again:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"tfliteTest.py\", line 55, in <module>\r\n>     interpreter.invoke()\r\n>   File \"/home/pi/.local/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 833, in invoke\r\n>     self._interpreter.Invoke()\r\n> RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 275 (FlexCropAndResize) failed to prepare.\r\n\r\nSomething to note: I looked at the interpreter.py file and thought this might be important.\r\n\r\n```\r\n    \"\"\"Loads delegate from the shared library.\r\n\r\n    Args:\r\n      library: Shared library name.\r\n      options: Dictionary of options that are required to load the delegate. All\r\n        keys and values in the dictionary should be serializable. Consult the\r\n        documentation of the specific delegate for required and legal options.\r\n        (default None)\r\n\r\n    Raises:\r\n      RuntimeError: This is raised if the Python implementation is not CPython.\r\n    \"\"\"\r\n```\r\n\r\nI am uncertain what to do now. Hope this shows the problem well.\r\n\r\nEdit: I know this is the right thing: https://www.tensorflow.org/lite/guide/ops_select\r\nSadly, there is only documentation for Android and IOS but no raspberry pi.", "Hi! @CodeMonkey3435 ,\r\nCould you please try on latest stable version of tf 2.6 or nightly and let us know if this is still an issue. you can also look at the [comment](https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-831019737) of similar issue Thanks!", "Yes, so I downloaded the version 2.6.0 of tensorflow and rerun this command:\r\n\r\n> sudo CI_DOCKER_EXTRA_PARAMS=\"-e CUSTOM_BAZEL_FLAGS=--define=tflite_pip_with_flex=true--local_ram_resources=2048\"   tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh aarch64\r\n\r\nWhen running this with the same old script I still get the same error message:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"tfliteTest.py\", line 59, in <module>\r\n>     interpreter.invoke()\r\n>   File \"/home/pi/.local/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 875, in invoke\r\n>     self._interpreter.Invoke()\r\n> RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 275 (FlexCropAndResize) failed to prepare.\r\n\r\nAlso the issue you linked @mohantym does not seem to be a solution because only some of the people chat about the issue being removed in 2020, and the topic has been closed on 5.june.2020. Its about a year later now and there still have been a couple of issues left, like for me. There still seems to be a lot of confusion. \r\nI do not know how to get a nightly version.", "`pip install tf-nightly `, will upgrade to nightly version .", "Okay, I do not think I could download tf-nightly like this because I was executing the commands to build the wheel file from the tensorflow folder. \r\nAnyway, I found a way for this to work.\r\n\r\nSpecial thanks to **@PINTO0309** !!!\r\n\r\nHe managed to find a way to compile wheel files for multiple versions of the raspberry pi and its ditributions. He also provides pre-built wheel files, that actually work. Additionally, he has a little guide for everyone who wants to compile the files themselves.\r\n\r\nThe **wheel files** are **select ops** enabled for **RASPBERRY PI 3/4**(Many variations)\r\n\r\nLink to his repo: **https://github.com/PINTO0309/TensorflowLite-bin**\r\n\r\nAlso thank you @mohantym and @abattery for replying so quickly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51657\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51657\">No</a>\n"]}, {"number": 51656, "title": "Fix TF serving build failure on s390x caused by aws lib target", "body": "This PR makes the `aws` lib target buildable on s390x and thus resolve a TF serving build failure. Please note that this PR does not aim at enabling any aws features on s390x, as for s390x build `-no-aws` flag is used as default. The only purpose of this PR is make TF serving build pass as TF serving does not support the `-no-aws` flag so this target needs to be analyzed.", "comments": ["We will be removing this dependency in the near future (#51032)"]}, {"number": 51655, "title": "Disable XNNPACK for s390x machines", "body": "This PR disable XNNPACK related build targets and function definitions on s390x because XNNPACK is not supported on s390x: https://github.com/google/XNNPACK#supported-architectures\r\nWithout this patch, a lot of Lite targets that are not necessarily depending on XNNPACK dependencies will fail to build on s390x, and this patch will allow them to build successfully", "comments": ["@lintian06/@terryheo Can you please review this PR ? Thanks!"]}, {"number": 51654, "title": "Error: 'tf.UnsortedSegmentJoin' op is neither a custom op nor a flex op", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution : google colab\r\n- TensorFlow installation (pip package or built from source): tensorflow 2.6 gpu\r\n- TensorFlow library: 2.6.0\r\n\r\n### 2. Code\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('translator') # path to the SavedModel directory\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n\r\nThe output from the converter invocation\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-78-a4bd67ed5756> in <module>()\r\n      4 converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n----> 5 tflite_model = converter.convert()\r\n\r\n6 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    311       for error_data in _metrics_wrapper.retrieve_collected_errors():\r\n    312         converter_error.append_error(error_data)\r\n--> 313       raise converter_error\r\n    314 \r\n    315   return _run_toco_binary(model_flags_str, toco_flags_str, input_data_str,\r\n\r\nConverterError: <unknown>:0: error: loc(callsite(callsite(callsite(callsite(\"RaggedSegmentJoin/UnsortedSegmentJoin@__inference_detokenize_676\" at \"StatefulPartitionedCall@__inference_restored_function_body_185084\") at \"StatefulPartitionedCall_2@__inference___call___187672\") at \"StatefulPartitionedCall@__inference_signature_wrapper_188041\") at \"StatefulPartitionedCall_4\")): 'tf.UnsortedSegmentJoin' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall_4\"): called from\r\n<unknown>:0: note: loc(callsite(callsite(callsite(callsite(\"RaggedSegmentJoin/UnsortedSegmentJoin@__inference_detokenize_676\" at \"StatefulPartitionedCall@__inference_restored_function_body_185084\") at \"StatefulPartitionedCall_2@__inference___call___187672\") at \"StatefulPartitionedCall@__inference_signature_wrapper_188041\") at \"StatefulPartitionedCall_4\")): Error code: ERROR_NEEDS_CUSTOM_OPS\r\n<unknown>:0: error: loc(callsite(callsite(callsite(callsite(\"RaggedSegmentJoin_1/UnsortedSegmentJoin@__inference_detokenize_676\" at \"StatefulPartitionedCall@__inference_restored_function_body_185084\") at \"StatefulPartitionedCall_2@__inference___call___187672\") at \"StatefulPartitionedCall@__inference_signature_wrapper_188041\") at \"StatefulPartitionedCall_4\")): 'tf.UnsortedSegmentJoin' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall_4\"): called from\r\n<unknown>:0: note: loc(callsite(callsite(callsite(callsite(\"RaggedSegmentJoin_1/UnsortedSegmentJoin@__inference_detokenize_676\" at \"StatefulPartitionedCall@__inference_restored_function_body_185084\") at \"StatefulPartitionedCall_2@__inference___call___187672\") at \"StatefulPartitionedCall@__inference_signature_wrapper_188041\") at \"StatefulPartitionedCall_4\")): Error code: ERROR_NEEDS_CUSTOM_OPS\r\n<unknown>:0: error: failed while converting: 'main': \r\nSome ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom \r\nCustom ops: UnsortedSegmentJoin\r\nDetails:\r\n\ttf.UnsortedSegmentJoin(tensor<?x!tf.string>, tensor<?xi64>, tensor<i32>) -> (tensor<1x!tf.string>) : {Tindices = i64, Tnumsegments = i32, device = \"\", separator = \" \"}\r\n\ttf.UnsortedSegmentJoin(tensor<?x!tf.string>, tensor<?xi64>, tensor<i32>) -> (tensor<?x!tf.string>) : {Tindices = i64, Tnumsegments = i32, device = \"\", separator = \" \"}\r\n\r\nI was trying to convert the model from https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb\r\nto a tflite model. The issue could be reproduced by running this in google colab and add \r\nconverter = tf.lite.TFLiteConverter.from_saved_model('translator') # path to the SavedModel directory\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\nat the bottom.", "comments": ["@MeghnaNatraj could you take a look at this flex op addition?", "Support added in https://github.com/tensorflow/tensorflow/commit/13196b41f38779020459fa3e64fa65a9ad60c641"]}, {"number": 51653, "title": "\"ZeroDivisionError: integer division or modulo by zero\" while backpropagating", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):\r\ntensorflow              2.5.0\r\ntensorflow-estimator    2.5.0\r\ntensorflow-probability  0.12.2\r\n\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 8101\r\n- GPU model and memory: GeForce GTX 1060\r\n\r\n**Describe the current behavior**\r\nTrying to train a component of my model throws an error during backprop calculation inside `grads = tape.gradient(loss, varibs)`. The error isn't very clear about what I'm doing wrong. Other parts of the model are training okay, but one section is throwing the following error.\r\n\r\n```\r\n    grads = tape.gradient(loss, varibs)\r\n  File \"C:\\Users\\Luke\\Anaconda3\\envs\\ScDreamer\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\", line 1074, in gradient\r\n    flat_grad = imperative_grad.imperative_grad(\r\n  File \"C:\\Users\\Luke\\Anaconda3\\envs\\ScDreamer\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\", line 71, in imperative_grad\r\n    return pywrap_tfe.TFE_Py_TapeGradient(\r\n  File \"C:\\Users\\Luke\\Anaconda3\\envs\\ScDreamer\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\", line 159, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \"C:\\Users\\Luke\\Anaconda3\\envs\\ScDreamer\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py\", line 229, in _ConcatGradV2\r\n    return _ConcatGradHelper(\r\n  File \"C:\\Users\\Luke\\Anaconda3\\envs\\ScDreamer\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py\", line 119, in _ConcatGradHelper\r\n    concat_dim._numpy().item(0) % input_values[0]._rank())  # pylint: disable=protected-access\r\nZeroDivisionError: integer division or modulo by zero\r\n```\r\n\r\nInvestigating the line of code, the issue is the rank of a scalar is being used in the modulo\r\n![image](https://user-images.githubusercontent.com/24449147/130629680-405695f9-642b-4be6-b4c0-acc6e3366267.png)\r\n\r\nI'm not sure what I've done wrong that has caused this.\r\n\r\n**Describe the expected behavior**\r\nIdeally the error would be caught before getting this deep and suggest a proper fix. At the moment, being told \"ZeroDivisionError: integer division or modulo by zero\" isn't helping me figure out what is wrong.\r\n\r\n**Standalone code to reproduce the issue**\r\nI don't really understand what the issue is here so I'm not sure where to start to try and reproduce this. If I can be pointed in the right direction to reproduce, I'll be happy to write some code.\r\n", "comments": ["I figured out a workaround, but I'm pretty sure that this is a bug.\r\n\r\nThe very last step of my loss calculation was concatenating a bunch of scalars together and taking the mean, by changing them from scalars to rank 1 tensors with keep_dims, the error no longer occurs.\r\n\r\nsome pseudo-code:\r\n\r\n```\r\nactor_losses = []\r\nfor e in error:\r\n    actor_loss = -tf.reduce_mean(e, axis=0, keepdims=True)        # removing axis & keepdims args will result in scalar output, causing error\r\n    actor_losses.append(actor_loss)\r\ntotal_loss = tf.reduce_mean(tf.concat(actor_losses, axis=0))\r\n```\r\n\r\n", "@LukeBolly ,\r\n In order to expedite the trouble-shooting process, could you please provide a complete code and the dataset you are using.Also please refer to these links [1](https://github.com/google/emoji-scavenger-hunt/issues/28) and [2](https://stackoverflow.com/questions/44998778/keras-zerodivisionerror-integer-division-or-modulo-by-zero) and let us know if it helped.Thanks!\r\n", "Hi! I am new at open source contribution. Is this issue still open? I would like to work on it.", "> Hi! I am new at open source contribution. Is this issue still open? I would like to work on it.\r\n\r\nYep, I probably won't have time to work on this for a while so it's all yours if you like. Let me know if you need any more information to reproduce the issue\r\n", "@LukeBolly Can you please help me with the review on my PR? It says to add a test for the behavior you're fixing.\r\n\r\n\r\n", "I think you need to add a bug reproduce into\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/kernel_tests\r\nI'm not 100% if bug repros go into kernel_tests, maybe one of the TF team can advise you on where the most appropriate location is.\r\n\r\nThe idea is that you write the test that causes the bug, then fix the code so that the test passes.", "@LukeBolly Could you help me reproduce the bug?", "What do you have so far? I don't have much time at the moment so I can't write the repro for you, but the gist of the error is that backpropagating through a list of scalars should trigger it.", "@LukeBolly ,\r\n\r\nPlease check the above [comment](https://github.com/tensorflow/tensorflow/issues/51653#issuecomment-905196574) and provide the complete code to reproduce the issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "how do I create unit tests?", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51653\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51653\">No</a>\n"]}, {"number": 51652, "title": "My libnnappi_delegate.so is debug or release", "body": "**System information**\r\n- OS Platform: ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MI 9(mobile phone\uff09 \r\n- TensorFlow installed from: Source \r\n- TensorFlow version: 2.5.0\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla K40m \r\n\r\n\r\n**Describe the problem**\r\nI use bazel to build libnnappi_delegate.so.\r\nCommand :\r\n```\r\nbazel build -c opt  --config=android_arm64  --cxxopt=\"-std=c++11\"  --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain     //tensorflow/lite/delegates/nnapi:nnapi_delegate \r\n```\r\nget libnnappi_delegate.so .\r\nHow can I know the shared library is \"Debug\" and \"Release\"?\r\n\r\n", "comments": ["Hi @yahuuu  !This does not seem to be  a bug or feature request, for any further queries you may open this issue in [TF discussion forum ](https://discuss.tensorflow.org/)as there is a larger community there.Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51652\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51652\">No</a>\n"]}, {"number": 51651, "title": "NotFoundError:", "body": "![3](https://user-images.githubusercontent.com/26819449/130610197-91537368-ef94-4440-b884-515bba21b770.JPG)\r\n\r\nI could not find anything related to solving this Error.\r\nI tried each and every possible way possible.\r\nPlease help.\r\nThankyou", "comments": ["@starboyvarun In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Ok sure.\r\n\r\n![3 1](https://user-images.githubusercontent.com/26819449/130612353-5c742024-0b9b-4bc3-8f99-d9fff4a2a9f5.JPG)\r\n", "@starboyvarun Please, share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "ok sure.\r\nhttps://colab.research.google.com/drive/1mBzzJGbyUhAg-HD0yC74ncXorI_oWDWA?usp=sharing\r\nThank You.", "@sushreebarsa  did you check it?", "@starboyvarun Could you please refer to the [tutorial ](https://github.com/BIMSBbioinfo/janggu/blob/master/docs/tutorial.rst) and let us know if it helps ? Could you please try to upgrade TF version to 2.6.0  ? Thanks!", "\r\n![image](https://user-images.githubusercontent.com/26819449/130725984-31a75f24-ff03-4341-bedb-0e91347a0a5b.png)\r\n\r\nThat's what I was saying I am implementing this library only but the error coming on the first cell of code.\r\nI have already tried running this tutorial.\r\nThat's why I was asking.\r\njust tell me how should I solve this.\r\nThankYou.\r\n\r\n", "I tried implementing a tutorial that you told me another error is coming can you help me .?\r\nThankyou.\r\nhttps://colab.research.google.com/drive/1Dv7srewnjAb6qUiXrKmjSNIvfMNwimkz?usp=sharing", "@starboyvarun Could you please refer to this [issue](https://stackoverflow.com/questions/43189302/syntaxerror-unexpected-eof-while-parsing) in stackoverflow  and let us know if it helps?Thanks!", "@sushreebarsa ,\r\nok, I referred, But I am not using any Loops in that code snippet. It doesn't help.\r\nI could not understand that link properly.\r\nThank you.\r\n\r\n", "@Saduf2019 Was able to reproduce the issue on colab using TF  v[2.2 ](https://colab.research.google.com/gist/sushreebarsa/d4fb83673df61ceb9f6d5bdedfbc1c70/experiment2.ipynb#scrollTo=bH_wlxJuWugd)& [ 2.6](https://colab.research.google.com/gist/sushreebarsa/ed3a002a84ecf313a99d946c2612db28/experiment2.ipynb#scrollTo=_NCcnMNBZ2Rs)  , please find the attached gists for reference.Thank you! "]}, {"number": 51650, "title": "Update Delegate.swift", "body": "Fixes \"Using 'class' keyword to define a class-constrained protocol is deprecated; use 'AnyObject' instead\" warning", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51650) for more info**.\n\n<!-- need_sender_cla -->", "@arielelkin  Can you please sign CLA. Thanks!", "@gbaned done."]}]