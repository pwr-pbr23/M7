[{"number": 33165, "title": "TF1.13.1 Bazel Build Error", "body": "\r\n**System information**\r\n- OS Platform and Distribution :Windows 10\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.5.2\r\n- Bazel version :  0.20.0\r\n- CUDA/cuDNN version: 10.0/7.6.4\r\n\r\n**Describe the problem**\r\n\r\nERROR: Skipping '//tensorflow:libtensorflow_cc.so': error loading package 'tensorflow': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 1556\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 1395, in _create_local_cuda_repository\r\n                find_cc(repository_ctx)\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 239, in find_cc\r\n                _get_msvc_compiler(repository_ctx)\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 156, in _get_msvc_compiler\r\n                find_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace(\"\\\\\", \"/\")\r\ntype 'NoneType' has no method replace(string, string)\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 1556\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 1395, in _create_local_cuda_repository\r\n                find_cc(repository_ctx)\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 239, in find_cc\r\n                _get_msvc_compiler(repository_ctx)\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 156, in _get_msvc_compiler\r\n                find_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace(\"\\\\\", \"/\")\r\ntype 'NoneType' has no method replace(string, string)\r\n\r\n\r\n\r\n", "comments": ["@zzh123123ise \r\nJust to verify, did you follow the steps mentioned in the [Tensorflow build ](https://www.tensorflow.org/install/source_windows)official website. If not please follow the steps. Let us know how it progresses.Which protobuf version are you using? Thanks!", "Thanks!Now I followed the official document but failed too.\r\n\r\n`bazel build --config=opt --config=cuda --copt=-nvcc_options=disable-warnings`\r\nthe error occured like this:\r\n```\r\nERROR: no such package '@local_config_cuda//crosstool': Traceback (most recent call last):\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 1556\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 1395, in _create_local_cuda_repository\r\n                find_cc(repository_ctx)\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 239, in find_cc\r\n                _get_msvc_compiler(repository_ctx)\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 156, in _get_msvc_compiler\r\n                find_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace(\"\\\\\", \"/\")\r\ntype 'NoneType' has no method replace(string, string)\r\n```\r\nIt seems bazel doesn't recognize @local_config_cuda//crosstool as a package,the same error\r\nwith the previous one.\r\n\r\nI got protoc-3.10.0-win64 in my computer.", "Thanks for your report. Can you please attach a complete log of `./configure` and `bazel build` starting from a fresh environment on the `master` branch?", "./configuire:\r\n\r\n```\r\nC:\\D\\tensorflow-1.13.1\\source>python ./configure.py\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nnul\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: c5e9770e-6eec-4f79-b491-5e0bda52f691\r\nYou have bazel 0.20.0 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\Administrator\\Anaconda3\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]:\r\n\r\n\r\nPlease specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 5.2\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apacha Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n```\r\nbazel build:\r\n\r\n```\r\nC:\\D\\tensorflow-1.13.1\\source>bazel build --config=opt --config=cuda --copt=-nvcc_options=disable-warnings\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nc:\\d\\tensorflow-1.13.1\\source/.bazelrc\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nINFO: Invocation ID: 60bcaa7e-ab7d-433b-a1dc-207e4838d15c\r\nERROR: no such package '@local_config_cuda//crosstool': Traceback (most recent call last):\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 1556\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 1395, in _create_local_cuda_repository\r\n                find_cc(repository_ctx)\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 239, in find_cc\r\n                _get_msvc_compiler(repository_ctx)\r\n        File \"C:/d/tensorflow-1.13.1/source/third_party/gpus/cuda_configure.bzl\", line 156, in _get_msvc_compiler\r\n                find_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace(\"\\\\\", \"/\")\r\ntype 'NoneType' has no method replace(string, string)\r\nINFO: Elapsed time: 10.476s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    Fetching @local_config_cuda; fetching\r\n\r\n```", "Just to double check: you're building on `master`, right? If so, you'll need to install one of the versions of bazel we support at `master`, which you can find in `configure.py` (this is currently 0.24.1 through 0.29.1).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33165\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33165\">No</a>\n", "Thanks for your repilies! But I've already build TF 1.13.1 successful.An incomplete install of Visual Studio and Visual C++ leads to this.I should check my environment first before I asked questions,sorry about that. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33165\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33165\">No</a>\n", "When installing VS2015, we need to choose c++, that solved this problem for me"]}, {"number": 33164, "title": "Docs: fix example in SparseCategoricalCrossEntropy", "body": "Fixes the example in the docstring of `tf.keras.losses.SparseCategoricalCrossEntropy` to the one used in `tf.keras.losses.CategoricalCrossEntropy`.\r\n\r\nNote that the current example is non-sensical as `[.5, .89, .6]` is not normalized (but `[.05, .89, .06]` is normalized, so it's probably just a typo).", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33164) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33164) for more info**.\n\n<!-- ok -->", "@ddobbelaere Can you please resolve conflicts? Thanks!", "This PR has been superseded by another one that also fixes the docstring."]}, {"number": 33163, "title": "'Can save best model only with val_loss available, skipping.' during tf.keras.callbacks.ModelCheckpoint", "body": "My model is built like:\r\n```\r\nModel:  \"sequential\"\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense (Dense)                (None, 1379)              3801903   \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 1379)              0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 1379)              1903020   \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 1379)              0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 1379)              1903020   \r\n_________________________________________________________________\r\ndropout_2 (Dropout)          (None, 1379)              0         \r\n_________________________________________________________________\r\ndense_3 (Dense)              (None, 1379)              1903020   \r\n_________________________________________________________________\r\ndropout_3 (Dropout)          (None, 1379)              0         \r\n_________________________________________________________________\r\ndense_4 (Dense)              (None, 1)                 1380      \r\n=================================================================\r\nTotal params: 9,512,343\r\nTrainable params: 9,512,343\r\nNon-trainable params: 0\r\n```\r\nHere the model is fitted with a callback.\r\n```\r\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, mode='max', monitor='val_acc', verbose=2, save_best_only=True)\r\ncallbacks_list = [checkpoint]\r\nmodel.fit(train_dataset, epochs=1000, callbacks=callbacks_list, verbose=2, steps_per_epoch=(number_of_samples//BATCH_SIZE))\r\n\r\n```\r\nThis gives me the warning:\r\n```\r\nEpoch 2/1000\r\nW1009 01:11:32.446842 11824 callbacks.py:990] Can save best model only with val_acc available, skipping.\r\n```", "comments": ["_Please make sure you fill in the issue template to ensure that your issue can be troubleshooted correctly. Key information includes System Specs and Tensorflow/CUDA/cuDNN versions. Without this it's harder to help!_\r\n\r\nA quick look at this suggests this may be because you are not actually supplying validation data to perform the val_acc metric on. Try setting validation in the fit() call and see how you get on.", "@raceee ,\r\nCan you please try solution provided by @jubjamie? Thanks!", "@jubjamie Thanks for the friendly correction. I am new to posting issues here.\r\n\r\nI have tried your fix using a tf.data.Dataset as my validation_data:\r\n```\r\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, mode='max', monitor='val_acc', verbose=2, save_best_only=True)\r\ncallbacks_list = [checkpoint]\r\nmodel.fit(train_dataset, validation_data=x_test_dataset, epochs=1000, callbacks=callbacks_list, verbose=2, steps_per_epoch=(X_train_deleted_nans.shape[0]//BATCH_SIZE))\r\n```\r\n\r\nThen this error comes up:\r\n`ValueError: When passing an infinitely repeating dataset, you must specify the \"validation_steps\" argument.`\r\n\r\nSo I set validation_steps to my batch size:\r\n\r\n```\r\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, mode='max', monitor='val_acc', verbose=2, save_best_only=True)\r\ncallbacks_list = [checkpoint]\r\nmodel.fit(train_dataset, validation_data=x_test_dataset, validation_steps=BATCH_SIZE, epochs=1000, callbacks=callbacks_list, verbose=2, steps_per_epoch=(X_train_deleted_nans.shape[0]//BATCH_SIZE))\r\n\r\n```\r\n\r\nand then I get the original warning:\r\n`W1009 22:43:42.809758  5456 callbacks.py:990] Can save best model only with val_acc available, skipping.`", "Hi,\r\n\r\nI had similar problem and found that in my environment the metric name was \"val_accuracy\" and not \"val_acc\". I fixed it as below and it started to work:\r\n\r\nbest_weights = ModelCheckpoint('best.h5', verbose=1, monitor='**val_accuracy**', save_best_only=True, mode='auto') \r\n\r\nIn order to check what are the metrics names in your env, do this. It will print you the correct metrics names:\r\n\r\nhist = model.fit(...)\r\n**for key in hist.history:\r\n           print(key)**", "@Benzion18 Thanks! That worked for me. In the documentation it refers to the monitor as 'val_acc'. I wonder if it is just you and I that have this difference.", "In Google Colab it works with \"val_acc\", but in Kaggle with \"val_accuracy\". Why? No idea.", "I was working in pycharm on my machine and it was 'val_accuracy'", "> @Benzion18 Thanks! That worked for me. In the documentation it refers to the monitor as 'val_acc'. I wonder if it is just you and I that have this difference.\r\n\r\n@raceee Could you provide a link to the web page wherer it mentions `val_acc`? You could create a PR to update it. Thanks!", "@raceee Thanks for finding this typo. I found the link [here](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to the web page where we need to update the typo. Thanks!", "@Benzion18 Thank you so much!!", "Hi Everyone, \r\n\r\nNice work, it looks like you've worked out a solution. \r\n\r\nTo clarify what's happening here: The source of the confusion here is the `model.compile` line.\r\nThe name of the metric, later on, matches whatever you passed to `compile`, and if you pass a [metric object](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) it will use the `name` attribute of the metric\r\n\r\n\r\n```\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['acc'])\r\nhistory_acc = model.fit(train_images, train_labels, epochs=1)\r\nhistory_acc.history\r\n```\r\n\r\n```\r\n{'acc': [0.82243335], 'loss': [0.5034547645966212]}\r\n```\r\n\r\n\r\n```\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['acc'])\r\nhistory_acc = model.fit(train_images, train_labels, epochs=1)\r\n```\r\n```\r\n{'accuracy': [0.82243335], 'loss': [0.5034547645966212]}\r\n```", "This thread have helped me to use the right metric for TF1.x, but seems now for TF2.x it was fixed and 'val_acc' should be used\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\r\nFrom TF 2 docs:\r\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n    filepath=checkpoint_filepath,\r\n    save_weights_only=True,\r\n    monitor='val_acc',\r\n    mode='max',\r\n    save_best_only=True)\r\n", "Was able to save an h5 file to the designated drive folder while monitoring val_accuracy using the following:\r\n\r\n\r\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpointPath + 'best.h5', monitor = 'val_accuracy', mode = 'max', save_best_only = True)\r\n\r\n\r\nhistory = model.fit( x = x_train, y = y_train, validation_data = ( x_val, y_val ),\r\n                    batch_size = BATCH_SIZE, validation_batch_size = BATCH_SIZE, callbacks = [model_checkpoint_callback], \r\n                    epochs = EPOCHS, shuffle = True ) \r\n\r\nsaveTrainedModel(model, history)", "try replacing `val_acc`  with `val_accuracy` . it worked on my case", "So should the documentation here (https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) be updated or this is fixed in 2.3? Still need to be ```val_accuracy``` on 2.2.", "Why is this not working for me (2.3.0)?\r\n```python\r\nmodel.fit(\r\n    train_dataset,\r\n    batch_size=batch_size,\r\n    steps_per_epoch=15,\r\n    epochs=epochs,\r\n    validation_steps=5,\r\n    validation_data=dev_dataset,\r\n    validation_batch_size=validation_batch_size,\r\n    callbacks=[\r\n        tf.keras.callbacks.EarlyStopping(patience=2),\r\n        tf.keras.callbacks.TensorBoard(\r\n            log_dir=os.path.join(model_dir, 'logs'),\r\n            update_freq=10\r\n        ),\r\n        tf.keras.callbacks.ModelCheckpoint(\r\n            os.path.join(model_dir, 'ckpt-{epoch:06d}'),\r\n            monitor='val_loss',\r\n            save_best_only=True,\r\n            save_weights_only=False,\r\n            save_freq=15\r\n        ),\r\n    ]\r\n)\r\n```\r\n\r\nGetting\r\n\r\n```\r\nWARNING:tensorflow:Can save best model only with val_loss available, skipping\r\n```", "Remember that there's no \"right\" setting here, it just depends on what you name the metrics you pass to `model.compile`.\r\n\r\n@Burton2000: Yeah, that page could be clearer. I'll send a quick update.\r\n\r\n@stefan-falk: That's worrying, `loss` is the one metric that's populated automatically, so it would be hard to get the name wrong. Can you provide a minimal end-to-end reproduction? There's not enough context to reproduce the issue. ", "@MarkDaoust I think I just used it wrong. I wanted to save checkpoints during an epoch and became a victim of Keras wizardry a bit. I have a custom model which now returns `{m.name: m.result() for m in self.metrics}` in `train_step()` and `dict(loss=val_loss)` in `test_step()` and I let it safe the model after an epoch.", "@stefan-falk have you solved it? I have similar issues. All of the options 'val_loss', 'val_acc', and 'val_accuracy' will produce that warning 'Can save best model only with XXX available, skipping' . The tensorflow version is TF 1.14, using tf.keras will produce above warnings. However, the code works correctly in pure Keras. ", "This problem seems to persist unless I'm missing something. I've tried with Tensorflow 1.14 AND 2.1. I have also tried with \"acc\" - \"val_acc\" and the \"accuracy\" - \"val_accuracy\" variant mentioned. No matter what I try I get: \"WARNING:tensorflow:Can save best model only with val_acc available, skipping.\"\r\n\r\n`opt = tf.keras.optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.1, nesterov=False)\r\n           \r\n       # Compile model\r\n    model.compile(loss='categorical_crossentropy',\r\n                    optimizer=opt,\r\n                    metrics=['accuracy']\r\n                    )\r\n\r\n```\r\ncheckpoint = ModelCheckpoint(f\"models/{MODEL_ID}_best_epoch.hdf5\", monitor='val_accuracy', verbose=1,\r\n                                    save_best_only=True, mode='max', save_freq=1)\r\n\r\n```\r\n    model.fit(\r\n        X_train, y_train,\r\n        batch_size=BATCH_SIZE,\r\n        epochs=EPOCHS,\r\n        validation_data=(X_val, y_val),\r\n        callbacks=[tensorboard, checkpoint ]\r\n        )`", "It just found the solution to my problem. \"save_freq\" was set to 1 meaning it would save the model at each batch if val_acc improved. But as val_acc is computed after each epoch it didn't get any info on how each batch did. Changing to \"save_freq='epoch'\" solved my problem. Please note that 'save_freq' in a previous version was called something else. Don't remember it now, and I couldn't find the info by a quick Google search. ", "This issue should not be closed. As noted by @MichaelSoegaard if save_freq is specified, then no matter what validation metric you specify as the monitor key you will get this error and model check-pointing will not work. Even specifying the number of batches to be a multiple of the number of batches per epoch does not solve the problem. This could be easily fixed by instead being able to specify the number of epochs after which check-pointing will occur. I cannot use epoch mode in my case as it takes far too long to save the model relative to the time it takes to process one epoch.", "@marchss Yes, as in my previous comment:\r\n\r\n> I have a custom model which now returns `{m.name: m.result() for m in self.metrics}` in `train_step()` and `dict(loss=val_loss)` in `test_step()` and I let it safe the model after an epoch.\r\n\r\nbut that was with Tensorflow 2.1.0 or 2.3.0 (or something around that).", "> It just found the solution to my problem. \"save_freq\" was set to 1 meaning it would save the model at each batch if val_acc improved. But as val_acc is computed after each epoch it didn't get any info on how each batch did. Changing to \"save_freq='epoch'\" solved my problem. Please note that 'save_freq' in a previous version was called something else. Don't remember it now, and I couldn't find the info by a quick Google search.\r\n\r\nthis worked for me, Thanks!", "> It just found the solution to my problem. \"save_freq\" was set to 1 meaning it would save the model at each batch if val_acc improved. But as val_acc is computed after each epoch it didn't get any info on how each batch did. Changing to \"save_freq='epoch'\" solved my problem. Please note that 'save_freq' in a previous version was called something else. Don't remember it now, and I couldn't find the info by a quick Google search.\r\n\r\nThank you so much this solved the issue for me", "> It just found the solution to my problem. \"save_freq\" was set to 1 meaning it would save the model at each batch if val_acc improved. But as val_acc is computed after each epoch it didn't get any info on how each batch did. Changing to \"save_freq='epoch'\" solved my problem. Please note that 'save_freq' in a previous version was called something else. **Don't remember it now, and I couldn't find the info by a quick Google search.**\r\n\r\nIt was previously called 'period' before the rename to save_freq.", "The issue still exists at TensorFlow 2.4.1 (Ubuntu 20.04). I tried to use the Least Common Multiplier of batch size and `Keras.model.history.params.steps` as `save_freq` however the validation loss seems to be calculated after the callback. Thus, still no validation metric can be used.\r\n\r\n> \r\n> [LEChaney wrote](https://github.com/tensorflow/tensorflow/issues/33163#issuecomment-747191782):\r\n> This issue should not be closed. As noted by @MichaelSoegaard if save_freq is specified, then no matter what validation metric you specify as the monitor key you will get this error and model check-pointing will not work. Even specifying the number of batches to be a multiple of the number of batches per epoch does not solve the problem. This could be easily fixed by instead being able to specify the number of epochs after which check-pointing will occur. I cannot use epoch mode in my case as it takes far too long to save the model relative to the time it takes to process one epoch.\r\n\r\n", "Maybe we need a clearer warning about `save_freq=N`.\r\n\r\n@anomp, callbacks are easy to modify, I bet this will do what you want:\r\n\r\n```\r\nclass MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\r\n  def __init__(self, epoch_per_save=1, *args, **kwargs):\r\n    self.epochs_per_save = epoch_per_save\r\n    super().__init__(save_freq='epochs', *args, **kwargs)\r\n\r\n  def on_epoch_end(self, epoch, logs):\r\n    if epoch % self.epochs_per_save != 0:\r\n      super().on_epoch_end(epoch, logs)\r\n```", "> Maybe we need a clearer warning about `save_freq=N`.\r\n> \r\n> @anomp, callbacks are easy to modify, I bet this will do what you want:\r\n> \r\n> ```\r\n> class MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\r\n>   def __init__(self, epoch_per_save=1, *args, **kwargs):\r\n>     self.epochs_per_save = epoch_per_save\r\n>     super().__init__(save_freq='epochs', *args, **kwargs)\r\n> \r\n>   def on_epoch_end(self, epoch, logs):\r\n>     if epoch % self.epochs_per_save != 0:\r\n>       super().on_epoch_end(epoch, logs)\r\n> ```\r\n\r\n@MarkDaoust worked like a charm, thank you! These are my changes for anyone interested.\r\n```\r\nclass MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\r\n    def __init__(self, epoch_per_save=1, *args, **kwargs):\r\n        logging.debug(\"MyModelCheckpoint called with epoch_per_save={}\".format(epoch_per_save))\r\n        self.epochs_per_save = epoch_per_save\r\n        super().__init__(save_freq=\"epoch\", *args, **kwargs)\r\n\r\n    def on_epoch_end(self, epoch, logs):\r\n        if epoch % self.epochs_per_save == 0:\r\n            super().on_epoch_end(epoch, logs)\r\n```\r\nCallback definition:\r\n```\r\ncallback_checkpoint = MyModelCheckpoint(epoch_per_save=5,\r\n                                                    filepath=str(self.path_to_save_model),\r\n                                                    monitor=\"val_loss\"\r\n                                                    verbose=1,\r\n                                                    save_best_only=True,\r\n                                                    mode=\"min\"\r\n                                                    )\r\n```\r\nProduces output:\r\n```\r\nEpoch 1/1000                                                                                                                        \r\n1244/1245 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.2450                                                     \r\nEpoch 00001: val_loss improved from inf to 1.79107, saving model to output_directory/reports\r\nWARNING:tensorflow:From python/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a futureversion.                                                                                                               \r\nInstructions for updating:                                                                                                        \r\nIf using Keras pass *_constraint arguments to layers.                                              \r\nFrom python/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.                                                                                                                \r\nInstructions for updating:                                                                                                          \r\nIf using Keras pass *_constraint arguments to layers.                                                                                                                                                                                       \r\nINFO:tensorflow:Assets written to: output_directory/reports/assets                  \r\nAssets written to: output_directory/reports/assets                                 \r\n1245/1245 [==============================] - 11s 8ms/step - loss: 2.3031 - accuracy: 0.2450 - val_loss: 1.7911 - val_accuracy: 0.4351\r\nEpoch 2/1000                                                                                                                        \r\n1245/1245 [==============================] - 9s 7ms/step - loss: 1.8651 - accuracy: 0.3727 - val_loss: 1.5282 - val_accuracy: 0.4881\r\nEpoch 3/1000                                                                                                                        \r\n1245/1245 [==============================] - 9s 7ms/step - loss: 1.6901 - accuracy: 0.4361 - val_loss: 1.3601 - val_accuracy: 0.5576    \r\nEpoch 4/1000                                                                                                                        \r\n1245/1245 [==============================] - 9s 7ms/step - loss: 1.5703 - accuracy: 0.4826 - val_loss: 1.2823 - val_accuracy: 0.5964\r\nEpoch 5/1000                                                                          \r\n1245/1245 [==============================] - 9s 7ms/step - loss: 1.4985 - accuracy: 0.5120 - val_loss: 1.1593 - val_accuracy: 0.6311\r\nEpoch 6/1000\r\n1240/1245 [============================>.] - ETA: 0s - loss: 1.4409 - accuracy: 0.5312\r\nEpoch 00006: val_loss improved from 1.79107 to 1.08370, saving model to output_directory/reports\r\nINFO:tensorflow:Assets written to: output_directory/reports/assets\r\nAssets written to: output_directory/reports/assets\r\n1245/1245 [==============================] - 10s 8ms/step - loss: 1.4405 - accuracy: 0.5313 - val_loss: 1.0837 - val_accuracy: 0.6506\r\nEpoch 7/1000\r\n1245/1245 [==============================] - 9s 7ms/step - loss: 1.4030 - accuracy: 0.5499 - val_loss: 1.0493 - val_accuracy: 0.6690\r\nEpoch 8/1000\r\n1245/1245 [==============================] - 9s 7ms/step - loss: 1.3724 - accuracy: 0.5596 - val_loss: 1.0612 - val_accuracy: 0.6677\r\nEpoch 9/1000\r\n1245/1245 [==============================] - 9s 7ms/step - loss: 1.3402 - accuracy: 0.5700 - val_loss: 0.9856 - val_accuracy: 0.6969\r\nEpoch 10/1000\r\n1245/1245 [==============================] - 9s 7ms/step - loss: 1.3217 - accuracy: 0.5779 - val_loss: 0.9939 - val_accuracy: 0.6856\r\nEpoch 11/1000\r\n1245/1245 [==============================] - ETA: 0s - loss: 1.3052 - accuracy: 0.5834\r\nEpoch 00011: val_loss improved from 1.08370 to 0.94162, saving model to output_directory/reports\r\nINFO:tensorflow:Assets written to: output_directory/reports/assets\r\nAssets written to: output_directory/reports/assets\r\n```", "I think this was resolved. Recent TF page on ModelCheckpoint https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint has most of the updates discussed in this thread. \r\n\r\nIf you still have some issues with `ModelCheckpoint` callback, please feel free to open a new issue. \r\n\r\nI am closing this issue as this was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33163\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33163\">No</a>\n"]}, {"number": 33162, "title": "Continue training on SavedModel or load checkpoint from SavedModel", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.14\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIn tensorflow 1.14, it's obvious that tf.compat.v1.train.init_from_checkpoint can load ckpt to continue training (or to warm start). However, I couldn't find any corresponding approaches in SavedModel, and tf.estimator.WarmStartSetting only supports ckpt as well. It would be helpful if we could either load checkpoint in SavedModel or warm-start training from SavedModel.\r\n\r\n**Will this change the current api? How?**\r\n1. train.init_from_checkpoint could load SavedModel \r\n2. tf.estimator.WarmStartSettings could load SavedModel\r\n\r\n**Who will benefit with this feature?**\r\nThose who store models in SavedModel format and want to keep training (or warm start).\r\n\r\n**Any Other info.**\r\n", "comments": ["@davislf2 \r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.", "@ravikyram \r\nIn our use case, we serve our model with SavedModel format. For a live product, it's possible that we might want to add some new data without retraining the model from scratch, and init_checkpoint can save time. Since deploying and serving SavedModel is more encouraged, I think it might be more convenient if we can also warm start from SavedModel rather than checkpoints.\r\n\r\n", "Can we set it from `initial_value` of `tf.Variable`? If yes, then the question will be how to get the value from `tf.train.Saver().restore()`?\r\n\r\nHow does transfer learning do it? Does it load the model from `hdf5` and performs training?", "Are there any plans for work on this in the future?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 33161, "title": "TF2.0: Tensorboard The connection was reset", "body": "I have the same problem but w/ TF2.0-GPU, and I am on Chrome/Ubuntu, running it inside a docker container w/ juypter, miniconda, cuda-x, tensorrt. The error I get says: The connection was reset when trying to display it in the notebook, despite everything else working fine. (I'm not sure, but even when I goto localhost:6006 it doesn't load either).\r\n\r\n**Error 1:**\r\n![1](https://user-images.githubusercontent.com/5199900/66453018-6f651e00-ea28-11e9-8db4-6961c3cd15cc.png)\r\n\r\n**Error 2:**\r\n![2](https://user-images.githubusercontent.com/5199900/66453028-7be97680-ea28-11e9-890a-ec46d30abe9b.png)\r\n\r\n**Dockerfile:**\r\n```\r\nFROM nvidia/cuda:10.0-cudnn7-devel-ubuntu18.04\r\n\r\n# Core Linux Deps\r\nRUN DEBIAN_FRONTEND=noninteractive apt-get update && apt-get install -y --fix-missing --no-install-recommends apt-utils \\\r\n        build-essential \\\r\n        curl \\\r\n\tbinutils \\\r\n\tgdb \\\r\n        git \\\r\n\tfreeglut3 \\\r\n\tfreeglut3-dev \\\r\n\tlibxi-dev \\\r\n\tlibxmu-dev \\\r\n\tgfortran \\\r\n        pkg-config \\\r\n\tpython-numpy \\\r\n\tpython-dev \\\r\n\tpython-setuptools \\\r\n\tlibboost-python-dev \\\r\n\tlibboost-thread-dev \\\r\n        pbzip2 \\\r\n        rsync \\\r\n        software-properties-common \\\r\n        libblas3 \\\r\n        liblapack3 \\\r\n        liblapack-dev \\\r\n        libblas-dev \\\r\n        libboost-all-dev \\\r\n        libopenblas-dev \\ \r\n        libtbb2 \\\r\n        libtbb-dev \\\r\n        libjpeg-dev \\\r\n        libpng-dev \\\r\n        libtiff-dev \\\r\n\tlibgraphicsmagick1-dev \\\r\n        libavresample-dev \\\r\n        libavformat-dev \\\r\n        libhdf5-dev \\\r\n        libpq-dev \\\r\n\tlibgraphicsmagick1-dev \\\r\n\tlibavcodec-dev \\\r\n\tlibgtk2.0-dev \\\r\n\tliblapack-dev \\\r\n        liblapacke-dev \\\r\n\tlibswscale-dev \\\r\n\tlibcanberra-gtk-module \\\r\n        libboost-dev \\\r\n\tlibboost-all-dev \\\r\n        libeigen3-dev \\\r\n\twget \\\r\n        vim \\\r\n        qt5-default \\\r\n        unzip \\\r\n\tzip \\ \r\n        && \\\r\n    apt-get clean && \\\r\n    rm -rf /var/lib/apt/lists/*  && \\\r\n    apt-get clean && rm -rf /tmp/* /var/tmp/*\r\n\r\n\r\n# Install cmake version that supports anaconda python path\r\nRUN wget -O cmake.tar.gz https://github.com/Kitware/CMake/releases/download/v3.15.4/cmake-3.15.4-Linux-x86_64.tar.gz\r\nRUN tar -xvf cmake.tar.gz\r\nWORKDIR /cmake-3.15.4-Linux-x86_64\r\nRUN cp -r bin /usr/\r\nRUN cp -r share /usr/\r\nRUN cp -r doc /usr/share/\r\nRUN cp -r man /usr/share/\r\nWORKDIR /\r\nRUN rm -rf cmake-3.15.4-Linux-x86_64\r\nRUN rm -rf cmake.tar.gz\r\n\r\n\r\n# Install TensorRT (TPU Access)\r\nRUN apt-get update && \\\r\n        apt-get install -y nvinfer-runtime-trt-repo-ubuntu1804-5.0.2-ga-cuda10.0 && \\\r\n        apt-get update && \\\r\n        apt-get install -y libnvinfer5=5.0.2-1+cuda10.0\r\n\r\nRUN file=\"$(ls -1 /usr/local/)\" && echo $file\r\n\r\n\r\n# Fix conda errors per Anaconda team until they can fix\r\nRUN mkdir ~/.conda\r\n\r\n\r\n# Install Anaconda\r\nRUN wget --quiet https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh && \\\r\n/bin/bash Miniconda3-latest-Linux-x86_64.sh -f -b -p /opt/conda && \\\r\nrm Miniconda3-latest-Linux-x86_64.sh\r\nENV PATH /opt/conda/bin:$PATH\r\n\r\n\r\n# For CUDA profiling, TensorFlow requires CUPTI.\r\nENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\n\r\nARG PYTHON=python3\r\nARG PIP=pip3\r\n\r\n# See http://bugs.python.org/issue19846\r\nENV LANG C.UTF-8\r\n\r\nRUN apt-get update && apt-get install -y \\\r\n    ${PYTHON} \\\r\n    ${PYTHON}-pip\r\n\r\nRUN ${PIP} --no-cache-dir install --upgrade \\\r\n    pip \\\r\n    setuptools \\\r\n    hdf5storage \\\r\n    h5py \\\r\n    matplotlib \\\r\n    pyinstrument\r\n\r\n# Add auto-complete to Juypter\r\nRUN pip install jupyter-tabnine\r\nRUN pip install pydash\r\n\r\nRUN conda update -n base -c defaults conda\r\nRUN conda install -c anaconda jupyter \r\nRUN conda install pytorch torchvision cudatoolkit=10.0 -c pytorch\r\nRUN conda update conda\r\nRUN conda install numba\r\nRUN conda install -c anaconda cupy \r\nRUN conda install -c anaconda ipykernel \r\nRUN conda install -c anaconda seaborn \r\nRUN conda install -c anaconda ipython \r\n\r\n\r\n# Some TF tools expect a \"python\" binary\r\nRUN ln -s $(which ${PYTHON}) /usr/local/bin/python \r\n\r\nRUN pip install tf-nightly-gpu-2.0-preview\r\n#RUN pip install tensorflow-gpu\r\n\r\n#COPY bashrc /etc/bash.bashrc\r\n#RUN chmod a+rwx /etc/bash.bashrc\r\n\r\nRUN ${PIP} --no-cache-dir install jupyter  \r\n\r\n\r\nWORKDIR /\r\nRUN wget -O opencv.zip https://github.com/opencv/opencv/archive/master.zip\r\nRUN wget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/master.zip\r\nRUN unzip opencv.zip\r\nRUN unzip opencv_contrib.zip\r\nRUN mv opencv-master opencv\r\nRUN mv opencv_contrib-master opencv_contrib\r\nRUN mkdir /opencv/build\r\nWORKDIR /opencv/build\r\n\r\nRUN cmake -DBUILD_TIFF=ON \\\r\n\t\t  -DBUILD_opencv_java=OFF \\\r\n\t\t  -DWITH_CUDA=ON \\\r\n\t\t  -DENABLE_FAST_MATH=1 \\\r\n\t\t  -DCUDA_FAST_MATH=1 \\\r\n\t\t  -DWITH_CUBLAS=1 \\\r\n\t\t  -DENABLE_AVX=ON \\\r\n\t\t  -DWITH_OPENGL=ON \\\r\n\t\t  -DWITH_OPENCL=OFF \\\r\n\t\t  -DWITH_IPP=ON \\\r\n\t\t  -DWITH_TBB=ON \\\r\n\t\t  -DWITH_EIGEN=ON \\\r\n\t\t  -DWITH_V4L=ON \\\r\n\t\t#   -DBUILD_TESTS=OFF \\\r\n\t\t#   -DBUILD_PERF_TESTS=OFF \\\r\n\t\t  -DCMAKE_BUILD_TYPE=RELEASE \\\r\n\t\t  -DCMAKE_INSTALL_PREFIX=$(python -c \"import sys; print(sys.prefix)\") \\\r\n\t\t  -D PYTHON3_EXECUTABLE=$(which python3) \\\r\n                  -D PYTHON_INCLUDE_DIR=$(python3 -c \"from distutils.sysconfig import get_python_inc; print(get_python_inc())\") \\\r\n                  -D PYTHON_INCLUDE_DIR2=$(python3 -c \"from os.path import dirname; from distutils.sysconfig import get_config_h_filename; print(dirname(get_config_h_filename()))\") \\\r\n                  -D PYTHON_LIBRARY=$(python3 -c \"from distutils.sysconfig import get_config_var;from os.path import dirname,join ; print(join(dirname(get_config_var('LIBPC')),get_config_var('LDLIBRARY')))\") \\\r\n                  -D PYTHON3_PACKAGES_PATH=$(python3 -c \"from distutils.sysconfig import get_python_lib; print(get_python_lib())\") \\\r\n                  -DOPENCV_ENABLE_NONFREE=ON \\\r\n                  -DOPENCV_EXTRA_MODULES_PATH=/opencv_contrib/modules \\\r\n                  -DBUILD_EXAMPLES=ON \\\r\n                  -D CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-10.0 \\\r\n                  -DWITH_QT=ON ..\r\n                 \r\nRUN make -j4 \\\r\n        && make install \\\r\n\t&& rm /opencv.zip \\\r\n        && rm /opencv_contrib.zip \\\r\n\t&& rm -rf /opencv \\\r\n        && rm -rf /opencv_contrib\r\n\r\n\r\nWORKDIR /\r\n\r\n\r\n# dlib\r\nRUN cd ~ && \\\r\n    mkdir -p dlib && \\\r\n    git clone -b 'v19.16' --single-branch https://github.com/davisking/dlib.git dlib/ && \\\r\n    cd  dlib/ && \\\r\n    python3 setup.py install --yes USE_AVX_INSTRUCTIONS --yes DLIB_USE_CUDA --clean\r\n\r\n\r\nRUN mkdir /.local && chmod a+rwx /.local\r\n\r\nWORKDIR /tf\r\nEXPOSE 8888 6006\r\n\r\n\r\nRUN useradd -ms /bin/bash container_user\r\n\r\n\r\nRUN ${PYTHON} -m ipykernel.kernelspec\r\n\r\n\r\nCMD [\"bash\", \"-c\", \"source /etc/bash.bashrc && jupyter notebook --notebook-dir=/tf --ip 0.0.0.0 --no-browser --allow-root --NotebookApp.custom_display_url='http://localhost:8888'\"]\r\n\r\n``` #", "comments": ["@joehoeller,\r\nCan you please confirm if you have used   `tf.summary.trace_export` in your code so that Tensorboard uses the Log Files created using that command. If you have used `tf.summary.trace_export`, can you please share your code so that we can reproduce it and investigate about it. Thanks!", "@rmothukuru, here is the notebook:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport datetime, os\r\n\r\nfashion_mnist = tf.keras.datasets.fashion_mnist\r\n\r\n(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\ndef create_model():\r\n  return tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n    tf.keras.layers.Dense(512, activation='relu'),\r\n    tf.keras.layers.Dropout(0.2),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n\r\ndef train_model():\r\n  \r\n  model = create_model()\r\n  model.compile(optimizer='adam',\r\n                loss='sparse_categorical_crossentropy',\r\n                metrics=['accuracy'])\r\n\r\n  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\r\n  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\r\n\r\n  model.fit(x=x_train, \r\n            y=y_train, \r\n            epochs=5, \r\n            validation_data=(x_test, y_test), \r\n            callbacks=[tensorboard_callback])\r\n\r\ntrain_model()\r\n\r\n# Start TensorBoard within the notebook \r\n%load_ext tensorboard\r\n%tensorboard --logdir logs\r\n\r\ntrain_model()\r\n\r\nfrom tensorboard import notebook\r\nnotebook.list() # View open TensorBoard instance\r\n\r\n```", "@joehoeller,\r\nI have created a New Virtual Environment in Anaconda and installed `Numpy, Pandas, Matplotlib and Tensorflow-Gpu`. After that, I have executed your code and it worked fine. Please find the below screenshot.\r\n\r\n![image](https://user-images.githubusercontent.com/48206667/66622419-23bb9b80-ec05-11e9-83a5-c5e894d2d5b0.png).\r\n\r\nCan you please try the same. If it is working with a New Virtual Environment, can you please share your Dockerfile so that we can reproduce it. Thanks!\r\n\r\n", "I did share the Dockerfile, scroll up. It installs Miniconda btw.\n\nOn Thu, Oct 10, 2019 at 11:11 PM rmothukuru <notifications@github.com>\nwrote:\n\n> @joehoeller <https://github.com/joehoeller>,\n> I have created a New Virtual Environment in Anaconda and installed Numpy,\n> Pandas, Matplotlib and Tensorflow-Gpu. After that, I have executed your\n> code and it worked fine. Please find the below screenshot.\n>\n> [image: image]\n> <https://user-images.githubusercontent.com/48206667/66622419-23bb9b80-ec05-11e9-83a5-c5e894d2d5b0.png>\n> .\n>\n> Can you please try the same. If it is working with a New Virtual\n> Environment, can you please share your Dockerfile so that we can reproduce\n> it. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33161?email_source=notifications&email_token=ABHVQHFXXJ74P7MW62TSP5TQN74FHA5CNFSM4I62KHQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEA6VWFY#issuecomment-540891927>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABHVQHDBWIUQZFRT3JDOO73QN74FHANCNFSM4I62KHQA>\n> .\n>\n", "@joehoeller,\r\nSorry about that. Got the Dockerfile. ", "Nah, your fine. Let me know what I need to change in Docker plz.\n\nOn Fri, Oct 11, 2019 at 12:31 AM rmothukuru <notifications@github.com>\nwrote:\n\n> @joehoeller <https://github.com/joehoeller>,\n> Sorry about that. Got the Dockerfile.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33161?email_source=notifications&email_token=ABHVQHBTXDTWIMW55QIHLKTQOAFUJA5CNFSM4I62KHQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEA62QBI#issuecomment-540911621>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABHVQHCEPXJF27Y3NT7WBLTQOAFUJANCNFSM4I62KHQA>\n> .\n>\n", "Any updates?\r\n\r\n(Would it help to post the Docker commands?)", "Can you please try the suggestion given in this [issue](https://stackoverflow.com/questions/48819139/tensorboard-link-wont-load-tensorflow-graphs-page-in-browser-the-connection-wa) and let me know if it helps @joehoeller ", "There is another issue I found that I am working out which has to do with Conda env. But is this doc accurate? It says I cannot use CUDA 10.x with TF2.0:\r\n\r\n> GPU TensorFlow uses CUDA. On Windows only CUDA 9.0 is supported. On Linux CUDA 8.0, 9.0, and 9.2 are supported, and 9.2 is the default.\r\n> \r\n> To install GPU TensorFlow with a non-default CUDA version such as 8.0:\r\n\r\nLink:\r\nhttps://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/", "> \r\n> \r\n> Can you please try the suggestion given in this [issue](https://stackoverflow.com/questions/48819139/tensorboard-link-wont-load-tensorflow-graphs-page-in-browser-the-connection-wa) and let me know if it helps @joehoeller\r\n\r\n\r\nI had the same with TF2.0 no GPU running in docker and the information provided on the link solved the problem for me.\r\n\r\n\r\n", "Just saw this. Will do at some point today, sit tight. Sorry for the delay.\n\nOn Fri, Oct 18, 2019 at 6:17 AM garciacaravantes <notifications@github.com>\nwrote:\n\n> Can you please try the suggestion given in this issue\n> <https://stackoverflow.com/questions/48819139/tensorboard-link-wont-load-tensorflow-graphs-page-in-browser-the-connection-wa>\n> and let me know if it helps @joehoeller <https://github.com/joehoeller>\n> I had the same with TF2.0 no GPU running in docker and the information\n> provided on the link solved the problem for me.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33161?email_source=notifications&email_token=ABHVQHE3ZQLNFMW54EVPEKDQPGLLZA5CNFSM4I62KHQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBT5YIY#issuecomment-543677475>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABHVQHFYS5K552CYPL3JMJLQPGLLZANCNFSM4I62KHQA>\n> .\n>\n", "@joehoeller Were you able to solve the issue?", "Closing this issue as it has been inactive for more than a week. Please add additional comments and we can open the issue again. Thanks!", "Sorry I\u2019ve been busy on project work. You must know how it is \u2014 most ppl\nare on a 2 week cycle w me.\n\nOn Tue, Oct 29, 2019 at 3:44 PM gowthamkpr <notifications@github.com> wrote:\n\n> Closed #33161 <https://github.com/tensorflow/tensorflow/issues/33161>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33161?email_source=notifications&email_token=ABHVQHCLW5H2YDFI7N53GMTQRCOBFA5CNFSM4I62KHQKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOUQRG2XI#event-2753719645>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABHVQHHAKTCBY63ST7BAO63QRCOBFANCNFSM4I62KHQA>\n> .\n>\n", "> > Can you please try the suggestion given in this [issue](https://stackoverflow.com/questions/48819139/tensorboard-link-wont-load-tensorflow-graphs-page-in-browser-the-connection-wa) and let me know if it helps @joehoeller\r\n> \r\n> I had the same with TF2.0 no GPU running in docker and the information provided on the link solved the problem for me.\r\n\r\nI got the same problem with the host too @joehoeller. And the method provided on the link solved my problem. Thank god! ", "`%load_ext tensorboard\r\n%tensorboard --logdir /tf/notebooks/graphs --host 0.0.0.0`\r\n\r\nSolves the problem!", "I will try this when I get back from vacation, thank you.\n\nOn Wed, Dec 25, 2019 at 10:51 PM ijakparov <notifications@github.com> wrote:\n\n> %load_ext tensorboard %tensorboard --logdir /tf/notebooks/graphs --host\n> 0.0.0.0\n>\n> Solves the problem!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33161?email_source=notifications&email_token=ABHVQHHYTN67JF5BVL7II2LQ2RA4TA5CNFSM4I62KHQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHU6VCQ#issuecomment-568978058>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABHVQHGO5X7WUUXMPAUXRODQ2RA4TANCNFSM4I62KHQA>\n> .\n>\n"]}, {"number": 33160, "title": "Node 'gradients_boolean_mask_1_gatherv2_grad_shape_boolean_mask_1_reshape' expects to be colocated with unknown node 'boolean_mask_1/Reshape'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI was getting `ValueError: Node 'gradients_boolean_mask_1_gatherv2_grad_shape_boolean_mask_1_reshape' expects to be colocated with unknown node 'boolean_mask_1/Reshape'\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n```\r\ndef flow(y_e, y_a, y_t, y_pair, atoms, adjacency_map, coordinates, atom_in_mol,\r\n    bond_in_mol, angle_in_mol, torsion_in_mol, attr_in_mol):\r\n\r\n    per_mol_mask = tf.matmul(\r\n        tf.where(\r\n            atom_in_mol,\r\n            tf.ones_like(atom_in_mol, dtype=tf.float32),\r\n            tf.zeros_like(atom_in_mol, dtype=tf.float32)),\r\n        tf.transpose(\r\n            tf.where(\r\n                atom_in_mol,\r\n                tf.ones_like(atom_in_mol, dtype=tf.float32),\r\n                tf.zeros_like(atom_in_mol, dtype=tf.float32))))\r\n\r\n    bond_idxs, angle_idxs, torsion_idxs = gin.probabilistic.gn_hyper\\\r\n        .get_geometric_idxs(atoms, adjacency_map)\r\n\r\n    is_bond = tf.greater(\r\n        adjacency_map,\r\n        tf.constant(0, dtype=tf.float32))\r\n\r\n    distance_matrix = gin.deterministic.md.get_distance_matrix(\r\n        coordinates)\r\n\r\n    bond_distances = tf.boolean_mask(\r\n        distance_matrix,\r\n        is_bond)\r\n\r\n    angle_angles = gin.deterministic.md.get_angles(\r\n        coordinates,\r\n        angle_idxs)\r\n\r\n    torsion_dihedrals = gin.deterministic.md.get_dihedrals(\r\n        coordinates,\r\n        torsion_idxs)\r\n\r\n    u_bond = tf.math.reduce_sum(\r\n        tf.math.multiply(\r\n            y_e,\r\n            tf.math.pow(\r\n                tf.expand_dims(\r\n                    bond_distances,\r\n                    1),\r\n                tf.range(16, dtype=tf.float32))),\r\n        axis=1)\r\n\r\n    u_angle = tf.math.reduce_sum(\r\n        tf.math.multiply(\r\n            y_a,\r\n            tf.math.pow(\r\n                tf.expand_dims(\r\n                    angle_angles,\r\n                    1),\r\n                tf.range(16, dtype=tf.float32))),\r\n        axis=1)\r\n\r\n    u_dihedral = tf.math.reduce_sum(\r\n        tf.math.multiply(\r\n            y_t,\r\n            tf.math.pow(\r\n                tf.expand_dims(\r\n                    torsion_dihedrals,\r\n                    1),\r\n                tf.range(16, dtype=tf.float32))),\r\n        axis=1)\r\n\r\n    u_pair = tf.reduce_sum(\r\n            tf.multiply(\r\n                y_pair,\r\n                tf.math.pow(\r\n                    tf.expand_dims(\r\n                            tf.where(\r\n                                tf.logical_and(\r\n                                    tf.equal(\r\n                                        tf.eye(\r\n                                            tf.shape(\r\n                                                distance_matrix)[0],\r\n                                            dtype=tf.float32),\r\n                                        tf.constant(0, dtype=tf.float32)),\r\n                                    tf.greater(\r\n                                        distance_matrix,\r\n                                        tf.constant(0, dtype=tf.float32))),\r\n                                tf.pow(\r\n                                    distance_matrix + 1e-2,\r\n                                    -1),\r\n                                distance_matrix),\r\n\r\n                        axis=2),\r\n                    tf.range(1, 16, dtype=tf.float32))),\r\n            axis=2)\r\n\r\n    u_pair_mask = tf.linalg.band_part(\r\n        tf.nn.relu(\r\n            tf.subtract(\r\n                tf.subtract(\r\n                    per_mol_mask,\r\n                    adjacency_map),\r\n                tf.eye(\r\n                    tf.shape(per_mol_mask)[0]))),\r\n        0, -1)\r\n\r\n    u_pair = tf.multiply(\r\n        u_pair_mask,\r\n        u_pair)\r\n\r\n    u_bond_tot = tf.matmul(\r\n        tf.transpose(\r\n            tf.where(\r\n                bond_in_mol,\r\n                tf.ones_like(bond_in_mol, dtype=tf.float32),\r\n                tf.zeros_like(bond_in_mol, dtype=tf.float32))),\r\n        tf.expand_dims(\r\n            u_bond,\r\n            axis=1))\r\n\r\n    u_angle_tot = tf.matmul(\r\n        tf.transpose(\r\n            tf.where(\r\n                angle_in_mol,\r\n                tf.ones_like(angle_in_mol, dtype=tf.float32),\r\n                tf.zeros_like(angle_in_mol, dtype=tf.float32))),\r\n        tf.expand_dims(\r\n            u_angle,\r\n            axis=1))\r\n\r\n    u_dihedral_tot = tf.matmul(\r\n        tf.transpose(\r\n            tf.where(\r\n                torsion_in_mol,\r\n                tf.ones_like(torsion_in_mol, dtype=tf.float32),\r\n                tf.zeros_like(torsion_in_mol, dtype=tf.float32))),\r\n        tf.expand_dims(\r\n            u_dihedral,\r\n            axis=1))\r\n\r\n    u_pair_tot = tf.boolean_mask(\r\n        tf.matmul(\r\n            tf.transpose(\r\n                tf.where(\r\n                    atom_in_mol,\r\n                    tf.ones_like(atom_in_mol, dtype=tf.float32),\r\n                    tf.zeros_like(atom_in_mol, dtype=tf.float32))),\r\n            tf.reduce_sum(\r\n                u_pair,\r\n                axis=1,\r\n                keepdims=True)),\r\n        attr_in_mol)\r\n\r\n    u_tot = tf.squeeze(\r\n        u_bond_tot + u_angle_tot + u_dihedral_tot + u_pair_tot)\r\n\r\n    return u_tot\r\n\r\n with tf.GradientTape(persistent=True) as tape:\r\n                tape.watch(coordinates)\r\n                y_e, y_a, y_t, y_pair, bond_in_mol, angle_in_mol, torsion_in_mol = gn(\r\n                    atoms, adjacency_map, coordinates, atom_in_mol, attr_in_mol)\r\n\r\n                u_hat = flow(y_e, y_a, y_t, y_pair, atoms, adjacency_map,\r\n                    coordinates, atom_in_mol, bond_in_mol, angle_in_mol,\r\n                    torsion_in_mol, attr_in_mol)\r\n\r\n                jacobian_hat = tape.gradient(\r\n                    u_hat,\r\n                    coordinates)\r\n\r\n                jacobian_hat = tf.boolean_mask(\r\n                    jacobian_hat,\r\n                    tf.reduce_any(\r\n                        atom_in_mol,\r\n                        axis=1))\r\n\r\n                jacobian = tf.boolean_mask(\r\n                    jacobian,\r\n                    tf.reduce_any(\r\n                        atom_in_mol,\r\n                        axis=1))\r\n\r\n                u = tf.boolean_mask(\r\n                    u,\r\n                    attr_in_mol)\r\n\r\n                loss = tf.reduce_sum(tf.keras.losses.MSE(u, u_hat)) + tf.reduce_sum(tf.keras.losses.MSE(\r\n                    jacobian, jacobian_hat))\r\n\r\n                print(loss)\r\n\r\n            variables = gn.variables\r\n            grad = tape.gradient(loss, variables)\r\n\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 33159, "title": "AttributeError: module 'gast' has no attribute 'Num'", "body": "System Information\uff1a\r\n    python == 3.5.4\r\n    tensorflow == 2.0.0-beta0\r\n\r\nDescription\uff1a\r\nWhen I running the seq2seq with attention ,it report an warning\r\nWARNING:tensorflow:Entity <function train_step at 0x000000002BF42510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function train_step at 0x000000002BF42510>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method Encoder.call of <__main__.Encoder object at 0x00000000307476A0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Encoder.call of <__main__.Encoder object at 0x00000000307476A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <function standard_gru at 0x00000000143B1400> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x00000000143B1400>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function cudnn_gru at 0x00000000143B16A8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x00000000143B16A8>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x00000000307AE160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Decoder.call of <__main__.Decoder object at 0x00000000307AE160>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method Attention.call of <__main__.Attention object at 0x00000000307AA358>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Attention.call of <__main__.Attention object at 0x00000000307AA358>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\n", "comments": ["pip install gast==0.2.2", "Please upgrade pip and then you should be able to install tensorflow 2.0 which already has the issue fixed.\r\n\r\nOr, manually install gast 0.2.2\r\n\r\nClosing as duplicate of #32319"]}, {"number": 33158, "title": "Build fails to create GPU target ( nvptx64-nvidia-cuda ) for CUDA10.0/CUDNN7.0/VT100", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Debian GNU/Linux 9 \\n \\l\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.x\r\n- Python version:  2.7\r\n- Installed using virtualenv? pip? conda?:  pip\r\n- Bazel version (if compiling from source): 26.1\r\n- GCC/Compiler version (if compiling from source): g++ (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\n- CUDA/cuDNN version: 10.0/7.0\r\n- GPU model and memory: Tesla V100-SXM2, 16130MiB \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nBuild Fails\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nERROR: /home/george/tensorflow/tensorflow/core/kernels/BUILD:4675:1: C++ compilation of rule '//tensorflow/core/kernels:fake_quant_ops_gpu' failed (Exit 1)\r\nerror: unable to create target: 'No available targets are compatible with triple \"nvptx64-nvidia-cuda\"'\r\n1 error generated when compiling for sm_70.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 4036.064s, Critical Path: 83.02s\r\nINFO: 4647 processes: 4647 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Thanks for your report. Can you please attach a complete log of ./configure and bazel build starting from a fresh environment on the master branch?", "So, I ended up switching over to Python 3 ( Conda based ) and it looks like\neverything worked.\n\nI would be happy to try to debug the Python 2 issue I had again, and report\nback, but I'll need to redo the installation from scratch.\n\nI am renting my own GCP instance to do this - and its pretty expensive to\nrun a tensorflow build from scratch because it takes so long.\n\nCan you recommend any free or cheap resources with GPUs attached ?\n\n\nOn Fri, Oct 11, 2019 at 11:28 AM Austin Anderson <notifications@github.com>\nwrote:\n\n> Thanks for your report. Can you please attach a complete log of\n> ./configure and bazel build starting from a fresh environment on the master\n> branch?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33158?email_source=notifications&email_token=AADL6CKDVGZGDNNBNH3IVBLQODAVHA5CNFSM4I6YRFTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBA2BYQ#issuecomment-541171938>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AADL6CKLGYTZAUOJD3EM5OTQODAVHANCNFSM4I6YRFTA>\n> .\n>\n", "Cool, glad to hear it. We'll be deprecating Python 2.7 support in TensorFlow when Python 2.7 reaches EOL in January 2020, so you don't need to worry about extra debugging for Python 2. Thanks! I'll go ahead and close this issue for now.\r\n\r\nUnfortunately, I'm not aware of better resources for GPUs on GCP, aside from doing your own comparison of [the GPU prices](https://cloud.google.com/compute/gpus-pricing) or receiving GCP credits from some other source (Google sometimes runs promotions for this, but I'm not sure of any specific ones right now).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33158\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33158\">No</a>\n"]}, {"number": 33157, "title": "Enable quantized Div for toco", "body": "This addresses issue #32031 raised by @sunzhe09, \"Unimplemented: this graph contains an operator of type Div for which the quantized form is not yet implemented\". This was addressed in PR #26570 \"Added quantized division for uint8\" but quantize.cc was not modified to allow for TFLite conversion.", "comments": []}, {"number": 33156, "title": "issue with reuse embedding module", "body": "I was using the universal sentence encoder transformer module for some developments and had no issue before. But when I try to reuse the module today, I got an error, wondering if anyone had similar experience and solution with this?\r\n\r\nscript: embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\r\n\r\nerror: variable_scope module_20/ was unused but the corresponding name_scope was already taken.\r\n\r\nThanks.\r\n\r\n", "comments": ["What version of Tensorflow and Tensorflow Hub are you using? Also I am unable to reproduce this error wit Tensorflow 1.15rc3 and Tensorflow Hub 0.6. Please find the github gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/382e0579776b389e1ada66c73b84863a/copy-of-semantic-similarity-with-tf-hub-universal-encoder.ipynb). \r\n\r\nAlso I am closing the issue here. Please reference this issue and create a new issue in [Tensorflow/hub](https://github.com/tensorflow/hub/issues) as you will get an expedited response there. Thanks!"]}, {"number": 33155, "title": "Enable quantized Div for toco", "body": "This addresses issue #32031 raised by @sunzhe09, \"Unimplemented: this graph contains an operator of type Div for which the quantized form is not yet implemented\". This was addressed in PR #26570 \"Added quantized division for uint8\" but quantize.cc was not modified to allow for TFLite conversion.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33155) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it."]}, {"number": 33154, "title": " Attempt to convert a value (1.0) with an unsupported type (<class 'numpy.float32'>) to a Tensor.", "body": "Im getting this error in this step. Im trying to forecast sales similar to the lstm stock price prediction. I ensured that all the inputs i.e x_t_tf, y_t_tf, a and b are all tensors. Yet, I get this error. I am using tensorflow 2.0.0.\r\n\r\nhistory = model.fit(x_t_tf, y_t_tf, epochs=300, verbose=2, steps_per_epoch=BATCH_SIZE,validation_steps = 3,\r\n                        shuffle=False, validation_data=(a,\r\n                        b), callbacks=[es, mcp])\r\n\r\nPlease let me know how to resolve this. Im just using a jupyter notebook to run this. ", "comments": ["@Vidyapreethaam Can you provide a gist of this issue (preferably a colab notebook) inorder for me to reproduce this.  Its too vague for me to look into this issue now. Thanks!", "Im following steps similar to this article \r\nhttps://towardsdatascience.com/predicting-stock-price-with-lstm-13af86a74944 \r\nsimilar to this code\r\nhttps://github.com/DarkKnight1991/Stock-Price-Prediction/blob/master/stock_pred_main.py\r\n\r\nand in this part, the history= .fit() is creating the issue. So, in my code I ensured that all the inputs are tensors. yet Im getting this error. \r\n\r\nPlease look at the code above. \r\n", "Also, Im not using colab for this. Im doing it in a jupyter notebook", "\r\n[stockprice-master.zip](https://github.com/tensorflow/tensorflow/files/3704566/stockprice-master.zip)\r\nplease also check this notebook. If we run the .fit() step. It gives the same error.", "@Vidyapreethaam Please post this issue in [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) as github is only meant for issues related to bug, performance, feature requests, build/install and docs. Thanks!"]}, {"number": 33153, "title": "tflite_runtime standalone always results in \"ModuleNotFoundError: No module named '_interpreter_wrapper'\"", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): openSUSE Leap 15.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: tflite_runtime 1.14.0\r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?: pip3 (followed the instructions from https://www.tensorflow.org/lite/guide/python)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: NVIDIA 2080ti 11GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen trying to install just the TensorFlow Lite interpreter, I'm able to install everything with no errors and I can even import tflite_runtime, but when I put in \"from tflite_runtime.interpreter import Interpreter\", I always run into a ModuleNotFoundError\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI also installed Miniconda, installed python 3.7, pip3 installed the correct tflite_runtime for python 3.7, and ran into the exact same ModuleNotFoundError.\r\n", "comments": ["I got it to work when I had full tensorflow also installed, but then I did a full reinstall of openSUSE and ran into the same error. I was hoping to only install the Lite Interpreter instead of a full TF. ", "@wmol4 ,\r\nCan you please try `pip install tf-nightly.`\r\nYou may also need to change `tf.contrib.lite.Interpreter to tf.lite.Interpreter`\r\nKindly refer this [link](https://github.com/tensorflow/tensorflow/issues/26422) for more information.", "@oanush \r\nThe hope is to only need to install \"tflite_runtime\" as per this page: \r\nhttps://www.tensorflow.org/lite/guide/python#install_just_the_tensorflow_lite_interpreter\r\n\r\nThe issue is that I can only get the tflite_runtime library to work if I also have full tensorflow installed, which defeats the whole purpose. \r\n\r\nIt works when I am running both TF 2.0 and tflite_runtime 1.14. I run into the ModuleNotFoundError when I only install tflite_runtime 1.14. Any ideas how I can get tflite_runtime working without needing to install the full tensorflow as well? ", "Is anyone else getting this issue or is it just me??", "I'm trying to deploy AWS Lambda package containing tflite_runtime and getting the same error. Locally it is working correctly.", "Interesting. Full disclosure, I was trying it on Ubuntu and OpenSUSE 15.1 via Windows Subsystem for Linux. But after reading @Helmars message, I decided to try it on a Fedora laptop I have laying around, and it was able to install just fine. ", "It seems there is a workaround to copy _interpreter_wrapper.so file from tflite_runtime one directory out (to site-packages or current directory). This didn't help for AWS Lambda though, because Amazon Linux uses older version of glibc. I ended up creating minimal C program instead of using TF Lite Python package.", "Any updates on this? Running into this issue with a RPi3.\r\n\r\nEdit: also getting this on a fresh Ubuntu install on an old laptop I found. Is this just broken?", "Same problem in AWS Lambda. Any news about this?", "Hi, the interpreter_wrapper is migrated to pybind11 from SWIG, and the cause of the error might be different. Can you give more details &\r\n steps to reproduce?", "Facing the same problem using the wheel on Android. It tries to lazy-load the full TensorFlow module, which surely fails. I need to bundle only the tflite_runtime... The import is on line 30 in interpreter.py. Is there even other available working builds for ARMv7? It seems it is like that for half a year now. Either `ModuleNotFoundError: No module named '_interpreter_wrapper'` or `ModuleNotFoundError: No module named 'tensorflow'`\r\n\r\nCross-compiling it now is not an option for me, sadly. I am willing to cooperate as much as I can to fix this error.", "Hi hematogender, as I commented above the backend for _interpreter_wrapper has changed, and it might be a different bug. Can you file a separate bug with steps to reproduce?\r\n\r\nSeems you're trying to use python over Android rather than using it directly. If importing .aar works it would be great, but seems not so far :(\r\n\r\nCan you try with arm64 and see if the same error happens? Also, can you share why cross-compile is not an option? You can crossbuild whl from your machine, following https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package", "Same Error for AWS lambda\r\n\r\n```\r\nResponse:\r\n{\r\n  \"errorMessage\": \"Unable to import module 'lambda_function': No module named '_interpreter_wrapper'\",\r\n  \"errorType\": \"Runtime.ImportModuleError\"\r\n}\r\n```\r\n\r\nWhat i did:\r\n\r\n```\r\n## create docker container from python image ##\r\ndocker pull python:3.7-slim-buster\r\ndocker run -it python:3.7-slim-buster bash\r\ncd home\r\n\r\n## zip layer ##\r\nlayer=tensorflowLite2\r\nmkdir -p $layer/python/lib/python3.7/site-packages/\r\ncd $layer/python/lib/python3.7/site-packages/\r\npip install -t . https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_x86_64.whl\r\nrm -r *.dist-info\r\nrm -r __pycache__/\r\ncd ../../../../\r\napt-get update\r\napt-get install zip -y\r\nzip -r9 ../$layer.zip .\r\n\r\n```\r\n\r\nAny progress?", "I was running into the same issue `ModuleNotFoundError: No module named '_interpreter_wrapper'` on **Ubuntu 16.04 x86-64**. The problem was, as the complete error message writes `ImportError: /lib/x86_64-linux-gnu/libm.so.6: version 'GLIBC_2.27' not found`, that Ubuntu 16.04 is delivered with GLIBC 2.23. You can get your version by writing: `ldd --version`. I must also note, that I am using TFLite Runtime 2.1.0\r\n\r\nFor me the solution was to use Ubuntu 18.04, which is delivered with GLIBC 2.27.\r\n\r\nIt might worth considering to write a note for Linux users at page https://www.tensorflow.org/lite/guide/python that Ubuntu 16.04 is not supported.\r\n\r\nHope this helps", "> It seems there is a workaround to copy _interpreter_wrapper.so file from tflite_runtime one directory out (to site-packages or current directory). This didn't help for AWS Lambda though, because Amazon Linux uses older version of glibc. I ended up creating minimal C program instead of using TF Lite Python package.\r\n\r\nThis worked for me. It required me to remove \"tensorflow_wrap_interpreter_wrapper.pyo, tensorflow_wrap_interpreter_wrapper.py, _tensorflow_wrap_interpreter_wrapper.pyd out of tflite_runtime\" from the tflite_runtime folder and put them one directory up.", "> > It seems there is a workaround to copy _interpreter_wrapper.so file from tflite_runtime one directory out (to site-packages or current directory). This didn't help for AWS Lambda though, because Amazon Linux uses older version of glibc. I ended up creating minimal C program instead of using TF Lite Python package.\r\n> \r\n> This worked for me. It required me to remove \"tensorflow_wrap_interpreter_wrapper.pyo, tensorflow_wrap_interpreter_wrapper.py, _tensorflow_wrap_interpreter_wrapper.pyd out of tflite_runtime\" from the tflite_runtime folder and put them one directory up.\r\n\r\n\r\n\r\nHow did you do this.. I am getting same error..\r\nI created a virtual env and installed necessary packages (tflite and other).. then zipped the site-packages to a folder.. now in the zipped folder i placed the lambda_function.py.. when I am running this on lambda console.. getting the same error for tflite... other dependencies are imported smoothly..\r\nThanks in advance..", "virtualenv->Lib->site packages, find TFlite and remove the above-mentioned\nfiles up one directory. to be the same directory as the TFlite folder.\n\nOn Thu, Aug 13, 2020 at 5:30 AM Asheesh <notifications@github.com> wrote:\n\n> It seems there is a workaround to copy _interpreter_wrapper.so file from\n> tflite_runtime one directory out (to site-packages or current directory).\n> This didn't help for AWS Lambda though, because Amazon Linux uses older\n> version of glibc. I ended up creating minimal C program instead of using TF\n> Lite Python package.\n>\n> This worked for me. It required me to remove\n> \"tensorflow_wrap_interpreter_wrapper.pyo,\n> tensorflow_wrap_interpreter_wrapper.py,\n> _tensorflow_wrap_interpreter_wrapper.pyd out of tflite_runtime\" from the\n> tflite_runtime folder and put them one directory up.\n>\n> How did you do this.. I am getting same error..\n> I created a virtual env and installed necessary packages (tflite and\n> other).. then zipped the site-packages to a folder.. now in the zipped\n> folder i placed the lambda_function.py.. when I am running this on lambda\n> console.. getting the same error for tflite... other dependencies are\n> imported smoothly..\n> Thanks in advance..\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33153#issuecomment-673371191>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ALD7SX7VPTGCLBTOKSK3M2LSAOXC3ANCNFSM4I6WHYTA>\n> .\n>\n", "Hi, i had this similar issue when running my application on docker. I was using python:3.6.8 at that time. And by changing the python image to 3.6.11 solves my problem. Something that i notice is that there is a difference in gcc version that is being used by them (6.3 and 8.3 as i recall).", "> > > It seems there is a workaround to copy _interpreter_wrapper.so file from tflite_runtime one directory out (to site-packages or current directory). This didn't help for AWS Lambda though, because Amazon Linux uses older version of glibc. I ended up creating minimal C program instead of using TF Lite Python package.\r\n> > \r\n> > \r\n> > This worked for me. It required me to remove \"tensorflow_wrap_interpreter_wrapper.pyo, tensorflow_wrap_interpreter_wrapper.py, _tensorflow_wrap_interpreter_wrapper.pyd out of tflite_runtime\" from the tflite_runtime folder and put them one directory up.\r\n> \r\n> How did you do this.. I am getting same error..\r\n> I created a virtual env and installed necessary packages (tflite and other).. then zipped the site-packages to a folder.. now in the zipped folder i placed the lambda_function.py.. when I am running this on lambda console.. getting the same error for tflite... other dependencies are imported smoothly..\r\n> Thanks in advance..\r\n\r\nAfter copy files in tflite_runtime to the ../, there turn out to be:\r\n{\r\n  \"errorMessage\": \"Unable to import module 'handler': /lib64/libm.so.6: version `GLIBC_2.27' not found (required by /var/task/_interpreter_wrapper.so)\",\r\n  \"errorType\": \"Runtime.ImportModuleError\"\r\n}", "Having same issue... Any one found better solution for this ?\r\nLike @satya2343 mentioned. I fixed this issue by changing the base docker image from **3.7-slim-stretch** to **3.9-slim** (having higher version of GLIBC_2.27).", "Have same error \"Unable to import module 'handler': /lib64/libm.so.6: version `GLIBC_2.27' not found ..\" on AWS lambda. Building it locally on Ubunt18.04 works fine though.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33153\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33153\">No</a>\n", "@tensorflowbutler \r\nThis issue still exits with tf2.x\r\ntflite runtime needs GLIBC_2.27 which ubuntu16.04 does not come shipped with (it comes with GLIBC_2.23)\r\nIs there a hard requirement of tflite runtime interpreter for GLIB_2.27 ? Is there any workaround for ubuntu16.04, please let know.", "@Lucky94 did you find any solution for this. I am in same situation"]}, {"number": 33152, "title": "TPU has XLA compilation issue on TF 1.15rc3", "body": "**System information**\r\npython: `3.7.3`\r\ntensorflow version: `1.15.0rc3`\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3704020/tf_env.txt)\r\n\r\n**Describe the current behavior**\r\n\r\nError message I get is the following:\r\n\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nWARNING:tensorflow:From issue.py:19: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\r\n\r\nWARNING:tensorflow:From issue.py:30: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2019-10-08 11:14:04.056451: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2019-10-08 11:14:04.056480: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\r\n2019-10-08 11:14:04.056497: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cs-6000-devshell-vm-8cf51c20-59fd-4dc8-ad36-465b49377d09): /proc/driver/nvidia/version does not exist\r\n2019-10-08 11:14:04.056770: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-08 11:14:04.069196: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\r\n2019-10-08 11:14:04.070092: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560554da1f40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-10-08 11:14:04.070105: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:From issue.py:31: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\r\n\r\n2019-10-08 11:14:04.154903: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at xla_ops.cc:361 : Invalid argument: Detected unsupported operations when trying to compile graph cluster_3267298482081017018[] on XLA_CPU_JIT: Unique (No registered 'Unique' OpKernel for XLA_CPU_JIT devices compatible with node {{node sgd_momentum/update_embedding/embeddings/Unique}}\r\n\t.  Registered:  device='CPU'; T in [DT_BOOL]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_BOOL]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT64]\r\n  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n){{node sgd_momentum/update_embedding/embeddings/Unique}}\r\n\tThis error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=\"tf_xla_auto_jit=2\" which will attempt to use xla to compile as much of the graph as the compiler is able to.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph cluster_3267298482081017018[] on XLA_CPU_JIT: Unique (No registered 'Unique' OpKernel for XLA_CPU_JIT devices compatible with node {{node sgd_momentum/update_embedding/embeddings/Unique}}\r\n\t.  Registered:  device='CPU'; T in [DT_BOOL]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_BOOL]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT64]\r\n  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n){{node sgd_momentum/update_embedding/embeddings/Unique}}\r\n\tThis error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=\"tf_xla_auto_jit=2\" which will attempt to use xla to compile as much of the graph as the compiler is able to.\r\n\t [[cluster]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"issue.py\", line 32, in <module>\r\n    sess.run(loss)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 956, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph cluster_3267298482081017018[] on XLA_CPU_JIT: Unique (No registered 'Unique' OpKernel for XLA_CPU_JIT devices compatible with node node sgd_momentum/update_embedding/embeddings/Unique (defined at /usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) \r\n\t.  Registered:  device='CPU'; T in [DT_BOOL]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_BOOL]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT64]\r\n  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n)node sgd_momentum/update_embedding/embeddings/Unique (defined at /usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) \r\n\tThis error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=\"tf_xla_auto_jit=2\" which will attempt to use xla to compile as much of the graph as the compiler is able to.\r\n\t [[cluster]]\r\n```\r\n\r\n**Describe the expected behavior**\r\nIf I used tf.train.GradientDescentOptimizer instead it would work, I expect using momentum optimizer to work as well.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.python.compiler.xla import xla\r\nfrom tensorflow.keras.layers import Embedding\r\n\r\nLEARNING_RATE = 0.1\r\nVOCAB_SIZE = 100\r\nHIDDEN_SIZE = 200\r\nMAX_SEQ_LEN = 150\r\n\r\ndef my_model(features, labels):\r\n    emb = Embedding(VOCAB_SIZE, HIDDEN_SIZE)(features)\r\n    logits = tf.keras.layers.Dense(units=VOCAB_SIZE)(emb)\r\n    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n        labels=labels,\r\n        logits=logits\r\n    )\r\n    loss = tf.reduce_mean(crossent)\r\n    optimizer = tf.train.MomentumOptimizer(\r\n        learning_rate=LEARNING_RATE,\r\n        momentum=0.9,\r\n        name='sgd_momentum')\r\n    train_op = optimizer.minimize(loss)\r\n    return loss\r\n\r\nsrc = tf.constant(0, dtype=tf.int32, shape=[MAX_SEQ_LEN])\r\ntgt = tf.constant(1, dtype=tf.int32, shape=[MAX_SEQ_LEN])\r\n(loss,) = xla.compile(my_model, [src, tgt])\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(loss)\r\nsess.close()\r\n```\r\n\r\n**Other info / logs**\r\n\r\nIssue occurs when using keras `Embedding` layer alongside an optimizer that is not sgd.\r\nThe issue can be fixed by modifying keras implementation by changing:\r\n```\r\ntf.gather(weights, inputs)\r\n```\r\nto\r\n```\r\ntf.gather(tf.multiply(weights,1), inputs)\r\n```\r\n\r\nThe issue is in the weights being an IndexedSlice which causes any non-sgd optimizer to use the `unique` operation which is unsupported by XLA. However, if you do `tf.multiply(weights,1)` it makes that expression the IndexedSlice and weights itself doesn't become an IndexedSlice but rather a tensor so the optimizer implementation in tensorflow doesn't call `unique`.\r\n", "comments": ["I Could reproduce the issue on colab with TF 1.15.0rc3. Please see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/28d349cf4f5b7b9a60254f0e1d25d747/untitled187.ipynb). Thanks!", "An intermediate solution is to densify the gradients yourself. (I took the liberty of switching your example to use the keras SGD rather than the tf.train one.)\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.python.compiler.xla import xla\r\nfrom tensorflow.keras.layers import Embedding\r\n\r\nLEARNING_RATE = 0.1\r\nVOCAB_SIZE = 100\r\nHIDDEN_SIZE = 200\r\nMAX_SEQ_LEN = 150\r\n\r\ndef my_model(features, labels):\r\n    emb_layer = Embedding(VOCAB_SIZE, HIDDEN_SIZE)\r\n    dense_layer = tf.keras.layers.Dense(units=VOCAB_SIZE)\r\n    emb = emb_layer(features)\r\n    logits = dense_layer(emb)\r\n\r\n    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n        labels=labels,\r\n        logits=logits\r\n    )\r\n    loss = tf.reduce_mean(crossent)\r\n    optimizer = tf.keras.optimizers.SGD(\r\n        learning_rate=LEARNING_RATE,\r\n        momentum=0.9,\r\n        name='sgd_momentum')\r\n    weights = emb_layer.trainable_weights + dense_layer.trainable_weights\r\n    grads = optimizer.get_gradients(loss, weights)\r\n\r\n    # This will \"densify\" the gradients\r\n    grads = [tf.convert_to_tensor(i) for i in grads]\r\n    optimizer.apply_gradients(zip(grads, weights))\r\n    return loss\r\n\r\nsrc = tf.constant(0, dtype=tf.int32, shape=[MAX_SEQ_LEN])\r\ntgt = tf.constant(1, dtype=tf.int32, shape=[MAX_SEQ_LEN])\r\n(loss,) = xla.compile(my_model, [src, tgt])\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(loss)\r\nsess.close()\r\n```\r\n\r\n@sanjoy How practical is it to support this more generally in XLA? Does the inherent dynamism of sparsity preclude it?", ">\u00a0@sanjoy How practical is it to support this more generally in XLA? Does the inherent dynamism of sparsity preclude it?\r\n\r\nIf you're asking about TF's `Unique` op, and if the output shape of `Unique` depends on the input _data_ then supporting it in XLA (as it is today) is not possible.  However, @yunxing might have something to say here.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33152\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33152\">No</a>\n"]}, {"number": 33151, "title": "Inference On-Cloud for Object Detection", "body": "Hello, \r\nI built an android app for object detection using tensorflow lite and on-Device inference. I want to make inference on-Cloud and saw that the tensorflow serving client is still unavaliable for android. Do you know a way to use cloud for inference at my object detection mobile app? Thanks an advance.\r\n", "comments": ["@giannisfourfouris Follow this [article](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) and also this [one](https://cloud.google.com/solutions/creating-object-detection-application-tensorflow) and it should help you in soving your problem. "]}, {"number": 33150, "title": "TF2.0: Translation model: Error when restoring the saved model: Unresolved object in checkpoint (root).optimizer.iter: attributes", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs Mojave 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below):' 2.0.0-beta1'\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to restore the checkpoints and predict on different sentences NMT Attention Model. While restoring the checkpoints and predicting, I am getting gibberish results with warning below:\r\n\r\n`Unresolved object in checkpoint (root).optimizer.iter: attributes {\r\n  name: \"VARIABLE_VALUE\"\r\n  full_name: \"Adam/iter\"\r\n  checkpoint_key: \"optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE\"\r\n}`\r\n\r\nBelow is the additional warnings that I am getting and the results:\r\n\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW1008 09:57:52.766877 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.iter\r\nW1008 09:57:52.767037 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta_1\r\nW1008 09:57:52.767082 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta_2\r\nW1008 09:57:52.767120 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.decay\r\nW1008 09:57:52.767155 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer.learning_rate\r\nW1008 09:57:52.767194 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.embedding.embeddings\r\nW1008 09:57:52.767228 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.gru.state_spec\r\nW1008 09:57:52.767262 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.fc.kernel\r\nW1008 09:57:52.767296 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.fc.bias\r\nW1008 09:57:52.767329 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.embedding.embeddings\r\nW1008 09:57:52.767364 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.state_spec\r\nW1008 09:57:52.767396 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.gru.cell.kernel\r\nW1008 09:57:52.767429 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.gru.cell.recurrent_kernel\r\nW1008 09:57:52.767461 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.gru.cell.bias\r\nW1008 09:57:52.767493 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W1.kernel\r\nW1008 09:57:52.767526 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W1.bias\r\nW1008 09:57:52.767558 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W2.kernel\r\nW1008 09:57:52.767590 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W2.bias\r\nW1008 09:57:52.767623 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.V.kernel\r\nW1008 09:57:52.767657 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.V.bias\r\nW1008 09:57:52.767688 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.cell.kernel\r\nW1008 09:57:52.767721 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.cell.recurrent_kernel\r\nW1008 09:57:52.767755 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.cell.bias\r\nW1008 09:57:52.767786 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.embedding.embeddings\r\nW1008 09:57:52.767818 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.fc.kernel\r\nW1008 09:57:52.767851 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.fc.bias\r\nW1008 09:57:52.767884 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.embedding.embeddings\r\nW1008 09:57:52.767915 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.kernel\r\nW1008 09:57:52.767949 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.recurrent_kernel\r\nW1008 09:57:52.767981 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.bias\r\nW1008 09:57:52.768013 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W1.kernel\r\nW1008 09:57:52.768044 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W1.bias\r\nW1008 09:57:52.768077 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W2.kernel\r\nW1008 09:57:52.768109 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W2.bias\r\nW1008 09:57:52.768143 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.V.kernel\r\nW1008 09:57:52.768175 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.V.bias\r\nW1008 09:57:52.768207 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.gru.cell.kernel\r\nW1008 09:57:52.768239 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.gru.cell.recurrent_kernel\r\nW1008 09:57:52.768271 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.gru.cell.bias\r\nW1008 09:57:52.768303 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.embedding.embeddings\r\nW1008 09:57:52.768335 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.fc.kernel\r\nW1008 09:57:52.768367 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.fc.bias\r\nW1008 09:57:52.768399 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.embedding.embeddings\r\nW1008 09:57:52.768431 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.kernel\r\nW1008 09:57:52.768463 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.recurrent_kernel\r\nW1008 09:57:52.768495 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.gru.cell.bias\r\nW1008 09:57:52.768527 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W1.kernel\r\nW1008 09:57:52.768559 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W1.bias\r\nW1008 09:57:52.768591 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W2.kernel\r\nW1008 09:57:52.768623 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.W2.bias\r\nW1008 09:57:52.768654 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.V.kernel\r\nW1008 09:57:52.768686 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).decoder.attention.V.bias\r\nW1008 09:57:52.768718 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.gru.cell.kernel\r\nW1008 09:57:52.768750 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.gru.cell.recurrent_kernel\r\nW1008 09:57:52.768782 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.gru.cell.bias\r\nW1008 09:57:52.768816 4594230720 util.py:252] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\r\nInput: <start> hola <end>\r\nPredicted translation: ? attack now relax hello \r\n```\r\nThe warning at the very end says:\r\n\r\n'A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used...' what does it mean?\r\n**Describe the expected behavior**\r\nCheckpoints should be restored properly\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\ncheckpoint_dir = './training_checkpoints'\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\r\n                                 encoder=encoder,\r\n                                 decoder=decoder)\r\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please take a look at the similar [issue](https://github.com/tensorflow/tensorflow/issues/27937) and let me know if it helps!!. Also try to test it with Tensorflow 2.0 and let me know if the issue still persists.", "@gowthamkpr \r\nI had upgraded to TF2.0 and tested it, but still has the same error. \r\nFrom [https://github.com/tensorflow/tensorflow/issues/27937](url), I tried following the steps:\r\n- Saved the checkpoint\r\n- Re-initialized the encoder and decoder \r\n- Reloaded/restored the checkpoints to predict\r\n- The results are gibberish\r\n\r\nIs it possible that the tokenizer is assigning random ID s to tokens when reloading the checkpoint?", "Can you please share a github gist of this issue (preferably) in colab so that I can expedite the process. Thanks!", "@nlpds Were you able to solve the issue? or else please provide a gist of minimum reproducible code. Thanks!", "Closing this issue as it has been inactive for more than 15 days. Please add additional comments and we can open this issue again. Thanks!", "Hopefully the simplest example that can reproduce the issue\r\n```python\r\n# Tested with TF 2.0.0, Linux Ubuntu 18.04, Python 3.7.3, TF installed from binary\r\nimport tensorflow as tf\r\n\r\n# create model and optimizer and checkpoint\r\nmodel = tf.keras.models.Sequential([tf.keras.layers.Dense(5)])\r\nopt = tf.keras.optimizers.Adam(0.1)\r\ncheckpoint_dir = 'ckpts'\r\nckpt = tf.train.Checkpoint(opt=opt, model=model)\r\nmanager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=3)\r\n\r\n# train with one example\r\nexample_x = tf.constant([[1.]])\r\nexample_y = tf.constant([[1.,2.,3.,4.,5.]])\r\nmodel.compile(loss=\"mean_squared_error\", optimizer=opt)\r\nmodel.fit(example_x, example_y, epochs=1)\r\n\r\nsave_path = manager.save()\r\nprint(\"Saved checkpoint: {}\".format(save_path))\r\n\r\n# ========== restart from scratch but restore from checkpoint\r\nmodel = tf.keras.models.Sequential([tf.keras.layers.Dense(5)])\r\nopt = tf.keras.optimizers.Adam(0.1)\r\n\r\nckpt = tf.train.Checkpoint(opt=opt, model=model)\r\nmanager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=3)\r\nprint('restoring...')\r\nstatus = ckpt.restore(manager.latest_checkpoint)\r\n# assert_consumed() fails with:\r\n    # AssertionError: Unresolved object in checkpoint (root).opt.iter: attributes {\r\n    #   name: \"VARIABLE_VALUE\"\r\n    #   full_name: \"Adam/iter\"\r\n    #   checkpoint_key: \"opt/iter/.ATTRIBUTES/VARIABLE_VALUE\"\r\nstatus.assert_consumed()\r\n```", "I got the same error message and warning message in tensorflow-gpu 2.0\r\n```\r\ncheckpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\r\ncheckpoint.save(checkpoint_prefix)\r\n```\r\nThe optimizer is Adam\r\n```\r\n  lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\r\n    initial_learning_rate=0.001,\r\n    decay_steps=1000000,\r\n    decay_rate=math.exp(-1),\r\n    staircase=False)\r\n  optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\r\n```\r\nThe model is a subclassed tf.keras.Model\r\n", "This is my take:\r\nThe issue is that the new model object does not have the variable `iter` which is created when the optimiser is called on training samples. If you add these lines after you created the new model:\r\n```\r\nmodel.compile(loss=\"mean_squared_error\", optimizer=opt)\r\nmodel.fit(example_x, example_y, epochs=1)\r\n```\r\nThen the issue is resolved.\r\nOr by using `status.assert_existing_objects_matched()`, you can avoid the error as well, as it only ensures variables in the new model gets restored.", "Same issue here, unable to properly load checkpoints without calling a tf.function function before loading the checkpoint.", "I have a similar issue, and couldn't resolve it.\r\nError log:\r\n\r\n```\r\nEpoch 00004: saving model to checkpoints/yolov3_train_4.tf\r\n9/9 [==============================] - 112s 12s/step - loss: 337.0419 - yolo_output_0_loss: 1.6753 - yolo_output_1_loss: 27.6452 - yolo_output_2_loss: 296.4273- val_loss: 1033934.1250 - val_yolo_output_0_loss: 1.5464 - val_yolo_output_1_loss: 17083.2285 - val_yolo_output_2_loss: 1016838.5000\r\nEpoch 00004: early stopping\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer-8\r\nW0424 06:25:47.220819 140280049981248 util.py:144] Unresolved object in checkpoint: (root).layer-8\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer-9\r\nW0424 06:25:47.221170 140280049981248 util.py:144] Unresolved object in checkpoint: (root).layer-9\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer-10\r\nW0424 06:25:47.221260 140280049981248 util.py:144] Unresolved object in checkpoint: (root).layer-10\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer-11\r\nW0424 06:25:47.221323 140280049981248 util.py:144] Unresolved object in checkpoint: (root).layer-11\r\nWARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\nW0424 06:25:47.221471 140280049981248 util.py:152] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\n```", "I have the same issue, can this please be reopened?", "I have also the same issue, could you please reopen this issue?", "The error even shows up in the guide\r\nhttps://www.tensorflow.org/tutorials/keras/save_and_load\r\n\r\nTrain on 1000 samples\r\nEpoch 1/5\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\r\nWARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter", "Same problem, please reopen this issue", "I also encountered this issue. However, most of my warnings were for training parameters like optimizers state. When looking the issue up, I came across an answer that said this occurs because I was loading weights from a checkpoint and then only using it for inference/prediction and not for training. Therefore the training parameters (like optimizer state) in the checkpoint were being unused and hence the warnings.\r\n\r\nIt would be great if someone could let me know how accurate an explanation this is. (Have attached the link below if someone wants to check the answer out)\r\n\r\nhttps://stackoverflow.com/a/59582698", "Finally found out a cause and solution for this problem.\r\nBasically, it's because ``Adam`` is initializing its attributes like ``beta_1``, ``beta_2``, etc, as a Python float object, which are not tracked by tensorflow. By converting them into ``tf.Variable``, we can let them tracked by tensorflow and thus we can load weights without issues.\r\n\r\nhttps://gist.github.com/yoshihikoueno/4ff0694339f88d579bb3d9b07e609122", "I also found that many people won't even notice this issue because they are not using ``assert_consumed`` method, which will postpone the checks. The conversion into ``tf.Variable`` will take place once the optimizer actually gets called, then the weights (``beta_1`` for example) will be loaded silently.", "Also for those who seeing errors or warnings for the model components like below:\r\n```\r\nW1008 09:57:52.767493 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W1.kernel\r\nW1008 09:57:52.767526 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W1.bias\r\nW1008 09:57:52.767558 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W2.kernel\r\nW1008 09:57:52.767590 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.W2.bias\r\nW1008 09:57:52.767623 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.V.kernel\r\nW1008 09:57:52.767657 4594230720 util.py:244] Unresolved object in checkpoint: (root).decoder.attention.V.bias\r\nW1008 09:57:52.767688 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.cell.kernel\r\nW1008 09:57:52.767721 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.cell.recurrent_kernel\r\nW1008 09:57:52.767755 4594230720 util.py:244] Unresolved object in checkpoint: (root).encoder.gru.cell.bias\r\nW1008 09:57:52.767786 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.embedding.embeddings\r\nW1008 09:57:52.767818 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.fc.kernel\r\nW1008 09:57:52.767851 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.fc.bias\r\nW1008 09:57:52.767884 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.embedding.embeddings\r\nW1008 09:57:52.767915 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.kernel\r\nW1008 09:57:52.767949 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.recurrent_kernel\r\nW1008 09:57:52.767981 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.gru.cell.bias\r\nW1008 09:57:52.768013 4594230720 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).decoder.attention.W1.kernel\r\n```\r\nmaybe that's because your model is not (partially) built. In my case, I had some components left unbuilt within my ``build`` method and those were appearing in warnings like above.", "> This is my take:\r\n> The issue is that the new model object does not have the variable `iter` which is created when the optimiser is called on training samples. If you add these lines after you created the new model:\r\n> \r\n> ```\r\n> model.compile(loss=\"mean_squared_error\", optimizer=opt)\r\n> model.fit(example_x, example_y, epochs=1)\r\n> ```\r\n> \r\n> Then the issue is resolved.\r\n> Or by using `status.assert_existing_objects_matched()`, you can avoid the error as well, as it only ensures variables in the new model gets restored.\r\n\r\nThis method resolves the issue, but I think TF should resolve such issue internally... I believe this issue should be reopened", "> I have a similar issue, and couldn't resolve it.\r\n> Error log:\r\n> \r\n> ```\r\n> Epoch 00004: saving model to checkpoints/yolov3_train_4.tf\r\n> 9/9 [==============================] - 112s 12s/step - loss: 337.0419 - yolo_output_0_loss: 1.6753 - yolo_output_1_loss: 27.6452 - yolo_output_2_loss: 296.4273- val_loss: 1033934.1250 - val_yolo_output_0_loss: 1.5464 - val_yolo_output_1_loss: 17083.2285 - val_yolo_output_2_loss: 1016838.5000\r\n> Epoch 00004: early stopping\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-8\r\n> W0424 06:25:47.220819 140280049981248 util.py:144] Unresolved object in checkpoint: (root).layer-8\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-9\r\n> W0424 06:25:47.221170 140280049981248 util.py:144] Unresolved object in checkpoint: (root).layer-9\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-10\r\n> W0424 06:25:47.221260 140280049981248 util.py:144] Unresolved object in checkpoint: (root).layer-10\r\n> WARNING:tensorflow:Unresolved object in checkpoint: (root).layer-11\r\n> W0424 06:25:47.221323 140280049981248 util.py:144] Unresolved object in checkpoint: (root).layer-11\r\n> WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\n> W0424 06:25:47.221471 140280049981248 util.py:152] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\n> ```\r\n\r\nanyone could solve it?\r\ni'm using efficientDet", "Hi,\r\nI am trying to use centernet for one my projects. Current I was just testing it out on the normal RBC dataset, but I am getting checkpoint related error when using the CentreNet Model. I am attaching my config and log file\r\n[Config.txt](https://github.com/tensorflow/tensorflow/files/5125041/Config.txt)\r\n[Error_CenterNet.txt](https://github.com/tensorflow/tensorflow/files/5125045/Error_CenterNet.txt)\r\n\r\n", "Same problem in TF 2.x GPU. I was shocked to see my model exporting script failing just because I removed unnecessary stuffs from the training script. Instantiating a tf.data.Dataset which is not used somehow fixes the problem.", "hey guys, i met the same problem, and i found it is because what i fed is not the correct format(no pics in test), hope this may help", "@xiankgx Can you copy paste the config you are using and point to which checkpoint you are trying to use it with ?", "same probleme here when trying to import bert converted checkpoint to tf2.x ", "For custom tensorflow models, I found that I needed to feed in input into them before loading the weights/checkpoints. That's because some model weight shapes do not get properly configured before any input is fed into the network. ", "\r\n@nlpds @thierryherrmann @cockroachzl @cfrancesco @saikrishnadas @JulianFerry @pgagliar @kirahman2 @KacperKubara @arvindas @YurongYou @yoshihikoueno @y9luiz @pretidav @Wingtail \r\n\r\nGuys, this issue was closed without any solution. There are couple of workarounds (use predict method before loading, use tf.Variable in Adam constructor) but they are not a solution. Some of you asked to reopen the issue, but tensorflowers keep silence. So, I have opened related issue which demonstrates the bug with checkpoints in colab. If you are still interested in fixing it, feel free to boost attention of maintainers to [issue 52346](https://github.com/tensorflow/tensorflow/issues/52346). You can comment or set emoji. Thank you!", "I was able to solve it by deleting the last checkpoint, it was probably corrupted. \r\nSo in my case I deleted checkpoint `ckpt-46.index`  and `ckpt-46.data-00000-of-00001`\r\nThen the model started training from\r\n`ckpt-45.index`  and `ckpt-45.data-00000-of-00001`\r\nwith no issues"]}, {"number": 33149, "title": "AutoGraph error with XLA, MKL-DNN, Eager Execution, and custom keras layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary, using conda\r\n- TensorFlow version (use command below): unknown 1.14.0, with intel MKL\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: no gpu\r\n- GPU model and memory: no gpu\r\n\r\npython build version: ('default', 'Aug 13 2019 20:35:49')\r\npython compiler version: GCC 7.3.0\r\npython implementation: CPython\r\nos kernel version: #1 SMP Mon Jul 29 17:46:05 UTC 2019\r\nos release version: 3.10.0-957.27.2.el7.x86_64\r\nos platform: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core\r\nlinux distribution: ('CentOS Linux', '7.6.1810', 'Core')\r\nlinux os distribution: ('centos', '7.6.1810', 'Core')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\n== check pips ===================================================\r\nnumpy                1.16.4   \r\nprotobuf             3.9.2    \r\ntensorflow           1.14.0   \r\ntensorflow-estimator 1.14.0   \r\ngast                    0.3.2\r\n\r\n**Describe the current behavior**\r\nAutoGraph not working properly with custom Keras Layer and eager execution.\r\n```\r\n2019-10-08 17:22:48.398589: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2019-10-08 17:22:48.404575: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2399845000 Hz\r\n2019-10-08 17:22:48.404752: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56240ed9a1e0 executing computations on platform Host. Devices:\r\n2019-10-08 17:22:48.404799: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-10-08 17:22:48.404944: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nERROR:tensorflow:Error converting <bound method SegmentedMean.call of <ml_utils.SegmentedMean object at 0x7f9d9e8e6610>>\r\nTraceback (most recent call last):\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 524, in to_graph\r\n    return conversion.convert(entity, program_ctx)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 306, in convert\r\n    entity, program_ctx, free_nonglobal_var_names)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 229, in _convert_with_cache\r\n    entity, program_ctx)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 433, in convert_entity_to_ast\r\n    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 624, in convert_func_to_ast\r\n    node = node_to_graph(node, context)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 667, in node_to_graph\r\n    node = converter.apply_(node, context, return_statements)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/core/converter.py\", line 380, in apply_\r\n    node = converter_module.transform(node, context)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/converters/return_statements.py\", line 412, in transform\r\n    node = transformer.visit(node)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/core/converter.py\", line 317, in visit\r\n    return super(Base, self).visit(node)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 480, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/ast.py\", line 262, in visit\r\n    return visitor(node)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/converters/return_statements.py\", line 363, in visit_FunctionDef\r\n    converted_body = self._visit_statement_block(node, node.body)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/converters/return_statements.py\", line 287, in _visit_statement_block\r\n    nodes = self.visit_block(nodes, after_visit=self._postprocess_statement)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 371, in visit_block\r\n    replacement = self.visit(node)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/core/converter.py\", line 317, in visit\r\n    return super(Base, self).visit(node)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 480, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/ast.py\", line 262, in visit\r\n    return visitor(node)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/converters/return_statements.py\", line 237, in visit_Return\r\n    retval=retval)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/templates.py\", line 260, in replace\r\n    replacements[k] = _convert_to_ast(replacements[k])\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/templates.py\", line 222, in _convert_to_ast\r\n    return gast.Name(id=n, ctx=None, annotation=None)\r\n  File \"/home/matej.racinsky/anaconda3/envs/mil_tensorflow/lib/python3.7/site-packages/gast/gast.py\", line 19, in create_node\r\n    format(Name, nbparam, len(Fields))\r\nAssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method SegmentedMean.call of <ml_utils.SegmentedMean object at 0x7f9d9e8e6610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SegmentedMean.call of <ml_utils.SegmentedMean object at 0x7f9d9e8e6610>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\n```\r\n\r\n**Describe the expected behavior**\r\nError should not be raised, code should work.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom sklearn.preprocessing import MultiLabelBinarizer\r\n\r\nclass SegmentedMean(tf.keras.layers.Layer):\r\n    def __init__(self, *args, **kwargs):\r\n        super(SegmentedMean, self).__init__(*args, **kwargs)\r\n\r\n    def call(self, inputs, **kwargs):\r\n        features, segments = inputs\r\n        return tf.math.segment_mean(features, segments)\r\n\r\n\r\ndf = pd.DataFrame({'list': [\r\n    [[1, 2, 3, 4], [2, 3, 6]],\r\n    [[1, 2], [1, 5, 8]],\r\n    [[6, 7, 8], [2, 4, 10], [1, 6]],\r\n    [[3, 4, 6, 8], [1, 8], [2]],\r\n    [[3, 6, 8]],\r\n],\r\n    'label': [0, 0, 1, 1, 1]})\r\ndf['list_len'] = df['list'].apply(len)\r\nbatch_size = 8\r\n# list of all unique numbers used\r\nnums_used = list(set([i for j in df['list'].apply(lambda k: [i for j in k for i in j]) for i in j]))\r\nlist_enc = MultiLabelBinarizer().fit([[i] for i in nums_used])\r\n\r\n\r\ndef create_batch():\r\n    batch_ub = df.sample(batch_size // 2)  # upper bound, will trim so the batch size is correct after expansion\r\n    batch_end = (batch_ub['list_len'].cumsum() <= batch_size)[::-1].idxmax()\r\n    batch = batch_ub.loc[:batch_end].copy()\r\n    batch['id'] = np.arange(0, len(batch))\r\n    feat_bags = batch['list'].apply(list_enc.transform)\r\n    feats = np.concatenate(feat_bags.values)\r\n    # so I know which data to group together during segmentation\r\n    segments = batch['id'].repeat(batch['list_len']).values\r\n    labels = batch['label'].values.astype(np.int32)\r\n    return (feats, segments), labels\r\n\r\n\r\nsettings = {'k': 40, 'iterations': 10}\r\nfeats_len = len(nums_used)\r\ninputs = tf.keras.Input(shape=(feats_len,), name='features')\r\nsegments = tf.keras.Input(shape=(), name='segments', dtype=tf.int32)\r\nx = tf.keras.layers.Dense(settings['k'], activation=tf.nn.relu)(inputs)\r\nx = tf.keras.layers.Dense(settings['k'])(x)\r\n\r\nx = SegmentedMean()((x, segments))\r\nx = tf.keras.layers.Dense(settings['k'], activation=tf.nn.relu)(x)\r\nlogits = tf.keras.layers.Dense(2, name='output_logits')(x)\r\nprobs = tf.keras.layers.Softmax()(logits)\r\nmodel = tf.keras.Model(inputs=(inputs, segments), outputs=(logits, probs), name='mil_model')\r\noptimizer = tf.keras.optimizers.Adam()\r\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nfor step in range(settings['iterations']):\r\n    x_train, y_train = create_batch()\r\n    with tf.GradientTape() as tape:\r\n        logits, probs = model(x_train)\r\n        loss_value = loss(y_train, logits)\r\n    grads = tape.gradient(loss_value, model.trainable_weights)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n```\r\n\r\n**Other info / logs**\r\nLogs are included above.", "comments": ["@racinmat, Looks like code is incomplete, please provide the complete code to replicate the issue. And also provide the `gast` version. Thanks!", "updated, runnable code with some dummy data and gast version added", "@racinmat, can you downgrade the `gast` version to 0.2.2 and try. Thanks!", "With gast version 0.2.2 it's working. So I guess it's not a bug, but I think it should be stated somewhere or dependencies should be managed in a way gast 0.2.2 is installed. I install all dependencies using conda, assuming it will figure out correct versions of packages.\r\n", "@racinmat, Pip install pull all the compatible dependencies of Tensorflow. The latest version of gast has caused this problem. \r\nGlad it worked. \r\nClosing the issue. Please feel free to reopen if issue still persists. Thanks! "]}, {"number": 33148, "title": "Masking LSTM: OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM", "body": "### System information\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.2 LTS\r\n- **TensorFlow installed from (source or binary)**: Binary, pip\r\n- **TensorFlow version (use command below)**: v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- **Python version**: Python 3.7.3\r\n- **CUDA/cuDNN version**: CUDA=10.0, CUDNN=7.6.2.24-1\r\n- **GPU model and memory**: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\n\r\n### Describe the problem\r\nIt seems there is an issue with the CuDNN LSTM implementation when using a `tf.keras.layers.Masking` layer. \r\n\r\n```\r\nbatch_size = 256\r\nnum_tsteps = 144\r\nnum_features = 130\r\nnum_units = 88\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),\r\n    tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),\r\n    tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=True, stateful=False),\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),\r\n    tf.keras.layers.Activation('sigmoid'),\r\n])\r\n```\r\nSimilar to #33069 I receive this error during training and I have strictly right-padded data (I am doing trimming and right-padding manually). However, in contrast to this issue, I confirmed that I do not have any inputs containing only zeroes via the following snippet:\r\n\r\n```\r\nfor i, e in enumerate(ds_train):\r\n    res = []\r\n    f, l = [x.numpy() for x in e]\r\n    for j in range(f.shape[0]):\r\n        if not (f[j] == 0.0).all():\r\n            res.append(1)\r\n        else:\r\n            res.append(0)\r\n    fin = [res[0]]\r\n    for e in res[1:]:\r\n        if e != fin[-1]:\r\n            fin.append(e)\r\n    print(\"i {}: {}\".format(i, fin))\r\n\r\n# Result:\r\ni 0: [1, 0]\r\ni 1: [1, 0]\r\ni 2: [1, 0]\r\ni 3: [1, 0]\r\ni 4: [1]\r\ni 5: [1, 0]\r\n...\r\n```\r\nIf I remove the Masking-layer, the error does not occur. I confirmed this by running a complete epoch (2324 batches), however, the training is probably pretty pointless when including the padded data.\r\n\r\nIs there any other pitfall that I am missing that could cause this issue?\r\n\r\n### Source code / logs\r\nPython output:\r\n```\r\nEpoch 1/1000\r\nWARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \r\n\r\n\r\nCancelledErrorTraceback (most recent call last)\r\n<ipython-input-7-1c503c2dd55c> in <module>\r\n----> 1 m.fit(train=True)\r\n\r\n/ws/tf/vol_local/_model_lstm.py in fit(self, train, verbose)\r\n    315             ]\r\n    316             self.model.fit(ds_train, epochs=num_epochs, verbose=verbose, shuffle=False,\r\n--> 317                                 validation_data=ds_val, validation_steps=None, callbacks=cbs)\r\n    318             #self.model.save(sess_hdf5_path)\r\n    319             self.model.save_weights(self.sess_h5_path.as_posix())\r\n\r\n/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\n/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    322                 mode=ModeKeys.TRAIN,\r\n    323                 training_context=training_context,\r\n--> 324                 total_epochs=epochs)\r\n    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    326 \r\n\r\n/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\n/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\n/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    518         # Lifting succeeded, so variables are initialized and we can run the\r\n    519         # stateless function.\r\n--> 520         return self._stateless_fn(*args, **kwds)\r\n    521     else:\r\n    522       canon_args, canon_kwds = \\\r\n\r\n/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1824 \r\n   1825   @property\r\n\r\n/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n   1139          if isinstance(t, (ops.Tensor,\r\n   1140                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1141         self.captured_inputs)\r\n   1142 \r\n   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1222     if executing_eagerly:\r\n   1223       flat_outputs = forward_function.call(\r\n-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n   1225     else:\r\n   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n\r\n/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    509               inputs=args,\r\n    510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 511               ctx=ctx)\r\n    512         else:\r\n    513           outputs = execute.execute_with_cancellation(\r\n\r\n/ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n/ws/miniconda3/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nCancelledError:  [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]\r\n\t [[loss/activation_loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_106/has_invalid_dims/concat/_28]] [Op:__inference_distributed_function_172102]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n```\r\n\r\nCommand line log:\r\n```\r\n2019-10-08 14:38:27.367875: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_169668_171093' and '__inference___backward_cudnn_lstm_with_fallback_169668_171093_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_172102' both implement 'lstm_dce676f4-acdd-4bb5-88d9-e8dd57573aba' but their signatures do not match.\r\n2019-10-08 14:38:27.536666: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-08 14:38:39.982582: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-08 14:38:41.215567: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n2019-10-08 14:38:41.215616: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n\t [[{{node cond_64/then/_0/CudnnRNNV3}}]]\r\n2019-10-08 14:38:41.215638: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]\r\n\t [[loss/activation_loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_106/has_invalid_dims/concat/_28]]\r\n2019-10-08 14:38:41.215693: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]\r\n```", "comments": ["@mimxrt ,\r\nThanks for reporting the issue, can you please provide simple and standalone code to reproduce the issue? ", "I am trying to create a standalone example, however, I am facing some other errors that prevent me from finishing it. I created the following example which can produce the error using one .tfrecord file of my dataset. In the example, I generate a .tfrecord file from random data (for reproducibility) but TensorFlow fails to parse the file afterwards (actually parsing works but I still get the error):\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\nassert tf.executing_eagerly()\r\n\r\nbatch_size = 256\r\nnum_tsteps = 144\r\nnum_features = 130\r\nnum_units = 88\r\n\r\n#n_files = 3320\r\nn_files = 10\r\nnum_epochs = 1000\r\n\r\nseq_len_max_trunc = batch_size * num_tsteps\r\nflen = 3728\r\n\r\n### Create TFRecord\r\n\r\nX = np.random.rand(flen + 1, num_features)\r\nn_label0 = int((flen + 1) * 0.2)\r\nY = np.concatenate((\r\n    np.zeros((n_label0, 1)), # label 0\r\n    np.ones((flen - n_label0 + 1, 1)), # label 1\r\n), axis=0)\r\nds_out = tf.data.Dataset.from_tensor_slices((X, Y))\r\nds_ser = ds_out.map(lambda *x: \r\n   tf.reshape(tf.py_function(lambda *v: \r\n       tf.train.Example(features=tf.train.Features(feature={\r\n           \"features\": tf.train.Feature(float_list=tf.train.FloatList(value=v[0].numpy())),\r\n           \"label\": tf.train.Feature(float_list=tf.train.FloatList(value=v[1].numpy())),\r\n       })).SerializeToString(), x, tf.string\r\n   ), ()), num_parallel_calls=tf.data.experimental.AUTOTUNE\r\n)\r\nfor i, e in enumerate(ds_ser):\r\n    ex = tf.train.Example.FromString(e.numpy())\r\n    print(\"{}: {}\".format(i, ex).replace(\" \", \"\").replace(\"\\n\", \" \")[:100])\r\nwriter = tf.data.experimental.TFRecordWriter(\"temp.tfrecord\")\r\nwriter.write(ds_ser)\r\n\r\n### Read TFRecord and train\r\n\r\nfiles = [\"temp.tfrecord\"] * n_files\r\n#files = [\"data/myfile.tfrecord\"] * n_files\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),\r\n    tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),\r\n    tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=True, stateful=False),\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),\r\n    tf.keras.layers.Activation('sigmoid'),\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n\r\ndef _prep_ds_file(file):\r\n    _ds = tf.data.TFRecordDataset(file)\r\n    for i, e in enumerate(_ds):\r\n        ex = tf.train.Example.FromString(e.numpy())\r\n        print(\"{}: {}\".format(i, ex).replace(\" \", \"\").replace(\"\\n\", \" \")[:100])\r\n    print(\"\\n\\n\\n\\n\\n\\n\\n\")\r\n    _ds = _ds.map(lambda x: tf.io.parse_single_example(x, {\r\n        \"features\": tf.io.FixedLenFeature([132], tf.float32),\r\n        \"label\": tf.io.FixedLenFeature([1], tf.float32),\r\n    }), num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n\r\n    _ds = _ds.flat_map(lambda v: tf.data.Dataset.from_tensors((v[\"features\"][2:], v[\"label\"])))\r\n\r\n    _trunc = min(seq_len_max_trunc, ((flen + 1) // num_tsteps) * num_tsteps)\r\n    _ds = _ds.take(_trunc)\r\n\r\n    _c_pad = (batch_size - ((flen + 1) // num_tsteps)) * num_tsteps\r\n    if _c_pad >= 0:\r\n        assert _c_pad + ((flen + 1) // num_tsteps * num_tsteps) == seq_len_max_trunc\r\n        _ds_pad = tf.data.Dataset.from_tensors((\r\n            tf.constant(0.0, shape=[num_features,]),\r\n            tf.constant(0.0, shape=[1,])))\r\n        _ds_pad = _ds_pad.repeat(_c_pad)\r\n        _ds = _ds.concatenate(_ds_pad) # pad to correct size\r\n\r\n    _ds = _ds.window(size=num_tsteps, shift=None, stride=1, drop_remainder=True)\r\n    _ds = _ds.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(num_tsteps), y.batch(num_tsteps))))\r\n\r\n    _ds = _ds.batch(batch_size, drop_remainder=True)\r\n    \r\n    return _ds\r\n\r\n\r\nds_fs = tf.data.Dataset.list_files(files, shuffle=True, seed=1)\r\nfs_train = ds_fs.take(int(n_files * 0.7))\r\nfs_val = ds_fs.skip(int(n_files * 0.7)).take(int(n_files * 0.1))\r\n\r\nds_train = [_prep_ds_file(f) for f in fs_train.take(1)][0]\r\nfor f in fs_train.skip(1):\r\n    ds_train = ds_train.concatenate(_prep_ds_file(f))\r\nds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\nds_val = [_prep_ds_file(f) for f in fs_val.take(1)][0]\r\nfor f in fs_val.skip(1):\r\n    ds_val = ds_val.concatenate(_prep_ds_file(f))\r\nds_val = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\ncbs = [\r\n    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\r\n]\r\nmodel.fit(ds_train, epochs=num_epochs, verbose=1, shuffle=False,\r\n          validation_data=ds_val, validation_steps=None, callbacks=cbs)\r\n```\r\n\r\nFor me this code produces the following error:\r\n```\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  {{function_node __inference_Dataset_map_<lambda>_32068}} Key: features.  Can't parse serialized Example.\r\n\t [[{{node ParseSingleExample/ParseSingleExample}}]]\r\n\t [[IteratorGetNext]]\r\n\t [[IteratorGetNext/_2]]\r\n  (1) Invalid argument:  {{function_node __inference_Dataset_map_<lambda>_32068}} Key: features.  Can't parse serialized Example.\r\n\t [[{{node ParseSingleExample/ParseSingleExample}}]]\r\n\t [[IteratorGetNext]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_distributed_function_64891]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n```\r\nThis is weird because it does iterate through the full file and prints the values on the screen (see the dataset iteration both when generating and when parsing the .tfrecord file in `_prep_ds_file()`). I can see that elements 0 to 3728 are printed. When switching to the file `myfile.tfrecord` (commented out) I can also see the elements until 3728 (same amount) and finally receive the error mentioned in the issue above.\r\n\r\nDo you have any idea what causes this error? If not I could provile the `myfile.tfrecord` file but of course that would not be ideal for reproducibility.", "Issue replicating with TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/11dcf7d43fb3a2ba2c1d97e22ff145d2/33148.ipynb) of colab.Thanks!", "I found my mistake and updated the code accordingly. Please update the gist to represent the corrected and less verbose version below. The error happens in TF-2.0 and _does not_ happen in TF-1.0 (1.14.0). Please also try to removing the `Masking` layer to confirm the issue only exists with masking.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# NOTE: Comment the block below for testing with TF-1.0\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n# NOTE: Uncomment the block below for testing with TF-1.0\r\n# tf.compat.v1.enable_eager_execution()\r\n# config = tf.compat.v1.ConfigProto()\r\n# config.gpu_options.allow_growth = True\r\n# sess = tf.compat.v1.Session(config=config)\r\n# tf.compat.v1.keras.backend.set_session(sess)\r\n\r\nassert tf.executing_eagerly()\r\n\r\nbatch_size = 256\r\nnum_tsteps = 144\r\nnum_features = 130\r\nnum_units = 88\r\n\r\n#n_files = 3320\r\nn_files = 10\r\nnum_epochs = 1000\r\n\r\nseq_len_max_trunc = batch_size * num_tsteps\r\nflen = 3728\r\n\r\n### Create TFRecord\r\n\r\nX = np.random.rand(flen + 1, num_features + 2)\r\nn_label0 = int((flen + 1) * 0.2)\r\nY = np.concatenate((\r\n    np.zeros((n_label0, 1)), # label 0\r\n    np.ones((flen - n_label0 + 1, 1)), # label 1\r\n), axis=0)\r\nds_out = tf.data.Dataset.from_tensor_slices((X, Y))\r\nds_ser = ds_out.map(lambda *x: \r\n   tf.reshape(tf.py_function(lambda *v: \r\n       tf.train.Example(features=tf.train.Features(feature={\r\n           \"features\": tf.train.Feature(float_list=tf.train.FloatList(value=v[0].numpy())),\r\n           \"label\": tf.train.Feature(float_list=tf.train.FloatList(value=v[1].numpy())),\r\n       })).SerializeToString(), x, tf.string\r\n   ), ()), num_parallel_calls=tf.data.experimental.AUTOTUNE\r\n)\r\nwriter = tf.data.experimental.TFRecordWriter(\"temp.tfrecord\")\r\nwriter.write(ds_ser)\r\n\r\n### Read TFRecord and train\r\n\r\nfiles = [\"temp.tfrecord\"] * n_files\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),\r\n    tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),\r\n    tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=True, stateful=False),\r\n    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),\r\n    tf.keras.layers.Activation('sigmoid'),\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n\r\ndef _prep_ds_file(file):\r\n    _ds = tf.data.TFRecordDataset(file)\r\n    _ds = _ds.map(lambda x: tf.io.parse_single_example(x, {\r\n        \"features\": tf.io.FixedLenFeature([132], tf.float32),\r\n        \"label\": tf.io.FixedLenFeature([1], tf.float32),\r\n    }), num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n        \r\n    _ds = _ds.flat_map(lambda v: tf.data.Dataset.from_tensors((v[\"features\"][2:], v[\"label\"])))\r\n\r\n    _trunc = min(seq_len_max_trunc, ((flen + 1) // num_tsteps) * num_tsteps)\r\n    _ds = _ds.take(_trunc)\r\n\r\n    _c_pad = (batch_size - ((flen + 1) // num_tsteps)) * num_tsteps\r\n    if _c_pad >= 0:\r\n        assert _c_pad + ((flen + 1) // num_tsteps * num_tsteps) == seq_len_max_trunc\r\n        _ds_pad = tf.data.Dataset.from_tensors((\r\n            tf.constant(0.0, shape=[num_features,]),\r\n            tf.constant(0.0, shape=[1,])))\r\n        _ds_pad = _ds_pad.repeat(_c_pad)\r\n        _ds = _ds.concatenate(_ds_pad) # pad to correct size\r\n\r\n    _ds = _ds.window(size=num_tsteps, shift=None, stride=1, drop_remainder=True)\r\n    _ds = _ds.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(num_tsteps), y.batch(num_tsteps))))\r\n\r\n    _ds = _ds.batch(batch_size, drop_remainder=True)\r\n    \r\n    return _ds\r\n\r\n\r\nds_fs = tf.data.Dataset.list_files(files, shuffle=True, seed=1)\r\nfs_train = ds_fs.take(int(n_files * 0.7))\r\nfs_val = ds_fs.skip(int(n_files * 0.7)).take(int(n_files * 0.1))\r\n\r\nds_train = [_prep_ds_file(f) for f in fs_train.take(1)][0]\r\nfor f in fs_train.skip(1):\r\n    ds_train = ds_train.concatenate(_prep_ds_file(f))\r\nds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\ncbs = [\r\n    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\r\n]\r\nmodel.fit(ds_train, epochs=num_epochs, verbose=1, shuffle=False,\r\n          validation_data=None, validation_steps=None, callbacks=cbs)\r\n```", "@mimxrt I could reproduce the issue with `tf-nightly-gpu`. However, if I use only cpu then there is no error (with and without masking) as shown in the original post. I think root-cause may be related to `tf.config.experimental.set_memory_growth(gpu, True)`. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/3a768611de0a28330d047f12e120f931/untitled553.ipynb) is the gist for your reference. Thanks! ", "Thank you for your reply. I'm not sure what you are saying though: did you try removing `tf.config.experimental.set_memory_growth(gpu, True)` at all? I tried and for me the same error happens. For me, the only difference is that TF occupies all available GPU memory.\r\n\r\nCould you please try again if the error also happens without memory growth? If so, I guess the issue could be in the GPU LSTM implementation (cuDNN)?", "@mimxrt I mentioned two things.\r\n1. I could reproduce the issue with `tf-nightly-gpu`\r\n2. Just tried to point some root-cause. I ran your code as it is in a cpu and I don't see any error. so I guess root-cause may be related to `tf.config.experimental.set_memory_growth(gpu, True)`. I may be wrong.\r\n\r\nThanks!\r\n", "I see, thank you for the clarification. So in summary we can say that this issue does not happen on the CPU and that it is related to the GPU implementation. As I tried running it without `tf.config.experimental.set_memory_growth(gpu, True)` and the error still occurs, it should probably be save to rule that out as the cause/issue.", "same problem here. did you find any solution?", "No sorry, still waiting for any help myself.", "I changed my model from:\r\n```python\r\n        self.model = Sequential([\r\n            Embedding(len(self.item_map), self.embed_dim, input_length = X.shape[1],mask_zeros=True),\r\n            LSTM(self.lstm_out),\r\n            Dense(len(self.item_map)-1),\r\n        ])\r\n```\r\nto:\r\n```python\r\n        self.model = Sequential([\r\n            Embedding(len(self.item_map), self.embed_dim, input_length = X.shape[1]),\r\n            Masking(mask_value=0),\r\n            LSTM(self.lstm_out),\r\n            Dense(len(self.item_map)-1),\r\n        ])\r\n```\r\nAnd solved my isssue\r\n\r\nI know @mimxrt's code has the same model and I dont know why it works for me,\r\nbut im adding this for anyone else comes here with the issue and maybe it can help with debugging", "@ynsgnr if you are running on CPU then it works properly, the problem is when you run it on GPU. By the way, you do not use the Timedistirbuted layer in your code since this problem shows up when you use Timedistributed with LSTM. ", "@jvishnuvardhan, I tried your suggestion but it still produces the same error. I am new to GitHub , how to tell this problem to someone from TensorFlow? do they see our conversation ? \r\n", "In my experience and example the issue stems from the Masking + LSTM combination, not from the `TimeDistributed` layer. Please try the example in https://github.com/tensorflow/tensorflow/issues/33148#issuecomment-540472342 without it, i.e.\r\n\r\n```\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),\r\n    tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),\r\n    tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=False, stateful=False),\r\n    tf.keras.layers.Dense(1),\r\n    tf.keras.layers.Activation('sigmoid'),\r\n])\r\n```\r\n(change `return_sequences` to `False` and remove `TimeDistributed`).\r\n\r\nYou can also try to remove the Masking layer in the example above and the error will go away.", "but I have this model:\r\n`        model=tf.keras.Sequential()\r\n        embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=True,mask_zero=True)\r\n        model.add(embeding_layer)\r\n        model.add(layers.LSTM(50))\r\n        model.add(layers.Dropout(0.5))\r\n        model.add(layers.Dense(3,activation='softmax'))\r\n        opt=tf.keras.optimizers.RMSprop(learning_rate=0.001)\r\n        model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\r\n        self.model=model\r\n`\r\nwhich works fine and with no error. As you can see I used masking and LSTM at the same time. so in my experience the problem stem from TimeDistributed and masking.", "Interesting. Can you provide a complete example like mine so I can try your code as well?", "the above example is a simple classifier, you can make a random dataset and feed the model with it, so you can see that it will work with no problem.\r\nthis is where the model does not work and produce an error while training :\r\n``        model=tf.keras.Sequential()\r\n        embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=False,mask_zero=True)\r\n        model.add(TimeDistributed(embeding_layer))\r\n        model.add(TimeDistributed(tf.keras.layers.LSTM(50)))\r\n        model.add(tf.keras.layers.Bidirectional(costumized_lstm.Costumized_LSTM(50)))\r\n        # model.add(tf.keras.layers.Bidirectional(costumized_lstm.Costumized_LSTM(100)))\r\n        model.add(layers.Dense(3,activation='softmax'))\r\n        opt=tf.keras.optimizers.Adam(learning_rate=0.001)\r\n        model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\r\n``\r\n\r\nin this example, if I set mask_zero=true at the embedding layer then, it crushes at the begging or sometimes at the end of the epoc when evaluating the validation loss. and this is the error message :\r\n`\r\n`C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\Scripts\\python.exe C:/Users/jalil/PycharmProjects/untitled1/main_file.py\r\n2019-11-14 14:11:36.983144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-11-14 14:11:45.638679: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-11-14 14:11:46.216495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038\r\npciBusID: 0000:01:00.0\r\n2019-11-14 14:11:46.216676: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-14 14:11:46.217282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-14 14:11:50.885396: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-11-14 14:11:51.214275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038\r\npciBusID: 0000:01:00.0\r\n2019-11-14 14:11:51.214484: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-14 14:11:51.218182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-14 14:11:51.905201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-14 14:11:51.905307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165] 0\r\n2019-11-14 14:11:51.905366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0: N\r\n2019-11-14 14:11:51.906228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4757 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\nWARNING:tensorflow:From C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3983: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nTrain on 35000 samples, validate on 6447 samples\r\nEpoch 1/1000\r\n2019-11-14 14:12:33.178251: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_671418_672877' and '__inference___backward_cudnn_lstm_with_fallback_671418_672877_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_675292' both implement 'lstm_81cdaa4a-fa6f-4675-abbb-02fb4cd0189b' but their signatures do not match.\r\n2019-11-14 14:12:33.544669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2019-11-14 14:12:34.397677: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n\r\n32/35000 [..............................] - ETA: 2:03:422019-11-14 14:12:35.151804: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n2019-11-14 14:12:35.152137: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n[[{{node cond_64/then/_0/CudnnRNNV3}}]]\r\n2019-11-14 14:12:35.152541: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: {{function_node __forward_cudnn_lstm_with_fallback_672876_specialized_for_sequential_time_distributed_1_lstm_StatefulPartitionedCall_at___inference_distributed_function_675292}} {{function_node __forward_cudnn_lstm_with_fallback_672876_specialized_for_sequential_time_distributed_1_lstm_StatefulPartitionedCall_at___inference_distributed_function_675292}} CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n[[{{node cond_64/then/_0/CudnnRNNV3}}]]\r\n[[sequential/time_distributed_1/lstm/StatefulPartitionedCall]]\r\n\r\n32/35000 [..............................] - ETA: 2:25:41Traceback (most recent call last):\r\nFile \"C:/Users/jalil/PycharmProjects/untitled1/main_file.py\", line 102, in\r\nmain_model_instance.train_model(train_batch_data,train_batch_labels,test_batch_data,test_batch_labels)\r\nFile \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\main_model.py\", line 103, in train_model\r\nhistory = self.model.fit(x=np.array(train_batch_data),y=np.array(train_batch_labels),validation_data=(np.array(test_batch_data),np.array(test_batch_labels)),epochs=1000,callbacks=[tensorboard_callback])\r\nFile \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 734, in fit\r\nuse_multiprocessing=use_multiprocessing)\r\nFile \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 324, in fit\r\ntotal_epochs=epochs)\r\nFile \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 123, in run_one_epoch\r\nbatch_outs = execution_function(iterator)\r\nFile \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 86, in execution_function\r\ndistributed_function(input_fn))\r\nFile \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 439, in call\r\nreturn self._stateless_fn(*args, *kwds)\r\nFile \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1822, in call\r\nreturn graph_function._filtered_call(args, kwargs) # pylint: disable=protected-access\r\nFile \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1141, in _filtered_call\r\nself.captured_inputs)\r\nFile \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1224, in _call_flat\r\nctx, args, cancellation_manager=cancellation_manager)\r\nFile \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 511, in call\r\nctx=ctx)\r\nFile \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\nsix.raise_from(core._status_to_exception(e.code, message), None)\r\nFile \"\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnknownError: [Derived] CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void)&padding_fill)'\r\n[[{{node cond_64/then/_0/CudnnRNNV3}}]]\r\n[[sequential/time_distributed_1/lstm/StatefulPartitionedCall]] [Op:__inference_distributed_function_675292]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function -> distributed_function\r\n`", "I have a similar issue, just posted a question on SO: \r\n[CUDNN_STATUS_BAD_PARAM when trying to perform inference on a LSTM Seq2Seq with masked inputs](https://stackoverflow.com/questions/58895694/cudnn-status-bad-param-when-trying-to-perform-inference-on-a-lstm-seq2seq-with-m)", "Sorry for the very late reply, I will take a close look this week. This issue is kind of tricky since the error is raised from cudnn kernel, which contains less information. The troubleshooting for root cause might take me sometime.", "@qlzh727 in case it helps, for me the problem goes away when I disable eager mode.", "@houtoms, the error is raised from cudnn kernel and it only says there is a bad param. Could you provide more details about which param is bad? I am not familiar with the cudnn kernel detail at all.", "Yes, I can repro the error. Turns out the cudnnSetRNNDataDescriptor gives the BAD PARAM error. From the log, I can see you want to process the input data as 256 (batch size) x 144 (time steps) x 130 (unit size). But, when using masks, I observe that some entire sequences are masked out. For example, there could be only 25 meaningful sequences (at least having one time step) out of the 256 sequences in one batch. In this case, it seems the cuDNN requires the batch size to be 25. @mimxrt Can you first confirm the scenario I mentioned is what you want to do? ", "Thank you for your reply. The reason for processing 256 x 144 x 130 data is that I make the assumption that my maximum sequence (in real data) has approximately `256 * 144 = 36864` time steps (each time step has 130 values/features -> unit size). The shortest sequence has actually 3728 time steps. Hence, the input dataset in my example is generated using this shortest length and during processing the input is zero-padded to the longest sequence in `_prep_ds_file()`. During training, the `Masking` layer should then allow ignoring the zero-valued batch elements. Did that answer your question?\r\n\r\nNow, I understand that you are saying that if any of my input batches does have at least one batch element that is completely filled with zeros (i.e. 144 x 130 zeros), cuDNN requires a different batch size for that batch. From my understanding [of the docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM#used_in_the_tutorials) the CuDNN kernel is used if\r\n\r\n>  Inputs are not masked or strictly right padded.\r\n\r\nAnd that is what I am doing, am I not? I use masking but made sure the data is strictly right padded. Some (or in this example code all) of the batches do have zero padding in the end that is bigger than one batch element (144 time steps) and therefore the entire batch element should be ignored.\r\n\r\nIf this is the wrong way of doing this, then what is the right way? Is it possible to change the batch size mid-training (i.e. per batch)? Otherwise, how can I train sequences of different length? Batch size 1?\r\n", "Yes, your understanding is correct. And cuDNN doesn't support zero samples in batch for now.\r\n\r\nI am not sure if it is possible/how to change the batch size during training using tf.dataset (@qlzh727 some tf.dataset experts?). Batch size 1 could work, but it might significantly affect the performance. \r\n\r\nOr you could try to change the way you split your sequence into the batch, like using seq_len/batch_size as the current 'num_tsteps' rather than the fixed 144. But still you need to make sure the minimum seq_len >= batch_size. ", "@houtoms, Thanks for providing the context.\r\n\r\nFrom high level API's perspective, I would expect the kernel to just return zeros for any of the sequence that is fully masked, rather than asking user to remove those values from the batch. It will be quite complicate to ask user to handle this on the python side.\r\n\r\n@houtoms, will it be complicated to add this support (fully masked sequence) in the cudnn kernel?", "Yes, I agree with you. I am contacting the cuDNN about this issue. Will update when I get any feedback.", "Got some feedback from cuDNN and it sounds not complicated to do that. Already filed a request to them. For now, users still have to make sure they don't have such fully masked sequences in batch.", "Thanks for the update.", "> \u6211\u5c06\u6a21\u578b\u4ece\u4ee5\u4e0b\u4f4d\u7f6e\u66f4\u6539\uff1a\r\n> \r\n> ```python\r\n>         \u81ea\u6211 .model =\u987a\u5e8f\uff08[\r\n>             \u5d4c\u5165\uff08len\uff08self .item_map\uff09\uff0cself .embed_dim\uff0cinput_length  = X.shape [ 1 ]\uff0cmask_zeros = True\uff09\uff0c\r\n>             LSTM\uff08self .lstm_out\uff09\uff0c\r\n>             \u5bc6\u96c6\uff08len\u4e2a\uff08\u81ea .item_map\uff09- 1\uff09\r\n>         ]\uff09\r\n> ```\r\n> \r\n> \u81f3\uff1a\r\n> \r\n> ```python\r\n>         \u81ea\u6211 .model =\u987a\u5e8f\uff08[\r\n>             \u5d4c\u5165\uff08len\uff08self .item_map\uff09\uff0cself .embed_dim\uff0cinput_length  = X.shape [ 1 ]\uff09\uff0c\r\n>             \u906e\u7f69\uff08mask_value = 0\uff09\uff0c\r\n>             LSTM\uff08self .lstm_out\uff09\uff0c\r\n>             \u5bc6\u96c6\uff08len\u4e2a\uff08\u81ea .item_map\uff09- 1\uff09\r\n>         ]\uff09\r\n> ```\r\n> \r\n> \u89e3\u51b3\u4e86\u6211\u7684\u95ee\u9898\r\n> \r\n> \u6211\u77e5\u9053@mimxrt\u7684\u4ee3\u7801\u5177\u6709\u76f8\u540c\u7684\u6a21\u578b\uff0c\u6211\u4e0d\u77e5\u9053\u4e3a\u4ec0\u4e48\u5b83\u5bf9\u6211\u6709\u7528\uff0c\r\n> \u4f46\u662f\u6211\u5728\u8fd9\u91cc\u4e3a\u5176\u4ed6\u4eba\u6dfb\u52a0\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u4e5f\u8bb8\u5b83\u53ef\u4ee5\u5e2e\u52a9\u8c03\u8bd5\r\n\r\n\r\n\r\n> I changed my model from:\r\n> \r\n> ```python\r\n>         self.model = Sequential([\r\n>             Embedding(len(self.item_map), self.embed_dim, input_length = X.shape[1],mask_zeros=True),\r\n>             LSTM(self.lstm_out),\r\n>             Dense(len(self.item_map)-1),\r\n>         ])\r\n> ```\r\n> \r\n> to:\r\n> \r\n> ```python\r\n>         self.model = Sequential([\r\n>             Embedding(len(self.item_map), self.embed_dim, input_length = X.shape[1]),\r\n>             Masking(mask_value=0),\r\n>             LSTM(self.lstm_out),\r\n>             Dense(len(self.item_map)-1),\r\n>         ])\r\n> ```\r\n> \r\n> And solved my isssue\r\n> \r\n> I know @mimxrt's code has the same model and I dont know why it works for me,\r\n> but im adding this for anyone else comes here with the issue and maybe it can help with debugging\r\n\r\nI think embedding(mask_zero=true) create this problem,there are two way I find to slove it \r\n1\"mask_zero=False\",but it changes code process\r\n2 your way  \r\nthanks a lot.", "Thank you @qlzh727 and @houtoms, very much looking forward to the new cuDNN feature. In the meantime I will try to re-arrange my input dimensions.", "@houtoms and @qlzh727 thank you for the help. I am trying to implement a text classification model where the shape of the input is (samples, sentences, words, word_embeding) and I used TimeDistributed to implement it. having a fully masked sequence in this kind of model is inevitable. therefore it would be great if cuDNN supports fully masked sequence.", "as @michal-au said, I deactivate eager mode and remove callbacks from model.fit and now the model is working and training with no error! @houtoms is it possible to be an eager mode issue, not the cudnn issue?", "I can reproduce that `mask_zeros=True` is causing the crash. Doesn't matter if eager is on or off, with or without callbacks.\r\n\r\nLSTM on a masked sequence is extremely common in NLP models, so this is a major bug in terms of impact.", "Thanks for the feedback.\r\n\r\nYes, I think no support of the fully masked sequences in cuDNN is the major issue. For now the cuDNN requires that \"Each element in seqLengthArray must be greater than 0 but less than or equal to maxSeqLength\" (https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnSetRNNDataDescriptor).", "@houtoms so why when I deactivate eager mode it works with no error?", "@jalilasadi , I repro the error using mimxrt's python script posted above on Oct 10. The script works fine in both eager or graph mode if Masking layer is removed. Also, the script breaks in both modes if Masking layer is added.\r\n", "I was recently dealing with the same problem. Do you have any new information regarding this issue? ", "@jalilasadi, when eager mode is disabled, the code went to a different path that doesn't use fused cudnn kernel, hence no error will be raised.", "@qlzh727 thank you for your answer. I do not know what fused cudnn is, I am new to TensorFlow .does it make a difference in the result of the model that we implement or it just affects the computational performance?", "THe fused cudnn lstm is faster to run on GPU, and should have the same math result as the standard implementation.", "ping @houtoms for a status update from Nvidia side.", "Sure. Let me check with the CUDNN team and will update soon.", "> Yes, your understanding is correct. And cuDNN doesn't support zero samples in batch for now.\r\n> \r\n> I am not sure if it is possible/how to change the batch size during training using tf.dataset (@qlzh727 some tf.dataset experts?). Batch size 1 could work, but it might significantly affect the performance.\r\n> \r\n> Or you could try to change the way you split your sequence into the batch, like using seq_len/batch_size as the current 'num_tsteps' rather than the fixed 144. But still you need to make sure the minimum seq_len >= batch_size.\r\n\r\n@houtoms As it seems the implementation takes more time than expected I wanted to continue and try your suggestion of using a dynamic number of timesteps (instead of the fixed 144). Unfortunately I get an error when doing this in TF 2.1.0 (calling `model.fit()` with an input of `<PrefetchDataset shapes: ((128, None, 128), (128, None, 1)), types: (tf.float32, tf.float32)>`:\r\n\r\n`\r\nInvalidArgumentError:  ValueError: Attempt to convert a value (<BatchDataset shapes: ((128, None, 128), (128, None, 1)), types: (tf.float32, tf.float32)>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>) to a Tensor.`\r\n\r\nCould it be that I misunderstood your suggestion (note the `None` in the input dimensions)? All ideas are much appreciated.", "@houtoms, I assume this should be fixed already with the latest CuDNN kernel?", "@qlzh727 I still get the same error with CuDNN 7.6.5. did you mean this version of CuDNN? because it is the last version of it.  this is my code \r\n\r\n``` python\r\ndef BGRU_BGRU(self,metrics_file_path):\r\n        self.metrics_file_path = metrics_file_path\r\n        # tf.compat.v1.disable_eager_execution()\r\n\r\n        model=tf.keras.Sequential()\r\n        # embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[self.word_embeding_matrix],trainable=True,mask_zero=True)\r\n        embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,mask_zero=True)\r\n        model.add(TimeDistributed(embeding_layer))\r\n        model.add(TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(50))))\r\n        # model.add(TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(50,dropout=0.2,recurrent_dropout=0.2))))\r\n        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(50)))\r\n        # model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,dropout=0.2,recurrent_dropout=0.2)))\r\n        # model.add(tf.keras.layers.Dense(30,activation='sigmoid'))\r\n        model.add(layers.Dense(self.config.number_of_classess,activation='softmax'))\r\n        lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(self.config.lr, decay_steps=self.config.decay_steps, decay_rate=1,staircase=False)\r\n        opt = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\r\n        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy', 'mse'])\r\n        self.model = model\r\n        return\r\n```\r\n\r\nand this is the error that running this code produce: \r\n\r\n```python\r\nTrain on 33171 samples, validate on 8293 samples\r\nEpoch 1/8\r\n2020-02-29 03:04:13.742695: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-02-29 03:04:14.303999: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-02-29 03:04:15.018554: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1430): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n2020-02-29 03:04:15.018854: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1430): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n\t [[{{node cond_31/then/_0/CudnnRNNV3}}]]\r\n2020-02-29 03:04:15.019228: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: {{function_node __forward_cudnn_gru_with_fallback_681666_specialized_for_sequential_time_distributed_1_bidirectional_backward_gru_StatefulPartitionedCall_at___inference_distributed_function_687655}} {{function_node __forward_cudnn_gru_with_fallback_681666_specialized_for_sequential_time_distributed_1_bidirectional_backward_gru_StatefulPartitionedCall_at___inference_distributed_function_687655}} CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1430): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n\t [[{{node cond_31/then/_0/CudnnRNNV3}}]]\r\n\t [[sequential/time_distributed_1/bidirectional/backward_gru/StatefulPartitionedCall]]\r\n\t [[Reshape_10/_28]]\r\n2020-02-29 03:04:15.019884: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: {{function_node __forward_cudnn_gru_with_fallback_681666_specialized_for_sequential_time_distributed_1_bidirectional_backward_gru_StatefulPartitionedCall_at___inference_distributed_function_687655}} {{function_node __forward_cudnn_gru_with_fallback_681666_specialized_for_sequential_time_distributed_1_bidirectional_backward_gru_StatefulPartitionedCall_at___inference_distributed_function_687655}} CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1430): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n\t [[{{node cond_31/then/_0/CudnnRNNV3}}]]\r\n\t [[sequential/time_distributed_1/bidirectional/backward_gru/StatefulPartitionedCall]]\r\n2020-02-29 03:04:15.023940: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1430): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n2020-02-29 03:04:15.024206: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1430): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n\t [[{{node cond_31/then/_0/CudnnRNNV3}}]]\r\n\r\n  300/33171 [..............................] - ETA: 26:472020-02-29 03:04:19.483701: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1430): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n2020-02-29 03:04:19.484099: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1430): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n\t [[{{node cond/then/_0/CudnnRNNV3}}]]\r\n2020-02-29 03:04:19.484494: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: {{function_node __inference_cudnn_gru_with_fallback_689679_specialized_for_sequential_time_distributed_1_bidirectional_backward_gru_StatefulPartitionedCall_at___inference_distributed_function_693569}} {{function_node __inference_cudnn_gru_with_fallback_689679_specialized_for_sequential_time_distributed_1_bidirectional_backward_gru_StatefulPartitionedCall_at___inference_distributed_function_693569}} CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1430): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n\t [[{{node cond/then/_0/CudnnRNNV3}}]]\r\n\t [[sequential/time_distributed_1/bidirectional/backward_gru/StatefulPartitionedCall]]\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\jalil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\contextlib.py\", line 130, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 2803, in variable_creator_scope\r\n    yield\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 397, in fit\r\n    prefix='val_')\r\n  File \"C:\\Users\\jalil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\contextlib.py\", line 130, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 771, in on_epoch\r\n    self.callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\", line 302, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\coding_paian_nameh\\model\\classification_models.py\", line 783, in on_epoch_end\r\n    y_test_pred = self.model.predict(self.X_test)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1013, in predict\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 498, in predict\r\n    workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 475, in _model_iteration\r\n    total_epochs=1)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 638, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"C:\\Users\\jalil\\PycharmProjects\\my_projects\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnknownError:  [_Derived_]  CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1430): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\r\n\t [[{{node cond/then/_0/CudnnRNNV3}}]]\r\n\t [[sequential/time_distributed_1/bidirectional/backward_gru/StatefulPartitionedCall]] [Op:__inference_distributed_function_693569]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function -> distributed_function\r\n```\r\nwhen I disable the eager mode or set mask_zero=False the code works with no error ! ", "@houtoms, do u know whether the fix has reach the public release? If that's the case, please share a script/colab that includes version and the working model. Thanks.", "@qlzh727 Not yet. We might need to wait for the next major release for the fix.", "> \r\n> \r\n> @qlzh727 Not yet. We might need to wait for the next major release for the fix.\r\n\r\nDo you have an estimate time for that?", "import tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nDissable eager execution and everything is running fine without the fused rnn kernel. Thx for the help guys :) ", "I have a work around that seems to work:  force TF to use the non CuDNN implementation by selecting a sigmoid activation instead of TANH\r\n\r\n`layers.LSTM(...,activation='sigmoid')`\r\n\r\nOutputs\r\n\r\nWARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\n\r\nThis forces TF to use a generic GPU kernel in place of CuDNN.  It's slower but a slower implementation is a lot faster than not working at all ;p", "If you really want that as a workaround, you can find the requirements for the cuDNN implementation [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM?hl=en#used-in-the-notebooks_1). \r\n\r\n> The requirements to use the cuDNN implementation are:\r\n> \r\n> 1. activation == tanh\r\n> 1. recurrent_activation == sigmoid\r\n> 1. recurrent_dropout == 0\r\n> 1. unroll is False\r\n> 1. use_bias is True\r\n> 1. Inputs are not masked or strictly right padded.\r\n\r\nI guess changing any one of these will result in the ~~CPU~~ non-cuDNN implementation being used.", "That's right mimxrt.  I guess the interesting point here is that the inputs to my LSTM are always masked, but I have to force the non-cudnn implementation using the activation function.  Might be a clue for someone who can fix this.", "I'd be nice to have a flag on LSTM layers which allows to disable the use of the CuDNN implementation instead of either disabling eager execution or using non-default activation functions.", "Facing this same issue and reported here: https://github.com/tensorflow/tensorflow/issues/40982. ", "> I'd be nice to have a flag on LSTM layers which allows to disable the use of the CuDNN implementation instead of either disabling eager execution or using non-default activation functions.\r\n\r\nThere is a private field you can set to disable the cudnn kernel once the layer is created.\r\n```\r\nlayer = tf.keras.layers.LSTM(4)\r\nlayer. _could_use_gpu_kernel = False\r\n```\r\nThis will focus the lstm layer to use the non-cudnn implementation, even landed on GPU.", "@queirozfcom what would you suggest to use between this option and disabling eager execution as when eager execution is disabled it does work? ", "Disable eager will have larger side effects since it change the runtime to execute in graph/session context. We don't recommend user to fallback to graph unless they really need some feature in graph/session.", "Thanks :)", "I am still facing this issue using TF 2.2.0. I also found the same workaround of forcing the LSTM to not use the cuDNN implementation to work, however it is nearly prohibitively slow. I found the generic GPU implementation took ~30 times longer to train per epoch than the cuDNN version. I hope this can be fixed soon.", "I have disable the cudnn code path if there is fully masked data in the batch. It will have some performance dip since it fallback to generic kernel, but won't error out. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33148\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33148\">No</a>\n", "facing this issue using TF 2.2.0 while evaluating", "Please try with the latest nightly version.", "> \r\n> \r\n> @qlzh727 Not yet. We might need to wait for the next major release for the fix.\r\n\r\ncudnn 8 is here, is there any info about this issue?", "@geetachavan1 I am using `tensorflow==2.3.0 but for this network:\r\n\r\n```python\r\ndef call(self, inputs, training=None, memory_states=None, **kwargs):\r\n\r\n    x = self.embedding(inputs, training=training)\r\n\r\n    if memory_states is None:\r\n        memory_states = self.get_initial_state(shape_list(x)[0])\r\n\r\n    next_memory_states = []\r\n\r\n    for i, (lstm, norm) in enumerate(self.lstm_stack):\r\n        x = norm(x, training=training)\r\n        outputs = lstm(x, training=training, initial_state=memory_states[i])\r\n        x, state = outputs[0], outputs[1:]\r\n        next_memory_states.append(state)\r\n\r\n    return x, next_memory_states\r\n```\r\n\r\nwhere we have:\r\n\r\n```python\r\nself.embedding = layers.Embedding(vocab_size, embedding_size, mask_zero=True)\r\nself.lstm_stack = list()\r\nfor _ in range(num_layers):\r\n    lstm = layers.LSTM(\r\n        units=lstm_units,\r\n        return_sequences=True,\r\n        return_state=True,\r\n        dropout=dropout\r\n    )\r\n    norm = layers.LayerNormalization()\r\n    self.lstm_stack.append((lstm, norm))\r\n```\r\n\r\nI am getting\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/sfalk/tmp/speech-v2/asr/bin/eval_transducer.py\", line 153, in <module>\r\n    main()\r\n  File \"/home/sfalk/tmp/speech-v2/asr/bin/eval_transducer.py\", line 85, in main\r\n    y_greedy = model.greedy_decode(encoder_inputs)[0]\r\n  File \"/home/sfalk/tmp/speech-v2/asr/model/transducer.py\", line 351, in greedy_decode\r\n    init_predict_network_state = PredictNetworkState(*self.predict_network(init_yseq))\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 985, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/home/sfalk/tmp/speech-v2/asr/model/transducer.py\", line 118, in call\r\n    outputs = lstm(x, training=training, initial_state=memory_states[i])\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 720, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 985, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\", line 1176, in call\r\n    last_output, outputs, new_h, new_c, runtime = gpu_lstm(\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\", line 1401, in gpu_lstm\r\n    outputs, h, c, _, _ = gen_cudnn_rnn_ops.cudnn_rnnv3(\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py\", line 1914, in cudnn_rnnv3\r\n    return cudnn_rnnv3_eager_fallback(\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py\", line 2011, in cudnn_rnnv3_eager_fallback\r\n    _result = _execute.execute(b\"CudnnRNNV3\", 5, inputs=_inputs_flat,\r\n  File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.UnknownError: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1521): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)' [Op:CudnnRNNV3]\r\n```\r\n\r\nShouldn't this be fixed? ", "Some update: the cudnn v8.0.5 (next release) should be able to fix the issue exposed by https://github.com/tensorflow/tensorflow/issues/33148#issuecomment-540472342.", "When will this release happen .....looks like without CuDNN the implementation of LSTM runs very slow.\r\n\r\n\r\n", "> When will this release happen .....looks like without CuDNN the implementation of LSTM runs very slow.\r\n\r\ncudnn 8.0.5 is already out https://docs.nvidia.com/deeplearning/cudnn/archives/index.html\r\n", "> Some update: the cudnn v8.0.5 (next release) should be able to fix the issue exposed by [#33148 (comment)](https://github.com/tensorflow/tensorflow/issues/33148#issuecomment-540472342).\r\n\r\n@kaixih where did you read it?\r\nI'd like to give it a try.\r\n", "> > When will this release happen .....looks like without CuDNN the implementation of LSTM runs very slow.\r\n> \r\n> cudnn 8.0.5 is already out https://docs.nvidia.com/deeplearning/cudnn/archives/index.html\r\n\r\nI tried with `cudnn=8.0.4` `cudatoolkit=11.0.221`  `tensorflow-gpu=2.4.0,` and it fixed my problem.\r\ncudnn can be installed by: \r\n`conda install -c nvidia cudnn`", "I have the same issue on Google Colab using a Tesla P100 (tf 2.7.0, cuda v11.1, cudnn v8), when training lstm-based seq2seq models with a preceding Masking Layer On CPU the exact same code runs fine.\r\nDo not know if it matters, but also intermediate steps are masked, not only left/right."]}, {"number": 33147, "title": "Regularisation losses in nested layers", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Manjaro testing, x86_64\r\n- TensorFlow installed from (source or binary): pypi binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\nTensorFlow returns duplicated regularisation losses for layers which hold references to regularised variables built in other layers.\r\n\r\n**Describe the expected behavior**\r\nReturn a single regularisation loss per variable.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nclass A(tf.keras.layers.Layer):\r\n    def __init__(self, layer):\r\n        super(A, self).__init__()\r\n        self.layer = layer\r\n\r\n    def call(self, inputs):\r\n        return self.layer(inputs)\r\n\r\n\r\nclass B(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(B, self).__init__()\r\n        self.obj = tf.keras.layers.Dense(13, kernel_regularizer=tf.keras.regularizers.l1(5))\r\n        self.layerB = A(self.obj)\r\n\r\n    def call(self, inputs):\r\n        return self.layerB(inputs)\r\n\r\nmodel = B()\r\n\r\noutput = model(tf.ones([5, 10]))\r\nprint(len(model.losses))\r\n```\r\nOnce we rename self.obj to obj, i.e. not saving it as a class member, we end up with a single regularisation loss.\r\n\r\n**Other info / logs**\r\nMore info and logs in https://github.com/tensorflow/addons/issues/577\r\nPlus @guillaumekln 's response https://github.com/tensorflow/addons/issues/577#issuecomment-539530375", "comments": ["Issue replicating with TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/2b96e1dc23de60e6992be9f598a74b53/33147.ipynb) of colab.Thanks!", "https://github.com/tensorflow/tensorflow/commit/e2f31134bd8aaf7cdc8b6f157f4fee3f1e069e0c", "@georgesterpu Issue seems to be fixed in latest TF version '2.2.0-rc3'. Could you please confirm and close the issue if it is resolved for you. Thanks!", "I just tested with TensorFlow 2.2.0rc3 and can confirm that `model.losses` is now a list of a single element.\r\n\r\nWelcome to github @saikumarchalla !"]}, {"number": 33146, "title": "tensorflow (cpu only) with mkl is much slower than without mkl! ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 4.8.5-4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.8\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.10.0\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI compile tensorflow 1.8 with mkl, and I use c api session to predict wide and deep model.\r\nThe Parallel Performance is very slow, avg reponse time is 40 microsecond in 60 qps, cpu load is only 200%\r\n\r\nexport KMP_BLOCKTIME=0\r\nexport KMP_AFFINITY=granularity=fine,verbose,compact,1,0\r\nexport OMP_NUM_THREADS=64\r\nexport KMP_SETTINGS=1\r\nexport MKLDNN_VERBOSE=1\r\n\r\nin the opposite\uff0c without mkl, The Parallel Performance is ok, avg reponse time is 9 microsecond in 60 qps. I don't know why.Maybe I use wrong!\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n\r\n    SessionOptions session_options;\r\n    ConfigProto& cp = session_options.config;\r\n    cp.set_intra_op_parallelism_threads(64);\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@goldbalance is there as a reason you are using a much older version of Tensorflow? We have most of our optimizations for W&D in 1.14", "Also looping in @wei-v-wang and @agramesh1 for settings.", "Thanks @penpornk  \r\n\r\n@goldbalance  As a quick suggestion, is it possible to try setting  export OMP_NUM_THREADS=1 and check the performance? Wide & Deep according to my understand, will benefit more from inter_op setting (graph parallelism) than from intra_op (data parallelism). \r\n\r\nWe are still identifying the gap with Eigen though.  ", "> @goldbalance is there as a reason you are using a much older version of Tensorflow? We have most of our optimizations for W&D in 1.14\r\n\r\nthank you for your advise, i will try later. I have to consider upgrade our framework software in order to use 1.14", "> Thanks @penpornk\r\n> \r\n> @goldbalance As a quick suggestion, is it possible to try setting export OMP_NUM_THREADS=1 and check the performance? Wide & Deep according to my understand, will benefit more from inter_op setting (graph parallelism) than from intra_op (data parallelism).\r\n> \r\n> We are still identifying the gap with Eigen though.\r\n\r\nHi, I ever knew about using mkl may lead to use much more threads running. In 1 qps, mkl is slower than without mkl.  In 60qps, when decreasing inter threads pool size,  My App runtime cannot use more cpu. When adding the size, there are too many threads to running. I try serveral times from 1 to 64, finally, for the best performance, I set just 1 thread pool size  . Therefore, i give up to using mkl tf 1.8", "> Thanks @penpornk\r\n> \r\n> @goldbalance As a quick suggestion, is it possible to try setting export OMP_NUM_THREADS=1 and check the performance? Wide & Deep according to my understand, will benefit more from inter_op setting (graph parallelism) than from intra_op (data parallelism).\r\n> \r\n> We are still identifying the gap with Eigen though.\r\n\r\nI wanna to know when mkl-dnn can adapt for eigen thread pool", "> Thanks @penpornk\r\n> \r\n> @goldbalance As a quick suggestion, is it possible to try setting export OMP_NUM_THREADS=1 and check the performance? Wide & Deep according to my understand, will benefit more from inter_op setting (graph parallelism) than from intra_op (data parallelism).\r\n> \r\n> We are still identifying the gap with Eigen though.\r\n\r\nOMP_NUM_THREADS=1\r\nSessionOptions session_options;\r\nConfigProto& cp = session_options.config;\r\ncp.set_inter_op_parallelism_threads(64);\r\ncp.set_intra_op_parallelism_threads(64);\r\n\r\nsorry, i met the worst performance using above settting, cpu load also is low", "@goldbalance Sorry to hear that. The above setting would generate 64 OMP threads though, eventually. Can you set inter op to e.g. 8 or 16, and then intra_op set to 64 - 8 (or 16)? \r\nHopefully this does not give you worse performance than worst :) ", "> @goldbalance Sorry to hear that. The above setting would generate 64 OMP threads though, eventually. Can you set inter op to e.g. 8 or 16, and then intra_op set to 64 - 8 (or 16)?\r\n> Hopefully this does not give you worse performance than worst :)\r\n\r\nexport KMP_BLOCKTIME=0\r\nexport KMP_AFFINITY=granularity=fine,verbose,compact,1,0\r\nexport OMP_NUM_THREADS=1\r\nexport KMP_SETTINGS=1\r\nexport MKLDNN_VERBOSE=1\r\nexport OMP_STACKSIZE=1G\r\n\r\nSessionOptions session_options;\r\nConfigProto& cp = session_options.config;\r\ncp.set_inter_op_parallelism_threads(8);\r\ncp.set_intra_op_parallelism_threads(64); // also 8 the same\r\n\r\nSorry, the performance is also low, 100ms\r\n\r\nbest performance is 25ms, when inter op is 1 and omp thread is setted 64. \r\nwithout mkl is only 10ms. \r\n\r\nI guess that it is blocked, because cpu is low with mkl.\r\n\r\n\r\n\r\n", "Ok, thank you for trying. Yes, it seems there are fundamental issues that needs to be resolved on our side to bring up the performance.  ", "Hi I am also experiencing slowness with 1.14 with mkl. It is slower than 1.14 without MKL. \r\nI'm using the intelaipg/intel-optimized-tensorflow:1.14.0-mkl-py3 docker image rather than building it myself, running darknet based yolov3. \r\nAny updates regarding these fundamental issues?", "@pigubaoza  We are testing and will be rolling out some new features. Regarding your Darknet based YoloV3, have you tuned OMP_NUM_THREADS, inter_op_parallelism_threads and intra_op_parallelism threads, KMP_BLOCKTIME setting?  Also, what CPU are you testing with?\r\n\r\nAs a last resort, can you try \"export TF_DISALBE_MKL=1\" and rerun your model and see if it recovers the performance similar to 1.14 w/o MKL? ", "> @pigubaoza We are testing and will be rolling out some new features. Regarding your Darknet based YoloV3, have you tuned OMP_NUM_THREADS, inter_op_parallelism_threads and intra_op_parallelism threads, KMP_BLOCKTIME setting? Also, what CPU are you testing with?\r\n> \r\n> As a last resort, can you try \"export TF_DISALBE_MKL=1\" and rerun your model and see if it recovers the performance similar to 1.14 w/o MKL?\r\n\r\nThanks wei-v-wang. After some testing I found out OMP_NUM_THREADS and KMP_BLOCKTIME should be as recommended. OMP_NUM_THREADS=4 (I am running an i7 6700K), and KMP_BLOCKTIME=0 (Running CNNs), intra_op_parallelism_threads=4, inter_op_parallelism_threads=2. I previously did not use these values. \r\n\r\nRunning with TF_DISABLE_MKL=1, my average latency is insignificantly different. ~0.01s\r\n\r\nCould it be that because I am running a 6700K, the benefits are not as obvious as running Xeon, or multiple Xeons? \r\n\r\nAlso, what are these new features that would be rolled out and where can I follow them? "]}, {"number": 33145, "title": "tf.keras.layers.LSTM and LSTMCell have different output formats", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe `tf.keras.layers.LSTM` and `tf.keras.layers.LSTMCell` classes currently have different output formats when using the `return_state=True` flag in `LSTM`. \r\n\r\n`LSTMCell` returns `[output, [h, c]]`\r\n`LSTM` returns `[output, h, c]`\r\n\r\nThis makes it complicated to design a layer wrapper for both types of cells, since the wrapper's call method will have to take into account the cell type.\r\nAre you okay with making `LSTM` adhere to the `LSTMCell` return format ?\r\n\r\n**Will this change the current api? How?**\r\nYes. In `keras.layers.recurrent_v2.py` (Line 983):\r\n```\r\n    if self.return_state:\r\n      #  return [output] + list(states)  # the current format\r\n      return [output] + [states]  # the `LSTMCell` format\r\n```\r\n\r\n**Who will benefit with this feature?**\r\nMe\r\n**Any Other info.**\r\nThis possibly applies to other cells like GRU.", "comments": ["Thanks for reporting the issue. I can't easily update this to break the API contract. Could u provide more details about your wrapper? I am curious about it since LSTM layer and cell works differently (LSTM process the whole sequence, while the cell only process one time step).", "It is a simple dropout wrapper based on the polymorphic `tf.keras.layers.Dropout` class, designed for one of my tests. However, I don't think that I am going to use it from now on. Please feel free to close the issue if you think there are no other situations where this output uniformity could be useful.\r\n", "I agree that the different format of output is bit confusing. The API contract predates my work for keras team, and changing it will be a somewhat expansive exercise. "]}, {"number": 33144, "title": "Tensorflow Lite  fully integer quantization error :Got tensor of type STRING but expected type FLOAT32", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):binary(pip)\r\n- TensorFlow version (or github SHA if from source):1.14.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n2019-10-08 20:47:37.470691: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-08 20:47:37.491949: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2194920000 Hz\r\n2019-10-08 20:47:37.492291: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5600095b3a90 executing computations on platform Host. Devices:\r\n2019-10-08 20:47:37.492321: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-10-08 20:47:39.487818: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-10-08 20:47:39.487989: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-10-08 20:47:41.530815: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-10-08 20:47:41.576395: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 941 nodes (-544), 973 edges (-544), time = 1222.25598ms.\r\n2019-10-08 20:47:41.576429: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 941 nodes (0), 973 edges (0), time = 214.279ms.\r\nINFO: Initialized TensorFlow Lite runtime.\r\nTraceback (most recent call last):\r\n  File \"/home/eagle/PycharmProjects/TFLite/main.py\", line 30, in <module>\r\n    quantized_model = converter.convert()\r\n  File \"/home/eagle/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 908, in convert\r\n    inference_output_type)\r\n  File \"/home/eagle/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 200, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n  File \"/home/eagle/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 75, in calibrate_and_quantize\r\n    self._calibrator.FeedTensor(calibration_sample)\r\n  File \"/home/eagle/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 112, in FeedTensor\r\n    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value)\r\nValueError: Cannot set tensor: Got tensor of type STRING but expected type FLOAT32 for input 0, name: input \r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nSource Code:\r\n\r\nimport tensorflow as tf\r\nimport os\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-2\"\r\n\r\nsaved_model_dir = \"./tmp/resnet/resnet_v2_101_299_frozen.pb\"\r\n\r\nif __name__ == \"__main__\":\r\n    input_arrays = [\"input\"]\r\n    output_arrays = [\"output\"]\r\n\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n        str(saved_model_dir), input_arrays, output_arrays, input_shapes={\"input\": [1, 299, 299, 3]})\r\n\r\n    (resnet_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\r\n    images = tf.cast(resnet_train[0], tf.float32)/255.0\r\n    resnet_ds = tf.data.Dataset.from_tensor_slices(images).batch(1)\r\n\r\n    def representative_data_gen():\r\n        for input_value in resnet_ds.take(100):\r\n            yield [input_value]\r\n\r\n\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = representative_data_gen\r\n    converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.qint8\r\n    converter.inference_output_type = tf.qint8\r\n    quantized_model = converter.convert()\r\n    open(\"./tmp/resnet/resnet_v2_101_299_frozen_weight_quantized.tflite\", \"wb\").write(quantized_model)\r\n", "comments": ["Its seems that your representative dataset is feeding string tensors can you double check the type of input_value and make sure it is the correct type (FLOAT)", "I have the same problem - the Tensor sent with input_value is float32.", "Similar issue, solved by feeding `numpy.ndarray` instead.", "Pass Numpy array instead it will solve the problem in tensorflow versions<2.0\r\nI mean don't use this \r\n`images = tf.cast(resnet_train[0], tf.float32)/255.0`\r\nuse this instead\r\n`images=resnet_train[0].astype('float32')/255`\r\nIt worked for me in a similar case\r\n\r\n", "I encountered same issue and still have no solutions after I tried all above suggestions.\r\n```\r\ntrain = np.random.uniform(0,255,(100,112,112,3))\r\nimages = train.astype(np.float32)/255.\r\nresnet_ds = tf.data.Dataset.from_tensor_slices(images).batch(1)\r\n\r\ndef representative_data_gen():\r\n    for input_value in resnet_ds.take(10):\r\n        yield [input_value]\r\nfor j,i in enumerate(resnet_ds.take(10)):\r\n    if j==2: break\r\n    print(i.shape)\r\n    print(i.dtype)\r\n# (?, 112, 112, 3)\r\n# <dtype: 'float32'>\r\n# (?, 112, 112, 3)\r\n# <dtype: 'float32'>\r\n\r\ninput_arrays = ['input_1']\r\noutput_arrays = ['output1/BiasAdd', 'output2/Softmax', 'output3/Softmax']\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\"ThreeOutputModel_Final.pb\", input_arrays , output_arrays, input_shapes={\"input_1\": [1, 112, 112, 3]})\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model = converter.convert()\r\nopen(\"ThreeOutputModel_Final.tflite\", \"wb\").write(tflite_model)\r\n```\r\nI got valueError in ` convert.convert()`\r\n\r\n> ValueError: Cannot set tensor: Got tensor of type STRING but expected type FLOAT32 for input 426, name: input_1 ", "Can you provide the model I mean ThreeOutputModel_Final.pb", "> Can you provide the model I mean ThreeOutputModel_Final.pb\r\n\r\nCould it be model's problem? \r\n[shorturl.at/AFGN3](https://shorturl.at/AFGN3)", "I got the same problem and my problem was that the dtype of the dataset was 'float64', not the 'float32'.\r\nSo the problem was solved by casting dtype using .astype.\r\n```train_images = np.load(dir_data/'train_images.npy')\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\r\ntrain_dataset = train_dataset.shuffle(buffer_size=10000).batch(128)\r\n\r\nrepresentative_dataset = tf.data.Dataset.from_tensor_slices(train_images.astype('float32'))\r\nrepresentative_dataset = representative_dataset.shuffle(buffer_size=10000).batch(1)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ndef representative_dataset_gen():\r\n    for i, samples in enumerate(representative_dataset.take(1)):\r\n        yield [samples]\r\n        \r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_model_int = converter.convert()\r\nwith tf.io.gfile.GFile('./models/model_quant_8b.tflite', 'wb') as f:\r\n    f.write(tflite_model_int)\r\n```", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33144\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33144\">No</a>\n"]}, {"number": 33143, "title": "Tensorflow Keras not allowing different size X and y for fit_generator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Professional Edition\r\n- TensorFlow installed from (source or binary): binary, installed using conda\r\n- TensorFlow version (use command below): unknown, 1.14.0\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.0, 7.6\r\n- GPU model and memory: T1000, 4GB VRAM\r\n\r\n**Describe the current behavior**\r\nWhen having model with two inputs, with shapes e.g. (486, 3673), (486, ) and one output (87, 1), after calling `model.fit_generator()`, the training crashes:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\contextlib.py\", line 130, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py\", line 236, in as_default\r\n    yield self\r\n  File \"C:/Projects/iotmap/py/train.py\", line 162, in train_fit_generator\r\n    model.fit_generator(generator, epochs=epochs, workers=4, callbacks=[MyCallback()])\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1433, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 264, in model_iteration\r\n    batch_outs = batch_function(*batch_data)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1153, in train_on_batch\r\n    extract_tensors_from_dataset=True)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 2688, in _standardize_user_data\r\n    training_utils.check_array_lengths(x, y, sample_weights)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\", line 483, in check_array_lengths\r\n    'and ' + str(list(set_y)[0]) + ' target samples.')\r\nValueError: Input arrays should have the same number of samples as target arrays. Found 486 input samples and 87 target samples.\r\n2019-10-08 13:32:22.899390: W tensorflow/python/util/util.cc:280] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n```\r\n\r\n**Describe the expected behavior**\r\nTraining should proceed without problems, the data is valid. \r\nWhen trained during eager execution, using \r\n```\r\nfor step in range(settings['iterations']):\r\n    x_train, y_train = batch_fn()\r\n    with tf.GradientTape() as tape:\r\n        logits, probs = model(x_train)\r\n        loss_value = loss(y_train, logits)\r\n    grads = tape.gradient(loss_value, model.trainable_weights)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n```\r\nthe model is trained without problems, and I expect using  `model.fit_generator()` would allow this behaviour too.\r\n\r\n**Code to reproduce the issue**\r\nmodel is built using\r\n```\r\nimport tensorflow as tf\r\n\r\nclass SegmentedMean(tf.keras.layers.Layer):\r\n    def __init__(self, *args, **kwargs):\r\n        super(SegmentedMean, self).__init__(*args, **kwargs)\r\n\r\n    def call(self, inputs, **kwargs):\r\n        features, segments = inputs\r\n        return tf.math.segment_mean(features, segments)\r\n\r\ninputs = tf.keras.Input(shape=(feats_len,), name='features')\r\nsegments = tf.keras.Input(shape=(), name='segments', dtype=tf.int32)\r\nx = tf.keras.layers.Dense(settings['k'], activation=tf.nn.relu)(inputs)\r\nx = tf.keras.layers.Dense(settings['k'])(x)\r\n\r\nx = SegmentedMean()((x, segments))\r\nx = tf.keras.layers.Dense(settings['k'], activation=tf.nn.relu)(x)\r\nlogits = tf.keras.layers.Dense(2, name='output_logits')(x)\r\nprobs = tf.keras.layers.Softmax()(logits)\r\nmodel = tf.keras.Model(inputs=(inputs, segments), outputs=(logits, probs), name='mil_model')\r\noptimizer = tf.keras.optimizers.Adam()\r\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nmodel.compile(optimizer=optimizer, loss={'output_logits': loss})\r\nmodel.fit_generator(generator, epochs=5, workers=4)\r\n```\r\n\r\n**Other info / logs**\r\nWhen using segmented aggregations, naturally the size of X and Y is different but after aggregation, dimensions fit, but keras model training don't allow this.\r\nUsing those aggregations is motivated by https://arxiv.org/pdf/1609.07257.pdf paper I am trying to use to some problem.\r\nKeras repo allows turning those checks of in [this PR](https://github.com/keras-team/keras/pull/11548), but it is not incorporated into tensorflow version of keras.", "comments": ["@racinmat Can you please provide a github gist or a colab notebook to reproduce the issue. Thanks!", "Of course, the gist is here https://gist.github.com/racinmat/a8eb2f727fcc4bd24745042eee5cd8d6", "I am getting an error @racinmat Please find my github gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/361962dd71b59b1bf19b62c0a53bb414/untitled210.ipynb).", "Oh, my bad, fixed now in the github gist and it produces the abovementioned error in the colab.", "I am still facing the same error @racinmat Please share your github gist.", "I updated the github gist, not the colab. Which error are you getting with the most recent version of the gist?", "Was able to reproduce it. Please find the gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/f0f721735d1855b103d171de87643f7f/untitled210.ipynb).", "@racinmat Thanks for the issue!\r\n\r\nThis is fixed in the latest tf-nightly: `pip install -U tf-nightly`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33143\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33143\">No</a>\n", "Hey @omalleyt12! I'm having the same issue here with TF 2.5. Is there any estimate for when this is going to be in stable?"]}, {"number": 33142, "title": "TFLiteLSTMCell and TfLiteRNNCell has large error with high hidden unit", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Pixel 3\r\n- TensorFlow version (use command below): 1.14\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nBidirectional RNN with LSTM and RNN perform significantly poorer on Android compared to running on Linux with high `n_hidden` (e.g. >300)\r\nIn our testings, we see that the amount of parameter has a positive relationship with error, we suspect it is some sort of rounding error in the Lite implementation.\r\n\r\nAlso, please fix the inconsistent capitalization of `TFLiteLSTMCell` and `TfliteRNNCell`.", "comments": ["@tobyclh,\r\nPlease provide the code snippet to reproduce the issue. Thanks!", "Hi, thanks for coming back,\r\nI realize it was a bug at our side, sorry about that."]}, {"number": 33141, "title": "Fix/_map_graph_network for multiple concatenation operations", "body": "PR that fixes bug, checking graph connectivity when concatenation is used multiple times for the same layer.\r\nFixes https://github.com/tensorflow/tensorflow/issues/30357 and https://github.com/tensorflow/tensorflow/issues/30355", "comments": ["After more testing I found out that the fix does not work for several cases, I'll open new PR at the time I'll fix it."]}, {"number": 33140, "title": "Cannot access RandomFourierFeatures keras layer in TensorFlow 2.0", "body": "### Problem Summary\r\n\r\nHello,\r\n\r\nI'm not entirely sure if this is a bug or if I have made an error in installation but I have not been able to access the `tf.keras.layers.kernelized.RandomFourierFeatures` module within the tensorflow 2.0 framework.\r\n\r\n\r\n### Problem Description\r\n\r\nI've seen the `kernelized` and `RandomFourierFeatures` layer within the code-base within the [init file](https://github.com/tensorflow/tensorflow/blob/e62dc433fcce833313b4174e20fc24c418593d27/tensorflow/python/keras/layers/__init__.py), the [actual layer](https://github.com/tensorflow/tensorflow/blob/ddde6c74155df8da7229854d5387f3df64ead88a/tensorflow/python/keras/layers/kernelized.py) and the [test file](https://github.com/tensorflow/tensorflow/blob/b4245c36bc9bcb752c0a2118728098b359fd97e2/tensorflow/python/keras/layers/kernelized_test.py) under the [keras layers](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras/layers) directory. However, I cannot call the layer from anywhere within the tensorflow layers frameworks.\r\n\r\n**Current Behaviour for importing `Kernelized` layer**\r\n\r\n<details>\r\n\r\n```python\r\ntf.keras.layers.kernelized\r\n```\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-122e54d3d99d> in <module>()\r\n----> 1 tf.keras.layers.kernelized\r\n\r\nAttributeError: module 'tensorflow_core.keras.layers' has no attribute 'kernelized'\r\n```\r\n</details>\r\n\r\n**Current Behaviour for importing `RandomFourierFeatures` layer**\r\n\r\n<details>\r\n\r\n```python\r\ntf.keras.layers.RandomFourierFeatures\r\n```\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-058ec5bf6f57> in <module>()\r\n----> 1 tf.keras.layers.RandomFourierFeatures\r\n\r\nAttributeError: module 'tensorflow_core.keras.layers' has no attribute 'RandomFourierFeatures'\r\n```\r\n\r\n</details>\r\n\r\n---\r\n### System Information\r\n\r\nI am currently using a Google Colab Notebook. [Link to Colab Notebook](https://colab.research.google.com/drive/1lMnpHXnP5BTAWy8vyKtEYgmLbq2Dlw3G).\r\n\r\n\r\n**TensorFlow Version**\r\n\r\n<details>\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.__version__\r\n!python --version\r\n```\r\n\r\n```python\r\n'2.1.0-dev20191008'\r\nPython 3.6.8\r\n```\r\n</details>\r\n\r\n**Colab CPU Info**\r\n\r\n<details>\r\n\r\n```python\r\nprocessor\t: 0\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 63\r\nmodel name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\r\nstepping\t: 0\r\nmicrocode\t: 0x1\r\ncpu MHz\t\t: 2300.000\r\ncache size\t: 46080 KB\r\nphysical id\t: 0\r\nsiblings\t: 2\r\ncore id\t\t: 0\r\ncpu cores\t: 1\r\napicid\t\t: 0\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\r\nbogomips\t: 4600.00\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 46 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 1\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 63\r\nmodel name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\r\nstepping\t: 0\r\nmicrocode\t: 0x1\r\ncpu MHz\t\t: 2300.000\r\ncache size\t: 46080 KB\r\nphysical id\t: 0\r\nsiblings\t: 2\r\ncore id\t\t: 0\r\ncpu cores\t: 1\r\napicid\t\t: 1\r\ninitial apicid\t: 1\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\r\nbogomips\t: 4600.00\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 46 bits physical, 48 bits virtual\r\npower management:\r\n```\r\n\r\n</details>\r\n\r\n**Memory Info**\r\n\r\n<details>\r\n\r\n```python\r\nMemTotal:       13341992 kB\r\nMemFree:         8670124 kB\r\nMemAvailable:   12753184 kB\r\nBuffers:          115848 kB\r\nCached:          3865368 kB\r\nSwapCached:            0 kB\r\nActive:          1160124 kB\r\nInactive:        3077576 kB\r\nActive(anon):     256804 kB\r\nInactive(anon):      324 kB\r\nActive(file):     903320 kB\r\nInactive(file):  3077252 kB\r\nUnevictable:           0 kB\r\nMlocked:               0 kB\r\nSwapTotal:             0 kB\r\nSwapFree:              0 kB\r\nDirty:               772 kB\r\nWriteback:             0 kB\r\nAnonPages:        256548 kB\r\nMapped:           167332 kB\r\nShmem:               656 kB\r\nSlab:             377108 kB\r\nSReclaimable:     355904 kB\r\nSUnreclaim:        21204 kB\r\nKernelStack:        3040 kB\r\nPageTables:         4044 kB\r\nNFS_Unstable:          0 kB\r\nBounce:                0 kB\r\nWritebackTmp:          0 kB\r\nCommitLimit:     6670996 kB\r\nCommitted_AS:    1194088 kB\r\nVmallocTotal:   34359738367 kB\r\nVmallocUsed:           0 kB\r\nVmallocChunk:          0 kB\r\nAnonHugePages:         0 kB\r\nHugePages_Total:       0\r\nHugePages_Free:        0\r\nHugePages_Rsvd:        0\r\nHugePages_Surp:        0\r\nHugepagesize:       2048 kB\r\nDirectMap4k:       59380 kB\r\nDirectMap2M:     4134912 kB\r\nDirectMap1G:    11534336 kB\r\n```\r\n</details>\r\n\r\n\r\n\r\nThank you.\r\n\r\n", "comments": ["I have made a few changes in the colab notebook and its working fine for me. \r\nIn tensorflow 2.0 you can access RandomFourierFeatures Keras layers this way\r\n\r\n`from tensorflow.python.keras.layers.kernelized import RandomFourierFeatures`\r\n\r\nPlease find the github gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/0ea14d917d5f8f841570cc92b7ef69fb/copy-of-rff_layer.ipynb).\r\n\r\n", "Thanks for looking into it.\r\n\r\nI noticed that the tensorflow version in the gist says that it is tensorflow version `1.15.0-rc3`.\r\n\r\nDoes this mean that it is not available for version `2.0.0` or `2.1.0-dev20191008`?\r\n\r\nThanks again."]}, {"number": 33139, "title": "Memory leak when training simple LSTM Network", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No custom code written\r\n\r\n> == check python ===================================================\r\n> python version: 3.7.4\r\n> python build version: ('default', 'Aug 13 2019 20:35:49')\r\n> python compiler version: GCC 7.3.0\r\n> python implementation: CPython\r\n> == check os platform ===============================================\r\n> os: Linux\r\n> os kernel version: #31 18.04.1-Ubuntu SMP Thu Sep 12 18:29:21 UTC 2019\r\n> os release version: 5.0.0-29-generic\r\n> os platform: Linux-5.0.0-29-generic-x86_64-with-debian-buster-sid\r\n> linux distribution: ('debian', 'buster/sid', '')\r\n> linux os distribution: ('debian', 'buster/sid', '')\r\n> architecture: ('64bit', '')\r\n> machine: x86_64\r\n> == are we in docker =============================================\r\n> No\r\n> == compiler =====================================================\r\n> c++ (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n> Copyright (C) 2017 Free Software Foundation, Inc.\r\n> This is free software; see the source for copying conditions.  There is NO\r\n> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n> == check pips ===================================================\r\n> numpy                  1.16.4   \r\n> protobuf               3.8.0    \r\n> tensorflow             1.14.0   \r\n> tensorflow-estimator   1.14.0   \r\n> == check for virtualenv =========================================\r\n> False\r\n> == tensorflow import ============================================\r\n> tf.version.VERSION = 1.14.0\r\n> tf.version.GIT_VERSION = unknown\r\n> tf.version.COMPILER_VERSION = 5.4.0\r\n\r\n**Describe the current behavior**\r\nWhen I run the code below the memory usage increases each epoch until my system is unresponsive.\r\n\r\n\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Dropout\r\nimport numpy as np\r\n\r\nnum_features = 205\r\ntime_lenth = 12\r\nnum_of_instances = 5000\r\ntrip_sets = np.random.rand(num_of_instances, time_lenth, num_features)\r\nprint('num features: ', num_features)\r\ndata_len = len(trip_sets)\r\ntest_split = np.arange(data_len)\r\nnp.random.shuffle(test_split)\r\nnew_dataset = np.array(trip_sets)\r\ntargets = np.random.rand(num_of_instances, time_lenth, 1)\r\n\r\ntest_data = new_dataset[test_split[:int(data_len*0.2)]]\r\ny_test_data = targets[test_split[:int(data_len*0.2)]]\r\ntrain_data = new_dataset[test_split[int(data_len*0.2):]]\r\ny_train_data = targets[test_split[int(data_len*0.2):]]\r\n\r\nmodel = Sequential()\r\nmodel.add(LSTM(75, return_sequences=True, input_shape=(None, num_features)))\r\nmodel.add(Dropout(0.3))\r\nmodel.add(LSTM(75, return_sequences=True))\r\nmodel.add(Dropout(0.3))\r\nmodel.add(TimeDistributed(Dense(1)))\r\n# Memory leak also occurs if i use a model.add(Dense(1)) below instead of Time Distributed\r\n# model.add(TimeDistributed(Dense(1)))\r\n\r\nadam = tf.keras.optimizers.Adam(lr=0.001)\r\nmodel.compile(loss='mse', optimizer=adam)\r\nhistory = model.fit(x=train_data, y=y_train_data, epochs=100, validation_data=(test_data,y_test_data))\r\n```", "comments": ["I upgraded to TF2.0 and the issue went away.  ", "@johmicrot ,\r\nHi,Code working fine in TF-2.0, try using the same version. please confirm if the issue can be closed ?Thanks!"]}, {"number": 33138, "title": "keras.LSTM in Tensorflow built with mkl is 5 times slower on CPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source (with or without mkl)\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.20.0\r\n- GCC/Compiler version (if compiling from source):  7.4.0\r\n- CUDA/cuDNN version: 7.6.2\r\n\r\n**Describe the current behavior**\r\nkeras.LSTM in Tensorflow built with mkl is 5 times slower on CPU\r\n\r\n**Other info / logs**\r\ntf.matmul and tf.keras.Conv2D are also tested and mkl version is faster.\r\n", "comments": ["Hi @npuichigo \r\nDid you built TF on source code or use the pre-built wheel package?\r\nCould you give more information about the tensorflow version and share the benchmark code?\r\nI can help to re-produce the issue.", "@Leslie-Fang I built the python wheel packages with or without mkl both from source code (tf 2.0.0). I simply compared the time used for tf.keras.LSTM(..., return_sequences=True) with or without mkl and find the difference. It's nice that you can help to re-produce the issue.", "Sure, I can try the latest master branch w/wo MKL.\r\nBut I am not familiar with this tf.keras.LSTM(..., return_sequences=True) API.\r\nCould you give me a snippet of code?", "```python\r\nimport timeit\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    lstm = keras.layers.LSTM(1024, return_sequences=True)\r\n    inputs = tf.random.normal((1, 100, 1024))\r\n    time_used = np.mean(timeit.repeat(lambda: lstm(inputs), repeat=10, number=1))\r\n    print(\"Time used: {}s\".format(time_used))\r\n```", "Hi @npuichigo \r\nHere is the result I got:\r\nNo-MKL: Time used: 0.8596639971015975s\r\nMKL: Time used: 0.8698679601977346s", "@Leslie-Fang Did you enable all the SSE/AVX features when compiling the packages?\r\nThe result I got:\r\nNo-MKL: Time used: 0.2749028032645583s\r\nMKLDNN: Time used: 1.2454360516741871s", "> @Leslie-Fang Did you enable all the SSE/AVX features when compiling the packages?\r\n> The result I got:\r\n> No-MKL: Time used: 0.2749028032645583s\r\n> MKLDNN: Time used: 1.2454360516741871s\r\n\r\nGood suggestion.  I can reproduce your performance data now.\r\nI will do some profile and let you know the result.", "I also met the same problem.", "@npuichigo I think this is similar to this [issue](https://github.com/tensorflow/tensorflow/issues/33146). Thanks! ", "@npuichigo @Leslie-Fang That's because the code is run in eager mode. TensorFlow-MKL doesn't support many operations in eager yet. See the list of supported ops in eager mode [here](https://github.com/tensorflow/tensorflow/blob/8cb8627abb5ef83a6fba34f8fd0e4ee430562eb1/tensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc#L79-L87). (There will be more soon in later releases.)\r\n\r\n@TensorFlow-MKL Maybe we should make sure that the ops that can't be rewritten doesn't get slower than normal TF?", "@penpornk I met a similar performance issue on TF `v1.14.0` when running my LSTM-based model on [intel-tensorflow docker](https://hub.docker.com/r/intelaipg/intel-optimized-tensorflow/). The profiling file can be found [here](https://gist.github.com/feihugis/f20529b25fe17ba3b46d31580ba7ef92).", "@penpornk Thanks for your information.\r\nhttps://github.com/tensorflow/tensorflow/blob/308a0d42c5bb7a37c259a73a282c5c0338aecb8d/tensorflow/python/keras/layers/recurrent_v2.py#L1262-L1279\r\nHere is the block which LSTM is repeated calculated, MKL version do spend more time in the calculation of this code block.\r\nFrom the log I see matmul has been rewrited to _mklmatmul. \r\n", "@npuichigo, Thanks for all the information. Looking into this, I will update once I find the root cause.", "@Leslie-Fang @jojivk73 @penpornk Why dosen't tensorflow directly call MKL-DNN's LSTM primitives? That's the fastest approach. See PyTorch's [to_mkldnn](https://github.com/pytorch/pytorch/pull/26387)", "The current performance is better now. A fix was added with DNN 1.2\r\n   MKL Time : 0.2231828489s", "@jojivk73 Which version of tensorflow can produce this result?", "@AStupidBear The fix is in latest branch TF 2.x.", "@jojivk73 Would you mind pasting the commit hash here? I'd like to check if it made it into the TF 2.3 branch. Thank you!", "@penpornk This is the related oneDNN [issue ](https://github.com/oneapi-src/oneDNN/issues/525#issuecomment-575375623) :  [commit](https://github.com/oneapi-src/oneDNN/commit/cf7edb6575b255a2fee54d99d0f2eef0e761939a)", "Thank you! TF-MKL 2.3 is currently using oneDNN v1.4 which doesn't have this fix. There's a plan to cherrypick a change to v1.5 if possible, but it seems v1.5 doesn't have this fix either. Do you plan to patch this into v1.5?\r\n\r\ncc: @agramesh1 ", "Hi @penpornk  The fix is in oneDNN 1.4 so we should be fine for  TF2.3 regarding this bug. The oneDNN commit is also in oneDNN 1.2 which was used in TF2.2.  So TF2.2 should resolve the issue.", "Hi @penpornk, The fix is modified to make more conservative : [commit](https://github.com/oneapi-src/oneDNN/commit/3a4b5c526805e8c6aaadb1e377b4725988da532b) .  It is in 1.4 ([525 last update](https://github.com/oneapi-src/oneDNN/issues/525#issuecomment-575377402))\r\n", "@agramesh1 @jojivk73 Thank you for the info!", "@npuichigo \r\nCould you check the answer?\r\nIs it possible to close this issue if it's OK to you?\r\n\r\nThank you!", "@agramesh1 @jojivk73 Thanks for the info. @NeoZhangJianyu It works for me now. I'll close the issue.", "Dear Sir\uff0c\r\n  I use tensorflow2.4.0 with use onednn1.4\uff0c test resnet50 infenrence\uff0cstill slower than not use onednn\u3002\r\nand the onednn  code is\uff1a\r\n https://github.com/oneapi-src/oneDNN/commit/3a4b5c526805e8c6aaadb1e377b4725988da532b", "> Dear Sir\uff0c\r\n> I use tensorflow2.4.0 with use onednn1.4\uff0c test resnet50 infenrence\uff0cstill slower than not use onednn\u3002\r\n> and the onednn code is\uff1a\r\n> [oneapi-src/oneDNN@3a4b5c5](https://github.com/oneapi-src/oneDNN/commit/3a4b5c526805e8c6aaadb1e377b4725988da532b)\r\n\r\nThe possible reason is running without configuration. Have you ever set below configuration? It's very important to TF with oneDNN:\r\nEnv var | value\r\n-- | --\r\nKMP_BLOCKTIME | 0 or 1\r\nKMP_AFFINITY | granularity=fine,verbose,compact,1,0\r\nOMP_NUM_THREADS | Cores/inter\r\nTF_NUM_INTRAOP_THREADS | Cores\r\nTF_NUM_INTEROP_THREADS | 1 or 2\r\n\r\nYou can get more detail here: https://software.intel.com/content/www/us/en/develop/articles/maximize-tensorflow-performance-on-cpu-considerations-and-recommendations-for-inference.html", "still slow\u3002\r\n--num_intra_threads=4 --num_inter_threads=2 data_format=NCHW --kmp_affinity=granularity=fine,compact,1,0 --kmp_blocktime=0\r\nor export above setting\u3002", "You can try the following on top of what you did previously regarding OpenMP tunings. \r\n\r\n**export TF_ENABLE_MKL_NATIVE_FORMAT=1** (this is latest Intel TF optimization feature and is likely to help with your keras based model). \r\npython ... as usual. ", "@zhangqiang-hf \r\nWhat's the format of resnet50 model? Keras HD5 or Tensorflow PB?\r\n \r\nIs it possible to share your test code for this issue? And what's the CPU & OS?\r\n\r\nSo, we could check it.", "Dear sir\uff0c \r\nthe format of resnet50 is keras HD5\u3002ok\uff0c I will share my test code for this issue\u3002\r\nTensorflow-2.4.0\r\noneDNN-1.5.0\r\n\r\nCPU\uff1amips64\r\nOS\uff1aloongnix\uff08fedora21\uff09\r\npython\uff1a3.7.9\r\nthe test code is as bellow\uff1a\r\n[\r\n[resnet50.zip](https://github.com/tensorflow/tensorflow/files/5600036/resnet50.zip)\r\n\r\n](url)\r\n\r\nTensorflow build command\uff1a\r\n**use mkl**\r\nbazel build --verbose_failures --config=noaws --config=mkl --config=opt --host_copt=-march=loongson3a --action_env=BAZEL_LINKLIBS=-l%:libstdc++.a --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**not use mkl\uff1a**\r\nbazel build --verbose_failures --config=noaws --config=opt --host_copt=-march=loongson3a --action_env=BAZEL_LINKLIBS=-l%:libstdc++.a --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\nthen \uff0c I test \uff1apython-3.7.9 resnet50.py\r\nthe result is:\r\n**resnet50 inference\uff1atensorflow-2.4.0 that use mkl is slower than tensorflow-2.4.0 which not use mkl\uff01**\r\n\r\nthan, I do the same on x86 platform, it is the same result.", "@zhangqiang-hf \r\nI see your CPU is MIPS64.\r\nTensorflow with MKL/oneDNN is optimized based on Intel CPU.\r\nWe don't know if the optimization is working on MIPS64. \r\nIt's out of the scope of Tensorflow with MKL/oneDNN.\r\n\r\nIn general, the AVX2, AVX512 intrinsic included in Intel CPU. They are used by MKL/oneDNN to speed up the TF.\r\nIf the CPU don't include them, the optimization can't work.\r\n\r\nThank you!", "> with MKL/oneDNN\r\n\r\nDear sir\uff0c\r\nthe oneDNN can used on mips64\uff0c we have done the job\u3002\r\nNow\uff0c\r\n**resnet50 inference\uff1atensorflow-2.4.0 that use mkl is slower than tensorflow-2.4.0 which not use mkl\uff01\uff0calso\uff0c it is the same phenomenon on x86 i7 platform\u3002x86 still slower when use mkl**\r\n**but\uff0c vgg16 and vgg19\uff0ctensorflow-2.4.0 that use mkl is quicker than tensorflow-2.4.0 which not use mkl\uff01**", "@zhangqiang-hf \r\nSorry, I missed to understand your words.\r\n\r\nOK, we will check it and feedback you as soon.\r\n\r\nThank you!", "> @zhangqiang-hf\r\n> Sorry, I missed to understand your words.\r\n> \r\n> OK, we will check it and feedback you as soon.\r\n> \r\n> Thank you!\r\n\r\nok\uff0cyou can use my test case\uff1aresnet50.zip to check this issue on x86 Platform :  tensorflow-2..4.0(with mkl or not with mkl) inference,compare the inference performance\u3002\r\nThank you sir\uff01\r\n", "> @zhangqiang-hf\r\n> Sorry, I missed to understand your words.\r\n> \r\n> OK, we will check it and feedback you as soon.\r\n> \r\n> Thank you!\r\n\r\nDear sir\uff0c\r\n  Could you reproduce this issue on x86 platform when use the test case\uff1f\r\nI\u2018m eager to check this issue\uff0c please give me some oppinion\uff0c thank you sir\uff01", "@zhangqiang-hf \r\nYes, it's ongoing.\r\n\r\nThank you!", "> @zhangqiang-hf\r\n> Yes, it's ongoing.\r\n> \r\n> Thank you!\r\n\r\nDear sir\uff0c\r\n  How about this issue\uff1fHave you reproduced this issue\uff1f", "@zhangqiang-hf \r\nI use your parameters to build TF for Intel CPU, there is compile error.\r\nSo, I use the default compile parameters. It's OK for non-MKL.\r\n\r\nI'm building the 2.4 with MKL now.\r\n\r\nI will update if it's finished.", "@zhangqiang-hf \r\nI share you part of progress and guide for you firstly.\r\n\r\nAfter apply the optimization setting as above suggestion, I got some result:\r\n(I got the result by your script. It's not good to test the performance. Like miss the warmup, only run 1 times per case.\r\nSo, following result maybe not stable in different test env.)\r\n\r\nWhen BS=1 (your case), the TF with MKL is a litter slower than stack TF.\r\nWhen BS=10, the performance is same.\r\nWhen BS=100, 1000, the TF with MKL is better than stack TF obviously.\r\n\r\nPS: we think when BS=1, the performance should be better too. The checking is on going.\r\n\r\nI test it with Xeon, instead of Core i7. But you can refer to the optimization setting.\r\n\r\n```\r\nexport TF_ENABLE_MKL_NATIVE_FORMAT=1  \r\nexport TF_NUM_INTEROP_THREADS=1\r\nexport TF_NUM_INTRAOP_THREADS=24\r\nexport OMP_NUM_THREADS=24\r\nexport KMP_BLOCKTIME=1\r\nexport KMP_AFFINITY=granularity=fine,compact,1,0\r\n```\r\n`TF_ENABLE_MKL_NATIVE_FORMAT` is key optimization, friendly to Keras model inference.\r\n`TF_NUM_INTRAOP_THREADS` & `OMP_NUM_THREADS` are set as the number of cores in the CPU. You could change it for better performance.\r\n`KMP_BLOCKTIME` are set 0, 1 or other number. depend on test result.\r\n\r\nWhen we optimize the Tensorflow inference, the Max Throughput & Latency should be considered in same time.\r\nIn this case, the latency of BS=1 and BS = 10 are very close. \r\nBS =1 and BS=100 are close.\r\nSo, you could increase the BS to reach good balance of performance tuning.\r\n\r\n\r\n", "Thank you sir\uff01\r\nyou means BS=100\uff1apredict 100 pictures? use for statement excute 100 times? mine is:\r\ndef predict_file_path(model, file_path):\r\n    files = os.listdir(file_path)\r\n    index = 0\r\n    start = time.time()\r\n    for f in files:\r\n        try:\r\n            predict_result(model, file_path+'/'+f)\r\n        except:\r\n            print(f)\r\n        index += 1\r\n        if index %100 == 0:\r\n            end = time.time()\r\n            print(index, '-> CPU\u6267\u884c\u65f6\u95f4(s):', end - start)\r\n\r\ncould you please share your test code for this case?", "@zhangqiang-hf \r\nFollowing is the code to create 100 images for BS=100, based on your script.\r\nBS=100 means input 100 images per time, instead of 1 image with 100 times.\r\n```\r\nnum_repeats = 100\r\nx = np.tile(x,(num_repeats, 1,1,1))\r\nprint('Input image shape:', x.shape)\r\nim = Image.open(file_path)\r\n#im.show()\r\n\r\n```\r\n", "@zhangqiang-hf \r\n\r\nI think your issue is new one for Resnet50 model.\r\nThis issue topic is LSTM.\r\n\r\nIs it possible to create a new github issue for your model?\r\nYou could @me in the new issue, so I can continue following it.\r\n\r\nThank you!", "> @zhangqiang-hf\r\n> \r\n> I think your issue is new one for Resnet50 model.\r\n> This issue topic is LSTM.\r\n> \r\n> Is it possible to create a new github issue for your model?\r\n> You could @me in the new issue, so I can continue following it.\r\n> \r\n> Thank you!\r\n\r\nok\uff01I will assign a new bug for you\uff01when one pic, why it is slower when use onednn, this  still a issue? am I right?\r\nalso, I have checked when bs=100, it is quicker when onednn. but this 100 pictures is the same, when the100 pictures are different, how about? ", "@zhangqiang-hf \r\n\r\nIt's great!\r\nCould you copy your questions in the new issue? I will answer by it.\r\n\r\nThank you!", "> @zhangqiang-hf\r\n> \r\n> It's great!\r\n> Could you copy your questions in the new issue? I will answer by it.\r\n> \r\n> Thank you!\r\nDear sir,\r\n I have assign a new issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/45289\r\n\r\ncould please continue it?", "@zhangqiang-hf \r\n\r\nYes.", "> > @zhangqiang-hf\r\n> > I think your issue is new one for Resnet50 model.\r\n> > This issue topic is LSTM.\r\n> > Is it possible to create a new github issue for your model?\r\n> > You could @me in the new issue, so I can continue following it.\r\n> > Thank you!\r\n> \r\n> ok\uff01I will assign a new bug for you\uff01when one pic, why it is slower when use onednn, this still a issue? am I right?\r\n> also, I have checked when bs=100, it is quicker when onednn. but this 100 pictures is the same, when the100 pictures are different, how about?\r\n\r\nBad performance is expected if you only run session once because TF with oneDNN will take more time in warmup (1st iteration). You will get similar performance with different pics as long as you don't change pic shape. In same word, TF with oneDNN doesn't care about data, it only cares shape.", "@zhangqiang-hf @Zantares \r\nYes. \r\nI just test to run 100 times for BS=1.\r\nThe performance of TF with MKL is better than stack TF.\r\n\r\n```\r\n    times = 100\r\n    a = time.time()    \r\n    for i in range(times):    \r\n        preds = model.predict(x)\r\n    b = time.time()\r\n    seconds = (b - a)/times\r\n    print(\"predict() cost time:\",seconds)\r\n\r\n```"]}, {"number": 33137, "title": "Fix for #33134", "body": "Updating softmax args description - logits can be of any type that can be passed to convert_to_tensor(), and associated errors. https://github.com/tensorflow/tensorflow/issues/33134", "comments": ["On the other hand, this is only changing the documentation of a deprecated v1 function. As we are living in a 2.0 world, maybe this is not as useful?\r\n\r\nCan you please make the changes to the v2 code if needed?"]}, {"number": 33136, "title": "[tf1.14][TPU] Can not save model trained on TPU using tf.saved_model.save", "body": "I want to deploy the model using TF serving, for which I require to save the model using low level API `tf.saved_model.save()`. I am using [this](https://colab.research.google.com/drive/1C2JLKR3Rn1j2cZ_z_ZrTIc36Ou2s1H0-) exact tutorial on TPU with everything default. In found this tutorial [here](https://cloud.google.com/tpu/docs/colabs). After `model.fit()` if I try to save the model using `tf.saved_model.save()`, it throws the error `AttributeError: 'TPUMirroredVariable' object has no attribute '_constraint'`\r\n\r\nHowever, this works fine with GPU strategies. Please help, or share some info. on how to deploy model which is trained on TPU.\r\n\r\nThank you,\r\nRishabh Sahrawat", "comments": ["Hi, in the first notebook you [linked](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb#scrollTo=ExQ922tfzSGA&line=20&uniqifier=1) in the 6th cell it says `training_model.save_weights('/tmp/bard.h5', overwrite=True`.\r\nWhen I run this notebook for me the saving works fine. \r\nCould you please share the code that throws the error? (Ideally as an colab notebook)", "Hello, I am sorry, I did not make any changes to it but here is the edited [file](https://colab.research.google.com/drive/1sQzyrz2zFOUVUTByqshJE__dhirxPYzd). I also updated the link in the original comment. I added two line for saving the model using `tf.saved_model.save()`. Thank you!", "I can reproduce your error. It seems that there are still incompatibilites with tensorflow 2.0 and TPU\u00b4s. Officially TPU\u00b4s only support tensorflow 1.14 [link](https://cloud.google.com/tpu/docs/supported-versions)", "Yes, I am aware of that. But I am not using TF2.0, I am using TF1.14. I also mentioned this in the title.\r\nI hope there is a solution, otherwise, what is the meaning of training models on TPUs if they can't be deployed. :(", "Hello @oanush , @gowthamkpr , is there anyone looking on this issue?. Please update!\r\nThank you.", "It looks like a bug. Thanks for reporting @rishabhsahrawat ", "Hello @jhseu , have we fixed the bug? Thanks.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33136\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33136\">No</a>\n"]}]