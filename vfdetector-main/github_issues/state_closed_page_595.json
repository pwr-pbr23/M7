[{"number": 35823, "title": "keras.models.Model.fit_generator returns  \"RuntimeError: You must compile your model before using it.\" even specifying all the input shapes", "body": "**System information**\r\n- Python 3.7\r\n- Keras 2.2.4\r\n\r\n** Issue **\r\n\r\nWhen using `fit_generator` in `keras.models.Model` returns `RuntimeError: You must compile your model before using it.` \r\n\r\nFollowing the reply [here](https://stackoverflow.com/questions/52721018/keras-fit-generator-raise-you-must-compile-your-model-before-using-it-error), I specified `input_shape` (and also `batch_shape`) both in `Input` layer and first `Conv1D` layer. But this didn't solve the error.\r\n\r\nI don't know if it is a related issue, but manually generating one batch of data and using `model.fit` raise `AttributeError: 'Model' object has no attribute '_output_tensor_cache'` which looks even more suspicious\r\n\r\n** Minimal working example **\r\n```\r\nfrom keras.utils import Sequence\r\n\r\nfrom keras.models import Model, Sequential\r\nfrom keras.layers import Input, Conv1D, Dense, Flatten\r\n\r\nimport numpy.random\r\n\r\ndef create_model():\r\n\r\n    input_layer = Input(shape=(60,1), batch_shape=(50,60,1))\r\n    out = Conv1D(64, 3, activation='relu', input_shape=(60, 1))(input_layer)\r\n    out = Flatten()(out)\r\n    out = Dense(1)(out)\r\n\r\n    #create model\r\n    model = Model()\r\n    model.compile(inputs    = input_layer,\r\n                  outputs   = out,\r\n                  loss      = 'mse',\r\n                  optimizer = 'adam',\r\n                  )\r\n\r\n    return model\r\n\r\nclass examples_generator(Sequence):\r\n\r\n    def __getitem__(self, _):\r\n        batch  = numpy.random.rand(50, 60, 1)\r\n        target = numpy.random.rand(50, 1)\r\n        return (batch, target)\r\n\r\n    def __len__(self):\r\n        return 2\r\n\r\nif __name__ == '__main__':\r\n    model = create_model()\r\n    training_batch_generator = examples_generator()\r\n\r\n    # This will generate RuntimeError: You must compile your model before using it.\r\n    model.fit_generator(training_batch_generator,\r\n                        steps_per_epoch=1,\r\n                        epochs=1,\r\n                        verbose=2)\r\n\r\n    one_batch = training_batch_generator[None]\r\n     # This will generate AttributeError: 'Model' object has no attribute '_output_tensor_cache'\r\n    model.fit(one_batch[0], one_batch[1])\r\n```\r\n\r\nUsing an identical model created using `keras.models.Sequential` works perfectly instead. Defining:\r\n\r\n```\r\ndef create_sequential_model():\r\n\r\n    #create model\r\n    model = Sequential()\r\n    model.add(Conv1D(64, 3, activation='relu', input_shape=(60, 1)))\r\n    model.add(Flatten())\r\n    model.add(Dense(1))\r\n    model.compile(loss      = 'mse',\r\n                  optimizer = 'adam',\r\n                  )\r\n\r\n    return model\r\n\r\nmodel = create_sequential_model()\r\n\r\n# Works\r\nmodel.fit_generator(training_batch_generator,\r\n                                steps_per_epoch=100,\r\n                                epochs=3,\r\n                                verbose=2)\r\n\r\n# Works\r\none_batch = training_batch_generator[None]\r\nmodel.fit(one_batch[0], one_batch[1])\r\n```\r\nand replacing `create_model` with `create_sequential_model` in the example above.\r\n\r\n\r\n", "comments": ["@pj1989 I am running into a different error. Please find my gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/9845da51bda0e78f94c142ace16adda3/untitled276.ipynb). Also please use Keras that embedded in Tensorflow as keras has been integrated into Tensorflow.\r\n\r\nThanks!", "Using `tensorflow.keras` indeed ~~solved~~ **changed** the error. \r\n\r\nI'm also getting your same error.\r\nI tried to understand the causes, but it's not completely straight forward.\r\n\r\nWhat I understood so far:\r\n\r\n- The error is caused on line 3047 of training.py by the fact that `self.output` is `None` (we are inside the `model` class, so `self` should be our model as far as I understand)\r\n\r\nTraceback:\r\n```\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1297, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\", line 265, in model_iteration\r\n    batch_outs = batch_function(*batch_data)\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 973, in train_on_batch\r\n    class_weight=class_weight, reset_metrics=reset_metrics)\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 253, in train_on_batch\r\n    extract_tensors_from_dataset=True)\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2433, in _standardize_user_data\r\n    self._compile_from_inputs(all_inputs, y_input, x, y)\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2668, in _compile_from_inputs\r\n    experimental_run_tf_function=self._experimental_run_tf_function)\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 343, in compile\r\n    endpoint.create_training_target(t, run_eagerly=self.run_eagerly)\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 3038, in create_training_target\r\n    self.loss_fn, K.dtype(self.output))\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 1249, in dtype\r\n    return x.dtype.base_dtype.name\r\n\r\nAttributeError: 'NoneType' object has no attribute 'dtype'\r\n```\r\n\r\n- On TF documentation ([here](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit_generator)) `fit_generator` is marked as **deprecated** (the same is not true on Keras website ([here](https://keras.io/models/model/#fit_generator)).  Switching from `fit_generator` to `fit` changes the traceback and the line where the error arises, but not the error itself that is always generated by `outputs` being `None`.\r\n\r\nTraceback:\r\n```\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 224, in fit\r\n    distribution_strategy=strategy)\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 547, in _process_training_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 610, in _process_inputs\r\n    training_v2_utils._prepare_model_with_inputs(model, adapter.get_dataset())\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 190, in _prepare_model_with_inputs\r\n    model._compile_from_inputs(inputs, target, dataset, None)\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2630, in _compile_from_inputs\r\n    target, self.outputs)\r\n\r\n  File \"C:\\Users\\Luca.amerio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\", line 1242, in cast_if_floating_dtype_and_mismatch\r\n    return cast_single_tensor(targets, dtype=outputs[0].dtype)\r\n\r\nAttributeError: 'NoneType' object has no attribute 'dtype'\r\n\r\n```\r\n\r\n- I tried to pinpoint the line where `outputs` is set to None. However, due to the classes inheritances, I failed this task. I know it is set 2 times: one during `model = Model()` where it is set to `[]`, and one more time that I could not identify (probably during `model.compile()`?)\r\n\r\n- I checked the syntax in my script, but I could not identify any clear syntax error. I suspect this to be a **bug in Keras** (happy to be contradicted)", "@pj1989 the issue is with these lines:\r\n\r\n```\r\n #create model\r\n    model = Model()\r\n    model.compile(inputs    = input_layer,\r\n                  outputs   = out,\r\n                  loss      = 'mse',\r\n                  optimizer = 'adam',\r\n                  )\r\n\r\n    return model\r\n```\r\n\r\nit should be:\r\n\r\n```python\r\n #create model\r\n    model = Model(input_layer, out)\r\n    model.compile(\r\n                  loss      = 'mse',\r\n                  optimizer = 'adam',\r\n                  )\r\n\r\n    return model\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35823\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35823\">No</a>\n"]}, {"number": 35822, "title": " tf.py_function eager execution causes memory leak", "body": "## Description:\r\nThe eager execution is a python function that loads a wave file\r\n\r\n```python\r\n    def safe_load(self, path, offset, duration, sample_rate, dtype):\r\n        get_logger().info(\r\n            f'Loading audio {path} from {offset} to {offset + duration}')\r\n        try:\r\n            (data, _) = self.load(\r\n                path.numpy(),\r\n                offset.numpy(),\r\n                duration.numpy(),\r\n                sample_rate.numpy(),\r\n                dtype=dtype.numpy())\r\n            return (data, False)\r\n        except Exception as e:\r\n            get_logger().warning(e)\r\n        return (np.float32(-1.0), True)\r\n```\r\n\r\nand\r\n\r\n```python\r\nresults = tf.py_function(\r\n                    func=self.safe_load,\r\n                    inp=[audio_descriptor, offset, duration, sample_rate, dtype],\r\n                    Tout=(tf.float32, tf.bool)),\r\n                waveform, error = results[0]\r\n````\r\n\r\nThe memory leak happens at every call to `estimator.predict`:\r\n\r\n```python\r\nwith session.as_default():\r\n        with session.graph.as_default():\r\n            prediction = estimator.predict(\r\n                lambda: get_dataset(\r\n                    audio_adapter,\r\n                    filenames_and_crops,\r\n                    sample_rate,\r\n                    n_channels, session),\r\n                yield_single_examples=False)\r\n```\r\n\r\n## System information \r\n- Custom library: https://github.com/deezer/spleeter/issues/229\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow version: 1.14\r\n- Python version: 3.7.4\r\n\r\n## Stacktrace\r\n```\r\n{\r\n\t\"message\": {\r\n\t\t\"traceback\": [{\r\n\t\t\t\t\"memory\": 2925,\r\n\t\t\t\t\"blocks\": 29192,\r\n\t\t\t\t\"stack\": [\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\\\", line 246\",\r\n\t\t\t\t\t\"    allow_broadcast=True)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\\\", line 290\",\r\n\t\t\t\t\t\"    name=name).outputs[0]\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\\\", line 507\",\r\n\t\t\t\t\t\"    return func(*args, **kwargs)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 3616\",\r\n\t\t\t\t\t\"    op_def=op_def)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 2005\",\r\n\t\t\t\t\t\"    self._traceback = tf_stack.extract_stack()\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_stack.py\\\", line 64\",\r\n\t\t\t\t\t\"    ret.append((filename, lineno, name, frame_globals, func_start_lineno))\"\r\n\t\t\t\t]\r\n\t\t\t},\r\n\t\t\t{\r\n\t\t\t\t\"memory\": 2756,\r\n\t\t\t\t\"blocks\": 12,\r\n\t\t\t\t\"stack\": [\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 1145\",\r\n\t\t\t\t\t\"    as_ref=False)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 1224\",\r\n\t\t\t\t\t\"    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\\\", line 305\",\r\n\t\t\t\t\t\"    return constant(v, dtype=dtype, name=name)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\\\", line 246\",\r\n\t\t\t\t\t\"    allow_broadcast=True)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\\\", line 254\",\r\n\t\t\t\t\t\"    t = convert_to_eager_tensor(value, ctx, dtype)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\\\", line 115\",\r\n\t\t\t\t\t\"    return ops.EagerTensor(value, handle, device, dtype)\"\r\n\t\t\t\t]\r\n\t\t\t},\r\n\t\t\t{\r\n\t\t\t\t\"memory\": 2188,\r\n\t\t\t\t\"blocks\": 21506,\r\n\t\t\t\t\"stack\": [\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/error_interpolation.py\\\", line 319\",\r\n\t\t\t\t\t\"    for frame in op.traceback:\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 2568\",\r\n\t\t\t\t\t\"    return tf_stack.convert_stack(self._traceback)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_stack.py\\\", line 123\",\r\n\t\t\t\t\t\"    line = linecache.getline(filename, lineno, frame_globals)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/linecache.py\\\", line 16\",\r\n\t\t\t\t\t\"    lines = getlines(filename, module_globals)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/linecache.py\\\", line 47\",\r\n\t\t\t\t\t\"    return updatecache(filename, module_globals)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/linecache.py\\\", line 137\",\r\n\t\t\t\t\t\"    lines = fp.readlines()\"\r\n\t\t\t\t]\r\n\t\t\t},\r\n\t\t\t{\r\n\t\t\t\t\"memory\": 1715,\r\n\t\t\t\t\"blocks\": 19490,\r\n\t\t\t\t\"stack\": [\r\n\t\t\t\t\t\"  File \\\"<frozen importlib._bootstrap>\\\", line 983\",\r\n\t\t\t\t\t\"  File \\\"<frozen importlib._bootstrap>\\\", line 967\",\r\n\t\t\t\t\t\"  File \\\"<frozen importlib._bootstrap>\\\", line 677\",\r\n\t\t\t\t\t\"  File \\\"<frozen importlib._bootstrap_external>\\\", line 724\",\r\n\t\t\t\t\t\"  File \\\"<frozen importlib._bootstrap_external>\\\", line 857\",\r\n\t\t\t\t\t\"  File \\\"<frozen importlib._bootstrap_external>\\\", line 525\"\r\n\t\t\t\t]\r\n\t\t\t},\r\n\t\t\t{\r\n\t\t\t\t\"memory\": 1620,\r\n\t\t\t\t\"blocks\": 16132,\r\n\t\t\t\t\"stack\": [\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\\\", line 587\",\r\n\t\t\t\t\t\"    \\\"ReadVariableOp\\\", resource=resource, dtype=dtype, name=name)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\\\", line 788\",\r\n\t\t\t\t\t\"    op_def=op_def)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\\\", line 507\",\r\n\t\t\t\t\t\"    return func(*args, **kwargs)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 3616\",\r\n\t\t\t\t\t\"    op_def=op_def)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 2005\",\r\n\t\t\t\t\t\"    self._traceback = tf_stack.extract_stack()\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_stack.py\\\", line 64\",\r\n\t\t\t\t\t\"    ret.append((filename, lineno, name, frame_globals, func_start_lineno))\"\r\n\t\t\t\t]\r\n\t\t\t},\r\n\t\t\t{\r\n\t\t\t\t\"memory\": 1370,\r\n\t\t\t\t\"blocks\": 13764,\r\n\t\t\t\t\"stack\": [\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\\\", line 1503\",\r\n\t\t\t\t\t\"    \\\"VarIsInitializedOp\\\", resource=resource, name=name)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\\\", line 788\",\r\n\t\t\t\t\t\"    op_def=op_def)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\\\", line 507\",\r\n\t\t\t\t\t\"    return func(*args, **kwargs)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 3616\",\r\n\t\t\t\t\t\"    op_def=op_def)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 2005\",\r\n\t\t\t\t\t\"    self._traceback = tf_stack.extract_stack()\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_stack.py\\\", line 64\",\r\n\t\t\t\t\t\"    ret.append((filename, lineno, name, frame_globals, func_start_lineno))\"\r\n\t\t\t\t]\r\n\t\t\t},\r\n\t\t\t{\r\n\t\t\t\t\"memory\": 1148,\r\n\t\t\t\t\"blocks\": 11418,\r\n\t\t\t\t\"stack\": [\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_control_flow_ops.py\\\", line 935\",\r\n\t\t\t\t\t\"    \\\"Switch\\\", data=data, pred=pred, name=name)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\\\", line 788\",\r\n\t\t\t\t\t\"    op_def=op_def)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\\\", line 507\",\r\n\t\t\t\t\t\"    return func(*args, **kwargs)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 3616\",\r\n\t\t\t\t\t\"    op_def=op_def)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 2005\",\r\n\t\t\t\t\t\"    self._traceback = tf_stack.extract_stack()\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_stack.py\\\", line 64\",\r\n\t\t\t\t\t\"    ret.append((filename, lineno, name, frame_globals, func_start_lineno))\"\r\n\t\t\t\t]\r\n\t\t\t},\r\n\t\t\t{\r\n\t\t\t\t\"memory\": 1140,\r\n\t\t\t\t\"blocks\": 5830,\r\n\t\t\t\t\"stack\": [\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\\\", line 788\",\r\n\t\t\t\t\t\"    op_def=op_def)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\\\", line 507\",\r\n\t\t\t\t\t\"    return func(*args, **kwargs)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 3616\",\r\n\t\t\t\t\t\"    op_def=op_def)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 2037\",\r\n\t\t\t\t\t\"    for i, output_type in enumerate(output_types)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 2037\",\r\n\t\t\t\t\t\"    for i, output_type in enumerate(output_types)\",\r\n\t\t\t\t\t\"  File \\\"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\\\", line 357\",\r\n\t\t\t\t\t\"    self._consumers = []\"\r\n\t\t\t\t]\r\n\t\t\t}\r\n\t\t]\r\n\t}\r\n}\r\n```\r\n\r\n## Discussion:\r\nUsing as a workaround:\r\n\r\n```python\r\ntf.reset_default_graph()\r\ntf.keras.backend.clear_session()\r\n```\r\n\r\nwe had memory leak decreasing from the whole variables retained in the python function (a `data` holding the wave file ~24MB), to a smaller fraction of ~600-700KB, but formally the leak is still there.\r\n\r\nPossibile duplicate of https://github.com/tensorflow/tensorflow/issues/35010 \r\nbut related to TF 1.14\r\n\r\n", "comments": ["Thanks for the issue. This is being tracked in the thread you mentioned above and is on TF 2.3 roadmap. Closing this issue as we are tracking it on #35010", "@ymodak thank you closing then."]}, {"number": 35821, "title": "Fix tf.range failure when `limit` is type of `tf.int32` and `dtype` is `tf.int64`", "body": "This PR tries to address the issue raised in #35710 where tf.range fails when `limit` is type of `tf.int32` and `dtype` is `tf.int64`.\r\n\r\nThe failure is a regression between TF 2.0.0 and 2.1.0\r\n\r\nThis fix adds additional cast to resolve the issue.\r\n\r\nThis fix fixes #35710.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Right, but this case should be an error; tf ops are not supposed to do\nsilent type promotion.\n\nOn Mon, Jan 13, 2020 at 12:57 PM Yong Tang <notifications@github.com> wrote:\n\n> *@yongtang* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/ops/math_ops.py\n> <https://github.com/tensorflow/tensorflow/pull/35821#discussion_r366023997>\n> :\n>\n> > @@ -1487,9 +1487,12 @@ def range(start, limit=None, delta=1, dtype=None, name=\"range\"):  # pylint: disa\n>      start, limit = 0, start\n>\n>    with ops.name_scope(name, \"Range\", [start, limit, delta]) as name:\n> -    start = ops.convert_to_tensor(start, dtype=dtype, name=\"start\")\n> -    limit = ops.convert_to_tensor(limit, dtype=dtype, name=\"limit\")\n> -    delta = ops.convert_to_tensor(delta, dtype=dtype, name=\"delta\")\n> +    if not isinstance(start, ops.Tensor):\n> +      start = ops.convert_to_tensor(start, dtype=dtype, name=\"start\")\n> +    if not isinstance(limit, ops.Tensor):\n> +      limit = ops.convert_to_tensor(limit, dtype=dtype, name=\"limit\")\n> +    if not isinstance(delta, ops.Tensor):\n> +      delta = ops.convert_to_tensor(delta, dtype=dtype, name=\"delta\")\n>\n> Thanks @alextp <https://github.com/alextp> for the review. In #35710\n> <https://github.com/tensorflow/tensorflow/issues/35710> the case was that\n> the dtype passed along with tf.range(start, limit, delta, dtype) is\n> different from the tensor of the start/limit/delta. In other words, the\n> following:\n>\n> tf.range(tf.constant(4, dtype=tf.int32), dtype=tf.int64)\n>\n> In that case, convert_to_tensor will try to convert tf.constant(4,\n> dtype=tf.int32) to dtype=tf.int64 and that is returning an error. That is\n> the issue from #35710\n> <https://github.com/tensorflow/tensorflow/issues/35710> .\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/35821?email_source=notifications&email_token=AAABHRO42RDZFWIHLQJVZZTQ5TIUBA5CNFSM4KGEOJB2YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCRSIVXI#discussion_r366023997>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRITXW4FHYM7UMJ5LCDQ5TIUBANCNFSM4KGEOJBQ>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp The issue #35710 was raised because\r\n```\r\ntf.range(tf.constant(4, dtype=tf.int32), dtype=tf.int64)\r\n```\r\n\r\nused to work in TF 2.0.0 but not 2.1.0, thus considered as a regression.\r\n\r\nIf the behavior is working-as-intended, maybe we could add a note in docstring to mention that `start`,`limit`, `delta` should have the same `dtype` as passed `dtype` for tf.range itself?"]}, {"number": 35820, "title": "Same op sequence is computed multiple times", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Not applicable\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: Not applicable\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Not applicable\r\n- GPU model and memory: Not applicable\r\n\r\n**Describe the current behavior**\r\nWhen applying a layer multiple times to the same input, it seems TF is computing it for every call, even if it's unnecessary, see colab:\r\nhttps://colab.research.google.com/drive/1PTvolVt1xQJvgp4MP0ZvgbemKBrqWy0U\r\n\r\n**Describe the expected behavior**\r\nThe duplicate parts of the graph should be squashed\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1PTvolVt1xQJvgp4MP0ZvgbemKBrqWy0U\r\n", "comments": ["Jit vs no-jit seems to make very little difference:\r\nhttps://colab.research.google.com/drive/1RtCXNSCgCLap1e4mlAbd-3hyD4zvufBZ", "I was able to reproduce your issue in Tf-Nightly , please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/ce4def258ad61c0ff30ebbc31930e82e/35650.ipynb).", "I was able to reproduce your issue in Tf 2.6 , please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/e31dd1d5169bcf5ddd38a81e69e03876/35650.ipynb#scrollTo=c3LN0qQa_b4E).", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35820\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35820\">No</a>\n"]}, {"number": 35819, "title": "AutoGraph could not transform ... (Bad argument number for Name: 3, expecting 4)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows, Linux\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.6\r\n\r\n**Describe the current behavior**\r\nWarning said to report to TensorFlow team, see below.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ndef test(data):\r\n    tf.cond(\r\n        tf.less(tf.random.uniform((), maxval=2), 1),\r\n        lambda: 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0,\r\n        lambda: 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0,\r\n    )\r\n    return data\r\n\r\ntf.data.Dataset.from_tensor_slices([[[0]]]).map(test)\r\n```\r\n**Other info / logs**\r\n```\r\n2020-01-13 16:01:26.037936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-13 16:01:28.948415: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-01-13 16:01:29.117581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:02:00.0 name: Quadro M5000 computeCapability: 5.2\r\ncoreClock: 1.038GHz coreCount: 16 deviceMemorySize: 7.50GiB deviceMemoryBandwidth: 196.99GiB/s\r\n2020-01-13 16:01:29.132706: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-13 16:01:29.150465: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-13 16:01:29.169042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-13 16:01:29.177916: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-13 16:01:29.197289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-13 16:01:29.210804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-13 16:01:29.230095: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-13 16:01:29.238015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-01-13 16:01:29.243127: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-01-13 16:01:29.256487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:02:00.0 name: Quadro M5000 computeCapability: 5.2\r\ncoreClock: 1.038GHz coreCount: 16 deviceMemorySize: 7.50GiB deviceMemoryBandwidth: 196.99GiB/s\r\n2020-01-13 16:01:29.269781: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-13 16:01:29.275906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-13 16:01:29.285202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-13 16:01:29.291266: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-13 16:01:29.299261: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-13 16:01:29.304025: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-13 16:01:29.311939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-13 16:01:29.316594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-01-13 16:01:30.029865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-13 16:01:30.036285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-01-13 16:01:30.040429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-01-13 16:01:30.045089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6016 MB memory) -> physical GPU (device: 0, name: Quadro M5000, pci bus id: 0000:02:00.0, compute capability: 5.2)\r\nWARNING:tensorflow:AutoGraph could not transform <function test at 0x000001977DE52438> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\n```", "comments": ["I believe this was related to having `gast==0.3.2` installed.", "Having `gast==0.3.2` installed (by accident, by trying out `tf-nightly`) can cause a host of AutoGraph-related errors. Here's another example - maybe this is helpful to someone:\r\n\r\n> OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph is disabled in this function. Try decorating it directly with @tf.function.\r\n```\r\nimport tensorflow as tf\r\n\r\ndef test(data):\r\n    if tf.random.uniform(()) < 0.5:\r\n        pass\r\n    return data\r\n\r\ntf.data.Dataset.from_tensor_slices([[[0]]]).map(test)\r\n```\r\n(And using `@tf.function` does not help, either.)\r\nThis is also fixed by `pip install gast==0.2.2`  for TF2.1."]}, {"number": 35818, "title": "Creating outputs from the final_state returned by dynamic_rnn causes conversion to fail", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): r1.14\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('./savedmodel')\r\ntflite_model = converter.convert()\r\nopen(\"lstm.tflite\", \"wb\").write(tflite_model)\r\n\r\n**The output from the converter invocation**\r\n\r\nTraceback (most recent call last):\r\n  File \"saved2lite.py\", line 4, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/workdir/env-cpu/lib/python2.7/site-packages/tensorflow/lite/python/lite.py\", line 898, in convert\r\n    **converter_kwargs)\r\n  File \"/workdir/env-cpu/lib/python2.7/site-packages/tensorflow/lite/python/convert.py\", line 404, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/workdir/env-cpu/lib/python2.7/site-packages/tensorflow/lite/python/convert.py\", line 172, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2020-01-13 14:56:27.199876: F tensorflow/lite/toco/tooling_util.cc:918] Check failed: GetOpWithOutput(model, output_array) Specified output array \"lstm/body/encoder/rnn/while/Identity_4\" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.\r\n\r\n\r\n**Failure details**\r\n\r\nModel conversion fails when final_state is returned as an output\r\n\r\n\r\n**Any other info / logs**\r\n\r\nrelevant code in model definition:\r\n\r\noutput, next_state = tf.compat.v1.lite.experimental.nn.dynamic_rnn(...)\r\nc_out = next_state[0].c\r\nh_out = next_state[0].h\r\n\r\nrelevant code in conversion to SavedModel (which does work):\r\n\r\ntensor_info_c_out = tf.saved_model.utils.build_tensor_info(c_out)\r\ntensor_info_h_out = tf.saved_model.utils.build_tensor_info(h_out)\r\n\r\nprediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs = {'x': x}, outputs = {'y': output, 'c_out': c_out, 'h_out':h_out), method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\r\n\r\n\r\nThe TFLite conversion works if I don't return the next state variables, c_out and h_out, but I need those to run the next step of the prediction.\r\n\r\nIf I run the conversion with tfnightly (using the same SavedModel file saved using 1.14) I get a different, seemingly unrelated, error:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, FULLY_CONNECTED, GATHER, MUL, NOT_EQUAL, RESHAPE, SOFTMAX, SQUEEZE, TOPK_V2. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.", "comments": ["Can you try using the new experimental converter on the `tf-nightly` or on 2.1 branch which has more comprehensive support for `dynamic_rnn`. You can enable the experimental converter by setting `converter.experimental_new_converter = True` when converting the model. The code will look something like the following:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter(saved_model_dir)\r\nconverter.experimental_new_converter = True\r\nconverter.convert()\r\n```", "Hi gargn, thanks for taking the time to respond, much appreciated.\r\n\r\nI installed tf-nightly-cpu, with tf.__version__: 2.1.0-dev20200102 and enabled the new converter.\r\n\r\nI now see different error, but it's not clear to me exactly what the error is this time:\r\n\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'lstm/symbol_modality_4099_512/target_emb/weights_0:0' shape=(4099, 512) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'lstm/symbol_modality_4099_512/softmax/weights_0:0' shape=(4099, 512) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'lstm/symbol_modality_4099_512/target_emb/weights_0:0' shape=(4099, 512) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'lstm/symbol_modality_4099_512/softmax/weights_0:0' shape=(4099, 512) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\n2020-01-14 08:53:56.676776: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-01-14 08:53:56.676908: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-01-14 08:53:57.025281: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize\r\n2020-01-14 08:53:57.025320: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 65 nodes (-37), 66 edges (-39), time = 209.377ms.\r\n2020-01-14 08:53:57.025331: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 65 nodes (0), 66 edges (0), time = 62.395ms.\r\n2020-01-14 08:53:57.025342: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: __inference_lstm_body_encoder_rnn_while_body_226_118_frozen\r\n2020-01-14 08:53:57.025353: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 109 nodes (-1), 110 edges (-2), time = 4.845ms.\r\n2020-01-14 08:53:57.025364: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 109 nodes (0), 110 edges (0), time = 2.665ms.\r\n2020-01-14 08:53:57.025374: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: __inference_lstm_body_encoder_rnn_while_cond_225_27_frozen\r\n2020-01-14 08:53:57.025385: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 25 nodes (0), 7 edges (0), time = 0.522ms.\r\n2020-01-14 08:53:57.025395: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 25 nodes (0), 7 edges (0), time = 0.458ms.\r\nTraceback (most recent call last):\r\n  File \"saved2lite.py\", line 5, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/workdir/exp/lstm/tf-nightly-env/lib/python2.7/site-packages/tensorflow_core/lite/python/lite.py\", line 490, in convert\r\n    **converter_kwargs)\r\n  File \"/workdir/exp/lstm/tf-nightly-env/lib/python2.7/site-packages/tensorflow_core/lite/python/convert.py\", line 476, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/workdir/exp/lstm/tf-nightly-env/lib/python2.7/site-packages/tensorflow_core/lite/python/convert.py\", line 215, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\nWARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .\r\n2020-01-14 08:53:59.632245: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:108] Ignored output_format.\r\n2020-01-14 08:53:59.632389: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:114] Ignored drop_control_dependency.\r\n2020-01-14 08:53:59.768696: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2593685000 Hz\r\n2020-01-14 08:53:59.771053: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x57b6950 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-14 08:53:59.771071: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n\r\nThe return code of the process called on line 201 of tensorflow_core/lite/python/convert.py is -11 if that helps.\r\n\r\n--\r\n\r\nI tried backing off to the conversion that works in 1.14 (without returning the LSTM state) and see a different error with the new converter in tf-nightly:\r\n\r\nerror: Convert ophint failed, malformed inputs or outputs.\r\nTraceback (most recent call last):\r\n  File \"/workdir/exp/lstm/tf-nightly-env/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/workdir/exp/lstm/tf-nightly-env/lib/python2.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/workdir/exp/lstm/tf-nightly-env/lib/python2.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/workdir/exp/lstm/tf-nightly-env/lib/python2.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/workdir/exp/lstm/tf-nightly-env/lib/python2.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/workdir/exp/lstm/tf-nightly-env/lib/python2.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: <unknown>:0: error: Convert ophint failed, malformed inputs or outputs.\r\n\r\nThis example works in 1.14, which leads me to suspect that there may be an incompatibility between SavedModel files created with 1.14 and the tflite converter in tf-nightly?", "@renjie-liu Any insight into this error?", "yeah, the final_state is not supported for the fused lstm op.\r\n\r\nCan you use normal keras lstm instead? please see the example here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb", "Thanks, I'm able to run that example.\r\n\r\nMy end-goal is to have an LSTM that I can run in streaming mode using TFLite. That example runs offline, but is it possible to return the state from the LSTM such that the LSTM can be fed one sample at a time (with the corresponding previous state) to give equivalent results to the offline mode?\r\n\r\nIs there any documentation as to how this could be achieved, and if it is possible, is there a similar solution that would work with the non-eager execution workflow (i.e. TF1.14).\r\n\r\nThanks again for your help", "To answer my own question (and help anyone that may be looking for the same info), I think I'm at least partially there after familiarising myself a little with the Keras layers.\r\n\r\nI modified the code as follows (maybe this is possible also within the tf.keras.models.Sequential function but I haven't looked in detail at that):\r\n\r\ninputs = tf.keras.layers.Input(shape=(28, 28), name='input')\r\noutput_lstm, **state_h, state_c** = tf.keras.layers.LSTM(20, **return_state=True**, name='encoder')(inputs)\r\noutput_flatten = tf.keras.layers.Flatten()(output_lstm)\r\noutput = tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')(output_flatten)\r\n\r\nmodel = tf.keras.Model([inputs], [output])\r\n**model2 = tf.keras.Model([inputs], [output, state_h, state_c])**\r\n\r\n...\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(**model2**)\r\n\r\ngives a TFLite model that provides the output class distribution and c/h elements of the LSTM state. Next step is to feed these back in to the model, but that looks to be the easier part.\r\n\r\n", "great, looks like you got an answer already! :)"]}, {"number": 35817, "title": "LayerNormalization dtype issues with mixed_precision.Policy('mixed_float16')", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab notebook\r\n- TensorFlow version (use command below): tf-nightly (2.2.0-dev20200113)\r\n- GPU model and memory: GPU 0: Tesla T4\r\n\r\nI'm making small edits to the colab notebook here: https://www.tensorflow.org/guide/keras/mixed_precision\r\n\r\nWhen using mixed precision computation via \r\n```python\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_policy(policy)\r\n```\r\n\r\nLayerNormalization doesn't appear to work\r\n```python\r\ninputs = keras.Input(shape=(784,), name='digits')\r\nif tf.config.list_physical_devices('GPU'):\r\n  print('The model will run with 4096 units on a GPU')\r\n  num_units = 4096\r\nelse:\r\n  # Use fewer units on CPUs so the model finishes in a reasonable amount of time\r\n  print('The model will run with 64 units on a CPU')\r\n  num_units = 64\r\ndense1 = layers.Dense(num_units, activation='relu', name='dense_1')\r\nx = dense1(inputs)\r\ndense2 = layers.Dense(num_units, activation='relu', name='dense_2')\r\nx = dense2(x)\r\nlayer_norm = layers.LayerNormalization()\r\nx = layer_norm(x)\r\n```\r\n\r\n```\r\nThe model will run with 4096 units on a GPU\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-7e586313e764> in <module>()\r\n     12 x = dense2(x)\r\n     13 layer_norm = layers.LayerNormalization()\r\n---> 14 x = layer_norm(x)\r\n\r\n5 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)\r\n     59           \"allowed values: %s\" %\r\n     60           (param_name, dtypes.as_dtype(dtype).name,\r\n---> 61            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\n     62 \r\n     63 \r\n\r\nTypeError: Value passed to parameter 'scale' has DataType float16 not in list of allowed values: float32\r\n```\r\n\r\nReplacing LayerNormalization with BatchNormalization works without issue.\r\n\r\n\r\n\r\n", "comments": ["Issue replicating for [tf-nightly ](https://colab.sandbox.google.com/gist/oanush/2987caecb765c20f78aa6f7d45d1b73c/35817.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35817\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35817\">No</a>\n"]}, {"number": 35815, "title": "model.fit outputs an extra \"Epoch 1/x\" for each epoch", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): see below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Linux\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.6\r\n\r\n**Describe the current behavior**\r\n```\r\nEpoch 1/2\r\nEpoch 1/2\r\n1/1 [==============================] - 0s 13ms/step - loss: 0.0000e+00\r\n1/1 [==============================] - 0s 481ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\r\nEpoch 2/2\r\nEpoch 1/2\r\n1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00\r\n1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\r\n```\r\n\r\n**Describe the expected behavior**\r\n\"Epoch 1/2\" should not be in each epoch.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ndata = [[1]]\r\ndataset = tf.data.Dataset.from_tensor_slices(data)\r\ninput_target = tf.data.Dataset.zip((dataset, dataset))\r\nlayer = tf.keras.layers.Input(shape=())\r\nmodel = tf.keras.models.Model(inputs=layer, outputs=layer)\r\nmodel.compile(optimizer=\"adam\", loss=\"mse\", experimental_run_tf_function=False)\r\nmodel.fit(x=input_target, validation_data=input_target, epochs=2)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This seems fixed in tf-nightly-2.2.0.dev20200112.", "@bersbersbers ,\r\nThats right, issue seems to be fixed in tf-nightly. [Colab Gist](https://colab.sandbox.google.com/gist/oanush/0c6b2dd594b3cfbbce82eb62ab01a2eb/35815.ipynb) for reference.", "@bersbersbers ,\r\nAny update on the issue ?Thanks!", "Not really, it's still and issue in 2.1, and I have no idea whether to expect that that this will be fixed in TF 2.1.x (or TF 2.2, for that matter).", "@bersbersbers Yes, This is still an issue with `TF2.1`. It was update recently in `tf-nightly`. If you want to test, please install `tf-nightly` and test. Otherwise, there will be stable `TF2.2` version released in the near future. Thanks!\r\n\r\nI am closing as it is resolved. Thanks!"]}, {"number": 35814, "title": "Failed to load the native TensorFlow runtime. even though I have specified all paths and everything", "body": "**System information**\r\n- OS: Windows 10\r\n- TensorFlow version: 2.1\r\n- Python version: 3.7.0\r\n- Installed using pip\r\n- CUDA version: 10.1\r\n- cuDNN version: v7.6.5.32\r\n- GPU model and memory: GTX 1080 Ti\r\n\r\n**Describe the problem**\r\nHello I tried installing tensorflow using the following command - \r\n```\r\npip --no-cache-dir install tensorflow\r\n```\r\nI am using no cache dir as I tried installing tensorflow many times and all of them failed. So i reinstalled python 4 times and then finally I used this command. It installed tensorflow.\r\nBut now  when I do - \r\n```\r\n>>> import tensorflow\r\n```\r\nIt gives me the following error - \r\n\r\nhttps://pastebin.com/T9niAwFt\r\n\r\nSo now I have installed everything and even added python to path and also the cuda and cudnn. But please note last time I installed python I didn't add it to path so I added it so it made 2 paths when I reinstalled python and they got duplicated and then I deleted the duplicated ones. And now here are my path variables - \r\n![image](https://user-images.githubusercontent.com/30827615/72258042-12829080-3633-11ea-8f3f-057141dbbceb.png)\r\nAnd by the way I am using vs code 2017 and I have everything installed and also by the way I am not using any venv as I want to acess tensorflow without having to activate any environment and writing lines to activate every time I wanna acess tensorflow.", "comments": ["I guess your problem is because of CUDA 10.2 which is not yet supported for v2.1.0\r\nRefer the [GPU Guide](https://www.tensorflow.org/install/gpu), it is mentioned that \"TensorFlow supports CUDA 10.1 (TensorFlow >= 2.1.0)\". ", "@adityaa30 New update I did 10.1 now and it still doesent work\r\n", "@rajcivils \r\n\r\nJust to verify did you follow the instructions for installing GPU for [windows ](https://www.tensorflow.org/install/gpu#windows_setup).\r\nWhat is make/model of your cpu?. Does your cpu supports AVX instructions sets.Thanks!\r\n", "@rajcivils \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35814\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35814\">No</a>\n"]}, {"number": 35813, "title": "Threading + tf.keras.layers.Input produces TypeError: 'NoneType' object is not iterable", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- No custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or binary): from pip\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) \r\n[GCC 7.3.0] on linux\r\n\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen I build a `tf.keras` model with the functional API in a `threading.Thread`, I often (not always) see an error in a thread:\r\n```python\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\nAfter the exception has appeared once, it does not appear again, until the python kernel is restarted.\r\n\r\nIn fact, it is not even necessary to build a whole model. Just instantiating a `tf.keras.layers.Input` layer is sufficient to get the error. I tested a few other layers (Dense, Conv1D) and they do not lead to the error, so am suspecting that there is something happening with Input)\r\n\r\n**Describe the expected behavior**\r\nI expect that no such error should appear in any thread when building a `tf.keras` model.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nfrom threading import Thread\r\nimport tensorflow as tf\r\n\r\ndef make_model():\r\n    tf.keras.layers.Input(10)\r\n    \r\n[Thread(target=make_model).start() for _ in range(10)]\r\n```\r\n\r\n**Other info / logs**\r\nThe error does not appear if i instantiate a `tf.keras.models.Model` subclass.\r\nfor example:\r\n\r\n```python\r\nfrom threading import Thread\r\nimport tensorflow as tf\r\n\r\nclass Model(tf.keras.models.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.dense = tf.keras.layers.Dense(10)\r\n    \r\n    def call(self, inputs):\r\n        return self.dense(inputs)\r\n    \r\ndef make_model():\r\n    Model()\r\n    \r\n[Thread(target=make_model).start() for _ in range(10)]\r\n```\r\n\r\nFull traceback:\r\n```python\r\nException in thread Thread-3:\r\nTraceback (most recent call last):\r\n  File \"/home/dani/miniconda3/envs/testtf/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/dani/miniconda3/envs/testtf/lib/python3.7/threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"<ipython-input-1-3fe088eac4c0>\", line 5, in make_model\r\n    tf.keras.layers.Input(10)\r\n  File \"/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_layer.py\", line 274, in Input\r\n    input_layer = InputLayer(**input_layer_config)\r\n  File \"/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_layer.py\", line 127, in __init__\r\n    ragged=ragged)\r\n  File \"/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\", line 1054, in placeholder\r\n    x = array_ops.placeholder(dtype, shape=shape, name=name)\r\n  File \"/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 2990, in placeholder\r\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 6676, in placeholder\r\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\r\n  File \"/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 759, in _apply_op_helper\r\n    return output_structure, op_def.is_stateful, op, outputs\r\n  File \"/home/dani/miniconda3/envs/testtf/lib/python3.7/contextlib.py\", line 119, in __exit__\r\n    next(self.gen)\r\n  File \"/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 418, in inner_cm\r\n    for fn in self._scope_exit_callbacks:\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\nNotes: the error does not appear in tf 2.0.", "comments": ["Issue replicating for tf-2.1 and [tf-nightly](https://colab.sandbox.google.com/gist/oanush/04f732545f614090df84351da1e693a5/35813.ipynb).Thanks!", "@jvishnuvardhan any news on this issue? any idea where we can start to look?\r\ni'd be happy to help if i can.", "@dswah When I add a return statement (`return tf.keras.layers.Input(10)`) to the `make_model` function, output from both the codes is same. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/82ca4e02d6378ab4b1b40597329a81dd/untitled771.ipynb). Thanks!", "@jvishnuvardhan No, that does not work.\r\n![image](https://user-images.githubusercontent.com/11619412/72851446-f2df0e00-3cab-11ea-9d35-8274b6a7fe8b.png)\r\nSometimes the error does not show up on the first try, so you may have gotten lucky. \r\nI just restarted the notebook (runtime >> restart and run all) and got the error using your sample.\r\n\r\nYou may have to do that a couple of times to get the error.", "@omalleyt12 hey there. Have you had a chance to take a look?\r\nAny idea where I could help out?\r\n\r\nthanks!", "I too faced this error while running on multiple threads. As a work around, I tried locking the model creation part (the _build_ function in my code below), and the error did seem to disappear. I don't know if it will manifest in some other form somewhere else.\r\n\r\nI wrote this code to check if multithreading works in TF 2.0 before migrating my production TF 1.0 code (which is multithreaded) to TF 2.0. Hence this code is kinda \"hello world\" program. Brief note on the code:\r\n\r\n- I am trying to solve the regression y = a*x + b. Input is a set of (x,y) pairs satisfying this equation for some _a_ and _b_. The model has to estimate _a_ and _b_.\r\n- Each thread solves different regression - i.e, _a_, _b_ are different for each thread.\r\n- At the end to training I write the model weights (which correspond to _a_ and _b_) to a file so that I could inspect if multiple threads crossed each other and screwed up training. Luckily, this did not happen, and each thread estimated _a_ and _b_ close enough. (_a_ was correct to the 4th decimal, but _b_ was off by +/- 2 in some cases, which I have to investigate further). \r\n- So, it seems to me that training simultaneously on multiple threads is working, but not the simultaneous model building (hence I had to lock model building part)\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras as tfkeras\r\nimport numpy as np\r\nimport threading\r\nimport multiprocessing\r\nimport itertools\r\nimport concurrent.futures\r\nimport sys\r\nimport traceback\r\n\r\nlock = threading.Lock()\r\n\r\nclass MyNN:\r\n    def __init__(self, a, b):\r\n        self.a = a\r\n        self.b = b\r\n\r\n    def build(self):\r\n        lock.acquire()\r\n        inp = tfkeras.layers.Input(shape=(1,))\r\n        oup = tfkeras.layers.Dense(1, activation=None)(inp)\r\n        self.model = tfkeras.Model(inputs=inp, outputs=oup)\r\n        opt = tfkeras.optimizers.Adam(learning_rate=0.1)\r\n        self.model.compile(loss='mean_squared_error',\r\n                      optimizer=opt,\r\n                      metrics=['mse', 'mae', 'mape'])\r\n        lock.release()\r\n\r\n    def train(self):\r\n        a = self.a\r\n        b = self.b\r\n        X_train = np.array([[x] for x in range(100000)])\r\n        y_train = np.array([[a*x[0]+b] for x in X_train])\r\n        X_test = np.array([[x+100000] for x in range(100)])\r\n        y_test = np.array([[a*x[0]+b] for x in X_test])     \r\n\r\n        history = self.model.fit(x=X_train, y=y_train, batch_size=10000, validation_data=(X_test, y_test), epochs=300, verbose=0)\r\n\r\n        with open(f\"c:/temp/res_{self.a}_{self.b}.txt\", \"w\") as f:\r\n            f.write(f\"a={self.a},b={self.b}\")\r\n            f.write(\"\\n\")\r\n            print(self.model.summary(), file=f)\r\n            f.write(\"\\n\")\r\n            print(self.model.get_weights(), file=f)\r\n            f.write(\"\\n\")\r\n            print(self.model.predict(X_test[0:10,:]), file=f)\r\n\r\ndef create_nn(config):\r\n    try:\r\n        print(f\"Running confing: {config}\")\r\n        nn = MyNN(config[0], config[1])\r\n        nn.build()\r\n        nn.train()\r\n    except:\r\n        e = sys.exc_info()[0]\r\n        print(e)\r\n        traceback.print_tb(sys.exc_info()[2])\r\n    print(\"Job Complete.\")\r\n\r\nwith concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\r\n    configs = [[5,6],[3,4],[6,9],[2,3],[4,5],[7,8],[8,6],[9,4],[1,2]]\r\n    for config in configs:\r\n        executor.submit(create_nn, config)\r\n        print(f\"Submitted config {config}\")\r\n\r\ninput(\"Press Enter to exit after all jobs are complete.\")\r\n\r\n```\r\n\r\nI am not saying it will solve your problem, nor am I clear if I will stumble upon any other problems when I migrate my production TF 1.0 code to TF 2.0. I thought of sharing my observations so far.\r\n\r\nAs you mentioned, definitely there seems to be some issue with multithreading in TF 2.0. Hope it will get fixed soon.\r\n\r\nThanks!", "This also affects TF 2.2\r\n\r\n@omalleyt12 @jvishnuvardhan Any update on this?", "Also had the issue on tf 2.2 when building several models in multiple threads from a topology using `tf.keras.model.model_from_json()`.\r\n\r\nI can confirm that @rkbairi 's solution of putting it in a locked section alleviates the issue:\r\n\r\n```python\r\nimport threading\r\nlock = threading.Lock()\r\n\r\nlock.acquire()\r\nmodel = tf.keras.model.model_from_json(...)\r\nlock.release()\r\n```", "Its possible this is now working with the latest nightly, but we dont make any guarantees using multi-threading with Keras at this time, but if you let us know why you use multi-threading we can re-evaluate. \r\n", "@dswah,\r\nCan you please respond to the above comment? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35813\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35813\">No</a>\n"]}, {"number": 35812, "title": "loose shape check in PointwiseToLinalgConverter", "body": "PointwiseToLinalgConverter only needs static rank not static shape, change accordingly to loose the restriction.", "comments": ["@jpienaar  add a test accordingly.", "@jpienaar @nicolasvasilache  Hi, could you help to take a look? Thanks!"]}, {"number": 35811, "title": "[Feature Request] Deformable Convolution v1 for tf-1.1.0 slim", "body": "**System information**\r\n- TensorFlow version (you are using): 1.1.0\r\nTensorFlow installed from conda command\r\n- Are you willing to contribute it (Yes/No): yes,if I have the ability\r\n\r\n**Describe the feature and the current behavior/state.**\r\nDeformable Convolutional [https://arxiv.org/pdf/1703.06211.pdf,https://arxiv.org/pdf/1703.06211.pdf](url), I used an old project with TF1.1.0 slim. Now I want to add Deformable Convolutional layer to train my network.\r\nBut, I can't find a way to achieve it.\r\n\r\n**Will this change the current api? How?**\r\nyes,add a slim layer\r\n**Who will benefit with this feature?**\r\nanyone who want to use deformable conv by tf slim", "comments": ["Apologies for the delay in response. duplicate #24387\r\nThis is a work in progress in tf addons repository. \r\nSee https://github.com/tensorflow/addons/issues/179\r\nClosing this thread since we are already tracking it in above threads. Thanks!"]}, {"number": 35810, "title": "Autograph transformation warning", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: 7.6\r\n- GPU model and memory: 2080ti\r\n\r\n\r\n**Describe the current behavior**\r\nI'm seeing the message \"AutoGraph could not transform and will run it as-is\"\r\nI found no performance issue, because a model shows almost the same losses compared to the exact code implemented with pytorch. I'm using gast version 0.2.2, so I doubt this is caused by gast. \r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nimport tensorflow.keras as keras\r\n\r\n\r\nos.environ['AUTOGRAPH_VERBOSITY'] = '10'\r\n\r\ndef shape_list(x):\r\n    static = x.shape.as_list()\r\n    dynamic = tf.shape(x)\r\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\r\n\r\n\r\nclass TFConv1D(layers.Layer):\r\n    def __init__(self, input_dim, output_dim, init_std=0.02, use_bias=True, **kwargs):\r\n        \"\"\" TFConv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)\r\n            Basically works like a Linear layer but the weights are transposed\r\n        \"\"\"\r\n        super(TFConv1D, self).__init__(**kwargs)\r\n        self.nf = output_dim\r\n        self.nx = input_dim\r\n        self.initializer_range = init_std\r\n        self.use_bias = use_bias\r\n        self.weight = self.add_weight(\r\n            \"{}_weight\".format(self.name),\r\n            shape=[self.nx, self.nf],\r\n            initializer=keras.initializers.TruncatedNormal(stddev=init_std))\r\n        if self.use_bias:\r\n            self.bias = self.add_weight(\r\n                \"{}_bias\".format(self.name),\r\n                shape=[1, self.nf],\r\n                initializer=tf.zeros_initializer())\r\n\r\n    def call(self, x):\r\n        x = tf.matmul(x, self.weight)\r\n        if self.use_bias:\r\n            x += self.bias\r\n        return x\r\n\r\n\r\nclass Adaptive_Softmax(layers.Layer):\r\n    def __init__(self, vocab_size: int, hidden_dim: int, cutoffs: list, padding_index: int, init_std=0.02):\r\n        super(Adaptive_Softmax, self).__init__()\r\n        self.padding_index = padding_index\r\n        self.vocab_size = vocab_size\r\n        self.hidden_dim = hidden_dim\r\n        self.n_clusters = len(cutoffs) + 1\r\n        self.cutoffs = [0] + cutoffs + [vocab_size]\r\n        self.cluster_logit = TFConv1D(hidden_dim, self.n_clusters)\r\n        self.logits = self.add_weight(\r\n            \"{}_weight\".format(self.name),\r\n            shape=[hidden_dim, vocab_size],\r\n            initializer=keras.initializers.TruncatedNormal(stddev=init_std))\r\n\r\n        self.bias = self.add_weight(\r\n            \"{}_bias\".format(self.name),\r\n            shape=[1, vocab_size],\r\n            initializer=tf.zeros_initializer())\r\n\r\n    def call(self, x, y):\r\n        x = x[:, :-1]\r\n        b, l, h = shape_list(x)\r\n        x = tf.reshape(x, [b * l, -1])\r\n        y = tf.reshape(y, [-1])\r\n        cl = self.cluster_logit(x)\r\n        cluster_ll = tf.nn.log_softmax(cl, axis=1)\r\n        nll = tf.zeros_like(y, dtype=x.dtype)\r\n        tail_weight = self.logits\r\n\r\n        for i in range(self.n_clusters):\r\n            l, r = self.cutoffs[i], self.cutoffs[i + 1]\r\n            mask = (y >= l) & (y < r)\r\n            indices = tf.where(mask)\r\n            target_i = tf.boolean_mask(y, mask) - l\r\n            tail_logit = tf.matmul(tf.boolean_mask(x, mask), tail_weight[:, l:r]) + self.bias[:, l:r]\r\n            tail_logprob_i = tf.nn.log_softmax(tail_logit, axis=1)  # [b,vocab]\r\n            # word_nll[indices] = -logprob_i\r\n            cur_ll = tf.gather_nd(cluster_ll, tf.concat([indices, tf.ones_like(indices) * i], 1)) + \\\r\n                     tf.gather_nd(tail_logprob_i,\r\n                                  tf.stack([tf.range(tf.size(target_i, out_type=target_i.dtype)), target_i], 1))\r\n            nll = tf.tensor_scatter_nd_update(nll, indices, -cur_ll)\r\n        return nll\r\n\r\nvocab_size = 51\r\nhidden_dim = 100\r\ncutoffs = [5,20]\r\npadding_index = 50\r\nx = tf.random.normal((800,51,100),dtype=tf.float32)\r\ny = tf.random.uniform((800,50),maxval=50,dtype=tf.int64)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((x,y))\r\nbatchfier = dataset.batch(4)\r\n\r\nmodel = Adaptive_Softmax(vocab_size,hidden_dim,cutoffs,padding_index)\r\noptimizer = keras.optimizers.Adam()\r\n\r\n@tf.function\r\ndef update_step(x, y):\r\n    with tf.GradientTape() as tape:\r\n        batch_loss = model(x,y)\r\n    step_grad = tape.gradient(batch_loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(step_grad, model.trainable_variables))\r\n    return batch_loss\r\n\r\nfor x,y in batchfier:\r\n    update_step(x,y)\r\n\r\n```\r\n**Other info / logs**\r\n\r\n> 2020-01-13 16:53:44.063623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2020-01-13 16:53:44.064637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\n2020-01-13 16:53:44.604399: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-01-13 16:53:44.613753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-13 16:53:44.614201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-01-13 16:53:44.614220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-13 16:53:44.614238: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-13 16:53:44.615203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-01-13 16:53:44.615364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-01-13 16:53:44.616463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-13 16:53:44.617020: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-13 16:53:44.617041: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-13 16:53:44.617092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-13 16:53:44.617634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-13 16:53:44.618056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-01-13 16:53:44.618258: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-13 16:53:44.641610: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz\r\n2020-01-13 16:53:44.642154: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ad9a2a6710 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-13 16:53:44.642165: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-01-13 16:53:44.686305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-13 16:53:44.686789: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ad9a925960 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-01-13 16:53:44.686818: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-01-13 16:53:44.686923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-13 16:53:44.687358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-01-13 16:53:44.687376: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-13 16:53:44.687383: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-13 16:53:44.687395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-01-13 16:53:44.687403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-01-13 16:53:44.687411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-13 16:53:44.687419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-13 16:53:44.687425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-13 16:53:44.687455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-13 16:53:44.687898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-13 16:53:44.688316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-01-13 16:53:44.688332: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-13 16:53:44.910219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-13 16:53:44.910242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-01-13 16:53:44.910247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-01-13 16:53:44.910389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-13 16:53:44.910886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-13 16:53:44.911318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9064 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nWARNING:tensorflow:AutoGraph could not transform <bound method Adaptive_Softmax.call of <__main__.Adaptive_Softmax object at 0x7ff7a6a7f320>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7ff79c3853c8>, <gast.gast.Return object at 0x7ff79c385c50>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method Adaptive_Softmax.call of <__main__.Adaptive_Softmax object at 0x7ff7a6a7f320>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7ff79c212198>, <gast.gast.Return object at 0x7ff79c2121d0>]\r\n2020-01-13 16:53:45.777560: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n\r\nProcess finished with exit code 0\r\n\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have tried on colab with TF version 2.1 .Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/3a5e0b854e4a42629569eb214f7706eb/untitled556.ipynb). Is this the expected behavior?.Thanks!", "> I have tried on colab with TF version 2.1 .Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/3a5e0b854e4a42629569eb214f7706eb/untitled556.ipynb). Is this the expected behavior?.Thanks!\r\n\r\nI guess it works as I expected. I manually checked output values and the result was the same as I expected. I guess the warning is caused by tensor_scatter_nd_update method, but I'm not sure", "This looks like #35765. Could you try removing the backslash continuation on this line:\r\n\r\n```\r\n            cur_ll = tf.gather_nd(cluster_ll, tf.concat([indices, tf.ones_like(indices) * i], 1)) + \\\r\n                     tf.gather_nd(tail_logprob_i,\r\n                                  tf.stack([tf.range(tf.size(target_i, out_type=target_i.dtype)), target_i], 1))\r\n```\r\n\r\nBreaking the line using parentheses should work:\r\n\r\n```\r\n            cur_ll = (\r\n                tf.gather_nd(cluster_ll, tf.concat([indices, tf.ones_like(indices) * i], 1)) +\r\n                tf.gather_nd(tail_logprob_i,\r\n                             tf.stack([tf.range(tf.size(target_i, out_type=target_i.dtype)), target_i], 1)))\r\n```", "@mdanatg removing backslash continuation solves the problem. Thanks", "Thank you for checking. We'll track fixing this issue in #35765.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35810\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35810\">No</a>\n"]}, {"number": 35831, "title": "Building the PoseNet example on iOS is failed", "body": "**System information**\r\n- OS Platform and Distribution: macOS 10.15 and iPadOS 13.3\r\n- Mobile device: iPad 2018\r\n- TensorFlow installed from (source or binary): pod\r\n\r\n**Describe the problem**\r\n\r\nI did all the steps from the readme of https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/ios \r\n\r\nbut got next error:\r\n\r\n> [!] CocoaPods could not find compatible versions for pod \"TensorFlowLiteSwift\":\r\n>  In snapshot (Podfile.lock):\r\n>    TensorFlowLiteSwift (= 0.0.1-nightly)\r\n\r\n> In Podfile:\r\n>    TensorFlowLiteSwift (= 0.0.1-nightly)\r\n\r\n> None of your spec sources contain a spec satisfying the dependencies: `TensorFlowLiteSwift (= 0.0.1-nightly), TensorFlowLiteSwift (= 0.0.1-nightly)`.\r\n\r\n>You have either:\r\n> * out-of-date source repos which you can update with `pod repo update` or with `pod install --repo-update`.\r\n> * mistyped the name or version.\r\n> * not added the source repo that hosts the Podspec to your Podfile.\r\n\r\n**The solution**\r\n\r\nTo make it work I just removed the Podfile.lock file and run again the command\r\n`pod install`\r\n\r\nMaybe the source repo should also be updated.\r\n\r\nThanks!\r\n\r\nP.S.: Initially it was posted there https://github.com/tensorflow/tensorflow/issues/35803 ", "comments": ["@AnyCPU please post tflite issues [tensorflow](https://github.com/tensorflow/tensorflow/issues/) repo.\r\n\r\n@oanush this is not related to tfjs, as the link above points to tflite examples.", "Hi @AnyCPU, though we can avoid this error by removing `Podfile.lock`, [it should not be removed](https://guides.cocoapods.org/using/using-cocoapods.html#benefits-of-ignoring-the-pods-directory):\r\n```\r\nWhether or not you check in the Pods directory,\r\nthe Podfile and Podfile.lock should always be kept under version control.\r\n```\r\n\r\nDid you run `pod repo update`?\r\n\r\nWhat version of cocoapod did you use? You can find it with `pod --version`.\r\nI used cocoapod version `1.7.5`, and failed to reproduce the error.\r\n", "@norangLemon \r\n\r\ncocoa pods version is 1.8.4\r\n\r\nThe `pod repo update` helped.\r\n\r\nIt is pretty strange I had trouble running it, anyway it works well now.\r\n\r\nThanks!"]}, {"number": 35809, "title": "Typo Error in `tflite_c02_transfer_learning.ipynb`", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_lite/tflite_c02_transfer_learning.ipynb\r\n\r\n## Description of issue (what needs changing):\r\n\r\n![Screenshot from 2020-01-13 12-53-58](https://user-images.githubusercontent.com/29497701/72238480-d7b53400-3603-11ea-847d-0eb7c0eb0716.png)\r\n\r\nIn description, it should be 'cats_vs_dogs'\r\n\r\n### Submit a pull request?\r\n\r\nYes", "comments": ["Can I fix this issue", "Yes. I'm not sure what's broken with that other PR, a clean resubmit would be easy to merge.", "Closing this issue since commit [9961988](https://github.com/tensorflow/examples/commit/996198828a5d758502f0cec9c5de58f04d200a0d) fixes the issue. Thanks!"]}, {"number": 35808, "title": "Update Pull Request Template", "body": "When I read the pull request, it is hard to read, so I think updating a Pull Request Template  that similar to the Issue Template will make pull requests more readable. Such as:\r\n######Description\r\n\r\n######Check List\r\n\r\n######My PR Type is:\r\n\r\nThank you!!\r\n", "comments": ["There is no template at the moment, but you can create a PR to add one under `https://github.com/tensorflow/tensorflow/tree/master/.github/ISSUE_TEMPLATE`", "I mean, provided that this is not a spam\r\n\r\n![VCA6DzWKgeZ](https://user-images.githubusercontent.com/323199/72463514-43280e80-3788-11ea-99eb-a623ee2be107.png)\r\n\r\nClosing issue, but please create PR if you did this in good spirits."]}, {"number": 35807, "title": "concat axis!=-1 and reshape change data layout don't support quant on dsp or NPU", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (use command below):1.14.0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nnnapi log said:concat axis!=-1 and reshape change data layout don't support accelator\r\n**Describe the expected behavior**\r\nit should run successful on dsp or hta\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n./benmakrk --graph=myquantized.tflite --use_nnapi=true\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nNew Comment: \"01-08 20:47:35.790 14384 14384 I TypeManager: Failed to read /vendor/etc/nnapi_extensions_app_allowlist ; No app allowlisted for vendor extensions use.\r\n01-08 20:47:35.799 919 1023 E OperationsUtils: NN_CHECK failed: inputShapes[0].scale == inputShapes[i].scale'\r\n01-08 20:47:35.799 919 1023 E OperationsUtils:\r\n01-08 20:47:35.799 919 1023 E OperationsUtils: NN_CHECK failed: inputShapes[0].offset == inputShapes[i].offset'\r\n01-08 20:47:35.799 919 1023 E OperationsUtils:\r\n01-08 20:47:35.801 919 1023 E hta-unnhal: RESHAPE doesn't support changing data layout\r\n01-08 20:47:35.801 919 1023 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.\r\n01-08 20:47:35.801 919 1023 E hta-unnhal: RESHAPE doesn't support changing data layout\r\n01-08 20:47:35.801 919 1023 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.\r\n01-08 20:47:35.801 919 1023 E hta-unnhal: RESHAPE doesn't support changing data layout\r\n01-08 20:47:35.801 919 1023 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.\r\n01-08 20:47:35.801 919 1023 E hta-unnhal: Only supports channel dimension for CONCATENATION\r\n01-08 20:47:35.801 919 1023 E hta-unnhal: {CONCATENATION, TENSOR_QUANT8} is not supported.\r\n01-08 20:47:35.801 919 1023 E hta-unnhal: RESHAPE doesn't support changing data layout\r\n01-08 20:47:35.801 919 1023 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.\r\n01-08 20:47:35.801 919 1023 E hta-unnhal: Only supports channel dimension for CONCATENATION\r\n01-08 20:47:35.801 919 1023 E hta-unnhal: {CONCATENATION, TENSOR_QUANT8} is not supported.\r\n01-08 20:47:35.805 919 1023 E OperationsUtils: NN_CHECK failed: inputShapes[0].scale == inputShapes[i].scale'\r\n01-08 20:47:35.805 919 1023 E OperationsUtils:\r\n01-08 20:47:35.806 919 1023 E OperationsUtils: NN_CHECK failed: inputShapes[0].offset == inputShapes[i].offset'\r\n01-08 20:47:35.806 919 1023 E OperationsUtils:\r\n01-08 20:47:35.810 919 1023 W android.hardware.neuralnetworks@1.2-service: pickAcceleratorByName cannot find the accelerator:adreno\r\n01-08 20:47:35.810 919 1023 W android.hardware.neuralnetworks@1.2-service:\r\n01-08 20:47:35.810 919 1023 W android.hardware.neuralnetworks@1.2-service: getSupportedOperations() cannot pick an accelerator\r\n01-08 20:47:35.810 919 1023 W android.hardware.neuralnetworks@1.2-service:\r\n01-08 20:47:35.814 919 919 E hta-unnhal: RESHAPE doesn't support changing data layout\r\n01-08 20:47:35.814 919 919 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.\r\n01-08 20:47:35.814 919 919 E hta-unnhal: RESHAPE doesn't support changing data layout\r\n01-08 20:47:35.814 919 919 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.\r\n01-08 20:47:35.814 919 919 E hta-unnhal: RESHAPE doesn't support changing data layout\r\n01-08 20:47:35.814 919 919 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.\r\n01-08 20:47:35.814 919 919 E hta-unnhal: Only supports channel dimension for CONCATENATION\r\n01-08 20:47:35.814 919 919 E hta-unnhal: {CONCATENATION, TENSOR_QUANT8} is not supported.\r\n01-08 20:47:35.814 919 919 E hta-unnhal: RESHAPE doesn't support changing data layout\r\n01-08 20:47:35.814 919 919 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.\r\n01-08 20:47:35.814 919 919 E hta-unnhal: Only supports channel dimension for CONCATENATION\r\n01-08 20:47:35.814 919 919 E hta-unnhal: {CONCATENATION, TENSOR_QUANT8} is not supported.\"\r\n", "comments": ["@miaowang14 what is the expected behavior here?", "@jdduke my model is from object detection api,", "This is the limitation of current driver. The driver may not be able to support all the operations in the model. And from the logcat, the HTA driver requires the quantization parameters of the input tensors to be the same, and the concatenation axis being the channel dimension.\r\n\r\nIf this is an important usecase, please let us know and we can communicate back to our partners.", "> This is the limitation of current driver. The driver may not be able to support all the operations in the model. And from the logcat, the HTA driver requires the quantization parameters of the input tensors to be the same, and the concatenation axis being the channel dimension.\r\n> \r\n> If this is an important usecase, please let us know and we can communicate back to our partners.\r\n\r\nyes,our team wants a low power detection model,the last rehsape and concat ops can't run on hta or dsp,and fall back to cpu.we want a better performance", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35807\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35807\">No</a>\n"]}, {"number": 35806, "title": "save_model_builder saved, then estimator load fail!", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\ntensorflow: 1.12.0\r\nenvironment: win10\r\ncode:\r\n```python3\r\nimport tensorflow as tf\r\nsaved_model_dir = \"./model_tst\"\r\n\r\ndef builder_and_save_model(saved_model_dir):\r\n    with tf.Session(graph=tf.Graph()) as sess:\r\n        x = tf.placeholder(tf.int32, name='input_x')\r\n        y = tf.placeholder(tf.int32, name='input_y')\r\n        b = tf.Variable(1, name='b')\r\n        xy = tf.multiply(x, y)\r\n        # \u8fd9\u91cc\u7684\u8f93\u51fa\u9700\u8981\u52a0\u4e0aname\u5c5e\u6027\r\n        op = tf.add(xy, b, name='output')\r\n\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        builder = tf.saved_model.builder.SavedModelBuilder(saved_model_dir)\r\n        # x \u4e3a\u8f93\u5165tensor, keep_prob\u4e3adropout\u7684prob tensor\r\n        inputs = {'input_x': tf.saved_model.utils.build_tensor_info(x),\r\n                  'input_y': tf.saved_model.utils.build_tensor_info(y)}\r\n\r\n        # y \u4e3a\u6700\u7ec8\u9700\u8981\u7684\u8f93\u51fa\u7ed3\u679ctensor\r\n        outputs = {'output': tf.saved_model.utils.build_tensor_info(op)}\r\n\r\n        signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs,\r\n                                                                           outputs=outputs,\r\n                                                                           method_name='variables')\r\n        builder.add_meta_graph_and_variables(sess=sess,\r\n                                             tags=[tf.saved_model.tag_constants.SERVING],\r\n                                             signature_def_map={'variables': signature},\r\n                                             assets_collection=None,)\r\n        builder.save()\r\n\r\nbuilder_and_save_model(saved_model_dir)\r\npredict_fn = tf.contrib.predictor.from_saved_model(saved_model_dir)\r\nprediction = predict_fn( {\"input_x\": 5, \"input_y\": 6})\r\nprint(prediction[\"output\"])\r\n```\r\n\r\nerror:\r\n```python3\r\nValueError: Got unexpected keys in input_dict: {'input_y', 'input_x'}\r\nexpected: set()\r\n```\r\n", "comments": ["@yongzhuo,\r\nIs there any specific reason that you are using `Tensorflow Version 1.12`.  You can upgrade your Tensorflow Version and can use \r\n\r\n`tf.saved_model.loader.load(sess, [\"tag\"], saved_model_dir)` instead of \r\n\r\n`predict_fn = tf.contrib.predictor.from_saved_model(saved_model_dir)` (it should work even for `1.12`)\r\n\r\nFore more information, please refer [this Documentation](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#loading-a-savedmodel-in-python) for Loading a Saved Model.", "ok, thanks!", "\r\n\r\n\r\n\r\n> ok, thanks!\r\n\r\nhow did you solve this problem ? still use predictor ?"]}, {"number": 35805, "title": "Win10: ImportError: DLL load failed: The specified module could not be found", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 home\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: PC\r\n- TensorFlow installed from (source or binary): 1.13.0\r\n- TensorFlow version:\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0 7.4.2.24\r\n- GPU model and memory: RTX 2080 Ti\r\n- CPU model and make: AMD Ryzen 7 3800X 8-Core Processor 3.89GHz\r\n- Anaconda Python Command Prompt\r\n\r\n**Describe the problem**\r\n\r\nTrying to build TF 1.13 from source. Issue at the creation of wheel file with bazel. Attached exact sequence of commands issued. Trying to conver a *.pb file into a TFLite file that was trained in tensorflow 1.13\r\n\r\n**Exact sequence of commands / steps executed before running into the problem**\r\n\r\nconda update -n base -c defaults conda\r\nconda update --all\r\nconda create -n tensorflow-build pip python=3.6\r\nconda activate tensorflow-build\r\npython -m pip install --upgrade pip\r\nconda install -c anaconda git\r\nset PATH=%PATH%;C:\\msys64\\usr\\bin\r\npip install six numpy wheel\r\npip install keras_applications==1.0.6 --no-deps\r\npip install keras_preprocessing==1.0.5 --no-deps\r\nconda install -c conda-forge bazel=0.21.0\r\nmkdir C:\\tensorflow-build\r\ncd C:\\tensorflow-build\r\ngit clone https://github.com/tensorflow/tensorflow.git \r\ncd tensorflow\r\ngit checkout r1.13\r\npython ./configure.py\r\n\r\nYou have bazel 0.21.0- (@non-git) installed. \r\n\r\nPlease specify the location of python. [Default is C:\\ProgramData\\Anaconda3\\envs\\tensorflow-build\\python.exe]: \r\n  \r\nFound possible Python library paths: \r\n\r\n  C:\\ProgramData\\Anaconda3\\envs\\tensorflow-build\\lib\\site-packages \r\n\r\nPlease input the desired Python library path to use.  Default is [C:\\ProgramData\\Anaconda3\\envs\\tensorflow-build\\lib\\site-packages] \r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n \r\nNo XLA JIT support will be enabled for TensorFlow. \r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n \r\nNo ROCm support will be enabled for TensorFlow. \r\n  \r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y \r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]: 10.0\r\n\r\nPlease specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.4\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 7.5\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y\r\nEigen strong inline overridden.\r\n\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**info / logs**\r\n\r\n////////////////////////////////////////////////////////LOG/////////////////////////////////////////////////////\r\n\r\nERROR: C:/tensorflow-build/tensorflow/tensorflow/BUILD:579:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/eduar/_bazel_eduar/j7bi4x5j/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin\r\n    SET PYTHON_BIN_PATH=C:/Users/eduar/.conda/envs/tensorflow-build/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/eduar/.conda/envs/tensorflow-build/lib/site-packages\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  C:/msys64/usr/bin/bash.exe bazel-out/x64_windows-opt/genfiles/tensorflow/tf_python_api_gen_v1.genrule_script.sh\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\eduar\\.conda\\envs\\tensorflow-build\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\eduar\\.conda\\envs\\tensorflow-build\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\eduar\\.conda\\envs\\tensorflow-build\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\eduar\\.conda\\envs\\tensorflow-build\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1319.085s, Critical Path: 291.62s\r\nINFO: 4651 processes: 4651 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I also get this failure on Windows 10 in a clean Conda env, installing TF 2.1 via pip on Python 3.7.6.", "@eduardoavila1m \r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements). Also go through #28848 and see if it helps you.Thanks!\r\n", "Hello, thaks for replying. My CPU model is a AMD Ryzen 7 3800X 8-Core Processor 3.89GHz. However I've tried the same instructions set with out the GPU support on an Intel I7 and the error is the same.", "I've reinstalled Anaconda and repeated the process. The error is persistent.", "Did you install Microsoft Visual Studio 2015?\r\nSee https://www.tensorflow.org/install/pip#1.-install-the-python-development-environment-on-your-system select `Windows`", "Yes, I've installed Visual Studio and it does not work.", "I try to build r1.13 in win10 , and get the same issue.\r\n\r\nERROR: C:/tensorflow-build/tensorflow/tensorflow/BUILD:579:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash.exe failed: error executing command\r\ncd C:/users/eduar/_bazel_eduar/j7bi4x5j/execroot/org_tensorflow\r\nSET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\nSET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\nSET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin <<= I think the problem is here\r\n..................\r\nI create a softlink in C:\\msys64, like this:\r\n    cd /d C:\\msys64\r\n    mklink /D bin \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\"\r\n\r\nthan the compile success\r\n\r\n ", "The Issue was solved by re-installing the CUDA versions and assuring that the tensorflow version was compatible with the CUDA and CuDNN versions found in the table https://www.tensorflow.org/install/source. Assure the system paths have been set correctly and install the tensorflow vesrion on a clean anaconda environment utilizing pip instal tensorflow-gpu==1.13.1. After some iterations this worked. I had to uninstall and re install CUDA multiple times.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35805\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35805\">No</a>\n"]}, {"number": 35804, "title": "Tensorboard 2.1 broken on Windows 10 (won't display anything on webpage)", "body": "I've ran the same code both before and after upgrading from 2.0 to 2.1 on two different machines.  What worked in 2.0 now in 2.1 serves up a blank web page instead of the usual graphs and stuff (page is completely blanked out).  Again, it seems to \"run\" as it would usually (no errors or anything, gives the typical localhost link, etc in the terminal -- just totally blank page served up).  \r\n\r\nI've upgraded many times and this is the first TB issue I've every run across.  Any ideas?\r\n\r\nIn case you want example code to test out....\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\ndef create_model():\r\n  return tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n    tf.keras.layers.Dense(512, activation='relu'),\r\n    tf.keras.layers.Dropout(0.2),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n\r\nmodel = create_model()\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nlog_dir=r\"C:\\Users\\justjo\\PycharmProjects\\tensorboardTest\\logs\\fit\"\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch = 100000000)\r\n\r\nmodel.fit(x=x_train,\r\n          y=y_train,\r\n          epochs=5,\r\n          validation_data=(x_test, y_test),\r\n          callbacks=[tensorboard_callback])\r\n```\r\n\r\nThen use \r\ntensorboard --logdir=C:\\Users\\justjo\\PycharmProjects\\tensorboardTest\\logs\\fit\r\n", "comments": ["@johnpjust \r\nAs the issue is more related to tensorboard can we track the [issue ](https://github.com/tensorflow/tensorboard/issues/3117) here.Thanks!\r\n\r\n"]}, {"number": 35803, "title": "Building the PoseNet example on iOS is failed", "body": "**System information**\r\n- OS Platform and Distribution: macOS 10.15 and iPadOS 13.3\r\n- Mobile device: iPad 2018\r\n- TensorFlow installed from (source or binary): pod\r\n\r\n**Describe the problem**\r\n\r\nI did all the steps from the readme of https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/ios \r\n\r\nbut got next error:\r\n\r\n> [!] CocoaPods could not find compatible versions for pod \"TensorFlowLiteSwift\":\r\n>  In snapshot (Podfile.lock):\r\n>    TensorFlowLiteSwift (= 0.0.1-nightly)\r\n\r\n> In Podfile:\r\n>    TensorFlowLiteSwift (= 0.0.1-nightly)\r\n\r\n> None of your spec sources contain a spec satisfying the dependencies: `TensorFlowLiteSwift (= 0.0.1-nightly), TensorFlowLiteSwift (= 0.0.1-nightly)`.\r\n\r\n>You have either:\r\n> * out-of-date source repos which you can update with `pod repo update` or with `pod install --repo-update`.\r\n> * mistyped the name or version.\r\n> * not added the source repo that hosts the Podspec to your Podfile.\r\n\r\n**The solution**\r\n\r\nTo make it work I just removed the Podfile.lock file and run again the command\r\n`pod install`\r\n\r\nMaybe the source repo should also be updated.\r\n\r\nThanks!", "comments": ["@AnyCPU ,\r\nHi, This is a TF.js issue . Kindly raise a new issue in TF.js repo using this [link](https://github.com/tensorflow/tfjs/issues/new).Thanks!"]}, {"number": 35802, "title": "recude_max on RaggedTensor fails to propagate gradients", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary(pip)\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.3.1\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nCurrently, passing a ragged tensor into a reduce_max an computing the gradient on the result fails. The following code is a minimal example of this:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nwith tf.GradientTape() as tape:\r\n    ragged_t = tf.RaggedTensor.from_row_splits(\r\n        [0.0, 1.0, 2.0, 3.0, 4.0, 5.0], [0, 1, 4, 6]\r\n    )\r\n    tape.watch(ragged_t.values)\r\n    max_t = tf.reduce_max(ragged_t, axis=-1)\r\n    # max_t = tf.reduce_max(ragged_t.to_tensor(default_value=np.nan), axis=-1)\r\n    gradients = tape.gradient(max_t, ragged_t.values)\r\n```\r\n\r\n**Describe the expected behavior**\r\nAs tensorflows main purpose is to compute gradients and reduce operations on ragged tensors is the most basic usage of ragged tensor representations I consider this behavior a bug. The above code shows a workaround in the commented line, but I expect the above example to work without this workaround and without the need of allocating in some cases a substantial amount of memory through the usage of `to_tensor`.\r\n\r\n**Other info / logs**\r\nThe full error trace of the above code example on my system is:\r\n```\r\nTraceback (most recent call last):\r\n  File \"./misc/reduce_max_ragged_gradients.py\", line 13, in <module>\r\n    gradients = tape.gradient(max_t, ragged_t.values)\r\n  File \"/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\", line 1014, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\", line 76, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\", line 138, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \"/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py\", line 455, in _UnsortedSegmentMaxGrad\r\n    return _UnsortedSegmentMinOrMaxGrad(op, grad)\r\n  File \"/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py\", line 432, in _UnsortedSegmentMinOrMaxGrad\r\n    _GatherDropNegatives(op.outputs[0], op.inputs[1])\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\r\n", "comments": ["I have actually had some code using `reduce_max` on ragged tensors in TF 1.13.0/1 which worked with the static graph gradient computation just fine. So this error seems to be introduced with TF 2.0 either through the eager execution, gradient tape or further changes to the ragged tensors.", "@DavidS3141 \r\nCan you try both 2.1.0-rc2 and tf-nighly and let us know whether the issue persists with the latest TF versions. I am not seeing any issue with 2.1 and nightly versions.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/e29ce0aad3912d1db4cfc7a4831b8238/untitled557.ipynb).Thanks!", "Yes those versions seem to work. I still hope, that TF2.0 gets patched, because TF2.1 seems to require already CUDA10.1 :(", "@DavidS3141 I can reproduce the issue with `TF2.0`. However it is resolved in recent stable version `TF2.2`. Can you please use `TF2.2`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/3d77f0d0f9179ca842983dff0beda51e/untitled557.ipynb) is a gist with `TF2.2`. Thanks\r\n\r\nAFAIK only security patches are getting patched with old `TF` versions. Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thaanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35802\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35802\">No</a>\n"]}, {"number": 35801, "title": "[Feature Request] MultiHeadAttention ops and Layer", "body": "Multi Head Self Attention is reaching SOTA results in a wide variety of fields:\r\n* NLP (Attention is all you need, GPT, Bert, etc)\r\n* [Graphs](https://arxiv.org/abs/1710.10903)\r\n* Computer Vision: [1](https://arxiv.org/abs/1906.05909) and [2](https://arxiv.org/abs/1911.03584)\r\n* [Time Series](https://arxiv.org/pdf/1909.07369.pdf)\r\n* [Audio/Speech Recognition](https://arxiv.org/pdf/1910.12977.pdf)\r\n\r\nNvidia already added this operation to [cudnn](https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_750.html) and pytorch already has an official [MultiHeadAttention](https://pytorch.org/docs/master/nn.html#multiheadattention) module. Its about time Tensorflow gets this operation too as it would enable researches and developers to get started quicker.\r\n\r\nCurrently there is an `Attention` layer in tf.keras but it only covers the main single head equation, has no learnable parameters, and can't be used as a basis for an efficient `MultiHeadAttention` implementation.\r\n", "comments": ["@cgarciae I would like to contribute.", "@monk1337 @fchollet Any updates?", "This layer is already implemented: \r\n\r\nhttps://www.tensorflow.org/addons/api_docs/python/tfa/layers/MultiHeadAttention\r\n\r\n", "> This layer is already implemented:\r\n> \r\n> https://www.tensorflow.org/addons/api_docs/python/tfa/layers/MultiHeadAttention\r\n\r\nThis layer do not use `cudnnMultiHeadAttnForward`.\r\n\r\nhttps://github.com/tensorflow/addons/commit/3b0d97804c15e65fdf9e8e3bbc2e93de0091e5bc", "cudnnMultiHeadAttnForward has not been defined as TF ops/kernel yet? @reedwm @zongweiz \r\nWe have tracked this. ", "@cgarciae,\r\nCan you please let us know if [MultiHeadAttention Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) is the functionality that you are looking for? Thanks!"]}, {"number": 35800, "title": "Add usage example for MaxPool2D", "body": "Added a usage example for tf.keras.layers.MaxPool2D", "comments": ["@Joseph-Rance please raise your PR against master , we only accept security fixes to release branch. Thank you\r\nCc @gbaned "]}, {"number": 35799, "title": "Incompatible shapes when using tf.keras.backend.ctc_decode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian Buster\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen using a `tf.keras.backend.ctc_decode` with a batch size <  the size of the model input, a ValueError is raised related to failure to broadcast input shapes.\r\n\r\n**Describe the expected behavior**\r\nI expect shapes to be consistent and therefore no `ValueError` to be raised.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef CTCDecoder():\r\n    def decoder(y_pred):\r\n        input_shape = tf.keras.backend.shape(y_pred)\r\n        input_length = tf.ones(shape=input_shape[0]) * tf.keras.backend.cast(input_shape[1], 'float32')\r\n        return tf.keras.backend.ctc_decode(y_pred, input_length)[0][0]\r\n    return tf.keras.layers.Lambda(decoder, name='decode')\r\n\r\ninput_layer = tf.keras.layers.Input((48, 37))\r\nx = CTCDecoder()(input_layer)\r\nmodel = tf.keras.models.Model(inputs=input_layer, outputs=x)\r\n\r\n# This never raises a ValueError. The batch size is equal to the length\r\n# of the input.\r\ny = model.predict(np.random.uniform(size=(100, 48, 37)), batch_size=100)\r\n\r\n# This usually raises a ValueError.\r\ny = model.predict(np.random.uniform(size=(100, 48, 37)), batch_size=32)\r\n\r\n# This always raises a ValueError.\r\ny = model.predict(np.random.uniform(size=(100, 48, 37)), batch_size=1)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHere is the full traceback for an example exception.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-145-9e000cf7055c> in <module>\r\n----> 1 y = model.predict(np.random.uniform(size=(100, 48, 37)), batch_size=1)\r\n\r\n/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n   1011         max_queue_size=max_queue_size,\r\n   1012         workers=workers,\r\n-> 1013         use_multiprocessing=use_multiprocessing)\r\n   1014 \r\n   1015   def reset_metrics(self):\r\n\r\n/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in predict(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    496         model, ModeKeys.PREDICT, x=x, batch_size=batch_size, verbose=verbose,\r\n    497         steps=steps, callbacks=callbacks, max_queue_size=max_queue_size,\r\n--> 498         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\r\n    499 \r\n    500 \r\n\r\n/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _model_iteration(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    473               mode=mode,\r\n    474               training_context=training_context,\r\n--> 475               total_epochs=1)\r\n    476           cbks.make_logs(model, epoch_logs, result, mode)\r\n    477 \r\n\r\n/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    177             batch_outs,\r\n    178             batch_start=step * batch_size,\r\n--> 179             batch_end=step * batch_size + current_batch_size)\r\n    180       cbks.make_logs(model, batch_logs, batch_outs, mode)\r\n    181       step += 1\r\n\r\n/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py in aggregate(self, batch_outs, batch_start, batch_end)\r\n    345     batch_outs = nest.flatten_up_to(self._structure, batch_outs)\r\n    346     for batch_element, result in zip(batch_outs, self.results):\r\n--> 347       result.aggregate(batch_element, batch_start, batch_end)\r\n    348 \r\n    349   def finalize(self):\r\n\r\n/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py in aggregate(self, batch_element, batch_start, batch_end)\r\n    278     num_elements = np.prod(batch_element.shape)\r\n    279     if num_elements < self._BINARY_SIZE_THRESHOLD:\r\n--> 280       self.results[batch_start:batch_end] = batch_element\r\n    281     else:\r\n    282       is_finished = threading.Event()\r\n\r\nValueError: could not broadcast input array from shape (1,46) into shape (1,48)\r\n```", "comments": ["For anyone coming across this issue, I've found that I can work around it by padding the output before returning results.\r\n\r\n```python\r\ndef CTCDecoder():\r\n    def decoder(y_pred):\r\n        input_shape = tf.keras.backend.shape(y_pred)\r\n        input_length = tf.ones(shape=input_shape[0]) * tf.keras.backend.cast(input_shape[1], 'float32')\r\n        unpadded = tf.keras.backend.ctc_decode(y_pred, input_length)[0][0]\r\n        unpadded_shape = tf.keras.backend.shape(unpadded)\r\n        # I don't *think* I should have to do this but it seems as though I do.\r\n        padded = tf.pad(\r\n            unpadded,\r\n            paddings=[[0, 0], [0, input_shape[1] - unpadded_shape[1]]],\r\n            constant_values=-1\r\n        )\r\n        return padded\r\n    return tf.keras.layers.Lambda(decoder, name='decode')\r\n```\r\n\r\nThe following snippet verifies consistency across different batch sizes.\r\n\r\n```python\r\ninput_layer = tf.keras.layers.Input((48, 37))\r\nx = CTCDecoder()(input_layer)\r\nmodel = tf.keras.models.Model(inputs=input_layer, outputs=x)\r\n\r\nX = np.random.uniform(size=(100, 48, 37))\r\n\r\ny100 = model.predict(X, batch_size=100)\r\ny1 = model.predict(X, batch_size=1)\r\ny32 = model.predict(X, batch_size=32)\r\n\r\nnp.testing.assert_almost_equal(y100, y1)\r\nnp.testing.assert_almost_equal(y100, y32)\r\n```", "It seems that small modifications to `keras.backend` could resolve this.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/30b98cc73a8b046c8ce0915f6c0c7398a4947192/tensorflow/python/keras/backend.py#L5817-L5832\r\n\r\nI would have thought that `st.dense_shape` should be something like `(input_shape[0], input_shape[1]` where `input_shape = tf.keras.backend.shape(y_pred)` (assuming you add this line before the transpose at the top of the function). But this causes the first assertion in the associated test (see below) to fail because it assumes that the output has not been padded with -1 values, which seems incorrect to me but I'm really not sure.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/03c045e146d441a0db4bd5cc1ecef1517e5c6708/tensorflow/python/keras/backend_test.py#L1779-L1783\r\n\r\nI'm not sure exactly what the desired behavior is here. If the current behavior is correct, then I propose that this issue be closed without action as the workaround above suits my purposes just fine. Otherwise, I'm glad to file a PR with adjustments to the function and associated test.", "@faustomorales, would you be open to send out a PR with your modification that fixes the problem? Thanks in advance.", "Yes, certainly I can do that. Will try to get it out soon.\n\nOn Thu, Jan 23, 2020 at 2:00 PM Rick Chao <notifications@github.com> wrote:\n\n> @faustomorales <https://github.com/faustomorales>, would you be open to\n> send out a PR with your modification that fixes the problem? Thanks in\n> advance.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35799?email_source=notifications&email_token=ACKIZ2OI6QV2U2SFF5MJH43Q7HZPNA5CNFSM4KFYS7QKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJYUXSQ#issuecomment-577850314>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACKIZ2PX3BPPYMTIDYORP23Q7HZPNANCNFSM4KFYS7QA>\n> .\n>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35799\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35799\">No</a>\n"]}, {"number": 35798, "title": "[TF2]     tf.TensorArray Error with Keras", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.6\r\n\r\nI took a `tf.TensorArray` [sample](https://www.tensorflow.org/guide/function#batching) from the documentation and wrapt it in a custom Keras Layer. The Layer works fine when I directly call the layer or the model, but when I user `model.predict` I get an error.\r\n\r\nCode sample:\r\n```\r\nimport tensorflow as tf\r\n\r\nclass Test_Layer(tf.keras.layers.Layer):\r\n\r\n    def __init__(self):\r\n        super(Test_Layer, self).__init__()\r\n\r\n    @tf.function\r\n    def call(self, x):\r\n        result = tf.TensorArray(tf.int32, size=x.shape[0])\r\n        for i in tf.range(x.shape[0]):\r\n            if x[i] > 0:\r\n                result = result.write(i, x[i] ** 2)\r\n            else:\r\n                result = result.write(i, x[i])\r\n        return result.stack()\r\n\r\n\r\n\r\ntest_layer = Test_Layer()\r\n\r\nout = test_layer(tf.range(-5, 5))\r\nprint(out) #works fine:= tf.Tensor([-5 -4 -3 -2 -1  0  1  4  9 16], shape=(10,), dtype=int32)\r\n\r\n\r\ntest_model = tf.keras.models.Sequential([test_layer])\r\ntest_model.compile(loss=tf.losses.mse)\r\n\r\n\r\nout = test_model(tf.range(-5, 5))\r\nprint(out) #works fine:= tf.Tensor([-5 -4 -3 -2 -1  0  1  4  9 16], shape=(10,), dtype=int32)\r\n\r\n\r\nout = test_model.predict(tf.range(-5, 5))\r\nprint(out) #ERROR\r\n```\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/media/jan/buffer/RNN/test_2.py\", line 34, in <module>\r\n    out = test_model.predict(tf.range(-5, 5))\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1013, in predict\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 498, in predict\r\n    workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 426, in _model_iteration\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 646, in _process_inputs\r\n    x, y, sample_weight=sample_weights)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2346, in _standardize_user_data\r\n    all_inputs, y_input, dict_inputs = self._build_model_with_inputs(x, y)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2572, in _build_model_with_inputs\r\n    self._set_inputs(cast_inputs)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2659, in _set_inputs\r\n    outputs = self(inputs, **kwargs)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 773, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 281, in call\r\n    outputs = layer(inputs, **kwargs)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 773, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 606, in _call\r\n    results = self._stateful_fn(*args, **kwds)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2362, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 3211, in bound_method_wrapper\r\n    return wrapped_fn(*args, **kwargs)\r\n  File \"lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    test_2.py:10 call  *\r\n        result = tf.TensorArray(tf.int32, size=x.shape[0])\r\n    lib/python3.6/site-packages/tensorflow_core/python/ops/tensor_array_ops.py:1078 __init__\r\n        name=name)\r\n    lib/python3.6/site-packages/tensorflow_core/python/ops/tensor_array_ops.py:444 __init__\r\n        raise ValueError(\"Size must be provided if flow is not provided\")\r\n\r\n    ValueError: Size must be provided if flow is not provided\r\n\r\n```", "comments": ["I have tried on colab with TF version 2.1.0-rc2, 2.2.0-dev20200113 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/240c734bb97240517cb6208aa5c0fbb4/untitled554.ipynb). Thanks!", "Thanks for reporting the issue.\r\n\r\nThe root cause is not because of TensorArray, but the way it is used. The important part here is \"x.shape[0]\". Note that x.shape returns the static shape of x. In the case of model.predict, since the batch size is not configured on the model, we tend to build the graph with a dynamic batch size, so that the same tf.function can be used with different batch size inputs. The result of x.shape is [None, 1] and then cause the tensorarray to fail to init.\r\n\r\nIf you change it to \"tf.shape(x)[0]\", which is the dynamic shape, tensorarray will correctly init even the batch is not known at the time. Hope this can be a workaround for you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35798\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35798\">No</a>\n"]}, {"number": 35797, "title": "Update api_def_StringLower.pbtxt", "body": "Update tf.string.lower docs.", "comments": ["This is not yet a doctest, https://www.tensorflow.org/community/contribute/docs_ref ", "@rohanreddych Can you please check reviewer comments and resolve conflicts? Thanks!", "@gbanned what is meant by a doctest?\nDo i have to test it on a local machine using `python pydoctest.py file` ? ", "Ping @gbaned ", "@rohanreddych yes. See https://www.tensorflow.org/community/contribute/docs_ref\r\n\r\nAlso, please solve conflicts", "There is already a doctest added for `StringLower`, from #35610\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2a39f5b83a8e354346bd0fa603d3d14fa33705b4/tensorflow/core/api_def/base_api/api_def_StringLower.pbtxt#L1-L11\r\n\r\nAs such, closing the PR as it became obsolete.", "@mihaimaruseac is doing a doctest \"necessary\" or is it like a compile method which will show if the syntax is correct or  not. Thanks for replying to the previous comments.", "It is necessary.\r\n\r\nConsider this example:\r\n\r\n```python\r\ndef foo(x, y):\r\n  \"\"\"Adds `x` and `y`.\r\n\r\n  Example:\r\n\r\n    foo(40, 2)  # prints 42\r\n\r\n  \"\"\"\r\n  return x + y\r\n```\r\n\r\nSuppose now that in the future the function is silently changed to instead of `return x + y` do `return x + y + 1`. Now, the example in the docstring is completely wrong, will confuse users and be completely useless.\r\n\r\nNow, suppose the docstring example is written as a doctest:\r\n\r\n```python\r\ndef foo(x, y):\r\n  \"\"\"Adds `x` and `y`.\r\n\r\n  Example:\r\n\r\n    >> foo(40, 2)\r\n    42\r\n\r\n  \"\"\"\r\n  return x + y\r\n```\r\n\r\nAs soon as the `return` line changes, CI would fail with an error like\r\n\r\n```\r\nValues are not equal:\r\nExpected: 42\r\nBut foo(40, 2) is: 43\r\n```\r\n\r\nThis time, the change author will also change the docstring and this will make the docstring still be useful and helpful.\r\n\r\nSure, the example here is very trivial, but imagine more complex cases where you have to look over multiple functions and one change in one function can impact a doctest of a totally different function that ends up calling the changed one.\r\n\r\nDoctests are best because they serve both as documentation and tests."]}, {"number": 35796, "title": "hlo_algorithm_blacklist.cc fails compilation", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): VS2019\r\n- CUDA/cuDNN version: 10.1 / 7.6\r\n- GPU model and memory: 2070 Max Q\r\n\r\nI setup it using configure with the following options selected : \r\n- XLA JIT\r\n- Cuda\r\n- /arch:AVX\r\n- Eigen strong inline overridden\r\n\r\nWhen attempting to build from source using the following command. \r\n\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow:libtensorflow.so\r\n\r\nI get the following error.\r\n\r\nERROR: C:/sdks/tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:1616:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:hlo_algorithm_blacklist' failed (Exit 2)\r\ncl : Command line warning D9002 : ignoring unknown option '-std=c++14'\r\ntensorflow/compiler/xla/service/gpu/hlo_algorithm_blacklist.cc(28): error C2131: expression did not evaluate to a constant\r\nexternal/com_google_absl\\absl/strings/string_view.h(186): note: a non-constant (sub-)expression was encountered\r\n", "comments": ["@oracle3001,\r\nIs this still an issue? \r\n\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/38951#issuecomment-620551293) from similar issue #38951 and check if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35796\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35796\">No</a>\n"]}, {"number": 35795, "title": "Saved model, KeyError: 'serving_default'", "body": "Have I written custom code: N/A\r\nOS Platform and Distribution: Ubuntu 18.04\r\nTensorFlow installed from : pip3\r\nTensorFlow version 1.14.0\r\nBazel version 1.1.0\r\nCUDA/cuDNN version: 10.1 / N/A\r\nGPU model and memory GeForce GTX 1050\r\nExact command to reproduce : Describe below\r\n\r\nI try to use that generated model:\r\nhttp://eugen-lange.de/download/ssd-4-traffic-sign-detection-frozen_inpherence_graph-pb/\r\n\r\nHere is my program:\r\n\r\n\r\n```\r\nclass TFEngine:\r\n    def __init__(self, model_name):\r\n        self.model = None\r\n\r\n        if \"ssd4tsd_full\" in model_name:\r\n            self.load_model_local(\"/home/xavier/Downloads/\"+model_name + \"/saved_model\")\r\n        else:\r\n            self.load_model(model_name)\r\n\r\n    def load_model(self, model_name):\r\n        base_url = 'http://download.tensorflow.org/models/object_detection/'\r\n        model_file = model_name + '.tar.gz'\r\n        model_dir = tf.keras.utils.get_file(\r\n            fname=model_name,\r\n            origin=base_url + model_file,\r\n            untar=True)\r\n        model_dir = pathlib.Path(model_dir) / \"saved_model\"\r\n        model = tf.saved_model.load(str(model_dir))\r\n        self.model = model.signatures['serving_default']\r\n        return model\r\n\r\n    def load_model_local(self, model_dir):\r\n        model = tf.saved_model.load(str(model_dir))\r\n        self.model = model.signatures['serving_default']\r\n        return model\r\n\r\n    def DetectWithImage(self, image):\r\n        image = np.array(image)\r\n        image = np.asarray(image)\r\n        # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\r\n        input_tensor = tf.convert_to_tensor(image)\r\n        # The model expects a batch of images, so add an axis with `tf.newaxis`.\r\n        input_tensor = input_tensor[tf.newaxis, ...]\r\n\r\n        # Run inference\r\n        output_dict = self.model(input_tensor)\r\n\r\n        # All outputs are batches tensors.\r\n        # Convert to numpy arrays, and take index [0] to remove the batch dimension.\r\n        # We're only interested in the first num_detections.\r\n        num_detections = int(output_dict.pop('num_detections'))\r\n        output_dict = {key: value[0, :num_detections].numpy()\r\n                       for key, value in output_dict.items()}\r\n        output_dict['num_detections'] = num_detections\r\n\r\n        # detection_classes should be ints.\r\n        output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\r\n\r\n        # Handle models with masks:\r\n        if 'detection_masks' in output_dict:\r\n            # Reframe the the bbox mask to the image size.\r\n            detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\r\n                output_dict['detection_masks'], output_dict['detection_boxes'],\r\n                image.shape[0], image.shape[1])\r\n            detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\r\n                                               tf.uint8)\r\n            output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\r\n\r\n        return output_dict\r\n\r\n.....\r\n\r\nimage_size = list([720, 1280])\r\ntf_engine = TFEngine(model_name)\r\noutput_dict = tf_engine.DetectWithImage(frame)\r\nobj_img = append_objs_to_img(frame, output_dict, category_index)\r\n```\r\n\r\nI get the error:\r\n`    return self._signatures[key]\r\nKeyError: 'serving_default'\r\n`\r\n\r\nI think the model is not in savedModel format so I try to convert it into savedModel :\r\n\r\n`python ~/dev/tensorflow/tensorflow/python/tools/saved_model_cli.py convert --dir /home/xavier/Downloads/ssd4tsd_full/ --output_dir /home/xavier/Downloads/ssd4tsd_full/saved_model/ --tag_set serve --signature_def serving_default\r\nusage: saved_model_cli.py convert [-h] --dir DIR --output_dir OUTPUT_DIR\r\n                                  --tag_set TAG_SET\r\n                                  {tensorrt} ...\r\nsaved_model_cli.py convert: error: invalid choice: 'serving_default' (choose from 'tensorrt')\r\n`\r\n\r\nI don't know how to solve it.\r\n\r\n", "comments": ["@xav12358, Tried replicating issue, looks like code is incomplete.\r\nPlease find the [gist](https://colab.sandbox.google.com/gist/gadagashwini/86e1c7f4052671936950b87c6751f388/untitled345.ipynb?authuser=1) and provide more information to replicate the reported issue. Thanks! ", "@xav12358, Did you get a chance to look at the provided gist. Thanks!", "Please make sure the title is descriptive. I tried replacing it but I'm not sure it matches the error you've encountered", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I have a query regarding \r\n\r\n**model_fn = model.signatures['serving_default']**\r\n\r\nWhat purpose does it serve?\r\nMy intention is to run prediction only when a particular condition is satisfied..\r\nThe comment line in the code mentions that the line is mentioned above does the prediction part utilizing the model.So I think I'm supposed to go inside this 'signatures' function and modify.\r\nPlease help me out with this.", "That question belongs to Stack Overflow."]}, {"number": 35794, "title": "Failed to allocate memory for tensor_info, 1320 bytes required", "body": "@tensorflow/micro\r\n\r\nHost OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nNAME=\"Ubuntu\"\r\nVERSION=\"18.04.3 LTS\"\r\nID=ubuntu\r\nPRETTY_NAME=\"Ubuntu 18.04.3 LTS\"\r\nVERSION_ID=\"18.04\"\r\nPython versionL 2.7.15+\r\nTarget platform : K64F\r\n\r\n**Describe the problem**\r\nI have build a GRU model in python and converted to a bin format for ARM K64F board. When I flash the binary I get the following error message in the serial console.  How can I fix it? Any help is appreciated.\r\n\r\n`Failed to allocate memory for tensor_info, 1320 bytes required\r\nAllocateTensors() failed\r\n\r\n++ MbedOS Fault Handler ++\r\n\r\nFaultType: HardFault\r\n\r\nContext:\r\nR0   : 00000000\r\nR1   : 00000000\r\nR2   : 00000000\r\nR3   : 20013FE8\r\nR4   : 00000000\r\nR5   : 00000000\r\nR6   : 00000000\r\nR7   : 20002368\r\nR8   : 20002364\r\nR9   : 00000000\r\nR10  : 00000000\r\nR11  : 00000000\r\nR12  : 000000B0\r\nSP   : 20013F10\r\nLR   : 00008E89\r\nPC   : 000010E4\r\nxPSR : 61070000\r\nPSP  : 20013EA8\r\nMSP  : 2002FFC0\r\nCPUID: 410FC241\r\nHFSR : 40000000\r\nMMFSR: 00000000\r\nBFSR : 00000004\r\nUFSR : 00000000\r\nDFSR : 00000008\r\nAFSR : 00000000\r\nMode : Thread\r\nPriv : Privileged\r\nStack: PSP\r\n\r\n-- MbedOS Fault Handler --\r\n\r\n\r\n\r\n++ MbedOS Error Info ++\r\nError Status: 0x80FF013D Code: 317 Module: 255\r\nError Message: Fault exception\r\nLocation: 0x10E4\r\nError Value: 0x1FFF0400\r\nCurrent Thread: main Id: 0x200013E8 Entry: 0x3F07 StackSize: 0x10000 StackMem: 0x20004010 SP: 0x20013F10\r\nFor more info, visit: https://mbed.com/s/error?error=0x80FF013D&tgt=K64F\r\n-- MbedOS Error Info --\r\n\r\n= System will be rebooted due to a fatal error =\r\n= Reboot count(=7) reached maximum, system will halt after rebooting\r\n`\r\n\r\n\r\n\r\n", "comments": ["This is the expected error if you are trying to load a model and your tensor arena is too small.  If you have extra memory, you can increase the tensor arena size.  If not, you will either need to compress your model and required tensors via quantization, or select a more compact model.", "I did try to increase the Tensor arena size but that resulted with another error **`Logic error in memory planner, tensor 17 has an invalid lifetime`**.  I dont understand this error. Secondly what kind of quantization should I use for data type of float. Any help regarding this appreciated.", "Apologies for the delay.  It's strange that you're seeing that error - as far as I know that's a sanity check to ensure that the memory planner has found valid lifetimes for each tensor in the graph - i.e. for each tensor it has a valid start and end.  I'm guessing you may be running into this error as a side-effect of something else going wrong.  Perhaps try further increasing arena size (we have seen some issues with marginal arena sizes) and make sure you allocate the arena in a valid way. Avoid allocating large arenas on the stack to avoid stack overflow issues.  Also make sure you aren't overrunning any structures in your application, since that could also cause memory corruption.\r\n\r\nWhen it comes to quantizing float tensors, we mainly support int8 quantization on TFLite Micro.  You should be able to quantize your model using the TFLite converter with OPTIMIZE_FOR_SIZE along with providing a representative dataset.  Note that we do not support running hybrid quantized models on TFLite Micro, hence needing a representative dataset at conversion time.", "I'm going to close this, since it's been a while since any update. Please reopen if this issue persists, or if new information is available.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35794\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35794\">No</a>\n"]}]