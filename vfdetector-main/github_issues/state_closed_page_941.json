[{"number": 25206, "title": "Update Estimator dependency version in setup.py.", "body": "", "comments": ["For some reason copybara is migrating my internal change. So making new change directly on branch to get it in", "This change is failing master builds for me:\r\n\r\nerror in tensorflow setup command: 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers; Invalid requirement, parse error at \"'< 1.14.0'\",\r\n\r\nI think a comma is needed like tensorboard in the line above. (or I might just need a new version of setuptools)", "Ah, thank you. Yup, pretty sure missing comma. WIll fix", "Updated", "@case540, this needs the new commit added", "Done. ", "this was already merged \r\nhttps://github.com/tensorflow/tensorflow/pull/25268"]}, {"number": 25205, "title": "TFTRT: Support conv2d_transpose (Conv2DBackpropInput)", "body": "Support conv2d_transpose in TFTRT, with unit tests.", "comments": ["Seems like that the following commit was merged before this PR and we will need to merge against it. Trevor, could you do that?\r\nhttps://github.com/tensorflow/tensorflow/commit/96526b3d24692426117fbf5b7ae572271172cdff", "@smit-hinsu Thanks Smit, I have rebased.", "This PR was merged in the following commit and for some reason it did not automatically close this.\r\nhttps://github.com/tensorflow/tensorflow/commit/ca2a1cb4cad2dba0d7c4bde662f36de592f5668f\r\n\r\nTrevor, could you rebase again to see if anything is left over and go through the above commit to make sure that no unintended changes are part of it? Looked good to me.\r\n\r\nWe can manually close this PR after that.", "@smit-hinsu Thanks for merging! Looks like this PR is bugged and thinks its not merged - should I close the PR?\r\n\r\nEdit: Just saw your comment. I'll rebase", "@smit-hinsu I think it's fixed now."]}, {"number": 25204, "title": "Add Estimator release notes to 1.13", "body": "This is in preparation to releasing Estimator 1.13, as per the instructions.\r\n\r\nLet me know if I should also do this on master (and/or cherry-pick from one branch to the other)", "comments": ["@aselle I think you should merge it, I cannot"]}, {"number": 25203, "title": "[Intel MKL] Requantization op perchannel support", "body": "## Requantization_Perchannel_op\r\n\r\n1. Adding two new hidden operators to support requantization to be performed perchannel and provided the mkl implementations for the same.\r\n\r\n        1. requantize_per_channel_op\r\n        2. requantization_range_per_channel_op\r\n\r\n2. Provided Unit test to test the implementation logic of both these above mentioned ops.", "comments": ["@hgadig \r\nReopening this PR: https://github.com/tensorflow/tensorflow/pull/25038 which had unusual git rebasing problem. \r\n", "@penpornk can you review this PR and provide updates or merge please!", "@penpornk Updated the code and comments to address the review comments! I refrained from clicking all the resolve comment button, which i guess you will be doing. Let me know if any further changes are needed! @rthadur - FYI", "@rthadur addressed all review comments! ", "Thank you for your fast response! I'll review this PR tomorrow.\r\n\r\n> I refrained from clicking all the resolve comment button, which i guess you will be doing.\r\n\r\nThank you! Being able to click the resolve button is convenient to me. :) (But please also feel free to mark them as resolved if it's easier for you to keep track as well!)", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Looks like the other author that @googlebot has a CLA problem with is me. Manually setting CLA to yes then.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@penpornk haven't completed it! will ping you late this evening or tomorrow and confirm edits. :) ", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "@nammbash Got it. Please take your time. :) And don't worry about the CLA. I have a feeling googlebot will do this a lot. I'll just change it again after we are done with the PR.", "@penpornk I have made the changes as requested! let me know. (As a side note, there is another PR that needs to go in after this on convolution Op support per channel. (https://github.com/tensorflow/tensorflow/pull/25506)\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/25203#discussion_r\r\nSure! Note that the transpose operation per some internal reviews is was very fast with MKL. Not sure with eigen though. TODO well warranted.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/25203#discussion_r253669164\r\nGood suggestion! Learnt something new thank you!\r\nThis was the delay as opposed to submitting it last Friday as the behavior was different in reality vs what was suggested. Took the time to verify logic and test suggestion.\r\nConclusion: It is reduction(max : out_min_max) and worked only with the max as a function and not with maximum as a logic! (So was in the middle of the testing things committed too soon). Changes upstreamed now!! \r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/25203#discussion_r253677900\r\nI see!", "@penpornk changes done!\r\n", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@nammbash gentle ping to  check test failures.", "@rthadur Interesting! \r\n1. some of these are not related to CPU. (Like the GPU CC) . Correct me if I am wrong.\r\n2. the test //tensorflow/tools/api/tests:api_compatibility_test passes in c++. Is there a python version of the same test? Let me check into that", "@rthadur @penpornk \r\nInteresting findings on the build server failures. per your issue-comment: https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461208892\r\n\r\n**I am ignoring the following failures as it does not pertain to my PR:**\r\n```\r\nGPU CC\r\nGPU python 3\r\nMacOS\r\nWindows Bazel GPU\r\n```\r\n**The pertaining ones then is this**\r\n\r\n```Ubuntu python2```\r\n\r\n**google build server error as displayed by web link points to the test:**\r\n```\r\ntensorflow/tools/api/tests:api_compatibility_test\r\n\r\nand hints problem to be in the pbtxt apis of RequantizePerchannel. RequantizeRnagePerchannel and requantize.\r\n```\r\n**Findings when I run it locally**\r\n1. Bazeltest  succeeds if I use \"bazel test\" directly. \r\n```\r\nbazel --output_base=../test_folder test tensorflow/tools/api/tests:api_compatibility_test\r\n```\r\n2. if I build it separately and then run it as the log online seems to indicate\r\n```\r\nbazel --output_base=../test_folder build tensorflow/tools/api/tests:api_compatibility_test\r\nthen\r\na. bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True (success)\r\nb. bazel-bin/tensorflow/tools/api/tests/api_compatibility_test  (Failed)\r\n\r\nyet the failure seems to not have any mention of perchannel or requantize ops.\r\n```\r\n**Analysis Conclusion:(see findings below)**\r\n```\r\n1.  I believe that google build server is having some issues, \r\nor \r\n2. maybe with this specific run \r\nor \r\n3. bazel bug??! Internal bazel build is different than what I have locally?? locally \r\nI am using bazel version: 0.19.2 running on Ubuntu xfce: 4.12\r\n```\r\n", "@nammbash You are right that other failures are unrelated. But the API ones are indeed because of this PR. \r\n\r\nI forgot about updating the API golden files. Sorry! Basically, when you add any new API pbtxt, you need to add an entry to the golden files too. I believe running the test with `--update_golden` modifies the necessary goldenfiles (e.g., tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt, ensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt, etc) for you.\r\n\r\nFor example, see how the golden files are modified in https://github.com/tensorflow/tensorflow/commit/de87e628e6d89382783ea948c21fa66b182d69d3. (You shouldn't modify it yourself. Let the test modify them and add the changes to the PR.)\r\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "@penpornk modifications done and committed. Please review and merge. https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461318566\r\n@rthadur  FYI\r\n\r\nAction:\r\nbuild test separately and then run\r\n```\r\nbazel --output_base=../test_folder build tensorflow/tools/api/tests:api_compatibility_test\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test  (Failed)\r\n\r\nbut\r\n\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True \r\nFails and updated the files\r\n\r\nbazel --output_base=../test_folder build tensorflow/tools/api/tests:api_compatibility_test\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test  (Success)\r\n\r\n", "@nammbash Hmm. This is weird. The golden files aren't related to your PR. (But the test passed without your PR.) I'll ask someone who actually knows more about them. Will get back to you soon.", "@penpornk \r\n:) Exactly my point from my comments yesterday: https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461262408.\r\n\r\nthank you for the internal reviews! Will await updates. ", "@nammbash So @annarev told me we actually don't need to modify the golden files at all since the visibility is hidden. The mismatch on your part might be the estimator version mismatch. But then I'm not sure why our Kokoro test failed on your PR and not on the current repo. Could you please try syncing your local copy? (Also, please drop the last commit changing the golden files.) Sorry for the inconvenience! ", "@nammbash I'm looking at the [test results from last night](https://source.cloud.google.com/results/invocations/dcc891bd-2e73-4b95-9a72-32de274bd127/log). The differences were actually the entries for your ops (`RequantizationPerChannel` and `RequantizationRangePerChannel`). That's why I thought they needed to be added to the golden files. Now there two questions: why did you see different results, and why did the test try to add your 'hidden' ops to the golden files. \r\n\r\n```\r\nE0207 07:22:46.303906 140336513210112 api_compatibility_test.py:216] TensorFlow API backwards compatibility test\r\nThis test ensures all changes to the public API of TensorFlow are intended.\r\nIf this test fails, it means a change has been made to the public API. Backwards\r\nincompatible changes are not allowed. You can run the test as follows to update\r\ntest goldens and package them with your change.\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\nYou will need an API approval to make changes to the public TensorFlow API. This\r\nincludes additions to the API.\r\nE0207 07:22:46.304153 140336513210112 api_compatibility_test.py:217] 1 differences found between API and golden.\r\nIssue 1\t: None :\r\n---\r\n+++\r\n@@ -2645,10 +2645,18 @@\r\n     argspec: \"args=[\\'input\\', \\'input_min\\', \\'input_max\\'], varargs=None, keywords=None, defaults=None\"\r\n   }\r\n   member_method {\r\n+    name: \"RequantizationRangePerChannel\"\r\n+    argspec: \"args=[\\'input\\', \\'input_min\\', \\'input_max\\', \\'clip_value_max\\'], varargs=None, keywords=None, defaults=None\"\r\n+  }\r\n+  member_method {\r\n     name: \"Requantize\"\r\n     argspec: \"args=[\\'input\\', \\'input_min\\', \\'input_max\\', \\'requested_output_min\\', \\'requested_output_max\\', \\'out_type\\'], varargs=None, keywords=None, defaults=None\"\r\n   }\r\n   member_method {\r\n+    name: \"RequantizePerChannel\"\r\n+    argspec: \"args=[\\'input\\', \\'input_min\\', \\'input_max\\', \\'requested_output_min\\', \\'requested_output_max\\', \\'out_type\\'], varargs=None, keywords=None, defaults=None\"\r\n+  }\r\n+  member_method {\r\n     name: \"Reshape\"\r\n     argspec: \"args=[\\'tensor\\', \\'shape\\'], varargs=None, keywords=None, defaults=None\"\r\n   }\r\n```", "@penpornk I was confused last afternoon on this question too. hence the question is there a build server/ specific run error.\r\n\r\nInterestingly  bazel test passes but bazel build and then running test fails! Why? and even if it fails it was not related to perchannel.\r\n\r\nbazel --output_base=../test_folder test tensorflow/tools/api/tests:api_compatibility_test passes.\r\n\r\nAnyways I am trying to undo based on https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461569094 and see where it takes us.\r\n", "@penpornk \r\nStatus Quo: https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461573084\r\n\r\nQ1: why did you see different results\r\nA1: No more after the merge with master from yesterday. I see the same error.\r\n\r\nQ2: why did the test try to add your 'hidden' ops to the golden files\r\nThis is confusing. test fails and investigating. Not sure why hidden is doing this. Yet still the changes made by the golden file is for the estimators and not my op. but the test passes after these changes? Is the problem even related! Weird!\r\n\r\nmaybe @gunan is able to provide some extra insights?!", "@nammbash Sorry for the delay! I was in meetings. \r\n@annarev thinks we are actually including hidden ops in the golden file now. She said you need to make sure those `raw_ops.*` golden files exist in your branch (you should have them after syncing), then pass `--only_test_core_api=True` to the test. \r\nLet us know if that works!", "To add to @penpornk reply, there is this RFC for exporting all ops to the raw_ops namespace:\r\nhttps://github.com/tensorflow/community/blob/master/rfcs/20181225-tf-raw-ops.md\r\nThis seems to include hidden ops as well. @alextp  would know more.\r\n\r\nAlso, to be more specific, --only_test_core_api=True needs to be passed in to api_compatibility_test when updating goldens. This flag is passed automatically when running test with \"bazel test\", but not when running it for golden update.", "Anna, should we update the error message in the test to instruct people to\npass it when updating goldens?\n\nOn Thu, Feb 7, 2019 at 3:26 PM annarev <notifications@github.com> wrote:\n\n> To add to @penpornk <https://github.com/penpornk> reply, there is this\n> RFC for exporting all ops to the raw_ops namespace:\n>\n> https://github.com/tensorflow/community/blob/master/rfcs/20181225-tf-raw-ops.md\n> This seems to include hidden ops as well. @alextp\n> <https://github.com/alextp> would know more.\n>\n> Also, to be more specific, --only_test_core_api=True needs to be passed in\n> to api_compatibility_test when updating goldens. This flag is passed\n> automatically when running test with \"bazel test\", but not when running it\n> for golden update.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461632527>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxcnOkmT6QfQhTfmMiMp6Wfpbytviks5vLLYwgaJpZM4aTxMo>\n> .\n>\n\n\n-- \n - Alex\n", "And yes, the raw_ops namespace includes hidden ops. Feel free to bypass API\nreview for it, though.\n\nOn Thu, Feb 7, 2019 at 4:14 PM Alexandre Passos <apassos@google.com> wrote:\n\n> Anna, should we update the error message in the test to instruct people to\n> pass it when updating goldens?\n>\n> On Thu, Feb 7, 2019 at 3:26 PM annarev <notifications@github.com> wrote:\n>\n>> To add to @penpornk <https://github.com/penpornk> reply, there is this\n>> RFC for exporting all ops to the raw_ops namespace:\n>>\n>> https://github.com/tensorflow/community/blob/master/rfcs/20181225-tf-raw-ops.md\n>> This seems to include hidden ops as well. @alextp\n>> <https://github.com/alextp> would know more.\n>>\n>> Also, to be more specific, --only_test_core_api=True needs to be passed\n>> in to api_compatibility_test when updating goldens. This flag is passed\n>> automatically when running test with \"bazel test\", but not when running it\n>> for golden update.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461632527>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AAATxcnOkmT6QfQhTfmMiMp6Wfpbytviks5vLLYwgaJpZM4aTxMo>\n>> .\n>>\n>\n>\n> --\n>  - Alex\n>\n\n\n-- \n - Alex\n", "@alextp I think we could just change the default value, so that passing this flag is not required. ", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@penpornk and @annarev thank you for the comments.\r\nLooks like that was a massive overhaul change with the new API and seems like I was the first to get caught trying the new method amidst @alextp changes! :)\r\n\r\nWhat was done:\r\n```\r\n1. Added the 2 new ops in file tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt\r\nLooks like alphabetical order matters too. (So some automated method to add them / generate them would be beneficial?!)\r\n2. Ran the test: \r\nbazel --output_base=../test test --cache_test_results=no tensorflow/tools/api/tests:api_compatibility_test\r\nSuccess.\r\n3. Separate passes only with the flag as mentioned.\r\nbazel --output_base=../test build --cache_test_results=no tensorflow/tools/api/tests:api_compatibility_test\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test --only_test_core_api=True\r\n```\r\n### **@penpornk and @annarev and @alextp thank you all for the help!**\r\n\r\nSuggestions:\r\n```\r\n1. It is confusing to have two tests for api.(which should i worry about?)\r\na. tensorflow/tools/api/tests:api_compatibility_test\r\nand\r\nb.tensorflow/core/api_def/api_test\r\n\r\n2. Yes some error message on what needs to be done pointing at the tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt would be helpful, including mention of alphabetical order(maybe)!?\r\n```\r\n", "1.\r\n@nammbash the two tests are different and both important:\r\na. tensorflow/tools/api/tests:api_compatibility_test: compares TensorFlow Python public API to golden files. It is important for checking backwards compatibility and monitoring API changes.\r\nb. tensorflow/core/api_def/api_test: checks consistency of api_def_OpName.pbtxt files specifically. For e.g. it checks that all registered ops should have api_def_OpName.pbtxt file. These .pbtxt files are used for all language APIs, so this is not specific to Python unlike api_compatibility_test.\r\n\r\n2. We should definitely look at improving error messages. Ideally, golden files should be updated automatically when running api_compatibility_test with --update_goldens=True. But I can see that at times this approach could hit issues (for e.g. in this case, you needed to pass additional flag).", "@rthadur I think the failures are all not related to my PR? if yes please merge, if not please let me know! thank you", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "@penpornk  thank you!\r\n```\r\nFYI\r\ntags = [\"no_mac\"],  # TODO(penporn): Re-enable the test on MacOS.\r\n```\r\nplease re-run the tests.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@rthadur can you please merge this PR?(making sure it is in before any new conflicts) thank you! :)", "> @rthadur can you please merge this PR?(making sure it is in before any new conflicts) thank you! :)\r\n\r\nSure. We are looking into this. As \"rthadur\" is off, i'll be taking this.", "@nammbash Our internal test says the v1 golden file should be updated as well. Sorry that this wasn't reflected in our external tests. Did you see any changes to `tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt` when you updated the golden files? If so, please include it. If not, we need to figure out why.", "@penpornk I updated tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt manually!\r\nDidn't know there was a flag to do this?\r\n--only_test_core_api=True --wil not modify anything.\r\n\r\nare you expecting --update_goldens=True to do something to the tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt?\r\nI can modify it manually nevertheless", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "@nammbash Are you able to generate the changes to `tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt`? If not, here is the diff. I guess we can modify it directly this time. (Line number might be slightly different.)\r\n```\r\n@@ -2645,10 +2645,18 @@\r\n     argspec: \"args=[\\'input\\', \\'input_min\\', \\'input_max\\'], varargs=None, keywords=None, defaults=None\"\r\n   }\r\n   member_method {\r\n+    name: \"RequantizationRangePerChannel\"\r\n+    argspec: \"args=[\\'input\\', \\'input_min\\', \\'input_max\\', \\'clip_value_max\\'], varargs=None, keywords=None, defaults=None\"\r\n+  }\r\n+  member_method {\r\n     name: \"Requantize\"\r\n     argspec: \"args=[\\'input\\', \\'input_min\\', \\'input_max\\', \\'requested_output_min\\', \\'requested_output_max\\', \\'out_type\\'], varargs=None, keywords=None, defaults=None\"\r\n   }\r\n   member_method {\r\n+    name: \"RequantizePerChannel\"\r\n+    argspec: \"args=[\\'input\\', \\'input_min\\', \\'input_max\\', \\'requested_output_min\\', \\'requested_output_max\\', \\'out_type\\'], varargs=None, keywords=None, defaults=None\"\r\n+  }\r\n+  member_method {\r\n     name: \"Reshape\"\r\n     argspec: \"args=[\\'tensor\\', \\'shape\\'], varargs=None, keywords=None, defaults=None\"\r\n   }\r\n```\r\n\r\n", "@nammbash  Please sign the CLA in order to proceed with merging.", "@hgadig He already signed the CLA. The bot detected his CLA and assigned `CLA: yes` since the beginning of the PR. Even the message right now says the bot found the CLA for @nammbash. What makes the bot assign `CLA: no` is because some of the commits are his accepting my online one-line suggestions (GitHub's new feature). For those commits, the author's emails that GitHub automatically used are `38869685+nammbash@users.noreply.github.com` and `38085909+penpornk@users.noreply.github.com`, which are not recognized by the googlebot. \r\n\r\nGooglebot wants us to verify that all the coauthors in this PR have signed the CLA. There are only two people contributing to this PR: @nammbash and I. So I think we are fine. :)", "A quick note that I'm not setting CLA to yes yet until we are done with all the changes (since googlebot will keep setting it to no).", "@penpornk Sorry was in a meeting and hence the delay! Thank you for the code suggestions again, appreciate it! \r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461952804 Here I meant to say v1 in the bottom comment file. Sorry about that confusion.(edited it)\r\n\r\nClearly there is difference between what google expect from these automation scripts and test flags and what I am experiencing locally. Feels like I am beta testing the changes . :)\r\n\r\nAnyways here is what I am experiencing from the latest master as available to external customers. Hoping it will identify the bug/process automation misses that can help google.\r\n1. Bazel test when I use the \"test\" option passes\r\n```\r\nbazel --output_base=../test2 test --cache_test_results=no //tensorflow/tools/api/tests:api_compatibility_test\r\n```\r\n2. Yet! bazel with \"build\" option and then tests run on the binary separately have different behaviors\r\n```\r\nbazel --output_base=../test2 build --cache_test_results=no //tensorflow/tools/api/tests:api_compatibility_test \r\n```\r\nthen\r\nTrial 1. Failure. No changes to any Files!\r\n``` \r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test \r\n```\r\n\r\nTrial 2.  Success (as it skipped 3 tests IMO) No changes in files\r\n```\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test --only_test_core_api=True\r\n```\r\n\r\nTrial 3.   Success (as it skipped 3 tests IMO) No changes in files\r\n```\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test --only_test_core_api=True --update_goldens=True\r\n```\r\n\r\nTrial 4. Success (as it skipped 3 tests IMO) - But 4 file changes(3 new and 1 addition)\r\n```\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens=True\r\n\r\nFiles changed are as below:\r\n(Modified 3)\r\nmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.-mode-keys.pbtxt\r\nmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.experimental.pbtxt\r\nmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.pbtxt\r\n(Added 1)\r\ntensorflow/tools/api/golden/v2/tensorflow.estimator.inputs.pbtxt\r\n\r\nNote: If I build with these changes in trial 4 , Trial 1 will pass\r\n```\r\n**NOTE: None of this Modifies the tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt or its similar v2 counterpart.**\r\n\r\nAll changes now: :)\r\n```\r\nYesterday: I changed v2 manually. ( tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt ) \r\nToday    : I changed v1 manually. ( tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt)\r\n\r\nQuestion: Do you want me to push all changes(modified files from Trial 4 above?) Let me know and I can do that too if it helps!\r\n```\r\n**FYI: All v1 tests at least does not seem to be running with the externally exposed script which is all I have.(which I think you indicated too). \r\nSo please beware!! I couldn't test any changes pertaining to that part of it.**\r\n\r\nand @hgadig you have you answer from penporn https://github.com/tensorflow/tensorflow/pull/25203#issuecomment-461987678", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@penpornk @annarev thank you! Let me know if anything else is required from me to merge this PR!", "@hgadig the 1 failure: Ubuntu Python3 PIP \u2014 Internal CI build failed  is not related to my changes in this PR. Let me know if anything else is required my side to merge this PR!", "@nammbash Your changes had been pulled in. We are having troubles with internal tests again, but I don't think they are related to your PR. I'll let you know if we need more changes here. Sorry for the inconvenience!", "It's finally merged! Thank everyone so much for the efforts, @nammbash, @rthadur, @annarev, @alextp, and @hgadig!"]}, {"number": 25202, "title": "TFTRT: Support LeakyRelu op", "body": "TRT adds support for a LeakyRelu activation in 5.1.\r\n\r\nTF LeakyRelu Op is registered here: https://github.com/tensorflow/tensorflow/blob/cb96167443b3a5d30183e8b95f615fd29cc1e77a/tensorflow/core/ops/nn_ops.cc#L1011", "comments": ["Thanks @smit-hinsu !", "Hey @smit-hinsu \r\n\r\nWe found that TRT has performance issues with this LeakyReLU layer. We are going to change the conversion to use other ops instead for the time being."]}, {"number": 25201, "title": "fix broken links and typo in NMT w/Attention example", "body": "The seq2seq tutorial has moved. This updates the links. (And fixes a typo.)\r\n\r\n(Kindly ignore the collab angle-bracket weirdness in the diff.)\r\n\r\nNB Review link:\r\nhttps://app.reviewnb.com/tensorflow/tensorflow/pull/25201/", "comments": ["@yashk2810 I think you're a good reviewer for this :-)"]}, {"number": 25200, "title": "tf.keras load_model fails to load a model created with Keras", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Win7 same issue on Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):   b'v1.11.0-rc2-4-gc19e29306c' 1.11.0\r\n- Python version:  Python 3.6.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: CUDA Toolkit 9.0 (Sept 2017) cuDNN v7.0.5 (Dec 5, 2017)\r\n- GPU model and memory: device: 0, name: Quadro M4000, compute capability: 5.2 memory_limit: 7801630311\r\n                                            device: 1, name: Quadro M4000, compute capability: 5.2 memory_limit: 7801630311\r\n- Keras version: 2.2.4\r\n\r\nI am trying to load a model saved with Keras (not tf.keras) to create an Estimator.    The Keras model is a slightly modified version of Inception_V3 from the Keras model library.  (The modification is that the final layer was replaced with a dense layer and sigmoid to use the model for regression.)  \r\n\r\nA co-worker was able to load a standard (unmodified) Keras model with tf.keras.  So I believe the intended behavior is that all Keras created models are loadable.  Please let me know if I have a misunderstanding about the intended functionality.\r\n\r\n**Describe the current behavior**\r\nNotebook1: Save the Keras model like this (abbreviated steps):\r\n\r\nfrom keras.applications.inception_v3 import InceptionV3\r\nbase_model =InceptionV3(include_top=True, weights=viewClassWeightsFile, input_shape=input_shape, classes=nb_classes)\r\nbase_model.layers.pop() \r\nbase_model.outputs = [base_model.layers[-1].output]\r\nbase_model.layers[-1].outbound_nodes=[]\r\noutput = base_model.get_layer('avg_pool').output\r\noutput = Dense(activation=\"sigmoid\", units=num_regression_targets)(output)\r\nnew_model = Model(base_model.input, output)\r\n\r\nnew_model.save(modelSaveFile) \r\n\r\nNotebook2: Load the Keras model with tf.keras\r\nnew_model = tf.keras.models.load_model(modelSaveFile)\r\n\r\nAttributeError: module 'tensorflow.python.keras.backend' has no attribute 'slice'\r\n\r\n**Describe the expected behavior**\r\n\r\nI expected that tf.keras.models.load_model() should be able to load a model created witih Keras.  \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHere is full traceback:\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-0e6b1a78cac7> in <module>\r\n      6 modelSaveFile = modelDir+'\\\\model_singlePt_mix6_mlx100_set2.h5'\r\n      7 \r\n----> 8 base_model = keras.models.load_model(modelSaveFile)\r\n      9 \r\n     10 \r\n\r\nc:\\users\\wolvci10\\appdata\\local\\continuum\\anaconda3\\envs\\tf_1p9\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py in load_model(filepath, custom_objects, compile)\r\n    228       raise ValueError('No model found in config file.')\r\n    229     model_config = json.loads(model_config.decode('utf-8'))\r\n--> 230     model = model_from_config(model_config, custom_objects=custom_objects)\r\n    231 \r\n    232     # set weights\r\n\r\nc:\\users\\wolvci10\\appdata\\local\\continuum\\anaconda3\\envs\\tf_1p9\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py in model_from_config(config, custom_objects)\r\n    308                     '`Sequential.from_config(config)`?')\r\n    309   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\r\n--> 310   return deserialize(config, custom_objects=custom_objects)\r\n    311 \r\n    312 \r\n\r\nc:\\users\\wolvci10\\appdata\\local\\continuum\\anaconda3\\envs\\tf_1p9\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py in deserialize(config, custom_objects)\r\n     62       module_objects=globs,\r\n     63       custom_objects=custom_objects,\r\n---> 64       printable_module_name='layer')\r\n\r\nc:\\users\\wolvci10\\appdata\\local\\continuum\\anaconda3\\envs\\tf_1p9\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    171             custom_objects=dict(\r\n    172                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +\r\n--> 173                 list(custom_objects.items())))\r\n    174       with CustomObjectScope(custom_objects):\r\n    175         return cls.from_config(config['config'])\r\n\r\nc:\\users\\wolvci10\\appdata\\local\\continuum\\anaconda3\\envs\\tf_1p9\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py in from_config(cls, config, custom_objects)\r\n   1300         if layer in unprocessed_nodes:\r\n   1301           for node_data in unprocessed_nodes.pop(layer):\r\n-> 1302             process_node(layer, node_data)\r\n   1303 \r\n   1304     name = config.get('name')\r\n\r\nc:\\users\\wolvci10\\appdata\\local\\continuum\\anaconda3\\envs\\tf_1p9\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py in process_node(layer, node_data)\r\n   1258       if input_tensors:\r\n   1259         if len(input_tensors) == 1:\r\n-> 1260           layer(input_tensors[0], **kwargs)\r\n   1261         else:\r\n   1262           layer(input_tensors, **kwargs)\r\n\r\nc:\\users\\wolvci10\\appdata\\local\\continuum\\anaconda3\\envs\\tf_1p9\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    767 \r\n    768       if not in_deferred_mode:\r\n--> 769         outputs = self.call(inputs, *args, **kwargs)\r\n    770         if outputs is None:\r\n    771           raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n\r\nc:\\users\\wolvci10\\appdata\\local\\continuum\\anaconda3\\envs\\tf_1p9\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py in call(self, inputs, mask)\r\n    715       arguments['mask'] = mask\r\n    716     print(self.function.__name__) # added by Cindy\r\n--> 717     return self.function(inputs, **arguments)\r\n    718 \r\n    719   def compute_mask(self, inputs, mask=None):\r\n\r\nc:\\users\\wolvci10\\appdata\\local\\continuum\\anaconda3\\envs\\tf_1p9\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py in get_slice(data, i, parts)\r\n    193         stride = K.concatenate([step, input_shape * 0], axis=0)\r\n    194         start = stride * i\r\n--> 195         return K.slice(data, start, size)\r\n    196 \r\n    197     # Relocate the model definition under CPU device scope if needed\r\n\r\nAttributeError: module 'tensorflow.python.keras.backend' has no attribute 'slice'\r\n\r\n", "comments": ["Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. Github is mainly for addressing bugs in installation and performance. Thanks!", "@jvishnuvardhan This does feel like a bug though? Are `tf.keras.models.load_model` and `keras.Model.save_model` not meant to be compatible? ", "Having the same issue.", "@danvargg Please post a new issue with details of your issue and a standalone code to reproduce the issue. Thanks!", "@jvishnuvardhan why not address the issue & code already linked here? Multiple people are having the same problem with similar code.", "This seems like a major backwards-compatability issue. And if it's not meant to be supported can you please say so?", "@danvargg I tried it in `TF2.x` and the [gist is here](https://colab.sandbox.google.com/gist/jvishnuvardhan/43e5c9782daaa067ebbef723af9dfdd0/untitled756.ipynb). Thanks!\r\n\r\n[Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/b85702ffcca2d0cfffa028acfd7f9499/untitled757.ipynb) is the gist with `TF1.15` which is the latest and last `TF1.x` version.  Thanks", "@jvishnuvardhan Your gist does not address the problem at all, as it imports `keras` from `tensorflow`. The issue is describing an icompatability between `tf.keras` and the original standalone `keras` module.", "I did find a workaround (which I don't like too much). My issue is `modelcheckpoint` not being compatible with loading the model in `TF2.x`.\r\n\r\nI managed to do so by saving the callback as `.hdf5` (which it seems to work with `TF2.x`) and then doing `model.save('model.h5')` on the `.hdf5`.\r\n\r\nAfter that, I was able to load the `.h5` model and convert it with `tfcoreml`. I'm currently testing if the predictions of all these models produce the same results to ensure nothing changed between saves.\r\n\r\nI really don't like doing workarounds like this, specially for models that will go to prod.\r\n\r\n@jvishnuvardhan, is there a better approach to load and convert models saved with `modelcheckpoint`?"]}, {"number": 25199, "title": "Support for half precision type in linear algebra operators", "body": "This PR fixes https://github.com/tensorflow/tensorflow/issues/22086\r\nAdds support for half precision type in linear algebra operators and relatives unit tests.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@vcarpani  Please sign CLA in order to proceed with next steps. Thank you !", "Signed it ;)", "CLAs look good, thanks!\n\n<!-- ok -->", "It seems to fail on an unrelated test (`checkpoint`)."]}, {"number": 25198, "title": "Build from source in RelWithDebInfo build type, get a error : LNK 1248 image size (10068FDA8) exceeds maximum allowable size (FFFFFFFF)", "body": "**System information**\r\n- OS:  Windows10 64bit pro  32g RAM\r\n- TensorFlow version:  r1.10\r\n- Python version:  3.5.2\r\n- cmake version : 3.6.3\r\n- GCC/Compiler version : VS2015 update3\r\n- CUDA/cuDNN version: CUDA9.0 Cudnn7.0\r\n- GPU model and memory:GTX1080 8G\r\n- sigwin version :3.0.10\r\n\r\n**Describe the problem**\r\nI have built from source in Release build type successfuly.\r\nBut when I try to build it in RelWithDebInfo , many errors occurred :\r\nFatal error LNK1248: image size (10068FDA8) exceeds maximum allowable size (FFFFFFFF) \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. I used cmake-gui 3.6.3 to generate the project with tensorflow_ENABLE_GPU is ON and tensorflow_BUILD_SHARED_LIB is ON\r\n\r\n2. I used the VS2015 x64 x86 cross tools Command Prompt to open the tensorflow.sln\r\n\r\n3. Change build type from Debug to RelWithDebInfo  , Choose Build->ALL_BUILD.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nFatal error LNK1248: image size (10068FDA8) exceeds maximum allowable size (FFFFFFFF) \r\ntf_core_kernels\tD:\\tensorflow-r1.10\\tensorflow-r1.10\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.dir\\RelWithDebInfo\\tf_core_kernels.lib\r\n\r\nFatal error LNK1248: image size(100206D1B) exceeds maximum allowable size (FFFFFFFF) tensorflow_static\tD:\\tensorflow-r1.10\\tensorflow-r1.10\\tensorflow\\contrib\\cmake\\build\\RelWithDebInfo\\tensorflow_static.lib\r\n\r\nFatal error LNK1248: image size(100062160)exceeds maximum allowable size(FFFFFFFF)\tpywrap_tensorflow_internal_static\tD:\\tensorflow-r1.10\\tensorflow-r1.10\\tensorflow\\contrib\\cmake\\build\\RelWithDebInfo\\pywrap_tensorflow_internal_static.lib\r\n", "comments": ["Unfortunately we don't provide support for the CMake build anymore, so I'm marking this issue as \"Community Support\". You may have more luck building with Bazel for Windows using `-c dbg`.", "@gyp2448565528,\r\n\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to `2.6` which is latest stable version of TF and let us know if the issue still persists in newer versions.You can build using `bazel` and use this [guide](https://www.tensorflow.org/install/source_windows) for your reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25198\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25198\">No</a>\n"]}, {"number": 25197, "title": "tensor_shape.cc : Check failed: 0 <= new_num_elements (0 vs. -1)", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Stretch 9\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: raspberry-pi 3B\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.11.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.15.2\r\n- GCC/Compiler version (if compiling from source): 6.3.0\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\nFirst you should know that this piece of code is compiled with the GCC flag : `add_definitions(-D_GLIBCXX_USE_CXX11_ABI=0)` whenever I am compiling for the raspberry-pi target because I was not able to build tensorflow for ARM32bit with the new ABI version.\r\n\r\nI'm having troubles running a piece of code on my raspberry-pi that actually works just fine on my local computer. The error comes from the declaration of a tensor::TensorShape supposed to contain the dimension of my input tensor, prior to filling a tensorflow::Tensor with my data contained in a cv::Mat.\r\nIt crashes on the declaration line with the error\r\n```\r\nF tensorflow/core/framework/tensor_shape.cc:249] Check failed: 0 <= new_num_elements (0 vs. -1)\r\n```\r\nIt seems I am giving a wrong input dimension. Which is weird as the exact same line works juste fine on my local machine : \r\n```\r\nauto shape = tensorflow::TensorShape({1,64,64,1});\r\n```\r\n(sparing you the variable names). The 64 are uint16_t while the 1s are int on 4 bytes.\r\nCould the error come from the 32bit architecture of my raspberry-pi ? Looking at tensor_shape.cc\r\n```\r\ntemplate <class Shape>\r\nvoid TensorShapeBase<Shape>::AddDim(int64 size) {\r\n  if (!kIsPartial) CHECK_GE(size, 0);\r\n  if (unknown_rank()) return;\r\n  CHECK_LT(ndims_byte(), MaxDimensions()) << \"Too many dimensions in tensor\";\r\n  int64 new_num_elements;\r\n  if (kIsPartial && (num_elements() < 0 || size < 0)) {\r\n    new_num_elements = -1;\r\n  } else {\r\n    new_num_elements = MultiplyWithoutOverflow(num_elements(), size);\r\n    CHECK_LE(0, new_num_elements);\r\n  }\r\n  UnsafeAddDim(size, new_num_elements);\r\n}\r\n```\r\nThe (crashing) 249th line is the check : `CHECK_LE(0, new_num_elements);` so it looks that something doesn't pass....\r\n\r\n**Code to reproduce the issue**\r\nCompile tensorflow v1.11.0 from source for raspberry-pi target architecture with gcc 6.3.0. Then simply try to declare a tensorshape.\r\n\r\n**Other info / logs**\r\nThe only error line was already reported.\r\n", "comments": ["I actually found where the error came from but I cannot clearly explain why. It seems like the TensorShape did not manage to correctly understand my array. I read somewhere in the source that it could need an input array up to dimension 6 so potentially an error in intepreting my array ? I may be completely wrong...\r\n\r\nThe workaround is to add dimensions one by one, using the adDim method.\r\n```\r\nauto shape = tensorflow::TensorShape();\r\nshape.AddDim(1);\r\nshape.AddDim(64);\r\nshape.AddDim(64);\r\nshape.AddDim(1);\r\n```\r\nI will try to make it more clean, but it is sufficient for me for the moment.\r\n\r\nClosed this issue as it is now solved."]}, {"number": 25196, "title": "How to do distributed training with multi-gpus", "body": "Hi, I got a problem of doing synchronous training on distributed training. I want to use multi-machines with multi-GPUs on distributed training. But I can't find any material about it. Is here any one giving me some help? Thanks a lot.\r\n\r\nHere is part of my code.\r\n\r\n    with tf.device(\"/job:worker/task:%d\" % FLAGS.task_index, cluster=cluster):\r\n            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n            alpha = dynamic_lrate(hparams, global_step)\r\n            optimizer = tf.contrib.opt.LazyAdamOptimizer(alpha, beta1=0.9, beta2=0.997, epsilon=1e-9)\r\n            rep_op = tf.train.SyncReplicasOptimizer(optimizer,\r\n                                                    replicas_to_aggregate=len(worker_hosts),\r\n                                                    total_num_replicas=len(worker_hosts),\r\n                                                    use_locking=True)\r\n\r\n            \"\"\"in RNNModel contains multi-gpus\"\"\"\r\n            model = RNNModel(None, hparam_list, train_ds, tf.estimator.ModeKeys.TRAIN, rep_op, global_step)\r\n            valid_model = RNNModel(model.graph, hparam_list, valid_ds, tf.estimator.ModeKeys.PREDICT, rep_op, global_step)\r\n\r\n            train_op = model.train_op\r\n\r\n            init_token_op = rep_op.get_init_tokens_op()\r\n            chief_queue_runner = rep_op.get_chief_queue_runner()\r\n\r\n            saver = saver_mod.Saver(max_to_keep=hparams.max_save,\r\n                            var_list=model.all_vars)\r\n\r\n            sv = tf.train.Supervisor(is_chief=(FLAGS.task_index==0),\r\n                                     logdir=log_dir,\r\n                                     saver=saver,\r\n                                     global_step=global_step,\r\n                                     save_summaries_secs=3600,\r\n                                     save_model_secs=hparams.sfreq)\r\n\r\n            with sv.prepare_or_wait_for_session(server.target, config=config) as sess:\r\n                if FLAGS.task_index == 0 and issync == 1:\r\n                    sv.start_queue_runners(sess, [chief_queue_runner])\r\n                    sess.run(init_token_op)\r\n\r\n                #tf.global_variables_initializer().run()\r\n\r\n\r\n                idx = 0\r\n                max_bleu_score = -10.0\r\n                best_path = os.path.join(log_dir, \"model_best\")\r\n                total_accu = 0.0\r\n                total_size = 0\r\n                for epoch in range(FLAGS.maxsteps):\r\n                    training_step = sess.run([global_step])\r\n                    step_num = int(training_step[0])\r\n\r\n                    res = sess.run([model.tower_print_loss,\r\n                                    model.tower_accu,\r\n                                    #model.tower_print_loss_2,\r\n                                    #model.tower_accu_2,\r\n                                    model.batch_size,\r\n                                    #model.fsrc,\r\n                                    #model.ftgt,\r\n                                    train_op\r\n                                    ])", "comments": ["with this code, work:1 always prints \"Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized\" while work:0 is training", "Please refer to this doc: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute#multi-worker-training\r\n\r\nAnd look here for examples: https://github.com/tensorflow/ecosystem/tree/master/distribution_strategy"]}, {"number": 25195, "title": "Analysis of target '//tensorflow/contrib/resampler:resampler_ops_xla_test_gpu' failed", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.13\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: CUDA 10, cudnn 7.4.2\r\n- GPU model and memory: 2 x RTX 2080 Ti\r\n\r\n**Describe the problem**\r\n\r\nDuring the build process described on the tensorflow website for r1.13, the command bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/... fails due to not finding the package '@com_google_protobuf//'. The full error message is:\r\n\r\n```\r\nERROR: /media/titan/resources/oss/machine-learning/Google/tensorflow/tensorflow/tensorflow/contrib/resampler/BUILD:103:1: in deps attribute of py_test rule //tensorflow/contrib/resampler:resampler_ops_xla_test_gpu: '//tensorflow/compiler/tf2xla/kernels:resampler_ops' does not have mandatory providers: 'py'. Since this rule was created by the macro 'tf_xla_py_test', the error might have been caused by the macro implementation in /media/titan/resources/oss/machine-learning/Google/tensorflow/tensorflow/tensorflow/compiler/tests/build_defs.bzl:94:20\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Create new conda environment\r\n2. Install Bazel using binary installer\r\n3. Install dependencies for the build\r\n4. Run `./configure` in the cloned repo with `r1.13` checked-out\r\n5. Run `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...`\r\n\r\nThen I encounter the mentioned error.\r\n", "comments": ["Same issue here, but on CentOS 7, Python 2.7, GPU V100.\r\n\r\nThis issue has also been discussed in #25000 and various solutions suggested there have not worked for me; e.g.:\r\n* protobuf 3.6.1.2, bazel, 0.19.2\r\n* protobuf 3.6.1.2, bazel, 0.19.2, with cherry-pick https://github.com/tensorflow/tensorflow/commit/8c22259497e9914c37a7adcd33aebaf754473a02\r\n* protobuf 3.6.1.2, bazel, 0.21.0, with cherry-pick https://github.com/tensorflow/tensorflow/commit/8c22259497e9914c37a7adcd33aebaf754473a02\r\n\r\nAnything else I should try?\r\n\r\nErrors:\r\nERROR: /home/dev/tensorflow/v1.13.0-rc0/tensorflow/tensorflow/contrib/resampler/BUILD:103:1: in deps attribute of py_test rule //tensorflow/contrib/resampler:resampler_ops_xla_test_gpu: '//tensorflow/compiler/tf2xla/kernels:resampler_ops' does not have mandatory providers: 'py'. Since this rule was created by the macro 'tf_xla_py_test', the error might have been caused by the macro implementation in /home/dev/sources/tensorflow/v1.13.0-rc0/tensorflow/tensorflow/compiler/tests/build_defs.bzl:94:20\r\n\r\nERROR: Analysis of target '//tensorflow/contrib/resampler:resampler_ops_xla_test_gpu' failed; build aborted: Analysis of target '//tensorflow/contrib/resampler:resampler_ops_xla_test_gpu' failed; build aborted\r\n", "Came across the same error when running the test `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...`. Searched online everywhere but couldn't solve it. I had to ignore the test and went on to build the package using `bazel build --config=opt --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package`. To my surprise the build was successful and installing `tensorflow 1.13` was successful too. I tested several applications and also tried training, everything went on smoothly without reporting any bug. \r\n\r\nNow I am not sure why is it like this. Anyone knows?", "Unfortunately, TF tests include a lot of tests that are only valid in very specific cases. Therefore, the test command we have in the build instructions can fail a lot erroneously. @lamberta Maybe we should remove the \"bazel test\" command from install from source instructions.\r\n\r\n", "@gunan While it would be nice to have an easy way to test this, I agree this command seems to change enough that keeping it up to date is a pain. I have a CL out to remove this section.\r\n", "Also, I added the necessary tag filters to the bazelrc, which should help with this. Closing the issue."]}, {"number": 25194, "title": "Keras added missing test case in image_test", "body": "Missing test cases added", "comments": []}, {"number": 25193, "title": "TensorflowLite GPU + DeepLab porting", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.12\r\n- Python version:2.7\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.0\r\n- GPU model and memory:k80 + 12GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI tried to work Tensorflowlite +gpu + deeplabv3_257_mv_gpu.tflite, but not working\r\nrunInference was success.\r\n(1) how can use output (pixelClasses)? Can u give sample code? I attached my code.\r\n(2) performance was very slow 600ms~900ms\r\n\r\n[ImageSegmentation.zip](https://github.com/tensorflow/tensorflow/files/2795483/ImageSegmentation.zip)\r\n\r\n```\r\n  protected float[][][][] pixelClasses;\r\n  pixelClasses = new float[1][getImageSizeX()][getImageSizeY()][21];\r\n Bitmap classifyFrame(Bitmap bitmap, SpannableStringBuilder builder) {\r\n        if (tflite == null) {\r\n            Log.e(TAG, \"Image classifier has not been initialized; Skipped.\");\r\n            builder.append(new SpannableString(\"Uninitialized Classifier.\"));\r\n            return bmp;\r\n        }\r\n        convertBitmapToByteBuffer(bitmap);\r\n        long startTime = SystemClock.uptimeMillis();\r\n        runInference();\r\n        long endTime = SystemClock.uptimeMillis();\r\n        Log.d(TAG, \"Timecost to run model inference: \" + Long.toString(endTime - startTime));\r\n        int batchNum = 0;\r\n        float[][][][] output = pixelClasses;\r\n         for(int x = 0; x < getImageSizeX(); x++) {\r\n            for (int y = 0; y < getImageSizeY(); y++) {\r\n                pixels[x * bmp.getHeight() + y] = argb(100, (int)(output[batchNum][y][x][2]), (int)(output[batchNum][y][x][1]), (int)(output[batchNum][y][x][0]));\r\n            }\r\n        }\r\n        bmp.setPixels(pixels, 0, bmp.getWidth(), 0, 0, bmp.getWidth(), bmp.getHeight());\r\n        return bmp;\r\n    }\r\n```\r\n\r\n**Describe the expected behavior**\r\noverlay color.\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["Did you get a chance to look at [tensorflow-lite-now-faster-with-mobile-gpus-doc](https://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7?linkId=62443226)?.", "@ymodak Thanks you comment. I did try but i want to have example code. Can you give sample?\r\n\r\nAnyone help me. ", "@ilous12 for (2) the GPU delegate performance problem, I got ~ 91 ms per inference for the `deeplabv3_257_mv_gpu.tflite` on Pixel 2, which looks reasonable to me. Test with my [quick and dirty little test program](https://github.com/freedomtan/glDelegateBench).", "Also refer [TensorFlow Lite GPU Delegate Tutorial](https://www.tensorflow.org/lite/performance/gpu).", "@ilous12 \r\n\r\nMy understanding is that the model has 21 classes:\r\n\r\nhttps://medium.freecodecamp.org/how-to-use-deeplab-in-tensorflow-for-object-segmentation-using-deep-learning-a5777290ab6b\r\n\r\nAnd given the dimensions, I would *assume* that a pixel at position [1, H, W, C] would indicate the probability of the pixel [H, W] belonging to class C.  Thus, you would want to find out which channel has the max value for the pixel [H, W] and then mark that pixel as belonging to a \"person\" or a \"bird\".", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hi guys. Thanks your helps.\r\n\r\nI has a question.\r\n\r\nWhen I saw deeplabv3plus codes, input_name was \"ImageTensor\" and output_name was \"SemanticPredictions\".\r\n\r\nBut deeplabv3_257_mv_gpu.tflite was inputname : sub_7, out_name : resizeBilinear_3.\r\nI did not understand why is different ?\r\n\r\nAnd I trained deeplab3plus(https://github.com/tensorflow/models/tree/master/research/deeplab), I checked by tensorboard, there is no resizeBilinear_3.\r\n\r\ntell me why anybody.\r\n", "@ilous12 \r\n\r\nre: input\r\n\r\nI have not run deeplabv3plus code you am referring to, but it looks like the input tensor usually goes through some normalization, e.g. 2.0 * (x / 255.0) - 1.0.  Apparently that has been removed, assuming the user would take care of the normalization before running the input.\r\n\r\nre: output\r\n\r\nI do see multiple resize bilinears in the model file?  https://github.com/tensorflow/models/blob/master/research/deeplab/model.py\r\nMust be one of those resize bilinears.  _3 is just appended to avoid name collision.", "We tried the same models and it was giving around 500ms on (OnePlus3 with Adreno 530) with proper segmentation mask.\r\nTry using a smaller input size for better performance (at cost of accuracy offcourse).. \r\nSee:https://github.com/intel/webml-polyfill/tree/master/examples/semantic_segmentation/model \r\nfor model conversion commands.\r\nAlso if we use quantized version of same model with multiple threads it seems to give a better performance than Float models on CPU.\r\nHope you have tried official gpu demo app from repo for classification(i.e  from tensorflow/tensorflow/lite/java/demo).\r\nWe can use an 'argmax' function to get pixel classes from the output.\r\nAlso, if we can use the gpu memory efficiently without additional copying (like: camera bytes->bitmap->intpixels-> model) as mentioned in the documentation, may be we can get better speed up overall...\r\nAlso checkout:https://github.com/dailystudio/ml/tree/master/deeplab", "@ilous12 the GPU delegate doesn't support ops before sub_7 and after ResizeBilinear_3 yet. With tools, such as [`//tensorflow/tools/graph_transforms:summarize_graph`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/summarize_graph_main.cc), you should be able to see ResizeBilinear_3.\r\n", "@freedomtan \r\n\r\n> @ilous12 the GPU delegate doesn't support ops before sub_7 and after ResizeBilinear_3 yet. With tools, such as //tensorflow/tools/graph_transforms:summarize_graph, you should be able to see ResizeBilinear_3\r\n\r\ni have trained deeplab on custom dataset,but when i visualise my trained model on netron i notice that it contains only sub_2 not sub_7 and resizeBilinear_2 not resize_3 even using this script \r\n[https://github.com/tensorflow/models/blob/master/research/deeplab/local_test_mobilenetv2.sh](https://github.com/tensorflow/models/blob/master/research/deeplab/local_test_mobilenetv2.sh)\r\ni got the same architecture \r\nso when i deploy the model on mobile i got a slow segmentation, is'it the problem because of the sub2 and resizeBilinear or what??\r\n`tflite_convert ----output_format=TFLITE --inference_type=FLOAT --inference_input_type=FlOAT --input_arrays=sub_2 --input_shapes=1,257,257,3 --output_arrays=ResizeBilinear_2 --output_file=mobilenet.tflite --grap-def=mobilenet.pb --mean_values=128 --std_dev_values=127 --allow_custom_ops --post_training_quantize`", "Why it should be `out_pixel[c] = (input_pixel[c] - input_mean) / input_std;` in `ProcessInputWithFloatModel` function? Rather than `input_pixel[c] / 255.0`"]}, {"number": 25192, "title": "Fix incorrect link in the docs", "body": "This fix fixes incorrect link in the doc.", "comments": []}, {"number": 25191, "title": "convert densenet169 model to .tflite model fails, report lack min/max data", "body": "### System information\r\n- TensorFlow-gpu 1.12\r\n-OS Platform e.g., Linux Ubuntu 18.\r\n-Python version3.5\r\n- gtx1070\r\n-bash scripts:\r\necho 'starting convert fake model into tflite model:'${model_name}.....\r\n\r\n    tflite_path=\"./models/${model_name}/fake/model.tflite\"\r\n\r\n    test -f ${tflite_path} || tflite_convert --output_file=./models/${model_name}/fake/model.tflite \\\r\n                    --graph_def_file=./models/${model_name}/fake/model.pb \\\r\n                        --inference_type=QUANTIZED_UINT8 \\\r\n                            --input_arrays=input \\\r\n                            --output_arrays=${model_output} \\\r\n                            --mean_values=${mean_values} \\\r\n                            --std_dev_values=${std_dev_values}\r\n\r\n-report nessage:\r\nTraceback (most recent call last):\r\n  File \"/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/venv/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 412, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/venv/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/venv/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 408, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/venv/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 162, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/home/venv/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py\", line 453, in convert\r\n    **converter_kwargs)\r\n  File \"/home/venv/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py\", line 342, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/home/venv/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py\", line 135, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\nb\"2019-01-24 11:24:42.409109: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2871 operators, 4306 arrays (0 quantized)\\n2019-01-24 11:24:42.496442: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2871 operators, 4306 arrays (0 quantized)\\n2019-01-24 11:24:43.560215: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 514 operators, 1027 arrays (1 quantized)\\n2019-01-24 11:24:43.575464: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 514 operators, 1027 arrays (1 quantized)\\n2019-01-24 11:24:43.584665: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 430 operators, 943 arrays (1 quantized)\\n2019-01-24 11:24:43.594373: F tensorflow/contrib/lite/toco/tooling_util.cc:1634] Array densenet169/dense_block1/conv_block1/x1/BatchNorm/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array densenet169/dense_block1/conv_block1/x1/Relu, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\\nAborted (core dumped)\\n\"\r\nNone\r\n\r\n\r\nwhen i convert the densenet .pb file to .tflite and i failed ,maybe that because lack of min/max value but i not sure how to fix this ,i there anyone who have get this problem or have any solution for this problem ?please let me know and i will appriciate it.", "comments": ["@Vincent630 : You are correct it is due to missing minimum and maximum values. For quantization you can get this information through:\r\n1.  [quantized training](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/README.md), this maybe hard since you may need to change the training code and retrain your model but this will give you better accuracy.\r\n2. The other option is [post training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization] which is simpler but may not give you as good accuracy as quantized training can.\r\n\r\nSince you are using densenet, we also have similar quantized models available in [model repo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models.md)", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to inactivity, @Vincent630 : please ping back if this is still an issue."]}, {"number": 25190, "title": "How to obtain testing_list.txt ?", "body": "Dear,\r\nIn the [project,](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands)\r\nthe testing_list.txt in data_dir file is not written in training data,it existed before,\r\nso how to write the testing_list.txt with my data?\r\nThx", "comments": ["testing_list.txt should only be used for testing outside of TensorFlow, so I don't think this is a bug? Closing, but please file a Stack Overflow issue if you need help doing something with TF."]}, {"number": 25189, "title": "Added SoftShrink transfer operator for Keras", "body": "New operator along with test cases added to Keras", "comments": ["@tanzhenyu , @pavithrasv , @nataliaponomareva & @rthadur can you pls review the PR", "@tanzhenyu , @pavithrasv , @nataliaponomareva & @rthadur can you pls review the PR", "Nagging Reviewer @pavithrasv: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 44 days with no activity and the `awaiting review` label has been applied.", "This looks like a candidate for add-ons repo and we can may be move it to core if the layer is getting used widely."]}, {"number": 25188, "title": "Fix incorrect link in the docs", "body": "This fix fixes incorrect link in the doc where there\r\nis a duplicate: `http://https://` -> `https://`\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 25187, "title": "IExecutionContext in TRTEngineOp is non thread-safe. ", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home: CentOS 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.11\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.18\r\n- GCC/Compiler version (if compiling from source): gcc 5.4\r\n- CUDA/cuDNN version: cuda-10.0, cudnn 7.3, tensorrt 5.0\r\n- GPU model and memory: Nvidia P40\r\n\r\n\r\n**Describe the Problem**\r\nWe are using DirectSession to do inference with c++ code in gpu environment. The model using TRTEngineOp to improve performance. But TRTEngineOp maintain a engine_map to process input.  This map used to keep tensorrt engines and their tensorrt execution context for given batch size. \r\nThe tensorrt engine(nvinfer1::ICudaEngine) is thread safety. But the tensorrt execution context(nvinfer1::IExecutionContext) is non thread-safe. So it will crashed when session processing two same batch_size task in the same time.\r\n\r\n**Describe the expected behavior**\r\nWe hope the engine_map in TRTEngineOp can maintain a pair, which the first value can be tensorrt engine.  the sescond value can be a tensorrt context pool.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nCrashed when session process two task in the same time.\r\n<pre><code>\r\n2019-01-25 11:57:59.952217: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-01-25 11:57:59.952296: F external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1\r\n2019-01-25 11:57:59.952430: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:1011] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered :: *** Begin stack trace ***\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\tclone\r\n*** End stack trace ***\r\n<code><pre>", "comments": ["@trevor-m @pooyadavoodi  Can you PTAL", "@trevor-m @pooyadavoodi, I believe this is fixed. Is that correct?", "I believe this was fixed in https://github.com/tensorflow/tensorflow/commit/e51fa30400b3a469b03111b25344bc47bdff96bd.  Please reopen if not."]}, {"number": 25186, "title": "Fix README.md typo error", "body": "", "comments": []}, {"number": 25185, "title": "tf 2.0 preview installed successfully, unable to import tensorflow, ImportError: DLL load failed", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home\r\n- TensorFlow version: tf 2.0 preview gpu\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10\r\n- GPU model and memory: Nvidia GTX 1050\r\n\r\n**Describe the problem**\r\nWindows 10 Home, Nvidia 1050 gpu, tensorflow installed successfully using \r\n`pip install tf-nightly-gpu-2.0-preview`\r\nHowever, I am unable to import tensorflow, please see log below.\r\n\r\n**Any other info / logs**\r\n\r\n```\r\n(tf2wpy36) C:\\Users\\prach>python\r\nPython 3.6.6 | packaged by conda-forge | (default, Jul 26 2018, 11:48:23) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\site-packages\\tensorflow\\__init__.py\", line 27, in <module>\r\n    from tensorflow._api.v2 import autograph\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\site-packages\\tensorflow\\_api\\v2\\autograph\\__init__.py\", line 20, in <module>\r\n    from tensorflow._api.v2.autograph import experimental\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\site-packages\\tensorflow\\_api\\v2\\autograph\\experimental\\__init__.py\", line 8, in <module>\r\n    from tensorflow.python.autograph import Feature\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\prach\\Anaconda3\\envs\\tf2wpy36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n```\r\n\r\n\r\n\r\n", "comments": ["I was able to resolve the issue by doing the following:\r\n\r\n1. Uninstall tensorflow\r\n2. Re-install CUDA 10 without VS integration\r\n3. Download and install CuDNN 7.4.2\r\n4. Install Tensorflow 2.0 GPU preview by running the command `pip install tf-nightly-gpu-2.0-preview`", "I have the same problem.\r\nI installed the CUDA10.1 and cuDNN10.1. \r\nI install tensorflow-gpu :\r\n pip3 install tensorflow-gpu==2.0.0-beta1 \r\n", "I have the same issue with:\r\n\r\nwindows 7\r\nPython 3.6.9\r\nCuda 10.0\r\ntf-nightly-gpu-2.0-preview\r\n", "I have the same issue with:\r\nWindows 10 1903\r\nPython 3.6.6 and 3.6.9\r\nCuda 10.0\r\ncudnn 7.6.1\r\ntensorflow-gpu==2.0.0-beta1"]}, {"number": 25184, "title": "Workaround MSVC bug that std::isnan cannot handle integral type", "body": "#15213", "comments": ["@jlebar Is this the right place to add the test?\r\nhttps://github.com/tensorflow/tensorflow/blob/70298632d0e4e2b5cd3b9270dd44b2d1bb24a874/tensorflow/compiler/xla/service/hlo_evaluator_test.cc#L134", "@rongjiecomputer yes!", "Ping. Test failures should be unrelated to my change, do I need to rebase?", "@hgadig I believe this is ready to pull?", "> @hgadig I believe this is ready to pull?\r\n\r\n@jlebar  Looks like the CL has merge conflicts. Request you to take a look.", "> Request you to take a look.\r\n\r\nSure, since this appears to be our mistake that this sat for two weeks with no response, I'd like to try merging this PR without asking the contributor to rebase.\r\n\r\nWhat's the best way to do this?", "@hgadig friendly ping", "@hgadig FYI I will be OOO tomorrow through Tuesday, so if we can't fix this today, please send the internal review to timshen or sanjoy.", "> @hgadig FYI I will be OOO tomorrow through Tuesday, so if we can't fix this today, please send the internal review to timshen or sanjoy.\r\n\r\nSure. I'm working with Yifei to resolve internal merge conflicts. Thanks !", "woohoo, thank you for your patience here.  Very happy this one is done."]}, {"number": 25183, "title": "Do not enable TF_GENERATE_BACKTRACE on Windows", "body": "For some reason `TF_GENERATE_BACKTRACE` gets defined when building XLA on Windows.\r\n\r\n#15213", "comments": ["\"Ubuntu Python3 PIP\", \"Windows Bazel\" and \"Windows Bazel GPU\" all failed. First two are test failures, the last one is cuda compiler crashing. Doesn't look like they are caused by my change."]}, {"number": 25182, "title": "Define _USE_MATH_DEFINES to get math constants for MSVC", "body": "#15213", "comments": []}, {"number": 25181, "title": "Accept h5py.Group in tf.keras save/load functions", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe save_model function in tf.keras.models currently accepts a filepath or a h5py.File model.\r\nIt would be nice if this was generalized to accepting h5py.Group objects. This makes it possible to store more than one model in one h5py file, or combine it with other objects.\r\nThis change has been implemented in keras.io as well (I'm not the OP), and it would be very nice to see this in TF.keras as well: \r\n* https://github.com/keras-team/keras/issues/10905 \r\n* https://github.com/keras-team/keras/pull/10912\r\n\r\n**Will this change the current api? How?**\r\nsave_model and load_model would accept filepaths and h5py.Group, which is more general.\r\n\r\n**Who will benefit with this feature?**\r\nEverybody using the keras h5py saving/loading options who wants to store complexer models such as ensembles.\r\n\r\nHappy to provide a PR if this has a chance to be merged.\r\n\r\nBest, Boris", "comments": ["@bolau It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!\r\n", "Just a noob question for now: what branch should I work from?", "You can [submit a PR](https://help.github.com/en/articles/creating-a-pull-request) referencing master branch. Thanks!", "Please CC me if you do a PR for this.", "Hi @bolau,  This has gone silent for a while, and can I take the torch from your hands and work on this one? ", "@lsgrep I would appreciate if you or someone else takes this up, I was sad to discover that this feature has not yet been transferred from keras to tensorflow.keras", "@bolau \r\nThis issue is resolved in the latest tf version, please verify an revert.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 25180, "title": "TFTRT: Set engine output datatypes based on what TF is expecting", "body": "TRT requires the datatype of engine output tensors to be set explicitly. We will now tell TRT to output in the type that tensorflow is expecting.", "comments": []}, {"number": 25179, "title": "how can I solve this error InvalidArgumentError during visualizing the output?", "body": "I had the following code for implementing an autoencoder. I implement it and after learning phase, I want to visualize the output of each layer, but when I want to show the output of layer 5 and more it produces this error and I can not see the output of layers. I'm a little confused. if I do not feed a value, how does the network finish training? I also can show the output of layers before 5th layer. please help me with this problem.\r\n  the error \r\n\r\n> InvalidArgumentError: You must feed a value for placeholder tensor\r\n> 'input_78' with dtype float and shape [?,28,28,1] \t [[{{node\r\n> input_78}} = Placeholder[dtype=DT_FLOAT, shape=[?,28,28,1],\r\n> _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]] \t [[{{node lambda_35/add/_2359}} = _Recv[client_terminated=false,\r\n> recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\",\r\n> send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\",\r\n> send_device_incarnation=1, tensor_name=\"edge_50_lambda_35/add\",\r\n> tensor_type=DT_FLOAT,\r\n> _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\n     \r\n\r\n   ```\r\n```\r\n```\r\n[ from keras.layers import Input, Concatenate, GaussianNoise,Dropout\r\n    from keras.layers import Conv2D\r\n    from keras.models import Model\r\n    from keras.datasets import mnist\r\n    from keras.callbacks import TensorBoard\r\n    from keras import backend as K\r\n    from keras import layers\r\n    import matplotlib.pyplot as plt\r\n    import tensorflow as tf\r\n    import keras as Kr\r\n    import numpy as np\r\n    import pylab as pl\r\n    import matplotlib.cm as cm\r\n```\r\n```\r\n    \r\n    #-----------------building w train---------------------------------------------\r\n    w_main = np.random.randint(2,size=(1,4,4,1))\r\n    w_main=w_main.astype(np.float32)\r\n    w_expand=np.zeros((1,28,28,1),dtype='float32')\r\n    w_expand[:,0:4,0:4]=w_main\r\n    w_expand.reshape(1,28,28,1)\r\n    w_expand=np.repeat(w_expand,49999,0)\r\n    \r\n    #-----------------building w test---------------------------------------------\r\n    w_test = np.random.randint(2,size=(1,4,4,1))\r\n    w_test=w_test.astype(np.float32)\r\n    wt_expand=np.zeros((1,28,28,1),dtype='float32')\r\n    wt_expand[:,0:4,0:4]=w_test\r\n    wt_expand.reshape(1,28,28,1)\r\n    wt_expand=np.repeat(wt_expand,9999,0)\r\n    #-----------------------encoder------------------------------------------------\r\n    #------------------------------------------------------------------------------\r\n    wtm=Input((28,28,1))\r\n    image = Input((28, 28, 1))\r\n    conv1 = Conv2D(16, (3, 3), activation='relu', padding='same', name='convl1e')(image)\r\n    conv2 = Conv2D(8, (3, 3), activation='relu', padding='same', name='convl2e')(conv1)\r\n    conv3 = Conv2D(8, (3, 3), activation='relu', padding='same', name='convl3e')(conv2)\r\n    DrO=Dropout(0.25)(conv3)\r\n    encoded =  Conv2D(1, (3, 3), activation='relu', padding='same',name='reconstructed_I')(conv3)\r\n    \r\n    \r\n    #-----------------------adding w---------------------------------------\r\n    #add_const = Kr.layers.Lambda(lambda x: x + Kr.backend.constant(w_expand))\r\n    add_const = Kr.layers.Lambda(lambda x: x + wtm)\r\n    encoded_merged = add_const(encoded)\r\n    \r\n    encoder=Model(inputs=image, outputs=encoded_merged)\r\n    encoder.summary()\r\n    \r\n    #-----------------------decoder------------------------------------------------\r\n    #------------------------------------------------------------------------------\r\n    \r\n    #encoded_merged = Input((28, 28, 2))\r\n    deconv1 = Conv2D(8, (3, 3), activation='relu', padding='same', name='convl1d')(encoded_merged)\r\n    deconv2 = Conv2D(8, (3, 3), activation='relu', padding='same', name='convl2d')(deconv1)\r\n    deconv3 = Conv2D(16, (3, 3), activation='relu',padding='same', name='convl3d')(deconv2)\r\n    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same', name='decoder_output')(deconv3) \r\n    \r\n    #decoder=Model(inputs=encoded_merged, outputs=decoded)\r\n    #decoder.summary()\r\n    model=Model(inputs=image,outputs=decoded)\r\n    #----------------------w extraction------------------------------------\r\n    convw1 = Conv2D(8, (3,3), activation='relu', padding='same', name='conl1w')(decoded)\r\n    convw2 = Conv2D(4, (3, 3), activation='relu', padding='same', name='convl2w')(convw1)\r\n    convw3 = Conv2D(2, (3, 3), activation='relu', padding='same', name='conl3w')(convw2)\r\n    pred_w = Conv2D(1, (3, 3), activation='relu', padding='same', name='reconstructed_W')(convw3)  \r\n    # reconsider activation (is W positive?)\r\n    # should be filter=1 to match W\r\n    watermark_extraction=Model(inputs=[image,wtm],outputs=[decoded,pred_w])\r\n    \r\n    \r\n    #----------------------training the model--------------------------------------\r\n    #------------------------------------------------------------------------------\r\n    #----------------------Data preparesion----------------------------------------\r\n    \r\n    (x_train, _), (x_test, _) = mnist.load_data()\r\n    x_validation=x_train[1:10000,:,:]\r\n    x_train=x_train[10001:60000,:,:]\r\n    #\r\n    x_train = x_train.astype('float32') / 255.\r\n    x_test = x_test.astype('float32') / 255.\r\n    x_validation = x_validation.astype('float32') / 255.\r\n    x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\r\n    x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format\r\n    x_validation = np.reshape(x_validation, (len(x_validation), 28, 28, 1))\r\n    \r\n    #---------------------compile and train the model------------------------------\r\n    # is accuracy sensible metric for this model?\r\n    watermark_extraction.compile(optimizer='adadelta', loss={'decoder_output':'mse','reconstructed_W':'mse'}, metrics=['mae'])\r\n    watermark_extraction.fit([x_train,w_expand], [x_train,w_expand],\r\n              epochs=4,\r\n              batch_size=128, \r\n              validation_data=([x_validation,wt_expand], [x_validation,wt_expand]),\r\n              callbacks=[TensorBoard(log_dir='C:/tmp/autoencoder', histogram_freq=0, write_graph=False)])\r\n    model.summary()\r\n    #model.fit([images, w], [images, w], batch_size=64, epochs=5)\r\n    \r\n    #--------------------visuallize the output layers------------------------------\r\n    inputs = [K.learning_phase()] + watermark_extraction.inputs\r\n    \r\n    _convout1_f = K.function(inputs, [watermark_extraction.layers[5].output])\r\n    def convout1_f(X):\r\n        # The [0] is to disable the training phase flag\r\n        return _convout1_f([0] + [X])\r\n    \r\n    # utility functions \r\n    i = 4600\r\n    \r\n    # Visualize the first layer of convolutions on an input image\r\n    X = x_test[i:i+1]    \r\n    # Visualize weights\r\n    W = model.layers[1].get_weights()[0][:,:,0,:]\r\n    w1=W.reshape(16,3,3)\r\n    W = np.squeeze(w1)\r\n    print(\"W shape : \", W.shape)\r\n    \r\n    for i in range(0,16):\r\n        plt.subplot(4,4,i+1)\r\n        plt.imshow(w1[i,:,:], interpolation='nearest',cmap='gray')\r\n    plt.show()\r\n    W = model.layers[2].get_weights()[0][:,:,0,:]\r\n    w2=W.reshape(8,3,3)\r\n    W = np.squeeze(w2)\r\n    print(\"W shape : \", W.shape)\r\n    \r\n    for i in range(0,8):\r\n        plt.subplot(4,4,i+1)\r\n        plt.imshow(w2[i,:,:], interpolation='nearest',cmap='gray')\r\n    plt.show()\r\n    \r\n    \r\n    # Visualize convolution result (after activation)\r\n    C1 = convout1_f(X)\r\n    C1 = np.squeeze(C1)\r\n    print(\"C1 shape : \", C1.shape)\r\n    for i in range(0,C1.shape[2]):\r\n        plt.subplot(4,4,i+1)\r\n        plt.imshow(C1[:,:,i], interpolation='nearest',cmap='gray')\r\n    plt.show()\r\n```", "comments": ["no one can help me? I could not understand why this error has happened?I need to visualize the output of layers after adding labmda layer.:(\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 25178, "title": "Inconsistent behavior of sample weight for Keras", "body": "TF: home compiled master from a few days ago\r\nKeras: 2.2.4 (pypi)\r\n\r\nHi\r\n\r\nConsider the following snippet\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport keras\r\nfrom tensorflow import keras as tf_keras\r\n\r\nx = np.array([[1.], [2], [3]])\r\ny = np.array([[1], [4], [5]])\r\nw = np.array([0.1, 0., 0.1])\r\n\r\ni_x = keras.layers.Input(shape=(1,))\r\nmodel = keras.Model(inputs=i_x, outputs=i_x)\r\n\r\nmodel.compile('sgd', 'mse')\r\nprint(model.evaluate(x, y, sample_weight=w, verbose=False))\r\n# ---> 0.2 = sum(w * squared_diff) / count(w not null)\r\n\r\nmodel.compile('sgd', keras.losses.mean_squared_error)\r\nprint(model.evaluate(x, y, sample_weight=w, verbose=False))\r\n# ---> 0.2\r\n\r\ni_x = tf_keras.layers.Input(shape=(1,))\r\nmodel = tf_keras.Model(inputs=i_x, outputs=i_x)\r\n\r\nmodel.compile('sgd', 'mse')\r\nprint(model.evaluate(x, y, sample_weight=w, verbose=False))\r\n# ---> 0.13333 = mean(w * squared_diff)\r\n\r\nmodel.compile('sgd', keras.losses.mean_squared_error)\r\nprint(model.evaluate(x, y, sample_weight=w, verbose=False))\r\n# ---> 2.0 = mean(w * squared_diff) / mean(w)\r\n```\r\n\r\nThis seems a bit all over the place. Keras behavior is cringeworthy as the loss is discontinuous in `w` . For w[1] = 1e-6, one gets 0.13333... but at least its consistent.\r\n\r\nI don't know what should be done but I'm sure others have wasted half a day of work because of this ;)\r\n\r\n", "comments": ["Possible #23767?", "Hi @0x0L, thanks for the issue!\r\n\r\nIt looks like the inconsistency b/t `'mse'` and `tf.keras.losses.mean_squared_error` is fixed in the latest nightly, I'm seeing `0.13` for both.\r\n\r\nI think the external Keras logic should be updated to reflect the `tf.keras` logic, as `0` values are being considered as `null` in external Keras and thus increasing the effective sample weight for other samples\r\n\r\nMade a tracking bug in external Keras: https://github.com/keras-team/keras/issues/12176", "@omalleyt12 \r\nHi, thanks for testing this. I wasn't able to try tf-nightly since I'm using python 3.7.\r\nI guess we should close this issue ?\r\n\r\nEDIT: I just recompiled master and can confirm tf gives 0.13 for both losses"]}, {"number": 25177, "title": "tf.function decorated training loop not working with tf-nightly-gpu-2.0-preview==2.0.0rc0", "body": "Using the autograph example, as is, from https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/autograph.ipynb\r\n\r\nWhen trying to run the 'Define the Training Loop' cell, I get this error\r\n![image](https://user-images.githubusercontent.com/93366/51704290-80bae700-1fcd-11e9-9a99-8bfcc9da4e1f.png)\r\n\r\n```\r\nFailedPreconditionError: Error while reading resource variable _AnonymousVar3 from Container: localhost. This could mean that the variable was uninitialized. Invalid argument: Trying to access resource _AnonymousVar3 located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\r\n\t [[{{node sequential/dense_1/MatMul/ReadVariableOp}}]]\r\n\t [[ReduceDataset]]\r\n\t [[ReduceDataset/_48]] [Op:__inference_train_915]\r\n```\r\n\r\nTensorflow version: tf-nightly-gpu-2.0-preview==2.0.0rc0\r\n\r\nSystem information:\r\n\r\n== cat /etc/issue ===============================================\r\nLinux 797af6742322 4.4.0-141-generic #167-Ubuntu SMP Wed Dec 5 10:40:15 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"18.04.1 LTS (Bionic Beaver)\"\r\nVERSION_ID=\"18.04\"\r\nVERSION_CODENAME=bionic\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 797af6742322 4.4.0-141-generic #167-Ubuntu SMP Wed Dec 5 10:40:15 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                            1.16.0\r\nprotobuf                         3.6.1\r\ntensorflow-estimator-2.0-preview 1.13.0.dev2019012200\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Jan 24 19:41:50 2019\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  TITAN V             Off  | 00000000:01:00.0  On |                  N/A |\r\n| 28%   38C    P8    27W / 250W |  11614MiB / 12033MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart_static.a\r\n", "comments": ["I was able to execute the python notebook successfully using Google Colab with 2.0.0-preview. Can you please confirm?\r\nTF 2.0.0.rc0 version is not officially released yet.", "We\u2019re you using the GPU on collab?\n\nThe notebook works fine on my machine using the tf2.0 preview CPU build,\nbut not with the GPU build.\n\nOn Fri, Jan 25, 2019 at 4:55 PM ymodak <notifications@github.com> wrote:\n\n> I was able to execute the python notebook successfully using Google Colab\n> with 2.0.0-preview. Can you please confirm?\n> TF 2.0.0.rc0 version is not officially released yet.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25177#issuecomment-457784008>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAFstgDPVLgOOYYAuMrmS_F0JewPpiefks5vG6dwgaJpZM4aRlxH>\n> .\n>\n-- \n-pr\n", "Yes I changed the notebook settings to GPU accelerator and was able to execute it successfully. Feel free to reopen if still running into problems. Thanks!", "@ymodak Did you test this with tf-nightly-gpu-2.0-preview on Colab? I think this is a separate issue, but CUDA 10 isn't loaded on Colab by default yet: \r\n![image](https://user-images.githubusercontent.com/93366/52244111-f60d9e00-2890-11e9-99fc-bbc3b0e627a2.png)\r\n\r\nMy issue happens on my local machine, using tf-nightly-gpu-2.0-preview and CUDA 10. I tried the latest pip install tf-nightly-gpu-2.0-preview today and I'm still getting the same issue. I'm not sure if this is related to my running this in an nvidia-docker container, or if this is some other known GPU issue."]}]