[{"number": 25146, "title": "INTEL MKL: Quantized Concat", "body": "This PR adds support for MKL Quantized Concat op.", "comments": ["Thank you @penpornk for the review. I addressed your comments. Please let me know if anything else is needed. ", "@penpornk Sure, no problem. I added a comment. Please let me know if anything else is needed. ", "This PR was rolled back because of test failures. I'll put the changes back once it doesn't cause anymore failures. In the meanwhile, please let me know if there are other PRs that depends on this and shouldn't be merged before this PR's changes are back.", "@penpornk Do you mean that we need to fix something in the PR so it does not cause test failures?", "@mahmoud-abuzaina I already have the fix. The problem came from registering `QuantizedConcatV2` in `quantized_concat_op.cc` without the `#ifdef INTEL_MKL` guard. So non config=mkl builds try to find this op but couldn't find it. \r\n\r\nNow I'm debating whether I should just re-submit the changes internally (but then it wouldn't show that you authored them anymore) or ask you to open a new PR. What do you think?", "The new PR route should also be quick. After you re-create a new PR, please mention me and I can re-review it quickly. Please let me know what way you prefer. :)", "Thank you @penpornk for the explanation. I will create a new PR now and mention you.", "@mahmoud-abuzaina Thank you and sorry for the inconvenience!", "@penpornk I have created the replacement PR #25533. Thank you for your patience."]}, {"number": 25145, "title": "Apollo3 updates for TFLiteMicro", "body": "1)Fixed AP3 BIN generation issue\r\n2)Migrated to public Ambiq Micro SDK", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@snease-abq  Request you to sign the CLA in order to proceed with merging.", "> @snease-abq Request you to sign the CLA in order to proceed with merging.\r\n\r\n@snease-abq  reminder to sign CLA", "New pull request submitted."]}, {"number": 25144, "title": "Build of 1.13 //tensorflow/python/eager:pywrap_tfe_lib fails with internal compiler error: unexpected expression \u2018I\u2019 of kind template_parm_index", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nFedora 28\r\ncuda-cusparse-10.0.130-1.fc28.x86_64\r\ncuda-cufft-devel-10.0.130-1.fc28.x86_64\r\ncuda-nvjpeg-devel-10.0.130-1.fc28.x86_64\r\ncuda-cublas-10.0.130-1.fc28.x86_64\r\ncuda-nvrtc-devel-10.0.130-1.fc28.x86_64\r\ncuda-gcc-c++-7.3.1-2.fc28.x86_64\r\nnvidia-driver-cuda-libs-415.27-1.fc28.x86_64\r\ncuda-cublas-devel-10.0.130-1.fc28.x86_64\r\ncuda-extra-libs-10.0.130-1.fc28.x86_64\r\ncuda-nvtx-10.0.130-1.fc28.x86_64\r\ncuda-cusparse-devel-10.0.130-1.fc28.x86_64\r\ncuda-cupti-10.0.130-1.fc28.x86_64\r\ncuda-gcc-7.3.1-2.fc28.x86_64\r\ncuda-cudart-devel-10.0.130-1.fc28.x86_64\r\ncuda-nvjpeg-10.0.130-1.fc28.x86_64\r\ncuda-curand-10.0.130-1.fc28.x86_64\r\ncuda-npp-devel-10.0.130-1.fc28.x86_64\r\ncuda-cudnn-devel-7.4.2.24-1.fc28.x86_64\r\nbazel-0.21.0-1.fc28.x86_64\r\ncuda-nvgraph-devel-10.0.130-1.fc28.x86_64\r\ncuda-nvgraph-10.0.130-1.fc28.x86_64\r\ncuda-cusolver-devel-10.0.130-1.fc28.x86_64\r\ncuda-npp-10.0.130-1.fc28.x86_64\r\ncuda-cudart-10.0.130-1.fc28.x86_64\r\ncuda-nvml-devel-10.0.130-1.fc28.x86_64\r\nnvidia-driver-cuda-415.27-1.fc28.x86_64\r\ncuda-cusolver-10.0.130-1.fc28.x86_64\r\ncuda-cupti-devel-10.0.130-1.fc28.x86_64\r\ncuda-devel-10.0.130-1.fc28.x86_64\r\ncuda-libs-10.0.130-1.fc28.x86_64\r\ncuda-nvtx-devel-10.0.130-1.fc28.x86_64\r\ncuda-10.0.130-1.fc28.x86_64\r\ncuda-curand-devel-10.0.130-1.fc28.x86_64\r\ncuda-nvrtc-10.0.130-1.fc28.x86_64\r\ncuda-cufft-10.0.130-1.fc28.x86_64\r\ncuda-cudnn-7.4.2.24-1.fc28.x86_64\r\n\r\n- **TensorFlow version (use command below)**: 1.13\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: bazel-0.21.0-1.fc28.x86_64\r\n- **GCC/Compiler version (if compiling from source)**: cuda-gcc-7.3.1-2.fc28.x86_64\r\n- **CUDA/cuDNN version**: cuda-cudnn-7.4.2.24-1.fc28.x86_64\r\n- **GPU model and memory**: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] 11GB RAM\r\n- **Exact command to reproduce**: \r\n- # ./configure                                                                                           \r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: fdbe3c4d-e72c-4726-a665-e2bc04669f74\r\nYou have bazel 0.21.0- (@non-git) installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.6/site-packages\r\n  /usr/lib64/python3.6/site-packages\r\n  /usr/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]: \r\n\r\n\r\nPlease specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: \r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr]: \r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: \r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the locally installed NCCL version you want to use. [Default is to use https://github.com/nvidia/nccl]: \r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 6.1\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/cuda-gcc\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apacha Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n- # bazel build --local_resources 2048,6,1.0 --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n...\r\n...\r\nERROR: /home/spryor/git/tensorflow/tensorflow/python/eager/BUILD:10:1: C++ compilation of rule '//tensorflow/python/eager:pywrap_tfe_lib' failed (Exit 1)\r\ntensorflow/python/eager/pywrap_tfe_src.cc: In function \u2018void TFE_Py_Execute(TFE_Context*, const char*, const char*, TFE_InputTensorHandles*, PyObject*, TFE_OutputTensorHandles*, TF_Status*)\u2019:\r\ntensorflow/python/eager/pywrap_tfe_src.cc:685:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < inputs->size() && TF_GetCode(out_status) == TF_OK;\r\n                     ~~^~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/python/eager/pywrap_tfe_src.cc:23:0:\r\nexternal/com_google_absl/absl/types/variant.h: In substitution of \u2018template<long unsigned int I, class T> using variant_alternative_t = typename absl::variant_alternative::type [with long unsigned int I = I; T = absl::variant<tensorflow::TensorShape, _object*>]\u2019:\r\nexternal/com_google_absl/absl/types/variant.h:581:7:   required by substitution of \u2018template<class T, long unsigned int I, class Tj, typename std::enable_if<(std::is_assignable<Tj&, T>::value && std::is_constructible<Tj, T>::value), void>::type* <anonymous> > absl::variant<tensorflow::TensorShape, _object*>& absl::variant<tensorflow::TensorShape, _object*>::operator=<T, I, Tj, <enumerator> >(T&&) [with T = const absl::variant<tensorflow::TensorShape, _object*>&; long unsigned int I = <missing>; Tj = <missing>; typename std::enable_if<(std::is_assignable<Tj&, T>::value && std::is_constructible<Tj, T>::value), void>::type* <anonymous> = <missing>]\u2019\r\ntensorflow/python/eager/pywrap_tfe_src.cc:914:20:   required from here\r\nexternal/com_google_absl/absl/types/variant.h:581:7: internal compiler error: unexpected expression \u2018I\u2019 of kind template_parm_index\r\n       class Tj = absl::variant_alternative_t<I, variant>,\r\n       ^~~~~\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <https://gcc.gnu.org/bugs/> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 600.890s, Critical Path: 104.25s\r\nINFO: 3259 processes: 3259 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n### Describe the problem\r\nAttempting to build r1.13 on Fedora 28 with the given packages/etc results in a build error\r\n\r\n\r\n[tensorflow_build.log](https://github.com/tensorflow/tensorflow/files/2789013/tensorflow_build.log)\r\n", "comments": ["@gunan, is this related to the gcc bugs  you were mentioning to me yesterday?", "The one I ran into is more around this one:\r\nhttp://eigen.tuxfamily.org/bz/show_bug.cgi?id=1647\r\nThis is I think fixed with  patch releases to GCC.\r\n\r\nNot sure about the compiler error reported on this issue.", "I'm having the same issue when trying to compile Tensorflow 1.13.1 (cpu only). I'm using bazel 0.21.0 on amazon linux. \r\n\r\nI've tried with gcc versions 7.3.0, 7.3.1, 7.4.0, 8.3.0 and got the same error on all of them. Does anyone have a solution for this?", "Managed to build it for skylake(-march=skylake-avx512) , using:\r\nbazel: 0.19.2\r\nprotobuf: 3.5.2\r\npython: 3.7\r\ngcc: 8.2.0\r\n", "I'm getting the same compiler error, using:\r\n- Fedora 29 and the NVidia Drivers + Cuda 10.0 libraries from [Negativo 17](https://negativo17.org/repos/nvidia/fedora-29/x86_64/)\r\n- bazel: 0.19.2\r\n- protobuf: 3.5.0\r\n- python: 3.7.3\r\n- gcc: 7.3.1\r\n- cuda: 10.0\r\n- cudnn: 7.4.2\r\n- nccl: 2.4.2\r\n\r\nI've attempted to match my versions as closely as possible to the [tested build configurations](https://www.tensorflow.org/install/source#linux) but still bombing out with similar output:\r\n\r\n```\r\nERROR: /path truncated for privacy/tensorflow-r1.13/tensorflow/compiler/xla/service/BUILD:3025:1: C++ compilation of rule '//tensorflow/compiler/xla/service:maybe_owning_device_memory' failed (Exit 1)\r\nIn file included from ./tensorflow/compiler/xla/service/maybe_owning_device_memory.h:20:0,\r\n                 from tensorflow/compiler/xla/service/maybe_owning_device_memory.cc:16:\r\nexternal/com_google_absl/absl/types/variant.h: In substitution of 'template<long unsigned int I, class T> using variant_alternative_t = typename absl::variant_alternative::type [with long unsigned int I = I; T = absl::variant<xla::OwningDeviceMemory, stream_executor::DeviceMemoryBase>]':\r\nexternal/com_google_absl/absl/types/variant.h:501:7:   required by substitution of 'template<class T, long unsigned int I, class Tj, absl::enable_if_t<std::is_constructible<Tj, T>::value, void>* <anonymous> > constexpr absl::variant<xla::OwningDeviceMemory, stream_executor::DeviceMemoryBase>::variant(T&&) [with T = absl::variant<xla::OwningDeviceMemory, stream_executor::DeviceMemoryBase>; long unsigned int I = <missing>; Tj = <missing>; absl::enable_if_t<std::is_constructible<Tj, T>::value, void>* <anonymous> = <missing>]'\r\n./tensorflow/compiler/xla/service/maybe_owning_device_memory.h:29:7:   required from here\r\nexternal/com_google_absl/absl/types/variant.h:501:7: internal compiler error: unexpected expression 'I' of kind template_parm_index\r\n       class Tj = absl::variant_alternative_t<I, variant>,\r\n       ^~~~~\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <https://gcc.gnu.org/bugs/> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 506.735s, Critical Path: 50.03s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 3097 processes: 3097 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "Same error here. This occurs on an Amazon Linux x1.32xlarge instance, in the stock Python 3.6 conda environment, when using `--config=opt`.", "@Xaenalt We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "I haven't tried in a while, I'll close it to get it out of the queue, I'll reopen a diff issue for new versions if I hit anything", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25144\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25144\">No</a>\n"]}, {"number": 25143, "title": "[XLA] Ambiguous/dynamic shape during `xla.compile` when using `tf.custom_gradient` and `tf.while_loop`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): public Colab runtime\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3\r\n\r\n**Describe the current behavior**\r\nUsing `xla.compile` on a gradient coming from a while loop within `tf.custom_gradient` seems to generate an inverse permutation op with undefined shape. While all intermediate tensors appear to have fully defined shapes, `xla.compile` will still raise an exception:\r\n\r\n```\r\nInvalidArgumentError: Input 0 to InvertPermutation operator must be a compile-time constant.\r\n\r\nXLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.\r\n\t [[{{node gradients/IdentityN_grad/backwards_fixed_point/fp_solve/while/InvertPermutation}} = InvertPermutation[T=DT_INT32](_arg3, ^_arg0)]]\r\n\t [[{{node gradients/IdentityN_grad/backwards_fixed_point/fp_solve/while/LoopCond}} = While[T=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT], body=_functionalize_body_4[], cond=_functionalize_cond_4[]](gradients/IdentityN_grad/backwards_fixed_point/fp_solve/while/iteration_counter, gradients/pow_grad/Reshape, gradients/IdentityN_grad/backwards_fixed_point/fp_solve/Fill, ConstantFolding/gradients/pow_grad/BroadcastGradientArgs-folded-1, gradients/IdentityN_grad/backwards_fixed_point/fp_solve/while/maximum_iterations, gradients/IdentityN_grad/backwards_fixed_point/Tensordot/Reshape_1, gradients/IdentityN_grad/backwards_fixed_point/fp_solve/Tensordot/Reshape, gradients/pow_grad/Reshape)]]\r\n\t [[{{node cluster}} = XlaLaunch[Nresources=0, Targs=[DT_FLOAT, DT_FLOAT, DT_FLOAT], Tconstants=[], Tresults=[DT_FLOAT, DT_FLOAT], function=cluster_55287096143340976_f15n_0[], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Const, Const_1, zeros)]]\r\n```\r\n\r\n**Code to reproduce the issue**\r\nI've been able to distill the issue into ~150 lines of code and package it in a colab. I'd rather not post this code publicly, any way for me to share it with the dev's without pasting it here?\r\n\r\n\r\n", "comments": ["To facilitate debugging this issue, here is a small [notebook](https://gist.github.com/gehring/802ebce4675360f7467b4e90b16ecd69) (~80 lines) which should reproduce the issue (tensorflow 1.13, python 3.6).\r\n\r\nReplacing the loop in the gradient function with a fixed graph seems to avoid the issue. The problem seems to be related to the combination of `tf.custom_gradient`, `tf.while_loop`, and `backprop.make_vjp`. Maybe @jlebar might know where the issue lies?", "Thank you for the clear STR, @gehring.\r\n\r\n@sanjoy can probably speak to this better than me, but it's not clear to me how XLA is supposed to be able to infer shapes here.  I guess we could theoretically examine the loop and notice that, with a particular set of input shapes, the shapes while running the loop are invariant?  That sounds possible but pretty challenging...\r\n\r\nI think manually rewriting the graph as you did is probably the best solution here.  Or otherwise you could use autojit and then it will cluster around this.", "Thanks for getting back to me so fast! @jlebar, I guess I understand XLA's shape inference a little less than I thought. My understanding is that, in this situation, tensorflow can infer the shapes from the graph but XLA cannot?\r\n\r\nAs for the static graph workaround, in my use case, the condition will likely terminate the loop before the maximum number of iterations. Would the XLA compiler handle chained `tf.cond`, and if so, how is the shape inference possible then? Also, typically, we'd want to use a large, conservative maximum iteration which I'm afraid this will cause memory issues as well as long compile time if unrolled.", "One off-topic observation first: IIUC `InvertPermutation` should not need its input to be a compile time constant since it is essentially a scatter.  I'll fix this, though I don't expect fixing this specific issue to resolve the matter since something else will likely force the output of `InvertPermutation` to be constant (probably a `Transpose`).\r\n\r\nAs a starting point I'd recommend trying out Justin's suggestion of using auto-clustering for this model.  We're trying to make auto-clustering do the Right Thing always.  You can enable auto-clustering by setting `TF_XLA_FLAGS` to `--tf_xla_auto_jit=2` or by setting the `SessionOptions::config::graph_options::optimizer_options::global_jit_level` to `ON_2`.\r\n\r\n> but it's not clear to me how XLA is supposed to be able to infer shapes here\r\n\r\nFor functional while loops, barring some special cases, XLA assumes that the shapes produced by the while loop body stay the same on every iteration.  See tensorflow/compiler/tf2xla/kernels/while_op.cc for the gory details.", "> My understanding is that, in this situation, tensorflow can infer the shapes from the graph but XLA cannot?\r\n\r\nThat's one way of thinking about it, I suppose.  Another way of thinking about it is:\r\n\r\nIn order to run an operation in TensorFlow *or XLA*, TF/XLA needs to know what are the concrete shapes of the input/output and any intermediate results.  In TF \"an operation\" is the degenerate case of one TF op, where it's trivial to know the output shapes given the input shapes.  In XLA, however \"an operation\" is a cluster of multiple TF ops.  This gives XLA additional power, but it comes with restrictions.", "> As a starting point I'd recommend trying out Justin's suggestion of using auto-clustering for this model.\r\n\r\nThanks, I'll look into that!\r\n\r\n> In XLA, however \"an operation\" is a cluster of multiple TF ops. This gives XLA additional power, but it comes with restrictions.\r\n\r\nI'm still confused as to why tensorflow can statically infer all the shapes but XLA cannot but, if this is expected behavior, I don't want to take any more of your time than I already have. If it isn't too inconvenient, would you be able to point out what causes the shape to become dynamic/ambiguous and why it would be hard to avoid it? Maybe I can think of an alternative implementation that would side step the issue.", "> I'm still confused as to why tensorflow can statically infer all the shapes but XLA cannot\r\n\r\nMaybe another way of putting it, if it helps, is that TF runs one operation at a time.  So it needs to infer the shape only of the next operation, after it knows the shapes of each of its inputs.  Whereas XLA needs to infer the shapes many steps into the future.\r\n\r\n> would you be able to point out what causes the shape to become dynamic/ambiguous\r\n\r\nhttps://medium.com/tensorflow/pushing-the-limits-of-gpu-performance-with-xla-53559db8e473 might help, it has some examples of TF code that causes dynamism.", "Thank you for those examples. However, I still am unclear how/where dynamism is introduced in my example. With all shapes constantly defined a priori and the assumption that the body of `tf.while_loop` remains constant, shouldn't shape inference be straightforward?\r\n\r\nAdditionally, I'm also confused by the fact that the code which led me to this issue compiles fine for a cloud TPU using `tf.contrib.tpu.rewrite` but raises the same error when using `xla.compile` (for gpu). I was under the impression that running on TPUs also required compiling to XLA and, as a result, had the same shape restrictions. Are there an important distinctions in the shape constraints between `tf.contrib.tpu.rewrite` and `xla.compile` that I might have missed?", "Good questions, I really don't know.  Perhaps Yanan or @hpucha can speak to this.  (I'm not sure what is Yanan's github handle.)", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "same issue, any solution\r\n\r\n", "CC @cheshire "]}, {"number": 25142, "title": "Update RELEASE.md", "body": "To reflect tf.data changes in 1.13", "comments": []}, {"number": 25141, "title": "RELEASE.md typos", "body": "Fix typos.", "comments": ["Nagging Reviewer : You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 44 days with no activity and the `awaiting review` label has been applied."]}, {"number": 25140, "title": "MKL DNN: Cleaning up mkl relu op (removing MKL ML related code)", "body": "Removed MKL ML related code from mkl relu op. Also did a clang style check and fixed the style.", "comments": []}, {"number": 25139, "title": "Basic Regression Docs Code typo", "body": "https://www.tensorflow.org/tutorials/keras/basic_regression#split_features_from_labels\r\n\r\n```\r\ndef plot_history(history):\r\n  plt.figure()\r\n  plt.xlabel('Epoch')\r\n  plt.ylabel('Mean Abs Error [MPG]')\r\n  plt.plot(hist['epoch'], hist['mean_absolute_error'],\r\n           label='Train Error')\r\n```\r\n\r\nThis function references `hist` instead of `history` so it doesn't parse.", "comments": ["duplicate #25108 \r\nClosing this issue so that we can focus on one thread. Thanks!"]}, {"number": 25138, "title": "Tensorflow v2 Limit GPU Memory usage", "body": "Need a way to prevent TF from consuming all GPU memory, on v1, this was done by using something like:\r\n```\r\nopts = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=opts))\r\n\r\n```\r\n\r\nOn v2 there is no Session and GPUConfig on tf namespace.\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04.1 (AWS EC2 P2)\r\n- TensorFlow installed from (source or binary): pip install tf-nightly-2.0-preview\r\n- TensorFlow version (use command below):'1.13.0-dev20190117'\r\n- Python version:Python 3.6.5\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla K80 12GB\r\n\r\n\r\n", "comments": ["@jaingaurav also transferring this issue to you since you'll be working on it (soon).", "A number of new API were added in `tf.config` namespace to support this use case. Please let me know if there is anything we missed regarding this specific issue.\r\n\r\nNote that here you'd probably have to setup a virtual gpu with a fixed memory limit. Please let us know if there is a strong reason to support the `per_process_gpu_memory_fraction` option.", "The two common settings you earlier used with sessions are found in `tf.config.gpu`. You should probably set them before you start working with any tensors and variables. (tested with 2.0.0-alpha0)\r\n```python\r\nimport tensorflow as tf\r\ntf.config.gpu.set_per_process_memory_fraction(0.75)\r\ntf.config.gpu.set_per_process_memory_growth(True)\r\n\r\n# your model creation, etc.\r\nmodel = MyModel(...)\r\n```\r\n\r\n@jaingaurav As a use case: I typically use this when _developing on my local machine_ and I want firefox to work well while training a network in the background. Otherwise firefox commonly crashes when you open a new tab because no gpu memory is left. Generally those types of settings are usefull to keep your _local machine_ usable while you have a network training in the background, while happily using your pc (with some stutters of course). However, this works perfectly fine with the current alpha version. :)", "@penguinmenac3: The new API still supports setting memory growth on a physical GPU. However, this would eventually use up all of the memory. If you'd like to limit the memory growth I'd suggest setting up a single virtual GPU with a memory limit. That way you can ensure that you have the necessary memory allocated upfront and it shouldn't intrude with your other desktop apps. Note that when setting a memory limit, the defined memory is allocated upfront.\r\n\r\nTry installing the 2.0 nightly and let me know if that works for you. It'd be great to know given that I removed the `tf.config.gpu.set_per_process_memory_fraction` API, and I'd like to ensure that your use case is covered.", "> I'd suggest setting up a single virtual GPU with a memory limit.\r\n\r\nDo you have a documentation reference?", "@matth79: Please see the `docstring tf.config.experimental.set_virtual_device_configuration` in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/config.py#L524. This should be part of the generated docs once we release 1.14 or the next 2.0 build.", "@jaingaurav \r\nHello, do you know how one can use the tf.config in a Keras session?\r\nPreviously, we did:\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.8\r\nconfig.gpu_options.allow_growth = True\r\nset_session(tf.Session(config=config))\r\n```\r\nBut now it's\r\n```\r\ntensorflow.config.gpu.set_per_process_memory_fraction(0.75)\r\ntensorflow.config.gpu.set_per_process_memory_growth(True)\r\n```\r\nBut how do we pass this session's configuration to Keras? `tf.Session()` is no longer available.\r\n\r\nThank you", "@FarzanT: Thanks for identifying this issue. I think you've uncovered an issue with the keras API and interaction with 2.0 when using custom session options. We are looking to get this addressed right away.", "@FarzanT: We just submitted the fix. Essentially all you should need to do is call the tf.config APIs and then keras should internally create a ConfigProto based on what you set.\r\n\r\nAlso, the `tf.keras.backend.set_session` call has now been removed from the v2 API, since it is not applicable in a v2 world. Note the `tf.keras.backend.set_session` symbol is still available in `tf.compat.v1` as is `tf.Session()`.", "@jaingaurav: Thank you very much. I see that `tf.config.gpu` was moved to `tf.config.experimental`, as I moved from the alpha version to the nightly releases. Unfortunately, I can't find the option to set the memory fraction used (`tensorflow.config.gpu.set_per_process_memory_growth`). Has this option been removed?\r\n\r\nThank you", "@FarzanT: That API was removed in favor of using virtual GPU devices with a fixed limit. See https://www.tensorflow.org/alpha/guide/using_gpu#limiting_gpu_memory_growth.", "Currently we can follow the official document to do that although the APIs are tedious now.\r\n\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\n\r\ntf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\r\n```", "@jaingaurav - regarding:\r\n> @FarzanT: We just submitted the fix. Essentially all you should need to do is call the tf.config APIs and then keras should internally create a ConfigProto based on what you set.\r\n> \r\n> Also, the `tf.keras.backend.set_session` call has now been removed from the v2 API, since it is not applicable in a v2 world. Note the `tf.keras.backend.set_session` symbol is still available in `tf.compat.v1` as is `tf.Session()`.\r\n\r\nAs of TF 2.0.0, it seems Keras doesn't pick up the `tf.config.experimental.set_memory_growth(gpu, True)` setting...\r\n", "> gpus = tf.config.experimental.list_physical_devices('GPU')\r\n> \r\n> tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\r\n\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-35-d1a1e7bd7d96> in <module>\r\n      1 gpus = tf.config.experimental.list_physical_devices('GPU')\r\n      2 \r\n----> 3 tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\config.py in set_virtual_device_configuration(device, virtual_devices)\r\n    554     virtual_devices: (optional) Need to update\r\n    555   \"\"\"\r\n--> 556   context.context().set_virtual_device_configuration(device, virtual_devices)\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\context.py in set_virtual_device_configuration(self, dev, virtual_devices)\r\n   1269     if self._context_handle is not None:\r\n   1270       raise RuntimeError(\r\n-> 1271           \"Virtual devices cannot be modified after being initialized\")\r\n   1272 \r\n   1273     self._virtual_device_map[dev] = virtual_devices\r\n\r\nRuntimeError: Virtual devices cannot be modified after being initialized\r\n```\r\n\r\nhave this error on your solution. can you explain me what is Virtual devices and why i can't modify them?\r\n\r\nhave same problem with memory growth and i don't know what to do with TF2", "germanjke@ what are you executing prior to this call? If you've executed any ops its too late for us to reconfigure the virtual device. The virtual device is simply a way of slicing up a physical device in separate devices with a fixed memory limit.", "@jaingaurav \r\nThe problem was that I had discovered too many JupyterNotebooks. When I closed them, launched one and rebooted kernel everything works. Thank you very much!", "`tf.config.experimental.set_memory_growth(gpu, True)` has no effect for me neither. Is there any known workaround? Alternatively, is there a way to get memory usage from the internal tensorflow allocator (I can't find any documentation on the topic)? ", "This solution works for me in TF 2.0.0  with RTX 2080 GPU. \r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.2\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n```", "> \r\n> \r\n> `tf.config.experimental.set_memory_growth(gpu, True)` has no effect for me neither. Is there any known workaround? Alternatively, is there a way to get memory usage from the internal tensorflow allocator (I can't find any documentation on the topic)?\r\n\r\nSame goes for me. Have you been able to figure something out?", "> This solution works for me in TF 2.0.0 with RTX 2080 GPU.\r\n> \r\n> ```\r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.per_process_gpu_memory_fraction = 0.2\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> ```\r\n\r\nabove works for me , under TF(gpu)2.0 + RTX2060 6GB + CUDA 10.0 + cuDNN 7.6.5\r\n----------------------------------------------------------------------------------------------------\r\ndoesn't work below\r\nhttps://www.tensorflow.org/alpha/guide/using_gpu#limiting_gpu_memory_growth\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\n\r\n# Currently, memory growth needs to be the same across GPUs\r\nfor gpu in gpus:\r\n        tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\ntf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\r\n```\r\n\r\nconsole log here:\r\n```\r\n2020-02-10 12:31:46.115996: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-02-10 12:31:46.983537: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-02-10 12:31:46.985830: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-02-10 12:31:46.987988: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node yolo_darknet/conv2d/Conv2D}}]]\r\n         [[yolo_nms/Reshape_9/_310]]\r\n2020-02-10 12:31:46.995276: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[{{node yolo_darknet/conv2d/Conv2D}}]]\r\nTraceback (most recent call last):\r\n  File \"detect_video.py\", line 118, in <module>\r\n    app.run(main)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"detect_video.py\", line 99, in main\r\n    boxes, scores, classes, nums = yolo.predict(img_in)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 909, in predict\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\", line 722, in predict\r\n    callbacks=callbacks)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\", line 393, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 3740, in __call__\r\n    outputs = self._graph_fn(*converted_inputs)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1081, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1121, in _call_impl\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node yolo_darknet/conv2d/Conv2D (defined at C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\r\n         [[yolo_nms/Reshape_9/_310]]\r\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node yolo_darknet/conv2d/Conv2D (defined at C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_keras_scratch_graph_17245]\r\n\r\nFunction call stack:\r\nkeras_scratch_graph -> keras_scratch_graph\r\n\r\n\r\n```", "@12343954 , what error are you seeing? \r\n\r\nThere is a typo in this line:\r\n```\r\ntf.config.experimental.set_memory_growth(gpu, True)\r\n```\r\n\r\nYou haven't `gpu` variable but `gpus`. `gpus` is a list so you need use the element, e.g. `gpus[0]`.", "> @12343954 , what error are you seeing?\r\n> \r\n> There is a typo in this line:\r\n> \r\n> ```\r\n> tf.config.experimental.set_memory_growth(gpu, True)\r\n> ```\r\n> \r\n> You haven't `gpu` variable but `gpus`. `gpus` is a list so you need use the element, e.g. `gpus[0]`.\r\n\r\n@VeLKerr Thank you for reminding this, i forgot to copy the full codes.", "Hi, Is there a reason why tf support gpu memory fraction, instead of memory size(ex. 4gb).\r\nI have build an API, which might be spawned on 8gb or 16gb gpu based on availability. I need to change the fraction accordingly.\r\nCurrently I need to obtain available memory from nvidia-smi, and generate appropriate ratio.", "The following worked for me: \r\n\r\nFirst use `nvidia-smi` to see if you have any extremely intensive programs running, use\r\n`sudo kill -9 PROCESS_ID` to kill it if you do\r\n\r\nBefore executing any tensorflow code, put this code:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)\r\n```\r\nAnd then execute whatever you want. \r\n\r\nTensorflow v2 has great doc regarding how to use GPU here: [https://www.tensorflow.org/guide/gpu](https://www.tensorflow.org/guide/gpu)\r\n", "Does anyone have a black screen in games after receiving `Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED` in tensorflow during training? I've found no solution to this other than rebooting, it fixes both the error (temporarily because it can happen again randomly) and the game issue (which would make it connected to the error itself)", "Hi @wuxiaohua1011  \r\n\r\nIn the solution you have mentioned , can limit the amount of memory getting allocated e.g. if I want to use only 70% of the gpu to run my code and keep rest 30% for operations(like running simulation) ?\r\n\r\nThank you", "Tried everything and nothing was working, turns out I was running this code I had found somewhere to check my GPU:\r\n\r\n`from tensorflow.python.client import device_lib`\r\n`device_lib.list_local_devices()`\r\n\r\nAfter removing this and using `set_memory_growth` as others have commented, tensorflow stopped using all my gpu memory. Had no idea that line would affect memory to the point of ignoring `set_memory_growth`, but I guess it did...", "> Tried everything and nothing was working, turns out I was running this code I had found somewhere to check my GPU:\r\n> \r\n> `from tensorflow.python.client import device_lib` `device_lib.list_local_devices()`\r\n> \r\n> After removing this and using `set_memory_growth` as others have commented, tensorflow stopped using all my gpu memory. Had no idea that line would affect memory to the point of ignoring `set_memory_growth`, but I guess it did...\r\n\r\nHi @pedrohpf \r\n\r\nSorry am new to Tensorflow. Where did you find the code `device_lib.list_local_devices()`? Also where should i put the `set_memory_growth` code? Is it in the model_main_tf2.py?\r\nThank you.", "Hello @Zabjaku\r\n\r\nAt that time I was trying to set tensorflow memory growth so it wouldn't automatically occupy all the memory. So I had used the following code for that, as others have mentioned something similar:\r\n\r\n```\r\ndevices = tf.config.list_physical_devices('GPU')\r\ntry:\r\n    tf.config.experimental.set_memory_growth(devices[0], True)\r\n    print(\"Success in setting memory growth\")\r\nexcept:\r\n    print(\"Failed to set memory growth, invalid device or cannot modify virtual devices once initialized.\")\r\n```\r\n\r\nHowever, that wasn't working and I was not sure the reason why.\r\nI then happened to remove a certain line of code that I had used before to check if everything was being setup correctly. That code was the following:\r\n\r\n```\r\nfrom tensorflow.python.client import device_lib\r\ndevice_lib.list_local_devices()\r\n```\r\n\r\nAfter removing this, the previous code of `set_memory_growth` started working correctly. I'm still not sure why that happens, but this is what I found out at the time.\r\n\r\nLet me be clear though, the use of `device_lib.list_local_devices()` is what was causing my initial memory growth not working problem. This might give some insight: https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow\r\n\r\nI'm not sure where you _should_ put the code in your example. I'm assuming model_main_tf2.py is the main script you use to run everything? I simply used the `set_memory_growth` code right after importing everything in my program. ", "Hi all,\r\n\r\nWould like to share how I 'fixed' the OOM issue after trying conuntless of install and uninstall and searching online. Seems like it is a must to install the cuda toolkit from nvidia (https://developer.nvidia.com/cuda-toolkit) (I was hesitant to do it as i am afraid it might affect my previous Tf1, which was working perfectly). After installing the toolkit, the training was able to run without having to add `set_memory_growth`. This installation also 'fix' the ptax unable to find issue. Also a cause of OOM is too large a `batch_size`, can try from 1 and work your way up. Below are my sys info:\r\n\r\nAnaconda: Python 3.8.12\r\nTensorflow, keras : 2.7.0\r\ncudatoolkit: 11.2.0\r\ncudnn: 8.1.0.77\r\nGraphics card: Nvidia 2070, 12GB\r\nWindows 10\r\n\r\nHope this helps:)"]}, {"number": 25137, "title": "Unable to build TF 1.12 with CUDA support", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows Server 2016 Datacenter, x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:-----\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12\r\n- Python version: Python 3.6.7\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): Build label: 0.19.2\r\n- GCC/Compiler version (if compiling from source): C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\r\n- CUDA/cuDNN version: Cuda v10, cudnn v7.4.2.24\r\n- GPU model and memory: Tesla K80 (0MiB / 11445MiB memory usage)\r\n\r\n\r\n\r\n**Describe the problem**\r\nI try to compile TF from source according to this guide: https://medium.com/@amsokol.com/update-2-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-61c26553f7e8\r\n\r\nThe bazel build fails with error: \r\n\r\n    ERROR: \r\n    C:/users/administrator/bin/tensorflow_bazel_src/tensorflow/tensorflow/core/kernels/BUILD:207:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:extract_image_patches_op_gpu':\r\n    this rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/extract_image_patches_op_gpu.cu.cc':\r\n     'C:/users/administrator/appdata/local/temp/2/nvcc_inter_files_tmp_dir/extract_image_patches_op_gpu.cu.cudafe1.stub.c'\r\n     'C:/users/administrator/appdata/local/temp/2/nvcc_inter_files_tmp_dir/extract_image_patches_op_gpu.cu.fatbin.c'\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nExactly as described here: https://medium.com/@amsokol.com/update-2-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-61c26553f7e8\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/2787302/log.txt)\r\n", "comments": ["TensorFlow 1.12 does not support cuda 10. However the good news is that, TensorFlow 1.13.0-rc0 is released and comes with GPU binaries built against CUDA 10.\r\nYou can give it a try. Thanks!", "I'm getting similar errors on r1.12 branch but it's with CUDA 9.0 / CuDNN v7.3:\r\n```\r\nSUBCOMMAND: # //tensorflow/core/kernels:crop_and_resize_op_gpu [action 'Compiling tensorflow/core/kernels/crop_and_resize_op_gpu.cu.cc']\r\ncd C:/builds/tc-workdir/.bazel_cache/output/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6 Tools\\x64\\;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python36/lib/site-packages\r\n    SET TEMP=C:\\Users\\TASK_1~1\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=3.0,3.5,3.7,5.2,6.0,6.1\r\n    SET TF_CUDA_VERSION=9.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\TASK_1~1\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Iexternal/png_archive /Ibazel-out/x64_windows-opt/genfiles/external/png_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /Iexternal/png_archive /Ibazel-out/x64_windows-opt/genfiles/external/png_archive /Ibazel-out/x64_windows-opt/bin/external/png_archive /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/crop_and_resize_op_gpu/crop_and_resize_op_gpu.cu.o /c tensorflow/core/kernels/crop_and_resize_op_gpu.cu.cc\r\nERROR: C:/builds/tc-workdir/deepspeech/tf/tensorflow/core/kernels/BUILD:2154:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:adjust_hue_op_gpu':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/adjust_hue_op_gpu.cu.cc':\r\n  'C:/users/task_1551794643/appdata/local/temp/nvcc_inter_files_tmp_dir/adjust_hue_op_gpu.cu.compute_61.cudafe1.stub.c'\r\n  'C:/users/task_1551794643/appdata/local/temp/nvcc_inter_files_tmp_dir/adjust_hue_op_gpu.cu.fatbin.c'\r\n```", "> I'm getting similar errors on r1.12 branch but it's with CUDA 9.0 / CuDNN v7.3:\r\n> \r\n> ```\r\n> SUBCOMMAND: # //tensorflow/core/kernels:crop_and_resize_op_gpu [action 'Compiling tensorflow/core/kernels/crop_and_resize_op_gpu.cu.cc']\r\n> cd C:/builds/tc-workdir/.bazel_cache/output/execroot/org_tensorflow\r\n>   SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n>     SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n>     SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;\r\n>     SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64;\r\n>     SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6 Tools\\x64\\;;C:\\Windows\\system32\r\n>     SET PWD=/proc/self/cwd\r\n>     SET PYTHON_BIN_PATH=C:/Python36/python.exe\r\n>     SET PYTHON_LIB_PATH=C:/Python36/lib/site-packages\r\n>     SET TEMP=C:\\Users\\TASK_1~1\\AppData\\Local\\Temp\r\n>     SET TF_CUDA_CLANG=0\r\n>     SET TF_CUDA_COMPUTE_CAPABILITIES=3.0,3.5,3.7,5.2,6.0,6.1\r\n>     SET TF_CUDA_VERSION=9.0\r\n>     SET TF_CUDNN_VERSION=7\r\n>     SET TF_NEED_CUDA=1\r\n>     SET TF_NEED_OPENCL_SYCL=0\r\n>     SET TF_NEED_ROCM=0\r\n>     SET TMP=C:\\Users\\TASK_1~1\\AppData\\Local\\Temp\r\n>   external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Iexternal/png_archive /Ibazel-out/x64_windows-opt/genfiles/external/png_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /Iexternal/png_archive /Ibazel-out/x64_windows-opt/genfiles/external/png_archive /Ibazel-out/x64_windows-opt/bin/external/png_archive /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/crop_and_resize_op_gpu/crop_and_resize_op_gpu.cu.o /c tensorflow/core/kernels/crop_and_resize_op_gpu.cu.cc\r\n> ERROR: C:/builds/tc-workdir/deepspeech/tf/tensorflow/core/kernels/BUILD:2154:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:adjust_hue_op_gpu':\r\n> this rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/adjust_hue_op_gpu.cu.cc':\r\n>   'C:/users/task_1551794643/appdata/local/temp/nvcc_inter_files_tmp_dir/adjust_hue_op_gpu.cu.compute_61.cudafe1.stub.c'\r\n>   'C:/users/task_1551794643/appdata/local/temp/nvcc_inter_files_tmp_dir/adjust_hue_op_gpu.cu.fatbin.c'\r\n> ```\r\n\r\nI'm 99.99% confindent that in my case, this was caused by values set by Bazel for `TEMP` and `TMP` which were following old DOS 8.3 convention. Setting them to a different value, and exposing them through `--action_env` my build does not error anymore because of that."]}, {"number": 25136, "title": "[Feature Request] [python API] make tf.RefVariable._strided_slice_assign public", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12 .0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently slice assignment is (publicly) accessible only indirectly through `strided_slice()` using the `var` kwarg, in a rather convoluted way creating a closure and adding it as an attribute to the generated op.\r\nAs a side effect this also adds an unnecessary (and mostly inaccessible) `strided_slice` op, bloating the graph needlessly.\r\n\r\n**Will this change the current api? How?**\r\nIt will add a public `tf.Variable.strided_slice_assign()` with exactly the same interface as the existing protected version.\r\nThe protected function is already de-facto part of the API when called using the supported slicing + assign() syntax, so there is no functional change. It will also not interfere with any existing code using that syntax.\r\n\r\n**Who will benefit with this feature?**\r\nPeople with use cases similar to mine:\r\nI have a rather elaborate slice assign operation than needs to be repeated on several variables, using the same slice. Naively one would just repeat \r\n`V[...elaborate slicing here...].assign(something)` for all variables V. This creates lots and lots of ops in the graph as the entire index expression gets replicated for each variable, plus the unnecessary and unused `strided_slice` read operation.\r\nInstead one could create the tensors for the indexing (begin, end, stride, masks) once, and then repeatedly call `.strided_slice_assign` for all the variables, resulting in a concise and manageable subgraph.\r\n\r\n I realize that the vast majority of people never call `strided_slice()` or `strided_slice_assign()` directly. However, for the same few for which `strided_slice()` is publicly available, so should `strided_slice_assign()` be.\r\n\r\n**Any Other info.**\r\nNone\r\n", "comments": ["The unnecessary ops in the graph, including the redundant read, can be easily pruned at execution time, so they shouldn't cost anything. Also calling strided_slice directly is _really hard_; the correspondence between the subjective python slicing arguments and what the op expects is nontrivial.\r\n\r\nFor these reasons I don't think we should expose this method."]}, {"number": 25135, "title": "Canonicalize symbolic link include paths", "body": "This change will canonicalize symbolic link include paths similar to the change for cuda include paths in https://github.com/tensorflow/tensorflow/issues/3985.\r\n\r\nIn my build environment the output of:\r\n`gcc -E -xc++ - -v`\r\n\r\nincludes (normalised):\r\n```\r\n/builds/gcc/4.9.4/12a8bec7dd/include/c++/4.9.4\r\n/builds/gcc/4.9.4/12a8bec7dd/include/c++/4.9.4/x86_64-unknown-linux-gnu\r\n/builds/gcc/4.9.4/12a8bec7dd/include/c++/4.9.4/backward\r\n/builds/gcc/4.9.4/12a8bec7dd/lib/gcc/x86_64-unknown-linux-gnu/4.9.4/include\r\n/builds/gcc/4.9.4/12a8bec7dd/lib/gcc/x86_64-unknown-linux-gnu/4.9.4/include-fixed\r\n```\r\n\r\nThe 12a8bec7dd directory is a symbolic link to another directory. This means that my build fails because of undeclared inclusions as bazel requires the canonical include paths when validating inclusions:\r\n```\r\nERROR: /user_data/.tmp/bazel/8138e9654dfcdfe7ea4dbdd7c7e74b8c/external/com_google_absl/absl/base/BUILD.bazel:58:1: undeclared inclusion(s) in rule '@com_google_absl//absl/base:dynamic_annotations':\r\nthis rule is missing dependency declarations for the following files included by 'external/com_google_absl/absl/base/dynamic_annotations.cc':\r\n  '/builds/gcc/4.9.4/53516a8e20/lib/gcc/x86_64-unknown-linux-gnu/4.9.4/include/stddef.h'\r\n```\r\n\r\n```\r\n$ readlink -f /builds/gcc/4.9.4/12a8bec7dd/lib/gcc/x86_64-unknown-linux-gnu/4.9.4/include\r\n/builds/gcc/4.9.4/53516a8e20/lib/gcc/x86_64-unknown-linux-gnu/4.9.4/include\r\n```\r\n\r\nThis change fixes my issue and may help others who have their C/C++ toolchain installed in a non-standard location with symbolic links.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Sorry to say but we cannot proceed until CLA reflects YES. Could you please open a new PR(which may resolve the CLA issue) and try. Closing this."]}, {"number": 25134, "title": "Fix init_ops_test on AVX512 builds", "body": "The init_ops_tests unit test is failing on AVX512 builds due\r\nto a hard coded packet size in eigen_contraction_kernel.h.\r\nThis commit fixes the issue by determining the appropriate\r\npacket size using internal::packet_traits.  It maintains\r\nthe minimum packet size of 8 used in the existing code and\r\nso should have no effect on non AVX512 builds.\r\n\r\nFixes: https://github.com/tensorflow/tensorflow/issues/25127\r\n\r\nSigned-off-by: Mark Ryan <mark.d.ryan@intel.com>", "comments": []}, {"number": 25133, "title": "DNNClassifier estimator train shows unsupported feed type error occasionally ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:  9.0\r\n- GPU model and memory: GeForce GTX 1070 with Max-Q Design\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI have a dataset contains labels with true and false, and a column of English test.\r\nLabels are converted to 1 and 0 as below:\r\n```\r\n      review                                          opinion\r\n0              1  cls late nite music club with shoji tabuchi wh...\r\n1              0  new south valley subdivision taking shape the ...\r\n2              0  celtics visit lakers aim to avoid fourth strai...\r\n3              1  russia signs ceasefire agreement with reservat...\r\n4              1  new j c penney ceo to have limited role report...\r\n\r\n```\r\nThe data are split into train_df and test_df then created an input_fn for training.\r\n\r\n```\r\n train_input_fn = tf.estimator.inputs.pandas_input_fn(\r\n    train_df, train_df['review'],batch_size=40, num_epochs=5, shuffle=True)\r\n```\r\nWhen I reach \r\n```estimator.train(input_fn=train_input_fn, steps=1000)```\r\n\r\nIt shows \r\n\r\n`tensorflow.python.framework.errors_impl.InternalError: Unsupported feed type`\r\n\r\nBut after I change the label to floats by df['review'] = df['review']*1.0 \r\n```\r\n      review                                          opinion\r\n0              1.0  cls late nite music club with shoji tabuchi wh...\r\n1              0.0  new south valley subdivision taking shape the ...\r\n2              0.0  celtics visit lakers aim to avoid fourth strai...\r\n3              1.0  russia signs ceasefire agreement with reservat...\r\n4              1.0  new j c penney ceo to have limited role report...\r\n\r\n```\r\nThe train function works again. \r\n\r\nBut this problem occurs occasionally, estimator.train works well if I have review labels in integer with 0 and 1 as the data in the first table.\r\n\r\n**Describe the expected behavior**\r\n```estimator.train(input_fn=train_input_fn, steps=1000)``` \r\nshould works without showing unsupported type\r\n", "comments": ["Can you please print your df['review'] data type before it raises error? Just want to check what the original data type looks like. If you can provide small reproducible test case, that will be great. Thanks!", "The data type is dtype('int64') while the type of df['review'] is pandas.core.series.Series . Thank you ! ", "Same Problem here for me. Ensured that data types in the target column are int64. Still getting an `tensorflow.python.framework.errors_impl.InternalError: Unsupported object type float\r\n`\r\nEDIT: Tried forcing target values to be float, still the same error.", "@raaaaay-c,\r\n\r\nHave you checked if you had any rows with missing values like `df[df['review'].isnan()]` since NaN is a special type of floating point value. \r\n\r\nAlso since this issue was with TF 1.x and the official support is ended for the same. Can you try updating the latest stable version `2.6.0` and let us know if you still face any challenges with the same. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25133\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25133\">No</a>\n"]}, {"number": 25132, "title": "Failed to apply delegate: WARNING: op code #43 cannot be handled by this delegate.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 - Android 5.1\r\n- Mobile device : Nexus  10 - Nexus 7 2012 Android 5.1\r\n- TensorFlow installed from: Binary\r\n- TensorFlow version : b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0 (0.0.0-gpu-experimental for mobile device)\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nThe tflite demo app crashes on Nexus 7 2012 and Nexus 10 when GPU is selected from device list. There was no such problem on the other devices I tested.\r\n\r\nLogs:\r\n```\r\n01-01 05:23:50.161 6539-6558/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n    Process: android.example.com.tflitecamerademo, PID: 6539\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: WARNING: op code #43 cannot be handled by this delegate.  Only the first 29 ops will run on the GPU, and the remaining 2 on the CPU.GpuDelegate Prepare: No EGL error, but eglChooseConfig failed.Node number 31 (GpuDelegate) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:83)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:60)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.recreateInterpreter(ImageClassifier.java:168)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.useGpu(ImageClassifier.java:176)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.lambda$updateActiveModel$0$Camera2BasicFragment(Camera2BasicFragment.java:379)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment$$Lambda$0.run(Unknown Source)\r\n        at android.os.Handler.handleCallback(Handler.java:739)\r\n        at android.os.Handler.dispatchMessage(Handler.java:95)\r\n        at android.os.Looper.loop(Looper.java:135)\r\n        at android.os.HandlerThread.run(HandlerThread.java:61)\r\n01-01 05:23:50.242 6539-6556/android.example.com.tflitecamerademo E/BufferQueueProducer: [unnamed-6539-2] queueBuffer: BufferQueue has been abandoned\r\n01-01 05:23:50.243 124-6580/? E/Surface: queueBuffer: error queuing buffer to SurfaceTexture, -19\r\n01-01 05:23:50.243 124-6580/? E/NvOmxCamera: Queue Buffer Failed. Skipping buffer.\r\n01-01 05:23:50.243 6539-6619/android.example.com.tflitecamerademo E/BufferQueueProducer: [unnamed-6539-2] dequeueBuffer: BufferQueue has been abandoned\r\n01-01 05:23:50.244 124-6580/? E/NvOmxCamera: Dequeue Buffer Failed\r\n01-01 05:23:50.275 6539-6555/android.example.com.tflitecamerademo E/BufferQueueProducer: [unnamed-6539-2] queueBuffer: BufferQueue has been abandoned\r\n01-01 05:23:50.277 124-6580/? E/Surface: queueBuffer: error queuing buffer to SurfaceTexture, -19\r\n01-01 05:23:50.277 124-6580/? E/NvOmxCamera: Queue Buffer Failed. Skipping buffer.\r\n01-01 05:23:50.277 6539-6556/android.example.com.tflitecamerademo E/BufferQueueProducer: [unnamed-6539-2] dequeueBuffer: BufferQueue has been abandoned\r\n01-01 05:23:50.277 124-6580/? E/NvOmxCamera: Dequeue Buffer Failed\r\n01-01 05:23:50.278 6539-6619/android.example.com.tflitecamerademo E/BufferQueueProducer: [unnamed-6539-2] cancelBuffer: BufferQueue has been abandoned\r\n01-01 05:23:50.279 6539-6555/android.example.com.tflitecamerademo E/BufferQueueProducer: [unnamed-6539-2] cancelBuffer: BufferQueue has been abandoned\r\n01-01 05:23:50.380 124-565/? E/NvOmxCamera: Already called release()\r\n01-01 05:25:48.050 473-554/? E/WifiStateMachine: cancelDelayedScan -> 6\r\n```", "comments": ["I belive the problem is from OpenGL ES unsupported version. I just noticed that the Nexus 10 OpenGL ES version is 3.0 and TensorFlow Lite delegate requires OpenGL ES 3.1 or higher.\r\nhttps://www.notebookcheck.net/ARM-Mali-T604-MP4.116274.0.html\r\nIt will be great if the support for older versions is added in the future.\r\n"]}, {"number": 25131, "title": "TFLite GPU Delegate has problem with MobileNetV2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution : Linux Ubuntu 16.04\r\n- Mobile device : Samsung Galaxy S9 Android 8.0 - Nexus 10 Android 5.1\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0 (0.0.0-gpu-experimental for mobile device)\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nGPU delegate has problem with MobileNetV2. When I select GPU from device list at tflite demo project (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo) the app crashes.\r\nThe only things I've changed on this project was changing MobileNet V1 float model to MobileNet v2. The MobileNetV2 model is taken from \"https://tfhub.dev/google/imagenet/mobilenet_v2_050_160/classification/2\" , retrained by \"https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py\" script and converted to tflite format using the following command:\r\n\r\n```\r\ntflite_convert \\\r\n  --output_file=graph.tflite \\\r\n  --graph_def_file=retrained_graph.pb \\\r\n  --input_arrays=Placeholder \\\r\n  --output_arrays=final_result\r\n --input_shapes=1,160,160,3\r\n```\r\nAll the necessary changes (such as changing the graph name, input size, etc.) in the ImageClassifierFloatMobileNet class is made.\r\n\r\nLogs:\r\n```\r\n2019-01-23 13:51:12.091 22222-22294/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n    Process: android.example.com.tflitecamerademo, PID: 22222\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: GpuDelegate Prepare: Dimension is empty.Node number 68 (GpuDelegate) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:83)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:60)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.recreateInterpreter(ImageClassifier.java:168)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.useGpu(ImageClassifier.java:176)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.lambda$updateActiveModel$0$Camera2BasicFragment(Camera2BasicFragment.java:379)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment$$Lambda$0.run(Unknown Source:8)\r\n        at android.os.Handler.handleCallback(Handler.java:789)\r\n        at android.os.Handler.dispatchMessage(Handler.java:98)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n2019-01-23 13:51:12.102 4411-8924/? E/CameraDeviceClient: Disconnect from CameraDeviceClient\r\n```\r\n", "comments": ["Similar problem also exist on some of the MobileNetV1 models (https://tfhub.dev/google/imagenet/mobilenet_v1_050_160/classification/1)\r\n```\r\n2019-01-23 19:59:54.482 5685-5775/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n    Process: android.example.com.tflitecamerademo, PID: 5685\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: WARNING: op code #40 cannot be handled by this delegate.  Only the first 29 ops will run on the GPU, and the remaining 4 on the CPU.GpuDelegate Prepare: Dimension is empty.Node number 33 (GpuDelegate) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:83)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:60)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.recreateInterpreter(ImageClassifier.java:168)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.useGpu(ImageClassifier.java:176)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.lambda$updateActiveModel$0$Camera2BasicFragment(Camera2BasicFragment.java:379)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment$$Lambda$0.run(Unknown Source:8)\r\n        at android.os.Handler.handleCallback(Handler.java:789)\r\n        at android.os.Handler.dispatchMessage(Handler.java:98)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n2019-01-23 19:59:54.506 4411-5699/? E/CameraDeviceClient: Disconnect from CameraDeviceClient\r\n2019-01-23 19:59:54.550 4761-4797/? E/PhoneWindow_APM\u00a0:: sendNotificationForAppPermissionMonitor callingPid = 4761, uid = 1000\r\n2019-01-23 19:59:54.550 4761-4797/? E/PhoneWindow_APM\u00a0:: sendNotificationForAppPermissionMonitor 2\r\n2019-01-23 19:59:54.550 4761-4797/? E/PhoneWindow_APM\u00a0:: sendNotificationForAppPermissionMonitor 21000_4761\r\n2019-01-23 19:59:54.550 4761-4797/? E/PhoneWindow_APM\u00a0:: isCalledPackage return false\r\n2019-01-23 19:59:54.550 4761-4797/? E/PhoneWindow_APM\u00a0:: sendNotificationForAppPermissionMonitor mIsMonitoredFeature = true\r\n2019-01-23 19:59:54.852 4261-5961/? E/ExynosCameraRequestManager: [CAM_ID(1)][]-ERR(m_resultCallbackThreadFunc[2417]):ResultCallback fail, ret(-110)\r\n2019-01-23 19:59:54.870 4261-4261/? E/libcsc: csc_deinit:: unsupported csc_hw_type\r\n2019-01-23 19:59:54.872 4261-4261/? E/libcsc: csc_deinit:: unsupported csc_hw_type\r\n2019-01-23 19:59:54.872 4261-4261/? E/libcsc: csc_deinit:: unsupported csc_hw_type\r\n2019-01-23 19:59:54.872 4261-4261/? E/libcsc: csc_deinit:: unsupported csc_hw_type\r\n2019-01-23 19:59:54.872 4261-4261/? E/libcsc: csc_deinit:: unsupported csc_hw_type\r\n2019-01-23 19:59:54.890 4261-4261/? E/libcsc: csc_deinit:: unsupported csc_hw_type\r\n2019-01-23 19:59:54.890 4261-4261/? E/libcsc: csc_deinit:: unsupported csc_hw_type\r\n2019-01-23 19:59:54.890 4261-4261/? E/libcsc: csc_deinit:: unsupported csc_hw_type\r\n2019-01-23 19:59:54.890 4261-4261/? E/libcsc: csc_deinit:: unsupported csc_hw_type\r\n2019-01-23 19:59:55.258 6499-6676/? E/PBSessionCacheImpl: sessionId[28584452507712951] not persisted.\r\n2019-01-23 19:59:55.440 6365-7849/? E/Launcher.WallpaperUtils: check bitmap.recycle() on releaseWallpaperDrawable\r\n2019-01-23 19:59:57.038 4761-5340/? E/Watchdog: !@Sync 667 [2019-01-23 19:59:57.038]\r\n2019-01-23 19:59:57.326 6189-6473/? E/ContactsProvider_EventLog: Flush buffer to file cnt : 1 size : 1Kb duration : 7ms lastUpdatedAfter : 38673 ms mFlush_time_threasold : 2000 mCurrentSize : 683\r\n2019-01-23 20:00:15.924 7874-7874/? E/Zygote: isWhitelistProcess - Process is Whitelisted\r\n2019-01-23 20:00:15.926 7874-7874/? E/libpersona: scanKnoxPersonas\r\n2019-01-23 20:00:15.926 7874-7874/? E/libpersona: Couldn't open the File - /data/system/users/0/personalist.xml - No such file or directory\r\n\r\n```", "@ramtin2080 \r\n\r\nThanks for trying out the GPU delegate, and sorry for the late response.  Was swamped with other things last week.  Can you share the two TFLite models?  Doesn't have to have the proper weights; you can share an uninitialized graph.  I want to take a look at the architecture and find out what's wrong.", "@impjdi\r\n\r\nThank you for the response.  The models are attached.\r\n[tf_files.zip](https://github.com/tensorflow/tensorflow/files/2814332/tf_files.zip)\r\n", "@ramtin2080 \r\n\r\nPerfect.  I'll check these and get back to you by EOD", "@ramtin2080 \r\n\r\nIn case of the v2 model, module_apply_default/hub_input/Mul/y and module_apply_default/hub_input/Sub/y have an empty dimension, why the GPU delegate is failing.\r\n\r\n@aselle \r\n\r\nIs empty dimension a right thing?  Does the user have to change the toco command and specify it to  something, or is implicit dimensions allowed?  If such implicit dimension is allowed, how is the tensor interpreted?  Scalar?  1x1xC vector, where C is the channel size of the other tensor?  same size as the other tensor?", "@impjdi, implicit size is not really supported by toco. However, tflite interpreter will not have right shape until a user calls AllocateTensors () (and it will use the input tensors shapes to propagate the shapes downstream), at least for a model with all non-dynamic shapes.", "@aselle Thanks! \ud83d\udc4d \r\n\r\n@ramtin2080 \r\n\r\nThe GPU delegate currently does not support dynamic tensor sizes.  Is it possible for you to explicitly specify the tensor dimensions of module_apply_default/hub_input/Mul/y and module_apply_default/hub_input/Sub/y in your TF graph, so that all tensor dimensions are known in advance?", "@impjdi\r\n\r\nUnfortunately I don't know how to do this. Is there any tutorial for it?\r\nI'm kinda new to TensorFlow.", "@ramtin2080 \r\n\r\nUnfortunately, not proficient with TF either.  I had some experience with the retrain.py, but I see that something new called module_spec is in the game... =/  If you can run toco like:\r\n\r\n> tflite_convert \\\r\n>   --output_file=graph.tflite \\\r\n>   --graph_def_file=retrained_graph.pb \\\r\n>   --input_arrays=module_apply_default/hub_input/Sub \\\r\n>   --output_arrays=final_result\r\n>  --input_shapes=1,160,160,3\r\n\r\nand essentially chop off the very first two ops which cause the failure but instead do the normalization of the image yourself, the GPU backend might work.  I would run it on CPU first to make sure the chopped off network works fine though.", "@impjdi\r\n\r\nThanks, it worked! but it seems without doing the normalization the accuracy has dropped a little bit.\r\nIs there an efficent way to do the normalization of the image on Android myself? (using tflite or any other libraries and codes)", "Assuming that the normalization is in the range of 0.0-1.0 as \r\n\r\n> For this module, the size of the input image is fixed to height x width = 160 x 160 pixels. The input images are expected to have color values in the range [0,1], following the common image input conventions.\r\n\r\nWe do the normalization in the demo app like here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/9a4c3a2eaaa1ca9a6a7d5bdb97c59814afa3c98b/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifierFloatMobileNet.java#L70-L72\r\n\r\nI think you can essentially just divide the values in the tensor by 255.", "@impjdi \r\n\r\nThe code can't increase the accuracy like the model normalization ops :(\r\nIs it possible for you guys to add dynamic tensor sizes support for the GPU delegate in the future? or any way so I can simulate the very two first ops in java.\r\nIt seems the ops use [this](https://arxiv.org/abs/1502.03167) article for normalizing images.", "@ramtin2080 \r\n\r\n> The code can't increase the accuracy like the model normalization ops :(\r\n\r\nHm, that's weird.  Maybe we got the parameters wrong, because it should have the same output.\r\n\r\n> Is it possible for you guys to add dynamic tensor sizes support for the GPU delegate in the future?\r\n\r\nYes, we will eventually add it, but it comes with bigger API plumbing, and won't be available even if we open source this in near future.  Dynamic tensors may be supported at best in the 2nd half of 2019, and even that would depend on demand... =/\r\n\r\nBatch normalization is something different.  With \"normalization\" I mentioned, is just dividing the input tensor by 255 for every element, but I see that's not easily doable with the current Java APIs.  Let me find other ways.\r\n", "@ramtin2080 \r\n\r\nI just noticed:\r\n\r\n> GPU delegate has problem with MobileNetV2. When I select GPU from device list at tflite demo project (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo) the app crashes.\r\n\r\nLet me try to take a look at these constants.  Maybe we can apply that in the Java layer directly.  Stay tuned.", "@ramtin2080 \r\n\r\nAite.  I looked into the constants, and it looks like it's doing 2.0 * X - 1.0 which is remapping the values from 0.0 - 1.0 to -1.0 - 1.0.  So let's do that.\r\n\r\n> imgData.putFloat(((pixelValue >> 16) & 0xFF) / 255.f * 2.f - 1.f); \r\n> imgData.putFloat(((pixelValue >> 8) & 0xFF) / 255.f * 2.f - 1.f); \r\n> imgData.putFloat((pixelValue & 0xFF) / 255.f * 2.f - 1.f); \r\n\r\nAgain, that's the code in `ImageClassifierFloatMobileNet.java`.", "@impjdi \r\n\r\nThank you so much! Now the accuracy is same as the original MobileNetV2 model =D\r\nSince the problem has been resolved, I'm closing this issue for now."]}, {"number": 25130, "title": "Lite: Floor-Div different datatypes supported", "body": "TFLite FloorDiv currently supports only Int32.\r\nThis PR add support for other datatypes as well.\r\nTest cases also updated.", "comments": ["Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied.", "@jdduke : Your comments are addressed now, Thanks!", "@rthadur : Gentle Reminder!"]}, {"number": 25129, "title": "TF Eager profiler_test added test to validate error case", "body": "Error case validation included in test case.", "comments": []}, {"number": 25128, "title": "Added erf support for keras backend", "body": "Computes the Gauss error function of x element-wise", "comments": ["@fchollet would be great if you can spend some time on the review, would really appriciate your comments on this, will help me to think more and contribute more towards the community.", "Thank you for the PR.\r\n\r\nThe Keras backend is meant to provide a shared interface to a core set of tensor operations across multiple platforms (in particular, TensorFlow, Theano, CNTK, and MXNet). Its API is not specific to tf.keras.\r\n\r\nThe proposed utility might make sense to have in the Keras backend API. However, in order to include it, we would need:\r\n- Demonstration that there is a need / a use case\r\n- Complete unit tests for the symbol that was added\r\n- Replication of the functionality in `keras-team/keras`, in particular in the Theano and CNTK backend implementations, and including unit tests.\r\n\r\nFor adding simple symbols to the Keras backend API I suggest instead making a PR in `keras-team/keras`.\r\n\r\nClosing the PR since it cannot be merged in this form. If you are interested please open a PR in `keras-team/keras`."]}, {"number": 25127, "title": "//tensorflow/python/kernel_tests:init_ops_test fails on AVX512 builds", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): ('v1.8.0-14337-g18b1875', '1.13.0-rc0')\r\n- Python version: Python 2.7.12\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n$  bazel test --config=opt  -- //tensorflow/python/kernel_tests:init_ops_test\r\n\r\n//tensorflow/python/kernel_tests:init_ops_test                           FAILED in 1 out of 4 in 22.9s\r\n\r\n**Describe the expected behavior**\r\n\r\nThe unit test should pass on AVX512 builds as it does on AVX2 builds.\r\n\r\n**Code to reproduce the issue**\r\n\r\n$ bazel test --config=opt  -- //tensorflow/python/kernel_tests:init_ops_test\r\n\r\n**Other info / logs**\r\n```\r\n======================================================================\r\nFAIL: testShapesValues (__main__.ConvolutionOrthogonal3dInitializerTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1103, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/init_ops_test.py\", line 1142, in testShapesValues\r\n    self.assertAllClose(self.evaluate(ratio), gain, rtol=tol, atol=tol)\r\n  File \"/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1013, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2121, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2090, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg)))\r\n  File \"/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2025, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\n  File \"/home/user/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 1452, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/home/user/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 789, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.001, atol=0.001\r\nMismatched value: a is different from b. \r\nnot close lhs = 3.15808677673\r\nnot close rhs = 3.14\r\nnot close dif = 0.0180867767334\r\nnot close tol = 0.00414\r\ndtype = float32, shape = ()\r\n(mismatch 100.0%)\r\n x: array(3.158087, dtype=float32)\r\n y: array(3.14)\r\n```\r\n", "comments": []}, {"number": 25126, "title": "Added Zeta support for Keras backend", "body": "Zeta operator support added for keras backend", "comments": ["@fchollet would be great if you can spend some time on the review, would really appriciate your comments on this, will help me to think more and contribute more towards the community", "Thank you for the PR.\r\n\r\nThe Keras backend is meant to provide a shared interface to a core set of tensor operations across multiple platforms (in particular, TensorFlow, Theano, CNTK, and MXNet). Its API is not specific to tf.keras.\r\n\r\nThe proposed utility might make sense to have in the Keras backend API. However, in order to include it, we would need:\r\n- Demonstration that there is a need / a use case\r\n- Complete unit tests for the symbol that was added\r\n- Replication of the functionality in `keras-team/keras`, in particular in the Theano and CNTK backend implementations, and including unit tests.\r\n\r\nFor adding simple symbols to the Keras backend API I suggest instead making a PR in `keras-team/keras`.\r\n\r\nClosing the PR since it cannot be merged in this form. If you are interested please open a PR in `keras-team/keras`."]}, {"number": 25125, "title": "Cherry pick openssl fix and R fix and update release notes.", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->"]}, {"number": 25124, "title": "Added reciprocal operator in backend", "body": "Added reciprocal operator in back end to support\r\nrint rsqrt etc operations with ease", "comments": ["@fchollet would be great if you can spend some time on the review, would really appriciate your comments on this, will help me to think more and contribute more towards the community", "Thank you for the PR.\r\n\r\nThe Keras backend is meant to provide a shared interface to a core set of tensor operations across multiple platforms (in particular, TensorFlow, Theano, CNTK, and MXNet). Its API is not specific to tf.keras.\r\n\r\nThe proposed utility might make sense to have in the Keras backend API. However, in order to include it, we would need:\r\n- Demonstration that there is a need / a use case\r\n- Complete unit tests for the symbol that was added\r\n- Replication of the functionality in `keras-team/keras`, in particular in the Theano and CNTK backend implementations, and including unit tests.\r\n\r\nFor adding simple symbols to the Keras backend API I suggest instead making a PR in `keras-team/keras`.\r\n\r\nClosing the PR since it cannot be merged in this form. If you are interested please open a PR in `keras-team/keras`."]}, {"number": 25123, "title": "ScheduleWithHint\u201d: is not \u201ctensorflow::thread::ThreadPool::Impl \u2018s member ", "body": "ScheduleWithHint\u201d: is not \u201ctensorflow::thread::ThreadPool::Impl \u2018s member\r\n\r\nD:\\tensorflow-1.13.0\\tensorflow\\core\\lib\\core\\threadpool.cc(100): error C2661: \u201cEigen::ThreadPoolDevice::ThreadPoolDevice\u201d: \u6ca1\u6709\u91cd\u8f7d\u51fd\u6570\u63a5\u53d7 3 \u4e2a\u53c2\u6570\r\nD:\\tensorflow-1.13.0\\tensorflow\\core\\lib\\core\\threadpool.cc(208): error C2039: \u201cScheduleWithHint\u201d: \u4e0d\u662f\u201ctensorflow::thread::ThreadPool::Impl\u201d\u7684\u6210\u5458\r\n  D:\\tensorflow-1.13.0\\tensorflow\\core\\lib\\core\\threadpool.cc(88): note: \u53c2\u89c1\u201ctensorflow::thread::ThreadPool::Impl\u201d\u7684\u58f0\u660e\r\nD:\\tensorflow-1.13.0\\tensorflow\\core\\lib\\core\\threadpool.cc(213): error C2039: \u201cSetStealPartitions\u201d: \u4e0d\u662f\u201ctensorflow::thread::ThreadPool::Impl\u201d\u7684\u6210\u5458\r\n  D:\\tensorflow-1.13.0\\tensorflow\\core\\lib\\core\\threadpool.cc(88): note: \u53c2\u89c1\u201ctensorflow::thread::ThreadPool::Impl\u201d\u7684\u58f0\u660e", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 25122, "title": "ScheduleWithHint", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 25121, "title": "[tflite] fix the location of build_ios_universal_lib.sh in TFLite bechmark doc", "body": "fix the location of build_ios_universal_lib.sh in TFLite benchmark for iOS README.md", "comments": []}, {"number": 25120, "title": "TF lite has link error when I try to build with build_rpi_lib.sh", "body": "When I try to build for Tensorflow lite(in latest version) occur link error like this.\r\n\r\n/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::NNAPIAllocation::~NNAPIAllocation()':\r\nnnapi_delegate.cc:(.text+0x2c): undefined reference to `NnApiImplementation()'\r\n/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::NNAPIAllocation::NNAPIAllocation(char const*, tflite::ErrorReporter*)':\r\nnnapi_delegate.cc:(.text+0x110): undefined reference to `NnApiImplementation()'\r\n/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::NNAPIDelegate::~NNAPIDelegate()':\r\nnnapi_delegate.cc:(.text+0x190): undefined reference to `NnApiImplementation()'\r\nnnapi_delegate.cc:(.text+0x1b4): undefined reference to `NnApiImplementation()'\r\n/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(nnapi_delegate.o): In function `tflite::addTensorOperands(tflite::Subgraph*, ANeuralNetworksModel*, unsigned int*, std::vector<long long, std::allocator<long long> >*)':\r\nnnapi_delegate.cc:(.text+0x214): undefined reference to `NnApiImplementation()'\r\n/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(nnapi_delegate.o):nnapi_delegate.cc:(.text+0x504): more undefined references to `NnApiImplementation()' follow\r\ncollect2: error: ld returned 1 exit status\r\n\r\nhow can I fix it? ", "comments": ["I also meet this error,  but it seemed to not  appear in earlier version. I just disable NNApi for the moment. I want to know how to fix it too.", "I could build this with `tensorflow/lite/nnapi/nnapi_implementation.cc`. But nnapi_implelementation.cc use shm_open. shm_open require build with shared libarry linked to  `-lrt`.", "But current implementation does not have delegate for GPU on raspberry pi. So won't get performance well.", "> I also meet this error, but it seemed to not appear in earlier version. I just disable NNApi for the moment. I want to know how to fix it too.\r\n\r\n I also meet this error, and I want to know how to disable NNApi?", "> > I also meet this error, but it seemed to not appear in earlier version. I just disable NNApi for the moment. I want to know how to fix it too.\r\n> \r\n> I also meet this error, and I want to know how to disable NNApi?\r\n\r\nin the Makefile (~/tensorflow/lite/tools/make/Makefile) for tflite, you should see this:\r\nBUILD_WITH_NNAPI=true\r\n\r\nchange it to false and then re-run build_rpi_lib.sh\r\n\r\n", "I succeeded in building and testing.\r\nAnd, Although it is still informal, I tried to enable multithreading.\r\n\r\n**PythonAPI Multi Thread - During review, Proposal for @freedomtan**\r\n**[tensorflow/tensorflow#25748](https://github.com/tensorflow/tensorflow/pull/25748)**\r\n**Avoiding build errors, Proposal for @jdinkel88**\r\n**[tensorflow/tensorflow#25120 (comment)](https://github.com/tensorflow/tensorflow/issues/25120#issuecomment-464296755)**\r\n\r\n**Build script**\r\n**https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/rpi.md**\r\n\r\n**Output destination path**\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/bin/benchmark_model\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/bin/minimal\r\n\r\n**Exec test (1) - View summary**\r\n```bash\r\n$ cd /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/bin\r\n$ ./minimal xxxx.tflite\r\n$ ./benchmark_model --graph=xxxx.tflite\r\n```\r\n\r\n**Exec test (2) - Model load test by Python3**\r\n```bash\r\n$ python3\r\nPython 3.5.3 (default, Sep 27 2018, 17:25:39) \r\n[GCC 6.3.0 20170516] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> import tensorflow.contrib.lite as lite\r\n>>> inter = lite.Interpreter(model_path = \"xxxx.tflite\")\r\n>>> inter.allocate_tensors()\r\n>>> input_details = inter.get_input_details()\r\n>>> print(input_details)\r\n[{'name': 'input', 'quantization': (0.0, 0), 'shape': array([  1, 128, 128,   3]), 'dtype': <class 'numpy.float32'>, 'index': 110}]\r\n```\r\n\r\n**Note that you must specify \"contrib\" at import time.**\r\n`import tensorflow.contrib.lite`", "I followed what @PINTO0309 did (or I think so)\r\n1) `git clone https://github.com/tensorflow/tensorflow.git`\r\n2) Edit of `./tensorflow/lite/tools/make/Makefile` file, \r\n`BUILD_WITH_NNAPI=true`   -->    `BUILD_WITH_NNAPI=false`\r\n3) `./tensorflow/lite/tools/make/download_dependencies.sh`\r\n4) `./tensorflow/lite/tools/make/build_rpi_lib.sh`\r\n\r\nIt all seems to end succesfully, but when I run:\r\n```\r\ncd /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/bin\r\n./minimal xxxx.tflite\r\n```\r\nI get error:\r\n`bash: ./minimal: cannot execute binary file: Exec format error`\r\n\r\nEDIT: Silly explanation. I run the script immedietaly after cross-compiling. Not on the target machine. After moving to target machine there is no such error (got some other, wrong glibc version, but I guess I'll be able to resolve it on my own)\r\n\r\nEDIT2:  My colleague finished this whole compilation process. \r\n1) He did cross-compile and got tflite library (`libtensorflow-lite.a`), disabling NNAPI as described above.\r\n2) Then, he compiled `minimal` example directly on Raspberry PI, using cross-compiled library. \r\nIf anyone is interested I can ask him if he can provide more information about this step and post it here.", "@aselle, do you have any ideas on this one?", "I encountered the same issue with the 64 bit variant of the build script  that I contributed earlier. IMO NNAPI should be disabled in these scripts by default. This can be easily done via a `make` parameter. With it disabled, everything builds fine, and benchmark_model works as expected. If this seems like a reasonable thing to do, I can submit a PR. \r\n\r\nIt doesn't seem like #26336 would address the 64-bit build use case. To address it, I think the principled solution would be to define something like `generic-aarc64` target type and handle it similarly in the makefile. I'll suggest it in the issue separately.", "I also experienced this error; it appears that rolling back to an earlier version of Tensorflow fixed this:\r\n` git checkout r1.95`", "I made Tensorflow v2.0.0-alpha + Tensorflow Lite v1.0 (MultiThread enabled) wheel installer. \r\nFor RaspberryPi3.\r\nSince it is an alpha version, there may be many bugs.\r\n**https://github.com/PINTO0309/Tensorflow-bin.git**\r\n**tensorflow-2.0.0a0-cp35-cp35m-linux_armv7l.whl**", "About \r\nhttps://github.com/tensorflow/tensorflow/issues/25120#issue-402094367\r\n\r\nI fix it as follow\r\n\r\nin the file: ./tensorflow/lite/tools/make/targets/linux_makefile.inc\r\n\r\nappend LIBS += -lrt\r\n", "This is a stale issue. \r\nBased on above comments, it looks like this was resolved. I will close this issue. Please feel free to reopen if this is still an issue with recent TF lite versions. Thanks!"]}, {"number": 25119, "title": "Can't import tensorflow in Anaconda", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n\r\n- TensorFlow version: \r\ntensorboard                        1.12.2\r\ntensorflow                         1.12.0\r\ntensorflow-gpu                     1.12.0\r\n\r\n- Python version: Python 3.6.8 | Anaconda Python 3.5.2\r\n- Installed using virtualenv? pip? conda?: pip & conda\r\n\r\n- CUDA/cuDNN version: cuDNN v7.4.2 for CUDA 9.0\r\n- GPU model and memory: NVIDIA GeForce 920mx | 8gb\r\n\r\n### Anaconda Prompt\r\n**import tensorflow as tf**\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\New User\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\New User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\New User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\New User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 102, in <module>\r\n    _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_EXPLICIT_swigconstant(_pywrap_tensorflow_internal)\r\nAttributeError: module '_pywrap_tensorflow_internal' has no attribute 'TFE_DEVICE_PLACEMENT_EXPLICIT_swigconstant'", "comments": []}, {"number": 25118, "title": "Any project about the evaluation of music or piano", "body": "Dear,\r\nIs there any project about evaluating the music or piano music,\r\nWhen we get many datasets of the same music from many people,\r\nHow could we evaluate one of them,that is ,the label is the grad of the music is A ,B,C or D, \r\nor just score between 0~100, Could we do something about this?\r\nAny advice or suggestion will be good.\r\nIf U have any question,Please do not hesitate to tell me.\r\nThx", "comments": ["Probably you can take a look at [TensorFlow Magenta](https://magenta.tensorflow.org/). If you have specific questions related to Magenta project you can post it on [Magenta Github repo](https://github.com/tensorflow/magenta/issues). Thanks!"]}, {"number": 25117, "title": "Can't import tensorflow-gpu or tensorflow (CPU only) 1.12.0", "body": "Ubuntu: 16.04\r\nTensorflow installed from Anaconda\r\nTensorflow version: 1.12.0\r\nPython: Python2.7\r\nCUDA: 9.0\r\nCudnn: 7.2.1\r\nGPU: Nvidia Quadro P5000 16GB\r\nCommand: import tensorflow\r\n\r\n[ysaputra@vulcan2 ~]$ python\r\nPython 2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nAnaconda is brought to you by Continuum Analytics.\r\nPlease check out: http://continuum.io/thanks and https://anaconda.org\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ysaputra/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/ysaputra/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 47, in <module>\r\n    import numpy as np\r\n  File \"/home/ysaputra/anaconda2/lib/python2.7/site-packages/numpy/__init__.py\", line 142, in <module>\r\n    from . import core\r\n  File \"/home/ysaputra/anaconda2/lib/python2.7/site-packages/numpy/core/__init__.py\", line 57, in <module>\r\n    from . import numerictypes as nt\r\n  File \"/home/ysaputra/anaconda2/lib/python2.7/site-packages/numpy/core/numerictypes.py\", line 111, in <module>\r\n    from ._type_aliases import (\r\n  File \"/home/ysaputra/anaconda2/lib/python2.7/site-packages/numpy/core/_type_aliases.py\", line 63, in <module>\r\n    _concrete_types = {v.type for k, v in _concrete_typeinfo.items()}\r\n  File \"/home/ysaputra/anaconda2/lib/python2.7/site-packages/numpy/core/_type_aliases.py\", line 63, in <setcomp>\r\n    _concrete_types = {v.type for k, v in _concrete_typeinfo.items()}\r\nAttributeError: 'tuple' object has no attribute 'type'\r\n>>> \r\n\r\nI installed new tensorflow-gpu 1.12.0 or tensorflow (CPU only) 1.12.0. However, I have the same problems using import tensorflow after successful installations. It seems the tensorflow is not imported properly. Do you know how to solve it?", "comments": ["you must add adjust m.a.x. count red button ", "Can you please describe the steps you followed to install TF?\r\nInstalling TF CPU version should be fairly easy using pip.", "> you must add adjust m.a.x. count red button\r\n\r\nI don't get what do you mean\r\n\r\n> Can you please describe the steps you followed to install TF?\r\n> Installing TF CPU version should be fairly easy using pip.\r\n\r\nI just simply installed as follows:\r\n\r\npip install --ignore-installed https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.12.0-cp27-none-linux_x86_64.whl for GPU or\r\n\r\npip install --ignore-installed https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.12.0-cp27-none-linux_x86_64.whl\r\n", "Can you try the setup in a virtual environment as given below:\r\n>virtualenv -p python venv-tf\r\nsource venv-tf/bin/activate\r\npip install tensorflow", "> Can you try the setup in a virtual environment as given below:\r\n> \r\n> > virtualenv -p python venv-tf\r\n> > source venv-tf/bin/activate\r\n> > pip install tensorflow\r\n\r\nActually, I installed inside Anaconda environment and the installation was success without any error. However when I start to use import tensorflow as tf, that error happens.", "@gunan Can you please take a look? Thanks!", "This looks to be an error in numpy.\r\nCould you check your numpy version?\r\nWhat do you see when you just try `import numpy as np`\r\nI suspect without even TF, the above will fail.\r\n\r\nYou may try uninstalling numpy, and installing 1.15.4.\r\n```\r\npip uninstall numpy\r\npip install numpy==1.15.4\r\n```\r\n\r\nOther than this, you can probably get more help from numpy forums.\r\n", "> pip install numpy==1.15.4\r\n\r\nThanks. It works. In my case, I uninstall numpy 1.16 first and then install 1.15.4. Finally, I install 1.16 version again. Now, tensorflow works well.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}]