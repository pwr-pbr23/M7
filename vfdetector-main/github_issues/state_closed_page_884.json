[{"number": 26966, "title": "Add a `length` or size attribute to the `tf.data.Dataset`", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\nTensorflow version 1.13.1, running on Ubuntu linux 18.04 LTS x64. Yes, I would be willing to contribute. \r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe feature request is to add a length or size method on the `tf.data.Dataset` class. This would allow users to check the number of records in a dataset without having to iterate through the entire dataset. \r\n\r\nHere is a simple use case. I have a bunch of images and labels that I encode to TFRecords format for Tensorflow. The data wrote to the TFRecords format with no trouble, and I can open the dataset with `tf.data.TFRecordDataset()`. I was even able to decode the images and make sure that the data was properly encoded. So far so good. \r\n\r\nHowever, one challenge is that there does not seem to be an easy way to find the number of records that were encoded into the TFRecords file. One thing I would like to check is whether the number of records encoded to the TFRecords file is equal to the number of images in the source directory. Right now the only way to count the records is to setup the `tf.data.Dataset()`, setup the parse function, the batches, and the iterator. And then I would need to iterate through the dataset in order to count the number of records. This seems to be a lot of work to do a simple check on the number of examples encoded to the TFRecords file. \r\n\r\nAnother caveat is that a user could just look at the size of a TFRecords file to get a sense if all of the data is encoded. But this approach has problems because the data encoding to string makes the size of the TFRecords vary substantially (plus or minus) from the sum of the sizes of the individual image and label files. So that metric does not really clarify whether all the data has been encoded. \r\n\r\n**Will this change the current api? How?**\r\nThis would change the `tf.data.Dataset` api. Seems like this would just add an additional method to the `Dataset` class to check for length, just as is done in `numpy` or `pandas`, etc. Even tensors have a `shape` attribute.\r\n\r\nI understand that the `Dataset` API is an iterator and is deliberately setup to not load all data into memory. So that is fine. But users still need to check that the data they are putting into the TFRecords files still need to check/test that the data is encoded properly. \r\n\r\n\r\n**Who will benefit with this feature?**\r\nAny user who uses the `tf.data.Dataset` API to pass data into Tensorflow. This feature will simplify checks/tests to ensure data credibility.\r\n\r\n\r\n**Any Other info.**\r\nHere are some Stack Exchange posts that show the cumbersome way to do this kind of check today. And even then I am not sure that these approaches are entirely successful. \r\n\r\nhttps://stackoverflow.com/questions/51871573/is-there-a-way-to-get-the-size-of-tfrecord-file-and-the-size-of-one-example-in-i\r\n\r\nhttps://stackoverflow.com/questions/42799007/number-of-examples-in-each-tfrecord\r\n\r\nhttps://stackoverflow.com/questions/39196955/how-to-get-the-total-number-of-entries-contained-in-a-tfrecord-file\r\n\r\n", "comments": ["The tf.data API provides the [cardinality](https://www.tensorflow.org/api_docs/python/tf/data/experimental/cardinality) method which returns the cardinality of a dataset (when the cardinality can be computed efficiently from the cardinality of its inputs). Unfortunately, the method will return \"unknown\" for TFRecord dataset as there is no other way to determine the number of records in a TFRecord file then reading the entire file to the end.\r\n", "I am still waiting for this feature of tf 2.x", "In general, to compute the size of the dataset, one would need to run:\r\n\r\n```\r\ni = 0\r\nfor elem in dataset: \r\n  i = i + 1\r\n```\r\n\r\nWhich is simple enough for users to run themselves (note that this can hang if the input dataset is infinite).", "I use a counter var to store the number of input when I build the data loader. but still, I think it will be more convenient for the user to get dataset.length such kind of attribute.", "I agree that it would be convenient but I also think that the expectation should be that `dataset.length` does not have to evaluate the entire input pipeline to return a value (nevermind the fact that it would need to solve the halting problem to determine whether the input pipeline is finite). In other words, I don't think it makes sense to have a \"length\" attribute.\r\n\r\n`cardinality` already provides the information when readily available (i.e. when it is known without having to execute the input pipeline). In cases when `cardinality` cannot be computed without executing the input pipeline, users can compute the information using three lines of code, which in my opinion more clearly communicates the fact that computation of cardinality may in general take a long time. "]}, {"number": 26965, "title": "freeze_graph successful from command line but fails from Python file", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tf-nightly-gpu `v1.12.0-10390-ge9a2281040 1.14.1-dev20190319`\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN 7.5.0 for CUDA 10.0\r\n- GPU model and memory: GeForce GTX 1070 8GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nI have a GraphDef and checkpoint I am trying to freeze.  I am able to do so from the command line, but not from a Python file.\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect the behavior from the Python file to match the CLI behavior. I am referencing [the args on freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py#L77) and failing to find an issue with my usage.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThis is my CLI command that works:\r\n```\r\npython3 -m tensorflow.python.tools.freeze_graph\r\n    --input_graph=mygraph.pb\r\n    --input_checkpoint=mygraph.ckpt\r\n    --input_binary=true\r\n    --output_graph=frozen_graph.pb\r\n    --output_node_names=mygraph/convolutional10/BiasAdd,mygraph/convolutional13/BiasAdd\r\n```\r\n\r\nAnd this is my attempted Python file.\r\n```\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\nfreeze_graph.freeze_graph(\r\n    'mygraph.pb', # input_graph_def\r\n    None, # input_saver_def\r\n    'mygraph.ckpt', # input_checkpoint\r\n    'mygraph/convolutional10/BiasAdd,mygraph/convolutional13/BiasAdd', # output_node_names\r\n    None, # restore_op_name\r\n    None, # filename_tensor_name\r\n    'frozen_graph.pb', # output_graph\r\n    False, # clear_devices\r\n    \"\", # initializer_nodes\r\n    \"\" # variable_names_whitelist\r\n    )\r\n```\r\n\r\nIf I don't include the `variable_names_whitelist` arg at the end, then I get this error: `TypeError: freeze_graph() missing 1 required positional argument: 'initializer_nodes'`. If I do include it, then I get past that weird error and encounter this one instead: `tensorflow.python.framework.errors_impl.NotFoundError: mygraph/convolutional10/BiasAdd,mygraph/convolutional13; No such file or directory`. Not sure what to make of either of these errors. Why is it using the `output_node_names` to find a file? Also, why is it truncating the string? `mygraph/convolutional10/BiasAdd,mygraph/convolutional13` \u2260 `mygraph/convolutional10/BiasAdd,mygraph/convolutional13/BiasAdd`\r\n\r\nAny ideas? Thanks.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nSee above.\r\n", "comments": ["Nupur, are you able to take a look at this? Thanks for your help!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Additionally, given that `freeze_graph.py` is going to be deprecated with 2.0, I am closing this issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26965\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26965\">No</a>\n"]}, {"number": 26964, "title": "Restoring model doesn't see variables", "body": "I saved model using:\r\n```\r\ndef serving_input_receiver_fn():\r\n    features = {'comment': tf.placeholder(dtype=tf.int64, shape=[None, 3])}\r\n    return tf.estimator.export.ServingInputReceiver(features, features)\r\n\r\nclassifier.export_saved_model('saved_model', serving_input_receiver_fn)\r\n```\r\nWhen I'm trying to restore and use it:\r\n```\r\npredict_fn = predictor.from_saved_model(model_path)\r\npred = predict_fn({'comment': [1, 2, 3]})['output']\r\n```\r\nI'm getting the following error:\r\n\r\n> ValueError: Got unexpected keys in input_dict: {'comment'}\r\n> expected: set()\r\n\r\nBut when I use cli everything looks ok:\r\n```\r\nsaved_model_cli show --dir saved_model/1553134286/ --all\r\n\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['predict']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['comment'] tensor_info:\r\n        dtype: DT_INT64\r\n        shape: (-1, 3)\r\n        name: Placeholder:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['class_ids'] tensor_info:\r\n        dtype: DT_INT64\r\n        shape: (-1, 1)\r\n        name: head/predictions/ExpandDims:0\r\n```", "comments": ["Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 26963, "title": "Typo in ops_compatibility", "body": "", "comments": []}, {"number": 26962, "title": "Fix broken links in Java api documentation", "body": "Alternatively, Java installation instruction can be found here https://www.tensorflow.org/install/lang_java as well;however, the github java README.md has more info.", "comments": []}, {"number": 26960, "title": "tf.compat.v2.summary.scalar has no attribute 'summary_scope'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, I have a minimal test file\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):\r\ntf.__version__\r\n'2.0.0-alpha0'\r\n- Python version:\r\n$ python3\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17)\r\n- Bazel version (if compiling from source): binary installer version 0.23\r\n- GCC/Compiler version (if compiling from source):\r\n$ gcc --version\r\ngcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: Not supported\r\n- GPU model and memory: Not supported\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\ntf.compat.v2.summary.scalar(\"stddev\",_st,step=count,description='stddev')\r\nis being processed as an _api.v1.summary\r\n\r\n**Describe the expected behavior**\r\n\r\ntf.compat.v2.summary.scalar(\"stddev\",_st,step=count,description='stddev')\r\nis processed as an _api.v2.summary\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n    tf_sdata = tf.convert_to_tensor(sdata,dtype=tf.float32,name=\"tf_sdata\")\r\n    tfdata_stddev = tf.data.Dataset.from_tensor_slices(tf_sdata[:,0])\r\n\r\n    tfstddev_iter = tfdata_stddev.__iter__()\r\n\r\n    stats_writer = tf.compat.v2.summary.create_file_writer(logs_path,max_queue=10,flush_millis=120000,\r\n            filename_suffix='.stats',name='stats_writer')\r\n\r\n    count = 1\r\n    for _st in tfstddev_iter:\r\n        tf.compat.v2.summary.scalar(\"stddev\",_st,step=count,description='stddev')\r\n        count += 1\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nTraceback (most recent call last):\r\n  File \"./tf2summary_bug.py\", line 71, in <module>\r\n    tf.compat.v2.summary.scalar(\"stddev\",_st,step=count,description='stddev')\r\n  File \"/usr/lib/python3/dist-packages/tensorboard/plugins/scalar/summary_v2.py\", line 55, in scalar\r\n    with tf.summary.summary_scope(\r\nAttributeError: module 'tensorflow._api.v1.summary' has no attribute 'summary_scope'\r\n", "comments": ["I think you need to mark the `stats_writer` as default. Either by calling `stats_writer.set_as_default()` (do not directly add this call to the creation of the SummaryFileWriter [#25976]) or by setting up the summary scope like this:\r\n\r\n```python\r\nwith stats_writer.as_default():\r\n    count = 1\r\n    for _st in tfstddev_iter:\r\n        tf.compat.v2.summary.scalar(\"stddev\",_st,step=count,description='stddev')\r\n        count += 1\r\n```\r\n\r\n", "meyerjo, thank you for your comment.\r\nI tried the context: with stats_writer.as_default():\r\nThis produced the same Traceback:\r\n\r\nTraceback (most recent call last):\r\n  File \"./tf2summary_bug.py\", line 72, in <module>\r\n    tf.compat.v2.summary.scalar(\"stddev\",_st,step=count,description='stddev')\r\n  File \"/usr/lib/python3/dist-packages/tensorboard/plugins/scalar/summary_v2.py\", line 55, in scalar\r\n    with tf.summary.summary_scope(\r\nAttributeError: module 'tensorflow._api.v1.summary' has no attribute 'summary_scope'\r\n\r\nI do not see why this would need a custom summary scope. The problem seems to be the code is executing in the summary_v2.py code as expected and encountering a v1.summary object that is not expected. I could be wrong. Just thinking out loud.", "@kshaeffer Please provide a simple standalone code to reproduce the issue. If the issue was already resolved, please close this issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26960\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26960\">No</a>\n"]}, {"number": 26959, "title": "Component function execution failed: Unknown: Fail to find the dnn implementation.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): should be 2.0.0 alpha; command below throws AttributeError\r\n- Python version: 3.6.7 (Anaconda)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.5\r\n- GPU model and memory: GTX 1050 2gb\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nUsing an LSTM with the Keras package works great until I try to fit my data. This error appears:\r\n\r\n```\r\nUnknownError: Fail to find the dnn implementation.\r\n\t [[{{node unified_lstm/CudnnRNN}}]] [Op:__inference_keras_scratch_graph_1590]\r\n```\r\n\r\nA similar error appears when I try to use CuDNNLSTM\r\n\r\n**Describe the expected behavior**\r\n\r\nThe call to `model.fit(X, Y)` should fit the model\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nmodel = tf.keras.Sequential()\r\n\r\nmodel.add(tf.keras.layers.LSTM(256, return_sequences=True))\r\nmodel.add(tf.keras.layers.Dropout(0.2))\r\nmodel.add(tf.keras.layers.LSTM(256))\r\nmodel.add(tf.keras.layers.Dropout(0.2))\r\nmodel.add(tf.keras.layers.Dense(1))\r\nmodel.compile(loss='mse', optimizer='sgd')\r\n```\r\n```\r\nmodel.fit(X, Y)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nDon't know what would be helpful, but `tf.test.is_gpu_available()` returns true. I followed the instructions for installing Tensorflow 2.0 with GPU support here: https://www.tensorflow.org/install/gpu\r\n\r\nAnother error I get along the way is \r\n```\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:338] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n```\r\n\r\nI've tried one of the solutions in #6698, but `tf.ConfigProto()` was not found. ", "comments": ["@tawe141 Were you able to run any TF code that requires GPU? Did you check whether the cuDNN and CUDA are referenced correctly. Could you create any GitHub gist with public data and share it. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I am getting the same error on Colab,  when running either tensorflow-gpu=2.0.0-alpha0 or tf-nightly-gpu, it throws Unknown: Fail to find the dnn implementation or Failed to get convolution algorithm. This is probably because cuDNN failed to initialize. \r\n\r\nIt works when I go use CPU only, but obviously very slow. This happens for any kind of keras layers. I am using subclass API.", "I am also getting this error on Pop! OS 19.04 running keras with cudnn in conda.", "Please create a new issue with details of your environment and with any reproducible code. Thanks!"]}, {"number": 26958, "title": "Error C2338 in tensorflow/compiler/tf2xla/cpu_function_runtime.h(71)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: branch r1.13\r\n- Python version: Python 3.6.8 :: Anaconda, Inc.\r\n- Installed using virtualenv? pip? conda?: conda 4.6.8\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): cl.exe C/C++ Optimizing Compiler Version 19.16.27026.1 for x64, Visual Studio Build Tools 2017\r\n- CUDA/cuDNN version: CUDA Toolkit V10.0.130, cuDNN 7.5.0\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nGot error C2338 while building tensorflow from `r13.1` branch in this section of code\r\n\r\n```\r\n28    class BufferInfo {\r\n29     public:\r\n\r\n[...]\r\n\r\n66      // Encodes this BufferInfo into two 64 bit integers that can be used to\r\n67      // reconstruct the BufferInfo later using the constructor.  We need this\r\n68      // because we use BufferInfo in places where using protocol buffers would\r\n69      // negatively impact binary size.\r\n70      std::pair<uint64, uint64> Encode() const {\r\n71        static_assert(sizeof(*this) == 16, \"\");    <====\r\n72        uint64 upper = Pack(kind(), size_);\r\n73        uint64 lower = entry_param_number_;\r\n74        return {upper, lower};\r\n75      }\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n(tf_gpu) D:\\Neural\\tensorflow>python ./configure.py\r\nERROR: Failed to query DisplayName of HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\{4A656C6C-D24A-473F-9747-3A8D00907A04}\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nnul\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: 083eb68f-20f4-4c71-8c8d-9b76a9b0d5cf\r\nYou have bazel 0.21.0- (@non-git) installed.\r\nPlease specify the location of python. [Default is D:\\Programs\\Anaconda3\\envs\\tf_gpu\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  D:\\Programs\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [D:\\Programs\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]:\r\n\r\n\r\nPlease specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 6.1\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]: /arch:AVX2\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apacha Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n(tf_gpu) D:\\Neural\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n[...]\r\n\r\nERROR: D:/neural/tensorflow/tensorflow/compiler/tf2xla/BUILD:98:1: C++ compilation of rule '//tensorflow/compiler/tf2xla:cpu_function_runtime' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/stepii/_bazel_stepii/5mniti2w/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\\\MSBuild\\15.0\\bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=D:/Programs/Anaconda3/envs/tf_gpu/python.exe\r\n    SET PYTHON_LIB_PATH=D:/Programs/Anaconda3/envs/tf_gpu/lib/site-packages\r\n    SET TEMP=C:\\Users\\Stepii\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\Stepii\\AppData\\Local\\Temp\r\n  D:/Programs/Anaconda3/envs/tf_gpu/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX2 /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/tf2xla/_objs/cpu_function_runtime/cpu_function_runtime.o /c tensorflow/compiler/tf2xla/cpu_function_runtime.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n.\\tensorflow/compiler/tf2xla/cpu_function_runtime.h(71): error C2338:\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1368.629s, Critical Path: 221.82s\r\nINFO: 2899 processes: 2899 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nFull log [here](https://github.com/tensorflow/tensorflow/files/2989879/FullLog.txt)\r\n", "comments": ["Found a workaround to fix this error message.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26958\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26958\">No</a>\n", "See #26679"]}, {"number": 26957, "title": "why this error have happen? 'NoneType' object has no attribute '_inbound_nodes'", "body": "Hi,\r\n I want to fill a tensor with the value of the specific index of another tensor, this is my full code\r\n```\r\nfrom keras.layers import Input, Concatenate, GaussianNoise,Dropout,BatchNormalization\r\nfrom keras.layers import Conv2D, AtrousConv2D\r\nfrom keras.models import Model\r\nfrom keras.datasets import mnist\r\nfrom keras.callbacks import TensorBoard\r\nfrom keras import backend as K\r\nfrom keras import layers\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\nimport keras as Kr\r\nfrom keras.optimizers import SGD,RMSprop,Adam\r\nfrom keras.callbacks import ReduceLROnPlateau\r\nfrom keras.callbacks import EarlyStopping\r\nfrom keras.callbacks import ModelCheckpoint\r\nimport numpy as np\r\nimport pylab as pl\r\nimport matplotlib.cm as cm\r\nimport keract\r\nfrom matplotlib import pyplot\r\nfrom keras import optimizers\r\nfrom keras import regularizers\r\n\r\nfrom tensorflow.python.keras.layers import Lambda;\r\n#-----------------building w train---------------------------------------------\r\nw_expand=np.zeros((49999,28,28),dtype='float32')\r\nwv_expand=np.zeros((9999,28,28),dtype='float32')\r\nwt_random=np.random.randint(2, size=(49999,4,4))\r\nwt_random=wt_random.astype(np.float32)\r\nwv_random=np.random.randint(2, size=(9999,4,4))\r\nwv_random=wv_random.astype(np.float32)\r\nw_expand[:,:4,:4]=wt_random\r\nwv_expand[:,:4,:4]=wv_random\r\nx,y,z=w_expand.shape\r\nw_expand=w_expand.reshape((x,y,z,1))\r\nx,y,z=wv_expand.shape\r\nwv_expand=wv_expand.reshape((x,y,z,1))\r\n\r\n#-----------------building w test---------------------------------------------\r\nw_test = np.random.randint(2,size=(1,4,4))\r\nw_test=w_test.astype(np.float32)\r\nwt_expand=np.zeros((1,28,28),dtype='float32')\r\nwt_expand[:,0:4,0:4]=w_test\r\nwt_expand=wt_expand.reshape((1,28,28,1))\r\n\r\n#-----------------------encoder------------------------------------------------\r\n#------------------------------------------------------------------------------\r\nwtm=Input((28,28,1))\r\nimage = Input((28, 28, 1))\r\nconv1 = Conv2D(64, (5, 5), activation='relu', padding='same', name='convl1e',dilation_rate=(2,2))(image)\r\nconv2 = Conv2D(64, (5, 5), activation='relu', padding='same', name='convl2e',dilation_rate=(2,2))(conv1)\r\nconv3 = Conv2D(64, (5, 5), activation='relu', padding='same', name='convl3e',dilation_rate=(2,2))(conv2)\r\nBN=BatchNormalization()(conv3)\r\nencoded =  Conv2D(1, (5, 5), activation='relu', padding='same',name='encoded_I',dilation_rate=(2,2))(BN)\r\n\r\n\r\ntemp=tf.reshape(wtm,(28,28))\r\nwfill=Kr.layers.Lambda(lambda x:tf.fill([28,28],x))\r\nwtm_Fill=wfill(temp[0,0])\r\nadd_const = Kr.layers.Lambda(lambda x: x[0] + x[1])\r\nencoded_merged = add_const([encoded,wtm_Fill])\r\n\r\n#wfill=Kr.layers.Lambda(lambda x:tf.fill([28,28],x))\r\n#value=wtm[0][0][0]\r\n#x=tf.fill((28,28,1),value)\r\n#add_const = Kr.layers.Lambda(lambda x: x[0] + x[1])\r\n#encoded_merged = add_const([encoded,x])\r\n#encoder=Model(inputs=[image,wtm], outputs= encoded_merged ,name='encoder')\r\n#encoder.summary()\r\n\r\n#-----------------------decoder------------------------------------------------\r\n#------------------------------------------------------------------------------\r\ndeconv1 = Conv2D(64, (5, 5), activation='relu', padding='same', name='convl1d',dilation_rate=(2,2))(encoded_merged)\r\ndeconv2 = Conv2D(64, (5, 5), activation='relu', padding='same', name='convl2d',dilation_rate=(2,2))(deconv1)\r\ndeconv3 = Conv2D(64, (5, 5), activation='relu',padding='same', name='convl3d',dilation_rate=(2,2))(deconv2)\r\ndeconv4 = Conv2D(64, (5, 5), activation='relu',padding='same', name='convl4d',dilation_rate=(2,2))(deconv3)\r\nBNd=BatchNormalization()(deconv3)\r\n\r\ndecoded = Conv2D(1, (5, 5), activation='sigmoid', padding='same', name='decoder_output',dilation_rate=(2,2))(BNd) \r\n\r\nmodel=Model(inputs=[image,wtm],outputs=decoded)\r\n\r\ndecoded_noise = GaussianNoise(0.5)(decoded)\r\n\r\n#----------------------w extraction------------------------------------\r\nconvw1 = Conv2D(64, (3,3), activation='relu', padding='same', name='conl1w',dilation_rate=(2,2))(decoded_noise)\r\nconvw2 = Conv2D(64, (3, 3), activation='relu', padding='same', name='convl2w',dilation_rate=(2,2))(convw1)\r\nconvw3 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conl3w',dilation_rate=(2,2))(convw2)\r\nconvw4 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conl4w',dilation_rate=(2,2))(convw3)\r\nconvw5 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conl5w',dilation_rate=(2,2))(convw4)\r\nconvw6 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conl6w',dilation_rate=(2,2))(convw5)\r\npred_w = Conv2D(1, (1, 1), activation='sigmoid', padding='same', name='reconstructed_W',dilation_rate=(2,2))(convw6)  \r\nw_extraction=Model(inputs=[image,wtm],outputs=[decoded,pred_w])\r\n\r\nw_extraction.summary()\r\n```\r\nI need sth like this. \r\n```\r\nwtm=\r\n0 1 1\r\n1 1 0\r\n0 0 1\r\n```\r\nwtm(0,0)=0 so I want to produce this new tensor with shape(28,28,1)\r\n\r\n```\r\n0 0 0 ... 0\r\n.\r\n. 0 0 ... 0\r\n0 0 0 ... 0\r\n```\r\nbut when I implement the code it produces this error:\r\n\r\n> Traceback (most recent call last):\r\n> \r\n> File \"\", line 106, in model=Model(inputs=[image,wtm],outputs=decoded)\r\n> \r\n> File \"D:\\software\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper return func(*args, **kwargs)\r\n> \r\n> File \"D:\\software\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\network.py\", line 93, in init self._init_graph_network(*args, **kwargs)\r\n> \r\n> File \"D:\\software\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\network.py\", line 231, in _init_graph_network self.inputs, self.outputs)\r\n> \r\n> File \"D:\\software\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\network.py\", line 1366, in _map_graph_network tensor_index=tensor_index)\r\n> \r\n> File \"D:\\software\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\network.py\", line 1353, in build_map node_index, tensor_index)\r\n> \r\n> File \"D:\\software\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\network.py\", line 1353, in build_map node_index, tensor_index)\r\n> \r\n> File \"D:\\software\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\network.py\", line 1353, in build_map node_index, tensor_index)\r\n> \r\n> File \"D:\\software\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\network.py\", line 1353, in build_map node_index, tensor_index)\r\n> \r\n> File \"D:\\software\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\network.py\", line 1353, in build_map node_index, tensor_index)\r\n> \r\n> File \"D:\\software\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\network.py\", line 1353, in build_map node_index, tensor_index)\r\n> \r\n> File \"D:\\software\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\network.py\", line 1353, in build_map node_index, tensor_index)\r\n> \r\n> File \"D:\\software\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\network.py\", line 1325, in build_map node = layer._inbound_nodes[node_index]\r\n> \r\n> AttributeError: 'NoneType' object has no attribute '_inbound_nodes'\r\n\r\n\r\nwhat should I do? how can I access to the specific value in tensor and do sth like the thing I mentioed before? I really need your explanation. Thanks", "comments": ["I solve the error by changing this part of code \r\n```\r\ntemp=tf.reshape(wtm,(28,28))\r\nwfill=Kr.layers.Lambda(lambda x:tf.fill([28,28],x))\r\nwtm_Fill=wfill(temp[0,0])\r\n``` \r\nto  \r\n```\r\nrep=Kr.layers.Lambda(lambda x:Kr.backend.repeat(x,28))\r\n    a=rep(Kr.layers.Lambda(lambda x:x[1,1])(wtm))\r\n```\r\nbut  ` Kr.layers.Lambda(lambda x:x[1,1])(wtm)` does not give me the value in row=1 and column=1 the output shape is (4,1) while I need (1,1) what should I do to solve this?\r\n", "@Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 26956, "title": "tensorflow/core/kernels/slice_op_gpu.cu.cc is never referenced in any BUILD file", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nThis file: https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/core/kernels/slice_op_gpu.cu.cc\r\n\r\nIs never referenced anywhere in the tree. When building with CUDA and a dependency on [//tensorflow/core/kernels:strided_slice_op](https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/core/kernels/BUILD#L119), it fails at link time with errors like this one:\r\n\r\nundefined reference to `tensorflow::functor::Slice<Eigen::GpuDevice, signed char, 5>::operator()(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<signed char, 5, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<signed char const, 5, 1, long>, 16, Eigen::MakePointer>, Eigen::DSizes<long, 5> const&, Eigen::DSizes<long, 5> const&)'\r\n\r\nThe problem seems to be that [tensorflow::HandleStridedSliceCase](https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/core/kernels/strided_slice_op_impl.h#L95) will call `functor::Slice` if the slice is a \"simple slice\", so when the template is instantiated for GPU [here](https://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/core/kernels/strided_slice_op_impl.h#L284-L290) it references the undefined GPU version of `functor::Slice`.\r\n\r\nSeems to me like somewhere in tensorflow/core/kernels/BUILD should be referencing slice_op_gpu.cu.cc, maybe the strided_slice_op target?", "comments": ["It is referenced through the bazel macros we define.\r\nHere is the build rule for `slice_op`\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/BUILD#L1107\r\nAs you can see it also has a dependency on `strided_slice_op`\r\n\r\nThis rule is an instance of tf_kernel_library. That is defined here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L1262\r\n\r\nThis rule automatically includes `prefix.cu.cc` files to be built:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L1314\r\n\r\nI will close this issue to avoid confusion.\r\nPlease file a new issue with the build failure you are seeing.\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26956\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26956\">No</a>\n", "@gunan The problem is that `strided_slice_op` does not have a dependency on `slice_op`, not the other way around. Here's the build rule for `strided_slice_op`: https://github.com/tensorflow/tensorflow/blob/93a7be53a9c314b1e4dc35fc97d5be8367e8fc01/tensorflow/core/kernels/BUILD#L121-L159"]}, {"number": 26955, "title": "[TF 2.0 API Docs] Multiple documentation fixes", "body": "#26538 Raises listed and defined for ``tf.zeros_initializer``.\r\n\r\nI will be subsequently making commits to other doc fixes mentioned in #26532", "comments": []}, {"number": 26954, "title": "Documentation Request: Transfer Learning in TF 2 with same and/or different final layer", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 2 alpha\r\n- Doc Link: N/A\r\n\r\n\r\n**Describe the documentation issue**\r\nHello, I'm in the process of porting some of our existing TF 1.x based code to be ready for TF 2.0. This means we are trying to be focused around tf.keras, so I was hoping for an example of the best way to do transfer learning in tf.keras. Specifically, we have the following (I assume common) use case.\r\n\r\n1. Train a model (say it's classification) with on dataset a, with 100 classes in the outputs.\r\n2. Finetune the model on dataset b, using the same architecture, also with 100 classes.\r\n3. Finetune the model on dataset c, which is the same except it has 50 classes, and thus the final layer has a different shape. So, we want to load all of the weights except the final layer,  and then train.\r\n\r\nIn TF 1.x, we used tf.estimator. For task 3, we would label the final layer with the name \"final_layer\", and then when warm-starting the models for finetuning, we would use the WarmstartSettings object to exclude weights that have final_layer in them. What is the equivalent for tf.keras in TF 2.x? Probably the cleanest way we could have the old functionality is if tf.keras.Model.load_weights had a regex field in the same form as WarmStartSettings, which only loaded a subset of variables.\r\n\r\nThanks!\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nI'm happy to write the docs if someone could give a high-level summary of how this is done. All the ways I've come up with are pretty jank, and I assume there is a better way to do this given how elegant it was in TF 1.X.\r\n", "comments": ["The easiest would be to collect the common layers in a single Model and save and load weights on that. While training each configuration (50 / 100 classes) you can save_weights and load_weights on the slightly larger Model which includes the common Model, but to make the transfer I'd use the common Model.\r\n\r\nDoes that make sense? So three Models, one common and one which includes it for each task. It should just work with current saving APIs.", "Thanks very much for the help!\r\n\r\nTo clarify, are you suggesting something like this:\r\n``` python\r\ncommon_model = tf.keras.Model(..)\r\nmodel_50 = tf.Sequential([common_model, tf.keras.Dense(50)])\r\nmodel_100 = tf.Sequential([common_model, tf.keras.Dense(100)])\r\n```\r\nthen, for example, I could train model_100, and then save_weights on that. \r\n\r\nWhen I want to train model_50, would first running model_50.load_weights on the exported weights from model_100 work? I would have guessed that would yell at me since the architectures are different. I'd like to use the SavedModel format for backwards compat with other stuff if possible.\r\n", "Yep, those are the models I had in mind. Then if you use `common_model.save_weights` after training model_100, you can load it into a compatible `common_model` with `common_model.load_weights` when you want to train model_50. (Or if you can keep them both in the same program they'll naturally share weights.)\r\n\r\nIt's also possible to extract `common_model`'s weights from `model_100`'s checkpoint, by making a `tf.Sequential([common_model])` and using `load_weights` on that. It'll ignore the weights from the missing final `Dense`, and then you can use `common_model` in `model_50`.\r\n\r\nNot sure what you mean by the SavedModel comment. You can save in the TensorFlow checkpoint format from `save_weights`; that's the default as long as you don't use a `.h5` suffix. You could also save a SavedModel for `common_model` and re-use that if you wanted.", "Got it, that makes sense!\r\n\r\nIs there any equivalent (or planned equivalent) to the regex-based selection system for loading weights as was the case in WarmstartSettings? That was very convenient since it let us train a model in 1 way, but then flexibly experiment with warmstarting different subsets of it.", "No, I don't know of any plans for regex matching of variables. We don't use variable names to match checkpoints with variable objects in 2.x (except in Estimator), so it'd be a pretty weird API.\r\n\r\nIt's certainly possible to programatically filter weights like you describe, though. Just mirror the objects in the real model, attaching just the variable objects you want to load. There's some mention of this [in the 2.x checkpoints guide](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/checkpoints.ipynb). Without too much work you could wrap that in an API that takes regular expressions.", "Got it, thanks. \r\n\r\nOK, I like that idea. Could you just clarify what you mean by \"mirror the objects in the real model\"? My thinking is I have a model that's something like,\r\n``` python\r\ndef build_model():\r\n    return tf.keras.Sequential([\r\n        Dense(10, input_shape=(50, ), activation=\"relu\"), \r\n        Dense(5, name=\"final_layer\")\r\n    ])\r\n\r\nmodel = build_model()\r\nmodel.fit(...)\r\nmodel.save_weights(..)\r\n\r\n```\r\nNow, when I want to load just the weights associated with the first dense layer in the model via some regex, here is my thinking of how to do it:\r\n\r\n```\r\nmodel = build_model()\r\nmodel_mirror = build_model()\r\nmodel_mirror.load_weights(..)\r\n\r\nfor variable, mirrored_variable in zip(model.variables, model_mirror.variables):\r\n    if regex_matches(variable.name):\r\n        variable.assign(mirrored_variable.value())\r\n```\r\n\r\nDoes that match what you were thinking?\r\n    \r\n", "That's better than what I was thinking. Eventually you should be able to do a traversal over the object graph and re-construct it with modifications. The problem is we don't yet have a public API for iterating over checkpoint dependencies with their names.\r\n\r\nIf we did, you could do something like this:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential(\r\n    [tf.keras.layers.Dense(5),\r\n     tf.keras.layers.Dense(6),\r\n     tf.keras.layers.Dense(7)])\r\nmodel(tf.constant([[1.]]))  # create variables\r\n\r\ndef visit_and_filter(obj, substitute, variable_predicate):\r\n  for k, v in obj._checkpoint_dependencies:\r\n    if isinstance(v, tf.Variable):\r\n      if variable_predicate(v):\r\n        setattr(substitute, k, v)\r\n    elif isinstance(v, tf.keras.layers.Layer):\r\n      v_substitute = tf.keras.Model()\r\n      setattr(substitute, k, v_substitute)\r\n      visit_and_filter(v, v_substitute, variable_predicate)\r\n\r\nmirrored = tf.keras.Model()\r\nvisit_and_filter(model, mirrored, lambda v: \"bias\" in v.name)\r\n\r\nprint([w.name for w in model.weights])\r\nprint([w.name for w in mirrored.weights])\r\n```\r\n\r\nWhich works, but uses a non-stable/private API (`_checkpoint_dependencies`). I've considered adding a stable API for this in the past, but haven't gotten something everyone was happy with. __dict__ or vars() doesn't really work because Sequential tracks things without assigning them to attributes; changing that would be another fix. \r\n\r\nThis would also let you filter whole Layer or Model objects, which might be neat. And if you filtered Layer objects instead of Variables directly, you wouldn't even need the `model()` call to create variables and could instead use restore-on-create.", "Got it, this makes a lot of sense. OK thanks so much for the help!", "@ankitvgupta Is this resolved? If yes, please close the issue. Thanks!", "I think it was resolved. I am closing the issue. But, please let me know if I'm mistaken."]}, {"number": 26953, "title": "Reformatted with Python syntax.", "body": "", "comments": []}, {"number": 26952, "title": "OSError: libcudart.so.9.0: cannot open shared object file: No such file or directory", "body": "**I know that my question isn't nothing to do with tensorflow but i'll appreciate any help**\r\nwhen i was trying to prepare voc dataset for training pre trained model i got this error\r\n\r\n> OSError: libcudart.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n`from gluoncv.data import VOCDetection\r\nclass VOCLike(VOCDetection):\r\n    CLASSES = ['iris']\r\n    def __init__(self, root, splits, transform=None, index_map=None, preload_label=True):\r\n        super(VOCLike, self).__init__(root, splits, transform, index_map, preload_label)\r\ndataset = VOCLike(root='/home/dell/Bureau/myDataset', splits=((2019, 'train'),))\r\nprint(dataset[0])`\r\n**Knowing that I can't use cuda or gpu because i haven't a graphic card nvidia**\r\n", "comments": ["@essalahsouad I think some function in your code is forcing to use GPU. which is why it was searching for CUDA9.0 related files and throws error as there is no GPU and no CUDA files. Could you run a simple cpu based model and check whether there is any bug. Please check there are lots of community support on stackoverflow. Thanks!", "@jvishnuvardhan \r\nthank you for your reply", "I think it was resolved. I am closing the issue. Please open a new ticket if you see similar issue again. Thanks!"]}, {"number": 26951, "title": "EagerTensor.numpy() fails with string dtype when called from py_function on GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.1\r\n- Bazel version (if compiling from source): 0.20.0 \r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla K80\r\n\r\n**Describe the current behavior**\r\nWhen a `py_function` op is defined for GPU (the default if one is available), and it calls a function that takes a string argument, a `RuntimeError` is raised if one attempts to access the value of the argument by calling `EagerTensor.numpy()`.\r\n\r\nIf the `py_function` op is defined for CPU no error is raised.\r\n\r\n**Describe the expected behavior**\r\nOne of the following:\r\n  - String arguments are disallowed for a `py_function` running on GPU, and the TF documentation explains this behavior.\r\n  - The value of a string argument is accessible in the scenario described above.\r\n\r\n**Code to reproduce the issue**\r\nThis does not work:\r\n```python\r\ndef func_with_string_arg(arg):\r\n    return arg.numpy()\r\n\r\nwith tf.Graph().as_default():\r\n    tf_arg = tf.convert_to_tensor('abcdef')\r\n    ret = tf.py_function(\r\n        func_with_string_arg,\r\n        [tf_arg],\r\n        tf.dtypes.string\r\n    )\r\n    with tf.Session() as sess:\r\n        print(sess.run(ret))\r\n```\r\n\r\nSnippet of the error:\r\n```\r\nUnknownError: RuntimeError: Error copying tensor to device: CPU:0. Can't copy 38 bytes of a tensor into another with 32 bytes buffer.\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/asarroff/anaconda3/envs/tf-1.13.1/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 205, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"/home/asarroff/anaconda3/envs/tf-1.13.1/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 107, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"<ipython-input-5-590852b9a5d8>\", line 2, in func_with_string_arg\r\n    return arg.numpy()\r\n\r\n  File \"/home/asarroff/anaconda3/envs/tf-1.13.1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 725, in numpy\r\n    return self._cpu_nograd()._numpy()  # pylint: disable=protected-access\r\n\r\n  File \"/home/asarroff/anaconda3/envs/tf-1.13.1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 892, in _cpu_nograd\r\n    return self._copy_nograd(context.context(), \"CPU:0\")\r\n\r\n  File \"/home/asarroff/anaconda3/envs/tf-1.13.1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 840, in _copy_nograd\r\n    new_tensor = self._copy_to_device(context=ctx._handle, device=device_name)\r\n\r\nRuntimeError: Error copying tensor to device: CPU:0. Can't copy 38 bytes of a tensor into another with 32 bytes buffer.\r\n\r\n\r\n\t [[{{node EagerPyFunc}}]]\r\n\t [[{{node EagerPyFunc}}]]\r\n```\r\n\r\nThis works:\r\n```python\r\nwith tf.Graph().as_default():\r\n    tf_arg = tf.convert_to_tensor('abcdef')\r\n    with tf.device('/device:CPU:*'):\r\n        ret = tf.py_function(\r\n            func_with_string_arg,\r\n            [tf_arg],\r\n            tf.dtypes.string\r\n        )\r\n    with tf.Session() as sess:\r\n        print(sess.run(ret))\r\n```\r\n", "comments": ["I am having this issue with 2.0.0-rc0, is this problem supposed to be fixed in that release?", "The same problem for TF 2.0.", "@RomanSteinberg @frantracer As your issue is related to `TF2.0`, Please create a new issue with details and a standalone code to reproduce the issue. Thanks! ", "@woodshop,\r\nI've executed your code in Tensorflow Version 1.15 and it works without any error. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/8686d44a0583713b6e00c44c4032f144/gh_26951_gpu.ipynb) of the working code. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26951\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26951\">No</a>\n"]}, {"number": 26950, "title": "FailedPreconditionError when training on TPU", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):1.13.1\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI want to train my model using AdaBound optimizer but the model after compilation is throwing error using the TPU. \r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport re\r\nclass AdaBoundOptimizer(tf.train.Optimizer):\r\n    \"\"\"Optimizer that implements the AdaBound algorithm.\r\n    See [Luo et al., 2019](https://openreview.net/forum?id=Bkg3g2R9FX)\r\n    ([pdf](https://openreview.net/pdf?id=Bkg3g2R9FX)).\r\n    \"\"\"\r\n\r\n    def __init__(self,\r\n                 learning_rate=0.001,\r\n                 final_lr=0.1,\r\n                 beta1=0.9,\r\n                 beta2=0.999,\r\n                 gamma=1e-3,\r\n                 epsilon=1e-8,\r\n                 amsbound=False,\r\n                 decay=0.,\r\n                 weight_decay=0.,\r\n                 exclude_from_weight_decay=None,\r\n                 use_locking=False, name=\"AdaBound\"):\r\n        super(AdaBoundOptimizer, self).__init__(use_locking, name)\r\n\r\n        if final_lr <= 0.:\r\n            raise ValueError(\"Invalid final learning rate : {}\".format(final_lr))\r\n        if not 0. <= beta1 < 1.:\r\n            raise ValueError(\"Invalid beta1 value : {}\".format(beta1))\r\n        if not 0. <= beta2 < 1.:\r\n            raise ValueError(\"Invalid beta2 value : {}\".format(beta2))\r\n        if not 0. <= gamma < 1.:\r\n            raise ValueError(\"Invalid gamma value : {}\".format(gamma))\r\n        if epsilon <= 0.:\r\n            raise ValueError(\"Invalid epsilon value : {}\".format(epsilon))\r\n\r\n        self._lr = learning_rate\r\n        self._beta1 = beta1\r\n        self._beta2 = beta2\r\n        self._final_lr = final_lr\r\n        self._gamma = gamma\r\n        self._epsilon = epsilon\r\n        self._amsbound = amsbound\r\n        self._decay = decay\r\n        self._weight_decay = weight_decay\r\n        self._exclude_from_weight_decay = exclude_from_weight_decay\r\n\r\n        self._base_lr = learning_rate\r\n\r\n    def apply_gradients(self, grads_and_vars, global_step=None, name=None):\r\n        lr = self._lr\r\n        t = tf.cast(global_step, dtype=tf.float32)\r\n\r\n        if self._decay > 0.:\r\n            lr *= (1. / (1. + self._decay * t))\r\n\r\n        t += 1\r\n\r\n        bias_correction1 = 1. - (self._beta1 ** t)\r\n        bias_correction2 = 1. - (self._beta2 ** t)\r\n        step_size = (lr * tf.sqrt(bias_correction2) / bias_correction1)\r\n\r\n        # Applies bounds on actual learning rate\r\n        # lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay\r\n        final_lr = self._final_lr * lr / self._base_lr\r\n        lower_bound = final_lr * (1. - 1. / (self._gamma * t + 1.))\r\n        upper_bound = final_lr * (1. + 1. / (self._gamma * t))\r\n\r\n        assignments = []\r\n        for grad, param in grads_and_vars:\r\n            if grad is None and param is None:\r\n                continue\r\n\r\n            param_name = self._get_variable_name(param.name)\r\n\r\n            m = tf.get_variable(\r\n                name=param_name + \"/adabound_m\",\r\n                shape=param.shape.as_list(),\r\n                dtype=tf.float32,\r\n                trainable=False,\r\n                initializer=tf.zeros_initializer())\r\n            v = tf.get_variable(\r\n                name=param_name + \"/adabound_v\",\r\n                shape=param.shape.as_list(),\r\n                dtype=tf.float32,\r\n                trainable=False,\r\n                initializer=tf.zeros_initializer())\r\n            if self._amsbound:\r\n                v_hat = tf.get_variable(\r\n                    name=param_name + \"/adabound_v_hat\",\r\n                    shape=param.shape.as_list(),\r\n                    dtype=tf.float32,\r\n                    trainable=False,\r\n                    initializer=tf.zeros_initializer())\r\n\r\n            m_t = (\r\n                    tf.multiply(self._beta1, m) + tf.multiply(1. - self._beta1, grad))\r\n            v_t = (\r\n                    tf.multiply(self._beta2, v) + tf.multiply(1. - self._beta2, tf.square(grad)))\r\n\r\n            if self._amsbound:\r\n                # Maintains the maximum of all 2nd moment running avg. till now\r\n                v_hat_t = tf.maximum(v_hat, v_t)\r\n\r\n                # Use the max. for normalizing running avg. of gradient\r\n                denom = (tf.sqrt(v_hat_t) + self._epsilon)\r\n            else:\r\n                denom = (tf.sqrt(v_t) + self._epsilon)\r\n\r\n            step_size_p = step_size * tf.ones_like(denom)\r\n            step_size_p_bound = step_size_p / denom\r\n\r\n            lr_t = m_t * tf.clip_by_value(t=step_size_p_bound,\r\n                                          clip_value_min=lower_bound,\r\n                                          clip_value_max=upper_bound)\r\n            p_t = param - lr_t\r\n\r\n            if self._do_use_weight_decay(param_name):\r\n                p_t += self._weight_decay * param\r\n\r\n            update_list = [param.assign(p_t), m.assign(m_t), v.assign(v_t)]\r\n            if self._amsbound:\r\n                update_list.append(v_hat.assign(v_hat_t))\r\n\r\n            assignments.extend(update_list)\r\n\r\n        # update the global step\r\n        assignments.append(global_step.assign_add(1))\r\n\r\n        return tf.group(*assignments, name=name)\r\n\r\n    def _do_use_weight_decay(self, param_name):\r\n        \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\r\n        if not self._weight_decay:\r\n            return False\r\n        if self._exclude_from_weight_decay:\r\n            for r in self.exclude_from_weight_decay:\r\n                if re.search(r, param_name) is not None:\r\n                    return False\r\n        return True\r\n\r\n    @staticmethod\r\n    def _get_variable_name(param_name):\r\n        \"\"\"Get the variable name from the tensor name.\"\"\"\r\n        m = re.match(\"^(.*):\\\\d+$\", param_name)\r\n        if m is not None:\r\n            param_name = m.group(1)\r\n        return param_name\r\n\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\ntpu_model = tf.contrib.tpu.keras_to_tpu_model(\r\n      res_model,\r\n      strategy=tf.contrib.tpu.TPUDistributionStrategy(\r\n          tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n      )\r\n  )\r\ntpu_model.compile(optimizer=AdaBoundOptimizer(learning_rate=0.0001, final_lr=0.1, beta1=0.9, beta2=0.999, amsbound=False), loss=tf.keras.losses.binary_crossentropy, metrics=['acc'])\r\n \r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nlabeled_files size : 220025\r\ntest_files size : 57458\r\n\r\nFOLD: 0\r\nTrain: \r\n{0: 104725, 1: 71292}\r\nVal: \r\n{0: 26182, 1: 17824}\r\nINFO:tensorflow:Querying Tensorflow master (grpc://10.111.5.58:8470) for TPU system metadata.\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6439601644184290312)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5999939926878101922)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 2915973256505920338)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 302180528578565888)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7907037798004119455)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 8233517434947874609)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 16859822429318257159)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11771514073813576570)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 8064934956870564867)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 14384107720867386803)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 13575223746004231404)\r\nWARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\r\nEpoch 1/10\r\nINFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 96, 96, 3), dtype=tf.float32, name='input_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_1_target_10')]\r\nINFO:tensorflow:Overriding default placeholder.\r\nINFO:tensorflow:Remapping placeholder for input_1\r\nINFO:tensorflow:Started compiling\r\nINFO:tensorflow:Finished compiling. Time elapsed: 12.17287826538086 secs\r\nINFO:tensorflow:Setting weights on TPU model.\r\n---------------------------------------------------------------------------\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1333     try:\r\n-> 1334       return fn(*args)\r\n   1335     except errors.OpError as e:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1318       return self._call_tf_sessionrun(\r\n-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1320 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1406         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1407         run_metadata)\r\n   1408 \r\n\r\nFailedPreconditionError: Combined status information from 9 operations:\r\n\r\nStatus code: Failed precondition [9x]\r\n  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.\r\n  \t [[{{node tpu_140680564147536//dense_1/kernel/adabound_m}}]] [1x]\r\n  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.\r\n  \t [[{{node tpu_140680564147536//dense_1/kernel/adabound_m}}]]\r\n  \t [[{{node TPUReplicateMetadata}}]] [1x]\r\n  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.\r\n  \t [[{{node tpu_140680564147536//dense_1/kernel/adabound_m}}]]\r\n  \t [[{{node cluster/_variable_copy/_80}}]] [7x]\r\n(0 successful operations.)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-37-fe905384dbd9> in <module>()\r\n     75 \r\n     76   history = tpu_model.fit_generator(data_gen(train,id_label_map,train_batch_size,do_train_augmentations),steps_per_epoch=train_steps,epochs = 10,\r\n---> 77                                    validation_data = data_gen(val,id_label_map,val_batch_size,do_inference_aug),validation_steps = val_steps,callbacks = callbacks)\r\n     78 \r\n     79 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1424         use_multiprocessing=use_multiprocessing,\r\n   1425         shuffle=shuffle,\r\n-> 1426         initial_epoch=initial_epoch)\r\n   1427 \r\n   1428   def evaluate_generator(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\r\n    189       progbar.on_batch_begin(step, batch_logs)\r\n    190 \r\n--> 191       batch_outs = batch_function(*batch_data)\r\n    192       if not isinstance(batch_outs, list):\r\n    193         batch_outs = [batch_outs]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n   1189       else:\r\n   1190         self._make_fit_function()\r\n-> 1191         outputs = self._fit_function(ins)  # pylint: disable=not-callable\r\n   1192 \r\n   1193     if reset_metrics:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in __call__(***failed resolving arguments***)\r\n   1267         tpu_model_ops.infeed_op, tpu_model_ops.execute_op,\r\n   1268         tpu_model_ops.outfeed_op\r\n-> 1269     ], infeed_dict)\r\n   1270     return self._process_outputs(outfeed_outputs)\r\n   1271 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    927     try:\r\n    928       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 929                          run_metadata_ptr)\r\n    930       if run_metadata:\r\n    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1151       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1152                              feed_dict_tensor, options, run_metadata)\r\n   1153     else:\r\n   1154       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1326     if handle is None:\r\n   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1328                            run_metadata)\r\n   1329     else:\r\n   1330       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1346           pass\r\n   1347       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1348       raise type(e)(node_def, op, message)\r\n   1349 \r\n   1350   def _extend_graph(self):\r\n\r\nFailedPreconditionError: Combined status information from 9 operations:\r\n\r\nStatus code: Failed precondition [9x]\r\n  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.\r\n  \t [[node tpu_140680564147536//dense_1/kernel/adabound_m (defined at <ipython-input-36-baab4250bb83>:77) ]] [1x]\r\n  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.\r\n  \t [[node tpu_140680564147536//dense_1/kernel/adabound_m (defined at <ipython-input-36-baab4250bb83>:77) ]]\r\n  \t [[node TPUReplicateMetadata (defined at <ipython-input-37-fe905384dbd9>:77) ]] [1x]\r\n  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.\r\n  \t [[node tpu_140680564147536//dense_1/kernel/adabound_m (defined at <ipython-input-36-baab4250bb83>:77) ]]\r\n  \t [[{{node cluster/_variable_copy/_80}}]] [7x]\r\n(0 successful operations.)\r\n\r\nCaused by op 'tpu_140680564147536//dense_1/kernel/adabound_m', defined at:\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-37-fe905384dbd9>\", line 77, in <module>\r\n    validation_data = data_gen(val,id_label_map,val_batch_size,do_inference_aug),validation_steps = val_steps,callbacks = callbacks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1426, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\", line 191, in model_iteration\r\n    batch_outs = batch_function(*batch_data)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1191, in train_on_batch\r\n    outputs = self._fit_function(ins)  # pylint: disable=not-callable\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\", line 1260, in __call__\r\n    infeed_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\", line 1169, in _tpu_model_ops_for_input_specs\r\n    infeed_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\", line 1079, in _specialize_model\r\n    _model_fn, inputs=[[] for _ in range(self._tpu_assignment.num_towers)])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 689, in split_compile_and_replicate\r\n    outputs = computation(*computation_inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\", line 1033, in _model_fn\r\n    self._cloned_model._make_fit_function()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1926, in _make_fit_function\r\n    '_fit_function', [self.total_loss] + metrics_tensors)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1895, in _make_train_function_helper\r\n    params=self._collected_trainable_weights, loss=self.total_loss)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizers.py\", line 763, in get_updates\r\n    grads, global_step=self.iterations)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_optimizer.py\", line 171, in apply_gradients\r\n    return self._opt.apply_gradients(summed_grads_and_vars, global_step, name)\r\n  File \"<ipython-input-36-baab4250bb83>\", line 77, in apply_gradients\r\n    initializer=tf.zeros_initializer())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1479, in get_variable\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1220, in get_variable\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 530, in get_variable\r\n    return custom_getter(**custom_getter_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 682, in custom_getter\r\n    return getter(name, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 499, in _true_getter\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 911, in _get_single_variable\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 213, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 176, in _variable_v1_call\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 155, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2488, in default_variable_creator\r\n    import_scope=import_scope)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 217, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 294, in __init__\r\n    constraint=constraint)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 413, in _init_from_args\r\n    graph_mode=self._in_graph_mode)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 67, in eager_safe_variable_handle\r\n    container=container)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 1266, in var_handle_op\r\n    shared_name=shared_name, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nFailedPreconditionError (see above for traceback): Combined status information from 9 operations:\r\n\r\nStatus code: Failed precondition [9x]\r\n  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.\r\n  \t [[node tpu_140680564147536//dense_1/kernel/adabound_m (defined at <ipython-input-36-baab4250bb83>:77) ]] [1x]\r\n  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.\r\n  \t [[node tpu_140680564147536//dense_1/kernel/adabound_m (defined at <ipython-input-36-baab4250bb83>:77) ]]\r\n  \t [[node TPUReplicateMetadata (defined at <ipython-input-37-fe905384dbd9>:77) ]] [1x]\r\n  Error while reading resource variable tpu_140680564147536//dense_1/kernel/adabound_m from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/tpu_140680564147536//dense_1/kernel/adabound_m/N10tensorflow3VarE does not exist.\r\n  \t [[node tpu_140680564147536//dense_1/kernel/adabound_m (defined at <ipython-input-36-baab4250bb83>:77) ]]\r\n  \t [[{{node cluster/_variable_copy/_80}}]] [7x]\r\n(0 successful operations.)", "comments": ["The issue is that you're creating the variables in apply_gradients, so as the error message says, they're uninitialized.\r\n\r\nSee _create_slots here for an example of where to create them:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/adam.py", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26950\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26950\">No</a>\n", "@jing-dong for an example of an error message we should try to improve.", "@jhseu Thanks for the example!"]}, {"number": 26949, "title": "Use SourceVersion#latest instead of latestSupported", "body": "latestSupported doesn't work if the JDK is newer than the compiler\r\nversion; see https://github.com/bazelbuild/bazel/issues/7776", "comments": ["@gunan This fix is also needed for updating Bazel to 0.23.2", "I am not the owner for this code. Please assign to the owner. Thanks. "]}, {"number": 26948, "title": "why do i get empty array at tf.keras.Sequential(layers).trainable_variables?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- TensorFlow version: 2.0.0-alpha0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n**why do I get empty array ?**\r\n\r\nI'm trying to follow tf2.0 tuts on the website to perform a simple perceptron but seems like the package manager isn't able to pick up the new release but also not throwing any error.\r\nwhat could be the reason to this?\r\n\r\n![well](https://user-images.githubusercontent.com/26624903/54701853-c920fc00-4b5b-11e9-9fcd-ff39fff60284.jpg)\r\n", "comments": ["The Layers aren't built yet. You can either pass an input_shape to Sequential, which will build everything, or you can call build() explicitly. You can also call the model with data and they will build automatically.\r\n\r\nWhichever option, the Layer just needs to know its input shape before it can create its variables."]}, {"number": 26947, "title": "Tensorflow 2.0 compile for raspberry pi failed with file not found error", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **Python version**: tried both 2.7 and 3.6\r\n- **Bazel version (if compiling from source)**: 0.13.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**:10.0.130\r\n- **GPU model and memory**: GTX 1070 Mobile 8GB\r\n- **Exact command to reproduce**:\r\nfrom https://www.tensorflow.org/install/source_rpi\r\n```\r\ngit checkout r2.0\r\nsudo tensorflow/tools/ci_build/ci_build.sh PI \\\r\n    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n```\r\n### Describe the problem\r\nI am trying to compile Tensorflow 2.0 for the raspberry pi to install it in Python 2 for use with ROS Kinetic. The pi is using Ubuntu Mate 16.04 and I am doing the cross compilation on my laptop with Ubuntu 16.04.\r\nI used the instructions mentioned on the tensorflow website and tried to compile for both 2.7 and 3. The build fails in both cases with this error:\r\n```\r\n/usr/include/python3.4/pyconfig.h:13:55: fatal error: arm-linux-gnueabihf/python3.4m/pyconfig.h: No such file or directory\r\n #  include <arm-linux-gnueabihf/python3.4m/pyconfig.h>\r\n                                                       ^\r\ncompilation terminated.\r\n\r\n```\r\n\r\n### Source code / logs\r\n```\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: 7644074a-82e2-4b8a-bcb6-10884a6300b8\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: 8e36ba59-8da1-4b1c-8840-493d798ee2ca\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: e48bec62-a84a-4ec3-89f5-c1a7a71fb94f\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100   159    0   159    0     0    418      0 --:--:-- --:--:-- --:--:--   418\r\n  0     0    0  323M    0     0  2932k      0 --:--:--  0:01:53 --:--:-- 7693k\r\nCloning into '/tmp/openblas_src'...\r\nNote: checking out '5a6a2bed9aff0ba8a18651d5514d029c8cae336a'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n\r\n  git checkout -b new_branch_name\r\n\r\nHEAD is now at 5a6a2be... Merge pull request #1623 from fenrus75/fast-thread\r\n/tmp/toolchain_install//tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/bin/arm-linux-gnueabihf-ar: creating ../libopenblas_armv6p-r0.3.1.dev.a\r\nmemory.c: In function 'get_num_procs':\r\nmemory.c:191:7: warning: unused variable 'n' [-Wunused-variable]\r\n int i,n;\r\n       ^\r\nmemory.c:191:5: warning: unused variable 'i' [-Wunused-variable]\r\n int i,n;\r\n     ^\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: 4ffd1046-cb13-4bd9-836b-68b4e27ba4fb\r\nLoading: \r\nLoading: 0 packages loaded\r\nAnalyzing: 4 targets (3 packages loaded)\r\nAnalyzing: 4 targets (4 packages loaded, 0 targets configured)\r\nAnalyzing: 4 targets (82 packages loaded, 2059 targets configured)\r\nAnalyzing: 4 targets (173 packages loaded, 7791 targets configured)\r\nAnalyzing: 4 targets (306 packages loaded, 12072 targets configured)\r\nAnalyzing: 4 targets (356 packages loaded, 17197 targets configured)\r\nAnalyzing: 4 targets (383 packages loaded, 20919 targets configured)\r\nWARNING: /workspace/tensorflow/python/BUILD:3239:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /workspace/tensorflow/python/BUILD:102:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /workspace/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /workspace/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /workspace/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /workspace/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /workspace/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed 4 targets (383 packages loaded, 22853 targets configured).\r\nBuilding: no action\r\nINFO: Found 4 targets...\r\n[5 / 17] [-----] BazelWorkspaceStatusAction stable-status.txt\r\n[43 / 352] no action\r\n[1,465 / 7,100] [-----] Creating runfiles tree bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles [for host] ... (2 actions, 0 running)\r\n[1,922 / 9,165] no action\r\n[2,190 / 10,960] no action\r\nINFO: From Compiling tensorflow/lite/toco/toco_graphviz_dump_options.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/flatbuffers/src/util.cpp:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option \"-Wno-implicit-fallthrough\"\r\n[2,625 / 12,575] Compiling external/org_sqlite/sqlite3.c; 1s local ... (7 actions running)\r\nINFO: From Compiling tensorflow/lite/util.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option \"-Wno-extern-c-compat\"\r\nINFO: From Compiling tensorflow/lite/c/c_api_internal.c:\r\nIn file included from tensorflow/lite/c/c_api_internal.c:16:0:\r\n./tensorflow/lite/c/c_api_internal.h:60:34: warning: 'struct TfLiteContext' declared inside parameter list\r\n   TfLiteStatus (*Refresh)(struct TfLiteContext* context);\r\n                                  ^\r\n./tensorflow/lite/c/c_api_internal.h:60:34: warning: its scope is only this definition or declaration, which is probably not what you want\r\nINFO: From Compiling external/flatbuffers/src/code_generators.cpp:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option \"-Wno-implicit-fallthrough\"\r\nINFO: From Compiling tensorflow/lite/simple_memory_arena.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option \"-Wno-extern-c-compat\"\r\nINFO: From Compiling external/flatbuffers/src/idl_gen_fbs.cpp:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option \"-Wno-implicit-fallthrough\"\r\nINFO: From Compiling tensorflow/lite/minimal_logging_default.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option \"-Wno-extern-c-compat\"\r\nINFO: From Compiling tensorflow/lite/minimal_logging.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option \"-Wno-extern-c-compat\"\r\nINFO: From Compiling tensorflow/lite/kernels/internal/quantization_util.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nINFO: From Compiling tensorflow/lite/string_util.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\ntensorflow/lite/string_util.cc: In member function 'int tflite::DynamicBuffer::WriteToBuffer(char**)':\r\ntensorflow/lite/string_util.cc:89:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < offset_.size(); i++) {\r\n                     ^\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option \"-Wno-extern-c-compat\"\r\nINFO: From Compiling tensorflow/lite/arena_planner.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\ntensorflow/lite/arena_planner.cc: In member function 'virtual TfLiteStatus tflite::ArenaPlanner::PlanAllocations()':\r\ntensorflow/lite/arena_planner.cc:156:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < graph_info_->num_nodes(); ++i) {\r\n                     ^\r\ntensorflow/lite/arena_planner.cc: In member function 'virtual TfLiteStatus tflite::ArenaPlanner::ExecuteAllocations(int, int)':\r\ntensorflow/lite/arena_planner.cc:196:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < graph_info_->num_tensors(); ++i) {\r\n                     ^\r\ntensorflow/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateAllocationOfInternalTensors(int)':\r\ntensorflow/lite/arena_planner.cc:287:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (node_index < graph_info_->num_nodes()) {\r\n                  ^\r\ntensorflow/lite/arena_planner.cc: In member function 'TfLiteStatus tflite::ArenaPlanner::CalculateDeallocationOfInternalTensors(int)':\r\ntensorflow/lite/arena_planner.cc:300:18: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (node_index < graph_info_->num_nodes()) {\r\n                  ^\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option \"-Wno-extern-c-compat\"\r\nINFO: From Compiling tensorflow/lite/kernels/internal/mfcc_dct.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nINFO: From Compiling tensorflow/lite/kernels/kernel_util.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nINFO: From Compiling tensorflow/lite/kernels/internal/mfcc.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nINFO: From Compiling tensorflow/lite/kernels/internal/reference/portable_tensor_utils.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/flatbuffers/src/idl_gen_text.cpp:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option \"-Wno-implicit-fallthrough\"\r\n[2,657 / 12,575] Compiling external/org_sqlite/sqlite3.c; 4s local ... (5 actions running)\r\nINFO: From Compiling external/flatbuffers/src/reflection.cpp:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option \"-Wno-implicit-fallthrough\"\r\nINFO: From Compiling tensorflow/lite/kernels/internal/mfcc_mel_filterbank.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nINFO: From Compiling tensorflow/core/grappler/costs/robust_stats.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nINFO: From Compiling tensorflow/lite/kernels/internal/spectrogram.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/flatbuffers/src/idl_gen_general.cpp:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option \"-Wno-implicit-fallthrough\"\r\n[2,928 / 12,575] Compiling external/org_sqlite/sqlite3.c; 8s local ... (2 actions running)\r\nINFO: From Compiling external/jsoncpp_git/src/lib_json/json_value.cpp:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/jsoncpp_git/src/lib_json/json_writer.cpp:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nINFO: From Compiling tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nINFO: From Compiling tensorflow/core/lib/db/snapfn.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nERROR: /workspace/tensorflow/lite/python/interpreter_wrapper/BUILD:9:1: C++ compilation of rule '//tensorflow/lite/python/interpreter_wrapper:numpy' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command \r\n  (cd /home/sohaib/tensorflow_pi_compile/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH='' \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  /home/sohaib/tensorflow_pi_compile/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/arm-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -isystem /usr/include/arm-linux-gnueabihf -isystem /usr/include/python3.4 -isystem /usr/include/ -MD -MF bazel-out/armeabi-opt/bin/tensorflow/lite/python/interpreter_wrapper/_objs/numpy/numpy.pic.d '-frandom-seed=bazel-out/armeabi-opt/bin/tensorflow/lite/python/interpreter_wrapper/_objs/numpy/numpy.pic.o' -fPIC -iquote . -iquote bazel-out/armeabi-opt/genfiles -iquote bazel-out/armeabi-opt/bin -iquote external/local_config_python -iquote bazel-out/armeabi-opt/genfiles/external/local_config_python -iquote bazel-out/armeabi-opt/bin/external/local_config_python -iquote external/bazel_tools -iquote bazel-out/armeabi-opt/genfiles/external/bazel_tools -iquote bazel-out/armeabi-opt/bin/external/bazel_tools -isystem external/local_config_python/numpy_include -isystem bazel-out/armeabi-opt/genfiles/external/local_config_python/numpy_include -isystem bazel-out/armeabi-opt/bin/external/local_config_python/numpy_include -isystem external/local_config_python/python_include -isystem bazel-out/armeabi-opt/genfiles/external/local_config_python/python_include -isystem bazel-out/armeabi-opt/bin/external/local_config_python/python_include '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c tensorflow/lite/python/interpreter_wrapper/numpy.cc -o bazel-out/armeabi-opt/bin/tensorflow/lite/python/interpreter_wrapper/_objs/numpy/numpy.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nIn file included from /usr/include/python3.4/Python.h:8:0,\r\n                 from ./tensorflow/lite/python/interpreter_wrapper/numpy.h:49,\r\n                 from tensorflow/lite/python/interpreter_wrapper/numpy.cc:17:\r\n/usr/include/python3.4/pyconfig.h:13:55: fatal error: arm-linux-gnueabihf/python3.4m/pyconfig.h: No such file or directory\r\n #  include <arm-linux-gnueabihf/python3.4m/pyconfig.h>\r\n                                                       ^\r\ncompilation terminated.\r\n[4,900 / 12,575] Compiling external/org_sqlite/sqlite3.c; 12s local ... (6 actions running)\r\nINFO: Elapsed time: 35.627s, Critical Path: 12.95s\r\nINFO: 42 processes: 42 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["same problem, when when cross-compile opencv for raspberry", "any suggestions/updates on this?", "@hamzeah, Have you tried to follow [a guide](https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend#raspbian-stretch) (excluding Intel's IE related stuff)? It describes how to create a docker image for Raspberry Pi cross compilation.", "@dkurt , yes, actually I followed that link 2-3 weeks ago. \r\nBut it generated some errors. I think the instruction is old and part 4.Build should be updated. I tried to modify it, but not successful and ended up with the error on this page!\r\n", "@hamzeah, Have you used docker to compile OpenCV? If you followed the guide step-by-step but faced any problem please don't hesitate to open [an issue](https://github.com/opencv/opencv/issues) providing the error message.", "@dkurt , thanks. Yes, I followed the guide exactly as it is. I opened an issue for that here:\r\nhttps://github.com/opencv/opencv/issues/14280\r\nthank you for following this. I need cross-compile to use OpenCV tracker functions (not available in Openvino-OpenCV)", "@ArifSohaib,\r\nIs this still an issue? Could you please update TensorFlow to v2.3 and check if you are still facing the same error. Thanks!", "I can confirm the error in branches 2.3 and 2.4 in the same terms as @ArifSohaib describes, use \r\n**apt-get install -y libpython2.7-dev:armhf** in _/tensorflow/tensorflow/tools/ci_build/install_pi_python3x_toolchain.sh_ may solve the problem", "Seems the best option is remove Python2 dev with:\r\n**apt-get remove -y libpython2.7-dev**\r\n\r\nOptionally you may need point _/usr/bin/python_ to Python3.7 with something like (simpler than using _alternatives_).\r\n**ln -sf /usr/bin/python3.7 /usr/bin/python**\r\n\r\nIf the process takes more than 30-45' it's better stop it and restart again (not clear the reason) seems sometimes process get stucks\r\n\r\nTested in Windows10 with WSL (changes done in Dockerfile.pi-python3.7)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26947\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26947\">No</a>\n"]}, {"number": 26946, "title": "Clang format changes", "body": "Only clang formatting changes. (Spacing issues)\r\nIssue: Always creates unnecessary diff when we do some changes in these files and does clang-format for formatting.", "comments": []}, {"number": 26945, "title": "ImportError: cannot import name 'calibration_pb2'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO\r\n- OS Platform and Distribution \"Debian GNU/Linux 9 (stretch)\"\r\n- TensorFlow installed from: source\r\n- TensorFlow version: b'unknown' 1.10.0         \r\n- Python version: 3.5.6\r\n- Bazel version: 0.15.2\r\n- GCC/Compiler version gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\n- CUDA/cuDNN version: 10.0.130\r\n- GPU model and memory:\r\nNVIDIA-SMI 410.72       Driver Version: 410.72       CUDA Version: 10.0  \r\nTesla K80 Memory: 11441MiB \r\n\r\nThis issue occours in trainig command.\r\n`python legacy\r\n/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config`\r\n\r\n**Error Code:**\r\n`  File \"legacy/train.py\", line 51, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"/home/tensorflow1/models/research/object_detection/builders/model_builder.py\", line 27, in <mo\r\ndule>\r\n    from object_detection.builders import post_processing_builder\r\n  File \"/home//tensorflow1/models/research/object_detection/builders/post_processing_builder.py\", line \r\n22, in <module>\r\n    from object_detection.protos import post_processing_pb2\r\n  File \"/home/tensorflow1/models/research/object_detection/protos/post_processing_pb2.py\", line 15, in\r\n <module>\r\n    from object_detection.protos import calibration_pb2 as object__detection_dot_protos_dot_calibration__pb2\r\nImportError: cannot import name 'calibration_pb2'`\r\n\r\nThanks in advance!\r\n", "comments": ["Instead of excecuting this command :\r\n\r\n    protoc --python_out=. .\\object_detection\\protos\\anchor_generator.proto .\\object_detection\\protos\\argmax_matcher.proto .\\object_detection\\protos\\bipartite_matcher.proto .\\object_detection\\protos\\box_coder.proto .\\object_detection\\protos\\box_predictor.proto .\\object_detection\\protos\\eval.proto .\\object_detection\\protos\\faster_rcnn.proto .\\object_detection\\protos\\faster_rcnn_box_coder.proto .\\object_detection\\protos\\grid_anchor_generator.proto .\\object_detection\\protos\\hyperparams.proto .\\object_detection\\protos\\image_resizer.proto .\\object_detection\\protos\\input_reader.proto .\\object_detection\\protos\\losses.proto .\\object_detection\\protos\\matcher.proto .\\object_detection\\protos\\mean_stddev_box_coder.proto .\\object_detection\\protos\\model.proto .\\object_detection\\protos\\optimizer.proto .\\object_detection\\protos\\pipeline.proto .\\object_detection\\protos\\post_processing.proto .\\object_detection\\protos\\preprocessor.proto .\\object_detection\\protos\\region_similarity_calculator.proto .\\object_detection\\protos\\square_box_coder.proto .\\object_detection\\protos\\ssd.proto .\\object_detection\\protos\\ssd_anchor_generator.proto .\\object_detection\\protos\\string_int_label_map.proto .\\object_detection\\protos\\train.proto .\\object_detection\\protos\\keypoint_box_coder.proto .\\object_detection\\protos\\multiscale_anchor_generator.proto .\\object_detection\\protos\\graph_rewriter.proto\r\n\r\nYou need to execute this one : \r\n\r\n    protoc --python_out=. .\\object_detection\\protos\\anchor_generator.proto .\\object_detection\\protos\\argmax_matcher.proto .\\object_detection\\protos\\bipartite_matcher.proto .\\object_detection\\protos\\box_coder.proto .\\object_detection\\protos\\box_predictor.proto .\\object_detection\\protos\\eval.proto .\\object_detection\\protos\\faster_rcnn.proto .\\object_detection\\protos\\faster_rcnn_box_coder.proto .\\object_detection\\protos\\grid_anchor_generator.proto .\\object_detection\\protos\\hyperparams.proto .\\object_detection\\protos\\image_resizer.proto .\\object_detection\\protos\\input_reader.proto .\\object_detection\\protos\\losses.proto .\\object_detection\\protos\\matcher.proto .\\object_detection\\protos\\mean_stddev_box_coder.proto .\\object_detection\\protos\\model.proto .\\object_detection\\protos\\optimizer.proto .\\object_detection\\protos\\pipeline.proto .\\object_detection\\protos\\post_processing.proto .\\object_detection\\protos\\preprocessor.proto .\\object_detection\\protos\\region_similarity_calculator.proto .\\object_detection\\protos\\square_box_coder.proto .\\object_detection\\protos\\ssd.proto .\\object_detection\\protos\\ssd_anchor_generator.proto .\\object_detection\\protos\\string_int_label_map.proto .\\object_detection\\protos\\train.proto .\\object_detection\\protos\\keypoint_box_coder.proto .\\object_detection\\protos\\multiscale_anchor_generator.proto .\\object_detection\\protos\\graph_rewriter.proto .\\object_detection\\protos\\calibration.proto", "Its work. Thanks.", "it works thanks a lot", "Thank you very much. Its work. :like", "hi,\r\ncheck this one, run the below command from research path\r\nyou can do manually\r\n\r\nFrom tensorflow/models/research/\r\nexport PYTHONPATH=$PYTHONPATH:pwd:pwd/slim\r\n\r\nprotoc --python_out=. .\\object_detection\\protos\\calibration.proto\r\nprotoc --python_out=. .\\object_detection\\protos\\preprocessor.proto\r\nprotoc --python_out=. .\\object_detection\\protos\\anchor_generator.proto\r\n\r\nand verify below video\r\nhttps://youtu.be/nZUxoHPFf4", "@Rajamohanreddyai  The video is unavailable ", "it works thanks a lot\r\n"]}, {"number": 26944, "title": "Clang format changes", "body": "Only clang formatting changes, always create unnecessary diff when we do some fix in these file and does clang-format.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26944) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot cla signed", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26944) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 26943, "title": "MarkForCompilationPass: don't create clusters with control-flow", "body": "When using while loops, sometimes Tensorflow crashes with the error:\r\n`Found control flow node in clustering worklist: Enter`\r\n\r\nThis is because FindCompilationCandidates() in MarkForCompilationPass doesn't explicitly disallow control-flow ops when checking if an op is valid. Later in RunImpl(), the codes checks if the representative of the cluster is a control-flow op and exits with the above message.\r\n\r\nWhen doing resnet with a tf.while_loop, I had this cluster being created:\r\nrepresentative: resnet_model/block2/Enter\r\nnode: resnet_model/block2/Merge\r\n\r\nThis patch makes FindCompilationCandidates() reject control-flow ops outright. I don't know enough of TF if it fuses clusters with control-flow, but if so, the above assertion must be changed.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26943) for more info**.\n\n<!-- need_sender_cla -->", "@nunoplopes please sign CLA", "Hey Sanjoy! Yes, I've been playing around with XLA :)\r\nThanks for your explanation. I managed to reduce the test case, but it doesn't crash with tensorflow with CPU, just with our backend. The offending op is there in both cases ('gradients/GreaterEqual/Enter'), but somehow it doesn't get considered for clustering.\r\nSo the bug is elsewhere, not where I patched. I'm investigating further. I'll let you know if the problem is in XLA.\r\nThank you!"]}, {"number": 26942, "title": "Add SetDefaultDevice equivalent in c_api", "body": "TensorFlow version: 1.11, 1.12, 1.3\r\n-Are you willing to contribute it (Yes/No): Yes\r\n\r\nIn bazel build for tensorflow_cc.lib, SetDefaultGraph does not work as it has previously. \r\n\r\nMy recommendation would be to add an entry into the c_api so that one can assign a session to a specific GPU.", "comments": []}, {"number": 26941, "title": "[TF2.0] ref types in TF1 vs TF2", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n- TensorFlow installed from (source or binary):Binary\r\n- TensorFlow version (use command below):2.0.0-alpha0 and 1.13.0\r\n- Python version:3.6\r\n\r\n**TensorFlow 1.13.0**\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nprint(tf.__version__)\r\nx = np.random.randn(3, 4)\r\nx_tf = tf.Variable(x)\r\nprint(x_tf.dtype)\r\n```\r\n```python\r\n1.13.1\r\n<dtype: 'float64_ref'>\r\n```\r\n**TensorFlow 2.0.0-alpha0**\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nprint(tf.__version__)\r\nx = np.random.randn(3, 4)\r\nx_tf = tf.Variable(x)\r\nprint(x_tf.dtype)\r\n```\r\n```python\r\n2.0.0-alpha0\r\n<dtype: 'float64'>\r\n```\r\nWhile playing around with TensorFlow 2.0, I noticed that there is a difference in calling ```tf.Variable```.  Just wanted to know if ```_ref``` is being phased out in TF2.0?", "comments": ["Yes, ref is no longer a thing in tf v2."]}, {"number": 26940, "title": "Allocation of ram/memory while training 50 gb of data", "body": "Hello,\r\n\r\nI am training a model and my specs are \r\n\r\nTesla K80 GPU  12 GB ram\r\n61 gb ram physical ram\r\n\r\nmy training data set is of 50 gb\r\n\r\nI want to know that how tensorflow use the memory for loading the data as it gives \"OOM\" error,\r\nwhile loading images/dataset how it load whether loads all the in gpu and doesn't utilize the ram \r\nas while training it uses gpu so does it do half data on ram and half data on gpu, i want to know allocation of memory and how tensorflow does that.\r\n\r\nAnd what if i want to use both gpu and cpu for tensorflow so it doesn't have memory issue so total i will have 61 + 12 gb of ram which can load dataset efficiently.\r\n\r\nCan anyone give me exact details that how tensorflow does the computation and allocation of memory for dataset. \r\n\r\nThanks", "comments": ["@xyzdcgan Please check there are lot of info on latest TF website. Here is link to [distribution_strategy](https://www.tensorflow.org/guide/distribute_strategy) and this [link](https://www.tensorflow.org/guide/using_gpu) might be helpful.\r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26939, "title": "Relu activation optimizations", "body": "", "comments": ["@siju-samuel gentle ping to address comments", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 26937, "title": "Typo in the section \"3. Install the TensorFlow pip package\"", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:\r\n- Doc Link: https://www.tensorflow.org/install/pip\r\n\r\n\r\n**Describe the documentation issue**\r\nIn the \"Install TensorFlow with pip\", section \"3. Install the TensorFlow pip package\", the sentence \"**tensorflow==2.0.0-alpha0-gpu** \u2014Preview TF 2.0 Alpha build with GPU support (unstable, Ubuntu and Windows)\" is wrong.\r\nIt's should \"**tensorflow-gpu==2.0.0-alpha0**\" instead of \"**tensorflow==2.0.0-alpha0-gpu**\"\r\n\r\nBecause I can't install with \"pip install --upgrade tensorflow==2.0.0-alpha0-gpu\"\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Thanks for letting us know! :slightly_smiling_face: \r\n\r\nI made the change, and you can track progress on the PR here:\r\nhttps://github.com/tensorflow/docs/pull/402"]}, {"number": 26936, "title": "Lite: Slice Op refactored", "body": "Remove unnecessary reversal of input dimension", "comments": ["@jianlijianli : Gentle Reminder!", "@jianlijianli : Gentle Reminder!", "@jianlijianli : Gentle Reminder!", "@jianlijianli : Gentle Reminder!", "Can one of the admins verify this patch?", "Could you please add a high level description what this CL does? Thanks.", "@ANSHUMAN87 Could you please address the reviewer comments and resolve conflicts? Thanks!", "> Could you please add a high level description what this CL does? Thanks.\r\n\r\nUpdated!", "@jianlijianli: Gentle reminder! ", "@gbaned: Gentle reminder! Please help merge the PR! ", "@ANSHUMAN87 Can you please check @rthadur's comments and keep us posted. Thanks!", "@gbaned , @rthadur : It is strange i am unable to reproduce the errors as you mentioned.", "@gbaned , @rthadur : The failure is resolved now, Thanks!", "@gbaned , @rthadur : I do not think there is anything left for this PR as @jdduke : already approved this PR earlier. Can you please expedite and merge this PR. Thanks!", "@ANSHUMAN87 sorry for the delay , will try to pull this again , thank you ", "@ANSHUMAN87 getting below error , can you please check \r\n`/tensorflow/lite/kernels/test_util.cc:317\r\nExpected equality of these values:\r\n  CountPartitionsDelegatedTo(interpreter_.get(), TestNnApiDelegate())\r\n    Which is: 0\r\n  1\r\nExpecting operation to be accelerated but cannot find a partition associated to the NNAPI delegate`", "> @ANSHUMAN87 getting below error , can you please check\r\n> `/tensorflow/lite/kernels/test_util.cc:317 Expected equality of these values: CountPartitionsDelegatedTo(interpreter_.get(), TestNnApiDelegate()) Which is: 0 1 Expecting operation to be accelerated but cannot find a partition associated to the NNAPI delegate`\r\n\r\n@rthadur : Sorry for late response! The error you posted does not seem to be relevant to my change! Please let me know, if anything needs to be done from my end. Thanks!", "@jianlijianli @jdduke  can you please help with internal failures ?", "@ANSHUMAN87 Can you please check @jdduke's comments and keep us posted. Thanks!", "@gbaned : Thanks for tracking it! I have updated based on @jdduke comment for the internal failure reported by @rthadur .\r\nCan you please approve it, and check for merge, as the new change is only to disable the test cases ? TIA!", "Thanks @jdduke, @rthadur, @gbaned!"]}, {"number": 26935, "title": "how to edit kernel weight at certain global_step using estimator", "body": "i want to set some weights to zero at certation global step. ", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}]