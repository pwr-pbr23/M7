[{"number": 13411, "title": "Branch 170208694", "body": "", "comments": []}, {"number": 13410, "title": "Branch 170207994", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 13409, "title": "R0.12", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@RyujinKaryu can you take a look at the merge conflicts"]}, {"number": 13408, "title": "Branch 170213262", "body": "", "comments": []}, {"number": 13407, "title": "Branch 170207579", "body": "", "comments": []}, {"number": 13406, "title": "Changed hyperlinks from http to https", "body": "Change links in \"Windows CPU-only:\", \"Windows GPU:\" and \"Android:\" https.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Change links in \"Windows CPU-only:\", \"Windows GPU:\" and \"Android:\" https.", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 13405, "title": "tensorflow.contrib.data.Dataset", "body": "Issue with tensorflow.contrib.data module \r\n\r\n![screenshot from 2017-09-30 10-25-36](https://user-images.githubusercontent.com/13569817/31042660-9b2323a6-a5ca-11e7-88f9-2960cd7439d2.png)\r\n", "comments": ["What version of TF are you using? Dataset is introduced in 1.3.0.", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 13404, "title": "Branch 170216177", "body": "", "comments": []}, {"number": 13403, "title": "tensorflow multi -GPU lstm :ValueError: None values not supported.", "body": "I am trying to implement a Multi-GPU LSTM Using the [muti-gpu cifar10 ](https://github.com/normanheckscher/mnist-multi-gpu/blob/master/mnist_multi_gpu_batching_train.py)\r\nMy code is present [here ](https://github.com/AbdalaDiasse/ancun/blob/master/multi_gpu_lstm.py)\r\nHow ever when i run the code I got the following issues:\r\n\r\n`Traceback (most recent call last):\r\n  File \"multi_gpu_lstm.py\", line 227, in <module>\r\n    tf.app.run()\r\n  File \"/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"multi_gpu_lstm.py\", line 224, in main\r\n    training()\r\n  File \"multi_gpu_lstm.py\", line 164, in training\r\n    tower_grads_avg = average_gradients(tower_grads)\r\n  File \"multi_gpu_lstm.py\", line 105, in average_gradients\r\n    expanded_g = tf.expand_dims(g, 0)\r\n  File \"/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 168, in expand_dims\r\n    return gen_array_ops._expand_dims(input, axis, name)\r\n  File \"/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1051, in _expand_dims\r\n    result = _op_def_lib.apply_op(\"ExpandDims\", input=input, dim=dim, name=name)\r\n  File \"/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 504, in apply_op\r\n    values, as_ref=input_arg.is_ref).dtype.name\r\n  File \"/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 702, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 110, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 99, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 360, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n`\r\nThe problem is raised at this line:\r\n`expanded_g = tf.expand_dims(g, 0)`\r\nit's because g gets None values . if I modify to:\r\n`if g is not None:\r\n    expanded_g = tf.expand_dims(g, 0)`\r\neverything work very fine. After investigation I have realized grads get None value at this line :\r\n`grads = optimizer.compute_gradients(loss_op)`\r\nWhen I print grads this is what I got : \r\n`[(<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/MatMul_grad/tuple/control_dependency_1:0' shape=(28, 1) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac80ff32110>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/MatMul_1_grad/tuple/control_dependency_1:0' shape=(1, 1) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac810ad1f90>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/add_grad/tuple/control_dependency_1:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac810ad19d0>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/add_1_grad/tuple/control_dependency_1:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac810ae9d50>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(2, 4) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814428c50>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(4,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814438bd0>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/mul/Enter_grad/b_acc_3:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814446650>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/mul_2/Enter_grad/b_acc_3:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814446690>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/mul_4/Enter_grad/b_acc_3:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac8144466d0>)]\r\n  [(None, <tensorflow.python.ops.variables.Variable object at 0x2ac80ff32110>), (None, <tensorflow.python.ops.variables.Variable object at 0x2ac810ad1f90>), (None, <tensorflow.python.ops.variables.Variable object at 0x2ac810ad19d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x2ac810ae9d50>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(2, 4) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814428c50>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(4,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814438bd0>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/mul/Enter_grad/b_acc_3:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814446650>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/mul_2/Enter_grad/b_acc_3:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814446690>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/mul_4/Enter_grad/b_acc_3:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac8144466d0>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/MatMul_grad/tuple/control_dependency_1:0' shape=(28, 1) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac80ff32150>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/MatMul_1_grad/tuple/control_dependency_1:0' shape=(1, 1) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac8152d9b90>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/add_grad/tuple/control_dependency_1:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac8152d9c50>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/add_1_grad/tuple/control_dependency_1:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac8152fd090>)]`\r\n\r\nWe can clearly see that some None value from grads\r\n\r\nI don't know why but it seems to be very strange to be .I think its probably related to some variable that are wrongly set. PLEASE HELP ME TO FIND OUT \r\n\r\n", "comments": ["i don't know if i still remember but, last time i got this error is when i was adding an non differentiable variable into my graph, i think", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nA gradient is None is there is no connection from the loss to the Variable, or if the connection is through an op that doesn't have gradients. Ask on StackOverflow if you have more questions."]}, {"number": 13402, "title": "Allow `tfexample_decoder.BoundingBox` to be created from dense tensor", "body": "Modife the `.tensor_to_items()` method on the `BoundingBox` so that it\r\ncan be created from dense tensors, as well as sparse tensors (which are\r\ncurrently required).", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "@tensorflow-jenkins test this please", "Test failure unrelated to code change. "]}, {"number": 13401, "title": "Branch 170200011", "body": "", "comments": []}, {"number": 13400, "title": "Add `tfexample_decoder.Image` support for `shape_keys`", "body": "Allow for the shape of an image decoded with the\r\n`tfexample_decoder.Image` decoder to be determined by fields in the\r\nserialized example being decoded. The functionality mirrors the use of\r\nthe `shape_keys` kwarg of the `Tensor` decoder.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please", "@flacjacket any update on this?", "@flacjacket any updates?", "@rmlarsen Sorry for taking so long to get back to this. I have updated the tests to include 2d image tensors. However, it looks like the TF Test Suite check has been stalled overnight.", "@tensorflow-jenkins test this please", "@flacjacket Could you take a look at the failing tests:\r\n\r\n```\r\nFAIL: //tensorflow/contrib/slim/python/slim/data:tfexample_decoder_test (see /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test/test.log).\r\nINFO: From Testing //tensorflow/contrib/slim/python/slim/data:tfexample_decoder_test:\r\n==================== Test output for //tensorflow/contrib/slim/python/slim/data:tfexample_decoder_test:\r\n2017-11-14 00:39:20.997015: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n............E.....EE..........\r\n======================================================================\r\nERROR: testDecodeExampleWithNoShapeInfo (__main__.TFExampleDecoderTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.runfiles/org_tensorflow/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py\", line 169, in testDecodeExampleWithNoShapeInfo\r\n    image_format='jpeg', image_shape=image_shape)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.runfiles/org_tensorflow/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py\", line 93, in GenerateImage\r\n    num_pixels = image_shape[0] * image_shape[1] * image_shape[2]\r\nIndexError: tuple index out of range\r\n\r\n======================================================================\r\nERROR: testDecodeExampleWithShapeKeys (__main__.TFExampleDecoderTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.runfiles/org_tensorflow/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py\", line 191, in testDecodeExampleWithShapeKeys\r\n    image_format='jpeg', image_shape=image_shape)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.runfiles/org_tensorflow/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py\", line 93, in GenerateImage\r\n    num_pixels = image_shape[0] * image_shape[1] * image_shape[2]\r\nIndexError: tuple index out of range\r\n\r\n======================================================================\r\nERROR: testDecodeExampleWithShapeKeysList (__main__.TFExampleDecoderTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.runfiles/org_tensorflow/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py\", line 208, in testDecodeExampleWithShapeKeysList\r\n    image_format='jpeg', image_shape=image_shape)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.runfiles/org_tensorflow/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py\", line 93, in GenerateImage\r\n    num_pixels = image_shape[0] * image_shape[1] * image_shape[2]\r\nIndexError: tuple index out of range\r\n\r\n----------------------------------------------------------------------\r\nRan 30 tests in 3.924s\r\n\r\nFAILED (errors=3)\r\n================================================================================```", "Ping @flacjacket", "@flacjacket any luck with this?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing for now. Please re-open once you have an update."]}, {"number": 13399, "title": "Fix the Docker GPU build (adds a symlink + library path)", "body": "I was running a slightly modified build script the last time this passed, so you may want to test it again. ", "comments": ["Hello,\r\n\r\nI don't understand what this PR is solving. You shouldn't add the stubs to your `LD_LIBRARY_PATH`, you don't want to use them at runtime, just at build time.", "We were able to work around a bazel bug with this PR.\r\n@allenlavoie has more context.", "Some recent changes mean that we're linking in GPU code when generating op wrappers (while building).\r\n\r\nThe tools for Bazel genrules depend on libtensorflow_framework.so, and that depends on CUDA libraries when building with CUDA. It's likely avoidable by not linking libtensorflow_framework.so for these genrules, and instead relying on the cc_library rules directly.\r\n\r\nSo we need a stub to satisfy that (trivial/incidental) dependency while building. But you're right that we should avoid putting stubs in LD_LIBRARY_PATH in the final image. I'll fix it in another PR.", "Ok. I would suggest adding the fixups around the code that require them, e.g. something like that:\r\n```Dockerfile\r\nRUN export LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:$LD_LIBRARY_PATH && \\\r\n    ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 && \\\r\n    bazel [...] && \\\r\n    rm /usr/local/cuda/lib64/stubs/libcuda.so.1\r\n```\r\nThanks!"]}, {"number": 13398, "title": "Allow LMDB to be opened by multiple readers simultaneously", "body": "This PR is basically a one-line change by adding MDB_NOLOCK flag. Since LMDBReader is read-only, we don't need locks. LMDB by default creates a lock file in the same folder which prohibits the database to be opened by others.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "I have some difficulties at understanding the failures.\r\n* Linux GPU: failed at several tests irrelevant to what's being modified.\r\n//tensorflow/contrib/signal:window_ops_test\r\n//tensorflow/examples/adding_an_op:cuda_op_test\r\n//tensorflow/python:adam_test\r\n//tensorflow/python/kernel_tests:norm_op_test\r\n* Linux CPU Tests: can someone help me get the log?\r\n//tensorflow/python/kernel_tests:reader_ops_test\r\n`/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/testlogs/tensorflow/python/kernel_tests/reader_ops_test/test.log`\r\n", "@jhseu is there anything I can do to get it merged?", "@caisq can you kick off the Jenkins test?", "@tensorflow-jenkins test this please", "@caisq Can we kick off another jenkins test?\r\nI just merged the latest master. Hope the failures are resolved in master branch already.", "@tensorflow-jenkins test this please", "Jenkins, test this please.", "Jenkins, test this please.", "Hi,\r\n\r\nThe function mdb_env_setup_locks in mdb.c is not thread-safe. \r\n```c\r\n\t\tif (!mdb_sec_inited) {\r\n\t\t\tInitializeSecurityDescriptor(&mdb_null_sd,\r\n\t\t\t\tSECURITY_DESCRIPTOR_REVISION);\r\n\t\t\tSetSecurityDescriptorDacl(&mdb_null_sd, TRUE, 0, FALSE);\r\n\t\t\tmdb_all_sa.nLength = sizeof(SECURITY_ATTRIBUTES);\r\n\t\t\tmdb_all_sa.bInheritHandle = FALSE;\r\n\t\t\tmdb_all_sa.lpSecurityDescriptor = &mdb_null_sd;\r\n\t\t\tmdb_sec_inited = 1;\r\n\t\t}\r\n```\r\nmdb_sec_inited is a global var.\r\n\r\n\r\nI'm not sure if this function will be called, but, let's double-check it.\r\n\r\n\r\n"]}, {"number": 13397, "title": "tf.read_file isn't in the informed folder", "body": "the documentation below:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/read_file\r\n\r\nSays that this function is here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/r1.3/tensorflow/python/ops\r\n\r\nBut isn't .\r\n\r\n------------------------\r\n\r\nThanks,", "comments": ["Hi, @Silve1ra . \r\nDo you mean why `gen_io_ops.py` is missing? \r\nYes, `gen_` prefix indicates those files are generated automatically from files (similar location in `core`) in  c++ side. Hence, `read_file` is at [tensorflow/core/ops/io_ops](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/ops/io_ops.cc) in fact.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/107cc777af7880c140d089e44ad898a6ba929286/tensorflow/core/ops/io_ops.cc#L824-L830\r\n\r\nBy the way, perhaps this is useful for understanding: [Adding a New Op](https://www.tensorflow.org/extend/adding_an_op)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 13396, "title": "Fix LMDBReader crash due to not fully cleanup", "body": "This PR fixed a bug in LMDBReader cleanup. Some pointers were not reset and caused corruption when the LMDBReader opens the next file.\r\nA test case is constructed to verify the case.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "The failure looks weird:\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/7119/console\r\nCan someone help me take a look?", "@jhseu is there anything I can do to get it merged?", "Hi @jhseu , can we merge this PR?", "@caisq is on our rotation and can merge it when he has time.", "@tensorflow-jenkins test this please", "The CI tests seem quite weird and unrelated...", "@tensorflow-jenkins test this please", "Why does it fail so strangely? Can someone provide some hint?", "I just merged the latest master. Hope the failures are resolved in master branch already. Can we kick off a jenkins test again?", "@caisq Would you mind kicking off a CI test?", "@jhseu can you launch a jenkins test?", "Jenkins, test this please", "@jhseu Thanks for starting the test! Do you have any idea why \"Ubuntu CC\" is still pending?", "I found the following error from CI:\r\n```\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/contrib/image/BUILD:192:1: Couldn't build file tensorflow/contrib/image/gen_single_image_random_dot_stereograms_ops_py_wrappers_cc: \r\nLinking of rule '//tensorflow/contrib/image:gen_single_image_random_dot_stereograms_ops_py_wrappers_cc' failed (Exit 127): clang failed: error executing command\r\n```\r\nIt doesn't seem to have anything to do with my changes.\r\nWhy is CI so fragile and fails for unrelated reasons?\r\nCan anyone help me just get this five-line bug fix in?\r\n@jhseu @caisq ", "@caisq @jhseu Could anyone can help to let user get the merged branch? So that we can use LMDB as TFRecords. \r\n\r\nThanks"]}, {"number": 13395, "title": "Branch 170226583", "body": "", "comments": []}, {"number": 13394, "title": "Change tmp filename behavior in contrib.ffmpeg to support simultaneous decodes", "body": "Encountered ill-defined behavior when trying to decode two audio files in a single call to `sess.run`. Changed `GetTempFilename` to use `mkstemps` instead of `getpid`. Old behavior demonstrated below:\r\n\r\n### Source code\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef load_audio(wav_fp):\r\n  wav_bin = tf.read_file(wav_fp)\r\n  wav = tf.contrib.ffmpeg.decode_audio(\r\n      wav_bin,\r\n      file_format='wav',\r\n      samples_per_second=16000,\r\n      channel_count=1)\r\n  return wav\r\n\r\nwav_fp0 = tf.placeholder(tf.string, [])\r\nwav_fp1 = tf.placeholder(tf.string, [])\r\n\r\nwav0 = load_audio(wav_fp0)\r\nwav1 = load_audio(wav_fp1)\r\n\r\nwith tf.Session() as sess:\r\n  _wav_fp0 = 'test0.wav'\r\n  _wav_fp1 = 'test1.wav'\r\n\r\n  _wav0, _wav1 = sess.run(\r\n      [wav0, wav1],\r\n      feed_dict={\r\n        wav_fp0: _wav_fp0,\r\n        wav_fp1: _wav_fp1})\r\n\r\n  print _wav0.shape\r\n  print _wav1.shape\r\n```\r\n\r\n### Race condition outcome 1 (second file write completes before either decode)\r\n```sh\r\n2017-09-29 16:02:23.435427: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-29 16:02:23.435459: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nffmpeg version 2.8.11-0ubuntu0.16.04.1 Copyright (c) 2000-2017 the FFmpeg developers\r\n  built with gcc 5.4.0 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 20160609\r\n  configuration: --prefix=/usr --extra-version=0ubuntu0.16.04.1 --build-suffix=-ffmpeg --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --cc=cc --cxx=g++ --enable-gpl --enable-shared --disable-stripping --disable-decoder=libopenjpeg --disable-decoder=libschroedinger --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzvbi --enable-openal --enable-opengl --enable-x11grab --enable-libdc1394 --enable-libiec61883 --enable-libzmq --enable-frei0r --enable-libx264 --enable-libopencv\r\n  libavutil      54. 31.100 / 54. 31.100\r\n  libavcodec     56. 60.100 / 56. 60.100\r\n  libavformat    56. 40.101 / 56. 40.101\r\n  libavdevice    56.  4.100 / 56.  4.100\r\n  libavfilter     5. 40.101 /  5. 40.101\r\n  libavresample   2.  1.  0 /  2.  1.  0\r\n  libswscale      3.  1.101 /  3.  1.101\r\n  libswresample   1.  2.101 /  1.  2.101\r\n  libpostproc    53.  3.100 / 53.  3.100\r\nffmpeg version 2.8.11-0ubuntu0.16.04.1 Copyright (c) 2000-2017 the FFmpeg developers\r\n  built with gcc 5.4.0 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 20160609\r\n  configuration: --prefix=/usr --extra-version=0ubuntu0.16.04.1 --build-suffix=-ffmpeg --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --cc=cc --cxx=g++ --enable-gpl --enable-shared --disable-stripping --disable-decoder=libopenjpeg --disable-decoder=libschroedinger --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzvbi --enable-openal --enable-opengl --enable-x11grab --enable-libdc1394 --enable-libiec61883 --enable-libzmq --enable-frei0r --enable-libx264 --enable-libopencv\r\n  libavutil      54. 31.100 / 54. 31.100\r\n  libavcodec     56. 60.100 / 56. 60.100\r\n  libavformat    56. 40.101 / 56. 40.101\r\n  libavdevice    56.  4.100 / 56.  4.100\r\n  libavfilter     5. 40.101 /  5. 40.101\r\n  libavresample   2.  1.  0 /  2.  1.  0\r\n  libswscale      3.  1.101 /  3.  1.101\r\n  libswresample   1.  2.101 /  1.  2.101\r\n  libpostproc    53.  3.100 / 53.  3.100\r\nGuessed Channel Layout for  Input Stream #0.0 : mono\r\nInput #0, wav, from '/tmp/tmp_file_23229.wav':\r\n  Duration: 00:00:02.23, bitrate: 265 kb/s\r\n    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, 1 channels, s16, 256 kb/s\r\nOutput #0, s16le, to '/tmp/tmp_file_23229.raw':\r\n  Metadata:\r\n    encoder         : Lavf56.40.101\r\n    Stream #0:0: Audio: pcm_s16le, 16000 Hz, mono, s16, 256 kb/s\r\n    Metadata:\r\n      encoder         : Lavc56.60.100 pcm_s16le\r\nStream mapping:\r\n  Stream #0:0 -> #0:0 (pcm_s16le (native) -> pcm_s16le (native))\r\nsize=      70kB time=00:00:02.22 bitrate= 256.0kbits/s    \r\nvideo:0kB audio:70kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%\r\nGuessed Channel Layout for  Input Stream #0.0 : mono\r\nInput #0, wav, from '/tmp/tmp_file_23229.wav':\r\n  Duration: 00:00:02.23, bitrate: 265 kb/s\r\n    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, 1 channels, s16, 256 kb/s\r\nOutput #0, s16le, to '/tmp/tmp_file_23229.raw':\r\n  Metadata:\r\n    encoder         : Lavf56.40.101\r\n    Stream #0:0: Audio: pcm_s16le, 16000 Hz, mono, s16, 256 kb/s\r\n    Metadata:\r\n      encoder         : Lavc56.60.100 pcm_s16le\r\nStream mapping:\r\n  Stream #0:0 -> #0:0 (pcm_s16le (native) -> pcm_s16le (native))\r\nsize=      70kB time=00:00:02.22 bitrate= 256.0kbits/s    \r\nvideo:0kB audio:70kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%\r\n(35636, 1)\r\n(35636, 1)\r\n```\r\n\r\n### Race condition outcome 2 (file overwritten during decoding)\r\n```sh\r\n2017-09-29 16:02:18.596598: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-29 16:02:18.596631: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nffmpeg version 2.8.11-0ubuntu0.16.04.1 Copyright (c) 2000-2017 the FFmpeg developers\r\n  built with gcc 5.4.0 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 20160609\r\n  configuration: --prefix=/usr --extra-version=0ubuntu0.16.04.1 --build-suffix=-ffmpeg --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --cc=cc --cxx=g++ --enable-gpl --enable-shared --disable-stripping --disable-decoder=libopenjpeg --disable-decoder=libschroedinger --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzvbi --enable-openal --enable-opengl --enable-x11grab --enable-libdc1394 --enable-libiec61883 --enable-libzmq --enable-frei0r --enable-libx264 --enable-libopencv\r\n  libavutil      54. 31.100 / 54. 31.100\r\n  libavcodec     56. 60.100 / 56. 60.100\r\n  libavformat    56. 40.101 / 56. 40.101\r\n  libavdevice    56.  4.100 / 56.  4.100\r\n  libavfilter     5. 40.101 /  5. 40.101\r\n  libavresample   2.  1.  0 /  2.  1.  0\r\n  libswscale      3.  1.101 /  3.  1.101\r\n  libswresample   1.  2.101 /  1.  2.101\r\n  libpostproc    53.  3.100 / 53.  3.100\r\nffmpeg version 2.8.11-0ubuntu0.16.04.1 Copyright (c) 2000-2017 the FFmpeg developers\r\n  built with gcc 5.4.0 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 20160609\r\n  configuration: --prefix=/usr --extra-version=0ubuntu0.16.04.1 --build-suffix=-ffmpeg --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --cc=cc --cxx=g++ --enable-gpl --enable-shared --disable-stripping --disable-decoder=libopenjpeg --disable-decoder=libschroedinger --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzvbi --enable-openal --enable-opengl --enable-x11grab --enable-libdc1394 --enable-libiec61883 --enable-libzmq --enable-frei0r --enable-libx264 --enable-libopencv\r\n  libavutil      54. 31.100 / 54. 31.100\r\n  libavcodec     56. 60.100 / 56. 60.100\r\n  libavformat    56. 40.101 / 56. 40.101\r\n  libavdevice    56.  4.100 / 56.  4.100\r\n  libavfilter     5. 40.101 /  5. 40.101\r\n  libavresample   2.  1.  0 /  2.  1.  0\r\n  libswscale      3.  1.101 /  3.  1.101\r\n  libswresample   1.  2.101 /  1.  2.101\r\n  libpostproc    53.  3.100 / 53.  3.100\r\nGuessed Channel Layout for  Input Stream #0.0 : mono\r\nInput #0, wav, from '/tmp/tmp_file_23205.wav':\r\n  Duration: 00:00:02.31, bitrate: 256 kb/s\r\n    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, 1 channels, s16, 256 kb/s\r\nOutput #0, s16le, to '/tmp/tmp_file_23205.raw':\r\n  Metadata:\r\n    encoder         : Lavf56.40.101\r\n    Stream #0:0: Audio: pcm_s16le, 16000 Hz, mono, s16, 256 kb/s\r\n    Metadata:\r\n      encoder         : Lavc56.60.100 pcm_s16le\r\nStream mapping:\r\n  Stream #0:0 -> #0:0 (pcm_s16le (native) -> pcm_s16le (native))\r\nsize=      72kB time=00:00:02.31 bitrate= 256.0kbits/s    \r\nvideo:0kB audio:72kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%\r\nGuessed Channel Layout for  Input Stream #0.0 : mono\r\nInput #0, wav, from '/tmp/tmp_file_23205.wav':\r\n  Duration: 00:00:02.31, bitrate: 256 kb/s\r\n    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, 1 channels, s16, 256 kb/s\r\nOutput #0, s16le, to '/tmp/tmp_file_23205.raw':\r\n  Metadata:\r\n    encoder         : Lavf56.40.101\r\n    Stream #0:0: Audio: pcm_s16le, 16000 Hz, mono, s16, 256 kb/s\r\n    Metadata:\r\n      encoder         : Lavc56.60.100 pcm_s16le\r\nStream mapping:\r\n  Stream #0:0 -> #0:0 (pcm_s16le (native) -> pcm_s16le (native))\r\nsize=      72kB time=00:00:02.31 bitrate= 256.0kbits/s    \r\nvideo:0kB audio:72kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%\r\n2017-09-29 16:02:18.637254: F tensorflow/contrib/ffmpeg/default/ffmpeg_lib.cc:116] Non-OK-status: ReadFileToString(Env::Default(), filename, &raw_data) status: Not found: /tmp/tmp_file_23205.rawCould not read FFmpeg output file: /tmp/tmp_file_23205.raw\r\nAborted (core dumped)\r\n```", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Fixed my usage of `mkstemps`", "Jenkins, test this please."]}, {"number": 13393, "title": "Branch 170136839", "body": "", "comments": ["Jenkins, test this please\r\n", "@jhseu PTAL"]}, {"number": 13392, "title": "Branch 170099782", "body": "", "comments": []}, {"number": 13391, "title": "Branch 170050380", "body": "", "comments": ["Jenkins, test this please"]}, {"number": 13390, "title": "Branch 170073555", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 13389, "title": "Branch 170086044", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 13388, "title": "Add fp16 support to fused batchnorm op", "body": "Attention @zheng-xq \r\n\r\n- This commit adds a mixed-precision fused_batch_norm_v2 op.\r\n  The inputs and outputs are fp16, while the scale, offset, mean\r\n  and variance are kept in fp32.\r\n- The tf.nn.fused_batch_norm op has been modified to use the v2\r\n  fused batchnorm whenever inputs are fp16 (this does not affect\r\n  compatibility because fp16 was not previously supported).\r\n- The high-level layers API has also been updated to store the\r\n  scale, offset, mean and variance variables as fp32.", "comments": ["Can one of the admins verify this patch?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Looks like some sanity checks failed\r\n\r\nFAIL: Found 2 non-whitelited pylint errors:\r\ntensorflow/python/layers/normalization.py:246: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 6\r\n\r\ntensorflow/python/layers/normalization.py:248: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 6", "Fixed.", "Jenkins, test this please.", "CLA bot seems to be confused with multiple commits again. As all the CLAs seem to be signed, I can merge once the tests have passed.", "Jenkins, test this please.", "Looks like api compatibility test has detected a change in the public TF python API.\r\n```\r\nERROR:tensorflow:1 differences found between API and golden.\r\nERROR:tensorflow:Issue 1\t: Change detected in python object: tensorflow.layers.BatchNormalization.\r\n```\r\n\r\nCould you confirm the change is intended, run the test in a python2 + linux setup (you may use our devel docker images) and pack the change into this pull request?\r\nThen we will need an API review before we can merge this PR.", "The API change was actually unnecessary, as dtype can be passed to the underlying base.Layer in kwargs. The current PR should pass the api compatibility test.", "Thanks for checking. Could you also look into the merge conflicts before I trigger tests?", "The conflict has been resolved.", "Jenkins, test this please."]}, {"number": 13387, "title": "ERROR: Failed to import the TensorFlow module.", "body": "I was trying to install tensorflow-gpu. Using the tensorflow self check, it says all the required DLLs are present, but I still have an import error. I get the same error if I try to import numpy. There seems to be an issue with my conda command as well. Any help is greatly appreciated to resolve this issue. Thanks!\r\n\r\nPS C:\\Users\\andrew\\Documents> python tensorflow_self_check.py\r\nERROR: Failed to import the TensorFlow module.\r\n\r\n- Python version is 3.6.\r\n\r\n- TensorFlow is installed at: C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow\r\n\r\n- All required DLLs appear to be present. Please open an issue on the\r\n  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues\r\n\r\nPS C:\\Users\\andrew\\Documents> conda create -n tensorflow_gpu python=3.6\r\nusage: conda [-h]\r\n             {keygen,sign,unsign,verify,unpack,install,install-scripts,convert,version,help}\r\n             ...\r\nconda: error: invalid choice: 'create' (choose from 'keygen', 'sign', 'unsign', 'verify', 'unpack', 'install', 'install-\r\nscripts', 'convert', 'version', 'help')\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 13386, "title": "Branch 170517511", "body": "", "comments": ["Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 13385, "title": "Branch 170096704", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 13384, "title": "Tensorflow - Error: Cannot convert value dtype('<f4') to a TensorFlow DType", "body": "I receive an error when testing a Neural Style Transformation project with Tensorflow & OpenCV using the \"lion\" test image. The source of test project is:  [ [https://github.com/cysmith/neural-style-tf](https://github.com/cysmith/neural-style-tf) ]. Both Tensorflow and OpenCV packages were installed and compiled from source. The training model used is: imagenet-vgg-verydeep-19.mat and the runtime environment runs on a s390x CPU and does not use any CUDA support. The  current Python environment is 2.7 under Docker Linux Ubuntu 16.\r\n\r\nRun command:\r\n`bash stylize_image.sh ./image_input/lion.jpg ./styles/kandinsky.jpg`\r\n\r\nThe error message at log is:\r\n```\r\n File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/dtypes.py\", line 584, in as_dtype\r\n    \"Cannot convert value %r to a TensorFlow DType.\" % type_value)\r\nTypeError: Cannot convert value dtype('<f4') to a TensorFlow DType.\r\n```\r\n\r\nVersions installed: \r\n\r\n```\r\npython 2.7.12\r\nimport tensorflow as tf \r\n>>> print tf.VERSION\r\n1.3.1\r\nimport cv2 \r\n>>> print cv2.__version__ \r\n3.3.0\r\n>>> print np.__version__\r\n1.13.2\r\n\r\n```\r\nIs this a known issue? \r\n\r\n", "comments": ["This looks like an issue with neural-style-tf project (probably was developed for older version of tf)", "@claudefalbriard Perhaps `<f4` is forgot to be converted for neural-style-tf project.\r\n\r\n```python\r\nIn [8]: import tensorflow as tf\r\n\r\nIn [9]: import numpy as np\r\n\r\nIn [10]: tf.as_dtype(np.dtype(\"<f4\"))\r\nOut[10]: tf.float32\r\n```", "Closing to keep issues focused on  TensorFlow-specific bugs", "Thanks for the hints and orientation. The \"<\" symbol describes an array in \"Little Endian\" notation, while the runtime platform is a \"Big Endian\" CPU. The issue seems to point to the dtypes.py module in the tensorflow framework folder, which might not support it. I will check the demo project, or model file and see if its possible to adjust the data through a manual Numpy byte swapping, or TF casting function.  By adding the <f4 value to the _NP_TO_TF translation table, I was able to correct the run-time exception, but got an erroneous result in the output file, likely explained by the missing endianness adjustment.    "]}, {"number": 13383, "title": "Branch 170349499", "body": "", "comments": ["Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 13382, "title": "Fix of issue #13164", "body": "Fixes #13164.\r\n\r\ntf.gather and tf.gather_nd now support int32 and int64 ref tensors when running on GPU. \r\ntf.scatter_nd now supports int32 ref tensors when running on GPU. int64 is not supported as some CudaAtomic operations are not supported.\r\nThe tests have been updated.\r\nThe fix was not tested with SYCL.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "I've added my alternative email to both github account and google/cla.", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "There are some important reasons for why many ops tie their int32 calculations to CPU (host-pinned) only.  I cannot speak for scatter/gather, but my guess is this may have some very negative performance impact.  Will assign to Xiaoqiang.  He'll be back from leave in ~a week and should have final say.", "Regarding scatter_nd/gather_nd, I believe I recently pushed a change that adds int32 and int64 support on GPU.", "@ebrevdo The master branch on tensorflow doesn't have int32/int64 support yet: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gather_functor_gpu.cu.cc#L34\r\n\r\nAfter going through the tests I've discovered unrelated bug: \r\ngather/gather_nd operations that have bad indices raise errors on CPU, but on GPU they silently fetch zeros as output. This is mentioned in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gather_functor_gpu.cu.h#L108\r\nSo, I had to split the corresponding tests to separately run on CPU and GPU, and disabled the GPU tests for now. \r\n", "Yes; gather/gather_nd fetch zeros for invalid indices because it's hard / expensive to send an error message back to the CPU.\r\n\r\nRegarding gather/scatter, I did not modify those to add int32/int64.  only gather_nd/scatter_nd [those functors are in separate files].", "@ebrevdo I'm working with the master branch, int32/int64 are not supported:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gather_nd_op_gpu.cu.cc#L120\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gather_nd_op.cc#L227\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc#L150\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/scatter_nd_op.cc#L487\r\n", "I see it. I'm ok with changes to the _nd versions.\n\nOn Oct 1, 2017 3:16 AM, \"Daniyar Turmukhambetov\" <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> I'm working with the master branch,\n> int32/int64 are not supported:\n>\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/core/kernels/gather_nd_op_gpu.cu.cc#L120\n>\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/core/kernels/gather_nd_op.cc#L227\n>\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc#L150\n>\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/core/kernels/scatter_nd_op.cc#L487\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13382#issuecomment-333366749>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim84A8BePEP1fuPRTzQNUKnsCEEhxks5sn2ZlgaJpZM4Po5H8>\n> .\n>\n", "@frankchn Could you \"Jenkins, test this please.\", please? Thanks!", "Jenkins, test this please.", "It's been 4 weeks now.", "@zheng-xq, @ebrevdo, could you reply to comments?", "@josh11b can you take a look?", "@dantkz still seems to be running into merge conflicts\r\n", "@dantkz could you pull rebase and push again?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I thought I'd help with de-conflicting the changes. See #16368 for that.", "@Androbin\r\nThanks for the patch. Unfortunately I had issue with \"tf.int64\". TF would thrown an error if you wanted to minimize through tf.scatter_nd operation. \r\nBasically, computing a tf.gradient() for a computation graph that has a tf.scatter_nd op with tf.int64 indices throws an error.", "> computing `tf.gradient` for `tf.scatter_nd` with `tf.int64` indices throws an error\r\n\r\nCould you provide details on the error? Let's try to fix it.", "@Androbin \r\nI've found the script that used to throw an error, and it is not throwing it anymore. :)\r\n", "@dantkz is this still relevant? if so, you need to pull rebase and push again. Thanks!", "@drpngx Rebased and submitted as #16368.", "So close this one?", "@dantkz @Androbin can we close this?"]}]