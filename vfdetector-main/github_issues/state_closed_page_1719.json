[{"number": 1330, "title": "Problematic links in official website", "body": "In https://www.tensorflow.org/versions/r0.7/how_tos/tool_developers/index.html, some of the links that referred to this repository contain \"%0A\" in random places which make them invalid. \n", "comments": ["Okay I updated the website and pushed the fix.\n"]}, {"number": 1329, "title": "Our shell wrapper around swig is a bug according to bazel", "body": "Bazel likes to strip things it doesn't like out of `PATH`, as discussed at https://github.com/bazelbuild/bazel/issues/957.  For TensorFlow, this is a problem since we have a shell wrapper around swig that just says `swig \"$@\"`.  In my case, `swig` is in `~/homebrew/bin`.\n\nSince Bazel considers this a bug in TensorFlow rather than in Bazel, it may be more useful to have the discussion here than on the Bazel site.\n", "comments": ["Here is the problematic file: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/swig/swig.sh\n", "Perhaps we can use some kind of crosstool wrapper-type functionality?\n", "Alternatively set it up during the configure script, the way we do python: as a parameter in bazelrc.\n", "@ebrevdo: I think configure is the way to go.\n", "I think using configure is also the best thing to do as well. configure can find a way to plugin the location of swig through command line or generate an external local repository for swig. It needs to configure SWIG_LIB as well.\n", "`configure` hack complete.  I'll send it for review soon.\n"]}, {"number": 1328, "title": "default initializer for seq2seq example", "body": "What is the default initialization for the weight in seq2seq example? I can find it no where, it is completely not defined.\n", "comments": ["https://github.com/tensorflow/tensorflow/blob/795f35da2d458cbae477ac2fe2bff80c1427a771/tensorflow/python/ops/rnn.py#L92 -- I think the initialization of the state in the rnn cells is zero.  Is that what you were wondering?\n", "no, I mean the weights parameters, the embedding, the weights of the grus, the output projections etc. no default initializer is provided. I cannot find it anywhere.\nhttps://github.com/tensorflow/tensorflow/blob/795f35da2d458cbae477ac2fe2bff80c1427a771/tensorflow/python/ops/rnn_cell.py#L675\n", "https://github.com/tensorflow/tensorflow/blob/0e9240b72613aa084728dcba55f7fae3591e86ad/tensorflow/python/ops/variable_scope.py#L65\n\nDoes that help?\n", "I saw it, but the definition of `UniformUnitScalingInitializer` is missing. I assumed it is `6/sqrt(n_in + n_out)`. But I manually check some of the initial weights, some of the weights follow the rule, some of them doesn't. Seems different weights have different initialization. For example, the output projection matrix and decoder embedding matrix have the same shape, but their initial values are quite different.\n", "https://github.com/tensorflow/tensorflow/blob/0e9240b72613aa084728dcba55f7fae3591e86ad/tensorflow/python/ops/init_ops.py#L149\n"]}, {"number": 1327, "title": "Fix PCRE link in get started docs", "body": "This is just a simple typo correction. Does this still require the [Contributor License Agreements](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#contributor-license-agreements) mentioned in the contributing guidelines?\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Looks like the answer is yes :smile: \n\nI signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks, can you update to master?\n", "Sure thing, it should be up to date now. Let me know if there's anything else I need to do.\n", "Apparently github changed their icons for when the changes are out of date, and they chose a color that matched the indication for 'this has a merge conflict'.  Sorry for the noise.\n", "Merged\n"]}, {"number": 1326, "title": "Add forget_bias option to LSTMCell", "body": "Currently `BasicLSTMCell` accepts a `forget_bias` option but `LSTMCell` does not and always defaults to a forget bias of 1 that cannot be controlled by the user.\n", "comments": ["May not get to this till next week.\n", "If it's not too much trouble :-), it'd be great if one can control the weight and bias initializers independently, as in the `tf.contrib.layers` functions like `fully_connected` and `convolution2d`, which accept separate `weight_init` and `bias_init` options. They would be separate from the forget bias of course.\n", "@ebrevdo: Are you still working on this, or should I mark it contributions welcome? \n", "This seems to already be resolved, https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/LSTMCell, closing!\r\n"]}, {"number": 1325, "title": "tf.get_variable() cannot recognize existing variables", "body": "`with tf.variable_scope(\"conv\", reuse=True):\n    x = tf.get_variable(\"w\",[1])`\n\nThe above code cannot recognize an existing variable, but clearly the existing variable was created before as print out of the variable .name shows : `conv/w:0`\n\nI get an error when using tf.get_variable:\n`ValueError: Under-sharing: Variable conv/w does not exist, disallowed. Did you mean to set reuse=None in VarScope?`\n\nIf set to reuse=None...\n`with tf.variable_scope(\"conv\", reuse=None):\n    x = tf.get_variable(\"w\",[1])`\n\nThen it creates another variable with .name  `conv/w_1:0`\n\nIt's a bug! Now I have 2 variables, with names `'conv/w'`  and `'conv/w_1'`\n", "comments": ["I check the `self._vars = {}` in python/ops/variable_scope.py, got it to print the `_vars`, and the dictionary was empty `{}`. None of the variables created previously got stored in `_vars`\n", "Creating the variable the usual way using `tf.Variable` does not add the variable name to `self._vars` of `tf.variable_scope`\n", "Indeed -- variables created in other ways than with tf.get_variable(...), esp. by the lower-level tf.Variable, are not added or recognized by variable_scope. This is partly intentional (as some special variables may need to be treated specially) and partly a result of how the variable sharing process developed.\n\nMy suggestion for you would be to always use tf.get_variable if you're relying on it later -- why not?\n", "Thanks!\n", "Perhaps this should be made part of the documentation. \n", "Definitely! Re-opening so we don't forget it, thanks!\n", "Thank you for the answer. I had the same problem. \nOn the other hand, I would like to add a note that if `tf.get_variable_scope().reuse_variable()` is (accidentally)  used without a context manager `with tf.variable_scope(...):`, it will set any variable scope's `reuse` to True.\n\n``` python\nwith tf.variable_scope('foo'):\n  print (tf.get_variable_scope().reuse)\n  print (tf.get_variable_scope().name)\n  # >>> False\n  # >>> foo\n\n# Set reuse without a context manager\ntf.get_variable_scope().reuse_variables()\n\n# Any variable scope's reuse is changed to True\nwith tf.variable_scope('foo'):\n  print (tf.get_variable_scope().reuse)\n  # >>> True\n\nwith tf.variable_scope('other_name'):\n  print(tf.get_variable_scope().reuse)\n  # >>> True\n```\n", "Indeed - any sub-scope of a reusing scope is also reusing. So just setting things with .reuse_variables() can be a bit dangerous. We could forbid it and have only context, but on the other hand it's convenient from time to time.\n", "Actually why not just make all variables callable with `tf.get_variable()`? It would be so much more convenient and there wont be any confusion.\n", "Some input on this issue, variable loaded by `tf.Saver.restore()` are also not recognized by `tf.get_variable()`. The cleanest method I could think of for accessing individual variables loaded as such was to create a dictionary with the variable names:\n\n```\nsaved = tf.train.import_meta_graph('save.ckpt.meta')\nsaved.restore(self.sess, 'save.ckpt')\nsaved_dict = {}\nfor x in tf.trainable_variables():\n  saved_dict[x.name] = x\n# access e.g. saved_dict['some/scope/weight:0']\n```\n", "Yeah I got this bug too! Variables loaded by tf.Saver.restore() don't work with tf.get_variable()!\n", "@lukaszkaiser: Are you planning to make the documentation fix, or should I mark this contributions welcome or close? \n", "I think there is a true bug here, not a documentation problem. Variables loaded with saver should be imported to with with get_variable. It is being addressed in tf.learn (and also in the context to make partitioned variables work ok). I think Illia was working on that, but I'm not sure how far it got.\n", "@ilblackdragon: Reassigning to you; feel free to adjust if that's wrong.\n", "did this problem fix ? my code used get_varaible(), and  validation when train is ok, but when i restore from checkpoint, predict will always the same weight, not correct\n@girving \n", "+1, got the same problem symptom as @mlinxiang \n", "+1. I have the same problem as @rlrs, @mightyroy, @mlinxiang & @MingStar: I can't load variables with tf.Saver.restore() if they were created with tf.get_variable().\n\nI can easily use the workaround described by @rlrs for some of the variables, but I can't find an easy way to do the same with variables that were created with a call to tf.nn.rnn(...), which uses tf.get_variable().\n", "I have actually got this problem fixed for my case. \n\nThe way I did it was to construct the graph, restore the checkpoint file (of tf variables) and run the session in the SAME thread.\n", "@ilblackdragon Friendly ping, please update this bug with the current status.  Thanks!\n", "I'm facing the same issue. I was wondering if somebody has found a way around it. I can't really use the trainable variables hack that @rlrs suggested as I'm also using the default RNN code of tensorflow and don't really want to hack into the rnn definitions for reusing the variables. It would be great if somebody can suggest a way around this without having the need to hack into the rnn code definition in tensorflow. Thanks!\n", "I'm not exactly sure where the problem occurs.\n\nIf you are using tf.learn - currently you need to have the code around to restore the model. E.g. recreate `Estimator` with the same `model_fn` and pass the `model_dir` for the directory with checkpoints.\n\nThe same thing is true if you are using `Saver` - you should recreate graph as well first and then run `Saver.restore`.\n\nThere is `saved_model` functionality is built, to be able to load packaged graph and checkpoints without having code. If that's what you want.\n", "@ilblackdragon While following the directions as given on https://www.tensorflow.org/versions/master/api_docs/python/state_ops.html#exporting-and-importing-meta-graphs, when I use the code:\n\n`new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')`\n`new_saver.restore(sess, 'my-save-dir/my-model-10000')`\n\nand then try to reuse the variables, it says the variables aren't defined. I can see the variable names in `tf.trainable_variables()` but still it's giving the error about variables being undefined. On the other hand, when I don't use the metagraph and just use `tf.train.Saver()` instead of `tf.train.import_meta_graph()`, the variables seem to have loaded. I've recreated the graph for both the above cases before restoring the model. Sorry if I'm really wrong, I initially felt that by loading the metagraph, I can directly reuse the variables without having to recreate the graph. I felt the metagraph functionality was initially created in order to remove the requirement of recreating the graph.  I'm currently just using the `tf.train.Saver()` in order to load my variables. Let me know if I can be of any help by providing any more details.\n\nThanks!\n", "@lukaszkaiser  Thanks for your comments and I found the same phenomenon (not retrievable by get_variable) with any defining method other than tf.get_variable(). As noticing you are one of the main contributor of TF, may I kindly ask you 2 questions:\n1. It seems even in the same scope, if using tf.Variable([1], name='weights') to define multiple variables (every parameter are the same including the string name), there won't be any conflict; the resulted variables actually will have different string names, whereby the system will add postfix to each string such as weights1, weights2, ... So my question is, does this imply that the variables defined using tf.Variable() are local variables instead of global (static) variables defined by tf.get_variable()?\n2. If the answer to question 1 is True, then if we define a variable with tf.Variable() in a function, and when each time this function gets called, will a new local variable be created that does not store previous state? Does that mean when defining a neural network graph, we HAVE TO USE tf.get_variable() to define weights and biases, so that the weights can be shared and previous state got kept? However, I saw some examples that use tf.Variable in graph definition function and the training also works...\nI am so confused.\nThank you very much for your help! \n", "(1) Yes: tf.Variable is a low-level function that will always create a variable node. It is used inside tf.get_variable. The variables can be local and global -- the only difference is that it's low-level. I'd recommend to never use it.\n\n(2) It's not about local and global, tf.Variable is just lower-level. It's used inside tf.get_variable, so in principle you can do everything just with it, yes. But it will not respect variable scope, not use the initializer you defined there, not do partitioning, and not do bookkeeping. It's just low-level. Use tf.get_variable to avoid problems.\n", "@lukaszkaiser  Thanks for your response! As for the local/global issue, may I confirm that \n1. Anything defined with tf.get_variable() is guaranteed to be global, so even it's defined in a function it can be retrieved, shared and its old information kept (Just like a static variable in C++)? \n2. While with tf.Variable(), it can be either local or global, but I am just wondering that, if I define a graph's weights using tf.Variable() in a function, and not setting it as a class member, it cannot be retrieved by tf.get_variable(), and each time we run that function, it creates a new variable, how can the session.run(optimizer, feed_dict={...}) retrieve the unique variable and update its value based on its old value with gradients (I heard it works in this way)?\n\nSorry to bother with many questions. Hope these are the last two, otherwise I got to read through the source code. I just hope to get everything clear other than blindly following some example to use tensorflow.\nThanks again!\n", "I think you have a different notion of local and global than the TF one (which only concerns distributed training). In the notion you say, I think you're right with (1) -- but it's hard for me to be clear about what \"global\" means for you. All nodes in TF are part of the graph, so they are global in this sense.\n\nIf you define something with tf.Variable, it will not be retrieved by tf.get_variable, that's right. It still has a node name in the graph, so you can feed to it -- but you need to be careful which suffix tf.Variable added to the name and be sure to use exactly this suffix.\n", "@lukaszkaiser I think I kind of get the big idea. So previously when I say \"global\" I mean its life time does not end when the function exits, unlike a variable defined in stack in C/C++. It is either defined in stationary memory or in heap, because our initialize_graph function is just a function, every variable is defined inside the function, if it is a stack variable, after calling the initialize_graph function, those variables are freed, the session.run() function won't see/retrieve it. That's why I say the reason to use tf.get_variable() is to not define it in stack, and make it global for later use.\n\nNow I kindly get that my such thought are not how TF works, TF does NOT NEED my above mechanism to keep variables global; they are intrinsically global either using tf.Variable or tf.get_variable. I.e., even we define those variables using tf.Variable WITHIN A Function, those variables are actually attached to a hidden \"graph\" class, so they are global and life persisting. Thus the session.run() function can somehow retrieve them by calling graph.some_weights/biases. And as what you mentioned, the tf.get_variable method is mainly for distributed training that even in different gpus/machines, we have a consistent/convenient way to retrieve the same unique variable?\n\nIn terms of the suffix thing, truely that I noticed the system will add a suffix to the string name we gave when defining, yielding a string_suffix name in reality.\n", "You wite: \"those variables are actually attached to a hidden \"graph\" class, so they are global and life persisting\" -- and that's true. It might be one of the harder things to grasp about TF, but this is the case, there is a global graph and all nodes are global as they belong there.\n\nThe get_variable is in this sense just a bookkeeping function that tried to help you not get lost in this large global graph. It can help you make 4 copies of your model to run on 4 gpus but still sharing variables, things like that. Hope this helps :).\n", "@lukaszkaiser  Definitely very helpful. In terms of the \"hidden graph\", would you please point out in which code line does the variables be attached to the graph class(belong it)? Is the graph just the entire program space so that in any code space, in any function, any variable defined as a \"variable\" type other than a \"tensor\" type are automatically belonging to the \"graph\" class? Danke!\n", "Every TensorFlow operation (tf.Variable, tf.add, even t1 \\* t2 when t1 and t2 are tensors) creates, in the background, an op that is stored in the graph. So if you write y = t1 \\* t2  then you're creating an Mul-Op in the global TF graph. It all happens in the background, you can look in python/framework/ops.py if you want to know more. The graph is created (and can be changed) in the with tf.Graph().as_default(): context.\n", "So the phenomenon that variables restored by Saver.restore func are not retrievable by get_variable is bug for fix?  ", "@lukaszkaiser  When I use tf.get_variable instead of tf.Variable(), my training process is not very good. Specifically, I think the parameters does not update in every epoch. I have experimented it  in the program publiced in tensorflow tutorial, it does has something wrong. Have you experiment it in your program? or I use it in a wrong way?\r\n ", "@MingStar   I came across the same thing, how do you fix it latter? ", "The function tf.get_variable() returns a tf.Variable, it has generally no influence on how it is updated later. One thing that might cause differences in training is initialization: what values are you initializing your tf.Variable with? What initializer are you using with tf.get_variable?", "@lukaszkaiser I initialized the parameters like these:\r\ndef weight_variable(name,shape):\r\n\tinitial=tf.truncated_normal(shape,stddev=0.1)\r\n  \treturn tf.Variable(initial,name)\r\ndef weight_variable(name,shape):\r\n\treturn tf.get_variable(name=name,shape=shape,initializer=tf.random_normal_initializer())", "Looks like in the first case you initialize with standard deviation of 0.1 (and truncate, but that's probably less important), and in the second case you use the default standard deviation of 1.0 in tf.random_normal_initializer. That's probably too high a deviaiton for your model. Try using the same initializer in the second case, i.e., change the last line to `return tf.get_variable(name=name,shape=shape,initializer=tf.truncated_normal_initializer(stddev=0.1))`. Hope that helps!", "@lukaszkaiser  It is very helpful, the program works well now.Thank you soooooo much\uff01", "@thatsvishalhere @lukaszkaiser @SJTUzhanglj @IsaacBanjo I am running into the same issue with tf.get_variable not recognizing variables restored by tf.train.import_meta_graph(). Have you found a workaround for this?", "@eringrant Not sure if you are still seeing this problem. I am able to save variables created with tf.get_variable() using tf.train.saver. ", "An update - to run inference, I am now using tf.get_variable to initialize the model parameters (randomly), and am then overwriting the values for each parameter with the learned model loaded using tf.train.import_meta_graph(). Clearly a workaround and sub-optimal, but necessary given the situation.", "@jrajagopal were you able to do that with an RNN model? I'm using tf.nn.bidirectional_dynamic_rnn() and struggling to restore the weights any sense...", "@VerityStone Yes, I am using the same. I load the variables into a dictionary where each variable is referenced by its name. For inference, I use the same code which contains tf.nn.bidirectional_dynamic_rnn(), which means it will create the same set of variables but with an \"_1\" suffix in the name. It will also initialize each variable randomly.\r\n\r\nI then overwrite each \"_1\" variable thus initialized, with the value of the corresponding variable in the dictionary, i.e., the ones without the \"_1\" . To do that, I use \"for v in tf.trainable_variables()\" along with sess.run(v.assign()). \r\n\r\nJust keep in mind that this creates two superflous copies of the parameters in memory, the one that is loaded and the one in the dictionary. So, if you are running a multi-layer net, with large node sizes and input and/or output projections, you are looking at many GBs and may run out of memory.", "@jrajagopal I did not use the skflow package at that time, instead I wrote my own graph which worked, but I heard the skflow people have updated their library and they re-defined the restore function.", "@IsaacBanjo Ok good to know - thanks!", "It seems like this is working as intended, so I am automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I am running into a similar problem: \r\n\r\n0. In the contrib.learn.Estimator`  class, define some variable say `W`.\r\n1. Export a `contrib.learn.Estimator` with `export_savedmodel`. End the python process.\r\n2. Restart a new python process, and import the `SavedModel` back into python with `saved_model.load`\r\n3. `tf.get_variable` do not recognize the variable `W` defined in step 0.\r\n\r\nIs it really working as intended?", "Hello, \r\nI think this issue still persists. \r\nOn loading a file I can print out a list of variables from the checkpoint file using the below code. \r\n```\r\ncheckpoint_file=tf.train.latest_checkpoint(\"model/.\")\r\nprint(checkpoint_file)\r\nsaver =\ttf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\r\nsaver.restore(sess,checkpoint_file)\r\nfor t in tf.global_variables():\r\n\tprint(t.name)\r\n```\r\nResulting in:\r\n```\r\nmodel/.\\acgan-1-0\r\nglobal_step:0\r\ngenerator/biases:0\r\ngenerator/weights:0\r\n```\r\nIt then fails to load using:\r\n```\r\ndef Generator(Z, outputsize, batchsize, reuse=None):\r\n\tprint(\"Generator\")\r\n\tfor t in tf.global_variables():\r\n\t\tprint(t.name)\r\n\twith tf.variable_scope(\"generator\") as scope:\r\n\t\tif reuse:\r\n\t\t\tscope.reuse_variables()\r\n\r\n\t\tinputsize =  Z.shape[1]\r\n\t\tG_b1 = tf.get_variable(\"biases\", [outputsize],initializer=tf.constant_initializer(0))\r\n```\r\n\r\nWhich results in:\r\n```\r\nValueError: Variable generator/biases does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n```\r\n", "@lukaszkaiser , any suggestions?", "This is a very old problem with saving, Sherry worked on it a while ago, no idea why it was dropped.", "@sherrym do you have any idea of the current status of this?", "I have the same problem as @rustyoz .", "Variables restored from the saver are assigned a different output index. So the variable I'm creating using tf.get_variable ends up being a completely different one.\r\n\r\n```python\r\nwith tf.variable_scope(\"var_scope\"):`\r\n    W = tf.get_variable('W', shape=[784, 10], initializer=tf.random_normal_initializer(stddev=0.1))\r\n```\r\n\r\nIf I restore variables from the saver I end up with two sets of variables:\r\nThe first: 'var_scope/W'\r\nAnd the second restored by the saver: 'var_scope/W_1' (Notice the added output index)\r\nI don't get why the saver won't use the variable defined by `tf.get_variable()`\r\n\r\nI uploaded this small program to demonstrate the effect. If anyone wants to try it out. [here](https://gist.github.com/kashefy/9f8c57a7b1eb41b8491d8c37b94b0f82).\r\n\r\nSo far I've only managed to restore variables defined with `tf.get_variable()` by calling\r\n `graph.get_tensor_by_name(full_var_scope_and_var_name_and_output_index)`\r\nafter restoring them from the saver.\r\n\r\nThanks!", "Are people still experiencing this problem?", "@michaelisard, yes, i do experience )", "Also having this issue. Any updates?", "@sherrym Could you update with the status of this issue?\r\n", "insert   tf.reset_default_graph() at the beginning of the block.\r\nBy running the script multiple times on the same graph,  you are adding the variables to the graph for each run...", "sample proble, use `import_meta_graph` can not get variable by `tf.get_variable()`", "just run tf.reset_default_graph() ", "I'm having this problem too... I passed a variable name to `tf.add` but it won't let me retrieve it using `tf.get_variable`.", "@sherrym  - Any update on this ?", "Any news on this? When I load variables from the saver they are not recognized by tf.get_variable, which creates a new variable with the same name plus '_1'. How can I get the reference to the variables loaded from the saver?", "@mightyroy Is this issue resolved? Please close If it was resolved already. Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Tested with v1.15 and this issue still exists.", "@wenting-zhao Please create a new issue by providing a standalone code to reproduce the issue. thanks!"]}, {"number": 1324, "title": "GLIBC error", "body": "I searched for this issue but did not find any useful way. I installed tensorflow with pip and I am using Ubuntu 12.04 and python 2.7.3.\nWhen in python I write import tensorflow, it raises the following error:\n\nImportError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.17' not found (required by /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so)\n\nAny useful suggestion?\nThanks\n", "comments": ["Have you solve this issue finaly?\n", "Yes. Tensor flow has a problem with Ubuntu 12. I installed Ubuntu 14.04 and that worked like a charm.\n"]}, {"number": 1323, "title": "BasicLSTMCell cannot calculate output shape correctly when batchsize is 1", "body": "I found that `BasicLSTMCell` cannot infer output shape correctly when the input batchsize is 1, after my graph construction failed within this case.\n\nTensorflow version: 0.7.1, installed with pip. Operating system: Ubuntu 14.04\n\nMinimum code snip to reproduce:\n\n```\nimport tensorflow as tf\n\nbatchsize = 1\n# batchsize = 2\nD_in, D_out = 1000, 2000\n\ninput_ = tf.zeros([batchsize, D_in], tf.float32)\nlstm_cell = tf.nn.rnn_cell.BasicLSTMCell(D_out)\nstate = lstm_cell.zero_state(batchsize, tf.float32)\n\noutput, _ = lstm_cell(input_, state)\nprint(output.get_shape().as_list())\n```\n\nWhen `batchsize = 2`, the shape of output is `[2, 2000]`, with the first dimension equal to batchsize. However, when `batchsize = 1`, the shape of output is `[None, 2000]` instead of `[1, 2000]`.\n\nI'm not sure why the output shape is not correctly inferred as `[1, 2000]` when `batchsize = 1`. Could someone please have a look?\n", "comments": ["Just to clarify: the code works, it's just that shape inference does not infer the batch size, right?\n\nI think it's harder to infer sizes of 1 because they have different behaviour in terms of broadcasting. Assigning to Derek who know the shape inference code much better.\n", "Yes, this seems to happen because, internally, we compute `tf.mul(x, y)` where `x` has shape `[None, 2000]` and `y` has shape `[batch_size, 1000]`. If `batch_size` is 1, then it can't tell whether `None` is actually `1` (i.e. there's no broadcasting), or a number greater than `1` (i.e. broadcasting happens).\n\nThe typical way to deal with issues like this - when shape inference is hard to do - is to have the higher-level library function (e.g. `BasicLSTMCell()`) call `Tensor.set_shape()` with the statically known shape.\n", "@lukaszkaiser Yes, it's just that shape inference does not infer the batch size. The code works after I explicitly set the shape.\n\nThanks for your fast response!\n", "I was thinking about this, and I'd like to know more about your use-case. While we could add the extra shape inference code to BasicLSTMCell, the question arises where else should we do it. In every LSTM cell? In every layer in contrib, in every layer we write anywhere? I mean -- that's the reason we have automatic shape inference, so we don't write (and maintain) this code at every point.\n\nThe above is the reason why I'm hesitating to add extra shape code, and is did not seem to be needed until this point. But of course we need to make your code work, so it would be good to see if it's just BasicLSTMCell or more places or an instance of a more general problem. Could you share more about your code and how the lack of the shape on batch dimension hurts you?\n", "@lukaszkaiser: Closing for now since the original issue is not fixable given broadcasting. \n"]}, {"number": 1322, "title": "Custom padding for convolutions", "body": "For the moment it looks like TF only supports VALID or SAME convolutions ? Is there a clever work around to do custom padding convolutions like having a padding of length h defined by the user like in this [neon implementation](http://neon.nervanasys.com/docs/latest/generated/neon.layers.layer.Conv.html) ? Is implementing it in the pipeline ?\n", "comments": ["Define custom. I can only imagine that you want to use something other than zeros in the padding, in which case you can use VALID and manually pad the feature maps.\n", "Actually I meant having a padding equal to 1 for instance, but you are right I will edit my question it is too vague.\n", "But as you suggested I could just add manually a column of 0 and a line of 0 to the feature map and use VALID convolutions, but which tf function should I use in order not to disturb the computations ?\n", "I'm confused as to why you would even need this. Doing a 3x3 kernel conv with SAME does padding of 1. 5x5 does padding of 2, etc...\n", "I wanted non symmetrical padding like only one side is padded but I guess this is not a common thing and neon is actually doing symmetrical, I am therefore closing this issue.\n", "@jeandut how about http://stackoverflow.com/questions/37659538/custom-padding-for-convolutions-in-tensorflow?\n", "just use tf.pad op for your purpose"]}, {"number": 1321, "title": "Add binary flag to gunzip_file in tensorflow/models/rnn/translate/data_utils.py.", "body": "Python 3.4 does \"TypeError: must be str, not bytes\" unless the binary flag is present on line 69.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks, can you update to master?\n", "@tensorflow-jenkins: test this please\n", "@tensorflow-jenkins: test this please\n", "Merged\n"]}, {"number": 1320, "title": "Add _python_build to .gitignore", "body": "The directions for a development build say to make it, so .gitignore\nshould ignore it.\n", "comments": ["Can one of the admins verify this patch?\n", "Can you update to master?\n", "Merged\n"]}, {"number": 1319, "title": "Unable to execute example: models/image/mnist/convolutional.py on MacOS", "body": "For bugs/issues, please fill in the following.  The more information you\nprovide, the more likely we can help you.\n### Environment info\n\nOperating System: Mac OS (El Capitan / Python 2.7)\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   Virtualenv\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   0.7.1\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. python /Users/foo/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\n### Logs or other output that would be helpful\n\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting data/train-images-idx3-ubyte.gz\nTraceback (most recent call last):\n  File \"/Users/yuntatsai/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\", line 316, in <module>\n    tf.app.run()\n  File \"/Users/yuntatsai/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/Users/yuntatsai/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\", line 128, in main\n    train_data = extract_data(train_data_filename, 60000)\n  File \"/Users/yuntatsai/tensorflow/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\", line 75, in extract_data\n    buf = bytestream.read(IMAGE_SIZE \\* IMAGE_SIZE \\* num_images)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py\", line 268, in read\n    self._read(readsize)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py\", line 315, in _read\n    self._read_eof()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py\", line 354, in _read_eof\n    hex(self.crc)))\nIOError: CRC check failed 0x381c74b0 != 0x805e6c18L\n", "comments": ["I also get this error trying to run \n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\nfrom the tutorial.\n", "At first, I have met the same error, after \n\n```\n$ rm -r models/image/mnist/data\n```\n\nand execute `convolutional.py` again, everything works well.\n", "Hi, I met the same problem as you. \n\nI found that the error might be caused by the dataset downloading. After changing the line 37 `WORK_DIRECTORY` in `tensorflow/tensorflow/models/image/mnist/convolutional.py` to my own directory, everything worked fine when re-running the `convolutional.py` code.\n\nI hope this will help you.\n", "@ebrevdo: Could you take a look or reassign?  I'm not sure if there's a problem we can fix here or not.\n", "Sounds like a bad download. We can auto retry the download, or catch the error and ask the user to delete the gzipped files before rerunning. Not sure who wrote the original mnist fetching code.\n", "@ebrevdo: I don't think we should do too much work if the issue is a server outside of our control.  PRs welcome if someone wants to make this more robust: if so please comment and I can reopen the issue. \n", "@kristoffernolgren I got the same error too. Everything worked well when I mentioned the complete directory in read_data_sets command."]}, {"number": 1318, "title": "Lua API", "body": "Hi!\n\nIs somebody working on a Lua API? \nIf not, are you interested on contributing to develop a Lua API?\n\nThanks!\n", "comments": ["We wouldn't be able to actively support it, so a Lua layer would be best as a separate repo.  If you want to work on it, there is some discussion on https://github.com/tensorflow/tensorflow/issues/388 that may be relevant.\n", "If anyone does choose to embark on this, please do try to build off [c_api.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h) using the approach outlined in the [how-to](https://www.tensorflow.org/extend/language_bindings)", "Closing due to lack of recent activity. If there are any contributors interested in writing and maintaining this support then please reopen the issue. Thanks!", "@asimshankar Could you re-link the `how-to` page?", "Updated the link in the previous comment. For the record, it's: https://www.tensorflow.org/extend/language_bindings"]}, {"number": 1317, "title": "tf.get_variable() behavior somewhat inconsistent across reuse=True, reuse=None", "body": "By default, calling `tf.get_variable()` for a variable that does not exist yet creates that variable in the current scope, if the scope has `reuse=Nne`. However, if the scope has reuse=True, then  a Under-sharing error is raised.\n\n``` python\nimport tensorflow as tf\nwith tf.variable_scope(\"scope1\",reuse=None):\n  x=tf.get_variable(\"x\", [1,1]) # this works\nwith tf.variable_scope(\"rnnscope\",reuse=True):\n  w=tf.get_variable(\"w\",[1,1]) # this fails\nValueError: Under-sharing: Variable rnnscope/w does not exist, disallowed. Did you mean to set reuse=None in VarScope?\n```\n\nI think I understand the motivation behind this behavior - variables retrieved in a reuse=True scope should be in \"reuse-mode\", so an error should be raised if it hasn't been created yet.\n\nThe way I get around this is a global variable DO_SHARE=None that is permanently set to True for all t > 0. \n\nAn example implementation of building an RNN this way is here: https://github.com/ericjang/draw/blob/master/draw.py#L163\n\nHowever, this seems a bit inelegant - wouldn't it be easier to just have `get_variable()` create nonexistent variables, regardless of the value of `reuse`? Perhaps there is a better way to build RNNs that I'm not aware of?\n### Environment info\n\nOperating System: **Debian**\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   **GPU-enabled wheel for Linux**\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   **0.6.0**\n### Steps to reproduce\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["In some sense the reuse checks in tf.get_variable are indeed superfluous -- one could, as you say, just always create non-existent variables regardless of the reuse-check setting (and get rid of reuse).\n\nWhen designing variable_scope we discussed this issue at length. The decision to enforce reuse-checks was motivated by a history of research model development. It happened a few times that people trained models that had more variables than they thought, exactly because these parameters were silently created in the way you suggest. The models still gave reasonable results, so the problem was hard to debug -- but the results with proper reuse were much better. Taking this into account we decided to enforce reuse checks quite strictly. But it's python -- it's easy to disable the checks if you want to, e.g., like this.\n\n```\ndef unsafe_get_variable(name, ...):\n  try tf.get_variable(name, ...)\n  except _:\n    with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n      tf.get_variable(name, ...)\n```\n\nBut it's a dangerous road, the checks can save you some debugging later, so consider not disabling them.\n", "Thanks! I also just discovered that `make_template` [https://www.tensorflow.org/versions/r0.7/api_docs/python/state_ops.html#make_template](https://www.tensorflow.org/versions/r0.7/api_docs/python/state_ops.html#make_template) does the trick as well.\n"]}, {"number": 1316, "title": "Removed a broken link in index.md", "body": "A link referred to CIFAR-10 samples. However, this was a broken link. So, I removed the link and the references to this link.\n", "comments": ["Can one of the admins verify this patch?\n", "Merged\n"]}, {"number": 1315, "title": "Assign graph to cpu in example/udacity/5_word2vec.", "body": "Here's what I think is a fix for [1297](https://github.com/tensorflow/tensorflow/issues/1297). Please see the issue for discussion.\n\nIf there's a way to do this without all the whitespace changes please let me know. I'm new to Python. The only real change is addition of the `with tf.device():` statement.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Signed CLA.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "> May I ask you to do it on one line instead:\n\nSure thing Dr. V. There you go.\n"]}, {"number": 1314, "title": "fix #1303 doc link broken", "body": "capitalize name of type in C++ doc links, referencing #1303 \n", "comments": ["Can one of the admins verify this patch?\n", "Already did: https://github.com/tensorflow/tensorflow/commit/55be24524167c1d1879dba64b492c33c82244d1f\n"]}, {"number": 1313, "title": "Incorrect variable name", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins: test this please\n\n@martinwicke @jendap FYI, not sure what the implications of this was\n", "@tensorflow-jenkins: test this please\n", "Merged\n"]}, {"number": 1312, "title": "Fix typo 'overlaps' -> 'overlap'", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Merged\n"]}, {"number": 1311, "title": "dynamic_rnn incompatible with BasicRNNCell, GRUCell, and BasicLSTMCell", "body": "### Description\n\nI've been playing around with `rnn.dynamic_rnn` and have thus far only had success with it when using `rnn_cell.BasicLSTMCell` instances. When using any other cell, a `TypeError` is raised. Looking at the traceback, the culprit appears to be the calls to `rnn_cell.linear` at each time step, which fail when `rnn_cell.linear` attempts to get the shape of its input slice. This, in turn, fails because the input slices are produced from a `TensorArray` and have shape `<unknown>`.\n### Environment info\n\nOperating System: Ubuntu 14.04 LTS\nInstalled from source at:  b88971051fbc49fa1e0b91ec1b0b60defa11697e\n### Steps to reproduce\n\nThe following will attempt to call `dynamic_rnn` with three different cells, catch the `TypeError` that is raised, then print the exception + traceback to stdout.\n\n```\nimport sys\nimport traceback\nimport tensorflow as tf\nfrom tensorflow.python.ops.rnn import dynamic_rnn\n\ninputs = tf.placeholder('float32', [None, None, 32])\nmbsz = tf.shape(inputs)[0]\nmax_seq_len = tf.shape(inputs)[1]\nsequence_length = tf.fill(tf.expand_dims(mbsz, 0), max_seq_len)\n\nn_units = 32\ncells = [tf.nn.rnn_cell.BasicRNNCell(n_units),\n    tf.nn.rnn_cell.GRUCell(n_units),\n    tf.nn.rnn_cell.BasicLSTMCell(n_units)]\nfor cell in cells:\n    print('='*40)\n    print('Attempting with: %s' % cell.__class__.__name__)\n    print('='*40)\n    try:\n        rnn_out, _ = dynamic_rnn(cell, inputs, sequence_length, dtype='float32')\n    except TypeError as e:\n        tb = sys.exc_traceback\n        traceback.print_exception(TypeError, e, tb)\n        print('\\n'*3)\n```\n### Output of above snippet\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.4.0.4 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.0 locally\n========================================\nAttempting with: BasicRNNCell\n========================================\nTraceback (most recent call last):\n  File \"<ipython-input-1-aa87c2141668>\", line 20, in <module>\n    rnn_out, _ = dynamic_rnn(cell, inputs, sequence_length, dtype='float32')\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 432, in dynamic_rnn\n    parallel_iterations=parallel_iterations)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 501, in _dynamic_rnn_loop\n    parallel_iterations=parallel_iterations)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1590, in While\n    result = context.BuildLoop(cond, body, loop_vars)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1507, in BuildLoop\n    body_result = body(*vars_for_body_with_tensor_arrays)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 491, in _time_step\n    zero_output, state, lambda: cell(input_t, state))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 218, in _rnn_step\n    time < max_sequence_length, call_cell, empty_update)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1210, in cond\n    res_t = context_t.BuildCondBranch(fn1)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1133, in BuildCondBranch\n    r = fn()\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 491, in <lambda>\n    zero_output, state, lambda: cell(input_t, state))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py\", line 122, in __call__\n    output = tanh(linear([inputs, state], self._num_units, True))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py\", line 692, in linear\n    shapes = [a.get_shape().as_list() for a in args]\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 738, in as_list\n    return [dim.value for dim in self._dims]\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n========================================\nAttempting with: GRUCell\n========================================\nTraceback (most recent call last):\n  File \"<ipython-input-1-aa87c2141668>\", line 20, in <module>\n    rnn_out, _ = dynamic_rnn(cell, inputs, sequence_length, dtype='float32')\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 432, in dynamic_rnn\n    parallel_iterations=parallel_iterations)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 501, in _dynamic_rnn_loop\n    parallel_iterations=parallel_iterations)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1590, in While\n    result = context.BuildLoop(cond, body, loop_vars)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1507, in BuildLoop\n    body_result = body(*vars_for_body_with_tensor_arrays)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 491, in _time_step\n    zero_output, state, lambda: cell(input_t, state))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 218, in _rnn_step\n    time < max_sequence_length, call_cell, empty_update)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1210, in cond\n    res_t = context_t.BuildCondBranch(fn1)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1133, in BuildCondBranch\n    r = fn()\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 491, in <lambda>\n    zero_output, state, lambda: cell(input_t, state))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py\", line 151, in __call__\n    2 * self._num_units, True, 1.0))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py\", line 692, in linear\n    shapes = [a.get_shape().as_list() for a in args]\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 738, in as_list\n    return [dim.value for dim in self._dims]\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n========================================\nAttempting with: BasicLSTMCell\n========================================\nTraceback (most recent call last):\n  File \"<ipython-input-1-aa87c2141668>\", line 20, in <module>\n    rnn_out, _ = dynamic_rnn(cell, inputs, sequence_length, dtype='float32')\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 432, in dynamic_rnn\n    parallel_iterations=parallel_iterations)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 501, in _dynamic_rnn_loop\n    parallel_iterations=parallel_iterations)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1590, in While\n    result = context.BuildLoop(cond, body, loop_vars)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1507, in BuildLoop\n    body_result = body(*vars_for_body_with_tensor_arrays)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 491, in _time_step\n    zero_output, state, lambda: cell(input_t, state))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 218, in _rnn_step\n    time < max_sequence_length, call_cell, empty_update)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1210, in cond\n    res_t = context_t.BuildCondBranch(fn1)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1133, in BuildCondBranch\n    r = fn()\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 491, in <lambda>\n    zero_output, state, lambda: cell(input_t, state))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py\", line 203, in __call__\n    concat = linear([inputs, h], 4 * self._num_units, True)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py\", line 692, in linear\n    shapes = [a.get_shape().as_list() for a in args]\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 738, in as_list\n    return [dim.value for dim in self._dims]\nTypeError: 'NoneType' object is not iterable\n```\n", "comments": ["Working on a bugfix now.\n", "Should be fixed at HEAD.\n", "Verified working as of e928a43f79163bdbbe3cca6c240ea4630f75b1b0. Thanks!\n"]}, {"number": 1310, "title": "Could not specify explicit device specification ''", "body": "Several users have reported an issue that occurs with the following steps (e.g. Issue #1297):\n1. Start TensorFlow in a setting with a GPU.\n2. Define some variables with no explicit `tf.device()` set. This most often happens with embedding variables (used as arguments to `tf.gather()` or `tf.embedding_lookup()`).\n3. Initialize them. (They will be placed on the GPU, because it is the \"best available device\".)\n4. On running the first training step, the following error (or similar) is raised:\n\n```\nInvalidArgumentError: Cannot assign a device to node 'Adagrad/update_Variable_2/SparseApplyAdagrad': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\n```\n\nThe `SparseApplyAdagrad` op (or in general most `SparseApplyFoo` ops) is only defined on CPU. The variable has already been placed on GPU, so the graph is not placeable. Attempting to run the same program with `use_soft_placement=True` also fails, although with a stranger error.\n\n**TL;DR:** If this affects you, create your variables in a `with tf.device(\"/cpu:0\"):` block, until this issue is resolved.\n\nThe issue arises because (i) TensorFlow places variables on the first device where they run, (ii) it always prefers GPU over CPU when it is availabe, (iii) initialization ops are available on GPU, and (iv) it applies the placement algorith to the _pruned subgraph_ (not the entire client graph).\n\nOne workaround would be to apply the placement algorithm to the entire client graph. (This is the approach used in the separate `master_session.cc`/`simple_graph_execution_state.cc` codepath, used in the distributed runtime.) However, this has the effect of leaving the session in a broken state as soon as an unplaceable node is encountered. Switching to this behavior might cause issues for people doing exploratory graph construction in a REPL (IPython etc.) because the only remedy would be to recreate the entire graph on seeing such an error. Therefore, while failing fast in a non-interactive setting would be fine, a different solution for interactive use might be required.\n", "comments": ["Previously I had\n\n``` python\nwith tf.device('/job:ps/task:0'):\n    a = tf.constant(100)\n\nwith tf.device('/job:ps/task:1'):\n    b = tf.constant(100)\n\nwith tf.Session('grpc://10.201.106.12:2222', config=tf.ConfigProto(log_device_placement=True)) as sess:\n    print(sess.run(a + b))\n```\n\nand I encountered this `Could not satisfy explicit device specification '/job:ps/task:1'` error.\n\nThen I changed to\n\n``` python\nwith tf.device('/job:ps/task:0'):\n    with tf.device('/cpu:0'):\n        a = tf.constant(100)\n\nwith tf.device('/job:ps/task:1'):\n    with tf.device('/cpu:0'):\n        b = tf.constant(100)\n\nwith tf.Session('grpc://10.201.106.12:2222', config=tf.ConfigProto(log_device_placement=True)) as sess:\n    print(sess.run(a + b))\n```\n\nbut I still have this error.\n\nAm I getting the workaround right? Thanks!\n", "Looks my problem is not related to CPU/GPU you are talking about. More likely because of not being able to find peer worker.\n", "In my setup, I started with `./grpc_tensorflow_server --cluster_spec='worker|10.201.106.12:2222;10.201.106.11:2222' --job_name=worker --task_index=0` and the other with 0 replaced by 1. Is there any problem with the way I fire the servers? Why couldn't I place explicitly on the remote host?\n\nThanks\n", "Found the problem. It should be `--task_id` not `--task_index`. Please change `readme.md` to reflect this change.\n", "Has there been any more work to implement some of the SparseApplyOps on GPU? GPU-embeddings are dramatically faster than CPU ones, so anyone using RNNs would benefit significantly,\n", "(The root bug was fixed a few weeks ago by placing the entire graph on first run, not just the pruned ones).\n"]}, {"number": 1309, "title": "Fixing basic Ubuntu CMake Build", "body": "PR #728 added basic CMake support for Tensorflow. However, that PR was only tested on MacOSX, so had a few errors on Ubuntu. This PR adds in fixes to make basic model compile on Ubuntu 12.04, with gcc 4.9.2, glibc 2.15.\n\nThe compile model does not yet run to completion (segfaults). I hope to debug that issue in a follow-on PR.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please.\n\nTesting for core/lib/io change.\n", "Our tests are down for the weekend, so we'll have to wait a bit. Sorry about that.\n", "@martinwicke No problem. I made a couple small fixes in the meanwhile. As a quick addendum, what would be the best way to get the cmake build onto the jenkins test? I found that as I was making changes, I accidentally broke the build a couple times. It would be convenient to have jenkins catch this.\n", "@rbharath can you please squash the commits? It would be great to have cmake working! I will try to set up jenkins build for cmake later this week.\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@jendap Just squashed the commits. Great to hear you have time to get the cmake build up on jenkins! Please let me know if I can help. Right now the cmake build is pretty rudimentary. The tutorial compiles on Mac OSX/Ubuntu, but segfaults for some reason that needs to be fixed. We also need to add SWIG and GPU support to the cmake build. I'll try to work on this some over the next couple weeks, but would be great if any other folks are interested in helping :)\n", "Oops, looks like my squashing messed things up here. I'll fix this and update this thread once everything is sorted out.\n", "As a quick update, it appears that the cmake no longer builds tensorflow head due to recent changes in tensorflow :-(. Fixing this looks tricky, and I probably won't have time till next week or so. I'll update this thread then.\n", "Yeah, we are constantly messing with the location of header files etc.\nSorry. It should stabilize shortly.\n\nOn Thu, Mar 3, 2016 at 2:04 PM Bharath Ramsundar notifications@github.com\nwrote:\n\n> As a quick update, it appears that the cmake no longer builds tensorflow\n> head due to recent changes in tensorflow :-(. Fixing this looks tricky, and\n> I probably won't have time till next week or so. I'll update this thread\n> then.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1309#issuecomment-191986089\n> .\n", "I forget that cmake build. I will try to get it done by tomorrrow.\n", "There is very simple https://github.com/tensorflow/tensorflow/pull/1488. That should be enough for linux cmake cpu build.\n\nI'm have set it up to run on pull request but only when pull request touch cmake. (Bash condition `[ -z \"$(git log $master_sha1..HEAD tensorflow/contrib/cmake)\" ]` is used for triggering).\n\nThat should allow to run cmake tests while not showing errors on non-cmake tests.\n\nOnce we get cmake working well we can enable it for all the builds. Then we would love to create windows build as well.\n", "@rbharath can you please modify this PR to contain the intended changes only?\n", "@jendap This PR was broken by changes in tensorflow, and fixing the issue looks tricky. I don't have time to work on this for a bit (probably a couple weeks), so I'll close this PR for the time being. I'll open a new PR with fixes if somebody else doesn't fix the issue in the meanwhile.\n", "Ok, thanks! I hope somebody will help.\n"]}, {"number": 1308, "title": "ImportError: cannot import name server", "body": "Operating System:\nUbuntu 14.04\n\nBuilt tensor board from the source (from tensor flow git repo)\n\n`sudo apt-get install nodejs nodejs-legacy npm`\n\n`sudo npm install -g gulp bower`\n\ncd tensorflow/tensorflow/tensorboard/\n\n`npm install`\n\n`bower install`\n\nEverything goes fine. Now I try to run the TensorFlow using command\n\n`>> python tensorflow/tensorflow/tensorboard/tensorboard.py --logdir=bla_logs`\n\n```\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/tensorboard/tensorboard.py\", line 33, in <module>\n    from tensorflow.tensorboard.backend import server\nImportError: cannot import name server\n```\n\nThen I changed the following line in tensorboard.py \n\n`from tensorflow.tensorboard.backend import server`\n\nto\n\n`from tensorflow.tensorboard.backend import tensorboard_server as server`\n\nNow it works fine. \n", "comments": ["@danmane can you take a look?\n", "I was having the same issue yesterday and yes, changing `server` to `tensorboard_server` did solve the problem.\n", "@danmane @vrv  I have done some more digging into the issue. I have installed tensor flow using pip command and then cloned the git repository for examples. \n\nInside the git cloned repo on master branch inside  tensorboard/backend there is a file called server.py\n\nBut in /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/backend there is no file names server.py instead there is a file named tensorboard_server.py, that it why my suggested change was working in my environment. So it looks like there no problem with master branch, once pip package is built using current master branch this issue would go away\n\nhope this helps.\n", "Ah, that makes sense.  In general, if you have installed the pip package, we recommend checking out the github repo at that branch, since we make improvements to master which often updates the code that goes into the binary pip package.  Thanks for the help!\n"]}, {"number": 1307, "title": "Python import error, undefined symbol: secure_getenv", "body": "I got python import error, undefined symbol: secure_getenv\n\nOperating System:\nRedhat 6.7\n\nInstalled from sources using gcc 4.8 (I am using gcc 4.8 installed in a custom path since the default gcc is too old):\ncommit b88971051fbc49fa1e0b91ec1b0b60defa11697e\nDate:   Fri Feb 26 05:08:35 2016 -0800\n\n```\nPython 2.7.11 |Anaconda 2.2.0 (64-bit)| (default, Dec  6 2015, 18:08:32)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nAnaconda is brought to you by Continuum Analytics.\nPlease check out: http://continuum.io/thanks and https://anaconda.org\n>>> import tensorflow\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\n    from tensorflow import contrib\n  File \"/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/contrib/__init__.py\", line 23, in <module>\n    from tensorflow.contrib import layers\n  File \"/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/contrib/layers/__init__.py\", line 67, in <module>\n    from tensorflow.contrib.layers.python.framework.tensor_util import *\n  File \"/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/contrib/layers/python/framework/tensor_util.py\", line 21, in <module>\n    from tensorflow.python.framework.ops import Tensor\n  File \"/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 39, in <module>\n    from tensorflow.python.framework import versions\n  File \"/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/versions.py\", line 22, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 21, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: /nfs/m01/anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: secure_getenv\n```\n\nIf I do ldd ~/anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n\n```\n    linux-vdso.so.1 =>  (0x00007ffd393ef000)\n    libcudart.so.7.5 => /usr/local/cuda/7.5.18/lib64/libcudart.so.7.5 (0x00002ad591b36000)\n    libdl.so.2 => /lib64/libdl.so.2 (0x00002ad591dc3000)\n    libm.so.6 => /lib64/libm.so.6 (0x00002ad591fc7000)\n    libz.so.1 => /lib64/libz.so.1 (0x00002ad59224c000)\n    libpthread.so.0 => /lib64/libpthread.so.0 (0x00002ad592462000)\n    libstdc++.so.6 => /usr/local/gcc/4.8.4/lib64/libstdc++.so.6 (0x00002ad59267f000)\n    libgcc_s.so.1 => /usr/local/gcc/4.8.4/lib64/libgcc_s.so.1 (0x00002ad592989000)\n    libc.so.6 => /lib64/libc.so.6 (0x00002ad592b9f000)\n    /lib64/ld-linux-x86-64.so.2 (0x0000003a39400000)\n    librt.so.1 => /lib64/librt.so.1 (0x00002ad592f33000)\n```\n", "comments": ["The problem is in the external dependency gprc which uses secure_getenv which only appeared in glibc 2.17 - https://sourceware.org/glibc/wiki/Tips_and_Tricks/secure_getenv\n\nTurns out that gprc can be told to try to use other symbols as well but it needs the -DGPR_BACKWARDS_COMPATIBILITY_MODE option, e.g. I've managed to compile CPU only version by doing this:\nbazel build --linkopt '-lrt' --copt '-DGPR_BACKWARDS_COMPATIBILITY_MODE' -c opt //tensorflow/tools/pip_package:build_pip_package \nIt would be more correct to add that option only to gprc but I am far from fluent in bazel-ize.\n", "I can confirm that this works with the GPU version. Thanks very much. Including `EXTRA_BAZEL_ARGS` and the build command below for completeness. Some flags were included simply because they were suggested in other threads to circumvent other issues, but notice `--conlyopt=\"-std=c99\"`. It wouldn't compile for me without this.\n\n```\nexport EXTRA_BAZEL_ARGS='-s --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone --jobs 4'\n\nbazel build --linkopt '-lrt' --copt=\"-DGPR_BACKWARDS_COMPATIBILITY_MODE\" --conlyopt=\"-std=c99\" -c opt --config=cuda --verbose_failures --genrule_strategy=standalone --spawn_strategy=standalone //tensorflow/tools/pip_package:build_pip_package\n```\n", "I confirm @rdipietro's approach works. I was building a GPU version. CPU training worked for me. However, I am not able to use k40m GPU for GPU training.\n\n@rdipietro What's your GPU model? What is your operating system?\n\n```\n++ python cifar10_train.py\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.4 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.5 locally\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K40m\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:08:00.0\nTotal memory: 11.25GiB\nFree memory: 11.15GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:718] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:08:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256B\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 512B\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:107] Allocating 10.60GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:118] GPU 0 memory begins at 0x13047a0000 extends to 0x15aaa4019a\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\n./run.sh: line 4: 33880 Aborted                 python cifar10_train.py\n\n```\n", "Odd. I had only done a stupidly simple test before (`x = tf.Variable(1.0)` ... `print(sess.run(x))`), so I was worried it wouldn't work for me either, but it does:\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:107] Allocating 10.57GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:118] GPU 0 memory begins at 0x2303ee0000 extends to 0x25a8ae0e67\n2016-03-04 12:37:58.181180: step 0, loss = 4.68 (3.8 examples/sec; 34.059 sec/batch)\n2016-03-04 12:38:01.405984: step 10, loss = 4.65 (502.8 examples/sec; 0.255 sec/batch)\n2016-03-04 12:38:03.791843: step 20, loss = 4.64 (533.8 examples/sec; 0.240 sec/batch)\n2016-03-04 12:38:06.084344: step 30, loss = 4.61 (571.2 examples/sec; 0.224 sec/batch)\n```\n\nMy specs:\n\n```\nCentOS 6.7\ngcc 4.8.2\nTesla K80\ncuda 7.0\ncudnn 6.5\ncompute capability 3.5\nNVIDIA-SMI 352.63     Driver Version: 352.63\n```\n", "@rdipietro Thanks for your info. I guess it's an issue specific with k40m. I currently can only use CPU for training. It is interesting that the CPU speed is about half of your GPU speed. I thought GPU is much faster than CPU. My CPU is: Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz\n\n```\n++ python cifar10_train.py\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.4 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.5 locally\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nE tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagnostic information for host: r0215.ten.osc.edu\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: r0215.ten.osc.edu\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: 352.39\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.39  Fri Aug 14 18:09:10 PDT 2015\nGCC version:  gcc version 4.4.7 20120313 (Red Hat 4.4.7-16) (GCC) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.39\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:226] kernel version seems to match DSO: 352.39\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: \n2016-03-04 13:35:25.498518: step 0, loss = 4.67 (7.9 examples/sec; 16.105 sec/batch)\n2016-03-04 13:35:32.403652: step 10, loss = 4.66 (212.1 examples/sec; 0.604 sec/batch)\n2016-03-04 13:35:38.377604: step 20, loss = 4.64 (203.0 examples/sec; 0.631 sec/batch)\n2016-03-04 13:35:44.478986: step 30, loss = 4.62 (215.6 examples/sec; 0.594 sec/batch)\n2016-03-04 13:35:50.481260: step 40, loss = 4.60 (218.4 examples/sec; 0.586 sec/batch)\n2016-03-04 13:35:56.542513: step 50, loss = 4.58 (210.5 examples/sec; 0.608 sec/batch)\n2016-03-04 13:36:02.315769: step 60, loss = 4.55 (205.9 examples/sec; 0.622 sec/batch)\n2016-03-04 13:36:08.432561: step 70, loss = 4.54 (209.7 examples/sec; 0.610 sec/batch)\n2016-03-04 13:36:14.322653: step 80, loss = 4.52 (216.5 examples/sec; 0.591 sec/batch)\n\n```\n", "Hmm. Thanks for posting this. Right now I run multiple RNN experiments with multiple jobs on a single GPU, but this saturates the GPU pretty quickly. I'll see how things go with CPUs.\n\nActually, do you know if TensorFlow automatically uses multiple CPUs while processing batches? I'd have to guess that it does but am not sure. I guess it's not easy to tell with that example as multiple threads are used for the input queue. If so then it wouldn't scale as I'd hoped.\n", "@rdipietro Tensorflow should scale to mutiple CPU cores. I have 10 CPU cores (20 cores by hyperthreading) and I find each core has utilization around 50% (shown in htop command). So I am assuming all cores are used.\n", "I tried this out. Just to report back, if I use CPUs instead of a GPU, the RNN training is about 10x slower.\n", "@rdipietro What's the largest RNN you are able to run? I am stuck at 512 x 3 layers LSTM on Titan X (12 GB GPU memory).\n", "When you say stuck, what exactly happens? I can't imagine memory being even close to exhausted unless your batch size is enormous, or if your inputs are huge or something. (I don't think 3x512 layers is the limiting factor.)\n\nThat said, I don't know how high I can go. In my application I went with 1x256 because two layers didn't improve val performance and neither did 512 hidden units.\n", "Also take a look at the input/output projection layers\n", "@rdipietro Thanks. I mean I can run 512x3, but I would like to run 1024x4 and 1024x5. My batch size is 256. Shrinking batch size to 32, I still can't run 1024x4. So I was wondering if anyone has managed to run 1024x4 or larger. Mabybe model parallelism on multiple GPUs would help.\n", "I'm wondering what you mean by \"can't run.\" I doubt it's memory issue?. Can check this by running `nvidia-smi` while you train. If training is simply failing, maybe it's more likely that your gradients have gone to 0 or inf?\n", "@rdipietro Sorry. By \"can't run\" I mean I saw out of memory errors from tensorflow.\n", "Oh. No clue..\nOn Mar 29, 2016 7:39 PM, \"digitalsword\" notifications@github.com wrote:\n\n> @rdipietro https://github.com/rdipietro Sorry. By \"can't run\" I mean I\n> saw out of memory errors from tensorflow.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1307#issuecomment-203017449\n", "digitalsword did u sloved ur problem? i got same error\n", "@mrry: Is there any easy to get `DGPR_BACKWARDS_COMPATIBILITY_MODE` into our grpc build in a suitable version dependent way?\n", "@girving: Turns out it's quite easy: #2697.\n"]}, {"number": 1306, "title": "Multiple bugs in dynamic_rnn", "body": "I know this feature remains alpha and unsupported, but I've run into a number of various bugs so I thought it may be worth reporting.\n\nJust a simple RNN with a simple loss function + optimizer doesn't work. Take this code snippet for example:\n\n```\nnum_stepss = [34, 20, 44, 18]\ninputs = tf.constant(npr.rand(50, 4, 20).astype('float32'))\ncell = tf.nn.rnn_cell.LSTMCell(num_units=100, \n                               input_size=20, \n                               initializer=tf.constant_initializer(0.05),\n                               use_peepholes=True,\n                               cell_clip=None)\noutputs, _ = tf.nn.dynamic_rnn(cell=cell, inputs=inputs, sequence_length=num_stepss, dtype=tf.float32, time_major=True)\nloss = tf.reduce_sum(outputs)\ntrainer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n```\n\nThis fails with the following error: \n\n`ValueError: Shapes () and (120, 400) must have the same rank`\n\nA slightly fancier version of the above that encloses it within scope `'my_scope'` using `tf.variable_scope` gives the following error: \n\n`ValueError: Expected op/tensor name to start with gradients, got: my_scope/gradients/my_scope/Sum_grad/Tile:0`\n\nI've run into other issues as well, but I don't yet know if they're related to the above or they're distinct bugs.\n", "comments": ["The shape bug should be fixed at HEAD\n", "Yes you are right after compiling with the latest version the shape has gone away. The scope problem is still there however.\n", "Regarding scope issues, it appears that the `TensorArray` instances created in `dynamic_rnn` always have the same value for `tensor_array_name`, which causes issues if you try to use `dynamic_rnn` multiple times in the same graph; for instance, when building a bidirectional RNN.\n", "We have another commit in the works to fix the second problem.\nOn Feb 26, 2016 4:40 PM, \"nryant\" notifications@github.com wrote:\n\n> Regarding scope issues, it appears that the TensorArray instances created\n> in dynamic_rnn always have the same value for tensor_array_name, which\n> causes issues if you try to use dynamic_rnn multiple times in the same\n> graph; for instance, when building a bidirectional RNN.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1306#issuecomment-189539201\n> .\n", "@alquraishi I'd like to ask a question if I may. In your example, the **batch size** is 4(static), and **num_stepss** also is a static vector with length 4. But if I have another batch with different **num_stepss** [14, 30, 24, 17] (Which I mean every batch has different num_stepss...), how could I use the dynamic_rnn model? \n", "@ebrevdo: Assigning to you if something is in the works.\n", "alquraishi: can you provide a list of things still broken at head of master branch?\n", "@ebrevdo sorry I didn't see that this was still open. I'm using dynamic_rnn with no problems. I will close.\n"]}, {"number": 1305, "title": "Fixing nightly artifact links in README.md", "body": "Also added auto version update for README.md in update_version.sh. \n", "comments": ["Can one of the admins verify this patch?\n", "@vrv can you merge this? My workstation is packed up and I don't  have script access.\n", "Merged.  \n\nOur tests machines are down for the weekend so we'll probably hold off merging anything else until Monday.\n"]}, {"number": 1304, "title": "Training a model using GPUs across different machines", "body": "Hi, if/how can TensorFlow's distributed runtime be used for training a model using GPU resources across machines in a cluster?\n", "comments": ["It certainly can! Have a look at the [readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/README.md) for how to get started. You can place nodes on GPUs on different machines using `tf.device()` statements:\n\n``` python\nwith tf.device(\"/job:worker/task:0/gpu:0\"):\n  # ...\nwith tf.device(\"/job:worker/task:0/gpu:1\"):\n  # ...\nwith tf.device(\"/job:worker/task:1/gpu:0\"):\n  # ...\n# ...\n```\n"]}, {"number": 1303, "title": "Broken links in C++ API docs", "body": "Apparently most of the links in the main part of the [index of the C++ API documentation](https://www.tensorflow.org/versions/r0.7/api_docs/cc/index.html) are broken.\n\nThe problem seems to be that the first letter of each file name is in lowercase, so the links look like this:\n[https://www.tensorflow.org/versions/r0.7/api_docs/cc/classEnv.html](https://www.tensorflow.org/versions/r0.7/api_docs/cc/classEnv.html)\nInstead of this:\n[https://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassEnv.html](https://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassEnv.html)\n", "comments": ["@chemelnucfin: Is this bug fixed now? \n", "@girving see pull request #1314 \n"]}, {"number": 1302, "title": "Corrected unbalanced parentheses in docs.", "body": "Fixed small typo in the python API-docs for class tf.train.Optimizer.\n", "comments": ["Can one of the admins verify this patch?\n", "I think I merged this a few days back.\n"]}, {"number": 1301, "title": "Remove .pyc file and add *.pyc to .gitignore", "body": "- Remove `tensorflow/python/__init__.pyc`.\n- Add `*.pyc` to .gitignore\n", "comments": ["Can one of the admins verify this patch?\n", "Can you resolve the conflict?  Then we'll merge\n", "I resolved the conflict.\n", "merged\n"]}]