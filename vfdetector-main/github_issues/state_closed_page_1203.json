[{"number": 17087, "title": "TF Lite example segmentation fault", "body": "\r\n### System information\r\n\r\nOS Platform and Distribution - Ubuntu 16.04.2 LTS\r\nTensorFlow installed from: https://github.com/tensorflow/tensorflow.git\r\nTensorFlow version: last commit f66e9f92820804b7c2b4698147d07d5d2277c62f \r\nBazel version: Build label: 0.8.1- (@non-git)\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce: bazel build --verbose_failures --config opt --cxxopt=-std=c++11 --config monolithic //tensorflow/contrib/lite/examples/label_image:label_image \r\n./label_image\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\n== uname -a =====================================================\r\nLinux qds101 4.4.65 https://github.com/tensorflow/tensorflow/issues/1 SMP PREEMPT Mon Sep 18 13:34:00 CDT 2017 aarch64 aarch64 aarch64 GNU/Linux\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.5)\r\n== check for virtualenv =========================================\r\nFalse\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc1-1607-gbe4e5ef\r\ntf.COMPILER_VERSION = v1.3.0-rc1-1607-gbe4e5ef\r\n\r\n\r\n### Describe the problem\r\nI compile example/label_image nad when I run it I get segmentation fault.\r\nError comes from the following line in resize_bilinear.c (called from label_image.c resize<float>(interpreter->typed_tensor<float>(input), in, image_height,\r\n                    image_width, image_channels, wanted_height, wanted_width,\r\n                    wanted_channels, s);)\r\n \r\n\r\ntype::ResizeBilinear(GetTensorData<float>(input), GetTensorDims(input),   \\\r\n                       GetTensorData<int32>(size), GetTensorDims(size),     \\\r\n                       GetTensorData<float>(output), GetTensorDims(output), \\\r\n                       params->align_corners)\r\n\r\nbecause params is NULL\r\nI looked at code and i do not believe that this variable (builtin_data) ever initialized in this case because resize is called not within graph\r\n\r\n\r\n### Source code / logs\r\nuser@qds101:~/ml/tf_lite/tensorflow/tensorflow/contrib/lite/examples/label_image/exec$ ./label_image -v 1\r\nnnapi error: unable to open library libneuralnetworks.so\r\nLoaded model ./vgg16_conv1_opt.tflite\r\nresolved reporter\r\ntensors size: 6\r\nnodes size: 1\r\ninputs: 1\r\ninput(0) name: Placeholder\r\n0: Placeholder, 602112, 1, 0, 0\r\n1: conv1_1, 12845056, 1, 0, 0\r\n2: conv1_1/Conv2D_bias, 256, 1, 0, 0\r\n3: conv1_1/weights, 6912, 1, 0, 0\r\nlen: 150666\r\nwidth, height, channels: 224, 224, 3\r\ninput: 0\r\nnumber of inputs: 1\r\nnumber of outputs: 1\r\nInterpreter has 6 tensors and 1 nodes\r\nInputs: 0\r\nOutputs: 1\r\n\r\nTensor   0 kTfLiteFloat32  kTfLiteArenaRw     602112 bytes ( 0.6 MB)  1 224 224 3\r\n\r\nTensor   1 kTfLiteFloat32  kTfLiteArenaRw   12845056 bytes (12.2 MB)  1 224 224 64\r\n\r\nTensor   2 kTfLiteFloat32   kTfLiteMmapRo        256 bytes ( 0.0 MB)  64\r\n\r\nTensor   3 kTfLiteFloat32   kTfLiteMmapRo       6912 bytes ( 0.0 MB)  64 3 3 3\r\n\r\nTensor   4 kTfLiteFloat32  kTfLiteArenaRw    5419008 bytes ( 5.2 MB)  1 224 224 27\r\n\r\nTensor   5 kTfLiteFloat32  kTfLiteDynamic       6912 bytes ( 0.0 MB)  27 64\r\n\r\nNode   0 Operator Builtin Code   3\r\n  Inputs: 0 3 2\r\n  Outputs: 1\r\nparams = (nil) //my print\r\nSegmentation fault\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code", "This should be fixed by https://github.com/tensorflow/tensorflow/commit/1a0b637df8d082301118dd0f85ec63704f862aeb", "In that case closing. Thank you for report."]}, {"number": 17086, "title": "tf.Dimension raises TypeError for tf.DType", "body": "fix #17079.", "comments": ["jenkins test this please", "I have resolved the conflict, and could anyone help restart all tests?", "@skye @drpngx Excuse me, could you help restart all tests? Thanks.", "Thanks for pinging, sorry I missed the first email.", "@drpngx All tests pass. Thanks for your consistent help :)", "Woohoo!"]}, {"number": 17085, "title": "Spanish Translation", "body": "\r\nHello! I would like to translate the project into Spanish! is it already translated?\r\n", "comments": ["Duplicate #17044?", "@facaiy no, i dont know it", "I really want to contribute here with the translation, I have experience in the field of translations", "Yes, the idea sounds great, at least for me. However, we don't hear any plan for multiple language support. Please be patient and wait for reply from official tensorflower.", "Ok great! I'll wait for the answer!: :)", "We're seeing more and more people eager to contribute to translations (for example #16871).\r\n\r\nBut we don't have infrastructure to support this. We're looking into it.", "I also wish this project's translated document in Korean. \r\nI hope multiple language be supported one day :)\r\nThanks for the issue by the way. I was also curious about this issue."]}, {"number": 17084, "title": "Use relative and correct paths to get flatc and schema file.", "body": "Current visualize.py file uses wrong path for schema file.  In addition, it assume current directory is the top directory of tensorflow, which is inflexible.\r\n\r\nThis patch uses paths relative to the script itself, which makes it possible to invoke the script from anywhere in the file system.", "comments": ["You're right about the wrong path for the schema file. However, I keep getting a `third_party/flatbuffers/flatc: not found` error, no matter whether I fix only the first path or use your paths relative to the script file. Can you successfuly run the `visualize` script? Are you using the `bazel run` command or something else?", "@monkey-jsun, can you confirm @Johnson145's issue is resolved?", "Oops!  I missed the email notification for the @Johnson145 question.\r\n\r\nRegarding third_party/flatbuffers/flatc path, I use it only because existing code use this path as well.  I assumed there is a separate step to create this, but I cheated by just copying another copy I had to the path.   My patch basically changes it to relative path from anywhere inside the tree instead of top of the tree. \r\n\r\nYes, visualize.py works after the patch.  I simply just run the python script with input tflite file and output html file path. ", "OK, I dug in a little more and found out that flatc was generated under <tensorflow>/bazel-out/host/bin/external/flatbuffers/flatc\r\n\r\nWhat is the general guideline in tensorflow regarding install location of host tools?  The above path seems like a temporary output place, not the \"final\" location for flatc.\r\n\r\nI can easily change the patch to point to the above location instead of third_party/flatbuffers/flatc.  Just want to make sure this is the right thing to do.   Let me know.\r\n\r\n@Johnson145 @petewarden ", "but first of all, you may need bazel build tensorflow from source code instead of installing by pip straightly.", "Yes, I did build from source.  Not sure what you are trying to allude to...", "From what I can see, this script as currently written relies on some internal Google file locations. The changes seem to make it possible to run if you have built using Bazel, but I'm still a bit concerned that it won't work from a plain pip install. @aselle do you have any thoughts?", "Closing as this is quite old now."]}, {"number": 17083, "title": "tf.tile gradient supports IndexedSlice", "body": "Fix #16930", "comments": ["@mrry @cy89 Hi, could you take a look, or assign the PR to the corresponding reviewer? Thanks very much.", "@martinwicke Excuse me, do you know who could help review the PR? Thanks.", "@suharshs Could you take a look?", "I apply this fix manually every time I upgrade my tensorflow, it'd be helpful if it gets merged!", "@mrry Could you help assign it to a reviewer? Thanks.\r\n\r\n@talkhaldi Thanks for your patience :-)", "@drpngx Hi, could you give a hand? Thanks very much.", "I'm not familiar with this code, and so decline the review.", "Are you familiar enough to review this annarev@?", "Adding Zhifeng since I never actually worked on the _TileGrad function. He would know more about it.", "@zffchen78 Hi, the test case has been moved. Could you take a look again?", "Nagging Assignee @case540: It has been 18 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@annarev Hi, I think the PR is small, however it has been pending for a long time. Could you take a hand? Thanks :-)", "Hey @skye can you lend a hand? I saw gradient and thought of you. ", "many thanks, @martinwicke .\r\n\r\n@annarev sure, I have added a new test with rank 1 input."]}, {"number": 17082, "title": "Feature request: Add mode argument to discriminator function in tfgans GANEstimator", "body": "Can we add mode as an optional argument to the discriminator function in the tfgan GANEstimator? This would allow the discriminator function to perform batch normalisation and dropout.\r\nPosted this on stack overflow:\r\n\r\nhttps://stackoverflow.com/questions/48837537/why-cant-i-pass-mode-to-the-discriminator-function-in-tensorflows-ganestimator", "comments": ["Sorry just saw this feature has been completed"]}, {"number": 17081, "title": "keras multi_gpu_model broken going from 1.6.0-rc0 to 1.6.0-rc1", "body": "Have I written custom code: Yes\r\nOS Platform and Distribution: Linux Ubuntu 17.04\r\nTensorFlow installed from: source\r\nTensorFlow version: 1.6.0-rc1\r\nPython version: 3.6 \r\nBazel version: 0.10\r\nGCC/Compiler version: 6.0\r\nCUDA/cuDNN version: CUDA 9.1 cuDNN 7.0.5\r\nGPU model and memory: NVIDIA Titan Z 12GB\r\n`Exact` command to reproduce: multi_gpu_model(model, gpus=2)\r\n\r\nJust upgraded from rc0 to rc1 of release 1.6.0 and I'm now getting the following crash when running the multi_gpu_model function (was working fine with rc0):\r\n\r\n    parallel_model = multi_gpu_model(model, gpus=2)\r\n\r\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/utils/training_utils.py\", line 207, in multi_gpu_model\r\n\r\n    return Model(model.inputs, merged)\r\n\r\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py\", line 694, in __init__\r\n\r\n    self._init_graph_network(*args, **kwargs)\r\n\r\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py\", line 733, in _init_graph_network\r\n\r\n    if layer.is_placeholder:\r\n\r\nAttributeError: 'Lambda' object has no attribute 'is_placeholder'\r\n\r\nI rolled back to the 1.5 branch and I'm not having any issues running multi_gpu_model there.\r\n\r\n\r\n", "comments": ["Could you provide a minimal reproducible example?", "I just recompiled and reinstalled the 1.6.0-rc1 and the exact same code which produced the above error now seems to work perfectly. The only difference I can think of is last time I compiled 1.6 I used the compile flags \"-O3 -msse4.2 -mavx2 -mfma\" suggested in https://github.com/tensorflow/tensorflow/issues/17043 to prevent tf from seg faulting when run on a skylake CPU (which did fix that issue) I might try recompile again with these flags included but it seems like this might be a non-issue if 17043 gets fixed.", "Thanks for checking again. Then I will be closing this issue as a duplicate of #17043"]}, {"number": 17080, "title": "The method tf.graph_util.remove_training_nodes() is broken", "body": "On Windows 7 + tensorflow 1.4 the following code:\r\n\r\ntf.graph_util.remove_training_nodes(tf.get_default_graph())\r\n\r\nthrows out the following error message:\r\n\r\n......\r\n  File \"C:\\Program Files\\Python3\\lib\\site-packages\\tensorflow\\python\\framework\\graph_util_impl.py\", line 278, in remove_training_nodes\r\n    input_nodes = input_graph.node\r\nAttributeError: 'Graph' object has no attribute 'node'\r\n.....\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Here are more details about test:\r\n\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Windows 7, 64bit AMD CPU, GTX1070 Graphics card\r\nTensorFlow installed from: tensorflow_gpu-1.4.0-cp35-cp35m-win_amd64.whl\r\nTensorFlow version: 1.4\r\nBazel version: N/A\r\nCUDA/cuDNN version: \r\nGPU model and memory: 8 GB\r\nExact command to reproduce:  Custom code, but should be re-produceable with code in initial post, as it is static compilation error. \r\n", "Problem solved: the method remove_training_nodes() expected a GraphDef object. So, the following code works:\r\ntf.graph_util.remove_training_nodes(tf.get_default_graph().as_graph_def())"]}, {"number": 17079, "title": "tf.zeros should raise an error when passed a DType as shape", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NA\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab, colab.research.google.com\r\n- **TensorFlow installed from (source or binary)**:  unsure\r\n- **TensorFlow version (use command below)**: 1.6.0-rc1\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n```python\r\nimport tensorflow as tf\r\ntf.InteractiveSession()\r\ntf.zeros(tf.string).eval()\r\n```\r\n\r\n### Describe the problem\r\n\r\nThis returns a NumPy array with seven elements?!?\r\n```python\r\narray([0., 0., 0., 0., 0., 0., 0.], dtype=float32)\r\n```\r\n\r\nOf course, I mixed up the shape and dtype arguments to tf.zeros, The proper usage is `tf.zeros((), tf.string))`. So this should really raise TypeError (or maybe ValueError).", "comments": ["`tf.string` can be converted to integer, that's where the bug comes from:\r\n\r\n```python\r\nIn [2]: tf.string.__int__()\r\nOut[2]: 7\r\n\r\nIn [3]: int(tf.string)\r\nOut[3]: 7\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/00ff4912d49cc09737fd1425df6d877f29f69741/tensorflow/python/framework/dtypes.py#L288-L289\r\n", "OK, this might be a little harder to fix then I thought then.\r\n\r\nI'd argue that dtypes shouldn't be coercible into integers, but this won't be an easy thing to fix from a backwards compatibility perspective.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 17078, "title": "MKL: Updating performance guide with MKL info", "body": "Updating documentation wrt MKL", "comments": ["@indie Here is the public PR", "Closing. Need to fix a few things."]}, {"number": 17077, "title": "Use relative and correct path to find flatc and schema file.", "body": "The path to schema file is wrong and path to flatc is only correct from the top directory.\r\n\r\nThe proposed change uses relative paths to the script file to find those two files, which are more stable.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I have signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 17075, "title": "Compiling TensorFlow on Windows with NVIDIA 9.1 GPU?", "body": "Does anyone have this working, prior to 1.3.0 the nightly builds at tensorflow had GPU builds, these no longer appear to be available.\r\n\r\nI have the build running and makes it through all tests until the 'end', There are multiple errors shown below.  After reviewing other posts it appears that the GPU build with Windows is not working? \r\n\r\ncmake .. ^\r\n-A x64 ^\r\n-DCMAKE_BUILD_TYPE=Release ^\r\n-DSWIG_EXECUTABLE=%SWIGEXE% ^\r\n-DPYTHON_EXECUTABLE=%PYEXE% ^\r\n-DPYTHON_LIBRARIES=%PYLIB% ^\r\n-Dtensorflow_ENABLE_GPU=ON ^\r\n-DCUDNN_HOME=%CUDNNH% ^\r\n-Dtensorflow_BUILD_PYTHON_TESTS=OFF ^\r\n-Dtensorflow_BUILD_CC_TESTS=OFF ^\r\n-Dtensorflow_TF_NIGHTLY=OFF ^\r\n-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 ^\r\n-Dtensorflow_CUDA_VERSION=9.1 ^\r\n-Dtensorflow_ENABLE_GPU=ON\r\n\r\nmsbuild.exe ^\r\n/p:Configuration=Release ^\r\n/maxcpucount ^\r\n/verbosity:minimal ^\r\n/fileLogger ^\r\n/fileloggerparameters:logfile=%TFDIR%\\tf_msbuild.log ^\r\ntf_python_build_pip_package.vcxproj\r\n\r\n\r\n     3>Done building target \"FinalizeBuildStatus\" in project \"pywrap_tensorflow_internal_static.vcxproj\".\r\n     3>Target \"Build\" in file \"C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.BuildSteps.Targets\" from project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal_static.vcxproj\" (entry point):\r\n     3>Done building target \"Build\" in project \"pywrap_tensorflow_internal_static.vcxproj\".\r\n     3>Done Building Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal_static.vcxproj\" (default targets).\r\n     2>Done executing task \"MSBuild\" -- FAILED.\r\n     2>Done building target \"ResolveProjectReferences\" in project \"pywrap_tensorflow_internal.vcxproj\" -- FAILED.\r\n     2>Done Building Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (default targets) -- FAILED.\r\n   235>Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\_periodic_resample_op.vcxproj\" (235) is building \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (2:2) on node 1 (default targets).\r\n   2:2>Building with tools version \"14.0\".\r\n       Target \"_CheckForInvalidConfigurationAndPlatform\" skipped. Previously built successfully.\r\n       Target \"Build\" skipped. Previously built unsuccessfully.\r\n   2:2>Done Building Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (default targets) -- FAILED.\r\n   234>Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\_nearest_neighbor_ops.vcxproj\" (234) is building \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (2:3) on node 1 (default targets).\r\n   2:3>Building with tools version \"14.0\".\r\n       Target \"_CheckForInvalidConfigurationAndPlatform\" skipped. Previously built successfully.\r\n       Target \"Build\" skipped. Previously built unsuccessfully.\r\n   2:3>Done Building Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (default targets) -- FAILED.\r\n   233>Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\_lstm_ops.vcxproj\" (233) is building \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (2:4) on node 1 (default targets).\r\n   2:4>Building with tools version \"14.0\".\r\n       Target \"_CheckForInvalidConfigurationAndPlatform\" skipped. Previously built successfully.\r\n       Target \"Build\" skipped. Previously built unsuccessfully.\r\n   2:4>Done Building Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (default targets) -- FAILED.\r\n   232>Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\_gru_ops.vcxproj\" (232) is building \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (2:5) on node 1 (default targets).\r\n   2:5>Building with tools version \"14.0\".\r\n       Target \"_CheckForInvalidConfigurationAndPlatform\" skipped. Previously built successfully.\r\n       Target \"Build\" skipped. Previously built unsuccessfully.\r\n   2:5>Done Building Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (default targets) -- FAILED.\r\n     9>Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\_beam_search_ops.vcxproj\" (9) is building \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (2:6) on node 1 (default targets).\r\n   2:6>Building with tools version \"14.0\".\r\n       Target \"_CheckForInvalidConfigurationAndPlatform\" skipped. Previously built successfully.\r\n       Target \"Build\" skipped. Previously built unsuccessfully.\r\n   2:6>Done Building Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (default targets) -- FAILED.\r\n   235>Done executing task \"MSBuild\" -- FAILED.\r\n   235>Done building target \"ResolveProjectReferences\" in project \"_periodic_resample_op.vcxproj\" -- FAILED.\r\n   235>Done Building Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\_periodic_resample_op.vcxproj\" (default targets) -- FAILED.\r\n   234>Done executing task \"MSBuild\" -- FAILED.\r\n   234>Done building target \"ResolveProjectReferences\" in project \"_nearest_neighbor_ops.vcxproj\" -- FAILED.\r\n   234>Done Building Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\_nearest_neighbor_ops.vcxproj\" (default targets) -- FAILED.\r\n   233>Done executing task \"MSBuild\" -- FAILED.\r\n   233>Done building target \"ResolveProjectReferences\" in project \"_lstm_ops.vcxproj\" -- FAILED.\r\n   233>Done Building Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\_lstm_ops.vcxproj\" (default targets) -- FAILED.\r\n   232>Done executing task \"MSBuild\" -- FAILED.\r\n   232>Done building target \"ResolveProjectReferences\" in project \"_gru_ops.vcxproj\" -- FAILED.\r\n   232>Done Building Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\_gru_ops.vcxproj\" (default targets) -- FAILED.\r\n     9>Done executing task \"MSBuild\" -- FAILED.\r\n     9>Done building target \"ResolveProjectReferences\" in project \"_beam_search_ops.vcxproj\" -- FAILED.\r\n     9>Done Building Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\_beam_search_ops.vcxproj\" (default targets) -- FAILED.\r\n     7>Done executing task \"MSBuild\" -- FAILED.\r\n     7>Done building target \"ResolveProjectReferences\" in project \"tf_extension_ops.vcxproj\" -- FAILED.\r\n     7>Done Building Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_extension_ops.vcxproj\" (default targets) -- FAILED.\r\n     1>Done executing task \"MSBuild\" -- FAILED.\r\n     1>Done building target \"ResolveProjectReferences\" in project \"tf_python_build_pip_package.vcxproj\" -- FAILED.\r\n     1>Done Building Project \"C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default targets) -- FAILED.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", ":: - MSVC Community 2015\r\n:: - ANACONDA 4.4.4 (Python 3.5.5)\r\n:: - CMake 3.10.2\r\n:: - SWIG 3.0.12\r\n:: - GIT 2.15.1.windows.2\r\n:: - NVIDIA CUDA 9.1, CUDNN 7.05\r\n:: No BAZEL\r\n:: TensorFlow version (latest from GIT repository)", "Closing as this is a duplicate"]}, {"number": 17074, "title": "Add support for Maximum to TensorFlowLite.", "body": "I don't have that many tests (e.g., not sure if broadcasting works), but running `bazel test tensorflow/contrib/lite/kernels:maximum_test`, `bazel test tensorflow/contrib/lite/toco/tflite:operator_test`, and `bazel test //tensorflow/contrib/lite/testing:generated_examples_zip_test` all passed.\r\n\r\nTo help close #14661.", "comments": ["Can you please resolve the conflicts?", "@ekelsen , I have tried to update it and am currently running the tests, but it seems to be failing to due some other recent changes with `make_lstm_tests` (i.e., I just tried the same test on the `tensorflow/tensorflow` master banch):\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/s.antol/.cache/bazel/_bazel_s.antol/d5f2817d0f58daa7ca384d9ca065c3f9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/lite/testing/generate_examples.runfiles/org_tensorflow/tensorflow/contrib/lite/testing/generate_examples.py\", line 1950, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/s.antol/.cache/bazel/_bazel_s.antol/d5f2817d0f58daa7ca384d9ca065c3f9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/lite/testing/generate_examples.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/s.antol/.cache/bazel/_bazel_s.antol/d5f2817d0f58daa7ca384d9ca065c3f9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/lite/testing/generate_examples.runfiles/org_tensorflow/tensorflow/contrib/lite/testing/generate_examples.py\", line 1936, in main\r\n    dispatch[out](_path(out))\r\n  File \"/home/s.antol/.cache/bazel/_bazel_s.antol/d5f2817d0f58daa7ca384d9ca065c3f9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/lite/testing/generate_examples.runfiles/org_tensorflow/tensorflow/contrib/lite/testing/generate_examples.py\", line 1839, in make_lstm_tests\r\n    use_frozen_graph=True)\r\n  File \"/home/s.antol/.cache/bazel/_bazel_s.antol/d5f2817d0f58daa7ca384d9ca065c3f9/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/lite/testing/generate_examples.runfiles/org_tensorflow/tensorflow/contrib/lite/testing/generate_examples.py\", line 494, in make_zip_of_tests\r\n    \"Found %d errors while generating toco models\" % toco_errors)\r\nRuntimeError: Found 1 errors while generating toco models\r\nTarget //tensorflow/contrib/lite/testing:generated_examples_zip_test failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 366.084s, Critical Path: 92.76s\r\nFAILED: Build did NOT complete successfully\r\n\r\nExecuted 0 out of 1 test: 1 fails to build.\r\n```\r\nEdit\r\n---\r\nBuried between a bunch of warning messages, I found the actual error output:\r\n```\r\nWarning: cgi.escape is deprecated, use html.escape instead\r\n  fp.write(\"  <td>%s</td>\\n\" % cgi.escape(repr(params[p]), quote=True))\r\n/home/s.antol/.cache/bazel/_bazel_s.antol/3aa07abb655104a7d2d10105ebf2a120/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/lite/testing/generate_examples.runfiles/org_tensorflow/tensorflow/contrib/lite/testing/generate_examples_report.py:48: DeprecationWarning: cgi.escape is deprecated, use html.escape instead\r\n  s = cgi.escape(repr(x), quote=True)\r\n/home/s.antol/.cache/bazel/_bazel_s.antol/3aa07abb655104a7d2d10105ebf2a120/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/lite/testing/generate_examples.runfiles/org_tensorflow/tensorflow/contrib/lite/testing/generate_examples_report.py:48: DeprecationWarning: cgi.escape is deprecated, use html.escape instead\r\n  s = cgi.escape(repr(x), quote=True)\r\nERROR: /home/s.antol/tools/tensorflow/tensorflow/contrib/lite/testing/BUILD:16:1: Executing genrule //tensorflow/contrib/lite/testing:optest_lstm.zip.files failed (Exit 1)\r\nConverted 2 variables to const ops.\r\n-----------------\r\ntoco error!\r\nbazel-out/host/bin/tensorflow/contrib/lite/toco/toco --input_file=/tmp/tmpglb5uq8u --output_file=/tmp/tmpnuxkzqr4  --input_data_types=FLOAT --inference_type=FLOAT --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_arrays=split_0 --input_shapes=1,3 --output_arrays=rnn/basic_lstm_cell/Mul_2 --rnn_states='{state_array:rnn/BasicLSTMCellZeroState/zeros,back_edge_source_array:rnn/basic_lstm_cell/Add_1,size:4},{state_array:rnn/BasicLSTMCellZeroState/zeros_1,back_edge_source_array:rnn/basic_lstm_cell/Mul_2,size:4}' > /tmp/tmp5h5b4t7r 2>&1exited with code 34304\r\n------------------\r\n./tensorflow/contrib/lite/toco/args.h:170 StringMapList arguments not supported\r\nAborted (core dumped)\r\n\r\n-----------------\r\n\r\n/home/s.antol/.cache/bazel/_bazel_s.antol/3aa07abb655104a7d2d10105ebf2a120/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/lite/testing/generate_examples.runfiles/org_tensorflow/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\r\n```\r\n---\r\n\r\nAny suggestions?", "@StanislawAntol  Anything I can help with?\r\n\r\nIt looks like StringMapList is not supported outside of google. Sorry about that. It may be easiest to just disable the make_lstm_tests.\r\n\r\n", "I just pushed some commits that fixed any merge issues and pass the tests (commenting out the LSTM one).", "@yifeif , @andrehentz \r\nIs there anything I can do to speed up getting this merged into master so it does not need another set of merge conflicts?", "infra issue, rerunning", "I'm having trouble running a model converted with the code above to a tflite after modifying the tflite demo app for android (also a tiny-yolo model).  I'm guessing that I need to use the same tf java and c++ code when I build the app as when I converted the model?", "@frankcarey , yes, you will need to update the build system such that it includes and builds the maximum.cc file (we were building for Android via CMake).", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "@StanislawAntol Can I do anything to help here?", "@andrehentz , I've just been a bit busy at my new job and haven't gotten around to fixing the conflicts and dealing with the CLA issue.", "CLAs look good, thanks!\n\n<!-- ok -->", "@andrehentz , I updated everything and the 3 tests are passing for me.", "@andrehentz , @yifeif Can one of you launch the build checks?"]}, {"number": 17073, "title": "Python API Fix", "body": "Use _ for system packages so sealing doesn't break the API.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Yes, \"I'm ok with my commits being contributed to this project\". See https://github.com/tensorflow/tensorflow/pull/15890.", "@aselle  Looks like you just need to fix the conflict and call this done.  I checked and I am fairly sure CLA is all covered and we have a note from the secondary author.", "This is now fixed in 1.8.0. So I think we can close this."]}, {"number": 17072, "title": "Branch 186010810", "body": "", "comments": []}, {"number": 17071, "title": "Windows Build of Tensorflow, CMakeLists.txt error", "body": "The Cmake error is displayed when check for native architecture. The following change reports success for the same test on Windows.\r\n\r\n```\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Success\r\n```\r\n\r\n```\r\n:: EDIT %CMAKED%\\CMakeLists.txt to replace -march=native with /arch:AVX2 or if older CPU with one of [AVX2|FMA|SSE4.2|FPMATH|MMX] \r\nif (tensorflow_OPTIMIZE_FOR_NATIVE_ARCH)\r\n  include(CheckCXXCompilerFlag)\r\n  CHECK_CXX_COMPILER_FLAG(\"/arch:AVX2\" COMPILER_OPT_ARCH_NATIVE_SUPPORTED)\r\n  if (COMPILER_OPT_ARCH_NATIVE_SUPPORTED)\r\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /arch:AVX2\")\r\n  endif()\r\nendif()\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", ":: - MSVC Community 2015\r\n:: - ANACONDA 4.4.4 (Python 3.5.5)\r\n:: - CMake 3.10.2\r\n:: - SWIG 3.0.12\r\n:: - GIT 2.15.1.windows.2\r\n:: - NVIDIA CUDA 9.1, CUDNN 7.05\r\n:: No BAZEL\r\n:: TensorFlow version (latest from GIT repository)", "I apologize but I am having a hard time understanding what the problem is, and where the problem is. Can you describe this issue in more details?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Please close, Tensorflow with GPU is not supported on Windows.", "@apiszcz I think compiling Tensorflow from source on Windows is not _officially_ supported, but you can make it work nontheless. There are some instructions in the [README file](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake) how to enable GPU support and here is a nice [tutorial as well](https://joe-antognini.github.io/machine-learning/windows-tf-project).", "Has anyone compiled TensorFlow 1.5 or 1.6 on Windows 10 with GPU support?", "I did, although the compilation time was huge (something like 18 hours). However, I do have some problems with using it in another C++ project (check this [StackOverflow post](https://stackoverflow.com/questions/49496758/cannot-link-to-self-built-tensorflow-on-windows-with-visual-studio-when-wholear)), but I don't know whether this is related to how I compiled it. I was able to successfully run some provided example scripts."]}, {"number": 17070, "title": "add model restore support for tree and forest variables", "body": "Right now both tree variables and forest variables only support training.\r\n\r\nThis pr makes it possible for supporting  restore a tensor forest model from a serialized string", "comments": ["Ah... hey, yeah... Thanks!  Let me add some test cases \r\nAnd the intention was to use tensor_forest without train with it. I was using it here https://github.com/yupbank/tree_to_tensorflow\r\n\r\nbut let me add some test cases for sure", "@nataliaponomareva hey, i have added one test case, is that helping ? ", "@martinwicke  hey do you know what's the status of this?", "There were test failures, but they do look unrelated (clicking through, you'd eventually get to e.g., https://source.cloud.google.com/results/invocations/c80640b0-9ec3-475c-b001-03b96833a3ca/targets/%2F%2Ftensorflow%2Fpython%2Fkernel_tests:init_ops_test/log)\r\n\r\nIn such a case, you can simply trigger the tests again, or if you're very sure, you can request it be merged anyway (probably not an issue in this case). \r\n\r\nI triggered the tests again, assuming they pass, assign me and I'll merge this.", "@martinwicke  i think the final failure has nothing to do with this pr... "]}, {"number": 17069, "title": "tf.cast() can't cast string to number", "body": "OS: Win10 64bit\r\nTensorflow version: 1.5.0\r\n\r\nIt looks like tf.cast() can't cast string to number,  but the documents doesn't explain it.\r\nI found this problem when I migrated older versions of tensorflow code to a new version.\r\n\r\nWhen I used tf.cast() to cast string to float, I got error \"Unimplemented: Cast string to float is not supported\". \r\nAnd when I used tf.string_to_number () instead of tf.cast (), the problem was solved.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 17068, "title": "Get variable mapping (dictionary) after checkpoint restore", "body": "It is a feature request.\r\n\r\nPlease provide a way to get the list of Tensorflow variables restored from a checkpoint and corresponding Python variables.\r\n\r\nMore info:\r\n\r\nConsider that you have this code:\r\n\r\n```\r\n    matrix_1 = tf.Variable([[1, 2], [3, 4]], name=\"matrix1\")\r\n    matrix_2 = tf.Variable([[5, 6], [7, 8]], name=\"matrix2\")\r\n```\r\n\r\nWhen a tensorflow checkpoint is created it has matrix1 and matrix2 variables saved. It is easy to understand that matrix1 (in TF namespace) corresponds to matrix_1 (in Python namespace).\r\n\r\nNow consider that the code is:\r\n\r\n```\r\n    matrix_1 = tf.Variable([[1, 2], [3, 4]])\r\n    matrix_2 = tf.Variable([[5, 6], [7, 8]])\r\n```\r\n\r\nA tensorflow checkpoint will have Variable_1:0 and Variable_2:0 variables saved. And it is not obvious at all that Variable_1:0 (in TF namespace) corresponds to matrix_1 (in Python namespace). Especially if there are dozens of variables.\r\n\r\nThere should be a way to get a list of corresponding variables. For example:\r\n```\r\nVariable_1 = matrix_1\r\nVariable_2 = matrix_2\r\n...\r\nVariable_100 = relu_1_weights\r\n```\r\n\r\n\r\n", "comments": ["@asimshankar is there already a way to do this; might it be a reasonable feature request, and/or should I mark \"contributions welcome\"? ", "@Yagun : The name of the variable as far as TensorFlow is concerned is available via the `.name` property. For example:\r\n\r\n```python\r\nmatrix_1 = tf.Variable([[1, 2], [3, 4]])\r\nmatrix_2 = tf.Variable([[5, 6], [7, 8]])\r\nprint(matrix_1.name)  # Will print \"Variable:0\"\r\nprint(matrix_2.name) # Will print \"Variable_1:0\"\r\n```\r\n\r\nIs that sufficient? If not, could you provide some more detail on what you'd like?\r\n\r\nThanks.", "@asimshankar, thank you very much for your answer. It is definitely useful.\r\n\r\nBut, unfortunately, it is not enough. For example, I downloaded a project from GitHub. That project contains Python files and checkpoint files. Their author did not use tf.Variable() at all. But the checkpoint contains 48 variables (all of them are Variable_{number_here}). Is there a way to get a list of the variables and their corresponding Python names?\r\n\r\nThanks.", "@Yagun : That is in general not possible. For example, consider the following Python code:\r\n\r\n```python\r\ndef f(x):\r\n  v = tf.Variable(1.)\r\n  return v + 1\r\n\r\ndef g(x):\r\n  v = tf.Variable(2.)\r\n  return v + 1\r\n\r\ny = f(g(tf.constant(2.))\r\n```\r\n\r\nIn this, the Python variable is `v` for two distinct model parameters.\r\n\r\nThe checkpoint itself is written in a language independent manner, it has no association with Python, it corresponds to a TensorFlow graph, however that graph was created. Furthermore, the association between Python variables and TensorFlow variables is not well defined in general (as in the example above).\r\n\r\nYour best bet would be to inspect the Python code you're using and annotate it if needed.\r\n\r\nHope that helps."]}, {"number": 17067, "title": "Build Error Windows, No results found for more than one instance of overloaded function \"google::protobuf::Arena::CreateM essageInternal\".", "body": "No results found for more than one instance of overloaded function \"google::protobuf::Arena::CreateM essageInternal\".\r\n\r\n:: - MSVC Community 2015\r\n:: - ANACONDA 4.4.4 (Python 3.5.5)\r\n:: - CMake 3.10.2\r\n:: - SWIG 3.0.12\r\n:: - GIT 2.15.1.windows.2\r\n:: - NVIDIA CUDA 9.1, CUDNN 7.05\r\n\r\nTensorFlow pulled from git repo on 2/12/2018\r\n==\r\n\r\nC:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/arena.h(719): error : more than one instance of overloaded function \"google::protobuf::Arena::CreateM\r\nessageInternal\" matches the argument list: [C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n              function template \"T *google::protobuf::Arena::CreateMessageInternal<T>(google::protobuf::Arena *)\"\r\n              function template \"T *google::protobuf::Arena::CreateMessageInternal<T,Args...>(Args &&...)\"\r\n              argument types are: (google::protobuf::Arena *)\r\n            detected during:\r\n              instantiation of \"Msg *google::protobuf::Arena::CreateMaybeMessage<Msg>(google::protobuf::Arena *, google::protobuf::internal::true_type) [with Msg=tensorflow::TensorShapeProto_Dim]\"\r\n  (729): here\r\n              instantiation of \"T *google::protobuf::Arena::CreateMaybeMessage<T>(google::protobuf::Arena *) [with T=tensorflow::TensorShapeProto_Dim]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(648): here\r\n              instantiation of \"GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::New(google::protobuf::Arena *) [with GenericType=tensorflow::TensorShapeProto_Dim]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(675): here\r\n              instantiation of \"GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::NewFromPrototype(const GenericType *, google::protobuf::Arena *) [with GenericType=tensorflow::\r\n  TensorShapeProto_Dim]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(1554): here\r\n              instantiation of \"TypeHandler::Type *google::protobuf::internal::RepeatedPtrFieldBase::Add<TypeHandler>(TypeHandler::Type *) [with TypeHandler=google::protobuf::RepeatedPtrField<tensorflo\r\n  w::TensorShapeProto_Dim>::TypeHandler]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(2001): here\r\n              instantiation of \"Element *google::protobuf::RepeatedPtrField<Element>::Add() [with Element=tensorflow::TensorShapeProto_Dim]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build\\tensorflow/core/framework/tensor_shape.pb.h(471): here\r\n\r\nC:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/arena.h(719): error : more than one instance of overloaded function \"google::protobuf::Arena::CreateM\r\nessageInternal\" matches the argument list: [C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n              function template \"T *google::protobuf::Arena::CreateMessageInternal<T>(google::protobuf::Arena *)\"\r\n              function template \"T *google::protobuf::Arena::CreateMessageInternal<T,Args...>(Args &&...)\"\r\n  cwise_op_bitwise_and.cc\r\n              argument types are: (google::protobuf::Arena *)\r\n            detected during:\r\n              instantiation of \"Msg *google::protobuf::Arena::CreateMaybeMessage<Msg>(google::protobuf::Arena *, google::protobuf::internal::true_type) [with Msg=tensorflow::ResourceHandleProto]\"\r\n  (729): here\r\n              instantiation of \"T *google::protobuf::Arena::CreateMaybeMessage<T>(google::protobuf::Arena *) [with T=tensorflow::ResourceHandleProto]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(648): here\r\n              instantiation of \"GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::New(google::protobuf::Arena *) [with GenericType=tensorflow::ResourceHandleProto]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(675): here\r\n              instantiation of \"GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::NewFromPrototype(const GenericType *, google::protobuf::Arena *) [with GenericType=tensorflow::\r\n  ResourceHandleProto]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(1554): here\r\n              instantiation of \"TypeHandler::Type *google::protobuf::internal::RepeatedPtrFieldBase::Add<TypeHandler>(TypeHandler::Type *) [with TypeHandler=google::protobuf::RepeatedPtrField<tensorflo\r\n  w::ResourceHandleProto>::TypeHandler]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(2001): here\r\n              instantiation of \"Element *google::protobuf::RepeatedPtrField<Element>::Add() [with Element=tensorflow::ResourceHandleProto]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build\\tensorflow/core/framework/tensor.pb.h(1091): here\r\n\r\nC:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/arena.h(719): error : more than one instance of overloaded function \"google::protobuf::Arena::CreateM\r\nessageInternal\" matches the argument list: [C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n              function template \"T *google::protobuf::Arena::CreateMessageInternal<T>(google::protobuf::Arena *)\"\r\n              function template \"T *google::protobuf::Arena::CreateMessageInternal<T,Args...>(Args &&...)\"\r\n              argument types are: (google::protobuf::Arena *)\r\n            detected during:\r\n              instantiation of \"Msg *google::protobuf::Arena::CreateMaybeMessage<Msg>(google::protobuf::Arena *, google::protobuf::internal::true_type) [with Msg=tensorflow::VariantTensorDataProto]\"\r\n  (729): here\r\n              instantiation of \"T *google::protobuf::Arena::CreateMaybeMessage<T>(google::protobuf::Arena *) [with T=tensorflow::VariantTensorDataProto]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(648): here\r\n              instantiation of \"GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::New(google::protobuf::Arena *) [with GenericType=tensorflow::VariantTensorDataProto]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(675): here\r\n              instantiation of \"GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::NewFromPrototype(const GenericType *, google::protobuf::Arena *) [with GenericType=tensorflow::\r\n  VariantTensorDataProto]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(1554): here\r\n              instantiation of \"TypeHandler::Type *google::protobuf::internal::RepeatedPtrFieldBase::Add<TypeHandler>(TypeHandler::Type *) [with TypeHandler=google::protobuf::RepeatedPtrField<tensorflo\r\n  w::VariantTensorDataProto>::TypeHandler]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(2001): here\r\n              instantiation of \"Element *google::protobuf::RepeatedPtrField<Element>::Add() [with Element=tensorflow::VariantTensorDataProto]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build\\tensorflow/core/framework/tensor.pb.h(1121): here\r\n\r\nC:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/arena.h(719): error : more than one instance of overloaded function \"google::protobuf::Arena::CreateM\r\nessageInternal\" matches the argument list: [C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n              function template \"T *google::protobuf::Arena::CreateMessageInternal<T>(google::protobuf::Arena *)\"\r\n              function template \"T *google::protobuf::Arena::CreateMessageInternal<T,Args...>(Args &&...)\"\r\n              argument types are: (google::protobuf::Arena *)\r\n            detected during:\r\n              instantiation of \"Msg *google::protobuf::Arena::CreateMaybeMessage<Msg>(google::protobuf::Arena *, google::protobuf::internal::true_type) [with Msg=tensorflow::TensorProto]\"\r\n  (729): here\r\n              instantiation of \"T *google::protobuf::Arena::CreateMaybeMessage<T>(google::protobuf::Arena *) [with T=tensorflow::TensorProto]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(648): here\r\n              instantiation of \"GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::New(google::protobuf::Arena *) [with GenericType=tensorflow::TensorProto]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(675): here\r\n              instantiation of \"GenericType *google::protobuf::internal::GenericTypeHandler<GenericType>::NewFromPrototype(const GenericType *, google::protobuf::Arena *) [with GenericType=tensorflow::\r\n  TensorProto]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(1554): here\r\n              instantiation of \"TypeHandler::Type *google::protobuf::internal::RepeatedPtrFieldBase::Add<TypeHandler>(TypeHandler::Type *) [with TypeHandler=google::protobuf::RepeatedPtrField<tensorflo\r\n  w::TensorProto>::TypeHandler]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/repeated_field.h(2001): here\r\n              instantiation of \"Element *google::protobuf::RepeatedPtrField<Element>::Add() [with Element=tensorflow::TensorProto]\"\r\n  C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build\\tensorflow/core/framework/tensor.pb.h(1365): here\r\n\r\n  4 errors detected in the compilation of \"C:/Users/user/AppData/Local/Temp/tmpxft_0002aa34_00000000-12_adjust_contrast_op_gpu.cu.compute_52.cpp1.ii\".", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", ":: - MSVC Community 2015\r\n:: - ANACONDA 4.4.4 (Python 3.5.5)\r\n:: - CMake 3.10.2\r\n:: - SWIG 3.0.12\r\n:: - GIT 2.15.1.windows.2\r\n:: - NVIDIA CUDA 9.1, CUDNN 7.05\r\n:: No BAZEL\r\n:: TensorFlow version (latest from GIT repository)", "I get the same error. can you help me?\r\n\r\nOS Platform and Distribution  windows8.2 msys2\r\n TensorFlow installed from source\r\n TensorFlow version   r1.6\r\n Bazel version   0.10.0\r\n CUDA/cuDNN version  cuda 9.1  cuDNN 7.0\r\n GPU model and memory  NVIDIA GeForce GTX 850\r\n Exact command to reproduce  \r\n           bazel build -c opt --action_env=USE_MSVC_WRAPPER=1  --config=win-cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n", "After a few more trials, i am seeing that error 4 times, \r\n\r\nmore than one instance of overloaded function \"google::protobuf::Arena::CreateMessageInternal\" \r\n\r\nHere is the code section from arena.h: (line 719 called out in the error message) in the first return below\r\n```\r\n  // CreateMessage<T> requires that T supports arenas, but this private method\r\n  // works whether or not T supports arenas. These are not exposed to user code\r\n  // as it can cause confusing API usages, and end up having double free in\r\n  // user code. These are used only internally from LazyField and Repeated\r\n  // fields, since they are designed to work in all mode combinations.\r\n  template <typename Msg> GOOGLE_PROTOBUF_ATTRIBUTE_ALWAYS_INLINE\r\n  static Msg* CreateMaybeMessage(Arena* arena, google::protobuf::internal::true_type) {\r\n    return CreateMessageInternal<Msg>(arena);\r\n  }\r\n\r\n  template <typename T> GOOGLE_PROTOBUF_ATTRIBUTE_ALWAYS_INLINE\r\n  static T* CreateMaybeMessage(Arena* arena, google::protobuf::internal::false_type) {\r\n    return CreateInternal<T>(arena);\r\n  }\r\n\r\n  template <typename T> GOOGLE_PROTOBUF_ATTRIBUTE_ALWAYS_INLINE\r\n  static T* CreateMaybeMessage(Arena* arena) {\r\n    return CreateMaybeMessage<T>(arena, is_arena_constructable<T>());\r\n  }\r\n\r\n  // Just allocate the required size for the given type assuming the\r\n  // type has a trivial constructor.\r\n  template<typename T> GOOGLE_PROTOBUF_ATTRIBUTE_ALWAYS_INLINE\r\n  T* CreateInternalRawArray(size_t num_elements) {\r\n    GOOGLE_CHECK_LE(num_elements,\r\n             std::numeric_limits<size_t>::max() / sizeof(T))\r\n        << \"Requested size is too large to fit into size_t.\";\r\n    const size_t n = internal::AlignUpTo8(sizeof(T) * num_elements);\r\n    // Monitor allocation if needed.\r\n    AllocHook(RTTI_TYPE_ID(T), n);\r\n    return static_cast<T*>(impl_.AllocateAligned(n));\r\n  }\r\n\r\n```\r\n\r\n\r\n         ```\r\nC:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/arena.h(719): error : more than one instance of overloaded function \"google::protobuf::Arena::CreateMessageInternal\" matches the argument list: [C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/arena.h(719): error : more than one instance of overloaded function \"google::protobuf::Arena::CreateMessageInternal\" matches the argument list: [C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/arena.h(719): error : more than one instance of overloaded function \"google::protobuf::Arena::CreateMessageInternal\" matches the argument list: [C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src\\google/protobuf/arena.h(719): error : more than one instance of overloaded function \"google::protobuf::Arena::CreateMessageInternal\" matches the argument list: [C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n```\r\n", "The four templates for CreateMessageInternal appear at 852 - 881 in arena.h.\r\n\r\nThere are four argument signatures:\r\n854:(Args&&... args)\r\n862:()\r\n869:(const Arg& arg)\r\n877:(const Arg1& arg1, const Arg2& arg2)\r\n", "File: tf_core_gpu_kernels.vcxproj code block 235-247\r\n\r\n```\r\ncd C:\\g\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\CMakeFiles\\tf_core_gpu_kernels.dir\\__\\__\\core\\kernels\r\nif %errorlevel% neq 0 goto :cmEnd\r\nC:\r\nif %errorlevel% neq 0 goto :cmEnd\r\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -E make_directory C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/$(Configuration)\r\nif %errorlevel% neq 0 goto :cmEnd\r\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -D verbose:BOOL=OFF -D \"CCBIN:PATH=$(VCInstallDir)bin\" -D build_configuration:STRING=$(ConfigurationName) -D generated_file:STRING=C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/$(Configuration)/tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj -D generated_cubin_file:STRING=C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/$(Configuration)/tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.cubin.txt -P C:/g/tensorflow/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.Debug.cmake\r\nif %errorlevel% neq 0 goto :cmEnd\r\n:cmEnd\r\nendlocal &amp; call :cmErrorLevel %errorlevel% &amp; goto :cmDone\r\n:cmErrorLevel\r\nexit /b %1\r\n:cmDone\r\n```", "Is there conflict between bazel vesion and tensorflow\r\n This build done on Windows 8 , Is there GPU support?\r\n\r\nc:\\temp\\_bazel_nin\\aki74rxt\\execroot\\org_tensorflow\\external\\eigen_archiv                                                                                            e\\eigen\\src/Core/ArrayWrapper.h(94): warning: __declspec attributes ignored\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(57): warning: integer                                                                                             conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(304): warning: intege                                                                                            r conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(305): warning: intege                                                                                            r conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/map.h(1030): warning: invalid frie                                                                                            nd declaration\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/map_entry_lite.h(151): warning: ex                                                                                            ception specification for virtual function \"google::protobuf::internal::MapEntry                                                                                            Impl<Derived, Base, Key, Value, kKeyFieldType, kValueFieldType, default_enum_val                                                                                            ue>::~MapEntryImpl [with Derived=T, Base=google::protobuf::MessageLite, Key=Key,                                                                                             Value=Value, kKeyFieldType=kKeyFieldType, kValueFieldType=kValueFieldType, defa                                                                                            ult_enum_value=default_enum_value]\" is incompatible with that of overridden func                                                                                            tion \"google::protobuf::MessageLite::~MessageLite\"\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena.h(719): error: more than one                                                                                             instance of overloaded function \"google::protobuf::Arena::CreateMessageInternal                                                                                            \" matches the argument list:\r\n            function template \"T *google::protobuf::Arena::CreateMessageInternal                                                                                            <T>(google::protobuf::Arena *)\"\r\n            function template \"T *google::protobuf::Arena::CreateMessageInternal                                                                                            <T,Args...>(Args &&...)\"\r\n            argument types are: (google::protobuf::Arena *)\r\n          detected during:\r\n            instantiation of \"Msg *google::protobuf::Arena::CreateMaybeMessage<M                                                                                            sg>(google::protobuf::Arena *, google::protobuf::internal::true_type) [with Msg=                                                                                            tensorflow::TensorShapeProto_Dim]\"\r\n(729): here\r\n            instantiation of \"T *google::protobuf::Arena::CreateMaybeMessage<T>(                                                                                            google::protobuf::Arena *) [with T=tensorflow::TensorShapeProto_Dim]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(648): here\r\n            instantiation of \"GenericType *google::protobuf::internal::GenericTy                                                                                            peHandler<GenericType>::New(google::protobuf::Arena *) [with GenericType=tensorf                                                                                            low::TensorShapeProto_Dim]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(675): here\r\n            instantiation of \"GenericType *google::protobuf::internal::GenericTy                                                                                            peHandler<GenericType>::NewFromPrototype(const GenericType *, google::protobuf::                                                                                            Arena *) [with GenericType=tensorflow::TensorShapeProto_Dim]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(1554): here\r\n            instantiation of \"TypeHandler::Type *google::protobuf::internal::Rep                                                                                            eatedPtrFieldBase::Add<TypeHandler>(TypeHandler::Type *) [with TypeHandler=googl                                                                                            e::protobuf::RepeatedPtrField<tensorflow::TensorShapeProto_Dim>::TypeHandler]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(2001): here\r\n            instantiation of \"Element *google::protobuf::RepeatedPtrField<Elemen                                                                                            t>::Add() [with Element=tensorflow::TensorShapeProto_Dim]\"\r\nbazel-out/x64_windows-py3-opt/genfiles\\tensorflow/core/framework/tensor_shape.pb                                                                                            .h(471): here\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena.h(719): error: more than one                                                                                             instance of overloaded function \"google::protobuf::Arena::CreateMessageInternal                                                                                            \" matches the argument list:\r\n            function template \"T *google::protobuf::Arena::CreateMessageInternal                                                                                            <T>(google::protobuf::Arena *)\"\r\n            function template \"T *google::protobuf::Arena::CreateMessageInternal                                                                                            <T,Args...>(Args &&...)\"\r\n            argument types are: (google::protobuf::Arena *)\r\n          detected during:\r\n            instantiation of \"Msg *google::protobuf::Arena::CreateMaybeMessage<M                                                                                            sg>(google::protobuf::Arena *, google::protobuf::internal::true_type) [with Msg=                                                                                            tensorflow::ResourceHandleProto]\"\r\n(729): here\r\n            instantiation of \"T *google::protobuf::Arena::CreateMaybeMessage<T>(                                                                                            google::protobuf::Arena *) [with T=tensorflow::ResourceHandleProto]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(648): here\r\n            instantiation of \"GenericType *google::protobuf::internal::GenericTy                                                                                            peHandler<GenericType>::New(google::protobuf::Arena *) [with GenericType=tensorf                                                                                            low::ResourceHandleProto]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(675): here\r\n            instantiation of \"GenericType *google::protobuf::internal::GenericTy                                                                                            peHandler<GenericType>::NewFromPrototype(const GenericType *, google::protobuf::                                                                                            Arena *) [with GenericType=tensorflow::ResourceHandleProto]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(1554): here\r\n            instantiation of \"TypeHandler::Type *google::protobuf::internal::Rep                                                                                            eatedPtrFieldBase::Add<TypeHandler>(TypeHandler::Type *) [with TypeHandler=googl                                                                                            e::protobuf::RepeatedPtrField<tensorflow::ResourceHandleProto>::TypeHandler]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(2001): here\r\n            instantiation of \"Element *google::protobuf::RepeatedPtrField<Elemen                                                                                            t>::Add() [with Element=tensorflow::ResourceHandleProto]\"\r\nbazel-out/x64_windows-py3-opt/genfiles\\tensorflow/core/framework/tensor.pb.h(109                                                                                            1): here\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena.h(719): error: more than one                                                                                             instance of overloaded function \"google::protobuf::Arena::CreateMessageInternal                                                                                            \" matches the argument list:\r\n            function template \"T *google::protobuf::Arena::CreateMessageInternal                                                                                            <T>(google::protobuf::Arena *)\"\r\n            function template \"T *google::protobuf::Arena::CreateMessageInternal                                                                                            <T,Args...>(Args &&...)\"\r\n            argument types are: (google::protobuf::Arena *)\r\n          detected during:\r\n            instantiation of \"Msg *google::protobuf::Arena::CreateMaybeMessage<M                                                                                            sg>(google::protobuf::Arena *, google::protobuf::internal::true_type) [with Msg=                                                                                            tensorflow::VariantTensorDataProto]\"\r\n(729): here\r\n            instantiation of \"T *google::protobuf::Arena::CreateMaybeMessage<T>(                                                                                            google::protobuf::Arena *) [with T=tensorflow::VariantTensorDataProto]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(648): here\r\n            instantiation of \"GenericType *google::protobuf::internal::GenericTy                                                                                            peHandler<GenericType>::New(google::protobuf::Arena *) [with GenericType=tensorf                                                                                            low::VariantTensorDataProto]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(675): here\r\n            instantiation of \"GenericType *google::protobuf::internal::GenericTy                                                                                            peHandler<GenericType>::NewFromPrototype(const GenericType *, google::protobuf::                                                                                            Arena *) [with GenericType=tensorflow::VariantTensorDataProto]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(1554): here\r\n            instantiation of \"TypeHandler::Type *google::protobuf::internal::Rep                                                                                            eatedPtrFieldBase::Add<TypeHandler>(TypeHandler::Type *) [with TypeHandler=googl                                                                                            e::protobuf::RepeatedPtrField<tensorflow::VariantTensorDataProto>::TypeHandler]\"                                                                                            \r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(2001): here\r\n            instantiation of \"Element *google::protobuf::RepeatedPtrField<Elemen                                                                                            t>::Add() [with Element=tensorflow::VariantTensorDataProto]\"\r\nbazel-out/x64_windows-py3-opt/genfiles\\tensorflow/core/framework/tensor.pb.h(112                                                                                            1): here\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena.h(719): error: more than one                                                                                             instance of overloaded function \"google::protobuf::Arena::CreateMessageInternal                                                                                            \" matches the argument list:\r\n            function template \"T *google::protobuf::Arena::CreateMessageInternal                                                                                            <T>(google::protobuf::Arena *)\"\r\n            function template \"T *google::protobuf::Arena::CreateMessageInternal                                                                                            <T,Args...>(Args &&...)\"\r\n            argument types are: (google::protobuf::Arena *)\r\n          detected during:\r\n            instantiation of \"Msg *google::protobuf::Arena::CreateMaybeMessage<M                                                                                            sg>(google::protobuf::Arena *, google::protobuf::internal::true_type) [with Msg=                                                                                            tensorflow::TensorProto]\"\r\n(729): here\r\n            instantiation of \"T *google::protobuf::Arena::CreateMaybeMessage<T>(                                                                                            google::protobuf::Arena *) [with T=tensorflow::TensorProto]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(648): here\r\n            instantiation of \"GenericType *google::protobuf::internal::GenericTy                                                                                            peHandler<GenericType>::New(google::protobuf::Arena *) [with GenericType=tensorf                                                                                            low::TensorProto]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(675): here\r\n            instantiation of \"GenericType *google::protobuf::internal::GenericTy                                                                                            peHandler<GenericType>::NewFromPrototype(const GenericType *, google::protobuf::                                                                                            Arena *) [with GenericType=tensorflow::TensorProto]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(1554): here\r\n            instantiation of \"TypeHandler::Type *google::protobuf::internal::Rep                                                                                            eatedPtrFieldBase::Add<TypeHandler>(TypeHandler::Type *) [with TypeHandler=googl                                                                                            e::protobuf::RepeatedPtrField<tensorflow::TensorProto>::TypeHandler]\"\r\nexternal/protobuf_archive/src\\google/protobuf/repeated_field.h(2001): here\r\n            instantiation of \"Element *google::protobuf::RepeatedPtrField<Elemen                                                                                            t>::Add() [with Element=tensorflow::TensorProto]\"\r\nbazel-out/x64_windows-py3-opt/genfiles\\tensorflow/core/framework/tensor.pb.h(136                                                                                            5): here\r\n\r\n4 errors detected in the compilation of \"C:/Users/NIN~1/AppData/Local/Temp/nv                                                                                            cc_inter_files_tmp_dir/extract_image_patches_op_gpu.cu.compute_52.cpp1.ii\".\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: F:/machinelearning/tensorflow/tensorflow/tools/pip_package/BUILD:53:1 C++                                                                                             compilation of rule '//tensorflow/core/kernels:extract_image_patches_op_gpu' fa                                                                                            iled (Exit 1): msvc_cl.bat failed: error executing command\r\n  cd C:/temp/_bazel_ningzhihui/aki74rxt/execroot/org_tensorflow\r\n  SET CUDA_COMPUTE_CAPABILITIE=None\r\n    SET CUDA_PATH=F:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1\r\n    SET CUDA_TOOLKIT_PATH=F:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.                                                                                            1\r\n    SET CUDNN_INSTALL_PATH=F:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9                                                                                            .1\r\n    SET INCLUDE=E:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;E                                                                                            :\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program                                                                                             Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Win                                                                                            dows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\incl                                                                                            ude\\\\shared;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\um;C:\\Program Files                                                                                             (x86)\\Windows Kits\\8.1\\include\\\\winrt;\r\n    SET LIB=E:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;E:\\                                                                                            Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program                                                                                             Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\ucrt\\x64;C:\\Program Files (x86)\\Win                                                                                            dows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\lib\\                                                                                            winv6.3\\um\\x64;\r\n    SET NO_WHOLE_ARCHIVE_OPTION=1\r\n    SET PATH=F:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1/bin;E:\\Prog                                                                                            ram Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Micros                                                                                            oft\\TestWindow;E:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;                                                                                            C:\\windows\\Microsoft.NET\\Framework64\\v4.0.30319;E:\\Program Files (x86)\\Microsoft                                                                                             Visual Studio 14.0\\VC\\VCPackages;E:\\Program Files (x86)\\Microsoft Visual Studio                                                                                             14.0\\Common7\\IDE;E:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\To                                                                                            ols;E:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance T                                                                                            ools\\x64;E:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performa                                                                                            nce Tools;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x64;C:\\Program Files (x86)                                                                                            \\Windows Kits\\8.1\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\b                                                                                            in\\NETFX 4.6.1 Tools\\x64\\;;C:\\windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=E:/Python/Python35/python.exe\r\n    SET PYTHON_LIB_PATH=E:/Python/Python35/lib/site-packages\r\n    SET TEMP=C:\\Users\\NIN~1\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0\r\n    SET TF_CUDA_VERSION=9.1\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TMP=C:\\Users\\NIN~1\\AppData\\Local\\Temp\r\n    SET USE_MSVC_WRAPPER=1\r\n  external/local_config_cc/wrapper/bin/msvc_cl.bat /c tensorflow/core/kernels/ex                                                                                            tract_image_patches_op_gpu.cu.cc /Fobazel-out/x64_windows-py3-opt/bin/tensorflow                                                                                            /core/kernels/_objs/extract_image_patches_op_gpu/tensorflow/core/kernels/extract                                                                                            _image_patches_op_gpu.cu.o /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0                                                                                            600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_                                                                                            DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd                                                                                            4996 -Xcompilation-mode=opt -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -w /I. /Ibazel                                                                                            -out/x64_windows-py3-opt/genfiles /Iexternal/nsync /Ibazel-out/x64_windows-py3-o                                                                                            pt/genfiles/external/nsync /Iexternal/bazel_tools /Ibazel-out/x64_windows-py3-op                                                                                            t/genfiles/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows                                                                                            -py3-opt/genfiles/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-ou                                                                                            t/x64_windows-py3-opt/genfiles/external/local_config_sycl /Iexternal/com_google_                                                                                            absl /Ibazel-out/x64_windows-py3-opt/genfiles/external/com_google_absl /Iexterna                                                                                            l/gif_archive /Ibazel-out/x64_windows-py3-opt/genfiles/external/gif_archive /Iex                                                                                            ternal/jpeg /Ibazel-out/x64_windows-py3-opt/genfiles/external/jpeg /Iexternal/pr                                                                                            otobuf_archive /Ibazel-out/x64_windows-py3-opt/genfiles/external/protobuf_archiv                                                                                            e /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-py3-opt/genfiles/                                                                                            external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_w                                                                                            indows-py3-opt/genfiles/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x                                                                                            64_windows-py3-opt/genfiles/external/fft2d /Iexternal/highwayhash /Ibazel-out/x6                                                                                            4_windows-py3-opt/genfiles/external/highwayhash /Iexternal/png_archive /Ibazel-o                                                                                            ut/x64_windows-py3-opt/genfiles/external/png_archive /Iexternal/zlib_archive /Ib                                                                                            azel-out/x64_windows-py3-opt/genfiles/external/zlib_archive /Iexternal/snappy /I                                                                                            bazel-out/x64_windows-py3-opt/genfiles/external/snappy /Iexternal/local_config_c                                                                                            uda /Ibazel-out/x64_windows-py3-opt/genfiles/external/local_config_cuda /Iextern                                                                                            al/nsync/public /Ibazel-out/x64_windows-py3-opt/genfiles/external/nsync/public /                                                                                            Iexternal/bazel_tools/tools/cpp/gcc3 /Iexternal/eigen_archive /Ibazel-out/x64_wi                                                                                            ndows-py3-opt/genfiles/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel                                                                                            -out/x64_windows-py3-opt/genfiles/external/gif_archive/lib /Iexternal/gif_archiv                                                                                            e/windows /Ibazel-out/x64_windows-py3-opt/genfiles/external/gif_archive/windows                                                                                             /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-py3-opt/genfiles/externa                                                                                            l/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-p                                                                                            y3-opt/genfiles/external/farmhash_archive/src /Iexternal/png_archive /Ibazel-out                                                                                            /x64_windows-py3-opt/genfiles/external/png_archive /Iexternal/zlib_archive /Ibaz                                                                                            el-out/x64_windows-py3-opt/genfiles/external/zlib_archive /Iexternal/local_confi                                                                                            g_cuda/cuda /Ibazel-out/x64_windows-py3-opt/genfiles/external/local_config_cuda/                                                                                            cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-py3-                                                                                            opt/genfiles/external/local_config_cuda/cuda/cuda/include /Iexternal/local_confi                                                                                            g_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-py3-opt/genfiles/external/l                                                                                            ocal_config_cuda/cuda/cuda/include/crt /DEIGEN_MPL2_ONLY /D__CLANG_SUPPORT_DYN_A                                                                                            NNOTATION__ /DTENSORFLOW_USE_ABSL /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG                                                                                             -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true                                                                                             -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /D__VERSION__=\"MSVC\" /DPLATFORM_WI                                                                                            NDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_A                                                                                            RRAY /Iexternal/gemmlowp /wd4018 /U_HAS_EXCEPTIONS /D_HAS_EXCEPTIONS=1 /EHsc /DN                                                                                            OGDI /DTF_COMPILE_LIBRARY\r\nINFO: Elapsed time: 1063.446s, Critical Path: 244.98s\r\nFAILED: Build did NOT complete successfully\r\n", "same error here. macOS: 10.13.2, cuda: 9.1, cudnn: 7.0.5", "I am seeing the same error. macOS: 10.13.3, cuda: 9.1, cudnn: 7.0.5\r\n\r\nI tried to reduce `arena.h` to something that results in a similar error, but I have not succeeded: https://gist.github.com/dtrebbien/cf50157fe61a59d2a98d780bd2c92de6\r\n\r\nWhat I did to work around this issue is edit `arena.h`, changing line 666 to rename `CreateMessageInternal` to `CreateMessageInternal_` and changing line 719 to call `CreateMessageInternal_` instead. These changes appear to fix the compilation error; however, I am encountering another error:\r\n\r\n<pre>\r\nINFO: From Compiling tensorflow/contrib/image/kernels/image_ops_gpu.cu.cc:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/Half.h(508): error: explicit specialization of class \"std::__1::numeric_limits<Eigen::half>\" must precede its first use (\r\n(388): here)\r\n</pre>", "@dtrebbien Yes, when I build with tensorflow 1.5, same error of Eigen/src/Core/arch/CUDA/Half.h(508)", "The \"explicit specialization\" error is puzzling. The error message says that the explicit specialization of std::numeric_limits (for `Eigen::half`) must precede where it's used. That makes sense. However, line 388 of `Half.h` is the return line in:\r\n\r\n```c++\r\nEIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool (isfinite)(const half& a) {\r\n  return !(isinf EIGEN_NOT_A_MACRO (a)) && !(isnan EIGEN_NOT_A_MACRO (a));\r\n}\r\n```\r\n\r\nI don't see how that is using `std::numeric_limits<Eigen::half>`.", "arena.h locations and all four md5sums are different. \r\n\r\n./tensorflow/contrib/cmake/build/grpc/src/grpc/src/core/lib/support/arena.h\r\n./tensorflow/contrib/cmake/build/grpc/src/grpc/third_party/protobuf/src/google/protobuf/arena.h\r\n./tensorflow/contrib/cmake/build/protobuf/src/protobuf/src/google/protobuf/arena.h\r\n./tensorflow/core/lib/core/arena.h\r\n\r\n```\r\n75d243f7510cc93ab28c088cb9602a0b *./tensorflow/contrib/cmake/build/grpc/src/grpc/src/core/lib/support/arena.h\r\n66acc2ebe4d1d831f460330d4424d0d4 *./tensorflow/contrib/cmake/build/grpc/src/grpc/third_party/protobuf/src/google/protobuf/arena.h\r\n768863b9a7d853bff47c2bea50aaaaa9 *./tensorflow/contrib/cmake/build/protobuf/src/protobuf/src/google/protobuf/arena.h\r\nd78b033cb70a006c012a02690a0d46a1 *./tensorflow/core/lib/core/arena.h\r\n```\r\n\r\n\r\n", "The work around that I mentioned in a [previous comment](https://github.com/tensorflow/tensorflow/issues/17067#issuecomment-366496974) was half-baked. I have created a fork of protobuf containing all of the changes which appear to be necessary. To use this fork, I applied the following patch to `tensorflow/workspace.bzl`:\r\n\r\n```patch\r\n--- a/tensorflow/workspace.bzl\r\n+++ b/tensorflow/workspace.bzl\r\n@@ -353,11 +353,11 @@ def tf_workspace(path_prefix=\"\", tf_repo_name=\"\"):\r\n   tf_http_archive(\r\n       name = \"protobuf_archive\",\r\n       urls = [\r\n-          \"https://mirror.bazel.build/github.com/google/protobuf/archive/396336eb961b75f03b25824fe86cf6490fb75e3a.tar.gz\",\r\n-          \"https://github.com/google/protobuf/archive/396336eb961b75f03b25824fe86cf6490fb75e3a.tar.gz\",\r\n+          \"https://mirror.bazel.build/github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz\",\r\n+          \"https://github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz\",\r\n       ],\r\n-      sha256 = \"846d907acf472ae233ec0882ef3a2d24edbbe834b80c305e867ac65a1f2c59e3\",\r\n-      strip_prefix = \"protobuf-396336eb961b75f03b25824fe86cf6490fb75e3a\",\r\n+      sha256 = \"eb16b33431b91fe8cee479575cee8de202f3626aaf00d9bf1783c6e62b4ffbc7\",\r\n+      strip_prefix = \"protobuf-50f552646ba1de79e07562b41f3999fe036b4fd0\",\r\n   )\r\n \r\n   # We need to import the protobuf library under the names com_google_protobuf\r\n```", "I have created a pull request for eigen/eigen to fix the \"explicit specialization of class \"std::__1::numeric_limits\" must precede its first use\" error: https://bitbucket.org/eigen/eigen/pull-requests/369/\r\n\r\nTo use the patched eigen, I applied the following patch to `tensorflow/workspace.bzl`:\r\n\r\n```patch\r\n--- a/tensorflow/workspace.bzl\r\n+++ b/tensorflow/workspace.bzl\r\n@@ -120,11 +120,11 @@ def tf_workspace(path_prefix=\"\", tf_repo_name=\"\"):\r\n   tf_http_archive(\r\n       name = \"eigen_archive\",\r\n       urls = [\r\n-          \"https://mirror.bazel.build/bitbucket.org/eigen/eigen/get/2355b229ea4c.tar.gz\",\r\n-          \"https://bitbucket.org/eigen/eigen/get/2355b229ea4c.tar.gz\",\r\n+          \"https://mirror.bazel.build/bitbucket.org/dtrebbien/eigen/get/374842a18727.tar.gz\",\r\n+          \"https://bitbucket.org/dtrebbien/eigen/get/374842a18727.tar.gz\",\r\n       ],\r\n-      sha256 = \"0cadb31a35b514bf2dfd6b5d38205da94ef326ec6908fc3fd7c269948467214f\",\r\n-      strip_prefix = \"eigen-eigen-2355b229ea4c\",\r\n+      sha256 = \"fa26e9b9ff3a2692b092d154685ec88d6cb84d4e1e895006541aff8603f15c16\",\r\n+      strip_prefix = \"dtrebbien-eigen-374842a18727\",\r\n       build_file = str(Label(\"//third_party:eigen.BUILD\")),\r\n   )\r\n \r\n```", " I am encountering another error\r\n\r\n[958 / 973] Compiling tensorflow/core/kernels/conv_ops_3d.cc; 38028s local ... (4 actions running)\r\nERROR: F:/machinelearning/tensorflow/tensorflow/core/BUILD:762:1: C++ compilation of rule '//tensorflow/core:nn_grad' failed (Exit -1073741502): msvc_cl.bat failed: error executing command\r\n  cd E:/temp/_bazel_nin/aki74rxt/execroot/org_tensorflow\r\n  SET CUDA_COMPUTE_CAPABILITIE=None\r\n", "Does dtrebbien's fix work for you?", "Thank you, @dtrebbien. This helped me a lot, even on Mac OS 10.13.3, tensorflow v1.6. I'm using CUDA 9.1 with cudnn 7.", "Hi @dtrebbien and @tralpha, how to solve the @rpath problem?\r\nI build with options:\r\n```\r\nbazel build --config=cuda --config=opt --verbose_failures --action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nI'm using CUDA 9.1, cudnn 7 and Xcode 9.2 on MacOS 13.3.3.", "Do you mean something like:\r\n\r\n<pre>\r\ndyld: Library not loaded: @rpath/libcudart.9.1.dylib\r\n</pre>\r\n\r\nTake a look at this Stack Overflow answer: https://stackoverflow.com/a/40007947/196844", "Hi @dtrebbien I'm having a little trouble with applying the two patches you posted and build.  I get the following error \r\n`ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: error loading package 'tensorflow': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz, https://github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz] to /private/var/tmp/_bazel_temp/737438342ae219a6c1e340312025fb82/external/protobuf_archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz: All mirrors are down: []`\r\n\r\nI was able to pull down the tars from github and the checksums match. Any ideas? What commit ID did you build off of?\r\nI'm using CUDA 9.1, cudnn 7, tensorflow 1.6, Xcode 8.2 and bazel 0.10.0 on MacOS 10.12.6. I'm trying to build off of c6a12c77a50778e28de3590f4618bc2b62f3ecab so it seems like around line 127 and 361 for me. \r\n\r\nAppreciate the help, it's not to see others trying to get GPU support on Mac :)\r\n\r\nedited this, looks like I have a mis-configured proxy setting for the jdk.\r\nStill would like to know which commit you were able to build successfully off of?\r\n\r\n", "I built off of TensorFlow commit 00ff4912d49cc09737fd1425df6d877f29f69741 with a few cherry-picked commits and local changes. Just now, I have committed my local changes and pushed this as a new branch to my fork; see https://github.com/dtrebbien/tensorflow/tree/tensorflow-17067-reference-branch\r\n\r\nTo use this branch:\r\n\r\n```sh\r\ngit remote add dtrebbien-fork https://github.com/dtrebbien/tensorflow.git\r\ngit fetch dtrebbien-fork\r\ngit checkout -b tensorflow-17067-reference-branch dtrebbien-fork/tensorflow-17067-reference-branch\r\n```\r\n\r\nI am not sure that dtrebbien@993006fa764bbdecfee63f4ceead3d06a2821ce2 is needed; I merged it speculatively.\r\n\r\nHere are the configure and build commands that I used:\r\n\r\n```sh\r\nPYTHON_BIN_PATH=/usr/local/bin/python2 \\\r\nUSE_DEFAULT_PYTHON_LIB_PATH=1 \\\r\nTF_NEED_GCP=0 \\\r\nTF_NEED_HDFS=0 \\\r\nTF_NEED_S3=0 \\\r\nTF_NEED_KAFKA=0 \\\r\nTF_ENABLE_XLA=0 \\\r\nTF_NEED_GDR=0 \\\r\nTF_NEED_VERBS=0 \\\r\nTF_NEED_OPENCL_SYCL=0 \\\r\nTF_NEED_CUDA=1 \\\r\nTF_CUDA_VERSION=9.1 \\\r\nUSE_DEFAULT_CUDA_TOOLKIT_PATH=1 \\\r\nTF_CUDNN_VERSION=7.0.5 \\\r\nUSE_DEFAULT_CUDNN_INSTALL_PATH=1 \\\r\nTF_CUDA_COMPUTE_CAPABILITIES=3.0 \\\r\nTF_CUDA_CLANG=0 \\\r\nUSE_DEFAULT_GCC_HOST_COMPILER_PATH=1 \\\r\nTF_NEED_MPI=0 \\\r\nUSE_DEFAULT_CC_OPT_FLAGS=1 \\\r\nTF_SET_ANDROID_WORKSPACE=0 \\\r\n./configure\r\n\r\nbazel build --config=opt --config=cuda --save_temps --explain=explain.txt --verbose_explanations --verbose_failures --linkopt=-Wl,-rpath,/usr/local/cuda/lib //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nNote that I am building using [Homebrew](https://brew.sh/)'s Python 2. Also, I am running macOS 10.13.3 and Xcode 9.2.\r\n\r\nYou might see an error saying something like `dyld: Library not loaded: @rpath/libcudart.9.1.dylib`. If you do, follow the instructions in this Stack Overflow answer: https://stackoverflow.com/a/40007947/196844\r\n\r\nMy MacBook Pro has an NVIDIA GeForce GT 750M, which is CUDA compute capability 3.0. You might need to adjust the `TF_CUDA_COMPUTE_CAPABILITIES` configuration option if your Mac's NVIDIA GPU has a different compute capability.\r\n\r\nDisclaimer: I do not know if the binary produced by following these steps is stable. I have observed a SEGFAULT issue with my build, but I do not know if this is specific to my build or if this is a bug in TensorFlow: https://github.com/tensorflow/tensorflow/issues/9518#issuecomment-366854122", "I just updated Homebrew and noticed that the 'python' formula is now Python 3, 'python2' is now Python 2, and the location of Homebrew Python 2 binaries is `/usr/local/opt/python@2/bin`.", "@dtrebbien I do the tricks from [SO](https://stackoverflow.com/a/40007947/196844) but doesn't work, so I proceed with symbolic links in ```/usr/local/lib``` of the necessary CUDA libraries. \r\n\r\nFor the last version ```r1.6``` I have to remove ``` __align__(sizeof(T))``` like discribes in [TweakMind](https://tweakmind.com/tensorflow-1-5-macos-10-13-2/) because of align problems.\r\n```\r\nsed -i.bu 's/__align__(sizeof(T)) //g' tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\r\nsed -i.bu 's/__align__(sizeof(T)) //g' tensorflow/core/kernels/split_lib_gpu.cu.cc\r\nsed -i.bu 's/__align__(sizeof(T)) //g' tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc\r\n```\r\nAfter all, I could compile and and test the TF with GPU is loaded. But with some real tranning I have a SEGFAULT issue. The dumps doesn\u00b4t show any glue about the problem.", "Are you able to run any TensorFlow code, or does everything segfault?\r\n\r\nI am not using the work around of removing `__align__(sizeof(T))`. Instead, I use `__align__(sizeof(T) > 16 ? sizeof(T) : 16)` which will produce an error if `T` is ever more than 16 bytes; otherwise, the shared storage is always 16-byte aligned.", "@dtrebbien I could run some initial steps, but I couldn't run any real code. When the TF load data to GPU they show the SEGFAULT message and abort the run, even with simple computations on tensors.\r\n\r\nP.S. Next week I will try the ```__align__``` sugestion", "Last Updates @dtrebbien :\r\n\r\n```\r\nsed -i.bu 's/__align__(sizeof(T)) /__align__(sizeof(T) > 16 ? sizeof(T) : 16) /g' tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\r\nsed -i.bu 's/__align__(sizeof(T)) /__align__(sizeof(T) > 16 ? sizeof(T) : 16) /g' tensorflow/core/kernels/split_lib_gpu.cu.cc\r\nsed -i.bu 's/__align__(sizeof(T)) /__align__(sizeof(T) > 16 ? sizeof(T) : 16) /g' tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc\r\n```\r\n\r\nDuring the compile, I saw this message:\r\n```\r\nld: warning: cannot export hidden symbol std::__1::__vector_base<tensorflow::graph_transforms::OpTypePattern, std::__1::allocator<tensorflow::graph_transforms::OpTypePattern> >::__destruct_at_end(tensorflow::graph_transforms::OpTypePattern*) from bazel-out/darwin-py3-opt/bin/tensorflow/tools/graph_transforms/libtransforms_lib.pic.lo(remove_nodes.pic.o)\r\n```\r\n\r\nLoading results in these messages\r\n\r\n```\r\n2018-03-12 11:27:54.282823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:859] OS X does not support NUMA - returning NUMA node zero\r\n2018-03-12 11:27:54.282993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:\r\nname: GeForce GTX 780M major: 3 minor: 0 memoryClockRate(GHz): 0.784\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 286.09MiB\r\n2018-03-12 11:27:54.283018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-03-12 11:27:54.639034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22 MB memory) -> physical GPU (device: 0, name: GeForce GTX 780M, pci bus id: 0000:01:00.0, compute capability: 3.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 780M, pci bus id: 0000:01:00.0, compute capability: 3.0\r\n2018-03-12 11:27:54.649140: I tensorflow/core/common_runtime/direct_session.cc:297] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 780M, pci bus id: 0000:01:00.0, compute capability: 3.0\r\n```\r\n\r\nwith a simple example\r\n\r\n```\r\nimport tensorflow as tf\r\nwith tf.device('/gpu:0'):\r\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n    c = tf.matmul(a, b)\r\n\r\nwith tf.Session() as sess:\r\n    print (sess.run(c))\r\n```\r\n\r\nresults\r\n\r\n```\r\n2018-03-12 11:30:37.822237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-03-12 11:30:37.822477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11 MB memory) -> physical GPU (device: 0, name: GeForce GTX 780M, pci bus id: 0000:01:00.0, compute capability: 3.0)\r\n[[22. 28.]\r\n [49. 64.]]\r\n```\r\n\r\nNo more SEGFAULT !", "Trying to run [mnist_mlp.py](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py) I get a SEGFAULT.\r\n\r\nto get some more information I run\r\n\r\n```\r\nsudo dtruss python mnist_mlp.py 2> err_mnist.txt\r\n```\r\n\r\nSee attached [err_mnist.txt](https://github.com/tensorflow/tensorflow/files/1803314/err_mnist.txt).\r\n\r\n\r\n", "Hello @marcionicolau\r\n\r\nTrying out `mnist_mlp.py`, I am also observing a SEGFAULT. Here is a backtrace:\r\n\r\n<pre>\r\nThread 26 Crashed:\r\n0   libtensorflow_framework.so    \t0x0000000102959c10 void tensorflow::gtl::InlinedVector<tensorflow::EventMgr::InUse, 4>::emplace_back<tensorflow::EventMgr::InUse const&>(tensorflow::EventMgr::InUse const&&&) + 176\r\n1   libtensorflow_framework.so    \t0x000000010295902c tensorflow::EventMgr::PollEvents(bool, tensorflow::gtl::InlinedVector<tensorflow::EventMgr::InUse, 4>*) + 412\r\n2   libtensorflow_framework.so    \t0x00000001028f4022 tensorflow::EventMgr::ThenExecute(perftools::gputools::Stream*, std::__1::function<void ()>) + 194\r\n3   libtensorflow_framework.so    \t0x00000001028f4bad tensorflow::GPUUtil::CopyCPUTensorToGPU(tensorflow::Tensor const*, tensorflow::DeviceContext const*, tensorflow::Device*, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) + 797\r\n4   libtensorflow_framework.so    \t0x00000001028f6a05 tensorflow::GPUDeviceContext::CopyCPUTensorToDevice(tensorflow::Tensor const*, tensorflow::Device*, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) const + 117\r\n5   libtensorflow_framework.so    \t0x00000001029098b5 tensorflow::(anonymous namespace)::CopyHostToDevice(tensorflow::Tensor const*, tensorflow::Allocator*, tensorflow::Allocator*, tensorflow::StringPiece, tensorflow::Device*, tensorflow::Tensor*, tensorflow::DeviceContext*, std::__1::function<void (tensorflow::Status const&)>) + 437\r\n6   libtensorflow_framework.so    \t0x0000000102908b58 tensorflow::CopyTensor::ViaDMA(tensorflow::StringPiece, tensorflow::DeviceContext*, tensorflow::DeviceContext*, tensorflow::Device*, tensorflow::Device*, tensorflow::AllocatorAttributes, tensorflow::AllocatorAttributes, tensorflow::Tensor const*, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) + 3592\r\n7   libtensorflow_framework.so    \t0x0000000102942dbe tensorflow::IntraProcessRendezvous::SameWorkerRecvDone(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) + 1102\r\n8   libtensorflow_framework.so    \t0x000000010294385d std::__1::__function::__func<tensorflow::IntraProcessRendezvous::RecvAsync(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, std::__1::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>)::$_0, std::__1::allocator<tensorflow::IntraProcessRendezvous::RecvAsync(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, std::__1::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>)::$_0>, void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>::operator()(tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool&&) + 813\r\n9   libtensorflow_framework.so    \t0x000000010244e003 tensorflow::LocalRendezvousImpl::RecvAsync(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, std::__1::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>) + 883\r\n10  libtensorflow_framework.so    \t0x000000010294311f tensorflow::IntraProcessRendezvous::RecvAsync(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, std::__1::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>) + 799\r\n11  _pywrap_tensorflow_internal.so\t0x000000010b1a40a9 tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::__1::function<void ()>) + 1145\r\n12  libtensorflow_framework.so    \t0x00000001028ea458 tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::__1::function<void ()>) + 872\r\n13  libtensorflow_framework.so    \t0x0000000102918202 tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 4338\r\n14  libtensorflow_framework.so    \t0x000000010292210a std::__1::__function::__func<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&>, std::__1::allocator<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&> >, void ()>::operator()() + 58\r\n15  libtensorflow_framework.so    \t0x000000010255bdff Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 2047\r\n16  libtensorflow_framework.so    \t0x000000010255b4ff std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 47\r\n17  libtensorflow_framework.so    \t0x00000001025808a0 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*) + 48\r\n18  libsystem_pthread.dylib       \t0x00007fff53e376c1 _pthread_body + 340\r\n19  libsystem_pthread.dylib       \t0x00007fff53e3756d _pthread_start + 377\r\n20  libsystem_pthread.dylib       \t0x00007fff53e36c5d thread_start + 13\r\n</pre>\r\n\r\nThis appears similar to [the other SEGFAULT issue that I have observed](https://github.com/tensorflow/tensorflow/issues/9518#issuecomment-366854122). There, the SEGFAULT occurs within Copy**GPU**TensorToCPU(). Here, the SEGFAULT occurs within Copy**CPU**TensorToGPU().", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I believe @dtrebbien has removed his repo. If you need a protobuf version that will compile, use mine:\r\n\r\n```diff\r\n   tf_http_archive(\r\n       name = \"protobuf_archive\",\r\n       urls = [\r\n-          \"https://mirror.bazel.build/github.com/google/protobuf/archive/396336eb961b75f03b25824fe86cf6490fb75e3a.tar.gz\",\r\n-          \"https://github.com/google/protobuf/archive/396336eb961b75f03b25824fe86cf6490fb75e3a.tar.gz\",\r\n+          \"https://mirror.bazel.build/github.com/dinever/protobuf/archive/188578878eff18c2148baba0e116d87ce8f49410.tar.gz\",\r\n+          \"https://github.com/dinever/protobuf/archive/188578878eff18c2148baba0e116d87ce8f49410.tar.gz\",\r\n       ],\r\n-      sha256 = \"846d907acf472ae233ec0882ef3a2d24edbbe834b80c305e867ac65a1f2c59e3\",\r\n-      strip_prefix = \"protobuf-396336eb961b75f03b25824fe86cf6490fb75e3a\",\r\n+      sha256 = \"7a1d96ccdf7131535828cad737a76fd65ed766e9511e468d0daa3cc4f3db5175\",\r\n+      strip_prefix = \"protobuf-188578878eff18c2148baba0e116d87ce8f49410\",\r\n   )\r\n```", "> I believe @dtrebbien has removed his repo. If you need a protobuf version that will compile, use mine:\r\n> \r\n> ```diff\r\n>    tf_http_archive(\r\n>        name = \"protobuf_archive\",\r\n>        urls = [\r\n> -          \"https://mirror.bazel.build/github.com/google/protobuf/archive/396336eb961b75f03b25824fe86cf6490fb75e3a.tar.gz\",\r\n> -          \"https://github.com/google/protobuf/archive/396336eb961b75f03b25824fe86cf6490fb75e3a.tar.gz\",\r\n> +          \"https://mirror.bazel.build/github.com/dinever/protobuf/archive/188578878eff18c2148baba0e116d87ce8f49410.tar.gz\",\r\n> +          \"https://github.com/dinever/protobuf/archive/188578878eff18c2148baba0e116d87ce8f49410.tar.gz\",\r\n>        ],\r\n> -      sha256 = \"846d907acf472ae233ec0882ef3a2d24edbbe834b80c305e867ac65a1f2c59e3\",\r\n> -      strip_prefix = \"protobuf-396336eb961b75f03b25824fe86cf6490fb75e3a\",\r\n> +      sha256 = \"7a1d96ccdf7131535828cad737a76fd65ed766e9511e468d0daa3cc4f3db5175\",\r\n> +      strip_prefix = \"protobuf-188578878eff18c2148baba0e116d87ce8f49410\",\r\n>    )\r\n> ```\r\n\r\nI have got the following error after replacing the tf_http_archive you posted.\r\n\r\nerror: patch failed: tensorflow/core/common_runtime/gpu/gpu_device.cc:920\r\nerror: tensorflow/core/common_runtime/gpu/gpu_device.cc: patch does not apply\r\nerror: patch failed: tensorflow/core/framework/variant.h:152\r\nerror: tensorflow/core/framework/variant.h: patch does not apply\r\nerror: patch failed: tensorflow/core/grappler/clusters/utils.cc:124\r\nerror: tensorflow/core/grappler/clusters/utils.cc: patch does not apply\r\nerror: patch failed: tensorflow/core/kernels/bucketize_op_gpu.cu.cc:39\r\nerror: tensorflow/core/kernels/bucketize_op_gpu.cu.cc: patch does not apply\r\nerror: patch failed: tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc:69\r\nerror: tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc: patch does not apply\r\nerror: patch failed: tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc:172\r\nerror: tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc: patch does not apply\r\nerror: patch failed: tensorflow/core/kernels/split_lib_gpu.cu.cc:121\r\nerror: tensorflow/core/kernels/split_lib_gpu.cu.cc: patch does not apply\r\nerror: patch failed: tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:856\r\nerror: tensorflow/stream_executor/cuda/cuda_gpu_executor.cc: patch does not apply\r\nerror: patch failed: tensorflow/workspace.bzl:330\r\nerror: tensorflow/workspace.bzl: patch does not apply\r\nerror: patch failed: third_party/gpus/cuda/BUILD.tpl:110\r\nerror: third_party/gpus/cuda/BUILD.tpl: patch does not apply", "> > I believe @dtrebbien has removed his repo. If you need a protobuf version that will compile, use mine:\r\n> > ```diff\r\n> >    tf_http_archive(\r\n> >        name = \"protobuf_archive\",\r\n> >        urls = [\r\n> > -          \"https://mirror.bazel.build/github.com/google/protobuf/archive/396336eb961b75f03b25824fe86cf6490fb75e3a.tar.gz\",\r\n> > -          \"https://github.com/google/protobuf/archive/396336eb961b75f03b25824fe86cf6490fb75e3a.tar.gz\",\r\n> > +          \"https://mirror.bazel.build/github.com/dinever/protobuf/archive/188578878eff18c2148baba0e116d87ce8f49410.tar.gz\",\r\n> > +          \"https://github.com/dinever/protobuf/archive/188578878eff18c2148baba0e116d87ce8f49410.tar.gz\",\r\n> >        ],\r\n> > -      sha256 = \"846d907acf472ae233ec0882ef3a2d24edbbe834b80c305e867ac65a1f2c59e3\",\r\n> > -      strip_prefix = \"protobuf-396336eb961b75f03b25824fe86cf6490fb75e3a\",\r\n> > +      sha256 = \"7a1d96ccdf7131535828cad737a76fd65ed766e9511e468d0daa3cc4f3db5175\",\r\n> > +      strip_prefix = \"protobuf-188578878eff18c2148baba0e116d87ce8f49410\",\r\n> >    )\r\n> > ```\r\n> \r\n> I have got the following error after replacing the tf_http_archive you posted.\r\n> \r\n> error: patch failed: tensorflow/core/common_runtime/gpu/gpu_device.cc:920\r\n> error: tensorflow/core/common_runtime/gpu/gpu_device.cc: patch does not apply\r\n> error: patch failed: tensorflow/core/framework/variant.h:152\r\n> error: tensorflow/core/framework/variant.h: patch does not apply\r\n> error: patch failed: tensorflow/core/grappler/clusters/utils.cc:124\r\n> error: tensorflow/core/grappler/clusters/utils.cc: patch does not apply\r\n> error: patch failed: tensorflow/core/kernels/bucketize_op_gpu.cu.cc:39\r\n> error: tensorflow/core/kernels/bucketize_op_gpu.cu.cc: patch does not apply\r\n> error: patch failed: tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc:69\r\n> error: tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc: patch does not apply\r\n> error: patch failed: tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc:172\r\n> error: tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc: patch does not apply\r\n> error: patch failed: tensorflow/core/kernels/split_lib_gpu.cu.cc:121\r\n> error: tensorflow/core/kernels/split_lib_gpu.cu.cc: patch does not apply\r\n> error: patch failed: tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:856\r\n> error: tensorflow/stream_executor/cuda/cuda_gpu_executor.cc: patch does not apply\r\n> error: patch failed: tensorflow/workspace.bzl:330\r\n> error: tensorflow/workspace.bzl: patch does not apply\r\n> error: patch failed: third_party/gpus/cuda/BUILD.tpl:110\r\n> error: third_party/gpus/cuda/BUILD.tpl: patch does not apply\r\n\r\n@cc112358 have you solve the errors at the end?"]}, {"number": 17066, "title": "Update guide.md", "body": "Fixing grammatical errors in the Installation instructions", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed the Google Individual Contributor License Agreement", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17065, "title": "ImportError: cannot import name pywrap_dlopen_global_flags", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 17063, "title": "Add wheel dependency to cmake README", "body": "In order to run to create the pip package after compilation, wheel needs to be installed. This explicitly lists wheel as a prereq in the README. Otherwise the user will hit a \"invalid command 'bdist_wheel'\" error.", "comments": []}, {"number": 17062, "title": "Change Kernel dimension(height * width * input_channel * output_channel) to NCHW format", "body": "First of all, it seems that no one tries this due to the fact that I couldn't get any answers from stackoverflow or googling.\r\n\r\nI'm trying to use cudnn inside the custom op.\r\n\r\nBasically, custom op receives kernel layout (height * width * input_channel * output_channel).\r\n\r\nBut, according to cudnn policy, kernel layout has to be (output_channel * input_channel * height * width) inside custom op. But, I couldn't find anything regarding this issues.\r\n\r\nAre there any solutions for converting layout like this(HWCN to NCHW)?", "comments": ["@allencho1222 um, could you please be more specific about what were you failing to find? The layout conversion issue is central to GPU performance. The TF Performance Guide has a section on Data Formats: https://www.tensorflow.org/performance/performance_guide\r\n\r\nI see a number of hits on StackOverflow. This one talks about how to convert layouts: https://stackoverflow.com/questions/37689423/convert-between-nhwc-and-nchw-in-tensorflow\r\n\r\n", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17061, "title": "Added Deep Speech use", "body": "", "comments": []}, {"number": 17060, "title": "OSError: [Errno 13] Permission denied.. TensorFlow couldn't install", "body": "Exception:\r\nTraceback (most recent call last):\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/commands/install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_set.py\", line 784, in install\r\n    **kwargs\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_install.py\", line 851, in install\r\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_install.py\", line 1064, in move_wheel_files\r\n    isolated=self.isolated,\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/wheel.py\", line 345, in move_wheel_files\r\n    clobber(source, lib_dir, True)\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/wheel.py\", line 316, in clobber\r\n    ensure_dir(destdir)\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/utils/__init__.py\", line 83, in ensure_dir\r\n    os.makedirs(path)\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py\", line 157, in makedirs\r\n    mkdir(name, mode)\r\nOSError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/pbr-3.1.1.dist-info'", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler \r\nHave i written custom code: No, i haven't. I just started learning to code 3 days ago and found understanding tensorflow could be a powerful tool for different engineer mechanisms.\r\nOS platform and distribution: macOS High Sierra 10.13.1 64 bit\r\nTensorFlow installed from: https://www.tensorflow.org/versions/master/install/install_mac\r\nTensorFlow Version: Latest. I used Python 2.7 and pip was already upgraded.\r\nBazel version: ...\r\nCUDE/cuDNN version:\r\nGPU model, memory: Intel HD Graphics 4000 1536 MB, 8GB 1600 MHz DDR3\r\nExact command to reproduce: $ pip install tensorflow\r\n\r\nProblem: I already tried to install through native pip and Virtuaenv. In both, i get the same error where to error starts at line 215 in the \"main\" function from the pip/basecommand.py file.\r\n\r\n\r\n\r\n"]}, {"number": 17059, "title": "Dataset API does not pass dimensionality information for its output tensor", "body": "SYSTEM INFO\r\npython version: Python 2.7.12\r\ntensorflow version: 1.4.0\r\nCUDA version: 8.0\r\nUbuntu version: Ubuntu 16.04 LTS\r\n\r\n----------------------------------------------------------------UPDATE-------------------------------------------------------------------\r\n\r\nPlease directly go to my third post which reproduces my issue with the minimum amount of code\r\n\r\n----------------------------------------------------------UPDATE FINISHED----------------------------------------------------------\r\n\r\n[https://github.com/tensorflow/tensorflow/issues/13348](url)\r\n\r\nMy problem is very similar to the issue above but I did not find solution in that post.\r\n\r\n```\r\ndef image_parser(image_name):\r\n    im_file = os.path.join(cfg.DATA_DIR, 'demo', image_name)\r\n    im = cv2.imread(im_file)\r\n    blobs, im_scales = _get_blobs(im)\r\n    assert len(im_scales) == 1, \"Only single-image batch implemented\"\r\n    im_blob = blobs['data']\r\n    blobs['im_info'] = np.array([im_blob.shape[1], im_blob.shape[2], im_scales[0]], dtype=np.float32)\r\n    return blobs['data'], blobs['im_info'], im_scales, im\r\n\r\ndef _get_blobs(im):\r\n  \"\"\"Convert an image and RoIs within that image into network inputs.\"\"\"\r\n  blobs = {}\r\n  blobs['data'], im_scale_factors = _get_image_blob(im)\r\n\r\n  return blobs, im_scale_factors\r\n\r\ndef _get_image_blob(im):\r\n  \"\"\"Converts an image into a network input.\r\n  Arguments:\r\n    im (ndarray): a color image in BGR order\r\n  Returns:\r\n    blob (ndarray): a data blob holding an image pyramid\r\n    im_scale_factors (list): list of image scales (relative to im) used\r\n      in the image pyramid\r\n  \"\"\"\r\n  im_orig = im.astype(np.float32, copy=True)\r\n  im_orig -= cfg.PIXEL_MEANS\r\n\r\n  im_shape = im_orig.shape\r\n  im_size_min = np.min(im_shape[0:2])\r\n  im_size_max = np.max(im_shape[0:2])\r\n\r\n  processed_ims = []\r\n  im_scale_factors = []\r\n\r\n  for target_size in cfg.TEST.SCALES:\r\n    im_scale = float(target_size) / float(im_size_min)\r\n    # Prevent the biggest axis from being more than MAX_SIZE\r\n    if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:\r\n      im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)\r\n    im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\r\n            interpolation=cv2.INTER_LINEAR)\r\n    im_scale_factors.append(im_scale)\r\n    processed_ims.append(im)\r\n\r\n  # Create a blob to hold the input images\r\n  blob = im_list_to_blob(processed_ims)\r\n  return blob, np.array(im_scale_factors)\r\n\r\n\r\ndef im_list_to_blob(ims):\r\n  \"\"\"Convert a list of images into a network input.\r\n\r\n  Assumes images are already prepared (means subtracted, BGR order, ...).\r\n  \"\"\"\r\n  max_shape = np.array([im.shape for im in ims]).max(axis=0)\r\n  num_images = len(ims)\r\n  blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),\r\n                  dtype=np.float32)\r\n  for i in range(num_images):\r\n    im = ims[i]\r\n    blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\r\n\r\n  return blob\r\n```\r\n\r\nI defined a function `image_parser` which will parse image file name into 4 numpy arrays (it has three subsequent function calls,  ` _get_blobs`, ` _get_image_blob`,  `im_list_to_blob`)\r\n\r\nThen I construct a dataset with `py_func` mapping to form the input pipeline:\r\n\r\n   ```\r\n    images = []\r\n    for root, dirs, files in os.walk('./data/demo/'):\r\n        for file in files:\r\n            if file.endswith('jpg'):\r\n                images.append(file)\r\n\r\n    # dataset construction\r\n    im_dataset = tf.data.Dataset.from_tensor_slices(images)\r\n    im_dataset = im_dataset.map(lambda image:\r\n                                tuple(tf.py_func(image_parser, [image], [tf.float32, tf.float32, tf.float64, tf.uint8])),\r\n                                num_parallel_calls = 2)\r\n    im_dataset = im_dataset.prefetch(4)\r\n    print(\"output data type is \", im_dataset.output_types)\r\n    print(\"output data shape is \", im_dataset.output_shapes)\r\n    iterator = im_dataset.make_initializable_iterator()\r\n    with tf.Session() as sess:\r\n        sess.run(iterator.initializer)\r\n        a = sess.run(iterator.get_next())\r\n    print(\"shape of the run results are: \")\r\n    print(a[0].shape)\r\n    print(a[1].shape)\r\n    print(a[2].shape)\r\n    print(a[3].shape)\r\n```\r\n\r\nWhen I print the shape of my output tensors from dataset right after my dataset construction, the print is:\r\n```\r\noutput data type is  (tf.float32, tf.float32, tf.float64, tf.uint8)\r\noutput data shape is  (TensorShape(None), TensorShape(None), TensorShape(None), TensorShape(None))\r\n```\r\nthe shape of tensors is None.\r\n\r\nHowever I could print out the output tensor shape after my sess.run. \r\n```\r\nshape of the run results are: \r\n(1, 600, 800, 3)\r\n(3,)\r\n(1,)\r\n(375, 500, 3)\r\n```\r\n\r\nI need the shape of the output tensor from dataset to be defined to feed into my graph. Thanks!", "comments": ["dataset expert @mrry ", "I just confirmed that the problem comes from the use of `tf.py_func`. I wrote another simple piece of code, and found out that the shape is defined if the mapping function is within `tensorflow` (which means don't use `tf.py_func`), otherwise the shape is `None`\r\nFor example, try this first:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\ndef image_parser(image_name):\r\n    a = np.array([1.0,2.0,3.0], dtype=np.float32)\r\n    return a\r\n\r\nimages = [[1,2,3],[4,5,6]]\r\nim_dataset = tf.data.Dataset.from_tensor_slices(images)\r\nim_dataset = im_dataset.map(lambda image:tuple(tf.py_func(image_parser, [image], [tf.float32])), num_parallel_calls = 2)\r\nim_dataset = im_dataset.prefetch(4)\r\niterator = im_dataset.make_initializable_iterator()\r\nprint(im_dataset.output_shapes)\r\n\r\n```\r\nIt will give you `(TensorShape(None),)`\r\nIf you try this:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef image_parser(image_name)\r\n    return image_name\r\nimages = [[1,2,3],[4,5,6]]\r\nim_dataset = tf.data.Dataset.from_tensor_slices(images)\r\nim_dataset = im_dataset.map(image_parser)\r\nim_dataset = im_dataset.prefetch(4)\r\niterator = im_dataset.make_initializable_iterator()\r\nprint(im_dataset.output_shapes)\r\n\r\n```\r\nIt will give you the exact tensor dimension `(3,)`\r\nBut I need the `tf.py_func` in my case. Is there any way to resolve this?", "seems related to #16052?", "@facaiy Hi Thanks for pointing that out. I find the answer from that! I will close this issue"]}, {"number": 17058, "title": "Documentation api reference badge added in Readme.md and added new header for contribution guidelines", "body": "* There is no documentation link provided in Readme.md since added a nice badge with api reference link.\r\n* Also I have added a nice header for contribution guidelines.\r\n\r\nTo see how it looks, please see here:\r\nhttps://github.com/rajendraarora16/tensorflow\r\n", "comments": ["@rmlarsen @martinwicke  please review :)", "@gunan reminds me, the icons are served from ci.tensorfllow.org. ", "Well, there are some partial alternatives, but I have no replacement for these yet.\r\nFor now, on Feb 28 these badges are going away together with ci.tensorflow.org", "@martinwicke Thanks for reviewing :)\r\n\r\nI have found some google repos like **https://github.com/google/go-github** are also using badges from shields.io.\r\n\r\nCan we use the same for tensorflow?", "Thank you @martinwicke :)"]}, {"number": 17057, "title": "Fix typos in low-level introduction documentation", "body": "Remove extraneous comma.\r\nCapitalize 'Loss' title.\r\nAdd missing space to 'minimize the'.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17056, "title": "tf.reduce_min([inf, nan]).eval() == inf, should be nan", "body": "### Describe the problem\r\n\r\n`tf.reduce_min` and `tf.reduce_max` do not propagate nans correctly.  E.g.,\r\n\r\n    >>> tf.reduce_min([inf, nan]).eval()\r\n    inf\r\n\r\nbut the correct answer (the one most useful for debugging) is `nan`.  This is presumably due to a backwards comparison, similar to the old `tf.nn.relu(nan) == 0` bug that @alexalemi found.\r\n\r\n### Source code / logs\r\nHere's a colab illustrating the problem with TensorFlow 1.6.0-rc1: https://drive.google.com/file/d/1nDA0Q48PveBlx_D5Zurchbw8l7eczRSB/view?usp=sharing", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS Platform and distribution: colab\r\nTensorFlow installed from: intrinsic to colab\r\nTensorFlow version: 1.6.0-rc1\r\nBazel version: n/a\r\nCUDA/cuDNN version: none\r\nGPU model and memory: none\r\nExact command to reproduce: `tf.reduce_min([inf, nan]).eval()` or run the colab", "Oops, looks like the colab was only shared within OpenAI.  Should be fixed.", "Related issue: https://github.com/tensorflow/tensorflow/issues/12659", "@mrry Is the underlying issue a dupe of #12659?", "It sounds like it. Closing as a duplicate of #12659.", "@mrry A dupe means that fixing #12659 will necessarily fix this one, which isn't obvious to me.  Similar causes doesn't mean duplicate.", "Assigning to @benoitsteiner, since he's got #12569, and is best placed to figure out if there is a common solution.", "Gah, this is a bug in `fmin` itself: https://en.cppreference.com/w/c/numeric/math/fmin :(\r\n\r\n> If one of the two arguments is NaN, the value of the other argument is returned", "Julia thread which similarly concluded that the C standard is broken and fixed Julia to not follow the broken C standard: https://github.com/JuliaLang/julia/issues/7866", "We depend on  `Eigen::internal::MinReducer<float>()` for the implementation of `tf.reduce_min()`, so I'm assigning this to @rmlarsen, who's most qualified to comment on the Eigen behavior.", "Hi @girving ! This issue is not replicating in TF [2.7](https://colab.sandbox.google.com/gist/mohantym/9b256232072da5f2e830d54bb1085fe1/github_17056.ipynb#scrollTo=iwNRaKeRrK8M) .Closing as it is resolved in latest stable version 2.7 . Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}]