[{"number": 46018, "title": "Update Hello World README to point to new model", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46017, "title": "class_id support for PrecisionAtRecall, RecallAtPrecision, and related metrics", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`tf.keras.metrics.Precision` and `tf.keras.metrics.Recall` both include an optional `class_id` keyword argument to compute metrics in a multi-class setting for a given `class_id`. It would be nice if other metrics shared this feature including `PrecisionAtRecall` and `RecallAtPrecision` (could probably easily be added to any metric that uses `metrics_utils.update_confusion_matrix_variables`).\r\n\r\n**Will this change the current api? How?** No, `class_id` is an optional keyword argument.\r\n\r\n**Who will benefit with this feature?** Users wanting to monitor class-specific metrics in a multi-class setting.\r\n", "comments": ["PR to add support for `class_id` on all subclasses of `SensitivitySpecificityBase`: https://github.com/tensorflow/tensorflow/pull/46099", "Closed by https://github.com/tensorflow/tensorflow/pull/46099"]}, {"number": 46016, "title": "TypeError: sample_chain() got an unexpected keyword argument 'seed'", "body": "Error with Tensorflow 2.0 using MCMC on MacOS 10.13.6\r\n\r\n**The error on the console:**\r\n\r\n```\r\n2020-12-27 22:06:48.253835: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-27 22:06:48.254353: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\r\nobjc[69111]: Class zmAppHelper is implemented in both /Library/ScriptingAdditions/zOLPluginInjection.osax/Contents/MacOS/zOLPluginInjection (0x1a48eaf4f0) and /Library/Application Support/Microsoft/ZoomOutlookPlugin/zOutlookPlugin64.bundle/Contents/MacOS/zOutlookPlugin64 (0x1a490e0518). One of the two will be used. Which one is undefined.\r\nobjc[69111]: class `ERCalendarEventEditorWindowController' not linked into application\r\nTraceback (most recent call last):\r\n  File \"dc7.py\", line 131, in <module>\r\n    chains, kernel_results = run_chain(initial_state)\r\n  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 503, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/ram/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in converted code:\r\n\r\n    dc7.py:117 run_chain  *\r\n        return tfp.mcmc.sample_chain(\r\n\r\n    **TypeError: sample_chain() got an unexpected keyword argument 'seed'**\r\n```\r\n\r\n**Versions**\r\n\r\n```\r\nMacOS 10.13.6 High Sierra\r\n\r\ntensorflow                2.0.0           mkl_py37hda344b4_0  \r\ntensorflow-base           2.0.0           mkl_py37h66b1bf0_0  \r\ntensorflow-estimator      2.0.0              pyh2649769_0  \r\ntensorflow-probability    0.8.0                      py_0    conda-forge\r\njupyter_client            6.1.7                      py_0  \r\njupyter_core              4.7.0            py37hecd8cb5_0  \r\njupyterlab_pygments       0.1.2                      py_0  \r\nipython                   7.19.0           py37h01d92e1_0  \r\nipython_genutils          0.2.0              pyhd3eb1b0_1  \r\npython                    3.7.9                h26836e1_0  \r\npython-dateutil           2.8.1                      py_0  \r\npython_abi                3.7                     1_cp37m    conda-forge\r\n```\r\n\r\n**The source-code:**\r\n\r\n\r\n```\r\nimport os\r\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\r\n\r\nfrom pprint import pprint\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport seaborn as sns\r\n\r\n#import tensorflow as tf\r\n#print(tf.__version__)\r\n\r\nimport tensorflow.compat.v2 as tf\r\ntf.enable_v2_behavior()\r\n\r\nimport tensorflow_probability as tfp\r\n\r\nsns.reset_defaults()\r\nsns.set_context(context = 'talk', font_scale = 0.7)\r\nplt.rcParams['image.cmap'] = 'viridis'\r\n\r\n#%matplotlib inline\r\n\r\ntfd = tfp.distributions\r\ntfb = tfp.bijectors\r\n\r\n\r\n#### ============================================\r\n\r\n#@title Utils { display-mode: \"form\" }\r\ndef print_subclasses_from_module(module, base_class, maxwidth=80):\r\n  import functools, inspect, sys\r\n  subclasses = [name for name, obj in inspect.getmembers(module)\r\n                if inspect.isclass(obj) and issubclass(obj, base_class)]\r\n  def red(acc, x):\r\n    if not acc or len(acc[-1]) + len(x) + 2 > maxwidth:\r\n      acc.append(x)\r\n    else:\r\n      acc[-1] += \", \" + x\r\n    return acc\r\n  print('\\n'.join(functools.reduce(red, subclasses, [])))\r\n\r\n# Generate some data\r\ndef f(x, w):\r\n  # Pad x with 1's so we can add bias via matmul\r\n  x = tf.pad(x, [[1, 0], [0, 0]], constant_values=1)\r\n  linop = tf.linalg.LinearOperatorFullMatrix(w[..., np.newaxis])\r\n  result = linop.matmul(x, adjoint=True)\r\n  return result[..., 0, :]\r\n\r\nnum_features = 2\r\nnum_examples = 50\r\nnoise_scale = .5\r\ntrue_w = np.array([-1., 2., 3.])\r\n\r\nxs = np.random.uniform(-1., 1., [num_features, num_examples])\r\nys = f(xs, true_w) + np.random.normal(0., noise_scale, size=num_examples)\r\n\r\n# Visualize the data set\r\nplt.scatter(*xs, c=ys, s=100, linewidths=0)\r\n\r\ngrid = np.meshgrid(*([np.linspace(-1, 1, 100)] * 2))\r\nxs_grid = np.stack(grid, axis=0)\r\nfs_grid = f(xs_grid.reshape([num_features, -1]), true_w)\r\nfs_grid = np.reshape(fs_grid, [100, 100])\r\nplt.colorbar()\r\nplt.contour(xs_grid[0, ...], xs_grid[1, ...], fs_grid, 20, linewidths=1)\r\nplt.show()\r\n\r\n### Sampling the noise scale\r\n\r\n# Define the joint_log_prob function, and our unnormalized posterior.\r\ndef joint_log_prob(w, sigma, x, y):\r\n  # Our model in maths is\r\n  #   w ~ MVN([0, 0, 0], diag([1, 1, 1]))\r\n  #   y_i ~ Normal(w @ x_i, noise_scale),  i=1..N\r\n\r\n  rv_w = tfd.MultivariateNormalDiag(\r\n    loc=np.zeros(num_features + 1),\r\n    scale_diag=np.ones(num_features + 1))\r\n  \r\n  rv_sigma = tfd.LogNormal(np.float64(1.), np.float64(5.))\r\n\r\n  rv_y = tfd.Normal(f(x, w), sigma[..., np.newaxis])\r\n  return (rv_w.log_prob(w) +\r\n          rv_sigma.log_prob(sigma) +\r\n          tf.reduce_sum(rv_y.log_prob(y), axis=-1))\r\n\r\n# Create our unnormalized target density by currying x and y from the joint.\r\ndef unnormalized_posterior(w, sigma):\r\n  return joint_log_prob(w, sigma, xs, ys)\r\n\r\n\r\n# Create an HMC TransitionKernel\r\nhmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\r\n  target_log_prob_fn=unnormalized_posterior,\r\n  step_size=np.float64(.1),\r\n  num_leapfrog_steps=4)\r\n\r\n\r\n\r\n# Create a TransformedTransitionKernl\r\ntransformed_kernel = tfp.mcmc.TransformedTransitionKernel(\r\n    inner_kernel=hmc_kernel,\r\n    bijector=[tfb.Identity(),    # w\r\n              tfb.Invert(tfb.Softplus())])   # sigma\r\n\r\n\r\n# Apply a simple step size adaptation during burnin\r\n@tf.function\r\ndef run_chain(initial_state, num_results=1000, num_burnin_steps=500):\r\n  adaptive_kernel = tfp.mcmc.SimpleStepSizeAdaptation(\r\n      transformed_kernel,\r\n      num_adaptation_steps=int(.8 * num_burnin_steps),\r\n      target_accept_prob=np.float64(.75))\r\n\r\n  return tfp.mcmc.sample_chain(\r\n    num_results=num_results,\r\n    num_burnin_steps=num_burnin_steps,\r\n    current_state=initial_state,\r\n    kernel=adaptive_kernel,\r\n    seed=(0, 1),\r\n    trace_fn=lambda cs, kr: kr)\r\n\r\n\r\n# Instead of a single set of initial w's, we create a batch of 8.\r\nnum_chains = 8\r\ninitial_state = [np.zeros([num_chains, num_features + 1]),\r\n                 .54 * np.ones([num_chains], dtype=np.float64)]\r\n\r\nchains, kernel_results = run_chain(initial_state)\r\n\r\nr_hat = tfp.mcmc.potential_scale_reduction(chains)\r\nprint(\"Acceptance rate:\", kernel_results.inner_results.inner_results.is_accepted.numpy().mean())\r\nprint(\"R-hat diagnostic (per w variable):\", r_hat[0].numpy())\r\nprint(\"R-hat diagnostic (sigma):\", r_hat[1].numpy())\r\n\r\nw_chains, sigma_chains = chains\r\n```\r\n\r\n", "comments": ["@srinivasanram \r\nCan you please upgrade your tf version to latest and let us know if you face any issues.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46016\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46016\">No</a>\n"]}, {"number": 46015, "title": "Cross-compilation for Raspberry Pi fails on macOS", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: -\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0 20160609\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nCross-compilation for Raspberry Pi fails on macOS. The same steps work well on an Ubuntu server.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\ngit checkout r2.4\r\ntensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\\r\n    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n> WORKSPACE: /Users/aldemim/ws/tensorflow\r\n> CI_DOCKER_BUILD_EXTRA_PARAMS: \r\n> CI_DOCKER_EXTRA_PARAMS: \r\n> COMMAND: tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n> CI_COMMAND_PREFIX: ./tensorflow/tools/ci_build/builds/with_the_same_user ./tensorflow/tools/ci_build/builds/configured pi-python37\r\n> CONTAINER_TYPE: pi-python37\r\n> BUILD_TAG: tf_ci\r\n>   (docker container name will be tf_ci.pi-python37)\r\n> \r\n> Building container (tf_ci.pi-python37)...\r\n> [+] Building 2.0s (18/18) FINISHED                                                                                                                                                                       \r\n>  => [internal] load build definition from Dockerfile.pi-python37                                                                                                                                    0.0s\r\n>  => => transferring dockerfile: 49B                                                                                                                                                                 0.0s\r\n>  => [internal] load .dockerignore                                                                                                                                                                   0.0s\r\n>  => => transferring context: 2B                                                                                                                                                                     0.0s\r\n>  => [internal] load metadata for docker.io/library/ubuntu:16.04                                                                                                                                     1.9s\r\n>  => [auth] library/ubuntu:pull token for registry-1.docker.io                                                                                                                                       0.0s\r\n>  => [ 1/12] FROM docker.io/library/ubuntu:16.04@sha256:3355b6e4ba1b12071ba5fe9742042a2f10b257c908fbdfac81912a16eb463879                                                                             0.0s\r\n>  => [internal] load build context                                                                                                                                                                   0.0s\r\n>  => => transferring context: 1.74kB                                                                                                                                                                 0.0s\r\n>  => CACHED [ 2/12] COPY install/*.sh /install/                                                                                                                                                      0.0s\r\n>  => CACHED [ 3/12] RUN /install/install_bootstrap_deb_packages.sh                                                                                                                                   0.0s\r\n>  => CACHED [ 4/12] RUN add-apt-repository -y ppa:openjdk-r/ppa &&     add-apt-repository -y ppa:george-edison55/cmake-3.x                                                                           0.0s\r\n>  => CACHED [ 5/12] RUN /install/install_deb_packages.sh                                                                                                                                             0.0s\r\n>  => CACHED [ 6/12] RUN /install/install_pi_python3x_toolchain.sh \"3.7\"                                                                                                                              0.0s\r\n>  => CACHED [ 7/12] RUN /install/install_bazel.sh                                                                                                                                                    0.0s\r\n>  => CACHED [ 8/12] RUN /install/install_proto3.sh                                                                                                                                                   0.0s\r\n>  => CACHED [ 9/12] RUN /install/install_buildifier.sh                                                                                                                                               0.0s\r\n>  => CACHED [10/12] RUN /install/install_auditwheel.sh                                                                                                                                               0.0s\r\n>  => CACHED [11/12] RUN /install/install_golang.sh                                                                                                                                                   0.0s\r\n>  => CACHED [12/12] COPY install/.bazelrc /etc/bazel.bazelrc                                                                                                                                         0.0s\r\n>  => exporting to image                                                                                                                                                                              0.0s\r\n>  => => exporting layers                                                                                                                                                                             0.0s\r\n>  => => writing image sha256:0102277c114ff3d600ff1e82e705a7522bf542d9a22125c9c3fa391b0838979b                                                                                                        0.0s\r\n>  => => naming to docker.io/library/tf_ci.pi-python37                                                                                                                                                0.0s\r\n> Running 'tensorflow/tools/ci_build/pi/build_raspberry_pi.sh' inside tf_ci.pi-python37...\r\n> Reading package lists...\r\n> Building dependency tree...\r\n> Reading state information...\r\n> sudo is already the newest version (1.8.16-0ubuntu1.9).\r\n> 0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\r\n> dialout:x:20:\r\n> adduser: Warning: The home directory `/Users/aldemim/ws/tensorflow/bazel-ci_build-cache' does not belong to the user you are currently creating.\r\n> /workspace /workspace\r\n> You have bazel 3.1.0 installed.\r\n> Found possible Python library paths:\r\n>   /usr/lib/python3/dist-packages\r\n>   /usr/local/lib/python3.7/dist-packages\r\n> Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n> Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded.\r\n> \r\n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n> \r\n> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds.\r\n> \r\n> Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n>         --config=mkl            # Build with MKL support.\r\n>         --config=mkl_aarch64    # Build with oneDNN support for Aarch64.\r\n>         --config=monolithic     # Config for mostly static monolithic build.\r\n>         --config=ngraph         # Build with Intel nGraph support.\r\n>         --config=numa           # Build with NUMA support.\r\n>         --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n>         --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\n> Preconfigured Bazel build configs to DISABLE default on features:\r\n>         --config=noaws          # Disable AWS S3 filesystem support.\r\n>         --config=nogcp          # Disable GCP support.\r\n>         --config=nohdfs         # Disable HDFS support.\r\n>         --config=nonccl         # Disable NVIDIA NCCL support.\r\n> /workspace\r\n> Extracting Bazel installation...\r\n> TF_BUILD_INFO = {container_type: \"pi-python37\", command: \"tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\", source_HEAD: \"5485ec964e7d51b5e51139943e02f8241aa56cfa\", source_remote_origin: \"https://github.com/tensorflow/tensorflow.git\", OS: \"Linux\", kernel: \"4.19.121-linuxkit\", architecture: \"x86_64\", processor: \"Intel(R) Core(TM) i7-8569U CPU @ 2.80GHz\", processor_count: \"4\", memory_total: \"2036452 kB\", swap_total: \"1048572 kB\", Bazel_version: \"Build label: 3.1.0\", Java_version: \"1.8.0_275\", Python_version: \"2.7.12\", gpp_version: \"g++ (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\", swig_version: \"\", NVIDIA_driver_version: \"\", CUDA_device_count: \"0\", CUDA_device_names: \"\", CUDA_toolkit_version: \"\"}\r\n> You have bazel 3.1.0 installed.\r\n> Found possible Python library paths:\r\n>   /usr/local/lib/python3.7/dist-packages\r\n>   /usr/lib/python3/dist-packages\r\n> Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.7/dist-packages]\r\n> Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded.\r\n> \r\n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n> \r\n> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds.\r\n> \r\n> Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n>         --config=mkl            # Build with MKL support.\r\n>         --config=mkl_aarch64    # Build with oneDNN support for Aarch64.\r\n>         --config=monolithic     # Config for mostly static monolithic build.\r\n>         --config=ngraph         # Build with Intel nGraph support.\r\n>         --config=numa           # Build with NUMA support.\r\n>         --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n>         --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\n> Preconfigured Bazel build configs to DISABLE default on features:\r\n>         --config=noaws          # Disable AWS S3 filesystem support.\r\n>         --config=nogcp          # Disable GCP support.\r\n>         --config=nohdfs         # Disable HDFS support.\r\n>         --config=nonccl         # Disable NVIDIA NCCL support.\r\n> Configuration finished\r\n> Building for the Pi Two/Three, with NEON acceleration\r\n> INFO: Options provided by the client:\r\n>   Inherited 'common' options: --isatty=0 --terminal_columns=80\r\n> INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:\r\n>   Inherited 'common' options: --color=yes\r\n> INFO: Reading rc options for 'build' from /workspace/.bazelrc:\r\n>   Inherited 'common' options: --experimental_repo_remote_exec\r\n> INFO: Reading rc options for 'build' from /etc/bazel.bazelrc:\r\n>   'build' options: --verbose_failures --spawn_strategy=standalone --strategy=Genrule=standalone\r\n> INFO: Reading rc options for 'build' from /workspace/.bazelrc:\r\n>   'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\n> INFO: Reading rc options for 'build' from /workspace/.tf_configure.bazelrc:\r\n>   'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3.7 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.7/dist-packages --python_path=/usr/local/bin/python3.7 --action_env TF_CONFIGURE_IOS=0\r\n> INFO: Found applicable config definition build:short_logs in file /workspace/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\n> INFO: Found applicable config definition build:v2 in file /workspace/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n> INFO: Found applicable config definition build:monolithic in file /workspace/.bazelrc: --define framework_shared_object=false\r\n> INFO: Found applicable config definition build:linux in file /workspace/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\n> INFO: Found applicable config definition build:dynamic_kernels in file /workspace/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\n> Loading: \r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> Loading: 0 packages loaded\r\n> DEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\n> DEBUG: Repository io_bazel_rules_go instantiated at:\r\n>   no stack (--record_rule_instantiation_callstack not enabled)\r\n> Repository rule git_repository defined at:\r\n>   /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\n> Loading: 0 packages loaded\r\n> Analyzing: 4 targets (3 packages loaded, 0 targets configured)\r\n> Analyzing: 4 targets (60 packages loaded, 36 targets configured)\r\n> DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\n> DEBUG: Repository io_bazel_rules_docker instantiated at:\r\n>   no stack (--record_rule_instantiation_callstack not enabled)\r\n> Repository rule git_repository defined at:\r\n>   /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\n> Analyzing: 4 targets (183 packages loaded, 3775 targets configured)\r\n> \r\n> Analyzing: 4 targets (194 packages loaded, 3818 targets configured)\r\n> \r\n> Analyzing: 4 targets (196 packages loaded, 3818 targets configured)\r\n> Analyzing: 4 targets (196 packages loaded, 3818 targets configured)\r\n> INFO: Repository aws instantiated at:\r\n>   no stack (--record_rule_instantiation_callstack not enabled)\r\n> Repository rule third_party_http_archive defined at:\r\n>   /workspace/third_party/repo.bzl:216:28: in <toplevel>\r\n> WARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\n> ERROR: An error occurred during the fetch of repository 'aws':\r\n>    java.io.IOException: Error extracting /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws/1.7.336.tar.gz to /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws: /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws/aws-cpp-sdk-dataexchange/include/aws/dataexchange/model/CreateJobResult.h (Input/output error)\r\n> INFO: Repository aarch64_compiler instantiated at:\r\n>   no stack (--record_rule_instantiation_callstack not enabled)\r\n> Repository rule tf_http_archive defined at:\r\n>   /workspace/third_party/repo.bzl:131:19: in <toplevel>\r\n> ERROR: /workspace/tensorflow/tools/pip_package/BUILD:175:1: //tensorflow/tools/pip_package:licenses depends on @aws//:LICENSE in repository @aws which failed to fetch. no such package '@aws//': java.io.IOException: Error extracting /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws/1.7.336.tar.gz to /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws: /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws/aws-cpp-sdk-dataexchange/include/aws/dataexchange/model/CreateJobResult.h (Input/output error)\r\n> ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@aws//': java.io.IOException: Error extracting /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws/1.7.336.tar.gz to /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws: /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/aws/aws-cpp-sdk-dataexchange/include/aws/dataexchange/model/CreateJobResult.h (Input/output error)\r\n> INFO: Elapsed time: 357.129s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (206 packages loaded, 3958 targets configured)\r\n> ", "comments": ["Could you check if your docker has enough memory assigned?\r\nhttps://docs.docker.com/docker-for-mac/#resources\r\n\r\nMy docker has 2GB memory assigned by default which is not enough to build TF. I changed it to 8GB and it works well.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I increased Docker's memory to 8GB and cloned the repo fresh clean, but I'm getting this error now:\r\n\r\n```\r\n(base) aldemim@3c22fb0301d7 tensorflow % tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\\r\n    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\nWORKSPACE: /Users/aldemim/ws/tensorflow\r\nCI_DOCKER_BUILD_EXTRA_PARAMS: \r\nCI_DOCKER_EXTRA_PARAMS: \r\nCOMMAND: tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\nCI_COMMAND_PREFIX: ./tensorflow/tools/ci_build/builds/with_the_same_user ./tensorflow/tools/ci_build/builds/configured pi-python37\r\nCONTAINER_TYPE: pi-python37\r\nBUILD_TAG: tf_ci\r\n  (docker container name will be tf_ci.pi-python37)\r\n\r\nBuilding container (tf_ci.pi-python37)...\r\n[+] Building 1.6s (18/18) FINISHED                                                                                                                                    \r\n => [internal] load build definition from Dockerfile.pi-python37                                                                                                 0.0s\r\n => => transferring dockerfile: 49B                                                                                                                              0.0s\r\n => [internal] load .dockerignore                                                                                                                                0.0s\r\n => => transferring context: 2B                                                                                                                                  0.0s\r\n => [internal] load metadata for docker.io/library/ubuntu:16.04                                                                                                  1.5s\r\n => [auth] library/ubuntu:pull token for registry-1.docker.io                                                                                                    0.0s\r\n => [ 1/12] FROM docker.io/library/ubuntu:16.04@sha256:e74994b7a9ec8e2129cfc6a871f3236940006ed31091de355578492ed140a39c                                          0.0s\r\n => [internal] load build context                                                                                                                                0.0s\r\n => => transferring context: 1.74kB                                                                                                                              0.0s\r\n => CACHED [ 2/12] COPY install/*.sh /install/                                                                                                                   0.0s\r\n => CACHED [ 3/12] RUN /install/install_bootstrap_deb_packages.sh                                                                                                0.0s\r\n => CACHED [ 4/12] RUN add-apt-repository -y ppa:openjdk-r/ppa &&     add-apt-repository -y ppa:george-edison55/cmake-3.x                                        0.0s\r\n => CACHED [ 5/12] RUN /install/install_deb_packages.sh                                                                                                          0.0s\r\n => CACHED [ 6/12] RUN /install/install_pi_python3x_toolchain.sh \"3.7\"                                                                                           0.0s\r\n => CACHED [ 7/12] RUN /install/install_bazel.sh                                                                                                                 0.0s\r\n => CACHED [ 8/12] RUN /install/install_proto3.sh                                                                                                                0.0s\r\n => CACHED [ 9/12] RUN /install/install_buildifier.sh                                                                                                            0.0s\r\n => CACHED [10/12] RUN /install/install_auditwheel.sh                                                                                                            0.0s\r\n => CACHED [11/12] RUN /install/install_golang.sh                                                                                                                0.0s\r\n => CACHED [12/12] COPY install/.bazelrc /etc/bazel.bazelrc                                                                                                      0.0s\r\n => exporting to image                                                                                                                                           0.0s\r\n => => exporting layers                                                                                                                                          0.0s\r\n => => writing image sha256:5ccb89c8d4daef8c9a88ec9a954fd73880c7d018f934dd94afad44524dd6964b                                                                     0.0s\r\n => => naming to docker.io/library/tf_ci.pi-python37                                                                                                             0.0s\r\nRunning 'tensorflow/tools/ci_build/pi/build_raspberry_pi.sh' inside tf_ci.pi-python37...\r\nReading package lists...\r\nBuilding dependency tree...\r\nReading state information...\r\nsudo is already the newest version (1.8.16-0ubuntu1.9).\r\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\r\ndialout:x:20:\r\nadduser: Warning: The home directory `/Users/aldemim/ws/tensorflow/bazel-ci_build-cache' does not belong to the user you are currently creating.\r\n/workspace /workspace\r\nYou have bazel 3.1.0 installed.\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.7/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.7/dist-packages]\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: \r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n/workspace\r\nTF_BUILD_INFO = {container_type: \"pi-python37\", command: \"tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\", source_HEAD: \"85c8b2a817f95a3e979ecd1ed95bff1dc1335cff\", source_remote_origin: \"https://github.com/tensorflow/tensorflow\", OS: \"Linux\", kernel: \"4.19.121-linuxkit\", architecture: \"x86_64\", processor: \"Intel(R) Core(TM) i7-8569U CPU @ 2.80GHz\", processor_count: \"4\", memory_total: \"8156056 kB\", swap_total: \"1048572 kB\", Bazel_version: \"Build label: 3.1.0\", Java_version: \"1.8.0_275\", Python_version: \"2.7.12\", gpp_version: \"g++ (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\", swig_version: \"\", NVIDIA_driver_version: \"\", CUDA_device_count: \"0\", CUDA_device_names: \"\", CUDA_toolkit_version: \"\"}\r\nYou have bazel 3.1.0 installed.\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: \r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\nBuilding for the Pi Two/Three, with NEON acceleration\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /etc/bazel.bazelrc:\r\n  Inherited 'common' options: --color=yes\r\nINFO: Reading rc options for 'build' from /workspace/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /etc/bazel.bazelrc:\r\n  'build' options: --verbose_failures --spawn_strategy=standalone --strategy=Genrule=standalone\r\nINFO: Reading rc options for 'build' from /workspace/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /workspace/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3.7 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/local/bin/python3.7 --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /workspace/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /workspace/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:monolithic in file /workspace/.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:linux in file /workspace/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /workspace/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nLoading: \r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow ... (2 packages)\r\nAnalyzing: 4 targets (3 packages loaded, 0 targets configured)\r\nAnalyzing: 4 targets (23 packages loaded, 36 targets configured)\r\nAnalyzing: 4 targets (47 packages loaded, 36 targets configured)\r\nAnalyzing: 4 targets (58 packages loaded, 36 targets configured)\r\nAnalyzing: 4 targets (71 packages loaded, 61 targets configured)\r\nAnalyzing: 4 targets (72 packages loaded, 61 targets configured)\r\nAnalyzing: 4 targets (75 packages loaded, 180 targets configured)\r\nAnalyzing: 4 targets (82 packages loaded, 215 targets configured)\r\nAnalyzing: 4 targets (118 packages loaded, 779 targets configured)\r\nAnalyzing: 4 targets (159 packages loaded, 3138 targets configured)\r\nAnalyzing: 4 targets (198 packages loaded, 3947 targets configured)\r\nAnalyzing: 4 targets (225 packages loaded, 4235 targets configured)\r\nAnalyzing: 4 targets (238 packages loaded, 4934 targets configured)\r\nAnalyzing: 4 targets (260 packages loaded, 5512 targets configured)\r\nAnalyzing: 4 targets (267 packages loaded, 5600 targets configured)\r\nAnalyzing: 4 targets (267 packages loaded, 5600 targets configured)\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/www.sqlite.org/2020/sqlite-amalgamation-3340000.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nAnalyzing: 4 targets (272 packages loaded, 5794 targets configured)\r\nAnalyzing: 4 targets (293 packages loaded, 6686 targets configured)\r\nAnalyzing: 4 targets (314 packages loaded, 8590 targets configured)\r\nAnalyzing: 4 targets (314 packages loaded, 8590 targets configured)\r\nAnalyzing: 4 targets (314 packages loaded, 8590 targets configured)\r\nAnalyzing: 4 targets (343 packages loaded, 9198 targets configured)\r\nAnalyzing: 4 targets (343 packages loaded, 9198 targets configured)\r\nAnalyzing: 4 targets (343 packages loaded, 9198 targets configured)\r\nAnalyzing: 4 targets (345 packages loaded, 9671 targets configured)\r\nAnalyzing: 4 targets (345 packages loaded, 9694 targets configured)\r\nAnalyzing: 4 targets (345 packages loaded, 9694 targets configured)\r\nAnalyzing: 4 targets (345 packages loaded, 9694 targets configured)\r\nAnalyzing: 4 targets (349 packages loaded, 10112 targets configured)\r\nAnalyzing: 4 targets (349 packages loaded, 10112 targets configured)\r\nAnalyzing: 4 targets (349 packages loaded, 10112 targets configured)\r\nAnalyzing: 4 targets (354 packages loaded, 10138 targets configured)\r\nAnalyzing: 4 targets (355 packages loaded, 10235 targets configured)\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/developer.arm.com/-/media/Files/downloads/gnu-a/9.2-2019.12/binrel/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu.tar.xz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nAnalyzing: 4 targets (361 packages loaded, 14512 targets configured)\r\nAnalyzing: 4 targets (368 packages loaded, 16383 targets configured)\r\nAnalyzing: 4 targets (397 packages loaded, 23608 targets configured)\r\nAnalyzing: 4 targets (398 packages loaded, 25473 targets configured)\r\nAnalyzing: 4 targets (398 packages loaded, 25473 targets configured)\r\nAnalyzing: 4 targets (398 packages loaded, 25473 targets configured)\r\nINFO: Analyzed 4 targets (403 packages loaded, 33805 targets configured).\r\nINFO: Found 4 targets...\r\n[0 / 1,422] [Prepa] BazelWorkspaceStatusAction stable-status.txt ... (2 actions, 0 running)\r\n[159 / 1,611] Compiling external/com_google_protobuf/src/google/protobuf/compiler/main.cc [for host]; 3s local ... (4 actions, 3 running)\r\n[580 / 2,392] Compiling external/llvm-project/mlir/tools/mlir-tblgen/mlir-tblgen.cpp [for host]; 4s local ... (4 actions, 3 running)\r\n[928 / 2,891] Compiling tensorflow/core/lib/io/zlib_compression_options.cc [for host]; 2s local ... (4 actions, 3 running)\r\n[1,475 / 4,108] Compiling external/llvm-project/llvm/utils/TableGen/Attributes.cpp [for host]; 4s local ... (4 actions, 3 running)\r\n\r\n\r\n[3,041 / 9,193] Compiling external/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp; 12s local ... (4 actions, 3 running)\r\n[3,498 / 9,193] Compiling tensorflow/core/data/service/credentials_factory.cc; 9s local ... (4 actions, 3 running)\r\n[3,966 / 9,193] Compiling external/aws/aws-cpp-sdk-s3/source/S3Client.cpp; 17s local ... (4 actions, 3 running)\r\n[4,996 / 9,193] Compiling tensorflow/core/protobuf/cluster.pb.cc [for host]; 6s local ... (4 actions, 3 running)\r\n[5,342 / 9,193] Compiling tensorflow/compiler/xla/service/hlo_evaluator_typed_visitor_uint16.cc; 32s local ... (4 actions running)\r\n[5,714 / 9,193] Compiling tensorflow/core/kernels/padding_fifo_queue.cc; 21s local ... (4 actions, 3 running)\r\n[6,086 / 9,193] Compiling tensorflow/core/kernels/identity_reader_op.cc; 16s local ... (4 actions running)\r\n[6,425 / 9,193] Compiling tensorflow/core/kernels/batch_matmul_op_real.cc; 154s local ... (4 actions, 3 running)\r\nERROR: /workspace/tensorflow/core/kernels/BUILD:3376:1: C++ compilation of rule '//tensorflow/core/kernels:cwise_op' failed (Exit 4): arm-rpi-linux-gnueabihf-gcc failed: error executing command \r\n  (cd /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH='' \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/local/bin/python3.7 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n  /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/arm-rpi-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -isystem /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/lib/gcc/arm-rpi-linux-gnueabihf/6.5.0/include -isystem /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/lib/gcc/arm-rpi-linux-gnueabihf/6.5.0/include-fixed -isystem /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/arm-rpi-linux-gnueabihf/include/c++/6.5.0/ -isystem /Users/aldemim/ws/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_aldemim/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/arm-rpi-linux-gnueabihf/sysroot/usr/include/ -isystem /usr/include/python3.7 -isystem /usr/include/ -MD -MF bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_select.pic.d '-frandom-seed=bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_select.pic.o' -fPIC -DCURL_STATICLIB -DTF_USE_SNAPPY -DHAVE_SYS_UIO_H -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -iquote . -iquote bazel-out/armeabi-opt/bin -iquote external/com_google_absl -iquote bazel-out/armeabi-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/armeabi-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/armeabi-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/armeabi-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/armeabi-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/armeabi-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/armeabi-opt/bin/external/zlib -iquote external/com_googlesource_code_re2 -iquote bazel-out/armeabi-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/armeabi-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/armeabi-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/armeabi-opt/bin/external/highwayhash -iquote external/double_conversion -iquote bazel-out/armeabi-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/armeabi-opt/bin/external/snappy -iquote external/curl -iquote bazel-out/armeabi-opt/bin/external/curl -iquote external/boringssl -iquote bazel-out/armeabi-opt/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/armeabi-opt/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/armeabi-opt/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/armeabi-opt/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/armeabi-opt/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/armeabi-opt/bin/external/aws-checksums -isystem external/nsync/public -isystem bazel-out/armeabi-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/armeabi-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/armeabi-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/armeabi-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/armeabi-opt/bin/external/zlib -isystem external/farmhash_archive/src -isystem bazel-out/armeabi-opt/bin/external/farmhash_archive/src -isystem external/double_conversion -isystem bazel-out/armeabi-opt/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/armeabi-opt/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/armeabi-opt/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/armeabi-opt/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/armeabi-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/armeabi-opt/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/armeabi-opt/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/armeabi-opt/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/armeabi-opt/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/armeabi-opt/bin/external/aws-checksums/include -w -DAUTOLOAD_DYNAMIC_KERNELS '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -fno-tree-pre -fpermissive -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer '-std=c++14' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions -DTENSORFLOW_MONOLITHIC_BUILD -pthread -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c tensorflow/core/kernels/cwise_op_select.cc -o bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_select.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from external/com_google_absl/absl/container/inlined_vector.h:53:0,\r\n                 from ./tensorflow/core/lib/gtl/inlined_vector.h:19,\r\n                 from ./tensorflow/core/lib/gtl/array_slice.h:22,\r\n                 from ./tensorflow/core/framework/tensor_shape.h:26,\r\n                 from ./tensorflow/core/framework/resource_handle.h:19,\r\n                 from ./tensorflow/core/framework/register_types.h:21,\r\n                 from tensorflow/core/kernels/cwise_op_select.cc:23:\r\nexternal/com_google_absl/absl/container/internal/inlined_vector.h: In member function 'void absl::lts_2020_02_25::inlined_vector_internal::Storage<T, N, A>::Resize(ValueAdapter, absl::lts_2020_02_25::inlined_vector_internal::Storage<T, N, A>::size_type) [with ValueAdapter = absl::lts_2020_02_25::inlined_vector_internal::DefaultValueAdapter<std::allocator<long long int> >; T = long long int; unsigned int N = 4u; A = std::allocator<long long int>]':\r\nexternal/com_google_absl/absl/container/internal/inlined_vector.h:543:6: note: parameter passing for argument of type 'absl::lts_2020_02_25::inlined_vector_internal::DefaultValueAdapter<std::allocator<long long int> >' will change in GCC 7.1\r\n auto Storage<T, N, A>::Resize(ValueAdapter values, size_type new_size) -> void {\r\n      ^~~~~~~~~~~~~~~~\r\nexternal/com_google_absl/absl/container/internal/inlined_vector.h: In member function 'void absl::lts_2020_02_25::inlined_vector_internal::Storage<T, N, A>::Resize(ValueAdapter, absl::lts_2020_02_25::inlined_vector_internal::Storage<T, N, A>::size_type) [with ValueAdapter = absl::lts_2020_02_25::inlined_vector_internal::CopyValueAdapter<std::allocator<long long int> >; T = long long int; unsigned int N = 4u; A = std::allocator<long long int>]':\r\nexternal/com_google_absl/absl/container/internal/inlined_vector.h:543:6: note: parameter passing for argument of type 'absl::lts_2020_02_25::inlined_vector_internal::CopyValueAdapter<std::allocator<long long int> >' will change in GCC 7.1\r\nexternal/com_google_absl/absl/container/internal/inlined_vector.h:543:6: note: parameter passing for argument of type 'absl::lts_2020_02_25::inlined_vector_internal::CopyValueAdapter<std::allocator<long long int> >' will change in GCC 7.1\r\nIn file included from ./tensorflow/core/lib/gtl/inlined_vector.h:19:0,\r\n                 from ./tensorflow/core/lib/gtl/array_slice.h:22,\r\n                 from ./tensorflow/core/framework/tensor_shape.h:26,\r\n                 from ./tensorflow/core/framework/resource_handle.h:19,\r\n                 from ./tensorflow/core/framework/register_types.h:21,\r\n                 from tensorflow/core/kernels/cwise_op_select.cc:23:\r\nexternal/com_google_absl/absl/container/inlined_vector.h: In constructor 'tensorflow::BCastList<N>::BCastList(const Vec (&)[N], bool, bool) [with int N = 2]':\r\nexternal/com_google_absl/absl/container/inlined_vector.h:547:30: note: parameter passing for argument of type 'absl::lts_2020_02_25::inlined_vector_internal::DefaultValueAdapter<std::allocator<long long int> >' will change in GCC 7.1\r\n   void resize(size_type n) { storage_.Resize(DefaultValueAdapter(), n); }\r\n                              ^~~~~~~~\r\nexternal/com_google_absl/absl/container/inlined_vector.h:555:5: note: parameter passing for argument of type 'absl::lts_2020_02_25::InlinedVector<long long int, 4u>::CopyValueAdapter {aka absl::lts_2020_02_25::inlined_vector_internal::CopyValueAdapter<std::allocator<long long int> >}' will change in GCC 7.1\r\n     storage_.Resize(CopyValueAdapter(v), n);\r\n     ^~~~~~~~\r\nexternal/com_google_absl/absl/container/inlined_vector.h:555:5: note: parameter passing for argument of type 'absl::lts_2020_02_25::InlinedVector<long long int, 4u>::CopyValueAdapter {aka absl::lts_2020_02_25::inlined_vector_internal::CopyValueAdapter<std::allocator<long long int> >}' will change in GCC 7.1\r\nexternal/com_google_absl/absl/container/inlined_vector.h: In constructor 'tensorflow::BCastList<N>::BCastList(const Vec (&)[N], bool, bool) [with int N = 3]':\r\nexternal/com_google_absl/absl/container/inlined_vector.h:547:30: note: parameter passing for argument of type 'absl::lts_2020_02_25::inlined_vector_internal::DefaultValueAdapter<std::allocator<long long int> >' will change in GCC 7.1\r\n   void resize(size_type n) { storage_.Resize(DefaultValueAdapter(), n); }\r\n                              ^~~~~~~~\r\nexternal/com_google_absl/absl/container/inlined_vector.h:555:5: note: parameter passing for argument of type 'absl::lts_2020_02_25::InlinedVector<long long int, 4u>::CopyValueAdapter {aka absl::lts_2020_02_25::inlined_vector_internal::CopyValueAdapter<std::allocator<long long int> >}' will change in GCC 7.1\r\n     storage_.Resize(CopyValueAdapter(v), n);\r\n     ^~~~~~~~\r\nexternal/com_google_absl/absl/container/inlined_vector.h:555:5: note: parameter passing for argument of type 'absl::lts_2020_02_25::InlinedVector<long long int, 4u>::CopyValueAdapter {aka absl::lts_2020_02_25::inlined_vector_internal::CopyValueAdapter<std::allocator<long long int> >}' will change in GCC 7.1\r\nexternal/com_google_absl/absl/container/inlined_vector.h:555:5: note: parameter passing for argument of type 'absl::lts_2020_02_25::InlinedVector<long long int, 4u>::CopyValueAdapter {aka absl::lts_2020_02_25::inlined_vector_internal::CopyValueAdapter<std::allocator<long long int> >}' will change in GCC 7.1\r\narm-rpi-linux-gnueabihf-gcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <http://gcc.gnu.org/bugs.html> for instructions.\r\nINFO: Elapsed time: 9117.058s, Critical Path: 427.25s\r\nINFO: 4708 processes: 4708 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "@mstfldmr,\r\n\r\nWe are checking to see if you still need help on this issue. Can you try following this [guide](https://www.tensorflow.org/lite/guide/build_arm) to build TF lite for Rasperry-Pi and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46015\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46015\">No</a>\n"]}, {"number": 46014, "title": "How do I pass an array of bytes instead of an array of shorts", "body": "Good evening, I started to deal with this example https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/android but ran into a problem. Help me please, I want to pass an array of bytes, not shorts.\r\n```\r\nclass MainActivity : AppCompatActivity() {\r\n\r\n    companion object {\r\n\r\n        private const val TAG = \"MainActivity\"\r\n        private const val SAMPLE_RATE = 16_000\r\n        private const val BUFFER_SIZE_SECONDS = 0.3F\r\n        private const val DETECTION_THRESHOLD = 0.50F\r\n        private const val SUPPRESSION_MS = 1500\r\n        private const val MINIMUM_COUNT = 3\r\n        private const val MINIMUM_TIME_BETWEEN_SAMPLES_MS = 30L\r\n        private const val AVERAGE_WINDOW_DURATION_MS = 1_000L\r\n        private const val REQUEST_AUDIO_RECORD_PERMISSION = 200\r\n    }\r\n\r\n    private val labels = listOf(\"_silence_\", \"_unknown_\", \"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\")\r\n    private val tfLiteOptions = Interpreter.Options()\r\n    private val recordingBufferLock = ReentrantLock()\r\n\r\n    private var recordingOffset = 0\r\n    private var shouldContinue = true\r\n    private var recordingThread: Thread? = null\r\n    private var shouldContinueRecognition = true\r\n    private var recognitionThread: Thread? = null\r\n    private var recordingBuffer = ByteArray(SAMPLE_RATE)\r\n\r\n    private lateinit var tfLite: Interpreter\r\n    private lateinit var tfLiteModel: MappedByteBuffer\r\n    private lateinit var recognizeCommands: RecognizeCommands\r\n\r\n    override fun onCreate(savedInstanceState: Bundle?) {\r\n        super.onCreate(savedInstanceState)\r\n        setContentView(R.layout.activity_main)\r\n\r\n        recognizeCommands = RecognizeCommands(\r\n            labels,\r\n            AVERAGE_WINDOW_DURATION_MS,\r\n            DETECTION_THRESHOLD,\r\n            SUPPRESSION_MS,\r\n            MINIMUM_COUNT,\r\n            MINIMUM_TIME_BETWEEN_SAMPLES_MS\r\n        )\r\n\r\n        if (ContextCompat.checkSelfPermission(this, Manifest.permission.RECORD_AUDIO) == PermissionChecker.PERMISSION_GRANTED) {\r\n            initTfLite()\r\n        } else {\r\n            ActivityCompat.requestPermissions(this, arrayOf(Manifest.permission.RECORD_AUDIO), REQUEST_AUDIO_RECORD_PERMISSION)\r\n        }\r\n    }\r\n\r\n    private fun initTfLite() {\r\n        try {\r\n            tfLiteModel = loadModelFile()\r\n            tfLite = Interpreter(tfLiteModel, tfLiteOptions)\r\n\r\n            tfLite.resizeInput(0, intArrayOf(SAMPLE_RATE, 1))\r\n            tfLite.resizeInput(1, intArrayOf(1))\r\n\r\n            startRecording()\r\n            startRecognition()\r\n        } catch (exc: IOException) {\r\n            Log.e(TAG, \"Error: ${exc.message}\")\r\n        }\r\n    }\r\n\r\n    private fun startRecording() {\r\n        if (recordingThread == null) {\r\n            shouldContinue = true\r\n            recordingThread = Thread { record() }\r\n            recordingThread?.start()\r\n        }\r\n    }\r\n\r\n    private fun record() {\r\n        Process.setThreadPriority(Process.THREAD_PRIORITY_AUDIO)\r\n\r\n        val bufferSize = (SAMPLE_RATE.toFloat() * BUFFER_SIZE_SECONDS).roundToInt() * 2\r\n        val record = AudioRecord(\r\n            MediaRecorder.AudioSource.DEFAULT,\r\n            SAMPLE_RATE,\r\n            AudioFormat.CHANNEL_IN_MONO,\r\n            AudioFormat.ENCODING_PCM_16BIT,\r\n            bufferSize\r\n        )\r\n\r\n        if (record.state != AudioRecord.STATE_INITIALIZED) {\r\n            Log.e(TAG,\"Audio Record can't initialize!\")\r\n            return\r\n        }\r\n\r\n        record.startRecording()\r\n\r\n        while (shouldContinue) {\r\n            val audioBuffer = ByteArray(bufferSize)\r\n            val numberRead = record.read(audioBuffer, 0, audioBuffer.size)\r\n            val newRecordingOffset = recordingOffset + numberRead\r\n            val secondCopyLength = Math.max(0, newRecordingOffset - recordingBuffer.size)\r\n            val firstCopyLength = numberRead - secondCopyLength\r\n\r\n            recordingBufferLock.lock()\r\n            try {\r\n                System.arraycopy(audioBuffer, 0, recordingBuffer, recordingOffset, firstCopyLength)\r\n                System.arraycopy(audioBuffer, firstCopyLength, recordingBuffer, 0, secondCopyLength)\r\n                recordingOffset = newRecordingOffset % recordingBuffer.size\r\n            } finally {\r\n                recordingBufferLock.unlock()\r\n            }\r\n        }\r\n\r\n        record.stop()\r\n        record.release()\r\n    }\r\n\r\n    private fun startRecognition() {\r\n        if (recognitionThread == null) {\r\n            shouldContinueRecognition = true\r\n            recognitionThread = Thread { recognize() }\r\n            recognitionThread?.start()\r\n        }\r\n    }\r\n\r\n    private fun recognize() {\r\n        val inputBuffer = ByteArray(SAMPLE_RATE)\r\n        val floatInputBuffer = Array(SAMPLE_RATE) { FloatArray(1) }\r\n        val outputScores = Array(1) { FloatArray(labels.size) }\r\n        val sampleRateList = intArrayOf(SAMPLE_RATE)\r\n\r\n        while (shouldContinueRecognition) {\r\n            recordingBufferLock.lock()\r\n\r\n            try {\r\n                val maxLength = recordingBuffer.size\r\n                val firstCopyLength = maxLength - recordingOffset\r\n                val secondCopyLength = recordingOffset\r\n                System.arraycopy(recordingBuffer, recordingOffset, inputBuffer, 0, firstCopyLength)\r\n                System.arraycopy(recordingBuffer, 0, inputBuffer, firstCopyLength, secondCopyLength)\r\n            } finally {\r\n                recordingBufferLock.unlock()\r\n            }\r\n\r\n            for (i in 0 until SAMPLE_RATE) {\r\n                floatInputBuffer[i][0] = inputBuffer[i] / Byte.MAX_VALUE.toFloat()\r\n            }\r\n\r\n            val inputArray = arrayOf<Any>(floatInputBuffer, sampleRateList)\r\n            val outputMap: MutableMap<Int, Any> = HashMap()\r\n            outputMap[0] = outputScores\r\n\r\n            tfLite.runForMultipleInputsOutputs(inputArray, outputMap)\r\n\r\n            val result = recognizeCommands.processLatestResults(outputScores[0], System.currentTimeMillis())\r\n\r\n            if (!result.foundCommand.startsWith(\"_\") && result.isNewCommand) {\r\n                Log.d(TAG, \"Command: ${result.foundCommand} (${result.score})\")\r\n            }\r\n\r\n            try {\r\n                Thread.sleep(MINIMUM_TIME_BETWEEN_SAMPLES_MS)\r\n            } catch (exc: InterruptedException) {\r\n                Log.d(TAG, \"Error: ${exc.message}\")\r\n            }\r\n        }\r\n    }\r\n\r\n    @Throws(IOException::class)\r\n    private fun loadModelFile(): MappedByteBuffer {\r\n        val fileDescriptor = assets.openFd(\"conv_actions_frozen.tflite\")\r\n        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)\r\n        val fileChannel = inputStream.channel\r\n        val startOffset = fileDescriptor.startOffset\r\n        val declaredLength = fileDescriptor.declaredLength\r\n        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)\r\n    }\r\n\r\n    override fun onRequestPermissionsResult(\r\n        requestCode: Int,\r\n        permissions: Array<out String>,\r\n        grantResults: IntArray\r\n    ) {\r\n        super.onRequestPermissionsResult(requestCode, permissions, grantResults)\r\n        if (requestCode == REQUEST_AUDIO_RECORD_PERMISSION) {\r\n            initTfLite()\r\n        } else {\r\n            Toast.makeText(this, \"\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u0438\u0435\", Toast.LENGTH_LONG).show()\r\n            finish()\r\n        }\r\n    }\r\n}\r\n```", "comments": ["Dup of #46030", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46014\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46014\">No</a>\n"]}, {"number": 46013, "title": "Adjust error message of tf.debugging.assert_type", "body": "This PR tries to address the issue raised in #45975 where\r\nthe error message of tf.debugging.assert_type could be\r\nmisleading when the tf_type is not passed with a DType.\r\n\r\nThis PR adds additional check so that tf_type arg can be guarded\r\nif non-DType value (e.g., list, tuple etc) is passed.\r\n\r\nThis PR fixes #45975.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 46012, "title": "tf.function retracing", "body": "**System information**\r\n- OS: Ubuntu 18.04.5 LTS (Google Colab)\r\n- tf version: 2.4.0\r\n- tf git version: v2.4.0-0-g582c8d236cb\r\n\r\nCode:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom keras import *\r\nfrom keras.layers import *\r\n\r\nX = np.random.uniform(-1, 1, size = (1, 1000))\r\ny = np.array([[0.7974]])\r\n\r\nfor _ in range(6):\r\n  model = Sequential([\r\n    Input(shape = 1000),\r\n    Dense(1, activation = 'sigmoid'),\r\n  ])\r\n\r\n  model.compile(loss = 'mse', optimizer = 'adam')\r\n  model.fit(X, y, batch_size = 1, epochs = 100, verbose = 0)\r\n  print(model.predict(X))\r\n```\r\n\r\nOutput:\r\n```\r\n[[0.7982576]]\r\n[[0.7960699]]\r\n[[0.7987139]]\r\n[[0.79762185]]\r\nWARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f38dc45f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\n[[0.79733586]]\r\nWARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f38dbbf3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\n[[0.79833543]]\r\n```\r\n\r\nIs this warning a bug, or am I doing something wrong?", "comments": ["Its a warning but this behavior is deprecated. \r\nRetracing of graph is an expensive operation to do and when model was created for the first time a graph was initialized. while it is in the loop it is being retraced completely again and again over the same graph to find the answer which is initialized everytime the loop runs .so it is advised to keep the below part of the code outside the loop.\r\n\r\n![tf](https://user-images.githubusercontent.com/42847318/103238013-67680580-496f-11eb-925e-5107a4690afe.png)\r\n", "@sedol1339 \r\nThese are just warnings and will not have any impact on the performance, please follow as mentioned above.", "Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46012\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46012\">No</a>\n"]}, {"number": 46011, "title": "Fix md5sum command not found issue on mac", "body": "The PR fixes and issue where some systems viz., mac do not have `md5sum` command.\r\nIt uses alias `md5 -r` for the same which bash doesn't expand.\r\n\r\nCheck for the same and use `md5 -r` instead.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@rthadur It happens again. The butler rebuilds after the 2nd approval also without commits between approvals as we discussed some months ago at: https://github.com/tensorflow/tensorflow/pull/43754#issuecomment-704426064 \r\nAny news?"]}, {"number": 46010, "title": "Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reduce.cc:534 ", "body": "**Descriptions**\r\nWe successfully converted the Matterpot Mask RCNN object detection model (using hdf5 weight file) into tflite format using tensorflow (version 2.3.0). We got the prediction from the converted tflite model using python API of tf-nightly () due to Flex delegate issue raised when using tensorflow version 2.3.0.\r\n\r\n**Converted tflite model link** : https://drive.google.com/file/d/1kJjQnuf5FYdn0DcEAIkeMoWFXZFuM2eb/view?usp=sharing\r\n\r\nWhen we tried to get the prediction using our converted tflite model using the android application (https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) but the following error is raised at **runForMultipleInputsOutputs()** function.\r\n\r\n**Error raised in android :**\r\n```\r\n2020-12-28 11:30:01.263 6838-6857/org.tensorflow.lite.examples.detection E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.detection, PID: 6838\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reduce.cc:534 reference_ops::ReduceGeneric<T>( GetTensorData<T>(op_context->input), op_context->input->dims->data, op_context->input->dims->size, GetTensorData<T>(op_context->output), op_context->output->dims->data, op_context->output->dims->size, GetTensorData<int>(op_context->axis), num_axis, op_context->params->keep_dims, GetTensorData<int>(temp_index), GetTensorData<int>(resolved_axis), init_value, reducer) was not true.\r\n    No\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:158)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:343)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:334)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:182)\r\n        at android.os.Handler.handleCallback(Handler.java:761)\r\n        at android.os.Handler.dispatchMessage(Handler.java:98)\r\n        at android.os.Looper.loop(Looper.java:156)\r\n        at android.os.HandlerThread.run(HandlerThread.java:61)\r\n```\r\n\r\n**Used python code used for converting the model into tflite format**\r\n```\r\nmodel = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\r\nkeras_model = `model.keras_model`\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\nconverter.allow_custom_ops = True\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\nconverter.optimizations = [ tf.lite.Optimize.DEFAULT ]\r\ntflite_model = converter.convert()\r\n```\r\n\r\n\r\n**Java code used in android studio to the get the prediction**\r\nWe tested the code for single image and set input tensors accordly (images, img_metas and anchors).\r\n\r\n```\r\n// File:  org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel\r\n\r\nInterpreter.Options options = new Interpreter.Options();\r\n\r\n// Model file is loaded from assets.\r\nInterpreter Tflite = new Interpreter(modelFile, options);\r\n\r\nfloat[][][][] images = new float[1][1024][1024][3];\r\nfloat[][] img_metas = new float[1][14];\r\nfloat[][][] anchors = new float[1][261888][4];\r\n// above arrays are populated. \r\n\r\nObject[] inputArray = {images, img_metas, anchors};\r\ntensor0 = new float[1][1][1];\r\ntensor1 = new float[1][1000][2][4];\r\ntensor2 = new float[1][1000][2];\r\ntensor3 = new float[1][100][6];\r\ntensor4 = new float[1][100][28][28][2];\r\ntensor5 = new float[1][1][4];\r\ntensor6 = new float[1][1][2];\r\nMap<Integer, Object> outputMap = new HashMap<>();\r\n    outputMap.put(0, tensor0);\r\n    outputMap.put(1, tensor1);\r\n    outputMap.put(2, tensor2);\r\n    outputMap.put(3, tensor3);\r\n    outputMap.put(4, tensor4);\r\n    outputMap.put(5, tensor5);\r\n    outputMap.put(6, tensor6);\r\ntfLite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n```\r\n\r\n", "comments": ["Can you try our latest version 2.4.0 for both conversion and inference?\r\nI tried to run your model but it seemed to run successfully.", "@thaink Error is not raised in Python API but in android application (Java code). Following are the tensorflow lite dependencies used in the android application with above mentioned model.\r\n```\r\n    // Build off of nightly TensorFlow Lite\r\n    implementation('org.tensorflow:tensorflow-lite:0.0.0-nightly') { changing = true }\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'\r\n    implementation ('org.tensorflow:tensorflow-lite-metadata:0.0.0-nightly') { changing = true }\r\n```", "@Sudhan97 could you share the error message on the tf-nightly version in the android application?", "> @Sudhan97 could you share the error message on the tf-nightly version in the android application?\r\n\r\n@abattery Above I have included the **error raised in android**, **java code used android application** and **python code we used to convert into tflite format**.\r\nAnyway I'll include the error message again here.\r\n\r\n```\r\n2020-12-28 11:30:01.263 6838-6857/org.tensorflow.lite.examples.detection E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.detection, PID: 6838\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/reduce.cc:534 reference_ops::ReduceGeneric<T>( GetTensorData<T>(op_context->input), op_context->input->dims->data, op_context->input->dims->size, GetTensorData<T>(op_context->output), op_context->output->dims->data, op_context->output->dims->size, GetTensorData<int>(op_context->axis), num_axis, op_context->params->keep_dims, GetTensorData<int>(temp_index), GetTensorData<int>(resolved_axis), init_value, reducer) was not true.\r\n    No\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:158)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:343)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:334)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:182)\r\n        at android.os.Handler.handleCallback(Handler.java:761)\r\n        at android.os.Handler.dispatchMessage(Handler.java:98)\r\n        at android.os.Looper.loop(Looper.java:156)\r\n        at android.os.HandlerThread.run(HandlerThread.java:61)\r\n```", "I suspect that the attached error is not coming from the tf-nightly version. The line number is different with the tf-nightly version.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/reduce.cc#L558\r\n\r\nCould you confirm that your TFLite dependency is based on TF 2.5.0 in the android application?\r\n\r\nFYI, in the recent version, we have a fix for dealing with empty reduction inputs.", "@abattery thank you, I used the latest version 2.4 and I was able get the prediction. But when using the latest stable version(2.4.0) for converting the model to TFlite format, the converted model take around a hour for single image prediction output. Model converted using version 2.3.1 maximum took around 2 mins for prediction ouput.", "@abattery @thaink Thank you, we are able to get the prediction using latest tensorflow version 2.4.0. \r\nI'm closing this issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46010\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46010\">No</a>\n", "Hi @Sudhan97 I have followed the same steps you have outlined here. However I could do with some help understanding the outputs of the model. The details are here https://github.com/tensorflow/tensorflow/issues/46951\r\n\r\nWhich of the 7 output tensors provide the bounding box values? For me the values keep getting repeated for each detection and I don't seem to see any values that resemble the bounding box. ", "Hey @Sudhan97 how did you get the prediction in tf-nightly? I've tried many times and it always results in segmentation fault.", "@MeghaGhosh Hi, Sorry for this late reply. Output tensor 3 is detection and 4 is the masks. When I used tf-nightly(dev 2.5), I didn't get any segmentation errors. Try using tensorflow 2.4.0 version", "Thanks @Sudhan97 yes that finally worked for us. We were making a mistake with passing the image meta. I could not get it to run on any version of the python API though, even used a docker image. Anyway that is fine.\r\n\r\nSo the thing is, depending on how we resize/pad the image we always get different predictions. Using the tensorflow lite support library for image processing got us the closest to the keras h5 predictions, but the result is still a bit weird. Do you mind sharing which method you used for image resize in android and whether you got results consistent with the h5 file?"]}, {"number": 46009, "title": "ADDING DARK MODE TO TensorFlow WEBSITE", "body": "**   FEATURE REQUEST FOR DARK MODE ON WEBSITE **\r\n\r\nAs the bright interphase of our website affects our eyes i would request to put an dark mode feature to our TensorFlow website.\r\n  ", "comments": ["This repository is for the TF code, not the website. Closing."]}, {"number": 46008, "title": "SparseTensor mul. broadcasting gradient fails", "body": "## System information\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.12.1-47912-gec43aacb56f 2.5.0-dev20201219\r\n- Python version: 3.7\r\n\r\n## Current behavior\r\n\r\nComputing gradients of scaled `tf.SparseTensor`s fails. This is resolved be adding dimensions to scalar.\r\n\r\n## Expected behavior\r\n\r\nGradient computation compatible with automatic dimension adding when broadcasting leading dimensions.\r\n\r\n## Standalone code to reproduce the issue\r\n\r\n[Notebook](https://colab.research.google.com/drive/1R-HV0570iNzbY3yUGXSuqnikPWZd6aDO?usp=sharing)\r\n\r\nCode copied below for convenience\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nn = 5\r\nvalues = tf.Variable(tf.random.uniform((n,)))\r\nindices = tf.sparse.eye(n).indices\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(values)\r\n    st = tf.SparseTensor(indices, values, (n, n))\r\n    st = st * 2.                        # doesn't work\r\n    # st = st * tf.reshape(2., (1, 1))  # works\r\n    loss = tf.sparse.reduce_sum(st)\r\n\r\ngrad = tape.gradient(loss, values)\r\nprint(grad)\r\n```\r\n\r\n## Stack Trace\r\n\r\n```txt\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 13, in <module>\r\n    grad = tape.gradient(loss, values)\r\n  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\", line 1073, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\", line 77, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\", line 162, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/sparse_grad.py\", line 247, in _SparseDenseCwiseMulGrad\r\n    return _SparseDenseCwiseMulOrDivGrad(op, grad, True)\r\n  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/sparse_grad.py\", line 227, in _SparseDenseCwiseMulOrDivGrad\r\n    dense_vals = array_ops.gather_nd(y, scaled_indices)\r\n  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 5348, in gather_nd\r\n    return gen_array_ops.gather_nd(params, indices, name=name)\r\n  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3695, in gather_nd\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6870, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: params must be at least a vector [Op:GatherNd]\r\n```", "comments": ["I am able to replicate this issue on [tf 1.2](https://colab.research.google.com/gist/Saduf2019/a75de213ce562219ff1fd1bbd9eed796/untitled490.ipynb), and [2.x](https://colab.research.google.com/gist/Saduf2019/861409296603acc6294111438a54eedb/untitled489.ipynb). please find the gist [here](https://colab.research.google.com/gist/Saduf2019/6a81a7394447e5a06cac4022baa17b81/untitled488.ipynb).", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46008\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46008\">No</a>\n"]}, {"number": 46006, "title": "ConverterError: <unknown>:0: error: loc(\"lstm_bias_lstm_17\"): is not immutable (RNN)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): 2.4.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n`import tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_path)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float16]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS,\r\n  tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()`\r\n\r\n**The output from the converter invocation**\r\n`---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    212                                                  debug_info_str,\r\n--> 213                                                  enable_mlir_converter)\r\n    214       return model_str\r\n\r\n4 frames\r\nException: <unknown>:0: error: loc(\"lstm_bias_lstm_17\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    214       return model_str\r\n    215     except Exception as e:\r\n--> 216       raise ConverterError(str(e))\r\n    217 \r\n    218   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: <unknown>:0: error: loc(\"lstm_bias_lstm_17\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable`\r\n\r\n", "comments": ["Hi, do you have more details, like what model you're building?\r\n\r\nthanks", "> Hi, do you have more details, like what model you're building?\r\n> \r\n> thanks\r\n\r\nHi @renjie-liu \r\nactually, model is trained on PyTorch and I have converted it on ONNX to Tensorflow.\r\nModel is Tacotron2 for TTS.\r\n", "I see, unfortunately since we don't officially support PyTorch conversion, it might be somewhere broken either between pytorch -> onnx or onnx-> tensorflow.\r\n\r\nJae probably can point you about the tensorflow version of Tacotron2", "> I see, unfortunately since we don't officially support PyTorch conversion, it might be somewhere broken either between pytorch -> onnx or onnx-> tensorflow.\r\n> \r\n> Jae probably can point you about the tensorflow version of Tacotron2\r\n\r\nThere are other models(even TTS)  also which converted into TFLite from PyTorch. I can share ONNX graph.", "Hi, here is (unofficial) TF version of Tacotron2.\r\n\r\nhttps://github.com/TensorSpeech/TensorFlowTTS/blob/master/notebooks/TensorFlowTTS_Tacotron2_with_TFLite.ipynb\r\n\r\nYou can also ask any questions on that repo issues. Thanks.", "> Hi, here is (unofficial) TF version of Tacotron2.\r\n> \r\n> https://github.com/TensorSpeech/TensorFlowTTS/blob/master/notebooks/TensorFlowTTS_Tacotron2_with_TFLite.ipynb\r\n> \r\n> You can also ask any questions on that repo issues. Thanks.\r\n\r\nI have tried this TTS also. It is working but I want to convert my own model into TFLite. Is this issue is something unsolvable?", "Hi\r\nI have tried some things and now,error is changed.\r\n**tensorflow.lite.python.convert.ConverterError: input resource[0] expected type resource != float, the type of assignvariableop_resource_0[0]**\r\n@abattery @jaeyoo\r\n", "According to the converter error message, the TensorFlow model you've created has the mutable resource variables or the stateful LSTM nodes, which are not currently not supported. We are working on supporting those features. However, it will take some times.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46006\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46006\">No</a>\n", "Please try out TensorFlow 2.6 or tf-nightly version with `converter.experimental_enable_resource_variables = True`.", "Hi @abattery !\r\nI have try your method\r\n> Please try out TensorFlow 2.6 or tf-nightly version with `converter.experimental_enable_resource_variables = True`.\r\nAnd it can convert tensorflow to tflite. But when it put some warning and log error when start network forward.\r\n\r\nHere is the warning.\r\n\r\n```\r\n2021-09-17 10:46:25.251638: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1844] Graph contains the following resource op(s), that use(s) resource type. Currently, the resource type is not natively supported in TFLite. Please consider not using the resource type if there are issues with either TFLite converter or TFLite runtime:\r\nResource ops: AssignVariableOp, ReadVariableOp, VarHandleOp\r\nDetails:\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> ()\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> () : {device = \"\"}\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>, tensor<1x1xf32>) -> ()\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>, tensor<768x1024xf32>) -> () : {device = \"\"}\r\n        tf.ReadVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>) -> (tensor<1024xf32>) : {device = \"\"}\r\n        tf.ReadVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>) -> (tensor<?x?xf32>) : {device = \"\"}\r\n        tf.VarHandleOp() -> (tensor<!tf_type.resource<tensor<1024xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_bias_lstm_18\"}\r\n        tf.VarHandleOp() -> (tensor<!tf_type.resource<tensor<?x?xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_kernel_lstm_18\"}\r\n2021-09-17 10:46:25.252215: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1855] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):\r\nFlex ops: FlexAssignVariableOp, FlexReadVariableOp, FlexVarHandleOp\r\nDetails:\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> ()\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> () : {device = \"\"}\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>, tensor<1x1xf32>) -> ()\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>, tensor<768x1024xf32>) -> () : {device = \"\"}\r\n        tf.ReadVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>) -> (tensor<1024xf32>) : {device = \"\"}\r\n        tf.ReadVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>) -> (tensor<?x?xf32>) : {device = \"\"}\r\n        tf.VarHandleOp() -> (tensor<!tf_type.resource<tensor<1024xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_bias_lstm_18\"}\r\n        tf.VarHandleOp() -> (tensor<!tf_type.resource<tensor<?x?xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_kernel_lstm_18\"}\r\n\r\n```\r\n\r\nIt there any suggestion to solve the problem?\r\n\r\nThanks.\r\n", "Please do not upload a new issue at the closed one."]}, {"number": 46005, "title": "windows 10 build failed (ERROR: An error occurred during the fetch of repository 'local_config_cuda':)", "body": "I've tried to install tensorflow step by step from [source_windows](https://www.tensorflow.org/install/source_windows)\r\nbut got error\r\n**System information**\r\n- OS Platform and Distribution : windows10 x64\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.7.7\r\n- Bazel version : 3.7.2\r\n- GCC/Compiler version (if compiling from source): visual studio 2019\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: 960m, 4G\r\n\r\noutput of `python ./configure.py`:\r\n```\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\127051\\AppData\\Local\\Programs\\Python\\Python37\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\127051\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\127051\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.2 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include\r\nFound cuDNN 8 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 5.0\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n```\r\nInstall error:\r\n```\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=164\r\nINFO: Reading rc options for 'build' from d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/127051/AppData/Local/Programs/Python/Python37/python.exe\r\nINFO: Reading rc options for 'build' from d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from d:\\softwareinstaltion\\tensorflow-2.4.0\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/127051/AppData/Local/Programs/Python/Python37/python.exe --action_env PYTHON_LIB_PATH=C:/Users/127051/AppData/Local/Programs/Python/Python37/lib/site-packages --python_path=C:/Users/127051/AppData/Local/Programs/Python/Python37/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=5.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:windows in file d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file d:\\softwareinstaltion\\tensorflow-2.4.0\\.bazelrc: --define framework_shared_object=false\r\nINFO: Repository local_config_cuda instantiated at:\r\n  D:/softwareinstaltion/tensorflow-2.4.0/WORKSPACE:19:16: in <toplevel>\r\n  D:/softwareinstaltion/tensorflow-2.4.0/tensorflow/workspace.bzl:96:19: in tf_repositories\r\nRepository rule cuda_configure defined at:\r\n  D:/softwareinstaltion/tensorflow-2.4.0/third_party/gpus/cuda_configure.bzl:1430:33: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"D:/softwareinstaltion/tensorflow-2.4.0/third_party/gpus/cuda_configure.bzl\", line 1400, column 38, in _cuda_autoconf_impl\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"D:/softwareinstaltion/tensorflow-2.4.0/third_party/gpus/cuda_configure.bzl\", line 1244, column 56, in _create_local_cuda_repository\r\n                host_compiler_includes + _cuda_include_path(\r\n        File \"D:/softwareinstaltion/tensorflow-2.4.0/third_party/gpus/cuda_configure.bzl\", line 364, column 32, in _cuda_include_path\r\n                inc_entries.append(realpath(repository_ctx, cuda_config.cuda_toolkit_path + \"/include\"))\r\n        File \"D:/softwareinstaltion/tensorflow-2.4.0/third_party/remote_config/common.bzl\", line 277, column 19, in realpath\r\n                return execute(repository_ctx, [bash_bin, \"-c\", \"realpath \\\"%s\\\"\" % path]).stdout.strip()\r\n        File \"D:/softwareinstaltion/tensorflow-2.4.0/third_party/remote_config/common.bzl\", line 217, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\n/usr/bin/realpath: missing operand\r\nTry '/usr/bin/realpath --help' for more information.\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Repository command failed\r\n/usr/bin/realpath: missing operand\r\nTry '/usr/bin/realpath --help' for more information.\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Repository command failed\r\n/usr/bin/realpath: missing operand\r\nTry '/usr/bin/realpath --help' for more information.\r\nINFO: Elapsed time: 1.781s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n\r\n```", "comments": ["@aligoglos \r\nEvery TensorFlow release is compatible with a certain CUDA and cuDNN version. Please go through the [tested build configurations ](https://www.tensorflow.org/install/source#gpu)for more information.\r\n\r\nPlease try building TensorFlow 2.4 with CUDA 11 and cuDNN 8 and check if you are facing the same error. \r\nAlso refer to similar issues: #45762, #45001 , #44251 Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46005\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46005\">No</a>\n", "My GPU is GeForce RTX 2080ti, CUDA11.0, cuDNN 8.0.5\uff0ci\u2018ve tried to switch  tensorflow 2.4.0rc1, rc2, rc3, rc4 and 2.4.1, also tried to change the cuDNN version to 8.1.0, but they all still  got the same error as above...\r\nReally need some help."]}, {"number": 46004, "title": "Normalizing checkpoint path for Windows File System in Saver.py", "body": "https://github.com/keras-team/keras-tuner/issues/198\r\n\r\nThe hardcoded forward slash value for _SHARDED_SUFFIX broke on windows since the windows file system does not except forward slashes.\r\n\r\nSolution:\r\nUsing os.path.normpath is going to normalize path relative to the OS ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46004) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@Ather23  Can you please resolve conflicts? Thanks!", "Will review again once conflict is solved", "Thank you for the feedback. I will resolve conflicts shortly.", "@mihaimaruseac @gbaned I have resolved the conflicts. Please review.\r\nThank you.", "A note on failing builds on Windows.\r\n\r\nBased on my overview of the failing builds, the test function builds a path with a hard-coded train folder like so.\r\n\r\n![image](https://user-images.githubusercontent.com/2106106/103330866-5c75a980-4a31-11eb-86ee-17540a574f8c.png)\r\n\r\n\r\nWhich results in the following dir path\r\n` train/checkpoint_temp\\part-00000-of-00001.data-00000-of-00001.tempstate11227093142142859221`\r\nThis is not going to work on Windows.\r\n\r\nNotice that the checkpoint_temp has a \\ in the end correctly which is within the scope of this fix.\r\n\r\nThe test functions also need to have a normalized path in order for them to be successful based on the OS.\r\n\r\nRegardless of the failing build, I think this fix is going to run on windows correctly when a valid initial path is supplied.", "Let's remove the `/` at the end of `traindir` definition. All of the subsequent calls should be able to insert the proper path separator.", "@mihaimaruseac The `/` has been removed from traindir."]}, {"number": 46003, "title": "keras.Model.fit error with 2.4 and TPUv3", "body": "My code is worked in 2.3 and 2.4 with GPU. And it is failed when doing \"keras.Model.fit\" in Cloud TPU with 2.4, but worked in 2.3.1 and nightly.\r\n\r\nIn the past, this error is only encountered when the TensorFlow version in TPU does not fully matched the version in VM.", "comments": ["@edwardyehuang \r\n\r\nCan you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46003\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46003\">No</a>\n"]}, {"number": 46001, "title": "Weight Normalization layer doesn't work with mixed precision", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Minimal\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 'v2.4.0-rc4-71-g582c8d236cb'\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: CUDA 11  cudnn 8\r\n- GPU model and memory: RTX 2080 11gb\r\n\r\n**Describe the current behavior**\r\nwhen using mixed precision and the weight normalization layer from tensorflow_addons, it throws an error \r\n`    ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float16: <tf.Tensor 'sequential/weight_normalization/cond/data_dep_init/mul:0' shape=(10,) dtype=float16>`\r\nfrom\r\n```\r\nassign_tensors = self._data_dep_init(inputs)\r\n    \\tensorflow_addons\\layers\\wrappers.py:204 _data_dep_init\r\n        g_tensor = self.g.assign(self.g * scale_init)\r\n    \\tensorflow\\python\\keras\\mixed_precision\\autocast_variable.py:225 assign\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nlayer to build and run normally as it does in float32 and float16 policies\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\nimport numpy as np\r\n\r\npolicy = tf.keras.mixed_precision.Policy(\"mixed_float16\")\r\n#runs fine with all the same type\r\n#policy = tf.keras.mixed_precision.Policy(\"float16\")\r\n\r\ntf.keras.mixed_precision.set_global_policy(policy)\r\n\r\nmodel = tf.keras.Sequential()\r\n\r\nmodel.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(10, activation=\"relu\")))\r\n\r\nmodel.compile(loss=tf.keras.losses.MeanSquaredError())\r\n\r\nX = np.random.randn(1,20)\r\nY = np.random.randn(1,10)\r\n\r\nmodel.fit(X,Y)\r\n```\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nputting a tf.cast inside each call to assign in wrappers.py -> WeightNormalization stops the error, but I'm not certain if that is correct from a numerical stability perspective, and it seems like adding some functionality to the assign function in mixedPrecision/autocast_variable.py -> AutocastVariable would be better", "comments": ["cc @reedwm ", "I am able to replicate the issue on [tf 2.4 ](https://colab.research.google.com/gist/Saduf2019/9ef23bf617d4894247f5e05c7ea6366d/untitled489.ipynb)and [nightly](https://colab.research.google.com/gist/Saduf2019/16e724296666d25053cf6e19cc6e02a0/untitled488.ipynb)", "as a side note, there are 2 separate implementations of Weight Normalization in the tensorflow ecosystem (one in tensorflow_addons and one in tensorflow_probability). I'm not sure what the right architectural decision should be about how to unify that, but in the least we should make sure that any bugs that are fixed in one also are fixed in the other.", "cc @seanpmorgan @smokrow ", "Was able to reproduce te issue in TF 2.6 Nightly version as well.Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/27f72978f98493a33e5efc6207fd1ced/untitled90.ipynb).", "This should be filed in the [tensorflow/addons](https://github.com/tensorflow/addons) repository, so I'm closing the issue here.\r\n\r\nI am not familiar with WeightNormalization, but skimming the layer implementation, I think most the math should be done in float32. The error comes from setting a float32 variable to a float16 value [at this line](https://github.com/tensorflow/addons/blob/b68af5c611dd06894ad282ec263a92e1681c83db/tensorflow_addons/layers/wrappers.py#L204). The layer previously calls `tf.nn.moments`, which should also be done in float32", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46001\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46001\">No</a>\n"]}, {"number": 46000, "title": "Incorrect conda dependency information for tensorflow-gpu", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro + Anaconda\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.8.5\r\n- Installed using conda\r\n- CUDA/cuDNN version: 11.0\r\n\r\n\r\n**Describe the problem**\r\n\r\nTensorflow 2.3.0 is only compatible with CUDA 10.x but not 11.0. But a conda installation is unable to figure out the incompatibility during installation.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nMinimal example to reproduce the problem:\r\n\r\n`conda create --name tf_gpu python=3.8 tensorflow-gpu cudatoolkit --dry-run`\r\n\r\nAs shown in the list, CUDA 11.0 will be installed with tensorflow 2.3.0.\r\n\r\n**Any other info / logs**\r\nThere is another problem of the default conda dependency: it differs (and conflicts on certain constraints) with the pip dependency. Details of tensorflow 2.3.0 are shown in the following.\r\n\r\n<table style=\"width:100%\">\r\n  <tr>\r\n    <th>conda dependency</th>\r\n    <th>pip dependency</th>\r\n  </tr>\r\n  <tr>\r\n    <td>gast >=0.4.0,<0.4.1.0a0</td>\r\n    <td>gast==0.3.3</td>\r\n  </tr>\r\n  <tr>\r\n    <td>scipy >=1.5.2</td>\r\n    <td>scipy==1.4.1</td>\r\n  </tr>\r\n  <tr>\r\n    <td>keras-preprocessing >=1.1.0</td>\r\n    <td>keras-preprocessing<1.2,>=1.1.1</td>\r\n  </tr>\r\n  <tr>\r\n    <td>numpy >=1.16.6,<2.0a0</td>\r\n    <td>numpy<1.19.0,>=1.16.0</td>\r\n  </tr>\r\n</table>\r\n\r\nIf only one of `conda` or `pip` is used, it would be fine. However, in some cases I may need to use `pip` to install packages in a conda environment. If I need to install a package built on top of `tensorflow` using `pip`, a conflict would cause problems.\r\n\r\nIt would be nice if this problem could also be fixed.\r\n\r\n", "comments": ["@ZOUG \r\n\r\nSorry, but we don't provide support for issues with the conda environment.\r\nThis issue is more suitable on Continuum [Anaconda repo](https://github.com/ContinuumIO/anaconda-issues/issues) since its related to TF installation with Anaconda.\r\nPlease post it on [Continuum Anaconda.](https://github.com/ContinuumIO/anaconda-issues/issues)\r\nThanks!", "@ravikyram Thanks to your prompt response. This is a problem of the TF release on the Anaconda Cloud, not a problem of the conda environment itself. Is Continuum Anaconda responsible for building and publishing the release?\r\n\r\nBTW, I just realized that the problem is the `cudatoolkit` and `cudnn` packages are NOT even installed when I run `conda create --name tf_gpu python tensorflow-gpu=2.3` on Windows (they were installed for TF 2.1). Therefore it is a more serious problem than I intially thought. The tensorflow-gpu 2.3 installation cannot use GPU at all.", "We only control the pip dependencies. If Conda alters them there is nothing we can do.\r\n\r\nTF 2.3 cannot use CUDA 11.", "@mihaimaruseac We have too much Conda tickets traffic. I think that we finally need to add an advice on one of the first lines in the ISSUE template", "> We only control the pip dependencies. If Conda alters them there is nothing we can do.\r\n> \r\n> TF 2.3 cannot use CUDA 11.\r\n\r\nDo you know who is responsible for building and publishing the TF release on conda?", "@ZOUG  https://github.com/conda/conda", "> @ZOUG https://github.com/conda/conda\r\n\r\nI suppose they are dealing with issues of the conda manager itself. I'm not sure about the actual release procedure but it looks like anyone can upload their own package to anaconda.org (see [here](https://github.com/conda/conda#building-your-own-packages)). So I would like to confirm, are you sure that Anaconda is actually responsible for building and publishing the TF 2.3 release but not anyone else?", "If you want the Tensorflow recipes is at https://github.com/AnacondaRecipes/tensorflow_recipes", "> If you want the Tensorflow recipes is at https://github.com/AnacondaRecipes/tensorflow_recipes\r\n\r\nI had posted issues in that repository as well. I'm not trying to build my own release though. BTW, I just looked around and it looks like it is indeed Anaconda Inc. that is maintaing [the main channel](https://anaconda.org/anaconda).", "@ZOUG,\r\n\r\nAs you have already posted about this issue in conda repo, Can you confirm if we are good to close this issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46000\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46000\">No</a>\n"]}, {"number": 45996, "title": "Add VERSION_INFO", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/39795\r\n\r\n/cc @mihaimaruseac ", "comments": ["I've added the golden update for v2.", "Do we need to change that PR label here?", "It's pending API onwers review, hopefully we'll get it today.", "So API owners suggested to use `semver` or `distutils.version.LooseVersion` instead of adding a new API.\r\n\r\nMeans the workaround in https://github.com/tensorflow/tensorflow/issues/39795#issuecomment-751472512 is the recommended way.\r\n\r\nSorry that it gets to be closed after such a long wait.", "@mihaimaruseac As the gist was mine as this PR do you will accept a semver dep PR?", "I think we should ask the API owners on the new PR. I'll add tags to have them notified", "> I think we should ask the API owners on the new PR. I'll add tags to have them notified\n\nThanks I suppose that they meant to push this on the user side (outside Tensorflow) but just to be sure that we don't want a python only PR", "Yes, I think the same, that they wanted to have users that needed this implement on their code instead of relying on TF."]}, {"number": 45995, "title": "micro: port operator DIV kernel from lite with test", "body": "Complete implementation of TFLM operator DIV kernel and associated TFLM test code.\r\n\r\nThis PR represents steps 4 & 5 of the work to port operator DIV as tracked in Issue #45431", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", ":frowning_face: Sorry, but only Googlers may change the label `cla: yes`.", "@ddavis-2015 This PR is in draft, can you please take a look on this? Thanks!", "@ddavis-2015  This PR is in draft, any update on this? Please. Thanks!", "@ddavis-2015 seeing this error internally \r\n\r\n`third_party/tensorflow/lite/micro/kernels/div.cc:68:30: error: implicit conversion increases floating-point precision: 'float' to 'const double' [-Werror,-Wdouble-promotion]\r\n        input1->params.scale / (input2->params.scale * output->params.scale);\r\n        ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n1 error generated.`", "> @ddavis-2015 seeing this error internally\r\n> \r\n> `third_party/tensorflow/lite/micro/kernels/div.cc:68:30: error: implicit conversion increases floating-point precision: 'float' to 'const double' [-Werror,-Wdouble-promotion] input1->params.scale / (input2->params.scale * output->params.scale); ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 1 error generated.`\r\n\r\nThanks for the heads up.  A new commit has been pushed and hopefully there will be no more compile errors.\r\n", "It looks like third_party/tensorflow/lite/kernels:div_test is failing:\r\n\r\n```\r\n[ RUN      ] QuantizedDivOpTest.QuantizedNoActivationUInt8\r\n*** SIGABRT received by PID 3118 (TID 3118) on cpu 36 from PID 3118; stack trace: ***\r\nPC: @     0x7ff197c5e602  (unknown)  raise\r\n    @     0x7ff18b528bf6        960  FailureSignalHandler()\r\n    @     0x7ff1a126a9a0  (unknown)  (unknown)\r\n    @     0x7ff1f581eb04         80  tflite::reference_ops::DivElementwise<>()\r\n    @     0x7ff1f5823387        304  tflite::ops::builtin::div::EvalQuantized<>()\r\n    @     0x7ff1f581b5b2         80  tflite::ops::builtin::div::Eval<>()\r\n    @     0x7ff1ef10f8e5        160  tflite::Subgraph::Invoke()\r\n    @     0x7ff1ef113d50         96  tflite::Interpreter::Invoke()\r\n    @     0x7ff1f6618db4        112  tflite::SingleOpModel::Invoke()\r\n    @     0x7ff1f86662cb       1936  tflite::(anonymous namespace)::QuantizedDivOpTest_QuantizedNoActivationUInt8_Test::TestBody()\r\n    @     0x7ff1a583bdaa         48  testing::Test::Run()\r\n    @     0x7ff1a583d353         64  testing::TestInfo::Run()\r\n    @     0x7ff1a583e0c0         80  testing::TestSuite::Run()\r\n    @     0x7ff1a585023c        192  testing::internal::UnitTestImpl::RunAllTests()\r\n    @     0x7ff1a584f7f5         64  testing::UnitTest::Run()\r\n    @     0x7ff1f82497cd        112  main\r\n    @     0x7ff197c4abbd        208  __libc_start_main\r\n    @     0x56342f43ce69  (unknown)  ../sysdeps/x86_64/start.S:108 _start\r\n```\r\n\r\nYou should be able to reproduce this with something like:\r\n\r\n`bazel test third_party/tensorflow/lite/kernels:div_test`"]}, {"number": 45994, "title": "TF 2.4.0 crashes on startup with IndexError assuming that sys.argv[0] exists when it may be hosted by C++ (regression from 2.3.1)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Linux Debian 10\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.4.0\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.7.9\r\n- CUDA/cuDNN version: 11.0, 8.0.4 for 11.0\r\n- GPU model and memory: NVIDIA GTX 1080\r\n\r\n**Describe the current behavior**\r\n\r\nTF 2.4.0 crashes on startup with:\r\n```\r\nFile \"...\\site-packages\\tensorflow\\python\\distribute\\combinations.py\", line 159, in GPUCombination\r\nGPU_TEST = re.search(r\"(test_gpu|test_xla_gpu)$\", sys.argv[0])\r\nIndexError: list index out of range\r\n```\r\n\r\n**Describe the expected behavior**\r\nsys.argv is checked to be non-empty before indexing (no bug on TF 2.3.1)\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n1. Ensure that PYTHONHOME is set\r\n2. Py_Initialize()\r\n3. Append our module's directory to the system path: (PyImport_ImportModule(\"sys\"), PyObject_GetAttrString(sys, \"path\"), PyUnicode_FromString(\"...\"), PyList_Append(sysPath, ...))\r\n4. Import our module: PyImport_ImportModule(\"...\")\r\n5. Our module calls \"import tensorflow as tf\", which then crashes\r\n\r\n**Python-only mock repro**\r\n```\r\nimport sys\r\nsys.argv.clear()\r\nimport tensorflow as tf\r\n```\r\n\r\n**Workaround**\r\n```\r\n# Work around https://github.com/tensorflow/tensorflow/issues/45994\r\nimport sys\r\nif not sys.argv:\r\n  sys.argv.append(\"(C++)\")\r\n\r\nimport tensorflow as tf\r\n```\r\n\r\n**Notes**\r\n\r\nIt may help to write a unit test/build verification test that mocks this scenario by clearing sys.argv before importing tensorflow.", "comments": ["Some clarifications:\r\n- This also repros on Linux (e.g. the Debian 10 images on Google Cloud)\r\n- This is a runtime crash bug when trying to make use of TensorFlow, not a build/install issue\r\n\r\nHere's a stack from a Google Cloud VM:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 49, in <module>\r\n    from ._api.v2 import __internal__\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 11, in <module>\r\n    from . import distribute\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py\", line 10, in <module>\r\n    from . import combinations\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py\", line 10, in <module>\r\n    from tensorflow.python.distribute.combinations import generate\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/combinations.py\", line 145, in <module>\r\n    class GPUCombination(combinations_lib.TestCombination):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/combinations.py\", line 159, in GPUCombination\r\n    GPU_TEST = re.search(r\"(test_gpu|test_xla_gpu)$\", sys.argv[0])\r\nIndexError: list index out of range\r\n```", "Able to reproduce:\r\n\r\n```\r\n>>> import sys\r\n>>> print(sys.argv)\r\n['']\r\n>>> sys.argv.clear()\r\n>>> print(sys.argv)\r\n[]\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/tmp/tfff/venv/lib/python3.8/site-packages/tensorflow/__init__.py\", line 49, in <module>\r\n    from ._api.v2 import __internal__\r\n  File \"/tmp/tfff/venv/lib/python3.8/site-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 13, in <module>\r\n    from . import distribute\r\n  File \"/tmp/tfff/venv/lib/python3.8/site-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py\", line 10, in <module>\r\n    from . import combinations\r\n  File \"/tmp/tfff/venv/lib/python3.8/site-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py\", line 10, in <module>\r\n    from tensorflow.python.distribute.combinations import generate\r\n  File \"/tmp/tfff/venv/lib/python3.8/site-packages/tensorflow/python/distribute/combinations.py\", line 146, in <module>\r\n    class GPUCombination(combinations_lib.TestCombination):\r\n  File \"/tmp/tfff/venv/lib/python3.8/site-packages/tensorflow/python/distribute/combinations.py\", line 160, in GPUCombination\r\n    GPU_TEST = re.search(r\"(test_gpu|test_xla_gpu)$\", sys.argv[0])\r\nIndexError: list index out of range\r\n```\r\n\r\nRick, since this is in DistStrat, can you take a look please?", "Very recently I actually hit an even stronger case of this - sys didn't actually even have an \"argv\" attribute at all, hosting via C++. I believe this was on \"newer\" TPUs, using Ubuntu 18.04 LTS, Python 3.7, GCC 8.\r\n\r\nI updated my workaround to this to cover both cases:\r\n```\r\nimport sys\r\nif not hasattr(sys, \"argv\") or not sys.argv:\r\n  sys.argv = [\"\"]\r\n```\r\n\r\nThe no-\"argv\" case seems really strange though, probably beyond what library code needs to forgive?", "I can confirm that the steps in https://github.com/tensorflow/tensorflow/issues/45994#issuecomment-791823979 still reproduce the issue for me as well.\r\n\r\nPython 3.7.9\r\nDebian Linux 5.10.0-4-amd64\r\nNVIDIA 1080 TI\r\ncudnn 7.6.5 cudatoolkit 10.1.243\r\nTensorflow 2.4.1\r\n\r\nThe suggested workaround helps.", "This issue is replicating in [2.5](https://colab.research.google.com/gist/mohantym/3d3fc93953480c0115e2ebf43563a7d7/github_45994.ipynb#scrollTo=Hj-zPvNi7qta),[2.6](https://colab.research.google.com/gist/mohantym/44ccab068795696d9473c8151c5f6108/github_45994.ipynb#scrollTo=4EQ2wi5B7k_i) and [nightly ](https://colab.research.google.com/gist/mohantym/d311bea271fc1c6201f4a7bcf798b47e/github_45994.ipynb#scrollTo=Hj-zPvNi7qta).", "CC @rchao ", "Yes. Is there any concern over adding a check whether `sys.argv` has at least one element before indexing? If not, I can submit a PR to fix this. Thanks", "A fix on this issue would be appreciated -- in [embedded contexts](https://docs.python.org/3/extending/embedding.html), `sys.argv[0]` is not guaranteed to be set.", "Sounds good. I'm now working on a fix.", "@chrisbutner Is this still an issue for you? Can you please check with recent TF (i tried `tf-nightly`) and let us know whether this is still an issue. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45994\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45994\">No</a>\n", "This issue is still present, and can be identified by searching for `sys.argv[0]` instances in the codebase, and is still present in the nightly builds.", "This repro, importing tensorflow in embedded python, seems fixed (via https://github.com/tensorflow/tensorflow/commit/c8cb67f4c27ec312d228a344ed124bb34cbee74b, @scdub). Thanks!"]}, {"number": 45992, "title": "from tensorflow.contrib.layers.python.layers.initializers import variance_scaling_initializer", "body": "`from tensorflow.contrib.layers.python.layers.initializers import variance_scaling_initializer`<br>\r\nImportError: No module named slim.layers.python.layers.initializers<br>\r\nis there a new way to do this ? ", "comments": ["@andreixxi You can import `variance_scaling_initializer` as `from tensorflow.contrib.layers import variance_scaling_initializer`.\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/5ed76051d2a1f0623b88708a6f93ebb9/untitled69.ipynb). Thanks!\r\n\r\nI am closing this issue as it was resolved. Feel free to reopen if the issue persists. thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45992\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45992\">No</a>\n"]}, {"number": 45991, "title": "Tensorflow 2.4 requires BMI2 CPU extension (AVX is not enough)", "body": "**System information**\r\n- OS Platform and Distribution : Debian GNU/Linux 10 (buster) (and Ubuntu)\r\n- TensorFlow installed from (source or binary): binary\r\n\r\n```\r\n== check python ===================================================\r\npython version: 3.6.9\r\npython branch: \r\npython build version: ('default', 'Oct  8 2020 12:12:24')\r\npython compiler version: GCC 8.4.0\r\npython implementation: CPython\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #1 SMP Debian 4.19.160-2 (2020-11-28)\r\nos release version: 4.19.0-13-amd64\r\nos platform: Linux-4.19.0-13-amd64-x86_64-with-Ubuntu-18.04-bionic\r\nlinux distribution: ('Ubuntu', '18.04', 'bionic')\r\nlinux os distribution: ('Ubuntu', '18.04', 'bionic')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='90e48bf24072', release='4.19.0-13-amd64', version='#1 SMP Debian 4.19.160-2 (2020-11-28)', machine='x86_64', processor='x86_64')\r\narchitecture: ('64bit', 'ELF')\r\nmachine: x86_64\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n== check pips ===================================================\r\nnumpy                  1.19.4\r\nprotobuf               3.14.0\r\ntensorflow             2.4.0\r\ntensorflow-estimator   2.4.0rc0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.4.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /usr/local/lib/python3.6/dist-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 6, 9, 'final', 0)\r\n```\r\n\r\n**Describe the current behavior**\r\nimport tensorflow fails with \"Illegal Instruction\" on a CPU with AVX\r\n\r\n**Describe the expected behavior**\r\naccording to the documentation (https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements), all CPU with AVX are supported, so the import should not fail\r\n\r\n**Standalone code to reproduce the issue**\r\npython3\r\n`>>> import tensorflow`\r\n\r\n**Other info / logs**\r\nThe problem comes from an instruction from the \"BMI2\" extension. Output from GDB : \r\n```\r\nProgram received signal SIGILL, Illegal instruction.\r\n0x00007fb5fe658c48 in tensorflow::UnaryVariantOpRegistry::RegisterDeviceCopyFn(tensorflow::VariantDeviceCopyDirection, tensorflow::TypeIndex const&, std::function<tensorflow::Status (tensorflow::Variant const&, tensorflow::Variant*, std::function<tensorflow::Status (tensorflow::Tensor const&, tensorflow::Tensor*)>)> const&) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n```\r\n\r\nWhich points to instruction **SHLX**.\r\n\r\nSome older CPUs do have AVX, but not BMI2 : \r\n- Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\r\n- Intel(R) Core(TM) i3-2120 CPU @ 3.30GHz\r\n\r\nexample output of lscpu : \r\n`Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d`\r\n\r\nBest Regards", "comments": ["Related #45744, #45866", "@ant-pls-dev,\r\nCan you please confirm if we can close this issue as it is already being tracked in the issues [mentioned above](https://github.com/tensorflow/tensorflow/issues/45991#issuecomment-751809042) by @mihaimaruseac? Thanks! ", "Duplicating to #45744", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45991\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45991\">No</a>\n", "Hello, other tickets only mention avx2 and I saw no mention of BMI2 instructions, so I was not sure it was the same issue. If you are confident the commit will also fix the requirement on BMI2, then I'm fine with closing this ticket.\r\nThank you,\r\nBest regards"]}, {"number": 45990, "title": "How to read .bin file using Tensorflow C++ api?", "body": "Hey all,\r\n\r\nI need to read lidar point cloud from .bin folder. But I could not able to find an API in c++ to achieve my task. could anyone help me with this? \r\n\r\nTensorflow version 1.4.0\r\n\r\nThanks in advance.\r\n", "comments": ["@Suri12990 \r\nWe see that you are using 1.x which is not supported, please upgrade to 2.x\r\n\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.", "Thank you so much, I got the answer."]}, {"number": 45989, "title": "Fix some message typos", "body": "absense -> absence", "comments": ["Can you try and fix all typos in this file/directory? We are trying to limit the number of typo PRs we merge since they require a lot of computation resources for very little gain.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#general-guidelines-and-philosophy-for-contribution"]}, {"number": 45988, "title": "cannot execute exe of tensorflow py compiled with pyinstaller", "body": "**System information**\r\n- I used the basic example of tensorflow installing procedure:\r\n`  import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\r\n`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n  I don't know\r\n- TensorFlow installed from (source or binary):\r\n  installed via pip. pip install tensorflow and pip install --upgrade tensorflow\r\n- TensorFlow version (use command below):\r\n  2.4.0\r\n- Python version:\r\n  Python 3.8.7rc1\r\n- Bazel version (if compiling from source):\r\n  Not compiled from source\r\n- GCC/Compiler version (if compiling from source):\r\n Not compiled from source\r\n- CUDA/cuDNN version:\r\n  Tried:\r\n  1- Without CUDA\r\n  2- CUDA 11.1, no CUDNN\r\n  3- CUDA 11.1,  CUDNN 8.1\r\n  4- CUDA 11.1,  CUDNN 8.1, CUDA 10.1 over CUDA 11.1\r\nSame result\r\n- GPU model and memory:\r\n   NVIDIA GeForce MX250\r\n\r\n\r\n**Describe the current behavior**\r\nI run python with the script:\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.reduce_sum(tf.random.normal([1000, 1000])))\r\n```\r\nThe right output is:\r\ntf.Tensor(-29.82965, shape=(), dtype=float32)\r\n\r\nI launch pyinstaller --onefile prova.py and this is the output:\r\n C:\\Users\\fabio\\VSC>pyinstaller --onefile C:\\Users\\fabio\\VSC\\prova.py\r\n103 INFO: PyInstaller: 4.1\r\n103 INFO: Python: 3.8.7rc1\r\n104 INFO: Platform: Windows-10-10.0.19041-SP0\r\n106 INFO: wrote C:\\Users\\fabio\\VSC\\prova.spec\r\n108 INFO: UPX is not available.\r\n108 INFO: Extending PYTHONPATH with paths\r\n['C:\\\\Users\\\\fabio\\\\VSC', 'C:\\\\Users\\\\fabio\\\\VSC']\r\n131 INFO: checking Analysis\r\n735 INFO: checking PYZ\r\n1053 INFO: checking PKG\r\n1061 INFO: Building because C:\\Users\\fabio\\VSC\\build\\prova\\prova.exe.manifest changed\r\n1069 INFO: Building PKG (CArchive) PKG-00.pkg\r\n22364 INFO: Building PKG (CArchive) PKG-00.pkg completed successfully.\r\n22370 INFO: Bootloader c:\\windows\\system32\\venv\\lib\\site-packages\\PyInstaller\\bootloader\\Windows-64bit\\run.exe\r\n22373 INFO: checking EXE\r\n22374 INFO: Rebuilding EXE-00.toc because pkg is more recent\r\n22376 INFO: Building EXE from EXE-00.toc\r\n22383 INFO: Appending archive to EXE C:\\Users\\fabio\\VSC\\dist\\prova.exe\r\n22465 INFO: Building EXE from EXE-00.toc completed successfully.\r\n\r\nThen I run the exe and this is the output:\r\n(venv) C:\\Users\\fabio\\VSC>dist\\prova.exe\r\nTraceback (most recent call last):\r\n  File \"prova.py\", line 1, in <module>\r\n    import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\r\n  File \"c:\\windows\\system32\\venv\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow\\__init__.py\", line 41, in <module>\r\n  File \"c:\\windows\\system32\\venv\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow\\python\\__init__.py\", line 41, in <module>\r\n  File \"c:\\windows\\system32\\venv\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n  File \"c:\\windows\\system32\\venv\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 493, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"tensorflow\\python\\pywrap_tfe.py\", line 29, in <module>\r\nImportError: DLL load failed while importing _pywrap_tfe: Impossibile trovare il modulo specificato.\r\n[20820] Failed to execute script prova\r\n\r\n**Describe the expected behavior**\r\nThe exe file should return the tensor as the python script\r\n\r\n**Standalone code to reproduce the issue**\r\nCreate a python script prova.py:\r\nimport tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\r\n\r\nRun in the directory of the script\r\npyinstaller --onefile prova.py\r\n\r\nThen run the exe resulting\r\n\r\nHow can I build a onefile exe from a script using tensorflow?\r\nThanks.\r\n", "comments": ["@fafabone \r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167\r\n\r\nThanks!", "- My CPU is Intel\u00ae Core\u2122 i7-10510U Processor and it supports AVX 2 instruction set:\r\nhttps://ark.intel.com/content/www/us/en/ark/products/196449/intel-core-i7-10510u-processor-8m-cache-up-to-4-90-ghz.html#:~:text=Instruction%20Set%20Extensions,-Instruction%20Set%20Extensions&text=These%20can%20include%20SSE%20(Streaming,AVX%20(Advanced%20Vector%20Extensions).\r\n- I have installed 2015-2019 C++ redistributable, 32bit and 64bit.\r\n- I installed tensorflow via pip.\r\n- Python is 64 bit as tensorflow ask in installation guide:\r\npython -c \"import platform;print(platform.architecture()[0])\"\r\n64bit\r\n- My script py runs as expected, my exe created via pyinstaller doesn't.", "@fafabone \r\n\r\nJust to verify are you able to import tensorflow. Thanks!", "Do you know a solution for my problem? A clue?", "It seems it is a bug in pyinstaller not being able to properly load all needed DLLs. Not really a TF issue.", "Please, could you suggest a way to produce an exe running in windows starting from python script that imports tensorflow?\r\nThanks", "@fafabone ,\r\nCan you please refer this [document](https://www.tensorflow.org/install) for installation of tensorflow.It helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45988\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45988\">No</a>\n"]}, {"number": 45987, "title": "Tensorflow 2.4  CUDA 11   CUDA_ERROR_LAUNCH_FAILED", "body": "------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Win10  -2004\r\n-   **TensorFlow installed from (source or binary)**: binary\r\n-   **TensorFlow version (use command below)**:  2.4\r\n-   **Python version**: 3.6.1\r\n-   **CUDA/cuDNN version**: CUDA 11.0 -  cudNN 8.0.4\r\n-   **GPU model and memory**: GTX 1660 Super 6GB (+ GT 730 2GB) with 456.55 driver.\r\n-   **Exact command to reproduce**: py -3.6 generate.py\r\n\r\n### Describe the problem\r\nHello, I have had a problem with tensorflow and cuda for days ... the program works with the CPU but I would like to use the latter with my GPU. However, the program refuses to end. the program starts but as soon as it arrives at 5/54 or 6/54 it crashes with the message bellow.\r\nI tried to change the version of cuda + cudnn, change the driver version of my gpu but nothing works, I still find this error.\r\nThe program is [here](https://github.com/Migateak/tensorflowtextgen) with data, it's not mine but it's for a generation of auto text.\r\n\r\n### Source code / logs\r\n\r\n> Epoch 1/50\r\n>  5/54 [=>............................] - ETA: 18s - loss: 4.20602020-12-27 12:11:08.578867: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: \r\n> unspecified launch failure\r\n> 2020-12-27 12:11:08.578954: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_EXECUTION_FAILED\r\n> in tensorflow/stream_executor/cuda/cuda_dnn.cc(1972): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n> 2020-12-27 12:11:08.579262: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1\r\n> 2020-12-27 12:11:08.579796: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1926 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 1024, 1024, 1, 200, 64, 1024]\r\n> ", "comments": ["@Migateak Did you also try with cudnn-8.0.5.39 together with CUDA 11.0? GPU drivers can be the latest, mine are 460.89.\r\n\r\n`generate.py` finished with all 50 epochs under my Windows 10, CUDA 11.0, TF2.4 pypi wheel, CUDNN cudnn-8.0.5.39, python 3.6.8,  i7-9700K, RTX 2070S, venv created with pip -m venv.\r\n\r\nWhat's your full output from this?\r\n`python -c \"import tensorflow as tf;print(tf.config.list_physical_devices('GPU'))\"`\r\nAre you setting the GPU used by tensorflow manually? I'd presume only the 1660 is used? GTX1660S has the same compute capability as mine (7.5), so there shouldn't be much specific why the GPU shouldn't work, out of memory would give OOM throwable.\r\n\r\n```\r\nEpoch 1/50\r\n54/54 [==============================] - 15s 224ms/step - loss: 3.6513 - val_loss: 3.7777\r\nEpoch 2/50\r\n54/54 [==============================] - 14s 245ms/step - loss: 3.0833 - val_loss: 2.5486\r\n........\r\nEpoch 49/50\r\n54/54 [==============================] - 25s 436ms/step - loss: 0.4176 - val_loss: 2.3835\r\nEpoch 50/50\r\n54/54 [==============================] - 29s 511ms/step - loss: 0.4064 - val_loss: 2.4149\r\n```\r\n![image](https://user-images.githubusercontent.com/140952/103170900-37721280-4848-11eb-9ea4-032c46370dc1.png)\r\n", "@ahtik thank you for your answer, I try that and I edit as soon as it's done \ud83d\udc4d \r\nAnd the output of your command is:\r\n\r\n> 2020-12-27 15:12:50.970009: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n> 2020-12-27 15:12:57.940005: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n> 2020-12-27 15:12:57.946552: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n> 2020-12-27 15:12:58.027505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n> pciBusID: 0000:26:00.0 name: GeForce GTX 1660 SUPER computeCapability: 7.5\r\n> coreClock: 1.815GHz coreCount: 22 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s\r\n> 2020-12-27 15:12:58.027963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \r\n> pciBusID: 0000:25:00.0 name: GeForce GT 730 computeCapability: 3.5\r\n> coreClock: 0.9015GHz coreCount: 2 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n> 2020-12-27 15:12:58.028341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n> 2020-12-27 15:12:58.625568: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n> 2020-12-27 15:12:58.625810: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n> 2020-12-27 15:12:58.712507: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-12-27 15:12:58.753141: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n> 2020-12-27 15:12:59.100839: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-12-27 15:12:59.370497: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n> 2020-12-27 15:12:59.392870: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n> 2020-12-27 15:12:59.393487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1847] Ignoring visible gpu device (device: 1, name: GeForce GT 730, pci bus id: 0000:25:00.0, compute capability: 3.5) with core count: 2. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\r\n> 2020-12-27 15:12:59.393743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n> [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n\r\n------------------------\r\n**EDIT**\r\nafter having copied your config (CUDA 11.0, cudNN 8.0.5.39, Nvidia 480.89) I still have the same error ... I tried to reinstall everything again and ALWAYS the same ... \r\n-I start to tell myself that the 1660S doesn't have the right driver yet ... is there a way to choose which GPU to use? to force TF used the 1660S?\r\n-maybe there is no link but i have the same error as here #41169 , my processor is also a ryzen (r5 2600x)", "Hi. I also have the same error so frequently. It sometimes gives such error message, or just hangs.\r\n\r\nIt looks it is related with using RNNs. After update the tensorflow to the latest 2.4.0, using Conv or Dense layers were perfectly fine. Regardless of using Bidirectional, RNN seems causing this problem, randomly.\r\n\r\nI tried CUDA 10.1 + cuDNN 7.6.5, but it didn't solve the problem.\r\n\r\nUsing CPU only works totally fine, so I don't think it is related with the network layer design or data.\r\n\r\n\r\nMy settings are:\r\nHave I written custom code (as opposed to using a stock example script\r\nprovided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10 enterprise 1809\r\nTensorFlow installed from (source or binary): pip install tensorflow\r\nTensorFlow version (use command below): 2.4\r\nPython version: 3.8.6\r\nCUDA/cuDNN version: CUDA 11.2 - cudNN 8.0.5.39\r\nGPU model and memory: Quadro P2000 5GB with 460.89 driver.\r\n\r\nThis is my output of the command:\r\n> python -c \"import tensorflow as tf;print(tf.config.list_physical_devices('GPU'))\"\r\n2020-12-27 08:45:02.471099: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-27 08:45:05.003397: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-27 08:45:05.017145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-12-27 08:45:05.063024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:21:00.0 name: Quadro P2000 computeCapability: 6.1\r\ncoreClock: 1.4805GHz coreCount: 8 deviceMemorySize: 5.00GiB deviceMemoryBandwidth: 130.53GiB/s\r\n2020-12-27 08:45:05.088476: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-27 08:45:05.106946: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-27 08:45:05.113806: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-27 08:45:05.123704: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-27 08:45:05.130466: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-27 08:45:05.143146: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-27 08:45:05.152674: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-27 08:45:05.158809: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-27 08:45:05.166689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n\r\n", "@jihankim have you found a solution?", "@Migateak No. But just forcing using CPU works fine. I think this problem is related with the GPU implementation.", "@jihankim what is your CPU ? ", "> @jihankim what is your CPU ?\r\n\r\nmy CPU is Intel Xeon Silver 4116", "@jihankim Can you provide a minimal complete code with the dataset that is \ncrashing, so one could try to reproduce?\n\nOn December 27, 2020 9:39:31 PM Jihan Kim <notifications@github.com> wrote:\n>\n> @jihankim what is your CPU ?\n> my CPU is Intel Xeon Silver 4116\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n\n", "@Migateak,\r\nI did not face any errors while running the code, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/01c8df7cded1430e7cbcf29485ec495a/45987.ipynb).\r\n\r\nPlease try limiting the GPU memory growth as shown in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and check if you are facing the same issue. Thanks!", "> is there a way to choose which GPU to use? to force TF used the 1660S?\r\n\r\nTo select a particular GPU, please go through the [manual device placement](https://www.tensorflow.org/guide/gpu#manual_device_placement) section of the guide. Thanks!", "This stuff is in no way reproducible on a machine which does not have this error, so I don't get why code is asked. It's clearly a strange incompatibility of some drivers or hardware. \r\n\r\nThis may be caused by partially dead VRAM, but there aren't any applications to do memory tests for VRAM.\r\n\r\nPS\r\nDisabling eager execution disables the cudnn kernel acceleration so you can still run your model on your GPU, which is probably still faster than your cpu without the acceleration. ", "I tried my code on Colab, and it worked fine with gpu. I think there is some compatibility issues related with gpu model or cuda version or cudnn version or nvidia driver... It is going to be really tricky to find out which combination makes such issues..", "@amahendrakar by adding the code [here ](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) to my program a new error appears (nice !)\r\nBtw, if I add this:\r\n\r\n> import os\r\n> os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n> import tensorflow as tf\r\n> config = tf.compat.v1.ConfigProto()\r\n> config.gpu_options.allow_growth=True\r\n> session = tf.compat.v1.Session(config=config)\r\n\r\nwhich also limits the GPU (same output as the code above if I run it alone), It still does not work !\r\nalways the same error as the first post..\r\n\r\n@ion-elgreco concerning dead VRAM, what is it concretely?\r\nThe problem is that when launching the program, during the first \"batches\", my card responds normally: CUDA processor at 100%, increase in graphics memory ... in short in my opinion the problem is software but I do not know not how to test each element and see which one does not work with which given that I have the same software configuration as you ....\r\n\r\nand how to do that? (the PS)", "I downgraded my system to TF2.3, CUDA 10.1, and CUDNN 7.6.5. And the same code is working fine with GPU so far....\r\nIn summary, I tried\r\nTF2.4    CUDA11.1    CUDNN 8.0.5\r\nTF2.4    CUDA10.2    CUDNN 8.0.5\r\nTF2.4    CUDA10.1    CUDNN 7.6.5\r\nTF2.3    CUDA10.1    CUDNN 7.6.5 <= only this combination seems working fine so far...\r\n", "> I downgraded my system to TF2.3, CUDA 10.1, and CUDNN 7.6.5. And the same code is working fine with GPU so far....\r\n> In summary, I tried\r\n> TF2.4 CUDA11.1 CUDNN 8.0.5\r\n> TF2.4 CUDA10.2 CUDNN 8.0.5\r\n> TF2.4 CUDA10.1 CUDNN 7.6.5\r\n> TF2.3 CUDA10.1 CUDNN 7.6.5 <= only this combination seems working fine so far...\r\n\r\nYou may want to try Nvidia developer driver **465.21**: https://developer.nvidia.com/cuda/wsl/download\r\n\r\nThis has fixed the cuda error issues for me.", "I was going to say omg this works! with TF 2.3, CUDA 10.1 and cudNN 7.6.5 but in fact no ... the first 5 epochs, perfect, no problem and ... the return of the error :\r\n\r\n> Epoch 5/50\r\n>  2/54 [>.............................] - ETA: 7s - loss: 2.01372020-12-30 14:38:33.944420: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n> 2020-12-30 14:38:33.944495: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_EXECUTION_FAILED in tensorflow/stream_executor/cuda/cuda_dnn.cc(2011): 'cudnnRNNBackwardData\r\n\r\nI will try with the developer driver and edit this message\r\n\r\n**EDIT**\r\nVictory! finally, it works! with the dev driver 465.21 (thanks a lot @ion-elgreco )", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45987\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45987\">No</a>\n", "> I was going to say omg this works! with TF 2.3, CUDA 10.1 and cudNN 7.6.5 but in fact no ... the first 5 epochs, perfect, no problem and ... the return of the error :\r\n> \r\n> > Epoch 5/50\r\n> > 2/54 [>.............................] - ETA: 7s - loss: 2.01372020-12-30 14:38:33.944420: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n> > 2020-12-30 14:38:33.944495: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_EXECUTION_FAILED in tensorflow/stream_executor/cuda/cuda_dnn.cc(2011): 'cudnnRNNBackwardData\r\n> \r\n> I will try with the developer driver and edit this message\r\n> \r\n> **EDIT**\r\n> Victory! finally, it works! with the dev driver 465.21 (thanks a lot @ion-elgreco )\r\n\r\nGood to see that it works! Seems that developer driver is far more stable.", "> TF2.3 CUDA10.1 CUDNN 7.6.5 <= only this combination seems working fine so far...\r\n\r\n@ion-elgreco Wonderful, great to hear it works! Would you mind sharing how you got this combination working with an RTX 3080?\r\n\r\nI've been holding off the upgrade thinking the RTX 3xxx line requires CUDA11.1 -> build TF from source using unsupported CUDA version or doing some tweaks with the CUDA libs to trick the TF installation?\r\n\r\nApologies if asking the code and dataset didn't seem useful :-) It was my attempt to get the deeper understanding how cuDNN might be broken with the latest line of RTX. CuDNN doesn't seem to be super stable yet with the RTX 30 series.\r\n\r\nIf in trouble, sometimes easier to just remove the cuDNN from CUDA installation and try again. Nice to know the latest dev driver and Windows preview combination works. There are some known issues like https://github.com/pytorch/pytorch/issues/49592, hope it goes smooth and not affecting your model.", "> > TF2.3 CUDA10.1 CUDNN 7.6.5 <= only this combination seems working fine so far...\r\n> \r\n> @ion-elgreco Wonderful, great to hear it works! Would you mind sharing how you got this combination working with an RTX 3080?\r\n> \r\n> I've been holding off the upgrade thinking the RTX 3xxx line requires CUDA11.1 -> build TF from source using unsupported CUDA version or doing some tweaks with the CUDA libs to trick the TF installation?\r\n> \r\n> Apologies if asking the code and dataset didn't seem useful :-) It was my attempt to get the deeper understanding how cuDNN might be broken with the latest line of RTX. CuDNN doesn't seem to be super stable yet with the RTX 30 series.\r\n> \r\n> If in trouble, sometimes easier to just remove the cuDNN from CUDA installation and try again. Nice to know the latest dev driver and Windows preview combination works. There are some known issues like [pytorch/pytorch#49592](https://github.com/pytorch/pytorch/issues/49592), hope it goes smooth and not affecting your model.\r\n\r\nI think you may have tagged the wrong person. TF2.3 does not work for me, at least it works after a 20min slow startup of TensorFlow. But that is not practical.\r\n\r\nI have conda with all my packages, installed tensorflow 2.4 with pip inside my conda env. Then I have Cuda 11.0 update 1 installed and cudnn 8.0.4 with the developer driver 465.12. This developer driver was key for my current setup with a ryzen cpu and rtx 3080. I don't know why, but cudnn and cuda is stable now. \r\n\r\nI am not on an insider build of windows. I am running **Windows 10 Pro, Version 20H2, OS build 19042.685**\r\n\r\nWe need TensorFlow builds with cuda 11.1, because that's where the official support starts for RTX 30 series cards https://github.com/tensorflow/tensorflow/issues/46093. I don't see why these are not yet supplied.\r\n\r\nPS\r\n\r\n> Apologies if asking the code and dataset didn't seem useful :-) It was my attempt to get the deeper understanding how cuDNN might be broken with the latest line of RTX. CuDNN doesn't seem to be super stable yet with the RTX 30 series.\r\n\r\nNo problems, I was a bit sassy, because I had this issue for almost 2 weeks, my bad :(\r\n", "> \r\n> \r\n> > TF2.3 CUDA10.1 CUDNN 7.6.5 <= only this combination seems working fine so far...\r\n> \r\n> @ion-elgreco Wonderful, great to hear it works! Would you mind sharing how you got this combination working with an RTX 3080?\r\n> \r\n> I've been holding off the upgrade thinking the RTX 3xxx line requires CUDA11.1 -> build TF from source using unsupported CUDA version or doing some tweaks with the CUDA libs to trick the TF installation?\r\n> \r\n> Apologies if asking the code and dataset didn't seem useful :-) It was my attempt to get the deeper understanding how cuDNN might be broken with the latest line of RTX. CuDNN doesn't seem to be super stable yet with the RTX 30 series.\r\n> \r\n> If in trouble, sometimes easier to just remove the cuDNN from CUDA installation and try again. Nice to know the latest dev driver and Windows preview combination works. There are some known issues like [pytorch/pytorch#49592](https://github.com/pytorch/pytorch/issues/49592), hope it goes smooth and not affecting your model.\r\n\r\nHey @ahtik , Did you try with CUDA 11.1 and the dev driver 465.21 ?\r\ngiven that for the moment everything is working I don't want to break everything xD\r\nHave you tried it and if so, everything works? I would like to know before I break everything", "I have had the same problem and I don't think it is a driver issue but I have found a solution that is logical and works.\r\n \r\nWindows has a service called TDR (Timeout Detection and Recovery) which pings the GPU to check it hasn't frozen. If it does not get a reply within 2 seconds it resets the GPU causing the CUDA operation to fail. The point of failure can be a bit random depending on the operation that was conducted at the time of the reset. \r\n\r\nTo fix this problem start the Nvidia Nsight Monitor in admin mode (start it by right-clicking and chose run as Administrator). Then right click the task bar icon and select options. Now locate the TDR section and either disable the TDR or increase the timeout period. I increased the timeout period to 20 seconds which appears to be enough for my ML tasks.\r\n\r\nIt is likely that the downgrade in drivers works because they doesn\u2019t utilize the GPU as effectively as the newer driver why there is enough power left for the ping service to get its response back in time. This should however be a pretty pore reason not to upgrade drivers as it probably means slower processing of you task at hand. \r\n\r\nIn addition to this there was also a driver issue which appeared after the TDR fix which is similar to the one above but less random in its behavior, happened at the same place in the cudnn code every time and appears to be LSTM specific (I don't remember the exact error message). By upgrading to the latest driver version 461.40 That also disappeared and now everything works flawlessly. ", "> I have had the same problem and I don't think it is a driver issue but I have found a solution that is logical and works.\r\n> \r\n> Windows has a service called TDR (Timeout Detection and Recovery) which pings the GPU to check it hasn't frozen. If it does not get a reply within 2 seconds it resets the GPU causing the CUDA operation to fail. The point of failure can be a bit random depending on the operation that was conducted at the time of the reset.\r\n> \r\n> To fix this problem start the Nvidia Nsight Monitor in admin mode (start it by right-clicking and chose run as Administrator). Then right click the task bar icon and select options. Now locate the TDR section and either disable the TDR or increase the timeout period. I increased the timeout period to 20 seconds which appears to be enough for my ML tasks.\r\n> \r\n> It is likely that the downgrade in drivers works because they doesn\u2019t utilize the GPU as effectively as the newer driver why there is enough power left for the ping service to get its response back in time. This should however be a pretty pore reason not to upgrade drivers as it probably means slower processing of you task at hand.\r\n> \r\n> In addition to this there was also a driver issue which appeared after the TDR fix which is similar to the one above but less random in its behavior, happened at the same place in the cudnn code every time and appears to be LSTM specific (I don't remember the exact error message). By upgrading to the latest driver version 461.40 That also disappeared and now everything works flawlessly.\r\n\r\nSince 461.09 it was fixed. Good to see new driver versions are still working and not breaking it again.", "> > I downgraded my system to TF2.3, CUDA 10.1, and CUDNN 7.6.5. And the same code is working fine with GPU so far....\r\n> > In summary, I tried\r\n> > TF2.4 CUDA11.1 CUDNN 8.0.5\r\n> > TF2.4 CUDA10.2 CUDNN 8.0.5\r\n> > TF2.4 CUDA10.1 CUDNN 7.6.5\r\n> > TF2.3 CUDA10.1 CUDNN 7.6.5 <= only this combination seems working fine so far...\r\n> \r\n> You may want to try Nvidia developer driver **465.21**: https://developer.nvidia.com/cuda/wsl/download\r\n> \r\n> This has fixed the cuda error issues for me.\r\n\r\nThis is solved my issue. Thanks"]}, {"number": 45986, "title": "TFLite Benchmark Binary Erroring with FastSpeech Model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\nI used [nightly pre-built binary](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model_plus_flex) from [here](https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary)\r\n\r\nFastSpeech Model used for benchmark can be downloaded from [here](https://github.com/tulasiram58827/TTS_TFLite/blob/main/models/fastspeech_quant.tflite)\r\n\r\nError : \r\n\r\n```\r\nThe input model file size (MB): 31.1646\r\n Initialized session in 13.661ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\nERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.\r\nERROR: Node number 553 (EXPAND_DIMS) failed to prepare.\r\n\r\nERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.\r\nERROR: Node number 553 (EXPAND_DIMS) failed to prepare.\r\n\r\nERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.\r\nERROR: Node number 553 (EXPAND_DIMS) failed to prepare.\r\n\r\nERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.\r\nERROR: Node number 553 (EXPAND_DIMS) failed to prepare.\r\n\r\nERROR: tensorflow/lite/kernels/expand_dims.cc:40 axis <= input_dims.size was not true.\r\n```\r\n\r\nAfter this error I converted the model with fixed size and I got this error.\r\n\r\n```\r\nThe input model file size (MB): 31.1646\r\nInitialized session in 13.661ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\nERROR: tensorflow/lite/kernels/range.cc:45 (start >= limit && delta < 0) || (start <= limit && delta > 0) was not true.\r\nERROR: Node number 477 (RANGE) failed to invoke.\r\n\r\nERROR: tensorflow/lite/kernels/range.cc:45 (start >= limit && delta < 0) || (start <= limit && delta > 0) was not true.\r\nERROR: Node number 477 (RANGE) failed to invoke.\r\n\r\nERROR: tensorflow/lite/kernels/range.cc:45 (start >= limit && delta < 0) || (start <= limit && delta > 0) was not true.\r\nERROR: Node number 477 (RANGE) failed to invoke.\r\n\r\nERROR: tensorflow/lite/kernels/range.cc:45 (start >= limit && delta < 0) || (start <= limit && delta > 0) was not true.\r\nERROR: Node number 477 (RANGE) failed to invoke.\r\n```\r\n", "comments": ["The benchmark tool feeds random input data to the graph. The FastSpeech model may require some valid set of data. If the model requires that, the random input data may break like the above case. Could you make sure that? @tulasiram58827 \r\n\r\n+CCing @multiverse-tf ", "How to provide custom input to the model while performing benchmark?\n\nIs there any argument? I can't see anything like that in the benchmark guide page of tensorflow docs\n\n@abattery ", "I don't have good knowledge on that part neither. @multiverse-tf can provide more information regarding the benchmark tool.", "> How to provide custom input to the model while performing benchmark?\r\n> \r\n> Is there any argument? I can't see anything like that in the benchmark guide page of tensorflow docs\r\n> \r\n\r\nWe do provide parameters (i.e. --input_layer, --input_layer_shape, --input_layer_value_range, --input_layer_value_files) to help set values of input layers. Pls refer to definitions [here]( https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc#L289-L307) in the [benchmark_tflite_model.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc)\r\n\r\nFor example, if the model has 2 inputs, one could add \"--input_layer=in0,in1 --input_layer_shape=1,30:1 --input_layer_value_range=in0,0,4\" when running the benchmark tool. Noting that:\r\n1. one has to specify all inputs of the model for --input_layer and --input_layer_shape. However, the input name doesn't need to match that of the model. The tool simply assumes the same input tensor order that TFLite interpreter sees. Admittedly, this will be a bit inconvenient as one has to learn the exact order of input tensors but a TFLite model visualization tool (like [the one](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/visualize.py) TFLite provides or the [Netron](https://github.com/lutzroeder/netron)) will help.\r\n\r\n2. In this case, the value of the 1st input tensor (i.e. in0) will be initialized randomly but limited between 0 and 4.\r\n\r\n\r\nWe will document these parameters in related docs asap.\r\n\r\n> @abattery\r\n\r\n", "Thanks @multiverse-tf your suggestions worked out. Hope they will be added to docs in future.\r\n\r\nAlso as mentioned by @abattery random inputs are feeded while benchmarking so I ran benchmarking iteratively 3 or 4 times. Atleast once I got the results. So I think this hack can also be used. But yeah this depends on luck :).\r\n\r\nThanks @abattery and @multiverse-tf  once again", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45986\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45986\">No</a>\n"]}, {"number": 45985, "title": "Install Error \"github.com/tensorflow/examples\"", "body": "Would you tell me?\r\nI want install \"tensorflow example\" from Anaconda for windows10.\r\n\r\nHowever, no matter how many times I repeat it, I get an error.\r\n\r\nInstalled anaconda3\r\n\r\n**Procedure so far\r\nconda create -n insta1\r\nconda activate insta1\r\nconda install python=3.7.5\r\ngit clone https://github.com/tensorflow/examples\r\n(insta1) C:\\installation\\insta2020-01\\examples\\tensorflow_examples\\lite\\model_maker\\pip_package>pip install -e .\r\n\r\n**Error contents\r\n---------------------------------\r\n(insta1) C:\\installation\\insta2020-01\\examples\\tensorflow_examples\\lite\\model_maker\\pip_package>pip install -e .\r\nObtaining file:///C:/installation/insta2020-01/examples/tensorflow_examples/lite/model_maker/pip_package\r\n    ERROR: Command errored out with exit status 1:\r\n     command: 'C:\\anaconda3\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\installation\\\\insta2020-01\\\\examples\\\\tensorflow_examples\\\\lite\\\\model_maker\\\\pip_package\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\installation\\\\insta2020-01\\\\examples\\\\tensorflow_examples\\\\lite\\\\model_maker\\\\pip_package\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\toshi\\AppData\\Local\\Temp\\pip-pip-egg-info-su_59rf8'\r\n         cwd: C:\\installation\\insta2020-01\\examples\\tensorflow_examples\\lite\\model_maker\\pip_package\\\r\n    Complete output (11 lines):\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"C:\\installation\\insta2020-01\\examples\\tensorflow_examples\\lite\\model_maker\\pip_package\\setup.py\", line 232, in <module>\r\n        setup_extra = prepare_package_src()\r\n      File \"C:\\installation\\insta2020-01\\examples\\tensorflow_examples\\lite\\model_maker\\pip_package\\setup.py\", line 188, in prepare_package_src\r\n        namespace_packages = find_namespace_packages(where=BUILD_ROOT)\r\n      File \"C:\\anaconda3\\lib\\site-packages\\setuptools\\__init__.py\", line 64, in find\r\n        convert_path(where),\r\n      File \"C:\\anaconda3\\lib\\distutils\\util.py\", line 121, in convert_path\r\n        if pathname[0] == '/':\r\n    TypeError: 'WindowsPath' object is not subscriptable\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.", "comments": ["Can you try using `pip` to install `tflite-model-maker`?\r\n` pip install  tflite-model-maker`", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45985\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45985\">No</a>\n", "Hi there, I also reproducing the same issue\r\nI'm running python 3.8.7 and also carefully double-checked the required dependencies versions for running this example\r\nHere's my stacktrace:\r\n\r\nC:\\Users\\Walter\\Downloads\\tensorflow-examples\\tensorflow_examples\\lite\\model_maker\\pip_package>pip install -e .\r\nObtaining file:///C:/Users/Walter/Downloads/tensorflow-examples/tensorflow_examples/lite/model_maker/pip_package\r\n    ERROR: Command errored out with exit status 1:\r\n     command: 'C:\\Users\\Walter\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Walter\\\\Downloads\\\\tensorflow-examples\\\\tensorflow_examples\\\\lite\\\\model_maker\\\\pip_package\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Walter\\\\Downloads\\\\tensorflow-examples\\\\tensorflow_examples\\\\lite\\\\model_maker\\\\pip_package\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\Walter\\AppData\\Local\\Temp\\pip-pip-egg-info-fcpvhhz7'\r\n         cwd: C:\\Users\\Walter\\Downloads\\tensorflow-examples\\tensorflow_examples\\lite\\model_maker\\pip_package\\\r\n    Complete output (11 lines):\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"C:\\Users\\Walter\\Downloads\\tensorflow-examples\\tensorflow_examples\\lite\\model_maker\\pip_package\\setup.py\", line 240, in <module>\r\n        setup_extra = prepare_package_src()\r\n      File \"C:\\Users\\Walter\\Downloads\\tensorflow-examples\\tensorflow_examples\\lite\\model_maker\\pip_package\\setup.py\", line 196, in prepare_package_src\r\n        namespace_packages = find_namespace_packages(where=BUILD_ROOT)\r\n      File \"C:\\Users\\Walter\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\setuptools\\__init__.py\", line 64, in find\r\n        convert_path(where),\r\n      File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\distutils\\util.py\", line 121, in convert_path\r\n        if pathname[0] == '/':\r\n    TypeError: 'WindowsPath' object is not subscriptable\r\n    ----------------------------------------\r\nWARNING: Discarding file:///C:/Users/Walter/Downloads/tensorflow-examples/tensorflow_examples/lite/model_maker/pip_package. Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\r\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\r\n\r\nThank you in advance"]}, {"number": 45984, "title": "Parallel execution of ops on tensors in unstacked list ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, mapping/vector operations execute only batches in parallel, implying that if an op has to be performed on multiple tensors in parallel, all of them should be stacked, and therefore have the same shape.\r\nThis feature will allow execution of all tensors in a python iterable in parallel, without requiring them to be stacked. It will be analogous to the `map` function in python.\r\n\r\n**Will this change the current api? How?**\r\nThis will add a new function to the API, accepting two arguments: A callable and a python iterable of tensors, not necessarily of the same shape.\r\nThe callable takes a single argument, which can be a single or nested tensor.\r\n\r\n**Who will benefit with this feature?**\r\nThis feature has a wide range of applications, and can benefit the entire community.\r\n\r\n**Any Other info.**\r\n", "comments": ["@Susmit-A,\r\nYou mentioned \r\n\r\n> This feature has a wide range of applications \r\n\r\nCan you specify some applications? Thanks!", "@rmothukuru Off the top, the main application would be parallel processing of temporal data with different number of timesteps, and parallel processing of images with different dimensions, both without padding.\n\nAnother classic application is the Parallel For, provided by matlab. Currently, parallelizing a loop using `tf.while_loop` adds unnecessary constraints and complexity.", "@Susmit-A Could you provide a code example? \r\n\r\nI imagine that running python `map` on the unstacked list is not an option for you due to the large graph for the unrolled loop?\r\n\r\nRaggedTensors might work here depending on what you are trying to do cc: @edloper @mdanatg ", "I think the challenge here is that feeding a Python iterable/list into an efficient graph requires quite a few moving parts. It would be much more practical if that iterable was a TF structure that allowed uneven shapes, like RaggedTensor, TensorArray or Dataset.\r\n\r\nCome to think of it, would `Dataset.from_generator(...).map(...)` work for this purpose? See [Dataset.from_generator](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator).", "@saxenasaurabh That is correct. The same can also be said for pythonic for loops - they take impractically long times for large lists.\r\n\r\n@saxenasaurabh @mdanatg Please refer to [this file](https://github.com/Susmit-A/addons/blob/master/tensorflow_addons/layers/adaptive_pooling.py), specifically lines 76-83. This is a minimal example of an application, currently as a PR in tensorflow addons.\r\n\r\nHere, I need to slice a tensor in multiple places *unevenly*, using a list of starting and ending indices. \r\n\r\nRight now, this is implemented using a loop. RaggedTensors cannot be used here since they can't be created by stacking a list of tensors - they require a list of python/numpy arrays.", "`tf.map_fn` supports ragged tensors.  So you can do something like this:\r\n\r\n```\r\nx = tf.ragged.constant([[1, 2, 3], [4, 5], [6, 7, 8]])\r\ndef fn(row):\r\n  return tf.math.cumsum(row)\r\ntf.map_fn(fn, x)\r\n```\r\n\r\n(This is a trivial example, but you could do something more interesting in `fn`.)  \r\n\r\nIf your function returns a value with a different number of ragged dimensions than the input (or a different dtype than the input), then you'll need to specify `fn_output_signature` when calling `map_fn`.\r\n\r\nIf you want to use this with `tf.data`, you could do something along the lines of:\r\n\r\n```\r\ndef fn():\r\n  for i in range(1, 10):\r\n    yield range(i)\r\n\r\ndataset = tf.data.Dataset.from_generator(fn, output_types=tf.int32, output_shapes=[None])\r\nbatched = dataset.apply(tf.data.experimental.dense_to_ragged_batch(10))\r\nprint(next(iter(batched)))\r\n```\r\n\r\nFor more details on ragged tensors, see: https://www.tensorflow.org/guide/ragged_tensor.\r\n\r\nAlso, see the \"RaggedTensors\" section on this page: https://www.tensorflow.org/api_docs/python/tf/map_fn.\r\n", "> RaggedTensors cannot be used here since they can't be created by stacking a list of tensors - they require a list of python/numpy arrays.\r\n\r\nSee `tf.ragged.stack`.  https://www.tensorflow.org/api_docs/python/tf/ragged/stack", "I'm not sure what you mean by saying that RaggedTensors can't be created within a graph from a list of tensors.  E.g.:\r\n\r\n```python\r\n>>> # Stacking two dense tensors with different sizes.\r\n>>> t3 = tf.constant([[1, 2, 3], [4, 5, 6]])\r\n>>> t4 = tf.constant([[5], [6], [7]])\r\n>>> assert isinstance(t3, tf.Tensor) and isinstance(t4, tf.Tensor)\r\n>>> tf.ragged.stack([t3, t4], axis=0)\r\n<tf.RaggedTensor [[[1, 2, 3], [4, 5, 6]], [[5], [6], [7]]]>\r\n```", "@edloper Please refer to [this stackoverflow question](https://stackoverflow.com/questions/57346556/creating-a-ragged-tensor-from-a-list-of-tensors). This is the error thrown when creating RaggedTensors from a list of tensors. The solution provided in the link makes use of a for loop, rendering the use of RaggedTensors moot.\r\n\r\nAltough the code in the link uses `tf.ragged.constant`, `tf.ragged.stack` throws the same error.", "So you're getting an error from the following code?\r\n\r\n```\r\na = tf.convert_to_tensor([1,2])\r\nb = tf.convert_to_tensor([1,2,3])\r\ntf.ragged.stack([a, b])\r\n```\r\n\r\nI find that surprising.  Can you give me a full stack trace?", "Looking at lines 76-84 of the adaptive_pooling.py file you linked -- what exactly is it you want to happen here?  Do you want the `N=self.output_size[0]` different calls to `self.reduce_function` to be able to run in parallel?  If so, then that will already be the case -- `self.output_size[0]` is an int (not a tensor), so when the `call` method is wrapped in `tf.function`, the loop will be unrolled, and the resulting graph will contain different subgraphs for each iteration of the loop on line 76.  When the graph is run, the nodes in those subgraphs can run in parallel.", "The problem here is that the inputs to `call` may be of variable size, therefore requiring a graph retrace for every call. That's the reason the function needs to be eager (and another reason why `map_fn` can't be used).", "If you need `call` to be able to take any number of input tensors, then I'd recommend combining them all into a single ragged tensor input.  I.e., rather than having `call` take a list of `N` float tensors with shape [A], have it take a single raggedTensor with shape [N, A].  Otherwise, `call` will need to be retraced for each different number of input tensors.\r\n\r\nIf the number of inputs doesn't vary too much, but the shape of each tensor varies, then you could try using the `experimental_relax_shapes` argument when calling the `tf.function` decorator to reduce the amount of retracing that's required.\r\n", "It is the shape that varies. However, `experimental_relax_shapes` still causes retracing.", "With `experimental_relax_shapes`, it will retrace a couple times, but then should trace with a relaxed shape after that.  E.g., the following:\r\n\r\n```python\r\n@tf.function(experimental_relax_shapes=True)\r\ndef foo(x):\r\n  print(\"Retrace\", x.shape)\r\n  return x\r\n\r\nfor i in range(10):\r\n  foo(tf.range(i))\r\n```\r\n\r\nprints:\r\n\r\n```\r\nRetrace (0,)\r\nRetrace (1,)\r\nRetrace (None,)\r\n```\r\n\r\nSo the graph gets retraced a total of 3 times (and won't get retraced again after that, unless the dtype or rank of the input changes).  ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45982, "title": "Simplify GPU usage model", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1.0 - Python\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIMO many new developers are using TF as it supports CUDA, and they have a single GPU they want to use. So one common use case is a single GPU. To utilize a GPU device script writers are using the following code \r\n\r\n```python\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```\r\n\r\nCan we have a simple API that enables the GPU first device ?\r\n\r\nproposed API\r\n\r\nexperimental:\r\n```python\r\nconfigure_default_device(string physical_device_name, int device_number)\r\nget_default_device()\r\n```\r\n\r\nexample usage:\r\n```python\r\ntf.config.experimental.configure_default_device('GPU', 0)\r\n\r\nget_default_device()\r\n     example settings above would return\r\n                  \"GPU\" device 0\r\n\r\n```\r\n**Will this change the current api? How?**\r\nYes\r\nSimplify a common use case, and abstract away these settings so they can be changed in future without developers having to re-write their scripts ie when moving to later TF version\r\n\r\n**Who will benefit with this feature?**\r\nMany new developers with a single GPU\r\n\r\n**Any Other info.**\r\nIt would be good to extend the API to handle specific GPU devices, or to set the engine to use all of them. Another API would show the current settings.", "comments": ["@marcusobrien,\r\n[Tensorflow GPU Documentation](https://www.tensorflow.org/guide/gpu?hl=en) states: \r\n\r\n> TensorFlow code, and tf.keras models will transparently run on a single GPU with no code changes required.\r\n> \r\n> **Note: Use tf.config.experimental.list_physical_devices('GPU') to confirm that TensorFlow is using the GPU.**\r\n\r\nUsing 1 GPU is already straight forward. What do you think? ", "@marcusobrien,\r\nCan you please respond to the above comment? Thanks!  ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45982\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45982\">No</a>\n"]}]