[{"number": 38959, "title": "Add gcs_file_system to be part of the pip wheel build", "body": "This PR is related to https://github.com/tensorflow/io/pull/919, while trying to include\r\n'tensorflow/core/platform/cloud/gcs_file_system.h' with installed tf-nightly,\r\nthe Windows wheel is missing this file while Linux and macOS works file.\r\n\r\nIt seems that gcs_file_system was not included in windows tf-nightly build\r\n\r\nThis PR adds gcs_file_system to tf-niglty windows build.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @ebrevdo for the review. Removing the line `\"//tensorflow:windows\"` directly does not work as there are other complications. There are some related discussion in another thread:\r\nhttps://github.com/tensorflow/tensorflow/issues/19297#issuecomment-602843577"]}, {"number": 38958, "title": "Update examples in docstring to use TF 2.x code", "body": "The examples in docstrings of two APIs, tf.histogram_fixed_width_bins\r\nand tf.histogram_fixed_width still used TF 1.x code.\r\n\r\nThis PR updates the docstring to use TF 2.x code in examples.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @alextp for the review, the PR has been updated to conform to python doctest. Please take a look and let me know if there are any issues.", "@yongtang Can you please address Ubuntu Sanity errors? Thanks!", "Thanks @alextp for the help. I changed to `indices.numpy()` as `indices` directly will output more than 80 and pylint may fail (line-too-long). Let me know if this works."]}, {"number": 38957, "title": "Update iOS related rules", "body": "These rules repos no longer do releases. These are recent master commit\r\nshas that are compatible with the current release of bazel.\r\n\r\nThis update enables building `ios_static_framework` targets from `swift_library` targets for https://github.com/tensorflow/tensorflow/pull/38956", "comments": ["@mihaimaruseac can you help with the copybara failure here?", "Thanks!"]}, {"number": 38956, "title": "Add ios_static_framework bazel target for Swift", "body": "This adds an `ios_static_framework` target that builds the Swift\r\nlibrary for bundling it in other apps. This requires you include\r\nboth the Swift framework, and the C framework.", "comments": ["Hrm, this might not work end to end because of the Swift library exposing the API from `TensorFlowLiteC`. AFAIK you cannot expose multiple modules in a single framework, which we would have to do for this case since they can't be combined. \ud83e\udd14", "@dan-zheng Could you take a look at this PR?", "Any context on how this _should_ be setup to work would be great!", "I'm actually not able to review this patch since I don't work on TensorFlow Lite.\r\n@thaink: could you please ask someone from the TensorFlow Lite team to review instead? Thanks!", "Hi Tei,\r\nCould you take a look at this?", "Hi Keith, What is your use case? Maybe building your own version of custom TensorFlowLiteC library and not using CocoaPods for installing TFLite?\r\n", "Tried building and running this, and got some problems. It seems a tricky problem :/\r\n\r\n1. Using TensorFlowLiteSwift.framework only (the one built with this PR)\r\n-> Can't find TensorFlowLiteC, so the build fails\r\n2. Including extra TensorFlowLiteC.framework\r\n-> Duplicate symbol error occurs.\r\n\r\nThe TensorFlowLiteSwift is a collection of source files. So while we have real Swift framework, you might want to try setting up another TensorFlowLiteC podspec with your custom binary.", "So I pushed an update here that is actually working for us locally. But adding the ObjC dependency to the `avoid_deps` of the `ios_static_framework` it ensures that library isn't contained in the final archive. Then you just have to also use TensorFlowLiteC as you suggested when you're actually linking your app for this to work. I didn't initially go down this path because I was hoping we could have just a single framework for folks to depend on, but this seems to work fine.", "Hmm I tried the same but still got duplicate symbol error - will test on the clean project to make sure it builds. Nevertheless thanks for the PR!", "Locally I'm not seeing any duplicate symbol issues with these having also applied https://github.com/tensorflow/tensorflow/pull/38957\r\n\r\nWhat symbols specifically are failing for you?", "Hi Keith, sorry for the delay.\r\n\r\nI cleaned up my project, and added `libc++` dependency to make this work. Great, but wondering if you had to include `libc++` framework too and is there way to automatically do this. (The build failed with messaged like `Undefined symbol: std::__1::chrono::system_clock::now()`)", "I didn't but it's likely that our build automatically adds that for some other dependency at this point. I think there is some way we can add this, let me try something", "Hrm, I can't reproduce this yet, in what configuration are you building where you have that issue? Both linking a `ios_framework` and just dragging these into a fresh xcodeproj and building an app work fine for me", "Here's what I tried:\r\n\r\n1. build both frameworks\r\n2. drag them into PoseNet example app\r\n3. build -> fails with libc++ related errors.\r\n\r\nThe step 2. was not clean enough for me - I lifted out existing CocoaPods related files, and I might missed something while doing so. If (Build frameworks -> drag them into fresh project) is a working path, I'm fine with this.\r\n\r\nThanks!", "Dragging them into a fresh project, not CP related at all does seem to work for me", "Great. Thanks!\r\n\r\n@yyoon is there some other approvals needed for this to be merged?", "@yyoon Can you please take a look on the above comment from @teijeong. Thanks!", "@teijeong Yes, let me ping you with the instructions.", "> Hrm, this might not work end to end because of the Swift library exposing the API from `TensorFlowLiteC`. AFAIK you cannot expose multiple modules in a single framework, which we would have to do for this case since they can't be combined. \ud83e\udd14\r\n\r\nHey @keith, can you clarify what the issue is here? With the `ios_static_framework` you added, there should be a single dep on `TensorFlowLite` `swift_library`. That library *should* (see note below) have a single dep on [tensorflow_lite_c](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ios/BUILD.apple#L82) `cc_library`. When the framework is built, it should bundle both the `swift_library` and `cc_library` without needing to specify `avoid_deps`. Curious what build issues you were seeing?\r\n\r\nI can see this approach being an issue if your team also needs to invoke the [TensorFlowLiteC_framework](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ios/BUILD.apple#L33) APIs directly; in which case, the `avoid_deps` approach would be needed to avoid duplicated symbols when both `TensorFlowLiteC_framework` and `TensorFlowLite_framework` are added to your list of deps. But if your team is only invoking the Swift APIs, then I don't think their should be an issue with removing `avoid_deps` (I could be missing something though).\r\n\r\n* Note that the dep on [coreml_delegate](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/swift/BUILD.apple#L37) should only be included if you need CoreML delegation support. The team is currently working on providing a cleaner solution for including/excluding CoreML and Metal delegation. @teijeong ", "> Hey @keith, can you clarify what the issue is here? With the ios_static_framework you added, there should be a single dep on TensorFlowLite swift_library. That library should (see note below) have a single dep on tensorflow_lite_c  cc_library. When the framework is built, it should bundle both the swift_library and cc_library without needing to specify avoid_deps. Curious what build issues you were seeing?\r\n\r\n> Got a comment from merging this internally - what was the initial reason for bundling both TensorFlowLite_framework and TensorFlowLiteC_framework, and not just bundling TensorFlowLite_framework alone? Maybe I missed something..\r\n\r\nSo the issue is when you produce a Swift static framework, where it publicly depends on another module (it's public in this case because the Swift TF implementation exposes symbols from the `cc_library`) you must vendor that module as a separate framework since a single framework cannot expose multiple modules AFAIK\r\n\r\nHere's a sample project demoing the issue [SampleTFProject.zip](https://github.com/tensorflow/tensorflow/files/4659505/SampleTFProject.zip)\r\n\r\nI built the framework included in this project without the `avoid_deps` line. Here's the specific failure:\r\n\r\n```\r\n/tmp/SampleTFProject/TensorFlowLite.framework/Modules/TensorFlowLite.swiftmodule/x86_64.swiftinterface:6:8: error: no such module 'TensorFlowLiteC'\r\nimport TensorFlowLiteC\r\n       ^\r\n/tmp/SampleTFProject/SampleTFProject/AppDelegate.swift:10:8: error: failed to load module 'TensorFlowLite'\r\nimport TensorFlowLite\r\n       ^\r\n/tmp/SampleTFProject/TensorFlowLite.framework/Modules/TensorFlowLite.swiftmodule/x86_64.swiftinterface:6:8: error: no such module 'TensorFlowLiteC'\r\nimport TensorFlowLiteC\r\n       ^\r\n```\r\n\r\nThis makes sense since as far as this iOS project is concerned it's true that `TensorFlowLiteC` doesn't exist. If there is a workaround for this where we could somehow create a single framework that also bundled this module I would love to know about it and get rules_apple updated to produce it!\r\n", "Thanks @keith for the explanation. I see what you are saying now, which does make this a bit more challenging when depending on the static framework rather than having an `ios_application` depend on the `TensorFlowLite` `swift_library` directly. Hopefully it's not too much trouble to add both the frameworks to the xcodeproj.\r\n\r\nIn terms of workarounds for the Apple rules, I am not aware of any either and I don't think there's anything we can do in the Swift code to avoid publicly exposing the `TensorFlowLiteC` module and its symbols. Would definitely be open to considering other options if there's something we can do on the Swift side.\r\n\r\nPR looks good to me, other than should line 28 be:\r\n```\r\n\"//tensorflow/lite/experimental/ios:TensorFlowLiteC_framework\",\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ios/BUILD.apple#L22", "I don't think that line should be the framework, since the framework isn't in the dependency tree here, just the library it depends on?", "But I did update it to `//tensorflow/lite/experimental/ios:tensorflow_lite_c` which I believe is what we want here now that the `objc_library` target is gone", "> Note that the dep on coreml_delegate  should only be included if you need CoreML delegation support. The team is currently working on providing a cleaner solution for including/excluding CoreML and Metal delegation. @teijeong\r\n\r\nIs there any way I could help with this effort? Excluding coreml is something we want, and it also side-steps this bazel bug https://github.com/bazelbuild/bazel/issues/11223", "> But I did update it to `//tensorflow/lite/experimental/ios:tensorflow_lite_c` which I believe is what we want here now that the `objc_library` target is gone\r\n\r\nMy apologies for the confusion. What you have now is correct. It should be avoiding the dep on the `cc_library`.", "> > Note that the dep on coreml_delegate  should only be included if you need CoreML delegation support. The team is currently working on providing a cleaner solution for including/excluding CoreML and Metal delegation. @teijeong\r\n> \r\n> Is there any way I could help with this effort? Excluding coreml is something we want, and it also side-steps this bazel bug [bazelbuild/bazel#11223](https://github.com/bazelbuild/bazel/issues/11223)\r\n\r\nI believe @teijeong is looking into a solution for excluding/including CoreML and Metal delegation deps, so he could probably add more details. Perhaps we could create an issue to track and discuss?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/swift/BUILD.apple#L13-L24\r\n\r\n", "Sounds good filed https://github.com/tensorflow/tensorflow/issues/39737 !", "@tristane0 can you help merge this?", "> @tristane0 can you help merge this?\r\n\r\nReached out to @yyoon, who should be able to approve and merge once he is back online."]}, {"number": 38955, "title": "Fix incorrect usage of os.system().", "body": "The code wrongly assumed that os.system(cmd) returns the exit code of the\r\nprocess. Instead, it returns a concatenation of the exit code and signal\r\nnumber that terminated the process (if any). This change uses the proper\r\naccessor methods to extract the exit code and signal number from the\r\nreturn value. If a process terminated with exit(N) we return N. Else, we\r\nreturn the negative signal number. This is in line with the behaviour of\r\nsubprocess.run.\r\n\r\nSpecfically, this change fixes a bug where nvcc would exit with 1 but\r\nthe wrapper script would exit with 0. This is because os.system() would\r\nreturn 0x0100 which got truncated to 0x00 due to exit codes being\r\nlimited to 8-bit on Linux.", "comments": []}, {"number": 38954, "title": "Random uniform is inconsistent given same seed values", "body": " \r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n  _I was trying to use one of example usages from tensorflow docs and wanted to make results reproducible. After investigation it looks to me that `gen_random_ops.random_uniform` is inconsistent given seeds. That is why I have written a test ( custom code)_\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n   _Darwin-18.0.0-x86_64-i386-64bit_\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: _No_\r\n- TensorFlow installed from : _pip install tensorflow_\r\n- TensorFlow version : _v2.1.0-rc2-17-ge5bf8de410 2.1.0_\r\n- Python version: python version: _3.7.6_\r\n- Bazel version (if compiling from source): _brew installation_\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n**Describe the current behavior**\r\n`gen_random_ops.random_uniform` return different values even with same seeds\r\n**Describe the expected behavior**\r\nconsistent behaviour is expected give same seeds\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nshape = (8,12)\r\ndtype = 'float32'\r\nseed = 5\r\nseed2 = 1234\r\ntf.random.set_seed(seed)\r\nrnd = gen_random_ops.random_uniform(shape, dtype, seed=seed, seed2=seed2)\r\nrnd2 = gen_random_ops.random_uniform(shape, dtype, seed=seed, seed2=seed2)\r\n# rnd and rnd2 will be different\r\n```\r\n\r\n**Other info / logs** \r\ncode does not throw exceptions. Just expected behaviour is different from results.\r\n\r\nFrom `random_seed.py` I can see that probably it is expected behaviour but I don't understand why is that. This counter that keeps changing seed  by default is a way to avoid generation of same values? But usually for tests we need reproducibility. Does it mean that It should be done through re-setting of the `tf.random.set_seed(1234)`?\r\n", "comments": ["@deil87 \r\nCode provided seems incomplete please provide complete stand alone code for us to replicate the issue faced.", "@Saduf2019 I have realised that this is probably not a bug but a feature. It is just a bit unexpected behaviour -  usually you don't need to re-run script or set seed one more time to get consistency. But `random_seed.py` explains that this is how it is done in tensorflow. I'm going to close this one.", "Yes this is an undesirable property of the legacy stateful random uniform ops. Use either the generator API or the stateless random ops for sane behavior."]}, {"number": 38953, "title": "[ROCm] Updates for XLA unit-tests on the ROCm platform", "body": "This PR \r\n* adds no_rocm tag to tests within the `//tensforlow/compiler/xla/service/mlir_gpu` dir\r\n These tests need to be disabled on the ROCm platform, until we add ROCm support for them\r\n* fixes a bug that was causing some test failures\r\n @ekuznetsov139 originally applied the fix on the ROCm fork. this PR upstreams it.\r\n\r\n\r\n-------------------------------------\r\n\r\n/cc @cheshire @chsigg @nvining-work @ekuznetsov139 ", "comments": ["@gbaned, gentle ping.\r\n\r\nthis PR seems to have gotten stuck in the merge pipeline", "Seems auto-merge is not happening but the changes are committed so we can close this. Thank you for the PR."]}, {"number": 38952, "title": "Fix doc for tf.debugging.enable_check_numerics", "body": "This make sample code agrees with the explanation.", "comments": []}, {"number": 38951, "title": "hlo_algorithm_blacklist.cc compilation fails", "body": "\r\n**System information**\r\n- OS Platform and Distribution: Windows 10 (10.0.18363 N/A Build 18363)\r\n- Mobile device if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: r.2.1\r\n- Python version: 3.6.10\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): VS2019\r\n- CUDA/cuDNN version: 10.1, 7.6.5\r\n- GPU model and memory: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n\r\n**Describe the problem**\r\n\r\nI git cloned the tensorflow, followed the build instructions for a windows build...\r\nthe output from the configure script is:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"C:/Users/Eric/anaconda3/envs/compileTF/python.exe\"\r\nbuild --action_env PYTHON_LIB_PATH=\"C:/Users/Eric/anaconda3/envs/compileTF/lib/site-packages\"\r\nbuild --python_path=\"C:/Users/Eric/anaconda3/envs/compileTF/python.exe\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --config=xla\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --config=cuda\r\nbuild:opt --copt=/arch:AVX\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --config monolithic\r\nbuild --copt=-w --host_copt=-w\r\nbuild --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=-D_USE_MATH_DEFINES\r\nbuild --verbose_failures\r\nbuild --distinct_host_configuration=false\r\nbuild --define=override_eigen_strong_inline=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-no_windows,-gpu\r\ntest --build_tag_filters=-no_windows,-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\nthen I started the compilation with: \r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nthe error message is the following:\r\n```\r\ncl : Command line warning D9002 : ignoring unknown option '-std=c++14'\r\ntensorflow/compiler/xla/service/gpu/hlo_algorithm_blacklist.cc(28): error C2131: expression did not evaluate to a constant\r\nexternal/com_google_absl\\absl/strings/string_view.h(186): note: a non-constant (sub-)expression was encountered\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\n**Any other info / logs**\r\nthe code surrounding the error:\r\n```c++\r\n// MSVC requires the extra const. Without, it reports an\r\n// \"error C2131: expression did not evaluate to a constant\".\r\nconstexpr const absl::string_view kDefaultBlacklist = R\"pb(\r\n)pb\";\r\n```\r\nappears to indicate that such issue was already encounter before.\r\n\r\nThis issue appears in the context of trying to resolve issue #38867 .\r\nThis issue appears very close to issue #35796 .\r\n\r\n", "comments": ["@ravikyram and @angerson , \r\n\r\nI am not a specialist in c++ however as I was stuck I changed the line with the error from:\r\n\r\n```c++\r\nconstexpr const absl::string_view kDefaultBlacklist = R\"pb(\r\n)pb\";\r\n\r\n```\r\nto \r\n```c++\r\nconstexpr absl::string_view kDefaultBlacklist = absl::string_view((const char *)\"\\n\", (size_t) 1);\r\n```\r\n\r\nand was able to successfully compile. I am not sure that this is the right way of doing it though.\r\n", "@ericqu,\r\nThank you for the update.\r\n\r\nClosing this issue as it is resolved. Please feel free to re-open if necessary.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38951\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38951\">No</a>\n"]}, {"number": 38950, "title": "validation_data documentation entry incomplete", "body": "## URL(s) with the issue: https://www.tensorflow.org/api_docs/python/tf/keras/Model?authuser=1#fit\r\n\r\n## Description of issue (what needs changing):\r\nvalidation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split. validation_data could be:\r\n\r\nThis is incomplete. It can also be \r\nA generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample weights). A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below.\r\n\r\n### Correct links\r\n\r\nI'm failing to find the respective location in https://github.com/tensorflow/docs of this page to make a pull request directly\r\n\r\n\r\n", "comments": ["Hey @WurmD if you want to change the documentation then you need to make a pull request by changing the docstrings of the source file, the file for the above issue is https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/engine/training.py#L81-L2865 change what you think should be there and make a pull request. ", "The documentation has been updated. Thanks!", "was it? the pull request wasn't merged https://github.com/tensorflow/tensorflow/pull/38979/files", "The documentation for `model.fit` `validation_data` argument is now updated with latest [tf-nightly api docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#fit). Thanks!"]}, {"number": 38949, "title": "TF 2.x release for CPU only", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe current TF 2.x install size is very large and is not suitable for small size deployments due to many restrictions. A CPU only release would be very helpful in this case due to its smaller size than the GPU or combined release. \r\n\r\nA CPU only release will provide more flexibility to the user in terms of deployment options. \r\n\r\n\r\n", "comments": ["There are CPU-only version from some platforms already. Please check https://github.com/tensorflow/tensorflow/blob/master/README.md", "@jetjodh \r\nCould you please update as per above comment", "Yes, I tried the link provided by @freedomtan and it worked. But this information about the CPU version is not mentioned here https://www.tensorflow.org/install/pip. That is why I got confused on how to get a CPU only version. Thank you."]}, {"number": 38948, "title": "Tensor Manipulation and Slicing Feature supported by Auto Gradient", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):2.1.0\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**Yes, tf.Variable support slicing but assigning to it , it hinders the auto grad.\r\n\r\n**Who will benefit with this feature?**Every One Who want to do more custom work and want more control over tensors\r\n\r\n**Any Other info.**\r\n", "comments": ["@hksrathore \r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!"]}, {"number": 38947, "title": "WARNING:tensorflow:AutoGraph could not transform <function affine at 0x0000021B750C1E58> and will run it as-is.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version: 2.2.0-rc3\r\n- TensorFlow-probability version: 0.10.0-rc0\r\n- Python version: 3.7.6\r\n\r\n**Describe the current behavior**\r\nI am using Anaconda, TensorFlow, and Spyder. When I run my script, I get the following error message:\r\n```\r\nINFO:tensorflow:Converted call: <function affine at 0x0000021B778A51F8>\r\n    args: (<tf.Tensor 'x:0' shape=(6768,) dtype=float32>, <tf.Tensor 'kernel_diag:0' shape=(1,) dtype=float32>, <tf.Tensor 'bias:0' shape=() dtype=float32>)\r\n    kwargs: {}\r\n\r\nConverted call: <function affine at 0x0000021B778A51F8>\r\n    args: (<tf.Tensor 'x:0' shape=(6768,) dtype=float32>, <tf.Tensor 'kernel_diag:0' shape=(1,) dtype=float32>, <tf.Tensor 'bias:0' shape=() dtype=float32>)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Cache hit for entity <function affine at 0x0000021B778A51F8> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x0000021B790D2188>, frozenset()): _ConvertedEntityFactoryInfo(tf__affine in tmpta8nc9zk)\r\nCache hit for entity <function affine at 0x0000021B778A51F8> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x0000021B790D2188>, frozenset()): _ConvertedEntityFactoryInfo(tf__affine in tmpta8nc9zk)\r\nINFO:tensorflow:Error transforming entity <function affine at 0x0000021B778A51F8>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\jason\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 538, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"C:\\Users\\jason\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\conversion.py\", line 362, in convert\r\n    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)\r\n  File \"C:\\Users\\jason\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\conversion.py\", line 300, in _instantiate\r\n    factory = converted_entity_info.get_factory()\r\n  File \"C:\\Users\\jason\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\conversion.py\", line 94, in get_factory\r\n    assert self.module_name in sys.modules\r\nAssertionError\r\nError transforming entity <function affine at 0x0000021B778A51F8>\r\nWARNING:tensorflow:AutoGraph could not transform <function affine at 0x0000021B778A51F8> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: \r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function affine at 0x0000021B778A51F8> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: \r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nINFO:tensorflow:Converted call: <function affine at 0x0000021B778A51F8>\r\n    args: (<tf.Tensor 'x:0' shape=(6768,) dtype=float32>, <tf.Tensor 'kernel_diag:0' shape=(6768,) dtype=float32>, <tf.Tensor 'bias:0' shape=(6768,) dtype=float32>)\r\n    kwargs: {}\r\n\r\nConverted call: <function affine at 0x0000021B778A51F8>\r\n    args: (<tf.Tensor 'x:0' shape=(6768,) dtype=float32>, <tf.Tensor 'kernel_diag:0' shape=(6768,) dtype=float32>, <tf.Tensor 'bias:0' shape=(6768,) dtype=float32>)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Whitelisted <function affine at 0x0000021B778A51F8>: from cache\r\nWhitelisted <function affine at 0x0000021B778A51F8>: from cache\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\jason\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 538, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"C:\\Users\\jason\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\conversion.py\", line 362, in convert\r\n    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)\r\n  File \"C:\\Users\\jason\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\conversion.py\", line 300, in _instantiate\r\n    factory = converted_entity_info.get_factory()\r\n  File \"C:\\Users\\jason\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\conversion.py\", line 94, in get_factory\r\n    assert self.module_name in sys.modules\r\nAssertionError\r\n```\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n# tf.autograph.set_verbosity(10,True)\r\nimport tensorflow_probability as tfp\r\nfrom tensorflow_probability import distributions as tfd\r\n# Read the data\r\ndf = pd.read_csv(\"https://raw.githubusercontent.com/jfhawkin/bayes_discrete_choice/master/swissmetro.dat\",sep='\\t')\r\n\r\n\"\"\"Basic data and model setup\"\"\"\r\n\r\n# Removing some observations can be done directly using pandas.\r\nremove = (((df.PURPOSE != 1) & (df.PURPOSE != 3)) | (df.CHOICE == 0))\r\ndf.drop(df[remove].index,inplace=True)\r\n\r\n# Definition of new variables: as tensorflow vectors\r\ndf.reset_index(inplace=True)\r\nIDX = tf.expand_dims(tf.convert_to_tensor(df.index),1)\r\nTRAIN_TT_SCALED = tf.expand_dims(tf.convert_to_tensor(df.TRAIN_TT / 100, dtype=tf.float32),1)\r\nTRAIN_COST_SCALED = tf.expand_dims(tf.convert_to_tensor(df.TRAIN_CO / 100, dtype=tf.float32),1)\r\nSM_TT_SCALED = tf.expand_dims(tf.convert_to_tensor(df.SM_TT / 100, dtype=tf.float32),1)\r\nSM_COST_SCALED = tf.expand_dims(tf.convert_to_tensor(df.SM_CO / 100, dtype=tf.float32),1)\r\nCAR_TT_SCALED = tf.expand_dims(tf.convert_to_tensor(df.CAR_TT / 100, dtype=tf.float32),1)\r\nCAR_CO_SCALED = tf.expand_dims(tf.convert_to_tensor(df.CAR_CO / 100, dtype=tf.float32),1)\r\n\r\nDATA = tf.concat([TRAIN_TT_SCALED,TRAIN_COST_SCALED,SM_TT_SCALED,SM_COST_SCALED,CAR_TT_SCALED,CAR_CO_SCALED], axis=1)\r\n\r\n# Definition of new variables for availability, etc.: as tensorflow vectors\r\nCHOICE = tf.convert_to_tensor(pd.get_dummies(df.CHOICE), dtype=tf.float32)\r\nCAR_AV_SP =  tf.expand_dims(tf.convert_to_tensor(df.CAR_AV, dtype=tf.float32),1)\r\nTRAIN_AV_SP =  tf.expand_dims(tf.convert_to_tensor(df.TRAIN_AV, dtype=tf.float32),1)\r\nSM_AV_SP =  tf.expand_dims(tf.convert_to_tensor(df.SM_AV, dtype=tf.float32),1)\r\n\r\nAV = tf.concat([TRAIN_AV_SP, SM_AV_SP, CAR_AV_SP], axis=1)\r\n\r\nnum_idx = df.shape[0]\r\n\r\n\"\"\"Define the model as a partial pooling hierarchical model with varying slope for the time parameter\"\"\"\r\n\r\n@tf.function\r\ndef affine(x, kernel_diag, bias=tf.zeros([])):\r\n  \"\"\"`kernel_diag * x + bias` with broadcasting.\"\"\"\r\n  kernel_diag = tf.ones_like(x) * kernel_diag\r\n  bias = tf.ones_like(x) * bias\r\n  return x * kernel_diag + bias\r\n\r\ndef mmnl_func():\r\n    adj_AV_train = (tf.ones(num_idx) - AV[:,0]) * -9999\r\n    adj_AV_SM = (tf.ones(num_idx) - AV[:,1]) * -9999\r\n    adj_AV_car = (tf.ones(num_idx) - AV[:,2]) * -9999\r\n\r\n    return tfd.JointDistributionSequential([\r\n        tfd.Normal(loc=0., scale=1e5),  # mu_b_time\r\n        tfd.HalfCauchy(loc=0., scale=5),  # sigma_b_time\r\n        lambda sigma_b_time,mu_b_time: tfd.MultivariateNormalDiag(  # b_time\r\n        loc=affine(tf.ones([num_idx]), mu_b_time[..., tf.newaxis]),\r\n        scale_identity_multiplier=sigma_b_time),\r\n        tfd.Normal(loc=0, scale=1e5), # a_train\r\n        tfd.Normal(loc=0, scale=1e5), # a_car\r\n        tfd.Normal(loc=0, scale=1e5), # b_cost\r\n        lambda b_cost,a_car,a_train,b_time: tfd.Independent(tfd.Multinomial(\r\n          total_count=1,\r\n          logits=tf.stack([\r\n              affine(DATA[:,0], tf.gather(b_time, IDX[:,0], axis=-1), (a_train + b_cost * DATA[:,1] + adj_AV_train)),\r\n              affine(DATA[:,2], tf.gather(b_time, IDX[:,0], axis=-1), (b_cost * DATA[:,3] + adj_AV_SM)),\r\n              affine(DATA[:,4], tf.gather(b_time, IDX[:,0], axis=-1), (a_car + b_cost * DATA[:,5] + adj_AV_car))\r\n          ], axis=1)\r\n        ),reinterpreted_batch_ndims=1)\r\n    ])\r\n\r\nmmnl_func().sample()\r\n```\r\n\r\nFull code on colab:\r\nhttps://drive.google.com/file/d/10OBeDVNC4tN4c8Ro2SuYhw5YfGzpXnx4/view?usp=sharing\r\n\r\n**Other info / logs**\r\nFull tracebook is attached. Full code implements a sampler. Final error from full code is:\r\n`ValueError: Dimensions must be equal, but are 6768 and 3 for '{{node JointDistributionSequential_1/log_prob/JointDistributionSequential_1_log_prob_IndependentJointDistributionSequential_1_log_prob_Multinomial/log_prob/JointDistributionSequential_1_log_prob_Multinomial/log_prob/mul}} = Mul[T=DT_FLOAT](value, JointDistributionSequential_1/log_prob/JointDistributionSequential_1_log_prob_IndependentJointDistributionSequential_1_log_prob_Multinomial/log_prob/JointDistributionSequential_1_log_prob_Multinomial/log_prob/LogSoftmax)' with input shapes: [6768,3], [1,3,6768].`\r\n[output.txt](https://github.com/tensorflow/tensorflow/files/4540276/output.txt)\r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/8e8e4748b3664326e80821a4691db4ab/38947-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/cccb69088135ccf2469cec53108eb1ab/38947-tf-nightly.ipynb#scrollTo=5LR8sgPoHgGS). Please find the attached gist. Thanks!", "@jfhawkin,\r\nCould you please check [this StackOverflow comment](https://stackoverflow.com/a/44956689) from a similar issue and let us know if it helps. Thanks!", "@amahendrakar,\n\nI looked at the StackOverflow comment. They are building up neural network\nlayers from the data, so each of l2 and l3 builds on l1 (which is based on\nthe input data).\n\nIn my case, I have three independent calls to affine() based on the input\ndata. I ran the code through the debugger and it looks to me like on each\ncall the inputs (x, kernel_diag, and bias) are being passed as\npython.framework.ops.Tensor. It then gives me the warning \"AutoGraph could\nnot transform...\". I can then run the code in the function through the\ndebugger and the return type is also a tensor. When I run my full code,\nthis warning appears to lead to an AssertionError and the ultimate\nValueError with dimension mismatch.\n\nOn Tue, Apr 28, 2020 at 9:22 AM amahendrakar <notifications@github.com>\nwrote:\n\n> @jfhawkin <https://github.com/jfhawkin>,\n> Could you please check this StackOverflow comment\n> <https://stackoverflow.com/a/44956689> from a similar issue and let us\n> know if it helps. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38947#issuecomment-620604288>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AB6LW2O4E2FX4BTJ5Z6DDPLRO3KCBANCNFSM4MR7THFA>\n> .\n>\n", "I found a few responses to similar issues and the solution was downgrading gast to 0.2.2. However, this appears to be changed in recent updates to tensorflow. Many people were getting: `AssertionError: Bad argument number for Name: 3, expecting 4` with gast 0.3.X and I get the opposite when downgrading from 0.3.3 to 0.2.2 (i.e., `AssertionError: Bad argument number for Name: 4, expecting 3`).", "This looks like the same issue as #38691 - it's a known issue with Spyder.\r\n\r\n@jfhawkin could you try again with tf-nightly (leave the gast version that it installs - it should be 0.3.3).", "I reran in Spyder and the prompt using tf-nightly. I get a similar set of warnings/errors. It now gives three potential reasons: (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. The first case doesn't apply and I tried the second by setting experimental_relax_shapes=True (no change to warnings). This suggests to me that it is the third case but, as I said before, it looks to me like I am passing tensors (could be wrong). I get the same result in colab.", "It appears that the warning I listed in the issue header may be solved. I ran it with tf.autograph.set_verbosity(3,True) and it says it transformed affine(). The first warning to appear in the log is: `WARNING:tensorflow:5 out of the last 9 calls to <function affine at 0x0000017AA8F08C18> triggered tf.function retracing.` but this appears to also be an issue with passing python rather than tensorflow objects.", "Thank you. Yes, that is a different warning emitted by tf.function. https://github.com/tensorflow/tensorflow/commit/2eb9e7c2b68123002acab835711da90d35bebca3 should remove those warnings once it's included in tf-nightly. But if those continue to appear afterwards, please ping this thread or open up a new issue and we'll have a closer look."]}, {"number": 38946, "title": "Docs:  multi_worker_with_keras.ipynb shows init() instead of __init__()", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://github.com/tensorflow/docs/edit/master/site/en/tutorials/distribute/multi_worker_with_keras.ipynb\r\n\r\n## Description of issue (what needs changing):\r\nWhen rendered, the literal \\_\\_init\\_\\_ is replaced with init\r\n### Clear description\r\n\\_\\_init\\_\\_ is a \"built-in\" python function for classes.  In the ipynb source code it is correct.  However, when rendered at: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras , it is incorrect.\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\nYes, please see the notebook in the docs.  The text is:\r\n```\r\nNote: TF_CONFIG is parsed and TensorFlow's GRPC servers are started at the time MultiWorkerMirroredStrategy.init() is called, so TF_CONFIG environment variable must be set before a tf.distribute.Strategy instance is created.\r\n```\r\nvs\r\n```\r\nNote: TF_CONFIG is parsed and TensorFlow's GRPC servers are started at the time MultiWorkerMirroredStrategy.__init__() is called, so TF_CONFIG environment variable must be set before a tf.distribute.Strategy instance is created.\r\n```\r\nNotice that .init() is shown when .\\_\\_init\\_\\_() should be shown.\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\nNo:  I didn't know how to escape the underscores, but I believe it is as in this report using a backslash before each underscore.\r\n", "comments": ["I'm new here, so potentially dumb question/observation....When I forked the repo and run the multi_worker_with_keras notebook I see the output to be fixed. Should this issue be marked as closed?", "Closing as this fixed and PR has merged. Thanks!"]}, {"number": 38945, "title": "Memory leak on TF2.1 model.fit with validation_split", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.5\r\n- GPU model and memory: No GPU\r\n\r\n**Describe the current behavior**\r\nFitting a simple LSTM model causes memory leak\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport os\r\nimport psutil\r\n\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import (\r\n    LSTM,\r\n    Bidirectional,\r\n    Dense,\r\n    Input,\r\n)\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\n\r\ndef dummy_model(n_classes, n_features, seq_length):\r\n    input = Input(shape=(seq_length, n_features))\r\n    main = Bidirectional(LSTM(128, return_sequences=False))(input)\r\n\r\n    prediction = Dense(n_classes, activation=\"softmax\")(main)\r\n    optimiser = Adam(lr=1e-3)\r\n    model = Model(inputs=[input], outputs=prediction)\r\n    model.compile(optimiser, \"categorical_crossentropy\", metrics=[\"accuracy\"])\r\n    return model\r\n\r\n\r\ndef fit_model():\r\n    x_train = np.random.random_sample((1000, 50, 100))\r\n    x_train = x_train.astype(np.float32)\r\n    y_train = np.zeros((1000, 10), dtype=np.float32)\r\n    model = dummy_model(n_classes=10, n_features=100, seq_length=50)\r\n    model.fit(\r\n        x_train, y_train, epochs=1,\r\n        validation_split=0.1, batch_size=64,\r\n        verbose=0\r\n    )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    process = psutil.Process(os.getpid())\r\n    n = 20\r\n\r\n    for i in range(n):\r\n        fit_model()\r\n        print(f\"#--- Run {i + 1} of {n} memory used (MB): {process.memory_info().rss / 1e6}\")\r\n```\r\n\r\n\r\n\r\n**Other info / logs** \r\n```\r\n#--- Run 1 of 20 memory used (MB): 731.81184\r\n#--- Run 2 of 20 memory used (MB): 991.137792\r\n#--- Run 3 of 20 memory used (MB): 1027.985408\r\n#--- Run 4 of 20 memory used (MB): 1089.724416\r\n#--- Run 5 of 20 memory used (MB): 1127.616512\r\n#--- Run 6 of 20 memory used (MB): 1165.471744\r\n#--- Run 7 of 20 memory used (MB): 1211.96544\r\n#--- Run 8 of 20 memory used (MB): 1247.653888\r\n#--- Run 9 of 20 memory used (MB): 1272.410112\r\n#--- Run 10 of 20 memory used (MB): 1289.478144\r\n#--- Run 11 of 20 memory used (MB): 1298.57536\r\n#--- Run 12 of 20 memory used (MB): 1317.429248\r\n#--- Run 13 of 20 memory used (MB): 1346.781184\r\n#--- Run 14 of 20 memory used (MB): 1369.145344\r\n#--- Run 15 of 20 memory used (MB): 1402.55232\r\n#--- Run 16 of 20 memory used (MB): 1409.634304\r\n#--- Run 17 of 20 memory used (MB): 1413.599232\r\n#--- Run 18 of 20 memory used (MB): 1419.091968\r\n#--- Run 19 of 20 memory used (MB): 1450.14784\r\n#--- Run 20 of 20 memory used (MB): 1468.604416\r\n```\r\n\r\nIssue also appears to occur with `2.2.0rc3`\r\n\r\nExplicitly setting the following (as was recommended in this blog post - http://gregoryzynda.com/python/tensorflow/memory/leak/rnn/lstm/2019/10/17/lstm-memory-leak.html) appears to help mitigate this (although memory still increases):\r\n```\r\ntf.config.threading.set_intra_op_parallelism_threads(2)\r\ntf.config.threading.set_inter_op_parallelism_threads(5)\r\n```\r\n\r\nAs does explicitly running garbage collection `gc.collect()` after each iteration.", "comments": ["I was able to reproduce the issue in colab with TF 2.1.0 , 2.2.0rc3 . Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/b1726888ddd44b18ceb0174eeafd9779/untitled833.ipynb).Thanks!", "Sorry, is it related with this? or not?\r\nhttps://github.com/tensorflow/tensorflow/issues/38617\r\n", "@alimhanif I think #38617 is a different issue (that is specific to 2.2rc3 and related to distributed gpu training). \r\n\r\n", "Thanks for reporting the issue. I did following experiments with latest tf-nightly 2.2.0.dev20200503, and it seems that it might be a issue for TF runtime.\r\n\r\n1. I change to create model and data only once (move them from fit_model to be above the for loop before training), and the increasing of the memory usage still occurs.\r\n\r\n2. I change the model structure to remove the LSTM and Bidirectional wrapper, and only use the Dense layer, and increasing of the memory usage still occurs.\r\n\r\nWith this observation, I will send this to someone of TF core team for further investigation.", "Did one more test, it seems that the memory increase was greatly reduced with validation_split is not used. This indicates that keras model might have some leak when creating the validation dataset under the hood.", "Thanks @qlzh727. To confirm I can [reproduce](https://colab.research.google.com/drive/1s76eoH9lE21BiUFOESJVX2XeT1ACLCoy#scrollTo=_xyylnfl2W6G) your results with TF `2.2.0rc3`, creating the data outside of the loop and not using validation_split.\r\n\r\n", "Could you try calling `tf.keras.backend.clear_session()` on every iteration?", "@kkimdev Calling `tf.keras.backend.clear_session()` after each iteration does not appear to help", "@qlzh727 I just tried this again with TF 2.2.0 on Ubuntu 18.04.3. I use the same script as in the original post but create the training/validation data once outside of the loop and use this instead of validation_split (same as the final cell of this [notebook](https://colab.research.google.com/drive/1s76eoH9lE21BiUFOESJVX2XeT1ACLCoy#scrollTo=_Blz6Tj01MTO)).  Memory usage still seems to increase.\r\n\r\nResults from running on Ubuntu 18.04.3\r\n```\r\n #--- Run 1 of 20 memory used (MB): 578.285568\r\n#--- Run 2 of 20 memory used (MB): 603.820032\r\n#--- Run 3 of 20 memory used (MB): 636.522496\r\n#--- Run 4 of 20 memory used (MB): 648.904704\r\n#--- Run 5 of 20 memory used (MB): 659.59936\r\n#--- Run 6 of 20 memory used (MB): 669.63456\r\n#--- Run 7 of 20 memory used (MB): 674.701312\r\n#--- Run 8 of 20 memory used (MB): 685.817856\r\n#--- Run 9 of 20 memory used (MB): 692.137984\r\n#--- Run 10 of 20 memory used (MB): 694.984704\r\n#--- Run 11 of 20 memory used (MB): 706.154496\r\n#--- Run 12 of 20 memory used (MB): 709.722112\r\n#--- Run 13 of 20 memory used (MB): 714.248192\r\n#--- Run 14 of 20 memory used (MB): 717.27104\r\n#--- Run 15 of 20 memory used (MB): 718.78656\r\n#--- Run 16 of 20 memory used (MB): 721.498112\r\n#--- Run 17 of 20 memory used (MB): 723.771392\r\n#--- Run 18 of 20 memory used (MB): 724.639744\r\n#--- Run 19 of 20 memory used (MB): 727.764992\r\n#--- Run 20 of 20 memory used (MB): 729.41568\r\n```", "@dannyfriar. Thanks for the update. \r\n\r\nI updated your code to also create the model outside of the for loop, which is the more standard behavior. With that there isn't much memory leak.\r\n\r\nHaving said that, the memory leak still appears when validation_split is used. I will focus on that.\r\n\r\n/Users/scottzhu/tf-2.2/bin/python /Users/scottzhu/Library/Preferences/PyCharmCE2018.1/scratches/scratch_17.py\r\n2020-06-01 20:50:38.363168: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-01 20:50:38.385048: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x13b428490 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-01 20:50:38.385063: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n#--- Run 1 of 20 memory used (MB): 424.79616\r\n#--- Run 2 of 20 memory used (MB): 431.009792\r\n#--- Run 3 of 20 memory used (MB): 433.84832\r\n#--- Run 4 of 20 memory used (MB): 434.60608\r\n#--- Run 5 of 20 memory used (MB): 435.093504\r\n#--- Run 6 of 20 memory used (MB): 436.195328\r\n#--- Run 7 of 20 memory used (MB): 437.420032\r\n#--- Run 8 of 20 memory used (MB): 437.837824\r\n#--- Run 9 of 20 memory used (MB): 438.714368\r\n#--- Run 10 of 20 memory used (MB): 439.410688\r\n#--- Run 11 of 20 memory used (MB): 440.037376\r\n#--- Run 12 of 20 memory used (MB): 440.000512\r\n#--- Run 13 of 20 memory used (MB): 440.000512\r\n#--- Run 14 of 20 memory used (MB): 439.865344\r\n#--- Run 15 of 20 memory used (MB): 440.037376\r\n#--- Run 16 of 20 memory used (MB): 441.430016\r\n#--- Run 17 of 20 memory used (MB): 441.430016\r\n#--- Run 18 of 20 memory used (MB): 441.28256\r\n#--- Run 19 of 20 memory used (MB): 441.704448\r\n#--- Run 20 of 20 memory used (MB): 441.704448\r\n\r\nProcess finished with exit code 0\r\n\r\n", "With fix in https://github.com/tensorflow/tensorflow/commit/ce2f9824eee904d60fdc0444ac8a82b217ea9149, the memory usage for validation_split is reduced.\r\n\r\nPlease note that I update the snippet to defined the model only once, and fit within the for loop. This is more aligned to normal workflow by user.\r\n\r\nIf you need to define multiple model in a loop, you might want to use keras.backend.clear_session() to remove unused the models in the keras global graph.", "Memory log before change:\r\n#--- Run 1 of 20 memory used (MB): 420.94592\r\n#--- Run 2 of 20 memory used (MB): 455.458816\r\n#--- Run 3 of 20 memory used (MB): 480.89088\r\n#--- Run 4 of 20 memory used (MB): 504.799232\r\n#--- Run 5 of 20 memory used (MB): 465.563648\r\n#--- Run 6 of 20 memory used (MB): 485.797888\r\n#--- Run 7 of 20 memory used (MB): 506.544128\r\n#--- Run 8 of 20 memory used (MB): 526.76608\r\n#--- Run 9 of 20 memory used (MB): 547.782656\r\n#--- Run 10 of 20 memory used (MB): 487.981056\r\n#--- Run 11 of 20 memory used (MB): 508.862464\r\n#--- Run 12 of 20 memory used (MB): 528.904192\r\n#--- Run 13 of 20 memory used (MB): 549.933056\r\n#--- Run 14 of 20 memory used (MB): 570.032128\r\n#--- Run 15 of 20 memory used (MB): 510.455808\r\n#--- Run 16 of 20 memory used (MB): 530.501632\r\n#--- Run 17 of 20 memory used (MB): 551.559168\r\n#--- Run 18 of 20 memory used (MB): 571.408384\r\n#--- Run 19 of 20 memory used (MB): 529.518592\r\n#--- Run 20 of 20 memory used (MB): 549.376\r\n\r\nMemory log after change:\r\n#--- Run 1 of 20 memory used (MB): 441.933824\r\n#--- Run 2 of 20 memory used (MB): 463.753216\r\n#--- Run 3 of 20 memory used (MB): 465.801216\r\n#--- Run 4 of 20 memory used (MB): 466.366464\r\n#--- Run 5 of 20 memory used (MB): 467.0464\r\n#--- Run 6 of 20 memory used (MB): 467.709952\r\n#--- Run 7 of 20 memory used (MB): 468.668416\r\n#--- Run 8 of 20 memory used (MB): 468.62336\r\n#--- Run 9 of 20 memory used (MB): 474.35776\r\n#--- Run 10 of 20 memory used (MB): 474.353664\r\n#--- Run 11 of 20 memory used (MB): 474.472448\r\n#--- Run 12 of 20 memory used (MB): 474.648576\r\n#--- Run 13 of 20 memory used (MB): 474.697728\r\n#--- Run 14 of 20 memory used (MB): 474.750976\r\n#--- Run 15 of 20 memory used (MB): 474.804224\r\n#--- Run 16 of 20 memory used (MB): 474.800128\r\n#--- Run 17 of 20 memory used (MB): 474.857472\r\n#--- Run 18 of 20 memory used (MB): 474.918912\r\n#--- Run 19 of 20 memory used (MB): 475.086848\r\n#--- Run 20 of 20 memory used (MB): 475.348992"]}, {"number": 38944, "title": "UnrecognizedFlagError when using tf.vectorized_map() with lbfgs_minimize()", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2\r\n- Mobile device (e.g. iPhone UnrecognizedFlagErrortf8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.1.0   (v2.1.0-rc2-17-ge5bf8de)\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n**Describe the current behavior**\r\nwhen trying to access the `tfp.optimizer.lbfgs_minimize()` - object parameters during a `tf.vectorized_map()` operation, it throws the error: `UnrecognizedFlagError: Unknown command line flag 'f'` as well as `ERROR:tensorflow:Got error while pfor was converting op name: \"loop_body/PartitionedCall\"`\r\n\r\n\r\n**Describe the expected behavior**\r\nyou can simply extract the position of this object during the loop\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nthe issue was originally created with following code on my laptop (with above mentioned versions) but can also be reproduced in this colab: \r\nhttps://colab.research.google.com/drive/1gOF9qTqzdjYmlbbugIYxzXlH7MpBsEbl\r\n\r\n\r\n**Other info / logs** \r\nthe large error traceback file is appended\r\n\r\n[tf_vectorized_map_traceback.txt](https://github.com/tensorflow/tensorflow/files/4540178/tf_vectorized_map_traceback.txt)\r\n\r\n", "comments": ["@loipf,\r\nI was able to reproduce the error with [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/5007574fec22eea48ebad42bd700761f/38944-2-2.ipynb#scrollTo=wL6gGpxhkeA0). However, the issue seems to be fixed with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/b59a88f5e0254e2cf2099675385ccccc/38944-tf-nightly.ipynb) as I was able to run the code without any errors. Please find the attached gist. Thanks!", "great! I can confirm that the tf-nighly build in combination with tfp-nightly fixes this problem\r\nthanks a lot. this can be closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38944\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38944\">No</a>\n"]}, {"number": 38942, "title": "Impossible to create a simple LSTM layer with tensorflow > 2.1 version", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nTrying to follow these instructions : https://www.tensorflow.org/guide/keras/rnn\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tested on tf-nightly 2.2.0.dev20200422, tensorflow 2.2.0rc3, tensorflow 2.2.0rc2, tensorflow 2.2.0rc1, tensorflow 2.2.0rc0\r\n- Python version: 3.7\r\n\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:  10.1\r\n- GPU model and memory: \r\n\r\nThe following instructions is working with tensorflow 2.1 but do not work on tensorflow version > 2.1 in my environment. \r\n\r\n```\r\nimport collections\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(layers.Embedding(input_dim=1000, output_dim=64))\r\nmodel.add(layers.LSTM(128))\r\n```\r\n\r\nI get the following error in building LSTM layer:\r\n\r\n`ValueError: Shape must be at least rank 3 but is rank 2 for '{{node BiasAdd}} = BiasAdd[T=DT_FLOAT, data_format=\"NCHW\"](add, bias)' with input shapes: [?,512], [512].`\r\n\r\nThis is working correctly with tensorflow 2.1 and in google colab: https://colab.research.google.com/drive/1evEbvAtAElZqx3cqD4wCPJxC5OUmJeSp). \r\n\r\nIt seems that tensorflow is using cached parameters (512 might be a batch size used in previous tests...)", "comments": ["@q-55555 \r\n\r\nI have tried in colab with TF 2.1.0 and 2.2.0-rc3 and i am not seeing any issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/c3b61bdf9c6afbc099162ea820eef91a/untitled832.ipynb).Thanks!", "Thank you @ravikyram  \r\n\r\nI have found the problem : for an unknown reason, in my environment %USERPROFILE%/.keras/keras.json had \"image_data_format\": \"channels_first\"... Changing to \"image_data_format\": \"channels_last\" fixed the problem.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38942\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38942\">No</a>\n", "Although the issue was resolved, the bug mentioned here still persists: it is impossible to instantiate a Keras LSTM layer if the data_format was set to channels_first.\r\n\r\nAre there plans to fix this regression?"]}, {"number": 38941, "title": "how to dynamic link cuda library when building tensorflow?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nsource\r\nTensorFlow version:\r\nr1.14\r\nPython version:\r\n3.6.9\r\nInstalled using virtualenv? pip? conda?:\r\nbuild on raw linux environment, with pip installed\r\nBazel version (if compiling from source):\r\n0.24.1\r\nGCC/Compiler version (if compiling from source):\r\n4.8.5\r\nCUDA/cuDNN version:\r\nCUDA 10.0 CUDNN 7.6.5\r\nGPU model and memory:\r\nNvidia GTX 1660Ti 6GB\r\n\r\nDescribe the problem\r\ni want to build tensorflow  with gpu support, and it should dynamically link the cuda libraries(libcudart.so and so on). \r\nOn windows, I tried bazel build --config=opt --config=cuda --copt=-nvcc_options=cudart=shared //tensorflow/tools/pip_package:libtensorflow. And I used dependency walker on the tensorflow.dll it built, it had cuda related dll in its dependencies, so it seemed work.\r\nbut on ubuntu,i tried with the same command line, but get error: gcc un recognized command line\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --config=opt --config=cuda --copt=-nvcc_options=cudart=shared //tensorflow/tools/pip_package:libtensorflow\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n/root/.cache/bazel/_bazel_root/5ac94cf1f70fd575be3807b8a4a32ede/external/com_google_absl/absl/debugging/BUILD.bazel:190:1: C++ compilation of rule '@com_google_absl//absl/debugging:leak_check' failed (Exit 1)\r\ngcc: error: unrecognized command line option '-nvcc_options=cudart=shared'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 152.342s, Critical Path: 48.24s\r\nINFO: 1236 processes: 1236 local.\r\nFAILED: Build did NOT complete successfully", "comments": ["dynamic links to cuda library when building tensorflow on linux can be done by passing --linkopt=-lcudart option to bazel, this option tells the linker to link the cuda functions from the shared library libcudart.so (static linking use libcudart_static.a, which is nvcc's default option)\r\nmy complete command line is:\r\nbazel build --congif=opt --config=cuda --linkopt=-L/usr/local/cuda/lib64 --linkopt=-lcudart //tensorflow/tools/pip_package:build_pip_package", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38941\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38941\">No</a>\n", "> dynamic links to cuda library when building tensorflow on linux can be done by passing --linkopt=-lcudart option to bazel, this option tells the linker to link the cuda functions from the shared library libcudart.so (static linking use libcudart_static.a, which is nvcc's default option)\r\n> my complete command line is:\r\n> bazel build --congif=opt --config=cuda --linkopt=-L/usr/local/cuda/lib64 --linkopt=-lcudart //tensorflow/tools/pip_package:build_pip_package\r\n\r\nthrough this method, the pip package requires libcudart.so to run, but the cuda funciton calls in tensorflow still links to cuda library statically", "hit same problem starting from 1.14 (didn't happen on 1.13.1), any update on the problem? \r\nas the built out _pywrap_tensorflow_internal.so could dpends on cudart shared lib, but meanwhile has statically linked cudart API call, e.g, check by nm -s \r\nnm -s /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so |grep cudaGet\r\n000000002e880e38 b _ZGVZ13cudaGetDeviceE8func_ptr\r\n000000002e880e98 b _ZGVZ16cudaGetLastErrorE8func_ptr\r\n000000002e880e78 b _ZGVZ18cudaGetErrorStringE8func_ptr\r\n000000002e880e68 b _ZGVZ23cudaGetDevicePropertiesE8func_ptr\r\n000000002e880e40 b _ZZ13cudaGetDeviceE8func_ptr\r\n000000002e880ea0 b _ZZ16cudaGetLastErrorE8func_ptr\r\n000000002e880e80 b _ZZ18cudaGetErrorStringE8func_ptr\r\n000000002e880e70 b _ZZ23cudaGetDevicePropertiesE8func_ptr\r\n00000000074cd860 t cudaGetDevice\r\n00000000074cd520 t cudaGetDeviceProperties\r\n00000000074cd480 t cudaGetErrorString\r\n00000000074cd320 t cudaGetLastError\r\n", "After digging into tensorflow's source code, i found this question quite meaningless. In file tensorflow/stream_executor/platform/default/dso_loader.cc, tensorflow implement a shared library loader, and in tensorflow/stream_executor/cuda/, there are a bunch of symbol declaration files such as cuda_10_0.inc, claiming every single symbol declared in cuda libraries. Tensorflow do this to avoid any kind of linking(dynamic, statically) to the actual cuda libraries during compilation. And during runtime, DsoLoader will open the shared object( using dlopen on Linux, other function on other platform)  on the system it runs on, and loads all the addresses of the symbols in that shared object into its own memory space, it's only up to this point, can Tensorflow actually call the cuda functions.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38941\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38941\">No</a>\n"]}, {"number": 38940, "title": "Update check_cuda_libs.py", "body": "Fix https://github.com/tensorflow/tensorflow/issues/38660", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38940) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38940) for more info**.\n\n<!-- ok -->"]}, {"number": 38939, "title": "Tensorflow lite Quantization aware training in Keras FAIL", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux 39df84b83a78 4.19.104+ #1 SMP Wed Feb 19 05:26:34 PST 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n- TensorFlow installed from (source or binary): \r\n2.2.0-dev20200427\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nHere's the Colab https://colab.research.google.com/drive/1H_DGK2VjIKSNhNboW_XfqHr7kzXR-rrI\r\n\r\n```\r\n! pip uninstall -y tensorflow\r\n! pip install -q tf-nightly\r\n! pip install -q tensorflow-model-optimization\r\nimport tempfile\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n# Load MNIST dataset\r\nmnist = keras.datasets.mnist\r\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n\r\n# Normalize the input image so that each pixel value is between 0 to 1.\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\n\r\n# Define the MODIFIED model architecture.\r\nmodel = keras.Sequential([\r\n  keras.layers.InputLayer(input_shape=(28, 28)),\r\n  keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n  keras.layers.Conv2D(filters=12, kernel_size=(3, 3),name='conv1'),\r\n  keras.layers.BatchNormalization(),\r\n  keras.layers.ReLU(),\r\n  keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n  keras.layers.Flatten(),\r\n  keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\n\r\n# Train the digit classification model\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(\r\n  train_images,\r\n  train_labels,\r\n  epochs=1,\r\n  validation_split=0.1,\r\n)\r\nimport tensorflow_model_optimization as tfmot\r\nquantize_model = tfmot.quantization.keras.quantize_model\r\n# q_aware stands for for quantization aware.\r\nq_aware_model = quantize_model(model)\r\n\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-5fc1d8762a1f> in <module>()\r\n      4 \r\n      5 # q_aware stands for for quantization aware.\r\n----> 6 q_aware_model = quantize_model(model)\r\n\r\n8 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py in quantize_model(to_quantize)\r\n    136 \r\n    137   annotated_model = quantize_annotate_model(to_quantize)\r\n--> 138   return quantize_apply(annotated_model)\r\n    139 \r\n    140 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py in quantize_apply(model)\r\n    401   # layer_quantize_map gets modified by the transformations.\r\n    402   transformed_model, layer_quantize_map = quantize_transform.apply(\r\n--> 403       unwrapped_model, layer_quantize_map)\r\n    404 \r\n    405   # TODO(pulkitb): Think more about how to introduce Default specific code.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_layout_transform.py in apply(self, model, layer_quantize_map)\r\n     65     return model_transformer.ModelTransformer(\r\n     66         model, transforms,\r\n---> 67         layer_quantize_map.keys(), layer_quantize_map).transform()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/graph_transformations/model_transformer.py in transform(self)\r\n    550     else:\r\n    551       transformed_model = keras.Sequential.from_config(self._config,\r\n--> 552                                                        custom_objects)\r\n    553 \r\n    554     for layer in transformed_model.layers:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in from_config(cls, config, custom_objects)\r\n    498       layer = layer_module.deserialize(layer_config,\r\n    499                                        custom_objects=custom_objects)\r\n--> 500       model.add(layer)\r\n    501     if (not model.inputs and build_input_shape and\r\n    502         isinstance(build_input_shape, (tuple, list))):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    454     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    455     try:\r\n--> 456       result = method(self, *args, **kwargs)\r\n    457     finally:\r\n    458       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in add(self, layer)\r\n    227       # If the model is being built continuously on top of an input layer:\r\n    228       # refresh its output.\r\n--> 229       output_tensor = layer(self.outputs[0])\r\n    230       if len(nest.flatten(output_tensor)) != 1:\r\n    231         raise ValueError(SINGLE_LAYER_OUTPUT_ERROR_MSG)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    892         # are casted, not before.\r\n    893         input_spec.assert_input_compatibility(self.input_spec, inputs,\r\n--> 894                                               self.name)\r\n    895         if (any(isinstance(x, ragged_tensor.RaggedTensor) for x in input_list)\r\n    896             and not self._supports_ragged_inputs):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py in assert_input_compatibility(input_spec, inputs, layer_name)\r\n    178                          'expected ndim=' + str(spec.ndim) + ', found ndim=' +\r\n    179                          str(ndim) + '. Full shape received: ' +\r\n--> 180                          str(x.shape.as_list()))\r\n    181     if spec.max_ndim is not None:\r\n    182       ndim = x.shape.ndims\r\n\r\nValueError: Input 0 of layer conv1 is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: [None, 196]\r\n```\r\n", "comments": ["i am am able to replicate this issue, please find the [gist here for nightly](https://colab.sandbox.google.com/gist/Saduf2019/94c069a63230942168fa6a1a59edd2a5/38939.ipynb) and in [2.1 the gist](https://colab.sandbox.google.com/gist/Saduf2019/374c414cbce8839e935e403f415cb5dd/untitled156.ipynb) .Thanks!", "@asicdsm Team is working on supporting BatchNormalization for TF2 Quantization Aware Training (QAT). Please stay tuned. We will update you when it is ready. The progress is monitored with another similar issue https://github.com/tensorflow/tensorflow/issues/37376. Thanks! ", "Was able to run your code successfully without any error in Tf Nightly 2.6, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/98f827ea75c5d0d84b9f92e265506020/38939.ipynb).\r\nClosing the issue since it is resolved, feel free to reopen. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38939\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38939\">No</a>\n"]}, {"number": 38938, "title": "Keras: model.fit_generator or model.fit not working properly", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0-rc3\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: (colab)\r\n- GPU model and memory: (colab)\r\n\r\n**Describe the current behavior**\r\nWhile using ```model.fit``` or ```model.fit_generator``` the sub iterations in epoch shows unknown number of iterations which is goes on despite surpassing the batch size.\r\n\r\n**Describe the expected behavior**\r\nWhile using ```model.fit``` or ```model.fit_generator``` the sub iterations in epoch must be definite.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to [Colab](https://colab.research.google.com/drive/1dWbszUUjRagXyJUUqjdsM1yXtaAmKc23).\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```shell\r\nWARNING:tensorflow:From <ipython-input-23-2da3481afcb0>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use Model.fit, which supports generators.\r\nEpoch 1/20\r\n     13/Unknown - 20s 2s/step - loss: 1.0732 - accuracy: 0.2602\r\n```", "comments": ["@sanidhyamangal \r\n\r\nI am not able to access the colab link you have provided. Please, provide access so i can try to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram, I have provided you the access, kindly check. ", "I checked the colab notebook and was able to reproduce the issue. For now, you may use tf -2.1 which does not contain this issue.", "@jetjodh, I already rolled back to previous iterations. Just reporting the bug so that it doesn\u2019t appear in 2.2.0 stable release.", "I am not seeing any issue with TF 2.1.0 .However i am able to reproduce the issue with TF 2.2.0-rc3. Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/eeee97ce61ffb4fbf653ec375fe97fe8/untitled834.ipynb).Thanks!", "@ravikyram, I have already used migrated my code from 2.2.0-rc3 to 2.1.0 and 2.0.0. Thanks for gist. Hoping to see this issue resolved in upcoming stable build.", "@sanidhyamangal thanks for reporting the issue. this issue is fixed in the latest tf-nightly and will be available with the upcoming 2.3.0 release. ", "@sanidhyamangal \r\n\r\nThis was also resolved in TF nightly version(`2.3.0-dev20200617`). Please check the gist [here.](https://colab.research.google.com/gist/ravikyram/86d88c05a705c79b7ee1df625ef7ccad/untitled44.ipynb).You could use tf-nightly for now and in the next couple of months new stable version will be released.Please verify once and close the issue. Thanks!", "@ravikyram \r\n\r\nYes, this one is resolved and I am no longer able to reproduce this issue in new version. Thanks for resolving this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38938\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38938\">No</a>\n"]}, {"number": 38937, "title": "Tensorflow TPU - ValueError: No gradients provided for any variable", "body": "`Tensorflow version 2.2.0-rc3`\r\n\r\n\r\nI went through most of git posts, SO and others but unable to get around it.\r\n\r\nHere is how model and training pattern look like...\r\n\r\n```python\r\ndef create_model():    \r\n    input_img = Input(shape=(1280, 1280, 3)) \r\n    \r\n    x = Conv2D(256, (3, 3), activation='relu', padding='same')(input_img)\r\n    x = MaxPooling2D((2, 2), padding='same')(x)\r\n    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\r\n    x = MaxPooling2D((2, 2), padding='same')(x)\r\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\r\n    x = MaxPooling2D((2, 2), padding='same')(x)\r\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\r\n    x = MaxPooling2D((2, 2), padding='same')(x)\r\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\r\n    encoded = MaxPooling2D((2, 2), padding='same')(x)\r\n    \r\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\r\n    x = UpSampling2D((2, 2))(x)\r\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\r\n    x = UpSampling2D((2, 2))(x)\r\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\r\n    x = UpSampling2D((2, 2))(x)\r\n    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\r\n    x = UpSampling2D((2, 2))(x)\r\n    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\r\n    x = UpSampling2D((2, 2))(x)\r\n    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\r\n    \r\n    autoencoder = Model(input_img, decoded)\r\n    \r\n    return autoencoder\r\n\r\n\r\n## training\r\nwith strategy.scope():\r\n    model = create_model()\r\n    model.summary()\r\n    model.compile(optimizer='adam', loss='binary_crossentropy')\r\n    print(f\"optimizer >>> {autoencoder.optimizer.get_config()}\")\r\n    \r\nmodel.fit(training_dataset, steps_per_epoch=steps_per_epoch, epochs=50, validation_data=validation_dataset)\r\nmodel.save(SAVE_WEIGHT)\r\n```\r\n\r\n\r\n\r\n**This is same code which is running without any issues on CPU as well on GCP. With this same code I have already completed more than 70 epochs but due to slowness of CPU I decided to move to TPU.** \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nHere is stack trace of error.\r\n\r\n```python\r\n\r\noptimizer >>> {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'rho': 0.95, 'epsilon': 1e-07}\r\n\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv2d (Conv2D)              (None, 1280, 1280, 256)   7168      \r\n_________________________________________________________________\r\nmax_pooling2d (MaxPooling2D) (None, 640, 640, 256)     0         \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 640, 640, 256)     590080    \r\n_________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2 (None, 320, 320, 256)     0         \r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 320, 320, 64)      147520    \r\n_________________________________________________________________\r\nmax_pooling2d_2 (MaxPooling2 (None, 160, 160, 64)      0         \r\n_________________________________________________________________\r\nconv2d_3 (Conv2D)            (None, 160, 160, 32)      18464     \r\n_________________________________________________________________\r\nmax_pooling2d_3 (MaxPooling2 (None, 80, 80, 32)        0         \r\n_________________________________________________________________\r\nconv2d_4 (Conv2D)            (None, 80, 80, 16)        4624      \r\n_________________________________________________________________\r\nmax_pooling2d_4 (MaxPooling2 (None, 40, 40, 16)        0         \r\n_________________________________________________________________\r\nconv2d_5 (Conv2D)            (None, 40, 40, 16)        2320      \r\n_________________________________________________________________\r\nup_sampling2d (UpSampling2D) (None, 80, 80, 16)        0         \r\n_________________________________________________________________\r\nconv2d_6 (Conv2D)            (None, 80, 80, 32)        4640      \r\n_________________________________________________________________\r\nup_sampling2d_1 (UpSampling2 (None, 160, 160, 32)      0         \r\n_________________________________________________________________\r\nconv2d_7 (Conv2D)            (None, 160, 160, 64)      18496     \r\n_________________________________________________________________\r\nup_sampling2d_2 (UpSampling2 (None, 320, 320, 64)      0         \r\n_________________________________________________________________\r\nconv2d_8 (Conv2D)            (None, 320, 320, 256)     147712    \r\n_________________________________________________________________\r\nup_sampling2d_3 (UpSampling2 (None, 640, 640, 256)     0         \r\n_________________________________________________________________\r\nconv2d_9 (Conv2D)            (None, 640, 640, 256)     590080    \r\n_________________________________________________________________\r\nup_sampling2d_4 (UpSampling2 (None, 1280, 1280, 256)   0         \r\n_________________________________________________________________\r\nconv2d_10 (Conv2D)           (None, 1280, 1280, 3)     6915      \r\n=================================================================\r\nTotal params: 1,538,019\r\nTrainable params: 1,538,019\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nEpoch 1/50\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-9-41a8a30839cf> in <module>()\r\n     70 \r\n     71 \r\n---> 72 autoencoder.fit(training_dataset, steps_per_epoch=STEPS_PER_EPOCH, epochs=50, validation_data=validation_dataset)\r\n     73 \r\n     74 autoencoder.save(SAVE_WEIGHT)\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    849                 batch_size=batch_size):\r\n    850               callbacks.on_train_batch_begin(step)\r\n--> 851               tmp_logs = train_function(iterator)\r\n    852               # Catch OutOfRangeError for Datasets of unknown size.\r\n    853               # This blocks until the batch has finished executing.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    578         xla_context.Exit()\r\n    579     else:\r\n--> 580       result = self._call(*args, **kwds)\r\n    581 \r\n    582     if tracing_count == self._get_tracing_count():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    625       # This is the first call of __call__, so we have to initialize.\r\n    626       initializers = []\r\n--> 627       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    628     finally:\r\n    629       # At this point we know that the initialization is complete (or less\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    504     self._concrete_stateful_fn = (\r\n    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 506             *args, **kwds))\r\n    507 \r\n    508     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2444       args, kwargs = None, None\r\n   2445     with self._lock:\r\n-> 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2447     return graph_function\r\n   2448 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2775 \r\n   2776       self._function_cache.missed.add(call_context_key)\r\n-> 2777       graph_function = self._create_graph_function(args, kwargs)\r\n   2778       self._function_cache.primary[cache_key] = graph_function\r\n   2779       return graph_function, args, kwargs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2665             arg_names=arg_names,\r\n   2666             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2667             capture_by_value=self._capture_by_value),\r\n   2668         self._function_attributes,\r\n   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    979         _, original_func = tf_decorator.unwrap(python_func)\r\n    980 \r\n--> 981       func_outputs = python_func(*func_args, **func_kwargs)\r\n    982 \r\n    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    440         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    442     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    443 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nValueError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:170 run  **\r\n        return self.extended.tpu_run(fn, args, kwargs, options)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:863 tpu_run\r\n        return func(args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:930 tpu_function\r\n        padding_spec=padding_spec)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py:893 replicate\r\n        padding_spec=padding_spec)[1]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py:1280 split_compile_and_replicate\r\n        outputs = computation(*computation_inputs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:892 replicated_fn\r\n        result[0] = fn(*replica_args, **replica_kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:541 train_step  **\r\n        self.trainable_variables)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1807 _minimize\r\n        trainable_variables))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:521 _aggregate_gradients\r\n        filtered_grads_and_vars = _filter_grads(grads_and_vars)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1219 _filter_grads\r\n        ([v.name for _, v in grads_and_vars],))\r\n\r\n    ValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'conv2d_3/kernel:0', 'conv2d_3/bias:0', 'conv2d_4/kernel:0', 'conv2d_4/bias:0', 'conv2d_5/kernel:0', 'conv2d_5/bias:0', 'conv2d_6/kernel:0', 'conv2d_6/bias:0', 'conv2d_7/kernel:0', 'conv2d_7/bias:0', 'conv2d_8/kernel:0', 'conv2d_8/bias:0', 'conv2d_9/kernel:0', 'conv2d_9/bias:0', 'conv2d_10/kernel:0', 'conv2d_10/bias:0'].\r\n\r\n\r\n```\r\n\r\n", "comments": ["@lihost,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "@amahendrakar here is the link to colab notebook.\r\n\r\n[Colab Notebook](https://colab.research.google.com/drive/12rORv1i7clN5Pq88oWpJhRPk4ESQs_ot)", "@amahendrakar I have shared notebook. Let me know if this is not accessible to you.", "@lihost,\r\nI am facing an error while running the code because of the missing files at `TFRECS_TRAIN = \"gs://imager-tfrecs-train/*\"\r\n`\r\n\r\nCould you please share all the datasets and supporting files used in your code. Thanks!", "@amahendrakar I have made dataset(GCP bucket) public as per guidelines provided at [https://cloud.google.com/storage/docs/access-control/making-data-public](url) \r\n\r\nThis should make dataset accessible to you. Thanks...", "Was able to reproduce the issue with [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/42f90740db3c7c946917e672a37f45c1/38937.ipynb#scrollTo=NnUCE4wBqz6b).\r\n\r\nWhereas running the code with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/dc31111b79e8f84cc6297cbe35fcc02e/38937.ipynb#scrollTo=pLzHJliV7VSG) raises a `ValueError: Unknown metric function:val_loss`.\r\n\r\nPlease find the attached gist. Thanks!", "Tried with TF 2.1  and also changed metric to 'accuracy' in notebook [TF v2.1](https://colab.research.google.com/gist/amahendrakar/dc31111b79e8f84cc6297cbe35fcc02e/38937.ipynb#scrollTo=pLzHJliV7VSG)  and its running but got me into another error while in first epoch  \r\n\r\n`IndexError: tuple index out of range`\r\n\r\n\r\nHere is complete stacktrace...\r\n\r\n\r\n```python\r\n\r\noptimizer >>> {'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 1280, 1280, 3)]   0         \r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, 1280, 1280, 256)   7168      \r\n_________________________________________________________________\r\nmax_pooling2d (MaxPooling2D) (None, 640, 640, 256)     0         \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 640, 640, 256)     590080    \r\n_________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2 (None, 320, 320, 256)     0         \r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 320, 320, 64)      147520    \r\n_________________________________________________________________\r\nmax_pooling2d_2 (MaxPooling2 (None, 160, 160, 64)      0         \r\n_________________________________________________________________\r\nconv2d_3 (Conv2D)            (None, 160, 160, 32)      18464     \r\n_________________________________________________________________\r\nmax_pooling2d_3 (MaxPooling2 (None, 80, 80, 32)        0         \r\n_________________________________________________________________\r\nconv2d_4 (Conv2D)            (None, 80, 80, 16)        4624      \r\n_________________________________________________________________\r\nmax_pooling2d_4 (MaxPooling2 (None, 40, 40, 16)        0         \r\n_________________________________________________________________\r\nconv2d_5 (Conv2D)            (None, 40, 40, 16)        2320      \r\n_________________________________________________________________\r\nup_sampling2d (UpSampling2D) (None, 80, 80, 16)        0         \r\n_________________________________________________________________\r\nconv2d_6 (Conv2D)            (None, 80, 80, 32)        4640      \r\n_________________________________________________________________\r\nup_sampling2d_1 (UpSampling2 (None, 160, 160, 32)      0         \r\n_________________________________________________________________\r\nconv2d_7 (Conv2D)            (None, 160, 160, 64)      18496     \r\n_________________________________________________________________\r\nup_sampling2d_2 (UpSampling2 (None, 320, 320, 64)      0         \r\n_________________________________________________________________\r\nconv2d_8 (Conv2D)            (None, 320, 320, 256)     147712    \r\n_________________________________________________________________\r\nup_sampling2d_3 (UpSampling2 (None, 640, 640, 256)     0         \r\n_________________________________________________________________\r\nconv2d_9 (Conv2D)            (None, 640, 640, 256)     590080    \r\n_________________________________________________________________\r\nup_sampling2d_4 (UpSampling2 (None, 1280, 1280, 256)   0         \r\n_________________________________________________________________\r\nconv2d_10 (Conv2D)           (None, 1280, 1280, 3)     6915      \r\n=================================================================\r\nTotal params: 1,538,019\r\nTrainable params: 1,538,019\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nTrain for 7 steps\r\nEpoch 1/50\r\n1/7 [===>..........................] - ETA: 3s\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-12-623067813c39> in <module>()\r\n      6       model.summary()\r\n      7 \r\n----> 8 model.fit(training_dataset, steps_per_epoch=STEPS_PER_EPOCH, epochs=50, validation_data=validation_dataset)\r\n      9 \r\n     10 model.save(SAVE_WEIGHT)\r\n\r\n23 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    340                 mode=ModeKeys.TRAIN,\r\n    341                 training_context=training_context,\r\n--> 342                 total_epochs=epochs)\r\n    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    344 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    613       # This is the first call of __call__, so we have to initialize.\r\n    614       initializers = []\r\n--> 615       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    616     finally:\r\n    617       # At this point we know that the initialization is complete (or less\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    495     self._concrete_stateful_fn = (\r\n    496         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 497             *args, **kwds))\r\n    498 \r\n    499     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2387       args, kwargs = None, None\r\n   2388     with self._lock:\r\n-> 2389       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2390     return graph_function\r\n   2391 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2701 \r\n   2702       self._function_cache.missed.add(call_context_key)\r\n-> 2703       graph_function = self._create_graph_function(args, kwargs)\r\n   2704       self._function_cache.primary[cache_key] = graph_function\r\n   2705       return graph_function, args, kwargs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2591             arg_names=arg_names,\r\n   2592             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2593             capture_by_value=self._capture_by_value),\r\n   2594         self._function_attributes,\r\n   2595         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    976                                           converted_func)\r\n    977 \r\n--> 978       func_outputs = python_func(*func_args, **func_kwargs)\r\n    979 \r\n    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    437         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    438         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    440     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    441 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in distributed_function(input_iterator)\r\n     83     args = _prepare_feed_values(model, input_iterator, mode, strategy)\r\n     84     outputs = strategy.experimental_run_v2(\r\n---> 85         per_replica_function, args=args)\r\n     86     # Out of PerReplica outputs reduce or pick values to return.\r\n     87     all_outputs = dist_utils.unwrap_output_dict(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/tpu_strategy.py in experimental_run_v2(self, fn, args, kwargs)\r\n    163     # so autograph is on by default here.\r\n    164     fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx())\r\n--> 165     return self.extended.tpu_run(fn, args, kwargs)\r\n    166 \r\n    167 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/tpu_strategy.py in tpu_run(self, fn, args, kwargs)\r\n    681   def tpu_run(self, fn, args, kwargs):\r\n    682     func = self._tpu_function_creator(fn)\r\n--> 683     return func(args, kwargs)\r\n    684 \r\n    685   def _tpu_function_creator(self, fn):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/tpu_strategy.py in tpu_function(args, kwargs)\r\n    741             replicate_inputs,\r\n    742             device_assignment=self._device_assignment,\r\n--> 743             maximum_shapes=maximum_shapes)\r\n    744 \r\n    745       # Remove all no ops that may have been added during 'tpu.replicate()'\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tpu/tpu.py in replicate(computation, inputs, infeed_queue, device_assignment, name, maximum_shapes)\r\n    808       device_assignment,\r\n    809       name,\r\n--> 810       maximum_shapes=maximum_shapes)[1]\r\n    811 \r\n    812 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tpu/tpu.py in split_compile_and_replicate(***failed resolving arguments***)\r\n   1171       vscope.set_custom_getter(custom_getter)\r\n   1172 \r\n-> 1173       outputs = computation(*computation_inputs)\r\n   1174 \r\n   1175       vscope.set_use_resource(saved_use_resource)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/tpu_strategy.py in replicated_fn(replica_id, replica_args, replica_kwargs)\r\n    710         \"\"\"Wraps user function to provide replica ID and `Tensor` inputs.\"\"\"\r\n    711         with _TPUReplicaContext(strategy, replica_id_in_sync_group=replica_id):\r\n--> 712           result[0] = fn(*replica_args, **replica_kwargs)\r\n    713         return result[0]\r\n    714 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    290   def wrapper(*args, **kwargs):\r\n    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 292       return func(*args, **kwargs)\r\n    293 \r\n    294   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics, standalone)\r\n    414   x, y, sample_weights = model._standardize_user_data(\r\n    415       x, y, sample_weight=sample_weight, class_weight=class_weight,\r\n--> 416       extract_tensors_from_dataset=True)\r\n    417   batch_size = array_ops.shape(nest.flatten(x, expand_composites=True)[0])[0]\r\n    418   # If `model._distribution_strategy` is True, then we are in a replica context\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2381         is_dataset=is_dataset,\r\n   2382         class_weight=class_weight,\r\n-> 2383         batch_size=batch_size)\r\n   2384 \r\n   2385   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\r\n   2467           shapes=None,\r\n   2468           check_batch_axis=False,  # Don't enforce the batch size.\r\n-> 2469           exception_prefix='target')\r\n   2470 \r\n   2471       # Generate sample-wise weight values given the `sample_weight` and\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n    510                        'for each key in: ' + str(names))\r\n    511   elif isinstance(data, (list, tuple)):\r\n--> 512     if isinstance(data[0], (list, tuple)):\r\n    513       data = [np.asarray(d) for d in data]\r\n    514     elif len(names) == 1 and isinstance(data[0], (float, int)):\r\n\r\nIndexError: tuple index out of range\r\n```", "@lihost Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38937\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38937\">No</a>\n"]}, {"number": 38936, "title": "Shuffling and batching operations results in keras model not running", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Ubuntu)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source (Google Colab)\r\n- TensorFlow version (use command below): 2.2.0-rc3\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1\r\n\r\n\r\n\r\nI have a dataset of about 70,000 image files.  I'm following the tutorial - (https://www.tensorflow.org/tutorials/load_data/images#load_using_tfdata) to load them and preprocess them. However this function was causing problems:\r\n```\r\ndef prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\r\n  # This is a small dataset, only load it once, and keep it in memory.\r\n  # use `.cache(filename)` to cache preprocessing work for datasets that don't\r\n  # fit in memory.\r\n  if cache:\r\n    if isinstance(cache, str):\r\n      ds = ds.cache(cache)\r\n    else:\r\n      ds = ds.cache()\r\n\r\n  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\r\n\r\n  # Repeat forever\r\n  ds = ds.repeat()\r\n\r\n  ds = ds.batch(BATCH_SIZE)\r\n\r\n  # `prefetch` lets the dataset fetch batches in the background while the model\r\n  # is training.\r\n  ds = ds.prefetch(buffer_size=AUTOTUNE)\r\n\r\n  return ds\r\n\r\n```\r\nAfter applying this function as the final processing step, my model would run but not even start on the first epoch.  The same applied to even a model consisting of a single Dense unit.  Examining the function step by step  I tried to do just the shuffle step or batching step - \r\n` ds = ds.shuffle(buffer_size=shuffle_buffer_size)  `\r\n` ds = ds.batch(BATCH_SIZE)`\r\nThis alone resulted in the issue described above. \r\n\r\n\r\nThe preprocessing I have done before this is the same as in the tutorial.\r\n\r\nNote: The Image data has no labels (by design) .  This might possibly causing the issue? \r\n", "comments": ["i ran the code shared by you on nightly,  and do not face [any error](https://colab.sandbox.google.com/gist/Saduf2019/10b3b863cac05ace2b0dee87b3ccab20/38936.ipynb).\r\ncould you please share simple standalone code for us to replicate the error, or if possible please share a google colab gist for us to analyse the issue faced by you.", "This was an issue on my part. The batch size was too large for memory.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38936\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38936\">No</a>\n"]}, {"number": 38935, "title": "[RNN] Bidirectional LSTM conversion fail", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-bit\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (or github SHA if from source): 2.2.0-dev20200422\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(\r\n    'model.h5',\r\n    input_shapes={'embedding_input': (128,1900)}\r\n)\r\n\r\n# converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model) \r\nconverter.experimental_new_converter = True\r\n# # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n# #                                        tf.lite.OpsSet.SELECT_TF_OPS]\r\ntfmodel = converter.convert() \r\nopen ('model.tflite' , \"wb\") .write(tfmodel)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"convert.py\", line 25, in <module>\r\n    tfmodel = converter.convert()\r\n  File \"C:\\Users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 1326, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 536, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"C:\\Users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 241, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-04-27 15:43:22.425922: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:307] Ignored output_format.\r\n2020-04-27 15:43:22.426100: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:310] Ignored drop_control_dependency.\r\n2020-04-27 15:43:22.443931: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-04-27 15:43:22.449382: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x27c7543e550 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-27 15:43:22.449555: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nloc(callsite(\"sequential/bidirectional/backward_lstm/PartitionedCall/while\"(\"C:\\Users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py\":406:0) at callsite(\"C:\\Users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py\":680:0 at callsite(\"C:\\Users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\":1086:0 at \"convert.py\":13:0)))): error: body function result type tensor<?x128x128xf32> is incompatible with result type tensor<1900x1x128xf32> at index 3\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\jing_\\anaconda3\\envs\\tf2-gpu\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"c:\\users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: C:\\Users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py:406:52: error: body function result type tensor<?x128x128xf32> is incompatible with result type tensor<1900x1x128xf32> at index 3\r\n                                                   new_output_names)\r\n                                                   ^\r\nC:\\Users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py:680:3: note: called from\r\n  return _construct_concrete_function(func, output_graph_def, converted_inputs)\r\n  ^\r\nC:\\Users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1086:11: note: called from\r\n          concrete_func, lower_control_flow=False)\r\n          ^\r\nconvert.py:13:5: note: called from\r\n    input_shapes={'embedding_input': (128,1900)}\r\n    ^\r\nC:\\Users\\jing_\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py:406:52: note: see current operation: %75:11 = \"tf.While\"(%6, %17, %6, %0, %73, %74, %5, %70, %43, %46, %49) {_lower_using_switch_merge = false, _num_original_outputs = 11 : i64, _read_only_resource_inputs = [], body = @while_body_3631_frozen0, cond = @while_cond_3630_frozen0, device = \"\", is_stateless = true, output_shapes = [\"tfshape$\", \"tfshape$\", \"tfshape$\", \"tfshape$\", \"tfshape$dim { size: -1 } dim { size: 128 }\", \"tfshape$dim { size: -1 } dim { size: 128 }\", \"tfshape$\", \"tfshape$\", \"tfshape$dim { size: 128 } dim { size: 512 }\", \"tfshape$dim { size: 128 } dim { size: 512 }\", \"tfshape$dim { size: 512 }\"], parallel_iterations = 32 : i64} : (tensor<i32>, tensor<i32>, tensor<i32>, tensor<1900x1x128xf32>, tensor<128x128xf32>, tensor<128x128xf32>, tensor<i32>, tensor<1900x128x128xf32>, tensor<128x512xf32>, tensor<128x512xf32>, tensor<512xf32>) -> (tensor<i32>, tensor<i32>, tensor<i32>, tensor<1900x1x128xf32>, tensor<128x128xf32>, tensor<128x128xf32>, tensor<i32>, tensor<1900x128x128xf32>, tensor<128x512xf32>, tensor<128x512xf32>, tensor<512xf32>)\r\n                                                   new_output_names)\r\n                                                   ^\r\n```\r\n\r\n\r\n\r\n**Failure details**\r\n\r\nI am trying to utilize GPU delegate on the Android platform. When I use the normal way of converting i.e converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model) without enabling GPU delegate, everything works fine, just that the inference time cost is a bit high. So I want to enable GPU delegate to see if it improves the time cost. However, GPU delegate doesn't allow dynamic-sized tensors, thus, when converting, I need to set the size of my input layer, which I did in the above code, but I couldn't get the model to convert after that.\r\nSummary of my model is as follows:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nembedding (Embedding)        (None, 1900, 128)         377600\r\n_________________________________________________________________\r\nbidirectional (Bidirectional (None, 1900, 256)         263168\r\n_________________________________________________________________\r\nbatch_normalization (BatchNo (None, 1900, 256)         1024\r\n_________________________________________________________________\r\nglobal_max_pooling1d (Global (None, 256)               0\r\n_________________________________________________________________\r\ndense (Dense)                (None, 64)                16448\r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 32)                2080\r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 2)                 66\r\n=================================================================\r\nTotal params: 660,386\r\nTrainable params: 659,874\r\nNon-trainable params: 512\r\n```\r\n\r\n", "comments": ["@limjq45 \r\n\r\nCan you try with this code. Will it be possible to share model.h5 file to reproduce the issue in our environment.\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(\r\n    'model.h5',\r\n    input_shapes={'embedding_input': (128,1900)}\r\n)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(loaded_model) \r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True\r\nconverter.experimental_new_converter =True\r\ntfmodel = converter.convert() \r\nopen ('model.tflite' , \"wb\") .write(tfmodel)\r\n```\r\nThanks!", "> @limjq45\r\n> \r\n> Can you try with this code. Will it be possible to share model.h5 file to reproduce the issue in our environment.\r\n> \r\n> ```\r\n> converter = tf.lite.TFLiteConverter.from_keras_model_file(\r\n>     'model.h5',\r\n>     input_shapes={'embedding_input': (128,1900)}\r\n> )\r\n> \r\n> converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model) \r\n> converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n>                                        tf.lite.OpsSet.SELECT_TF_OPS]\r\n> converter.allow_custom_ops=True\r\n> converter.experimental_new_converter =True\r\n> tfmodel = converter.convert() \r\n> open ('model.tflite' , \"wb\") .write(tfmodel)\r\n> ```\r\n> \r\n> Thanks!\r\n\r\nHi , I did try the above code too, it is able to convert but when I load it into the device, it throws this error\r\n```\r\nInternal error: Failed to apply delegate: Encountered unresolved custom op: TensorListReserve.\r\n```\r\n\r\nHere's a copy of the model.h5 [https://drive.google.com/open?id=1vj8Ini-XhX57obGUNTdCNBwbkigWyFOx](url)\r\nThe embedding input shape is (128,300), still the same structure, just that the max_length is changed.\r\n\r\nP/s: anyway I tried to do a simple Embedding -> LSTM(unroll=True) -> Dense(softmax), somehow when unroll=true on LSTM, it allows me to delegate GPU on Android with the code I posted, but when I run it, it just throws this error on Android \r\n```\r\nA/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0\r\n```\r\n\r\nin my app's build.grade dependencies:\r\n```\r\nimplementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\nimplementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n```\r\n", "@limjq45 Which tflite application you are trying implement? I would like to reproduce the error on my device and see what is root-cause. Thanks! ", "> @limjq45 Which tflite application you are trying implement? I would like to reproduce the error on my device and see what is root-cause. Thanks!\r\n\r\n@jvishnuvardhan I am basically trying to build a Bidirectional LSTM text classifier. I used this code as my template:[https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification/android](url).\r\n\r\nGiven the network configuration in the first post, I am able to train out a model, convert it to tflite, and classify text successfully on my device.\r\n\r\nThe code I used to convert is as follows:\r\n```\r\nloaded_model = tf.keras.models.load_model('model.h5')\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(loaded_model) \r\ntfmodel = converter.convert() \r\nopen ('model.tflite' , \"wb\") .write(tfmodel)\r\n```\r\n\r\nThe problem is, the inference time is too long and I would like to try enabling GPU delegate on my device, however, the current model will not work as it has dynamic-sized tensors.\r\n\r\nI've googled and they mentioned to make it static-sized, I will need to specify the input shape during conversion, so I tried the following code:\r\n\r\n``` \r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(\r\n    'model.h5',\r\n    input_shapes={'embedding_input': (128,1900)}\r\n)\r\ntfmodel = converter.convert() \r\nopen ('model.tflite' , \"wb\") .write(tfmodel)\r\n```\r\nI did try with the experimental flag too, but it doesn't allow me to convert successfully.\r\n\r\nThe only time I am able to convert successfully with the above code is when the unroll flag in LSTM layer is set to True. I am also able to add GPU delegate successfully with the converted model, however, when I tried to classify a string of text, it just crashes with this error.\r\n\r\n```\r\nA/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0\r\n```\r\n", "@jvishnuvardhan were you able to reproduce the error? If not I can create a colab notebook for you", "@limjq45 We announced support for TF 2.0 Keras LSTM to TFLite fused LSTM. Please see\r\nhttps://groups.google.com/a/tensorflow.org/g/tflite/c/Ub4apUvblN8\r\n\r\nThis is an e2e solution (works with post training quantization of the fused LSTM). Do you want to try this out?", "@ashwinmurthy hi, the link you provided is not working. ", "I will close this issue and open another one that provides colab"]}, {"number": 38934, "title": "ValueError: Attempted to save a function b'__inference_GRU-Dense_layer_call_fn_29299' which references a symbolic Tensor Tensor(\"dropout/mul_1:0\", shape=(?, 384), dtype=float32) that is not a simple constant. This is not supported.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:2.1.0\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:virtualenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac4592b0>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac465668>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac4755f8>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac3fe940>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac465668>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac4592b0>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac4592b0>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac3fe940>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac4755f8>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac4755f8>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac4592b0>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac465668>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac4755f8>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac3fe940>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac465668>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac4592b0>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac4592b0>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac3fe940>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac4755f8>), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(?, ?), dtype=tf.float32, name='inputs'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f27ac4755f8>), {}).\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-68ef369094e8> in <module>\r\n      4 \r\n      5 # set_gpu_ratio(ratio=0.65)\r\n----> 6 train_model(train_csv_file, dict_path, model_file_path)\r\n\r\n<ipython-input-4-872233fb1b6b> in train_model(train_csv_file, dict_path, model_file_path)\r\n     56 #     model_file_path = '/data1/xuyingjie/project/zhihu_project2020Q1/user_feedback_algorithm_v2_py3/model/'\r\n     57 #     tf.saved_model.save(model, 'model_file_path')\r\n---> 58     tf.keras.models.save_model(model, 'model_file_path', save_format=\"tf\")\r\n     59 \r\n     60     # \u4fdd\u5b58\u4e3apd\u6a21\u578b\u7684\u65b9\u5f0f\u4e8c\uff1a\r\n\r\n~/.virtualenvs/py3venv/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    113   else:\r\n    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n--> 115                           signatures, options)\r\n    116 \r\n    117 \r\n\r\n~/.virtualenvs/py3venv/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)\r\n     76     # we use the default replica context here.\r\n     77     with distribution_strategy_context._get_default_replica_context():  # pylint: disable=protected-access\r\n---> 78       save_lib.save(model, filepath, signatures, options)\r\n     79 \r\n     80   if not include_optimizer:\r\n\r\n~/.virtualenvs/py3venv/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    907   object_saver = util.TrackableSaver(checkpoint_graph_view)\r\n    908   asset_info, exported_graph = _fill_meta_graph_def(\r\n--> 909       meta_graph_def, saveable_view, signatures, options.namespace_whitelist)\r\n    910   saved_model.saved_model_schema_version = (\r\n    911       constants.SAVED_MODEL_SCHEMA_VERSION)\r\n\r\n~/.virtualenvs/py3venv/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py in _fill_meta_graph_def(meta_graph_def, saveable_view, signature_functions, namespace_whitelist)\r\n    551   resource_initializer_ops = []\r\n    552   with exported_graph.as_default():\r\n--> 553     object_map, resource_map, asset_info = saveable_view.map_resources()\r\n    554     for resource_initializer_function in resource_initializer_functions:\r\n    555       asset_dependencies = []\r\n\r\n~/.virtualenvs/py3venv/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py in map_resources(self)\r\n    283                 (\"Attempted to save a function {} which references a symbolic \"\r\n    284                  \"Tensor {} that is not a simple constant. This is not \"\r\n--> 285                  \"supported.\").format(concrete_function.name, capture))\r\n    286           copied_tensor = constant_op.constant(capture_constant_value)\r\n    287           node_id = len(self.nodes)\r\n\r\nValueError: Attempted to save a function b'__inference_GRU-Dense_layer_call_fn_29299' which references a symbolic Tensor Tensor(\"dropout/mul_1:0\", shape=(?, 384), dtype=float32) that is not a simple constant. This is not supported.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@xuyingjie521,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "gru_dense = GRU(units=384,\r\n                    **dropout=0.2,\r\n                    recurrent_dropout=0.15,**\r\n                    return_sequences=True,\r\n                    name='GRU-Dense'\r\n                    )(transformed)\r\n\r\n    ave_pooing = GlobalAveragePooling1D(name='GlobalAveragePooling1D')(gru_dense)\r\n\r\n    x_out = Dense(label_num,\r\n                  activation='softmax',\r\n                  name='Out-Dense'\r\n                  )(ave_pooing)\r\n    MyModel = Model(inputs=xbert_model.model.input, outputs=x_out)\r\n    MyModel.summary()", "@xuyingjie521,\r\nI tried to reproduce the issue but I am facing an error stating `NameError: name 'transformed' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/8f5330e55fb2e9ee1c25e9bcb478a4fc/38934.ipynb).\r\nCould you please share the complete code to reproduce the issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38933, "title": "how to add -std=c++17 to any cc_library", "body": "gcc 7.3.0\r\ntensorflow 2.1\r\nos:centos\r\nbazel:0.27.1\r\n\r\nplease tell me how to add -std=c++17 to any cc_library !!!\r\n\r\nwhy any file compile with c++0x ?\r\n\r\nmy build command : bazel build -s -j 32 --verbose_failures -c opt --cxxopt=-D_GLIBCXX_USE_CXX17_ABI=1  --cxxopt=-std=c++11 --cxxopt='-std=c++17'  --copt=-march=native --copt=-mfpmath=both //tensorflow/tools/pip_package:build_pip_package --sandbox_debug --cxxopt=-D_GLIBCXX_USE_CXX17_ABI=1  --cxxopt='-std=c++17' --linkopt='-std=c++17'\r\n\r\nany compile log has c++0x.like this\r\n\r\ngcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '**-std=c++0x'** -MD -MF bazel-out/host/bin/external/local_config_mlir/_objs/mlir-tblgen/LLVMIRConversionGen.d '-frandom-seed=bazel-out/host/bin/external/local_config_mlir/_objs/mlir-tblgen/LLVMIRConversionGen.o' -DUNICODE -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -iquote external/local_config_mlir -iquote bazel-out/host/bin/external/local_config_mlir -iquote external/llvm -iquote bazel-out/host/bin/external/llvm -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -isystem external/local_config_mlir/include -isystem bazel-out/host/bin/external/local_config_mlir/include -isystem external/llvm/include -isystem bazel-out/host/bin/external/llvm/include -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -g0 -g0 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/local_config_mlir/tools/mlir-tblgen/LLVMIRConversionGen.cpp -o bazel-out/host/bin/external/local_config_mlir/_objs/mlir-tblgen/LLVMIRConversionGen.o\r\n\r\n\r\nwhy ?", "comments": ["your install manual tell me,\r\ngcc 7.3.1\r\ntensorflow 2.1\r\nos:centos\r\nbazel:0.27.1\r\nis ok.....\r\nbut I can't\r\n\r\n @zhaozheng09\r\n", "fuck bazel export BAZEL_CXXOPTS=-std=c++11", "fuck bazel export BAZEL_CXXOPTS=-std=c++11", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38933\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38933\">No</a>\n"]}, {"number": 38932, "title": "New TFLiteConverter not working with tf.complex64", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab CPU\r\n- TensorFlow version (use command below): 2.2.0-rc3\r\n\r\n**Describe the current behavior**\r\nNew `TFLiteConverter` not working with `tf.complex64`\r\nDisabling the new converter (`converter.experimental_new_converter = False`) works.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python3\r\n@tf.function\r\ndef foo(x, y):\r\n    return x @ y\r\n\r\n\r\nx = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.complex64)\r\ny = tf.constant([[2, 3], [4, 5], [6, 7]], dtype=tf.complex64)\r\n\r\nfoo_concrete = foo.get_concrete_function(x, y)\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([foo_concrete])\r\nfoo_tflite = converter.convert()\r\n```\r\n\r\n[colab example](https://colab.research.google.com/drive/1snYjxYYJaOYPQOeQ1ec2o0xPHQB0ILY2)\r\n\r\n\r\nThanks", "comments": ["@karimnosseir ,could you take a look? thanks!", "Any update on this?", "Sorry for the delay.\r\nI have a fix that should get merged soon.\r\n\r\nNote: that FullyConnected op in TFLite doesn't support complex, so you will need to enable TF_SELECT_OPS to run it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38932\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38932\">No</a>\n", "@karimnosseir Thanks! Are there plans to support complex for FullyConnected op in TFLite?", "@galah92 Sorry for the delay.\r\nNo immediate plans."]}, {"number": 38931, "title": "Tensorflow Lite Object Detection on mobile devices", "body": "Suppose, say I have 30 interested objects on an image, is it possible to get all 30 box proposals on a single image without splitting the image, or the number of proposals on an image is limited to 10.\r\nThank you for the help! ", "comments": ["@amrit-das It is a parameter in the model, so you will need to regenerate the model with different parameter as you wish. It is there in `TFLiteObjectDetectionAPIModel.java`. Thanks! \r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thank You! "]}, {"number": 38930, "title": "Build does not succeed and the error content changes every time", "body": "I want to install Tensorflow 1.8 gpu on macOS, but the build does not go through and the error content changes every time even if I try to refer to the following site. How can we make it work?\r\n\r\nhttps://developpaper.com/tensorflow-1-8-with-gpu-on-macos-high-sierra-10-13-6/\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):High Sierra 10.13.6(17g65)\r\n- TensorFlow installed from (source or binary):https://github.com/tensorflow/tensorflow -b r1.8\r\n- TensorFlow version:1.8\r\n- Python version:3.6.3\r\n- Bazel version (if compiling from source):0.14.0\r\n- CUDA/cuDNN version:9.2/7.2\r\n- GPU model and memory:GTX GeForce 1080 ti\r\n- Command Line Tool 8.2.1\r\n\r\n`ERROR: /Users/zen/tensorflow/tensorflow/core/kernels/BUILD:1201:1: output 'tensorflow/core/kernels/_objs/gather_functor_gpu/tensorflow/core/kernels/gather_functor_gpu.cu.pic.o' was not created\r\nERROR: /Users/zen/tensorflow/tensorflow/core/kernels/BUILD:1201:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n`\r\n\r\n`ERROR: /Users/zen/tensorflow/tensorflow/core/kernels/BUILD:3434:1: output 'tensorflow/core/kernels/_objs/histogram_op_gpu/tensorflow/core/kernels/histogram_op_gpu.cu.o' was not created\r\nERROR: /Users/zen/tensorflow/tensorflow/core/kernels/BUILD:3434:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n`\r\n\r\nI'm getting all sorts of errors when I do the exact same situation\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@zenboooooon \r\ni see you are using a very old version of tensorflow could you please upgrade/install 1.15 o r2.x versions and let us know if you are facing any issues", "@Saduf2019 Thank you for your reply.\r\nI was going to try it once, but the build method was completely different between 1.8 and 1.14, and the build didn't start, and there was no site that explained how to do it, so I gave it up. I searched and couldn't find anything describing a version newer than 1.8.", "@zenboooooon \r\nplease follow [this link for installation](https://www.tensorflow.org/install/source) , [link1](https://www.lewuathe.com/how-to-build-tensorflow-on-macos.html), [link2](https://www.pyimagesearch.com/2019/12/09/how-to-install-tensorflow-2-0-on-macos/), [link3](https://techpolymath.com/2017/12/14/tensorflow-from-source-on-mac/) [link4](https://stackoverflow.com/questions/43364264/how-to-install-tensorflow-from-source-for-mac/44299779).\r\n\r\nThere are plenty resources online to refer, please let us know if the links shared help you resolve the issue faced.", "@Saduf2019 I'd like to use tensorflow which has GPU support, but the link you sent me was one that doesn't have GPU support.", "@zenboooooon \r\nplease refer to [link1](https://medium.com/@saitejadommeti/building-tensorflow-gpu-from-source-for-rtx-2080-96fed102fcca) [link2](https://www.tensorflow.org/install/gpu) [link3](url)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38930\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38930\">No</a>\n"]}, {"number": 38929, "title": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.14\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA -- 10.0, cuDNN -- 7.4.2\r\n- GPU model and memory:\r\n\r\nQuadro RTX 4000 -- 8G\r\nGeForce RTX 2080 ti -- 12G\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI have two GPU on two machines, Quadro RTX 4000 and GeForce RTX 2080 ti. And I install the same NVIDIA-driver 440.59. I did the cuDNN sample test and it passed the test. And I ran the tensorflow [benchmark](https://github.com/tensorflow/models/tree/master/official/benchmark) using the below commands.  I used a small enough batch to avoid the OOM issue. The machine with GeForce RTX 2080 ti can successfully run the benchmark. But the machine with Quadro RTX 4000 cannot run the benchmark, which said \"Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\" Do you have any idea about this issue? Is it related to hardware? \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n$python tf_cnn_benchmarks.py --data_format=NCHW --batch_size=32 --model=resnet50 --optimizer=momentum --variable_update=replicated --nodistortions --gradient_repacking=8 --num_gpus=2 --num_epochs=90 --weight_decay=1e-4 --use_fp16 --train_dir=/home/lab2\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI0426 23:04:18.110438 139678773683968 session_manager.py:502] Done running local_init_op.\r\nINFO:tensorflow:Starting standard services.\r\nI0426 23:04:24.543723 139678773683968 supervisor.py:737] Starting standard services.\r\nINFO:tensorflow:Starting queue runners.\r\nI0426 23:04:24.543950 139678773683968 supervisor.py:743] Starting queue runners.\r\nRunning warm up\r\n2020-04-26 23:04:29.404714: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2020-04-26 23:04:30.068934: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-26 23:04:31.178428: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-04-26 23:04:32.225534: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-04-26 23:04:32.261894: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-04-26 23:04:32.295444: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.UnknownError'>, 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node tower_0/v0/cg/conv0/conv2d/Conv2D (defined at /tmp/tmpq2hyfuxj.py:12) ]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node tower_0/v0/cg/conv0/conv2d/Conv2D (defined at /tmp/tmpq2hyfuxj.py:12) ]]\r\n\t [[Reshape_603/_2957]]\r\n0 successful operations.\r\n1 derived errors ignored.\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node tower_0/v0/cg/conv0/conv2d/Conv2D:\r\n Cast (defined at /root/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:101)\t\r\n tower_0/v0/cg/conv0/Pad (defined at /root/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:204)\r\n\r\nInput Source operations connected to node tower_0/v0/cg/conv0/conv2d/Conv2D:\r\n Cast (defined at /root/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:101)\t\r\n tower_0/v0/cg/conv0/Pad (defined at /root/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:204)\r\n\r\nOriginal stack trace for 'tower_0/v0/cg/conv0/conv2d/Conv2D':\r\n  File \"tf_cnn_benchmarks.py\", line 73, in <module>\r\n    app.run(main)  # Raises error on invalid flags, unlike tf.app.run()\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"tf_cnn_benchmarks.py\", line 68, in main\r\n    bench.run()\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1880, in run\r\n    return self._benchmark_train()\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 2076, in _benchmark_train\r\n    build_result = self._build_graph()\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 2110, in _build_graph\r\n    (input_producer_op, enqueue_ops, fetches) = self._build_model()\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 2828, in _build_model\r\n    gpu_compute_stage_ops, gpu_grad_stage_ops)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 3345, in add_forward_pass_and_gradients\r\n    outputs = maybe_compile(forward_pass_and_gradients, self.params)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 3542, in maybe_compile\r\n    return computation()\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 3199, in forward_pass_and_gradients\r\n    input_list, phase_train, nclass)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/models/model.py\", line 293, in build_network\r\n    self.add_inference(network)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/models/resnet_model.py\", line 308, in add_inference\r\n    cnn.conv(64, 7, 7, 2, 2, mode='SAME_RESNET', use_batch_norm=True)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py\", line 209, in conv\r\n    kernel_initializer=kernel_initializer)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py\", line 134, in _conv2d_impl\r\n    use_bias=False)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/convolutional.py\", line 424, in conv2d\r\n    return layer.apply(inputs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1479, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/base.py\", line 537, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 634, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 146, in wrapper\r\n    ), args, kwargs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 450, in converted_call\r\n    result = converted_f(*effective_args, **kwargs)\r\n  File \"/tmp/tmpq2hyfuxj.py\", line 12, in tf__call\r\n    outputs = ag__.converted_call('_convolution_op', self, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (inputs, self.kernel), None)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 356, in converted_call\r\n    return _call_unconverted(f, args, kwargs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 255, in _call_unconverted\r\n    return f(*args)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1079, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 635, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 234, in __call__\r\n    name=self.name)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1953, in conv2d\r\n    name=name)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1071, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nI0426 23:04:32.326952 139678773683968 coordinator.py:224] Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.UnknownError'>, 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node tower_0/v0/cg/conv0/conv2d/Conv2D (defined at /tmp/tmpq2hyfuxj.py:12) ]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node tower_0/v0/cg/conv0/conv2d/Conv2D (defined at /tmp/tmpq2hyfuxj.py:12) ]]\r\n\t [[Reshape_603/_2957]]\r\n0 successful operations.\r\n1 derived errors ignored.\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node tower_0/v0/cg/conv0/conv2d/Conv2D:\r\n Cast (defined at /root/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:101)\t\r\n tower_0/v0/cg/conv0/Pad (defined at /root/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:204)\r\n\r\nInput Source operations connected to node tower_0/v0/cg/conv0/conv2d/Conv2D:\r\n Cast (defined at /root/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:101)\t\r\n tower_0/v0/cg/conv0/Pad (defined at /root/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:204)\r\n\r\nOriginal stack trace for 'tower_0/v0/cg/conv0/conv2d/Conv2D':\r\n  File \"tf_cnn_benchmarks.py\", line 73, in <module>\r\n    app.run(main)  # Raises error on invalid flags, unlike tf.app.run()\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"tf_cnn_benchmarks.py\", line 68, in main\r\n    bench.run()\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1880, in run\r\n    return self._benchmark_train()\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 2076, in _benchmark_train\r\n    build_result = self._build_graph()\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 2110, in _build_graph\r\n    (input_producer_op, enqueue_ops, fetches) = self._build_model()\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 2828, in _build_model\r\n    gpu_compute_stage_ops, gpu_grad_stage_ops)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 3345, in add_forward_pass_and_gradients\r\n    outputs = maybe_compile(forward_pass_and_gradients, self.params)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 3542, in maybe_compile\r\n    return computation()\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 3199, in forward_pass_and_gradients\r\n    input_list, phase_train, nclass)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/models/model.py\", line 293, in build_network\r\n    self.add_inference(network)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/models/resnet_model.py\", line 308, in add_inference\r\n    cnn.conv(64, 7, 7, 2, 2, mode='SAME_RESNET', use_batch_norm=True)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py\", line 209, in conv\r\n    kernel_initializer=kernel_initializer)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py\", line 134, in _conv2d_impl\r\n    use_bias=False)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/convolutional.py\", line 424, in conv2d\r\n    return layer.apply(inputs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1479, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/layers/base.py\", line 537, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 634, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 146, in wrapper\r\n    ), args, kwargs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 450, in converted_call\r\n    result = converted_f(*effective_args, **kwargs)\r\n  File \"/tmp/tmpq2hyfuxj.py\", line 12, in tf__call\r\n    outputs = ag__.converted_call('_convolution_op', self, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (inputs, self.kernel), None)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 356, in converted_call\r\n    return _call_unconverted(f, args, kwargs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 255, in _call_unconverted\r\n    return f(*args)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1079, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 635, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 234, in __call__\r\n    name=self.name)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1953, in conv2d\r\n    name=name)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1071, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node tower_0/v0/cg/conv0/conv2d/Conv2D}}]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node tower_0/v0/cg/conv0/conv2d/Conv2D}}]]\r\n\t [[Reshape_603/_2957]]\r\n0 successful operations.\r\n1 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tf_cnn_benchmarks.py\", line 73, in <module>\r\n    app.run(main)  # Raises error on invalid flags, unlike tf.app.run()\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"tf_cnn_benchmarks.py\", line 68, in main\r\n    bench.run()\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1880, in run\r\n    return self._benchmark_train()\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 2085, in _benchmark_train\r\n    return self._benchmark_graph(result_to_benchmark, eval_build_results)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 2294, in _benchmark_graph\r\n    is_chief, summary_writer, profiler)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 2430, in benchmark_with_session\r\n    collective_graph_key=collective_graph_key)\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 869, in benchmark_one_step\r\n    results = sess.run(fetches, options=run_options, run_metadata=run_metadata)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/root/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node tower_0/v0/cg/conv0/conv2d/Conv2D (defined at /tmp/tmpq2hyfuxj.py:12) ]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node tower_0/v0/cg/conv0/conv2d/Conv2D (defined at /tmp/tmpq2hyfuxj.py:12) ]]\r\n\t [[Reshape_603/_2957]]\r\n0 successful operations.\r\n1 derived errors ignored.\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node tower_0/v0/cg/conv0/conv2d/Conv2D:\r\n Cast (defined at /root/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:101)\t\r\n tower_0/v0/cg/conv0/Pad (defined at /root/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:204)\r\n\r\n", "comments": ["I added the following codes at the beginning of the benchmark file. The issue was gone.\r\n\r\n\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n\r\n\r\nBut I still want to figure out if it is related to hardware?\r\nIs there other stable or framework-based solution to solve it?", "@Young768 \r\n\r\nGlad to know the issue got resolved.Please, go through the l[ink1](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/config/experimental/set_memory_growth), [link2](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) , [link3](https://stackoverflow.com/questions/55788883/limiting-gpu-memory-usage-by-keras-tf-2019) and see if it helps you. Please, close this thread as your issue was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38929\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38929\">No</a>\n"]}]