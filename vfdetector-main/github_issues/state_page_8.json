[{"number": 54818, "title": "`tf.experimental.numpy.sqrt` lack support for bfloat16", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nx = tf.random.uniform([2,1], dtype=tf.bfloat16) \r\nprint(tf.math.sqrt(x)) # pass\r\nprint(tf.experimental.numpy.sqrt(x)) # AttributeError\r\n```\r\n\r\n**Describe the current behavior**\r\n`tf.experimental.numpy.sqrt` cannot accept a tensor of type `bfloat16`. \r\nFor the above code snippet, the error message is:\r\n```\r\nAttributeError: \r\n        'EagerTensor' object has no attribute 'astype'.\r\n        If you are looking for numpy-related methods, please run the following:\r\n        from tensorflow.python.ops.numpy_ops import np_config\r\n        np_config.enable_numpy_behavior()\r\n```\r\n\r\n", "comments": ["@chunduriv ,\r\nI was able to reproduce the issue in tf v2.8, 2.7 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/113610cd7f5c1fd4a18dfe258f73f66d/54818.ipynb).", "Thank you , will submit a fix soon.", "Thanks @rthadur .\r\nHere are more `tf.experimental.numpy` APIs that also lack `bfloat16` support:\r\n```\r\ntf.experimental.numpy.cos\r\ntf.experimental.numpy.exp\r\ntf.experimental.numpy.log1p\r\ntf.experimental.numpy.expm1\r\ntf.experimental.numpy.cosh\r\ntf.experimental.numpy.tanh\r\ntf.experimental.numpy.exp2\r\ntf.experimental.numpy.log2\r\n```"]}, {"number": 54790, "title": "tf.rank returns incorrect value in trace-compiled conditional when a SparseTensor is involved", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.9\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n- Python version: 3.7.7\r\n- CUDA/cuDNN version: CUDA 11.2.152 / cuDNN 8.1.1\r\n- GPU model and memory: GeForce RTX 2080 Ti 11GBytes\r\n\r\n**Describe the current behavior**\r\n`tf.rank()` returns the wrong value (at runtime) when used for conditional branching in trace-compiled code and when one of branches involves a `SparseTensor`.\r\nSpecifically, `tf.rank(v)` returns `2` for a tensor `v` of shape `(2,10,3)` when called as part of a conditional test, when one of the execution branches runs `tf.sparse.sparse_dense_matmul(L,v)` where `L` is a sparse matrix and `v` is a dense tensor.\r\n\r\n**Describe the expected behavior**\r\nIn this scenario, I would expect `tf.rank(v)` to return `3` for a tensor `v` of shape `(2,10,3)`.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function(input_signature=[\r\n    tf.SparseTensorSpec(shape=(None,None), dtype=tf.float32),\r\n    tf.TensorSpec(shape=None, dtype=tf.float32)\r\n])\r\ndef f(L,v):\r\n    tf.print(\"runtime: tf.rank(v) = \", tf.rank(v))\r\n    tf.print(\"runtime: tf.shape(v) = \", tf.shape(v))\r\n    if (tf.rank(v) == 2):\r\n        return tf.sparse.sparse_dense_matmul(L, v)\r\n    else:\r\n        return v[0,0,0] * v\r\n        \r\nL = tf.sparse.from_dense(tf.eye(10,dtype=tf.float32))\r\nf(L,tf.ones((2,10,3), dtype=tf.float32))\r\n```\r\nThe code above prints:\r\n```\r\nruntime: tf.rank(v) =  2\r\nruntime: tf.shape(v) =  [2 10 3]\r\n```\r\nBecause `tf.rank` returns the wrong value, the wrong branch is executed and results in the Traceback below.\r\nNote that if `L` is a dense matrix then `tf.rank` returns the correct result and the correct branch is executed -- here is a version of the code above with this modification:\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function(input_signature=[\r\n    tf.TensorSpec(shape=(None,None), dtype=tf.float32),\r\n    tf.TensorSpec(shape=None, dtype=tf.float32)\r\n])\r\ndef f(L,v):\r\n    tf.print(\"runtime: tf.rank(v) = \", tf.rank(v))\r\n    tf.print(\"runtime: tf.shape(v) = \", tf.shape(v))\r\n    if (tf.rank(v) == 2):\r\n        return tf.matmul(L, v)\r\n    else:\r\n        return v[0,0,0] * v\r\n        \r\nL = tf.eye(10,dtype=tf.float32)\r\nf(L,tf.ones((2,10,3), dtype=tf.float32))\r\n```\r\n\r\n**Other info / logs**\r\nTraceback obtained when running the first code example above:\r\n```\r\nruntime: tf.rank(v) =  2\r\nruntime: tf.shape(v) =  [2 10 3]\r\nTraceback (most recent call last):\r\n  File \"tmp/tf_cond.v3.py\", line 16, in <module>\r\n    f(L,tf.ones((2,10,3), dtype=tf.float32))\r\n  File \"/lucas/ilm/dept/rnd/home/sgrabli/src/fluxml/venv/lib64/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/lucas/ilm/dept/rnd/home/sgrabli/src/fluxml/venv/lib64/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  Tensor 'b' is not a matrix\r\n         [[node cond/SparseTensorDenseMatMul/SparseTensorDenseMatMul\r\n (defined at tmp/tf_cond.v3.py:11)\r\n]] [Op:__inference_f_61]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node cond/SparseTensorDenseMatMul/SparseTensorDenseMatMul:\r\nIn[0] cond/SparseTensorDenseMatMul/SparseTensorDenseMatMul/L:\r\nIn[1] cond/SparseTensorDenseMatMul/SparseTensorDenseMatMul/L_1:\r\nIn[2] cond/SparseTensorDenseMatMul/SparseTensorDenseMatMul/L_2:\r\nIn[3] cond/SparseTensorDenseMatMul/SparseTensorDenseMatMul/v:\r\n\r\nOperation defined at: (most recent call last)\r\n>>>   File \"tmp/tf_cond.v3.py\", line 16, in <module>\r\n>>>     f(L,tf.ones((2,10,3), dtype=tf.float32))\r\n>>> \r\n>>>   File \"tmp/tf_cond.v3.py\", line 10, in f\r\n>>>     if (tf.rank(v) == 2):\r\n>>> \r\n>>>   File \"tmp/tf_cond.v3.py\", line 11, in f\r\n>>>     return tf.sparse.sparse_dense_matmul(L, v)\r\n>>> \r\n\r\nFunction call stack:\r\nf -> cond_true_30\r\n```\r\n", "comments": ["@sgrabli I tried to  replicate the issue on colab using TF v2.8.0 , and faced different outcome.Could you please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/31fc4d6b3204bc2fa5884470d6ac4dfc/54790.ipynb)and  confirm the same?Thanks! ", "@sushreebarsa , thanks for taking a look.\r\nI ran your colab and got the same incorrect result I observed on my end:\r\n![image](https://user-images.githubusercontent.com/10070473/156614236-c9f53204-67e2-4338-bfe8-776ecfc149c2.png)\r\n", "@gadagashwini  I was able to reproduce the issue on colab using TF v2.8.0 and tf-nightly ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/31fc4d6b3204bc2fa5884470d6ac4dfc/54790.ipynb#scrollTo=dgx37EQmTQJS).Thanks!"]}, {"number": 54779, "title": "[INTEL oneDNN] Enable BF16 support for Round", "body": "Added BF16 support for the Round op.\r\n\r\nA special if-clause for the round op has been added in the test because the subsequent code computes the gradient of the op, which is zero for the round op (returns None in TF) (https://github.com/tensorflow/tensorflow/issues/5888). Therefore, returning early.", "comments": ["@penpornk  Can you please review this PR ? Thank you!"]}, {"number": 54751, "title": "Inconsistent behaviour between CPU and GPU using TFLite model on iOS", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  iOS 14.7.1 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 11 Pro\r\n- TensorFlow installed from (source or binary): binary using cocoapods\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: Apple A13 GPU\r\n\r\n**Describe the current behavior**\r\nI'm running a TFLite [object detection model](https://github.com/google/mediapipe/blob/master/mediapipe/modules/objectron/object_detection_3d_sneakers_1stage.tflite) with 2 main outputs and an auxiliary output that provides a segmentation map. The model is used in iOS application via Swift TFLite API where I'm using a Metal Delegate to utilise the GPU. While running on GPU, I'm constantly receiving a tensor of zeros from the auxiliary output. On CPU however, I'm getting non-zero values (which is a desired behaviour). Switching between GPU and CPU does not effect the two main outputs. I tried manipulating different options of the MetalDelegate, but to no success.  Can you point me towards reasons for such behaviour and any possible solutions? \r\n\r\n**Describe the expected behavior**\r\nReceiving non-zero values from the segmentation output, consistent with CPU performance, while running on GPU. \r\n\r\n**Other info / logs**\r\nBelow I'm posting a snippet from method responsible for configuration of an Interpreter. `InferenceConfig.accelerate` is a boolean flag - if true, the model runs on GPU.\r\n```\r\nif InferenceConfig.accelerate {\r\n    var options = MetalDelegate.Options()\r\n\r\n    options.isPrecisionLossAllowed = false\r\n    options.isQuantizationEnabled = false\r\n    options.waitType = .active\r\n    let gpuDelegate = MetalDelegate(options: options)\r\n\r\n    interpreter = try Interpreter(\r\n        modelPath: InferenceConfig.modelPath!,\r\n        delegates: [gpuDelegate]\r\n    )\r\n} else {\r\n    var interpreterOptions = Interpreter.Options()\r\n    interpreterOptions.isXNNPackEnabled = true\r\n\r\n    interpreter = try Interpreter(\r\n        modelPath: InferenceConfig.modelPath!,\r\n        options: interpreterOptions\r\n    )\r\n}\r\n```", "comments": ["I came across [this issue](https://github.com/tensorflow/tensorflow/issues/27240) which seems very closely related to the problem I described. The solution mentioned there implies changing the graph by introducing an addition operator with a very small constant value to remove the intermediate tensor problem. My question here is: is there a way to modify the graph in existing TFLite model? I don't have the access to TF model that I could change and convert to TFLite again.", "@witsemp ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.", "@tilakrayal, as this issue appears in a commercial project, I'll have to verify whether I can share anything more substantial. Meanwhile, as mentioned [here](https://github.com/tensorflow/tensorflow/issues/54751#issuecomment-1055684250), this [issue](https://github.com/tensorflow/tensorflow/issues/27240) seems to be exactly what I struggle with. Could you please point me towards any solution when it comes to changing an existing TFLite model?"]}, {"number": 54714, "title": "Linking process takes forever", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu18.04 (x86_64)\r\n- TensorFlow installed from (source or binary): https://github.com/tensorflow/tensorflow.git\r\n- TensorFlow version: latest\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip3\r\n- Bazel version (if compiling from source): bazel-5.0.0-linux-arm64\r\n- GCC/Compiler version (if compiling from source): gcc version 7.5.0 (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04)\r\n- CUDA/cuDNN version: 8.2.1.32-1+cuda10.2\r\n- CPU model and memory: CPU:       32 core Intel Xeon E5-2667 v2 (-MT-MCP-) speed: 3292 MHz (max) / 40GB RAM\r\n\r\n\r\n\r\nI'm trying to cross-compile Tensorflow for my NVIDIA Jetson Nano device, from a source code, on an Intel platform server machine. Got by the git clone. It seems like compilation process is finished, or the part of is finished, but the linking process stuck. It gives a tiny load on 1 of 32 cpu threads, but it already took almost a day, but nothing happens. Linking.... Also i checked, IO load, it's minimal. Is it ok, and it takes so much time, or i do something wrong ?\r\n\r\n> \r\n![image](https://user-images.githubusercontent.com/4686690/156057119-5578e436-e2e3-4dd4-abbd-d9fe58465562.png)\r\n\r\n![image](https://user-images.githubusercontent.com/4686690/156057201-384425a1-032c-4692-a719-f33192d97f24.png)\r\n\r\n![image](https://user-images.githubusercontent.com/4686690/156057731-d5c55dc8-30cd-4306-b56e-98abbf1c1f37.png)\r\n", "comments": ["@gmernov ,\r\nCan you please try installing TensorFlow v2.8 with GCC 7.3.1 and Bazel 4.2.1 as suggested in tested build configurations [here](https://www.tensorflow.org/install/source#cpu) and check if you are facing the same error. Thanks!\r\n", "Actually i tried multiple version of Bazel. But only 5.0 did the trick. All other versions gave me an error.\r\n\r\nroot@UBUNTU-SRV:/tmp/tensorflow-master/build# ../configure \r\nYou have bazel 4.2.1 installed.\r\nPlease upgrade your bazel installation to version 4.2.2 or higher to build TensorFlow!\r\n", "I also tried to build with 4.2.2\r\nIt segfaulted in the start of. Before the actuall compilation process started. Somewhere after bazel started and Download of something was initiated:\r\n```\r\n\r\n#\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGSEGV (0xb) at pc=0x0000005513a5d80c, pid=99, tid=339\r\n#\r\n# JRE version: OpenJDK Runtime Environment (Zulu11.37+48-CA) (11.0.6+9) (build 11.0.6+9-LTS)\r\n# Java VM: OpenJDK 64-Bit Server VM (11.0.6+9-LTS, mixed mode, tiered, compressed oops, g1 gc, linux-aarch64)\r\n# Problematic frame:\r\n# J 7167 c2 net.starlark.java.eval.Eval.eval(Lnet/starlark/java/eval/StarlarkThread$Frame;Lnet/starlark/java/syntax/Expression;)Ljava/lang/Object; (316 bytes) @ 0x0000005513a5d80c [0x0000005513a5d740+0x00000000000000cc]\r\n#\r\n# Core dump will be written. Default location: Core dumps may be processed with \"/usr/share/apport/apport %p %s %c %d %P %E\" (or dumping to /tmp/tensorflow-master/core.99)\r\n#\r\n# An error report file with more information is saved as:\r\n# /tmp/tensorflow-master/hs_err_pid99.log\r\nCompiled method (c2)  512027 7167       4       net.starlark.java.eval.Eval::eval (316 bytes)\r\n total in heap  [0x0000005513a5c790,0x0000005513a68aa0] = 49936\r\n relocation     [0x0000005513a5c908,0x0000005513a5d708] = 3584\r\n main code      [0x0000005513a5d740,0x0000005513a63b40] = 25600\r\n stub code      [0x0000005513a63b40,0x0000005513a65140] = 5632\r\n oops           [0x0000005513a65140,0x0000005513a65180] = 64\r\n metadata       [0x0000005513a65180,0x0000005513a654f0] = 880\r\n scopes data    [0x0000005513a654f0,0x0000005513a671a0] = 7344\r\n scopes pcs     [0x0000005513a671a0,0x0000005513a68240] = 4256\r\n dependencies   [0x0000005513a68240,0x0000005513a68250] = 16\r\n handler table  [0x0000005513a68250,0x0000005513a68928] = 1752\r\n nul chk table  [0x0000005513a68928,0x0000005513a68aa0] = 376\r\nCould not load hsdis-aarch64.so; library not loadable; PrintAssembly is disabled\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   http://www.azulsystems.com/support/\r\n#\r\n\r\n```\r\n![image](https://user-images.githubusercontent.com/4686690/157536615-027e11cd-2ece-420b-83a4-c096d5df73cd.png)\r\n", "As for now, it doesn't want to build even with 5.0.0 bazel.\r\n![image](https://user-images.githubusercontent.com/4686690/157863092-ef7832a0-e462-4687-80aa-fa4fed9581de.png)\r\n\r\n/root/.cache/bazel/_bazel_root/2b5c4e51f9e1005b4128ae187b9d110f/server/jvm.out  - has nothing in", "Tensorflow 2.7.1, bazel 4.2.2 the same thing: I also tried to install clang6.0 and clang10.0, but clang compilation provides an errors.\r\n\r\n![image](https://user-images.githubusercontent.com/4686690/158339107-d719f6b8-e9b7-46b6-a1c7-99f5156881cc.png)\r\n", "@gmernov,\r\nInstead of screenshot/image of an error, could you share the complete error log in the comment section. And also provide the `./configure` list. Thanks!", "It's not possible to share the complete error log because of: if i compile tensor using GCC, it gives me no error. Simply endless linking... i have to stop the compilation process using CTRL+C(3 times). My ./configure \r\n\r\n```\r\nroot@UBUNTU-SRV:/tmp/tensorflow-2.7.1# ./configure\r\nYou have bazel 4.2.2 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.8/dist-packages\r\n  /usr/local/lib/python3.8/site-packages/cv2/python-3.8\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.2 in:\r\n    /usr/local/cuda-10.2/targets/aarch64-linux/lib\r\n    /usr/local/cuda-10.2/targets/aarch64-linux/include\r\nFound cuDNN 8 in:\r\n    /usr/lib/aarch64-linux-gnu\r\n    /usr/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\nAlso the build command looks like: \r\n\r\n`bazel build --progress_report_interval=30 --show_loading_progress --show_progress --local_ram_resources=1500 --local_cpu_resources=32 -j 32 --verbose_failures=true --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n`\r\n\r\nActually i started with 2.8.0 and bazel 5.0.0 versions. and a build string:\r\n\r\n`bazel build  --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n`", "Anything you can advice ?", "@gmernov,\r\nAs per the Tensorflow [Tested build configuration](https://www.tensorflow.org/install/source#gpu), Tensorflow v2.8 supports CUDA 11.2 and cuDNN 8.1.  Linking to specific CUDA version is taking long time. Upgrade CUDA version and try. Thanks!", "I think it's not possible to upgrade CUDA version, because of i'm using latest Jetson JetPack. And it has it's certain version in. But i tried to build TF 2.7.1 using bazel =<3.99.0. The situation changed. Now it doesn't wait to the end to start linking process, but it does it on the run. Compiling,linking,compiling,linking. But it \"hangs\" somewhere in the middle of as well as TF 2.8.0 does.\r\nSo, still no go...", "Can i ask you to help me to compile python3.8 module for aarch64 architechture with GPU CUDA support, if you can help me with ? ", "@gmernov,\r\nYou can use binary package with GPU support\r\n`pip install tensorflow-aarch64`", "Linking large binaries is supposed to take a lot of time. Please also watch memory usage, in case it uses swap (which results in even more slowdown).", "> Linking large binaries is supposed to take a lot of time. Please also watch memory usage, in case it uses swap (which results in even more slowdown).\r\n\r\nYes, but the linking doesn't want to end. It produces no load. I don't think that it's gonna end while it does no load... I'm using 32 cores, and 40GB or RAM", "> @gmernov, You can use binary package with GPU support `pip install tensorflow-aarch64`\r\n\r\nThis build has a very dated version. I need something like 2.4-2.8 version for aarch64 with GPU CUDA support", "Trying to build TF 2.4.1 using bazel 3.1.0(built from sources as well). The same crap. hangs on linking... But now, i've enabled debug messages by --subcommands.\r\n\r\n![image](https://user-images.githubusercontent.com/4686690/161059874-ace48eb3-1abb-4923-bb28-1f741215b43c.png)\r\n\r\nThat's the system monitor, to see the \"load\" on the system: \r\n\r\n![image](https://user-images.githubusercontent.com/4686690/161059957-73f8e6e3-0786-47cf-9071-9f136750bf4b.png)\r\n", "Maybe it's not related exactly  for Tensorflow, but for linker. IDK. But i suffer this for multiple versions of tensorflow code.", "@gmernov,\r\nLatest release \r\n`pip install tensorflow-aarch64==2.8.0`", "> @gmernov, Latest release `pip install tensorflow-aarch64==2.8.0`\r\n\r\nDoes this version support CUDA ? I'm trying to build a version from sources because of i need tensorflow GPU support.", "@gmernov,\r\n`tensorflow-aarch64==2.8.0` it does support GPU. ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 54681, "title": "tf.saved_model.save is very slow if a tf.function contains an onnx model", "body": "I can successfully convert a pretrained Pytorch model into the onnx format and export to a Tensorflow proto file. I can load this file in Tensorflow, add some preprocessing/postprocessing steps and pack them into a `tf.function`. Everything works fine. Now it is very slow if I want to use `tf.saved_model.save` to save this new tf.function.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 4.19\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.7.11\r\n- CUDA/cuDNN version: 11.3\r\n- GPU model and memory: 2 NV TESLA K80 GPUs, 2x12GB \r\n\r\n**Describe the current behavior**\r\nIt took 500+ sec to save a simple resnet18, 7000+ sec to save a resnet34, and more than 40 hours to save a resnet50 in the following toy example.\r\nNot sure why this process is so slow or I did something wrong.\r\n\r\n**Describe the expected behavior**\r\nHopefully it could be faster.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport torch\r\nfrom torchvision.models import resnet18\r\nimport torch.onnx\r\n\r\nimport tensorflow as tf\r\n\r\nimport onnx\r\nfrom onnx_tf.backend import prepare\r\n\r\n# load a pytorch model\r\ntorch_model = resnet18(pretrained=True, progress=False)\r\n# save to onnx\r\nsave_dst = './torch_resnet18.onnx'\r\nx = torch.rand(1, 3, 224, 224)\r\ntorch.onnx.export(\r\n    torch_model, \r\n    x, \r\n    save_dst, \r\n    verbose=True,\r\n    opset_version=12,\r\n    export_params=True, \r\n    input_names = ['input'], \r\n    output_names = ['output'], \r\n    dynamic_axes={'input' : {0 : 'batch_size'}, 'output' : {0 : 'batch_size'}},\r\n    )\r\n# load and save to tf graph\r\nonnx_model = onnx.load(save_dst)\r\ntf_model = prepare(onnx_model)\r\ntf_model.export_graph('tf_resnet18')\r\n#%% modify\r\nclass TestModule(tf.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.t_model = tf.saved_model.load('tf_resnet18')\r\n        self.f = self.t_model.signatures['serving_default']\r\n\r\n    @tf.function(input_signature=[\r\n        tf.TensorSpec(shape=[None, 256, 256, 3], dtype=tf.float32),\r\n    ])\r\n    def __call__(self, x):\r\n        x = tf.image.resize(x, (224, 224))\r\n        x = tf.transpose(x, [0, 3, 1, 2])\r\n        return self.f(x)\r\n\r\nm = TestModule()\r\nx = tf.random.uniform((1, 256, 256, 3))\r\ny = m(x)\r\nprint(y)\r\n# it works fine above\r\ntf.saved_model.save(m, 'tf_new')\r\n# the last line takes a lot of time...\r\n```\r\n**Note**\r\nIt seems the program spent a lot of time on:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L1959\r\nwhich leads to a lower level implementation:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L1084", "comments": ["@chunduriv I was able to replicate the issue on colab using TF v2.8.0 and tf-nightly , please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/d68d8deabfc0a345bffdff3f52a8de37/54681.ipynb) for reference.Thanks! ", "Same problem here. ", "Never mind. In my specific case I just had to remove some if-elses form the Torch nn and make some operations more straighforward. Saving it in 3 seconds now. "]}, {"number": 54669, "title": "Replace deprecated tf.gradients with tf.GradientTape in custom_gradient.py", "body": "Updated tf.gradients with tf.GradientTape, as it was showing this error while executing code in TF 2.x \"RuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\"", "comments": ["@RenuPatelGoogle Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!", "> Please retitle the PR message (and in the future use a better commit message). See for example https://cbea.ms/git-commit/\r\n\r\nI tried to mention meaningful information now. Please check. ", "> @RenuPatelGoogle Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!\r\n\r\n@gbaned  Sure, Thank you!", "The title of the PR is unchanged"]}, {"number": 54659, "title": "TFLite conversion works in TF 2.8 / fails in 2.7.1, both produce large file sizes and incorrect input size of 1x1x1x3", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installation (pip package or built from source): pip install \r\n- TensorFlow library (version, if pip package or github SHA, if built from source): test 2.7.1 and 2.8\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\nTensorFlow 2.8\r\nhttps://colab.research.google.com/drive/1uYZOrPJWbSGfhf-DYEgMtCVmy5xLBE5G?usp=sharing\r\nTensorFlow 2.7.1\r\nhttps://colab.research.google.com/drive/15kzXMSF2olfzNskYN99WKq7DarUy-3tj?usp=sharing\r\n\r\n### 3. Failure after conversion\r\n\r\nUsed ssd_resnet101_v1_fpn_640x640_coco17_tpu-8 from model zoo to train a custom model since SSD models are supported in TF lite conversion. Exported the model using 'exporter_main_v2.py' from model zoo repo and the saved model was 16 Mb.\r\n\r\nAll TFLite models incorrectly have an input size of 1x1x1x3\r\n\r\nTF 2.8 converts all 3 different quantizations tested (see relevant colab)\r\nTF Lite Dynamic Range file size is 73.5 Mb\r\nTF Lite Int with float fallback file size is 267.7 Mb\r\nTF lite Full integer quantization file size is 271.2 Mb\r\n\r\nTF 2.7.1 integer quantization fails with\r\nFailed to quantize: <unknown>:0: error: loc(\"Postprocessor/Slice;Postprocessor/Slice\"): 'tfl.slice' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 2.5953195290639997E-4:-128> vs. !quant.uniform<i8:f32, 0.0011857558274641633:-128>\r\n\r\n### 5. (optional) Any other info / logs\r\n\r\nFor the large file size generation I have also commented over at #54405 as well but I created new issue for the incorrect input size and failure of conversion for 2.7.1\r\n\r\nI also tried using the 'export_tflite_graph_tf2.py' (not sure if that is what I should be using) to export my saved model which seems to correctly generate the input signature of 1,640,640,3 though I am not sure about the generated tflite yet but the converted tflite files are still 70 Mb.\r\n", "comments": ["@sachinprasadhs I was able to reproduce the issue on colab using TF v[2.7.1](https://colab.research.google.com/gist/sushreebarsa/af58b398f4910d5f3e66ed30831a8a13/tflite-conversion-with-tf-2-7-1.ipynb) and [2.8.0](https://colab.research.google.com/gist/sushreebarsa/f43a4b1e926a200bd38066d55ab57e6d/tflite-conversion.ipynb) .Please find the attached gists.Thanks!", "I also have a colab that allows one to easily test the conversion of the models available in the model zoo\r\nhttps://colab.research.google.com/drive/1nHWL9MaZ-em6gfKC8fBP4RnS9nQq4ZyP?usp=sharing\r\n\r\nIt allows you to quickly change the model name in the forms of tar.gz that are over at the model zoo along with the image input size and test that output of the conversion. Similar issue with the large file size can be seen there as well.\r\n\r\nThe 'export_tflite_graph_tf2.py' is used to export the graph as that seems to be the correct way to do it which also fixed the input signature as already mentioned.", "@cstamatopoulos I used the colab in your latest comment.\r\n1) I don't see the shape issue, can you explain how i can find it ?\r\n\r\n2) For the large files, i think you are missing few things:\r\n \r\n a) saved_model.pb stores the actual graph, but it doesn't store the variables or assets. Variables are stored in variables directory in the checkpoint format. \r\n TFLite have everything in the .tflite file, which means the variables values (either frozen or initialized as variables) are present inside the TFLite file.\r\n \r\n  b) TFLite converter optimizes the graph for inference latency, this means that in many cases if an op value can be inferred during conversion we will do this and fold the results as constant tensors in the graph to be used by dependent layers. This means that the amount of data in the file can be bigger than TF equivalent.\r\n  \r\n  Let me know if you have any questions.\r\n\r\nThanks ", "@karimnosseir The latest colab uses a different command to export the saved model.\r\nNamely \"export_tflite_graph_tf2.py\" vs \"exporter_main_v2.py\" from the model zoo.\r\nDocumentation is a bit unclear as the documentation for tf lite says that all that is needed a saved model and there are various posts about setting the fixed size and re-saving the model as I show below which does not work.\r\n\"export_tflite_graph_tf2.py\" seems to do the trick\r\n\r\nIf you notice in the first 2 colabs you will see that \r\n`'shape': array([1, 1, 1, 3]` \r\neven though I am saving the model as advised with an input size\r\n`model = tf.saved_model.load(tfmodel_path)\r\n        model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs[0].set_shape([1, 640, 640, 3])\r\n        tf.saved_model.save(model, \"saved_model_updated\", signatures=model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY])`\r\n\r\nI can provide the same input data to tf lite model maker and I get a model for efficientdet_lite3 that is around 12Mb.\r\nMaybe that is not fair comparison as as you say the size of the model variables is what matters.\r\nI was initially using TF 2.7.1 that was generating files that were 270 Mb.\r\nThen I tested 2.8 and it was 70 Mb so it made me skeptical.\r\n\r\n", "I see them now, thanks.\r\n\r\nSo this is working as expected. Let me explain the output to you\r\n```\r\n{'name': 'serving_default_input_tensor:0', 'index': 0, 'shape': array([1, 1, 1, 3], dtype=int32), 'shape_signature': array([ 1, -1, -1,  3], dtype=int32)\r\n```\r\n\r\nThere are 2 parts that have shape, shape and shape_signature\r\n- shape: Is the default shape of the tensor, IOW if you just decided to load the model and run this is the shape of the tensor.\r\nSo what is shape_signature then\r\n- shape_signature: Is similar to shape but instead might have some dimensions labeled with -1, these dimensions are the dimensions that are dynamic and can be resized.\r\n\r\nThat means that the model you are converting has shape [1, None, None, 3] and TFLite honor this and you can resize the 'None' dimensions at runtime.\r\n\r\nHope it is more clear now.\r\n\r\nThanks\r\n"]}, {"number": 54650, "title": "[TF:TRT] Add in-memory timing cache registry", "body": "Adds the usage of TensorRT autotuner timing cache to TF-TRT. Serialized timing caches are tracked using a global registry using a string ID. This change updates the converter to use a single timing cache, `default_cache`, but we could qualify timing caches based on configuration attributes (dynamic shape mode, precision mode, etc). This change speeds up converter unit tests by about 20% and will likely speed up TF-TRT-based services performing multiple model builds by a larger factor.", "comments": ["Depends on #53760 . \r\n\r\n@bixia1 for review when available. Thanks!", "> Adds the usage of TensorRT autotuner timing cache to TF-TRT. Serialized timing caches are tracked using a global registry using a string ID. This change updates the converter to use a single timing cache, `default_cache`, but we could qualify timing caches based on configuration attributes (dynamic shape mode, precision mode, etc). This change speeds up converter unit tests by about 20% and will likely speed up TF-TRT-based services performing multiple model builds by a larger factor.\r\n\r\nAccording to [this document](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_timing_cache.html), the timing cache records the timing data for tactic selection. I wonder why does TensorRT expects an application to manipulate timing cache as we do in this PR. What would be the reason that TensorRT doesn't include this logic in its implementation? Is it possible for TensorRT to set up a \"name ID\" for the time cache, and allow users to overwrite the default value?", "> I wonder why does TensorRT expects an application to manipulate timing cache as we do in this PR. What would be the reason that TensorRT doesn't include this logic in its implementation? Is it possible for TensorRT to set up a \"name ID\" for the time cache, and allow users to overwrite the default value?\r\n\r\nTo summarize here on what we discussed this week, the naming doesn't have anything to do with TensorRT. There was a question about whether sizes of inputs/outputs are taken into account when caching timings, and the answer is that they are (according to the TensorRT release notes). I see little risk with respect to performance by having a default in-memory cache that is used across builds within the same process ", "rebased and addressed comments / waiting on further discussion", "I understand how this PR may speed up the BUILD of multiple models, but for inference, we actually want to get the best performance for the engines and sharing a single auto tuning cache can hurt performance (because except for one engine, the rest engines aren't using their true input for auto tune?).\r\nWhat would you said about this?", "> I understand how this PR may speed up the BUILD of multiple models, but for inference, we actually want to get the best performance for the engines and sharing a single auto tuning cache can hurt performance (because except for one engine, the rest engines aren't using their true input for auto tune?).\r\nWhat would you said about this?\r\n\r\nI tried to address that above:\r\n\r\n> There was a question about whether sizes of inputs/outputs are taken into account when caching timings, and the answer is that they are\r\n\r\nSo if by \"true input\" you mean the size and format of a tensor, then we can expect that both engines will receive accurate timing values and one won't unnecessarily cause false results in the other's use of the cache.\r\n", "This pr says changes were requested, but nothing has been requested as far as I can see. @bixia1 ?", "@christopherbate Can you please address Ubuntu Sanity errors? Thank you!", "Fixed", "Build errors:\r\nIn file included from [third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:42](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc?l=42&ws=tap-prod-presubmit/180190443&snapshot=2):\r\n[./third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.h:35](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.h?l=35&ws=tap-prod-presubmit/180190443&snapshot=2):52: error: no member named 'ITimingCache' in namespace 'nvinfer1'\r\n  using TimingCachePtr = std::unique_ptr<nvinfer1::ITimingCache>;\r\n                                         ~~~~~~~~~~^\r\n[./third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.h:39](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.h?l=39&ws=tap-prod-presubmit/180190443&snapshot=2):45: error: no type named 'ITimingCache' in namespace 'nvinfer1'\r\n  void Upsert(const string& name, nvinfer1::ITimingCache* cache);\r\n                                  ~~~~~~~~~~^\r\n[./third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.h:44](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.h?l=44&ws=tap-prod-presubmit/180190443&snapshot=2):12: error: use of undeclared identifier 'TimingCachePtr'\r\n  StatusOr<TimingCachePtr> LookUp(const string& name,\r\n           ^\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:1249](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc?l=1249&ws=tap-prod-presubmit/180190443&snapshot=2):29: error: no member named 'ITimingCache' in namespace 'nvinfer1'\r\n  std::unique_ptr<nvinfer1::ITimingCache> timing_cache = nullptr;\r\n                  ~~~~~~~~~~^\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:1259](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc?l=1259&ws=tap-prod-presubmit/180190443&snapshot=2):23: error: no member named 'setTimingCache' in 'nvinfer1::IBuilderConfig'\r\n      builder_config->setTimingCache(*timing_cache, /*ignoreMismatch*/ false);\r\n      ~~~~~~~~~~~~~~~~^\r\n5 errors generated.\r\n\r\nIn file included from [third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc:17](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc?l=17&ws=tap-prod-presubmit/180190443&snapshot=2):\r\n[./third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.h:35](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.h?l=35&ws=tap-prod-presubmit/180190443&snapshot=2):52: error: no member named 'ITimingCache' in namespace 'nvinfer1'\r\n  using TimingCachePtr = std::unique_ptr<nvinfer1::ITimingCache>;\r\n                                         ~~~~~~~~~~^\r\n[./third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.h:39](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.h?l=39&ws=tap-prod-presubmit/180190443&snapshot=2):45: error: no type named 'ITimingCache' in namespace 'nvinfer1'\r\n  void Upsert(const string& name, nvinfer1::ITimingCache* cache);\r\n                                  ~~~~~~~~~~^\r\n[./third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.h:44](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.h?l=44&ws=tap-prod-presubmit/180190443&snapshot=2):12: error: use of undeclared identifier 'TimingCachePtr'\r\n  StatusOr<TimingCachePtr> LookUp(const string& name,\r\n           ^\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc:29](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc?l=29&ws=tap-prod-presubmit/180190443&snapshot=2):31: error: no member named 'TimingCachePtr' in 'tensorflow::tensorrt::convert::TimingCacheRegistry'\r\nStatusOr<TimingCacheRegistry::TimingCachePtr> TimingCacheRegistry::LookUp(\r\n         ~~~~~~~~~~~~~~~~~~~~~^\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc:31](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc?l=31&ws=tap-prod-presubmit/180190443&snapshot=2):3: error: no viable conversion from returned value of type '::tensorflow::Status' to function return type 'int'\r\n  TRT_ENSURE(builder_config != nullptr);\r\n  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n[./third_party/tensorflow/compiler/tf2tensorrt/common/utils.h:62](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/common/utils.h?l=62&ws=tap-prod-presubmit/180190443&snapshot=2):12: note: expanded from macro 'TRT_ENSURE'\r\n    return errors::Internal(__FILE__, \":\", __LINE__, \" TRT_ENSURE failure\"); \\\r\n           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc:35](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc?l=35&ws=tap-prod-presubmit/180190443&snapshot=2):38: error: no member named 'ITimingCache' in namespace 'nvinfer1'\r\n    return std::unique_ptr<nvinfer1::ITimingCache>(\r\n                           ~~~~~~~~~~^\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc:36](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc?l=36&ws=tap-prod-presubmit/180190443&snapshot=2):25: error: no member named 'createTimingCache' in 'nvinfer1::IBuilderConfig'\r\n        builder_config->createTimingCache(data.data(), data.size()));\r\n        ~~~~~~~~~~~~~~  ^\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc:40](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc?l=40&ws=tap-prod-presubmit/180190443&snapshot=2):36: error: no member named 'ITimingCache' in namespace 'nvinfer1'\r\n  return std::unique_ptr<nvinfer1::ITimingCache>(\r\n                         ~~~~~~~~~~^\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc:41](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc?l=41&ws=tap-prod-presubmit/180190443&snapshot=2):23: error: no member named 'createTimingCache' in 'nvinfer1::IBuilderConfig'\r\n      builder_config->createTimingCache(nullptr, 0));\r\n      ~~~~~~~~~~~~~~  ^\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc:45](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/timing_cache.cc?l=45&ws=tap-prod-presubmit/180190443&snapshot=2):44: error: no type named 'ITimingCache' in namespace 'nvinfer1'\r\n                                 nvinfer1::ITimingCache* cache) {\r\n                                 ~~~~~~~~~~^", "Thanks, must be some trt version difference I didn't catch. I will check and get back to you. Sorry for the inconvenience.", "@christopherbate Any update on this PR? Please. Thank you!"]}, {"number": 54645, "title": "Allow constant folding to remove add instructions used by compares", "body": "If we have a compare with a constant on the RHS and and add(op, constant) on the left, then move the LHS constant to the right hand side so that constant folding can eliminate the add instruction.\r\n\r\nWe could do the removing of the addition manually by modifying the constants literal but for the sake of \r\navoiding code duplication I think it's better to let the constant folding pass to remove the subtraction\r\n\r\nEliminating the this pattern allows the pattern matching in the while-loop-trip-count-annotator to hit some extra cases.", "comments": ["@r4nt Can you please review this PR ? Thank you!"]}, {"number": 54644, "title": "Simplify and/or instructions with identical operands", "body": "And/Or instructions with identical operands are no-ops so remove them, and replace with their input. \r\n\r\nThis optimisation allows the pattern matching in the while-loop-trip-count-annotator to hit some cases that are missed in a model I'm running.", "comments": ["@r4nt  Can you please review this PR ? Thank you!"]}, {"number": 54566, "title": "fix cmake build error for tflite c library", "body": "Fix cmake error when building tflite C library:\r\n\r\n```\r\nCMake Error at CMakeLists.txt:63 (add_library):\r\n  Cannot find source file:\r\n\r\n    common.c\r\n\r\n```", "comments": ["Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nFor more information, open the [CLA check for this pull request](https://github.com/tensorflow/tensorflow/pull/54566/checks?check_run_id=5328468213).", "@miaout17 Can you please review this PR ? Thank you!"]}, {"number": 54559, "title": "TFLite Converter: how to skip Dequantize node in FP16 models for GPU Delegate?", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Yocto dunfell \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: aarch64\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.6.1\r\n- Python version: 3.9.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Mali G72\r\n\r\n\r\n**Describe the current behavior**\r\nI converted mobilenet_v2_1.0_224 [https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz](url) to fp16 tflite model using the steps mentioned here [https://www.tensorflow.org/lite/performance/post_training_quantization#float16_quantization](url)\r\n\r\nthe converted mobilenet_v2_fp16.tflite model has multiple Dequantize steps inserted within which seem to convert float16 to float32, even though the model is supposed to be quantized to float16.\r\nBelow is a snippet of the model visualization from netron. The inputs to the converted model are float32 and its outputs are also float32. \r\n \r\n![test](https://user-images.githubusercontent.com/78979784/155630251-9e58343d-95a9-420c-8101-86fed9165372.PNG)\r\n\r\n\r\nAll the dequantize nodes have float16 input and float32 output. Node 0 dequantize node properties are visualized below:\r\n\r\n![image](https://user-images.githubusercontent.com/78979784/155630719-1c989e54-22d8-4423-846f-93eb911c22ff.png)\r\n\r\nall dequantize nodes have similar properties where the inputs to the layer comprise of only weights and/or bias. \r\n\r\nnow according to the official documentation, _\"By default, a float16 quantized model will \"dequantize\" the weights values to float32 when run on the CPU. (Note that the GPU delegate will not perform this dequantization, since it can operate on float16 data.)\"_ This means dequantize step is added just for cpu execution and gpu will not perform or override this node. But the question I have is, the dequantize node is already present, can a delegate simply skip a node in a model architecture during execution runtime, since the node is a part of the model graph and every node is executed with no op delegated to CPU? \r\n\r\n\r\nWhat is the main use of this dequantize node? And why is a dequantization step inserted during fp16 quantization? Is there a way to skip these dequantize nodes in the GPU delegate during post-training float16 quantization itself, for I want to run the model only on GPU and not CPU? The node seems to be executed by GPU delegate, as none of the actions are delegated to the CPU at runtime. Or are they being simply ignored by the GPU delegate?\r\n\r\nmobilenet_v2_fp16.tflite model is attached for your reference.\r\n[mobilenet_v2_fp16.zip](https://github.com/tensorflow/tensorflow/files/8137542/mobilenet_v2_fp16.zip)\r\n \r\nthanks\r\n\r\n\r\n\r\n", "comments": ["same question. I want to convert this quantized fp16 model using snpe-tflite-to-dlc and keep a bitwidth=16 structure. Thus, I would expect all the ops in .tflite to be directly executable on 16-precision", "Same question, any update?"]}, {"number": 54519, "title": "Added LZ4 support for TFRecordWriter", "body": "Related to https://github.com/tensorflow/tensorflow/pull/53385\r\n\r\nThe current API of `TFRecordOptions` only supports:\r\n- no compression\r\n- GZIP compression\r\n- ZLIB compression\r\n\r\nWith this PR, I have added support for a 4th compression algorithm, LZ4: https://github.com/lz4/lz4\r\n\r\nThe API for `TFRecordOptions` is therefore updated to accept the following parameters:\r\n\r\n```\r\n      compression_type: `\"GZIP\"`, `\"ZLIB\"`, `\"LZ4\"` or `\"\"` (no compression).\r\n      TODO\r\n```\r\n\r\nMaintaining backward compatibility with the current API.", "comments": ["@IAL32 This PR is in draft, any update on this? Please. Thank you!", "> @IAL32 This PR is in draft, any update on this? Please. Thank you!\r\n\r\nHi @gbaned , still no update on this from my side. As in https://github.com/tensorflow/tensorflow/pull/53385 it might take some time for me to get back to this."]}, {"number": 54517, "title": "Tensorflow-lite with Flex Delegates build fails for aarch64 source architecture using TF2.7", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 583dc6ac55ff 5.4.144+\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.7.0 \r\n- Python version: 3.7.12\r\n- Bazel version (if compiling from source): 3.7.2\r\n\r\n**Describe the current behavior**\r\nI'm trying to build a shared library for tensorflow-lite including the Flex Delegate by Modifying the BUILD to add following dependency:\r\n```\r\n\"//tensorflow/lite/delegates/flex:delegate\",\r\n```\r\nSince I'm targeting a Linux system running on my aarch64 board, the command I ran is the below:\r\n```\r\nbazel build --config=monolithic --config=noaws --config=nogcp --config=nohdfs \\\r\n --config=nonccl --config=elinux_aarch64 \\\r\n --experimental_ui_max_stdouterr_bytes=1073741819 -c opt --cxxopt=--std=c++14 \\\r\n //tensorflow/lite:libtensorflowlite.so\r\n```\r\nI also made sure to hide my OpenSSL from my /usr/include as suggested in the following issue https://github.com/tensorflow/tensorflow/issues/48401. I noticed that the build fails at compiling **icu** external source code.\r\n\r\n**Describe the expected behavior**\r\nThe expected behavior is a build completed successfully and a shared library libtensorflow-lite.so generated and supporting Flex Delegates with TF selected Ops.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Here](https://colab.research.google.com/drive/1Re6cVzm0mZBrbNYsH2o9WEtin1YTMnsE?usp=sharing) is a Google Colab gist to reproduce the Build issue. All you need to do is to modify the tensorflow/lite/BUILD by adding the following line to the libtensorflow-lite.so deps. \r\n```\r\n\"//tensorflow/lite/delegates/flex:delegate\",\r\n```\r\n\r\n**Other info / logs** \r\nHere is the full traceback of the error that shows in the middle of the build and makes it fail.\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/889612a75a81b3d8b4ed860522ba4e34/external/icu/BUILD.bazel:33:11: C++ compilation of rule '@icu//:icuuc' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command /root/.cache/bazel/_bazel_root/889612a75a81b3d8b4ed860522ba4e34/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections ... (remaining 41 argument(s) skipped)\r\nexternal/icu/icu4c/source/common/localebuilder.cpp: In member function 'icu_60::LocaleBuilder& icu_60::LocaleBuilder::setLanguageTag(icu_60::StringPiece)':\r\nexternal/icu/icu4c/source/common/localebuilder.cpp:63:24: error: 'forLanguageTag' is not a member of 'icu_60::Locale'\r\n     Locale l = Locale::forLanguageTag(tag, status_);\r\n                        ^~~~~~~~~~~~~~\r\nexternal/icu/icu4c/source/common/localebuilder.cpp: In function 'void icu_60::_copyExtensions(const icu_60::Locale&, icu_60::Locale*, bool, UErrorCode&)':\r\nexternal/icu/icu4c/source/common/localebuilder.cpp:166:23: error: invalid use of incomplete type 'class icu_60::StringEnumeration'\r\n     while ((key = iter->next(nullptr, errorCode)) != nullptr) {\r\n                       ^~\r\nIn file included from external/icu/icu4c/source/common/unicode/uloc.h:27,\r\n                 from external/icu/icu4c/source/common/ulocimp.h:14,\r\n                 from external/icu/icu4c/source/common/localebuilder.cpp:9:\r\n/usr/include/unicode/uenum.h:27:7: note: forward declaration of 'class icu_60::StringEnumeration'\r\n class StringEnumeration;\r\n       ^~~~~~~~~~~~~~~~~\r\nexternal/icu/icu4c/source/common/localebuilder.cpp:169:50: error: no matching function for call to 'icu_60::Locale::getKeywordValue(const char*&, icu_60::CharStringByteSink&, UErrorCode&) const'\r\n         from.getKeywordValue(key, sink, errorCode);\r\n                                                  ^\r\nIn file included from external/icu/icu4c/source/common/unicode/localebuilder.h:6,\r\n                 from external/icu/icu4c/source/common/localebuilder.cpp:10:\r\n/usr/include/unicode/locid.h:457:13: note: candidate: 'int32_t icu_60::Locale::getKeywordValue(const char*, char*, int32_t, UErrorCode&) const'\r\n     int32_t getKeywordValue(const char* keywordName, char *buffer, int32_t bufferCapacity, UErrorCode &status) const;\r\n             ^~~~~~~~~~~~~~~\r\n/usr/include/unicode/locid.h:457:13: note:   candidate expects 4 arguments, 3 provided\r\nexternal/icu/icu4c/source/common/localebuilder.cpp: In function 'void icu_60::_clearUAttributesAndKeyType(icu_60::Locale*, UErrorCode&)':\r\nexternal/icu/icu4c/source/common/localebuilder.cpp:191:55: error: 'class icu_60::Locale' has no member named 'createUnicodeKeywords'; did you mean 'createKeywords'?\r\n     LocalPointer<icu::StringEnumeration> iter(locale->createUnicodeKeywords(errorCode));\r\n                                                       ^~~~~~~~~~~~~~~~~~~~~\r\n                                                       createKeywords\r\nexternal/icu/icu4c/source/common/localebuilder.cpp:194:23: error: invalid use of incomplete type 'class icu_60::StringEnumeration'\r\n     while ((key = iter->next(nullptr, errorCode)) != nullptr) {\r\n                       ^~\r\nIn file included from external/icu/icu4c/source/common/unicode/uloc.h:27,\r\n                 from external/icu/icu4c/source/common/ulocimp.h:14,\r\n                 from external/icu/icu4c/source/common/localebuilder.cpp:9:\r\n/usr/include/unicode/uenum.h:27:7: note: forward declaration of 'class icu_60::StringEnumeration'\r\n class StringEnumeration;\r\n       ^~~~~~~~~~~~~~~~~\r\nexternal/icu/icu4c/source/common/localebuilder.cpp:195:17: error: 'class icu_60::Locale' has no member named 'setUnicodeKeywordValue'; did you mean 'setKeywordValue'?\r\n         locale->setUnicodeKeywordValue(key, nullptr, errorCode);\r\n                 ^~~~~~~~~~~~~~~~~~~~~~\r\n                 setKeywordValue\r\nexternal/icu/icu4c/source/common/localebuilder.cpp: In function 'void icu_60::_setUnicodeExtensions(icu_60::Locale*, const icu_60::CharString&, UErrorCode&)':\r\nexternal/icu/icu4c/source/common/localebuilder.cpp:206:17: error: 'forLanguageTag' is not a member of 'icu_60::Locale'\r\n         Locale::forLanguageTag(locale_str.data(), errorCode),\r\n                 ^~~~~~~~~~~~~~\r\nexternal/icu/icu4c/source/common/localebuilder.cpp: In member function 'icu_60::LocaleBuilder& icu_60::LocaleBuilder::setExtension(char, icu_60::StringPiece)':\r\nexternal/icu/icu4c/source/common/localebuilder.cpp:235:45: error: no matching function for call to 'icu_60::Locale::setKeywordValue(icu_60::StringPiece, char*, UErrorCode&)'\r\n                                      status_);\r\n                                             ^\r\nIn file included from external/icu/icu4c/source/common/unicode/localebuilder.h:6,\r\n                 from external/icu/icu4c/source/common/localebuilder.cpp:10:\r\n/usr/include/unicode/locid.h:473:10: note: candidate: 'void icu_60::Locale::setKeywordValue(const char*, const char*, UErrorCode&)'\r\n     void setKeywordValue(const char* keywordName, const char* keywordValue, UErrorCode &status);\r\n          ^~~~~~~~~~~~~~~\r\n/usr/include/unicode/locid.h:473:10: note:   no known conversion for argument 1 from 'icu_60::StringPiece' to 'const char*'\r\nexternal/icu/icu4c/source/common/localebuilder.cpp: In member function 'icu_60::LocaleBuilder& icu_60::LocaleBuilder::setUnicodeLocaleKeyword(icu_60::StringPiece, icu_60::StringPiece)':\r\nexternal/icu/icu4c/source/common/localebuilder.cpp:263:18: error: 'class icu_60::Locale' has no member named 'setUnicodeKeywordValue'; did you mean 'setKeywordValue'?\r\n     extensions_->setUnicodeKeywordValue(key, type, status_);\r\n                  ^~~~~~~~~~~~~~~~~~~~~~\r\n                  setKeywordValue\r\nexternal/icu/icu4c/source/common/localebuilder.cpp: In member function 'icu_60::LocaleBuilder& icu_60::LocaleBuilder::addUnicodeLocaleAttribute(icu_60::StringPiece)':\r\nexternal/icu/icu4c/source/common/localebuilder.cpp:290:69: error: no matching function for call to 'icu_60::Locale::getKeywordValue(const char*&, icu_60::CharStringByteSink&, UErrorCode&)'\r\n     extensions_->getKeywordValue(kAttributeKey, sink, localErrorCode);\r\n                                                                     ^\r\nIn file included from external/icu/icu4c/source/common/unicode/localebuilder.h:6,\r\n                 from external/icu/icu4c/source/common/localebuilder.cpp:10:\r\n/usr/include/unicode/locid.h:457:13: note: candidate: 'int32_t icu_60::Locale::getKeywordValue(const char*, char*, int32_t, UErrorCode&) const'\r\n     int32_t getKeywordValue(const char* keywordName, char *buffer, int32_t bufferCapacity, UErrorCode &status) const;\r\n             ^~~~~~~~~~~~~~~\r\n/usr/include/unicode/locid.h:457:13: note:   candidate expects 4 arguments, 3 provided\r\nexternal/icu/icu4c/source/common/localebuilder.cpp: In member function 'icu_60::LocaleBuilder& icu_60::LocaleBuilder::removeUnicodeLocaleAttribute(icu_60::StringPiece)':\r\nexternal/icu/icu4c/source/common/localebuilder.cpp:344:69: error: no matching function for call to 'icu_60::Locale::getKeywordValue(const char*&, icu_60::CharStringByteSink&, UErrorCode&)'\r\n     extensions_->getKeywordValue(kAttributeKey, sink, localErrorCode);\r\n                                                                     ^\r\nIn file included from external/icu/icu4c/source/common/unicode/localebuilder.h:6,\r\n                 from external/icu/icu4c/source/common/localebuilder.cpp:10:\r\n/usr/include/unicode/locid.h:457:13: note: candidate: 'int32_t icu_60::Locale::getKeywordValue(const char*, char*, int32_t, UErrorCode&) const'\r\n     int32_t getKeywordValue(const char* keywordName, char *buffer, int32_t bufferCapacity, UErrorCode &status) const;\r\n             ^~~~~~~~~~~~~~~\r\n/usr/include/unicode/locid.h:457:13: note:   candidate expects 4 arguments, 3 provided\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 11557.297s, Critical Path: 288.22s\r\nINFO: 5169 processes: 1657 internal, 3512 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Hello @teijeong \r\nHave you been able to take a look at this ?", "Hello @teijeong @jvishnuvardhan @mohantym,\r\nHas anyone took a look at this issue ? is it about some missing arguments ?\r\nThank you for your cooperation.\r\nOthmane,", "I've verified the issue. Will take a look."]}, {"number": 54510, "title": "Feature/device-annotation", "body": "The purpose of this pr is to replicate `colocate_gradients_with_ops` option to `tf.gradients` in 1.x to 2.x. ", "comments": ["Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nFor more information, open the [CLA check for this pull request](https://github.com/tensorflow/tensorflow/pull/54510/checks?check_run_id=5313728199).", "please refer to this [https://github.com/tensorflow/tensorflow/issues/54460](url) feature request post for more details"]}, {"number": 54501, "title": "Native irfft (and inverse_stft) ops in TFLite", "body": "**System information**\r\n- TensorFlow version (you are using): 2.8.0 (Python 3.9.10)\r\n- Are you willing to contribute it (Yes/No): No (or rather yes, but probably with a lot of help to find my way around)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nNow that TFLite supports natively ops for tf.signal.rfft and tf.signal.stft (see [#27030](https://github.com/tensorflow/tensorflow/issues/27303)), it would be really great to close the loop and add support for their inverse functions tf.signal.irfft and tf.signal.inverse_stft to support more broadly 1-dimensional data models.\r\n\r\nRight now, here is what happens with this MWE:\r\n```import os\r\nimport tensorflow as tf\r\nimport tensorflow.keras as tfk\r\nimport tensorflow.keras.layers as tfkl\r\n\r\nN = 128\r\nfft_length = 1024\r\n\r\ninput = tfk.Input((N, fft_length // 2 + 1), dtype=tf.dtypes.complex64)\r\noutput = tfkl.Lambda(lambda x: tf.signal.irfft(x), output_shape=(N, fft_length))(input)\r\n\r\nmodel = tfk.Model(inputs=input, outputs=output)\r\n\r\ntf.get_logger().warning(\"Begin Tensorflow test...\")\r\ntest_input = tf.complex(\r\n    tf.random.normal((1, N, fft_length // 2 + 1), dtype=tf.dtypes.float32),\r\n    tf.random.normal((1, N, fft_length // 2 + 1), dtype=tf.dtypes.float32))    \r\ntest_output = model(test_input)\r\ntf.get_logger().warning(\"Tensorflow test finished...\")\r\n\r\ntf.get_logger().warning(\"Begin TFLite conversion...\")\r\nmodel.save('./tflite')\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('./tflite')\r\ntflite_model = converter.convert()\r\nwith open(os.path.join('./tflite', 'model.tflite'), \"wb\") as f:\r\n    f.write(tflite_model)\r\ntf.get_logger().warning(\"TFLite conversion finished...\") \r\n```\r\n\r\nOutput:\r\n```\r\n2022-02-23 17:05:01.840430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-23 17:05:02.447990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-23 17:05:02.448941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-23 17:05:02.450203: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-02-23 17:05:02.450930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-23 17:05:02.451762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-23 17:05:02.452562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-23 17:05:03.149397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-23 17:05:03.150348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-23 17:05:03.151175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-23 17:05:03.152004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10794 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\r\nWARNING:tensorflow:Begin Tensorflow test...\r\nWARNING:tensorflow:Tensorflow test finished...\r\nWARNING:tensorflow:Begin TFLite conversion...\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\n2022-02-23 17:05:08.281960: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n2022-02-23 17:05:08.542623: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\r\n2022-02-23 17:05:08.542684: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\r\n2022-02-23 17:05:08.544198: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: ./tflite\r\n2022-02-23 17:05:08.546087: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\r\n2022-02-23 17:05:08.546141: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: ./tflite\r\n2022-02-23 17:05:08.549053: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\r\n2022-02-23 17:05:08.587791: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: ./tflite\r\n2022-02-23 17:05:08.594120: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 49927 microseconds.\r\n2022-02-23 17:05:08.602500: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nloc(callsite(callsite(fused[\"IRFFT:\", \"model/lambda/irfft@__inference__wrapped_model_39\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_138\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): error: 'tf.IRFFT' op is neither a custom op nor a flex op\r\nerror: failed while converting: 'main':\r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select\r\nTF Select ops: IRFFT\r\nDetails:\r\n\ttf.IRFFT(tensor<?x128x513xcomplex<f32>>, tensor<1xi32>) -> (tensor<?x128x1024xf32>) : {device = \"\"}\r\n\r\nTraceback (most recent call last):\r\n  File \"/fsx/findsimilar-experiments/tflite_irfft_test.py\", line 24, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/lite.py\", line 803, in wrapper\r\n    return self._convert_and_export_metrics(convert_func, *args, **kwargs)\r\n  File \"/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/lite.py\", line 789, in _convert_and_export_metrics\r\n    result = convert_func(self, *args, **kwargs)\r\n  File \"/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/lite.py\", line 1084, in convert\r\n    return self._convert_from_saved_model(graph_def)\r\n  File \"/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/lite.py\", line 967, in _convert_from_saved_model\r\n    result = _convert_saved_model(**converter_kwargs)\r\n  File \"/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py\", line 213, in wrapper\r\n    raise converter_error from None  # Re-throws the exception.\r\n  File \"/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py\", line 206, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/convert.py\", line 789, in convert_saved_model\r\n    data = convert(\r\n  File \"/fsx/findsimilar-experiments/venv-newtf28/lib/python3.9/site-packages/tensorflow/lite/python/convert.py\", line 306, in convert\r\n    raise converter_error\r\ntensorflow.lite.python.convert_phase.ConverterError: <unknown>:0: error: loc(callsite(callsite(fused[\"IRFFT:\", \"model/lambda/irfft@__inference__wrapped_model_39\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_138\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.IRFFT' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\r\n<unknown>:0: note: loc(callsite(callsite(fused[\"IRFFT:\", \"model/lambda/irfft@__inference__wrapped_model_39\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_138\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: failed while converting: 'main':\r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select\r\nTF Select ops: IRFFT\r\nDetails:\r\n\ttf.IRFFT(tensor<?x128x513xcomplex<f32>>, tensor<1xi32>) -> (tensor<?x128x1024xf32>) : {device = \"\"}\r\n```\r\n\r\nThe TF Select workaround mentioned in the error message does seem to work, so the community may have a workaround in the meantime (with all the caveats mentioned in [https://www.tensorflow.org/lite/guide/ops_select](https://www.tensorflow.org/lite/guide/ops_select)), but I think it'd be in the best interest of the community to have the inverse operators supported natively whenever possible, for a much facilitated deployment of models.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNot that I can think of, but maybe I don't fully understand the question.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEvery TensorFlow user needing to deploy a TFLite model that relies on STFT processing, so that could cover many people working on projects involving audio data, financial data, or any other form of 1-dimensional data series.\r\n\r\n**Any Other info**\r\n\r\nNothing. Just thanks to the community for keeping TensorFlow running and growing \ud83d\udc4d ", "comments": ["Hi @francoisgermain ! Putting resolved [gist ](https://colab.sandbox.google.com/gist/mohantym/97ca09bd67d0bc3a1ec51af41cf84a83/github_54501.ipynb)as a workaround for now. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thanks for posting the gist of the workaround @mohantym. As I indicated, I hope it will be possible to bring them into the officially supported ops soon enough. And if I can be of any help to that, happy to help, but I'll probably need a few pointers on where to start. Thanks again."]}, {"number": 54496, "title": "[TFLite] memory cost is much higher when enable xnnpack delegate", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: OPPO Find X3Pro\r\n\r\nDear\r\nI build TFLite benchmark_model from source code, based on commitID: 965c39fdf304a80eacd6cdca43241956084301c3 (CommitDate: Mon Feb 21 15:37:46 2022 -0800)\r\n\r\nbuild command: \r\n```\r\nbazel build -c opt --config=android_arm64 --define tflite_with_xnnpack=true --define tflite_with_xnnpack_qs8=true --define tflite_with_xnnpack_qu8=true tensorflow/lite/tools/benchmark:benchmark_model\r\n```\r\n\r\nthen I run benchmark_model on my Android Phone, I test with model  https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_quant_and_labels.zip\r\n\r\nwhen disable xnnpack, the memory cost is 8.8MB, see below:\r\nbuild command: \r\n```\r\nOP:/data/local/tmp # ./benchmark_model --graph=mobilenet_v1_1.0_224_quant.tflite --num_threads=4 --use_xnnpack=false\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nNum threads: [4]\r\nGraph: [mobilenet_v1_1.0_224_quant.tflite]\r\n#threads used for CPU inference: [4]\r\nUse xnnpack: [0]\r\nLoaded model mobilenet_v1_1.0_224_quant.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nThe input model file size (MB): 4.27635\r\nInitialized session in 1.004ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=43 first=22701 curr=10983 min=10906 max=22701 avg=11638.5 std=1768\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=91 first=11004 curr=10970 min=10864 max=11745 avg=11008.7 std=105\r\n\r\nInference timings in us: Init: 1004, First inference: 22701, Warmup (avg): 11638.5, Inference (avg): 11008.7\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nMemory footprint delta from the start of the tool (MB): init=3.32422 overall=8.83984\r\n```\r\n\r\nWhen enable xnnpack, the memory cost is 13.8MB, see below:\r\n```\r\nOP:/data/local/tmp # ./benchmark_model --graph=mobilenet_v1_1.0_224_quant.tflite --num_threads=4 --use_xnnpack=true\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nNum threads: [4]\r\nGraph: [mobilenet_v1_1.0_224_quant.tflite]\r\n#threads used for CPU inference: [4]\r\nUse xnnpack: [1]\r\nLoaded model mobilenet_v1_1.0_224_quant.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nXNNPACK delegate created.\r\nINFO: Replacing 28 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 4 partitions.\r\nExplicitly applied XNNPACK delegate, and the model graph will be partially executed by the delegate w/ 2 delegate kernels.\r\nThe input model file size (MB): 4.27635\r\nInitialized session in 14.79ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=66 first=13307 curr=7452 min=7431 max=13307 avg=7638.41 std=719\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=133 first=7588 curr=7541 min=7445 max=8710 avg=7556.99 std=159\r\n\r\nInference timings in us: Init: 14790, First inference: 13307, Warmup (avg): 7638.41, Inference (avg): 7556.99\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nMemory footprint delta from the start of the tool (MB): init=11.2891 overall=13.8906\r\n```\r\n\r\nCould you please help check this ?", "comments": ["> **System information**\r\n> \r\n> * Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: OPPO Find X3Pro\r\n> \r\n> Dear I build TFLite benchmark_model from source code, based on commitID: [965c39f](https://github.com/tensorflow/tensorflow/commit/965c39fdf304a80eacd6cdca43241956084301c3) (CommitDate: Mon Feb 21 15:37:46 2022 -0800)\r\n> \r\n> build command:\r\n> \r\n> ```\r\n> bazel build -c opt --config=android_arm64 --define tflite_with_xnnpack=true --define tflite_with_xnnpack_qs8=true --define tflite_with_xnnpack_qu8=true tensorflow/lite/tools/benchmark:benchmark_model\r\n> ```\r\n> \r\n> then I run benchmark_model on my Android Phone, I test with model https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_quant_and_labels.zip\r\n> \r\n> when disable xnnpack, the memory cost is 8.8MB, see below: build command:\r\n> \r\n> ```\r\n> OP:/data/local/tmp # ./benchmark_model --graph=mobilenet_v1_1.0_224_quant.tflite --num_threads=4 --use_xnnpack=false\r\n> STARTING!\r\n> Log parameter values verbosely: [0]\r\n> Num threads: [4]\r\n> Graph: [mobilenet_v1_1.0_224_quant.tflite]\r\n> #threads used for CPU inference: [4]\r\n> Use xnnpack: [0]\r\n> Loaded model mobilenet_v1_1.0_224_quant.tflite\r\n> INFO: Initialized TensorFlow Lite runtime.\r\n> The input model file size (MB): 4.27635\r\n> Initialized session in 1.004ms.\r\n> Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n> count=43 first=22701 curr=10983 min=10906 max=22701 avg=11638.5 std=1768\r\n> \r\n> Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\n> count=91 first=11004 curr=10970 min=10864 max=11745 avg=11008.7 std=105\r\n> \r\n> Inference timings in us: Init: 1004, First inference: 22701, Warmup (avg): 11638.5, Inference (avg): 11008.7\r\n> Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\n> Memory footprint delta from the start of the tool (MB): init=3.32422 overall=8.83984\r\n> ```\r\n> \r\n> When enable xnnpack, the memory cost is 13.8MB, see below:\r\n> \r\n> ```\r\n> OP:/data/local/tmp # ./benchmark_model --graph=mobilenet_v1_1.0_224_quant.tflite --num_threads=4 --use_xnnpack=true\r\n> STARTING!\r\n> Log parameter values verbosely: [0]\r\n> Num threads: [4]\r\n> Graph: [mobilenet_v1_1.0_224_quant.tflite]\r\n> #threads used for CPU inference: [4]\r\n> Use xnnpack: [1]\r\n> Loaded model mobilenet_v1_1.0_224_quant.tflite\r\n> INFO: Initialized TensorFlow Lite runtime.\r\n> INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n> XNNPACK delegate created.\r\n> INFO: Replacing 28 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 4 partitions.\r\n> Explicitly applied XNNPACK delegate, and the model graph will be partially executed by the delegate w/ 2 delegate kernels.\r\n> The input model file size (MB): 4.27635\r\n> Initialized session in 14.79ms.\r\n> Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n> count=66 first=13307 curr=7452 min=7431 max=13307 avg=7638.41 std=719\r\n> \r\n> Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\n> count=133 first=7588 curr=7541 min=7445 max=8710 avg=7556.99 std=159\r\n> \r\n> Inference timings in us: Init: 14790, First inference: 13307, Warmup (avg): 7638.41, Inference (avg): 7556.99\r\n> Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\n> Memory footprint delta from the start of the tool (MB): init=11.2891 overall=13.8906\r\n> ```\r\n> \r\n> Could you please help check this ?\r\n\r\nThx for reporting this! This is not surprising to us when we turned on XNNPACK by default, especially when the model is small and the relative memory footprint increase is quite large. We intentionally made a trade-off between memory consumption increase and performance gains across a wide range of platforms. \r\n\r\nNote we provide multiple options to disable using XNNPACK-by-default if the memory consumption increase doesn't fit your use cases. One could use any one of the following options:\r\n1. Add --define=tflite_with_xnnpack=false when compiling the code using Bazel.\r\n2. In C++ code, use [BuiltinOpResolverWithoutDefaultDelegates](https://github.com/tensorflow/tensorflow/blob/6f824c625ac538d6aebe198a73147e916230df85/tensorflow/lite/kernels/register.h#L39) when resolving Tensorflow Lite ops.\r\n3. In Python code, use [BUILTIN_WITHOUT_DEFAULT_DELEGATES](https://github.com/tensorflow/tensorflow/blob/6f824c625ac538d6aebe198a73147e916230df85/tensorflow/lite/python/interpreter.py#L338) when creating the Interpreter."]}, {"number": 54495, "title": "capture_tpu_profile on Cloud TPU VM", "body": "Hello\r\n\r\nIt seems that capture_tpu_profile only works for legacy TPU devices, but not for Cloud TPU VM.\r\n\r\nI was wondering if and when we can expect to have support for Cloud TPU VM. \r\nAt the moment, its is impossible to monitor Jax/Flax TPU workloads, as tensorboard instrumentation does not exhibit the right metrics (eg % utilization of TPU Matrix Units).\r\n\r\nOther related thread for legacy TPU devices: https://stackoverflow.com/questions/52427141/check-tpu-workload-utilization\r\n\r\nThanks!\r\n", "comments": []}, {"number": 54494, "title": "Optimized pad kernel", "body": "Faster implementation of pad op on Eigen CPU path. Currently we have implementations of 2D and 3D float32 type tensor.", "comments": ["hi @rohan100jain, could you review this PR?", "Do you have benchmarks?", "We have some benchmark results of kernel time performance boost of running a single pad op. \r\n\r\n![pad op benchmark results](https://user-images.githubusercontent.com/89124486/160325038-5ea66a63-f2d0-47ce-b659-0636d1f3dd84.png)\r\n\r\n\r\n", "Can you include a google benchmark as part of the PR in a file `pad_op_test.cc`?", "@xiangzez Can you please check @cantonios's comments and keep us posted ? Thanks!", "OK. I'll add a benchmark for this and update this PR.", "@xiangzez Any update on this PR? Please. Thank you!"]}, {"number": 54488, "title": "Read only tensor is mapped as input to IF operation", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: Google Colab Ubuntu 18.04\r\n- TensorFlow installed from binary\r\n- TensorFlow version: 2.8.0\r\n\r\n### 2. Code\r\n\r\nColab Notebook: [link](https://colab.research.google.com/drive/16guYzn6go7Sps_5XyVQSELMn5Euwlwpb?usp=sharing)\r\n\r\n### 3. Failure after conversion\r\nThe read-only tensors containing the weights are mapped as inputs for the `IF` operation in Subgraph `#0`. I agree on the fact that these are indeed inputs for the subgraph but this behavior causes some difficulties in combination with [TFLM](https://github.com/tensorflow/tflite-micro). More specifically, every input tensor is copied over to the \"subgraph's execution environment\". Especially for these (large) read-only tensors, it's a huge unnecessary overhead because these could be linked to the OPs that use the tensors directly. \r\n\r\n```md\r\nSubgraph#0 main(T#0, T#1) -> [T#4]\r\n  Op#0 IF(T#1, T#3, T#0, T#2, Then: Subgraph#2, Else: Subgraph#1) -> [T#4]\r\n\r\n...\r\nT#2(cond_1/input_2) shape:[10, 10], type:FLOAT32 RO 400 bytes\r\nT#3(cond_1/input_0) shape:[10, 10], type:FLOAT32 RO 400 bytes\r\n```\r\nI already posted this issue in the TensorFlow lite micro repository over here https://github.com/tensorflow/tflite-micro/issues/903.\r\n\r\n\r\nIs there any option besides modifying the .tflite file to get these tensors out of the input tensor list of the `IF` operation?\r\n\r\n\r\n", "comments": []}, {"number": 54485, "title": "XLA `LuDecomposition` support for CPU/GPU", "body": "**System information**\r\n- TensorFlow version (you are using): 2.8\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n`xla::LuDecomposition` in tensorflow/compiler/xla/client/lib/lu_decomposition.h is currently only implemented for TPU. I'd like to be able to use it on CPU and/or GPU.\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes, but in a backwards compatible way.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone wanting to build on XLA and do LU decomposition for e.g. calculating determinants. I want this as I'm building an API for XLA in Idris.", "comments": ["I realise I don't need this, but I'll leave it open since it's a TODO in XLA already. Feel free to close if you think that's best", "@joelberkeley ,\r\nCan you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!", "> Can you please elaborate about your Feature.\r\n\r\nHi @tilakrayal how would you like me to elaborate?\r\n\r\n> Also, please specify the Use Cases for this feature.\r\n\r\nThis would be helpful for calculating the inverse of matrices"]}, {"number": 54484, "title": "Made rounding in convert_image_dtype for numbers close to zero", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/48701\r\n\r\nThat issue already was fixed by https://github.com/tensorflow/tensorflow/pull/49868\r\nBut after merging all changes were overwritten in https://github.com/tensorflow/tensorflow/commit/56ab2308f72da337865cf765a1844ed9e990d02e#diff-a5a22434f0c18768fc2e10c0e0420ac6f111a5802f67e3df54f155bfefc7094f and now issue raises again", "comments": ["Any update on merging this PR?", "I'm not familiar with this code, so I'm probably not the right person to review it.  \r\n\r\nBut it looks like the original change was automatically rolled back because it caused a test in this file to fail: https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/compiler/tests/image_ops_test.py.  (Unfortunately, since it was rolled back several months ago, the logs from the failed test aren't available anymore.)", "@rohan100jain Can you please review this PR ? Thank you!"]}, {"number": 54481, "title": "TFLite with Hexagon delegate produces wrong results for a particular model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (host), Android 11 (target device)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: device with Snapdragon 660\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 4.2.1\r\n- GCC/Compiler version (if compiling from source): 8.4.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nFor a custom model I am trying to use, when I enable the Hexagon delegate, the output contains incorrect values. The Hexagon delegate seems to work fine with other models I have tried.\r\n\r\n**Describe the expected behavior**\r\nI expect the Hexagon delegate to produce the same results as CPU.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe difference between CPU and Hexagon delegate can be observed with your [Inference Diff tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/inference_diff#inference-diff-tool):\r\n```\r\nINFO: TfLiteHexagonDelegate delegate: 270 nodes delegated out of 270 nodes with 1 partitions.\r\nINFO: Replacing 270 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 1 partitions.\r\nnative : lite/tools/evaluation/stages/inference_profiler_stage.cc:77 Test interpreter has been initialized.\r\nnative : lite/tools/evaluation/stages/tflite_inference_stage.cc:128 \r\nnative : lite/tools/evaluation/stages/inference_profiler_stage.cc:91 Reference interpreter (1 thread on CPU) has been initialized.\r\nNum evaluation runs: 50\r\nReference run latency: avg=863334(us), std_dev=25697(us)\r\nTest run latency: avg=93164.5(us), std_dev=675(us)\r\nOutputDiff[0]: avg_error=86.0362, std_dev=0.0619768\r\nOutputDiff[1]: avg_error=48.6195, std_dev=0.168915\r\nOutputDiff[2]: avg_error=52.9178, std_dev=0.252582\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nModel used: [test_model.zip](https://github.com/tensorflow/tensorflow/files/8118439/test_model.zip)\r\n", "comments": ["@AB5247 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@sushreebarsa As I mentioned in my initial comment, the issue can easily be reproduced with the Inference Diff tool from this repository.", "Hi, are there any updates? Has anyone been able to take a look at this issue?", "@sushreebarsa @jvishnuvardhan @karimnosseir Can anyone provide an update on the status of this issue or if someone will be able to take a look at it soon? Thank you.", "@AB5247 Can you please check @karimnosseir 's previous responses as he responded to many of the Hexagon related issues. Thanks", "@jvishnuvardhan Sorry, but this is not related to anything reported before and answered by @karimnosseir. TFlite with the Hexagon delegate is simply giving completely wrong results for the model I am working with, someone from your team will have to look at it and tell me what is going on. https://github.com/tensorflow/tensorflow/issues/36804 was a similar problem  and it was a bug that you eventually fixed.", "Sorry for the late reply as i was out of office and came back today. Will check it and update the issue.\r\n\r\nThanks", "@karimnosseir Any updates on this? Thanks.", "@jvishnuvardhan @sushreebarsa Sorry to bother you again, but can this be assigned to someone else if @karimnosseir is not able to take a look? It has been almost 2 months since I opened this issue and no one has told me yet if you can even reproduce the same bug I experience. Thanks.", "Hi @AB5247 \r\nSorry for the late update, we didn't ignore you. I was sick for some time.\r\n\r\nThanks for reporting\r\n\r\nI can reproduce the issue, and found a bug related to relu layer and sent a fix for it.\r\nThere is another issue related to depthwise conv when stride h/w > 1 and shape has ?x5x5x? don't have a fix but will be disabling this until fix is ready - sending a change to it too..\r\n\r\nWill update the issue here when changes are in.\r\n\r\nThanks", "Thanks @karimnosseir, glad you found the bugs. Please, keep me updated about the fixes. Thanks."]}, {"number": 54479, "title": "XLA tf.bincount support", "body": "**System information**\r\n- TensorFlow version (you are using): master\r\n- Are you willing to contribute it (Yes/No): only if we have a clear path and a reviewer on how to contribute this\r\n**Describe the feature and the current behavior/state.**\r\n`tf.bincount` isn't supported by XLA\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nSpeedup functions/loops that rely on `tf.bincount`\r\n**Any Other info.**\r\nHow to reproduce it:\r\n```python\r\nimport tensorflow as tf\r\n@tf.function(jit_compile=True)\r\ndef compiled_bincount(values):\r\n  return tf.math.bincount(values)\r\n\r\nvalues = tf.constant([1,1,2,3,2,4,4,5])\r\nprint(compiled_bincount(values)) #[0 2 2 1 2 1]\r\n```\r\n\r\n```python\r\nInvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_compiled_bincount_296[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263] on XLA_CPU_JIT: Bincount (No registered 'Bincount' OpKernel for XLA_CPU_JIT devices compatible with node {{node bincount/Bincount}}){{node bincount/Bincount}}\r\nThe op is created at: \r\n```\r\n\r\nWithout this we had problems to compile intermediate Ms Coco recall function in keras-cv:\r\nhttps://github.com/keras-team/keras-cv/issues/141#issuecomment-1043692891\r\n\r\nExtra: \r\nPlease also note that the CPU/GPU TF2XLA supported ops tables are probably outdated (2018):\r\nhttps://github.com/tensorflow/tensorflow/issues/14798#issuecomment-1047796247\r\n\r\n/cc @joker-eph @LukeWood ", "comments": ["Thanks for opening this.  This would be really helpful for the KerasCV project.", "/cc @smit-hinsu for triage."]}, {"number": 54474, "title": "Several `tf.math` APIs lack input checking for `bool` arguments", "body": "Similar to [issue #54410](https://github.com/tensorflow/tensorflow/issues/54410), the following APIs also lack input validity checking for `bool` arguments.\r\n```\r\ntf.math.cumulative_logsumexp\r\ntf.math.reduce_mean\r\ntf.math.cumprod\r\n```\r\nExample code:\r\n```\r\nimport tensorflow as tf\r\ntf.math.cumulative_logsumexp([0.5, 1.0, 2.0, 4.0], axis=-1, exclusive=-1, reverse=0)\r\ntf.math.cumprod([0.5, 1.0, 2.0, 4.0], axis=-1, exclusive=-1, reverse=0)\r\ntf.math.reduce_mean(tf.random.uniform([1,2,2]), axis=[1,], keepdims=-1)\r\n# Pass without exceptions\r\n```", "comments": ["@ArrowIntoTheSky can you please check if there are any more arguments without validations? Will make a PR for the above issue by then", "@ArrowIntoTheSky Could you please update as per the comment above ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I think the issue has been fixed in https://github.com/tensorflow/tensorflow/pull/54441", "> I think the issue has been fixed in #54441\r\n\r\nThanks for the PR! However, I checked on tf-nightly, and the following code still passed:\r\n```\r\ntf.math.reduce_mean(tf.random.uniform([1,2,2]), axis=[1,], keepdims=-1)\r\n```\r\nhere `keepdims` should also be a `bool`, but no exception is raised."]}, {"number": 54470, "title": "`parallel_iterations` in `tf.map_fn` has no effect", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0\r\n- Python version: Python 3.9.7\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: RTX A6000, CPU: Intel(R) Xeon(R) W-2255 CPU @ 3.70GHz\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n`parallel_iterations` in `tf.map_fn` has no effect\r\n**Describe the expected behavior**\r\nsetting higher values in `parallel_iterations` should improve running speed\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): on\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\nimport numpy as np\r\n\r\ndef some_fn(x):\r\n    return tf.math.exp(x)\r\n\r\n@tf.function\r\ndef map_fn_test(samples):\r\n    some_fn_vals = tf.map_fn(some_fn, samples, parallel_iterations=1)\r\n    return some_fn_vals\r\n\r\nsamples = tf.constant(np.linspace(0,10,100000))\r\nstart_time = time.time()\r\nvalues = map_fn_test(samples)\r\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\r\n```\r\n\r\nChanging `parallel_iterations` to any other number gives approximately the same amount of time. It looks like there ain't any parallelism happening.\r\n\r\nThis bug was also reported [here](https://github.com/tensorflow/tensorflow/issues/34716#issuecomment-560936125) and also [here](https://github.com/tensorflow/tensorflow/issues/24774) but remain stalled.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi @charlielam0615 ! As this [document ](https://www.tensorflow.org/api_docs/python/tf/map_fn#args)suggests the number of iterations allowed to run in parallel default value is 10 in graph mode and is set to 1 in eager mode . I set values for parallel_operations accordingly and was getting different results. Attaching[ gist](https://colab.sandbox.google.com/gist/mohantym/d12855919bb832049bf64fb1457def39/github_54470.ipynb#scrollTo=C0uqpVUVmKrl) for reference. Can you let us know from your end?", "Hi @mohantym, as per the outputs of your gist, with the `parallel_iterations` increasing from 10 to 20, the code doesn't run significantly faster (if not slower). It makes one wonder if there is indeed parallelization happening behind the scene.\r\n```\r\n--- 0.029703855514526367 seconds ---\r\n--- 0.02797532081604004 seconds ---\r\n--- 0.02787494659423828 seconds ---\r\n--- 0.02804279327392578 seconds ---\r\n--- 0.027938365936279297 seconds ---\r\n--- 0.02780890464782715 seconds ---\r\n--- 0.08853292465209961 seconds ---\r\n--- 0.05671095848083496 seconds ---\r\n--- 0.026008129119873047 seconds ---\r\n--- 0.027454853057861328 seconds ---\r\n```\r\n", "Hi @gadagashwini ! Could you look at this issue? It is replicating in [2.7](https://colab.sandbox.google.com/gist/mohantym/6160d5ad155e8f455c39dbf237a2e745/github_54470.ipynb#scrollTo=3m9IAJirnvZZ), [2.8](https://colab.sandbox.google.com/gist/mohantym/d12855919bb832049bf64fb1457def39/github_54470.ipynb#scrollTo=3m9IAJirnvZZ) and [nightly.](https://colab.sandbox.google.com/gist/mohantym/b82990e604cbd135c685019a92391a40/github_54470.ipynb#scrollTo=3m9IAJirnvZZ)", "@charlielam0615,\r\nI tried your code on Eager execution and tf.function mode with parallel_iterations, Looks like there is difference in execution  time.\r\n\r\n**Graph mode: parallel execution**\r\n```\r\nimport tensorflow as tf\r\nimport time\r\nimport numpy as np\r\n\r\ndef some_fn(x):\r\n    return tf.math.exp(x)\r\n\r\n@tf.function\r\ndef map_fn_test(samples,iterations):\r\n    some_fn_vals = tf.map_fn(some_fn, samples, parallel_iterations=iterations)\r\n    return some_fn_vals\r\n\r\nsamples = tf.constant(np.linspace(0,10,100000))\r\n\r\nfor iterations in range(1,5):\r\n start_time = time.time()\r\n values = map_fn_test(samples,iterations)\r\n print(\"--- %s seconds ---\" % (time.time() - start_time))\r\n\r\n--- 40.14726758003235 seconds ---\r\n--- 38.71791315078735 seconds ---\r\n--- 34.22684836387634 seconds ---\r\n--- 34.73827767372131 seconds ---\r\n```\r\n\r\n**Eager mode: Sequential Execution**\r\n```\r\nimport tensorflow as tf\r\nimport time\r\nimport numpy as np\r\n\r\ndef some_fn(x):\r\n    return tf.math.exp(x)\r\n\r\ndef map_fn_test(samples,iterations):\r\n    some_fn_vals = tf.map_fn(some_fn, samples, parallel_iterations=iterations)\r\n    return some_fn_vals\r\n\r\nsamples = tf.constant(np.linspace(0,10,100000))\r\n\r\nfor iterations in range(1,5):\r\n start_time = time.time()\r\n values = map_fn_test(samples,iterations)\r\n print(\"--- %s seconds ---\" % (time.time() - start_time))\r\n\r\n--- 124.02748465538025 seconds ---\r\n--- 93.25651001930237 seconds ---\r\n--- 93.3725266456604 seconds ---\r\n--- 92.41289329528809 seconds ---\r\n```\r\n\r\n ", "Hi @gadagashwini, I do not observe speed improvement with higher `parallel_iterations` settings on **my own machine** (TF2.8, RTX A6000, CUDA 11.2).\r\n\r\n(Using your code)\r\n**Graph mode: parallel execution**\r\n```\r\n--- 21.58019709587097 seconds ---\r\n--- 21.391409873962402 seconds ---\r\n--- 21.302981853485107 seconds ---\r\n--- 21.093496322631836 seconds ---\r\n```\r\n**Eager mode: Sequential Execution**\r\n```\r\n--- 52.483237504959106 seconds ---\r\nWARNING:tensorflow:Setting parallel_iterations > 1 has no effect when executing eagerly. Consider calling map_fn with tf.function to execute fn in parallel.\r\n--- 53.701409578323364 seconds ---\r\n--- 55.06963491439819 seconds ---\r\n--- 50.95377826690674 seconds ---\r\n```\r\nWhile in Eager mode, there is indeed time difference, but as far as the warning tells, the time difference is not a result of different `parallel_iterations` settings as parallel_iterations>1 settings virtually have no effect.\r\n\r\n**Using Google Colab**, however, I indeed see some time difference in Graph Mode. But I suspect the time difference you see in **Graph mode** is not due to different `parallel_iteration` settings, as parallelism should give performance boost approximately linear in the number of threads in this situation. The time difference could be explained if some overhead is calculated in the first iteration. Changing `for iterations in range(1,5):` to `for iterations in [1,1,2,3,4]:` confirms this conjecture (see [this notebook](https://colab.research.google.com/drive/1LjerXUgeOw5Z6rNQqUKqu73sqa1Df72I?usp=sharing))\r\n```\r\n--- 58.61037349700928 seconds --- # parallel_iteration=1\r\n--- 50.79203748703003 seconds --- # parallel_iteration=1\r\n--- 50.37450909614563 seconds --- # parallel_iteration=2\r\n--- 51.25666522979736 seconds --- # parallel_iteration=3\r\n--- 50.71102428436279 seconds --- # parallel_iteration=4\r\n```\r\n"]}, {"number": 54461, "title": "`in` operation in TF takes incredibly long time", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n- Python version: 3.9.7\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: RTX A6000\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n`in` operation in tensors took an unreasonably amount of time\r\n\r\n**Describe the expected behavior**\r\nshould be very fast to execute\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\na = tf.range(256)\r\nstart = time.time()\r\nres = tf.constant([i in a for i in range(256)])\r\nprint(\"Using tf: took\", time.time()-start, \"seconds.\")\r\n\r\na_ = a.numpy()\r\nstart = time.time()\r\nres = tf.constant([i in a_ for i in range(256)])\r\nprint(\"Using numpy: took\", time.time()-start, \"seconds.\")\r\n```\r\n\r\nIn my own machine, this gives\r\n```\r\nUsing tf: took 3.605224847793579 seconds.\r\nUsing numpy: took 0.0006427764892578125 seconds\r\n```\r\n\r\nWell, it's pretty obvious something is wrong when using `in` operations with tensors.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@charlielam0615 can you please check the type of the variable res and tell here?", "`print(res.dtype)` gives `<dtype: 'bool'>`", "I am able to reproduce the issue in `TF2.8` and `tf-nightly(2.9.0-dev20220228)`. Please find the [gist](https://colab.research.google.com/gist/chunduriv/5068c1273eed5f1297ce7dea1f168bbb/54461.ipynb) here for reference.Thanks!", "Hi @chunduriv! Yes indeed, that's why this issue is submitted. Can we get somebody to resolve this problem?"]}, {"number": 54458, "title": "InvalidArgumentError: Node '.../while_grad': Connecting to invalid output 4 of source node while which has 4 outputs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): 2.8.0 (Colab)\r\n- Python version: 3.7.12\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nI get the exception\r\n```\r\nNode 'gradients/while_grad/while_grad': Connecting to invalid output 4 of source node while which has 4 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).\r\n```\r\nwhen executing a `NoOp` without any control dependencies.\r\n\r\nBut also when executing my real code, I get the same exception. I'm not sure if these are two separate issues or the same.\r\n\r\n**Describe the expected behavior**\r\n\r\nA NoOp without control dependencies should not depend on anything, so I would never expect such exception.\r\n\r\nFor my real code, I also would not expect such exception. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nCode (to be executed with disabled eager mode):\r\n```\r\nv = tf.Variable(1.)\r\n\r\ndef cond(i, x):\r\n  return tf.less(i, 10)\r\n\r\ndef body(i, x):\r\n  return i + 1, x * 1.\r\n\r\nj, y = tf.while_loop(cond, body, [0., v])\r\nloss = tf.reduce_sum(y ** 2)\r\n\r\nsession.run(tf.compat.v1.global_variables_initializer())\r\n\r\nopt = tf.compat.v1.train.GradientDescentOptimizer(0.1)\r\nopt_op = opt.minimize(loss)\r\n\r\nno_op = tf.no_op()\r\nsession.run(no_op)  # here it crashes already!\r\n\r\nprint(session.run((j, y, opt_op)))\r\n```\r\n\r\nColab: https://colab.research.google.com/drive/1jjXpz8SAeU-8Cg6nuWu7tjxK_sAU5lVc?usp=sharing\r\n\r\n**Other info / logs**\r\n\r\nWhen executing locally, I additionally see this log output:\r\n```\r\n...\r\n2022-02-19 23:07:57.636413: W tensorflow/c/c_api.cc:349] Operation '{name:'while' id:11 op device:{requested: '', assigned: ''} def:{{{node while}} = StatelessWhile[T=[DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_VARIANT], _lower_using_switch_merge=true, _num_original_outputs=5, _read_only_resource_inputs=[], _stateful_parallelism=false, body=while_body_8_rewritten[], cond=while_cond_7_rewritten[], output_shapes=[[], [], [], [], []], parallel_iterations=10](while/loop_counter, while/maximum_iterations, Const, ReadVariableOp, gradients/while_grad/Placeholder_1_0/accumulator:0)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\r\nTraceback (most recent call last):\r\n  File \"/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1380, in _do_call\r\n    return fn(*args)\r\n  File \"/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1362, in _run_fn\r\n    self._extend_graph()\r\n  File \"/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1403, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'gradients/while_grad/while_grad': Connecting to invalid output 4 of source node while which has 4 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/az/Programmierung/playground/tf-while-v2.py\", line 37, in <module>\r\n    main()\r\n  File \"/Users/az/Programmierung/playground/tf-while-v2.py\", line 31, in main\r\n    session.run(no_op)\r\n  File \"/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 970, in run\r\n    result = self._run(None, fetches, feed_dict, options_ptr,\r\n  File \"/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1193, in _run\r\n    results = self._do_run(handle, final_targets, final_fetches,\r\n  File \"/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1373, in _do_run\r\n    return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n  File \"/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1399, in _do_call\r\n    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'gradients/while_grad/while_grad': Connecting to invalid output 4 of source node while which has 4 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).\r\n```\r\n\r\nMy hypothesis is that the first session call `session.run(tf.compat.v1.global_variables_initializer())` will somehow freeze the while loop function graph (which is strange though as it would not depend on it), and then the further code which adds the gradients causes this warning `Operation ... StatelessWhile ... was changed by setting attribute after it was run by a session`.\r\n\r\nI don't really understand why the first session call does that even though it is independent from the loop.\r\n\r\nI don't really understand why it causes the error for the NoOp execution.\r\n\r\nIs there any workaround?\r\n", "comments": ["There are some related issues here, like: https://github.com/tensorflow/tensorflow/issues/39908.\r\n\r\nSome suggestion is to use eager mode, but I want to keep using graph mode.\r\n", "Hi @albertz,\r\n\r\nI noticed you recently enabled\r\n`tf.compat.v1.experimental.output_all_intermediates(True)` in your application. Be aware that the flag may increase the memory usage.\r\n\r\nAnother workaround to try, is to move all calls to `session.run` after mutations of the graph, e.g., (in your colab), \r\n\r\n```\r\n...\r\n      opt = tf.compat.v1.train.GradientDescentOptimizer(0.1)\r\n      opt_op = opt.minimize(loss)\r\n      no_op = tf.no_op()\r\n\r\n      session.run(tf.compat.v1.global_variables_initializer())      \r\n      session.run(no_op)\r\n      session.run((j, y, opt_op))\r\n...\r\n```\r\nDepending on your application's structure, this change may be infeasible. (Also refer to the Keras situation mentioned in docstring of output_all_intermediates)."]}, {"number": 54456, "title": "Performance of nn.conv1d and keras.layers.Conv1D is low the first time any given input size is processed even if retracing is prevented! ", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\nyes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\nLinux Ubuntu 20.04\r\n\r\n- TensorFlow installed from (source or binary):\r\n\r\npip\r\n\r\n- TensorFlow version (use command below):\r\n\r\ntested with TF 2.4, 2.6, 2.8\r\n\r\n- Python version:\r\n\r\n3.7\r\n\r\n- CUDA/cuDNN version:\r\n\r\nCuda Toolkit 11.3.1\r\nCUDNN 8.3.1 \r\n\r\n- GPU model and memory:\r\n\r\nTested with GeForce GTX 1050 Ti  and GeForce GTX 1080 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nIn both cases using eager mode and using a `tf.function` with `experimental_relax_shapes=True`\r\nrunning tf.nn.conv1d is slow the first time a tensor of any given size is processed and the processing of a new tensor of the same size then becomes 4 times (on GPU 1050 TI) or 10 times (on GPU 1080 Ti) faster the second or further times\r\n.\r\nThe observed behavior is a severe problem for running inference with audio signals because audio signals generally have very different sizes and therefore in a production environment the code will run only with 10% of maximum performance (on a GPU 1080 Ti) \r\nfor the first few 100k examples until the model has seen sufficiently many lengths to achieve peak performance.\r\n\r\n**Describe the expected behavior**\r\n\r\nconv1d processing time should depend on the size of the input vector and not on the number of times the same size has been seen.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\ncolab notebook is \r\n[here](https://colab.research.google.com/gist/roebel/5cc15bb93566d374e87da9d2c9e885f2/performance_issue.ipynb)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem.\r\n\r\nResult running on colab with GPU\r\n\r\n```\r\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n=== run 1 ===\r\nlen 10000 time: 10.187 speed 9.82kHz su (10, 9991, 100)\r\nlen 10001 time: 0.603 speed 165.93kHz su (10, 9992, 100)\r\nlen 10002 time: 0.613 speed 163.08kHz su (10, 9993, 100)\r\nlen 10003 time: 0.628 speed 159.26kHz su (10, 9994, 100)\r\nlen 10004 time: 0.587 speed 170.35kHz su (10, 9995, 100)\r\n=== run 2 ===\r\nlen 10000 time: 0.051 speed 1946.11kHz su (10, 9991, 100)\r\nlen 10001 time: 0.050 speed 2000.30kHz su (10, 9992, 100)\r\nlen 10002 time: 0.066 speed 1518.87kHz su (10, 9993, 100)\r\nlen 10003 time: 0.056 speed 1788.66kHz su (10, 9994, 100)\r\nlen 10004 time: 0.059 speed 1705.02kHz su (10, 9995, 100)\r\n```\r\n\r\n*Notes:*\r\n- You notice 10 times increase the second time the inner loop is run. If you run the same notebook again the first run will be fast as well, which indicates the GPU is caching something. \r\n- to have the effect again your need to restart the notebook\r\n- the behavior is not CUDA imposed because an equivalent PyTorch script runs the first and second pass of the inner loop without speed difference.", "comments": ["I believe to have made some progress using tensorboard profiler. The first time the convolution is run on a given size the profiler displays running all the following kernels\r\n \r\n- `maxwell_scudnn_128x128_relu_small_nn_v1`,  \r\n- `implicit_convolve_sgemm`, \r\n- `cudnn::cnn::im2col4d_kernel`, \r\n- `explicit_convolve_sgemm`, \r\n- `fft1d_c2r_256`, \r\n- `maxwell_gcgemm_32x32_nt`. \r\n \r\nThe second time any given size is processed only a single kernel is used. In the present case these are either  `maxwell_gcgemm_32x32_nt` or `maxwell_scudnn_128x128_relu_small_nn_v1`. So it appears tensorflow is trying to optimize by means of adaptively selecting the best kernel for each size. \r\n\r\nThis is problematic notably for inference with audio where size of audio files can vary between 1 second (16000 samples) and 40 seconds (640000 samples) because as mentioned above the software will spend most of the time trying to optimize and will therefore need more than a day before it starts to perform optimally. The question here would be whether this trial stage can be prevented by means of manually preselecting a strategy. \r\n", "Hi @chunduriv ! Could you please look at this issue? Its replicating in [2.8](https://colab.sandbox.google.com/gist/mohantym/20beff230772268114e37ec0b2d9eeed/performance_issue.ipynb#scrollTo=M521fw4_sNSA) and throwing different error in [2.7](https://colab.sandbox.google.com/gist/mohantym/966d7b901aa5063ad79c278eceec9679/performance_issue.ipynb#scrollTo=0SB4KiqP_a67) and [nightly](https://colab.sandbox.google.com/gist/mohantym/d201db4c877782d9334802cf953a529f/performance_issue.ipynb#scrollTo=0SB4KiqP_a67). Thanks!", "@roebel ! Did you try the same in [distribution training ](https://www.tensorflow.org/guide/distributed_training#:~:text=Overview-,tf.,code%20with%20minimal%20code%20changes.)yet?", "@mohantym Thanks for your reply. But I wonder why you suggest distribution training? I don't have a problem with training I have a problem with inference. Also for my use cases, I don't have access to multiple GPUs so I cannot use distribution inference - or could I? If these tests would be run in parallel on the same GPU this may help but would also require more memory. "]}]