[{"number": 49920, "title": "Changes in Input size of the model do not save when calling saved_model.save", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 2004\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: irrelevant?\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nChanges in input shape of the model are not applied when saving the model, as can be seen that Input shape is changed before saving the model, and when opening the saved model the input shape reverts to [None,None,None,3] .\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nInput shape in the new saved model should change to the [1,640,640,3].\r\n\r\nThis is mainly required to convert the model into TFlite compatible one.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\n\r\nmodel = tf.saved_model.load(\"exported-models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model\") #taken from Object detection zoo\r\nmodel.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs[0].set_shape([1, 640, 640, 3])\r\ntf.saved_model.save(model, \"saved_model_updated\", signatures=model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY])\r\nprint(model.signatures['serving_default'].inputs[0])\r\n\r\n\r\n\r\nmodel2 = tf.saved_model.load(\"saved_model_updated\")\r\n\r\nprint(model2.signatures['serving_default'].inputs[0])\r\n\r\n####################################\r\n2.5.0\r\n\r\nWARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 155). These functions will not be directly callable after loading.\r\n\r\nINFO:tensorflow:Assets written to: saved_model_updated\\assets\r\n\r\nINFO:tensorflow:Assets written to: saved_model_updated\\assets\r\n\r\nTensor(\"input_tensor:0\", shape=(1, 640, 640, 3), dtype=uint8)\r\nTensor(\"input_tensor:0\", shape=(None, None, None, 3), dtype=uint8)\r\n", "comments": ["@KapitanKabzon \r\n\r\nIn order to expedite the trouble-shooting process, please provide a code snippet/colab gist to reproduce the issue reported here. Thanks!\r\n", "@KapitanKabzon I tried your code with different model and I don't see the issue. only batch_size dimension is set as `None` for obvious reasons. [Here](https://colab.research.google.com/gist/jvishnuvardhan/8c7558d191c01242ffd5ee077300fb85/untitled.ipynb) is a gist for a reference. \r\n\r\nPlease share the model you are using to verify it. Thanks! \r\n", "i am using this model: http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\r\nfrom the https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md.\r\n\r\nSlightly modified my code for clarification.\r\n`\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\n\r\nmodel = tf.saved_model.load(\"exported-models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model\") #taken from Object detection zoo\r\nprint('Model Original input size: ')\r\nprint(model.signatures['serving_default'].inputs[0])\r\nmodel.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs[0].set_shape([1, 640, 640, 3])\r\ntf.saved_model.save(model, \"saved_model_updated\", signatures=model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY])\r\nprint('Model modified input size: ')\r\nprint(model.signatures['serving_default'].inputs[0])\r\n\r\n\r\n\r\nmodel2 = tf.saved_model.load(\"saved_model_updated\")\r\n\r\nprint('Model loaded modified input size')\r\nprint(model2.signatures['serving_default'].inputs[0])\r\n`\r\n########### Notebook output #############\r\n2.4.1\r\nModel Original input size: \r\nTensor(\"input_tensor:0\", shape=(1, None, None, 3), dtype=uint8)\r\n\r\nWARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 155). These functions will not be directly callable after loading.\r\nWARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 155). These functions will not be directly callable after loading.\r\n\r\nINFO:tensorflow:Assets written to: saved_model_updated\\assets\r\n\r\nINFO:tensorflow:Assets written to: saved_model_updated\\assets\r\n\r\nModel modified input size: \r\nTensor(\"input_tensor:0\", shape=(1, 640, 640, 3), dtype=uint8)\r\nModel loaded modified input size\r\nTensor(\"input_tensor:0\", shape=(1, None, None, 3), dtype=uint8)\r\n", "> \r\n> \r\n> @KapitanKabzon I tried your code with different model and I don't see the issue. only batch_size dimension is set as `None` for obvious reasons. [Here](https://colab.research.google.com/gist/jvishnuvardhan/8c7558d191c01242ffd5ee077300fb85/untitled.ipynb) is a gist for a reference.\r\n> \r\n> Please share the model you are using to verify it. Thanks!\r\n\r\nBTW, if you look at the loaded_model and model2 input tensors you can see that the batch size doesn't save, and quantization requires all the inputs to be set in stone.", "@KapitanKabzon Can you try using `set_shape` method of set shape as shown in [this](https://github.com/tensorflow/tensorflow/issues/26708#issuecomment-474152784) GitHub response.  May be the following links may help you. [link2](https://github.com/tensorflow/tensorflow/issues/35736), [Link3](https://github.com/tensorflow/tensorflow/issues/34350). \r\n\r\nHope it helps. Thanks!", "nothing changes if i use this method to change the input parameters.\r\n\r\nconcrete_func = model.signatures[\r\n    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\nconcrete_func.inputs[0].set_shape([1, 640, 640, 3])\r\n\r\nIts still the same behavior, i guess a workaround of changing the input shape when specifically doing the TFlite conversion is doable, but it is weird that changes to the model do not save.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49920\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49920\">No</a>\n", "@KapitanKabzon Is it possible to share one standalone code to reproduce the issue? I see the parts of the code but if you can put together in one colab gist or Jupyter notebook, then it will help us resolving faster. Thanks!  ", "Here is the link to the folder Untitled.ipynb is the code, the model that is loaded is included in the folder.\r\nhttps://github.com/KapitanKabzon/EiT_Metal/tree/master/TensorFlowIssue", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "making this error: \"AttributeError: 'str' object has no attribute 'signatures'\"", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49920\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49920\">No</a>\n", "i have tflite model please change input size of my model provide me a code in which i can change my model input size \r\nthanks"]}, {"number": 49917, "title": "tf random gives the same sequence of numbers when it shouldn't", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip3 install\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA11.3 cudnn 8.2.0\r\n- GPU model and memory: NVIDIA TITAN V 12 GB\r\n\r\n**Describe the current behavior**\r\nsee below\r\n**Describe the expected behavior**\r\ngenerate different numbers \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\ncolab: https://colab.research.google.com/drive/1ffXUYw6M7rDokL2EkPqKjna8AD5QEslA?usp=sharing\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef _h(model,x):\r\n    model(x)\r\n\r\nclass b(tf.keras.layers.Layer):\r\n    def call(self, inputs):\r\n        if tf.constant(True):\r\n            tf.print(tf.random.uniform([5,]),summarize=-1)\r\n        else:\r\n            pass\r\n        return inputs\r\n\r\ntf.random.set_seed(123)\r\ninputs = tf.keras.Input(shape=(784,))\r\n\r\nx=b()(inputs)\r\nx=b()(x)\r\nx=b()(x)\r\noutputs=b()(x)\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nh=tf.function(_h)\r\n\r\nprint('first run:')\r\nh(model,tf.constant(np.random.rand(64,784)))\r\nprint('second run:')\r\nh(model,tf.constant(np.random.rand(64,784)))\r\n```\r\noutput:\r\nfirst run:\r\n[0.277247906 0.994074821 0.379808426 0.71479249 0.50061965]\r\n[0.277247906 0.994074821 0.379808426 0.71479249 0.50061965]\r\n[0.277247906 0.994074821 0.379808426 0.71479249 0.50061965]\r\n[0.277247906 0.994074821 0.379808426 0.71479249 0.50061965]\r\nsecond run:\r\n[0.176383495 0.109812617 0.334476113 0.66576612 0.116794825]\r\n[0.176383495 0.109812617 0.334476113 0.66576612 0.116794825]\r\n[0.176383495 0.109812617 0.334476113 0.66576612 0.116794825]\r\n[0.176383495 0.109812617 0.334476113 0.66576612 0.116794825]\r\n\r\nThe phenomenon generalize to other related api like `tf.random.categorical` , etc.\r\nI read this issue, https://github.com/tensorflow/tensorflow/issues/33297, and realized that tf random could be buggy, but there is no any warning to new users in the api page.(e.g. https://www.tensorflow.org/api_docs/python/tf/random/uniform) The warning is shown in https://www.tensorflow.org/guide/random_numbers, which is too difficult to notice. The random number docs is inadequate as well, how do I use more complicated api like `tf.random.categorical` from `tf.random.Generator`?\r\n\r\n", "comments": ["@laplacericky \r\nThe issue is using tf.constant in your code, i have changed that and able to produce different sequence with  tf random, please find the [gist here.](https://colab.research.google.com/gist/Saduf2019/1f43ab5329ebae2b53832c829443a02d/untitled606.ipynb).\r\n\r\nKindly open a [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) issue for this as it is not a bug or feature request, Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions.\r\nThanks!", "> @laplacericky\r\n> The issue is using tf.constant in your code, i have changed that and able to produce different sequence with tf random, please find the [gist here.](https://colab.research.google.com/gist/Saduf2019/1f43ab5329ebae2b53832c829443a02d/untitled606.ipynb).\r\n> \r\n> Kindly open a [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) issue for this as it is not a bug or feature request, Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions.\r\n> Thanks!\r\n\r\nThat is a very common use case that I want to have some comparisons which involves tensor (i.e. tf.cond) inside call function of a custom keras layers. In here in my bug reproduction code, I used `tf.constant(True)` just for the sake of having an if-else clause which will be converted by autograph and I want to have minimal code to reproduce the bug. In reality case, I could be comparing a tensor with another tensor. \r\n\r\nThe bug can only be reproduced inside tf.cond which further demonstrates it is a dangerous bug because it sometimes work and sometimes don't work. Unless there is clear docs about the situation, more and more people will fall into this trap and thought it was generating random numbers but indeed it generates the same sequence of numbers.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@laplacericky \r\nThis is not a bug, please go through the documentation for random seed and open issue at Stackoverflow/tf discussion forum", "@Saduf2019 \r\nThank you for your reply. Please provide explanations that why this behavior is not considered as bug. I will also provide codes for  several ways to reproduce this behavior and how to avoid this behavior.", "Way 1 to reproduce this behavior\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef _h(model,x):\r\n    model(x)\r\n\r\nclass buggy_random(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        self.boolean_var=tf.Variable(True)\r\n        super().__init__(**kwargs)\r\n    def call(self, inputs):\r\n        if self.boolean_var:\r\n          tf.print(tf.random.uniform([5,]))\r\n\r\n        return inputs\r\n\r\ntf.random.set_seed(123)\r\ninputs = tf.keras.Input(shape=(784,))\r\n\r\nx=buggy_random()(inputs)\r\nx=buggy_random()(x)\r\nx=buggy_random()(x)\r\noutputs=buggy_random()(x)\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\nmodel.layers[1].boolean_var.assign(False)\r\nmodel.layers[3].boolean_var.assign(False)\r\n\r\n\r\nh=tf.function(_h)\r\nprint('first run:')\r\nh(model,tf.constant(np.random.rand(64,784)))\r\nprint('second run:')\r\nh(model,tf.constant(np.random.rand(64,784)))\r\n'''\r\nfirst run:\r\n[0.277247906 0.994074821 0.379808426 0.71479249 0.50061965]\r\n[0.277247906 0.994074821 0.379808426 0.71479249 0.50061965]\r\nsecond run:\r\n[0.176383495 0.109812617 0.334476113 0.66576612 0.116794825]\r\n[0.176383495 0.109812617 0.334476113 0.66576612 0.116794825]\r\n'''\r\n```", "Way 2 to reproduce this behavior:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef _h(model,x):\r\n    model(x)\r\n\r\nclass buggy_random(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n    def call(self, inputs):\r\n        for i in tf.range(tf.constant(1)):\r\n          tf.print(tf.random.uniform([5,]))\r\n\r\n        return inputs\r\n\r\ntf.random.set_seed(123)\r\ninputs = tf.keras.Input(shape=(784,))\r\n\r\nx=buggy_random()(inputs)\r\nx=buggy_random()(x)\r\nx=buggy_random()(x)\r\noutputs=buggy_random()(x)\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nh=tf.function(_h)\r\nprint('first run:')\r\nh(model,tf.constant(np.random.rand(64,784)))\r\nprint('second run:')\r\nh(model,tf.constant(np.random.rand(64,784)))\r\n\r\n'''\r\nfirst run:\r\n[0.277247906 0.994074821 0.379808426 0.71479249 0.50061965]\r\n[0.277247906 0.994074821 0.379808426 0.71479249 0.50061965]\r\n[0.277247906 0.994074821 0.379808426 0.71479249 0.50061965]\r\n[0.277247906 0.994074821 0.379808426 0.71479249 0.50061965]\r\nsecond run:\r\n[0.176383495 0.109812617 0.334476113 0.66576612 0.116794825]\r\n[0.176383495 0.109812617 0.334476113 0.66576612 0.116794825]\r\n[0.176383495 0.109812617 0.334476113 0.66576612 0.116794825]\r\n[0.176383495 0.109812617 0.334476113 0.66576612 0.116794825]\r\n'''\r\n```", "The new API is the only working randomness API in tensorflow.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef _h(model,x):\r\n    model(x)\r\n\r\nclass fixed_random(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        self.g = tf.random.Generator.from_seed(np.random.randint(2147483647))\r\n        super().__init__(**kwargs)\r\n    def call(self, inputs):\r\n        for i in tf.range(tf.constant(1)):\r\n            tf.print(self.g.uniform([5,]),summarize=-1)\r\n        return inputs\r\n\r\ntf.random.set_seed(123)\r\nnp.random.seed(88883)\r\ninputs = tf.keras.Input(shape=(784,))\r\n\r\nx=fixed_random()(inputs)\r\nx=fixed_random()(x)\r\nx=fixed_random()(x)\r\noutputs=fixed_random()(x)\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nh=tf.function(_h)\r\n\r\nprint('first run')\r\nh(model,tf.constant(np.random.rand(64,784)))\r\nprint('second run')\r\nh(model,tf.constant(np.random.rand(64,784)))\r\n'''\r\nfirst run\r\n[0.898076177 0.814805269 0.685280442 0.624640703 0.637957335]\r\n[0.438643932 0.923692703 0.202350736 0.259410977 0.551186323]\r\n[0.137919307 0.688777804 0.0962672234 0.532677054 0.639819145]\r\n[0.533203602 0.589777708 0.596117854 0.147099137 0.760685086]\r\nsecond run\r\n[0.391331553 0.65578413 0.0532933474 0.92483747 0.198105812]\r\n[0.0157146454 0.148599625 0.300893545 0.211638093 0.460565567]\r\n[0.310432553 0.377720356 0.230332017 0.0171860456 0.905866146]\r\n[0.927248478 0.930060267 0.906018138 0.391339064 0.362027764]\r\n'''\r\n```"]}, {"number": 49915, "title": "Cumsum reference op test", "body": "This adds the test  for the CUMSUM reference op code.\r\n\r\nThis PR is in reference to issue #49913 ", "comments": ["Thanks for creating this PR. Instead of creating duplicated test cases, could you merge these new test cases with the existing testing cases under cumsum_test.cc file?\r\n\r\nFor example, \r\n\r\n```\r\n\r\nconst auto kKernelMap = new std::map<string, TfLiteRegistration*>({\r\n    {\"Reference\", ops::builtin::Register_TRANSPOSECONV_REF()},\r\n    {\"GenericOptimized\", ops::builtin::Register_TRANSPOSECONV_GENERIC_OPT()},\r\n});\r\n...\r\n\r\n\r\nINSTANTIATE_TEST_SUITE_P(\r\n    TransposeConvOpTest, TransposeConvOpTest,\r\n    ::testing::Combine(\r\n        ::testing::ValuesIn(SingleOpTest::GetKernelTags(*kKernelMap)),\r\n        ::testing::Values(TestType::kConst, TestType::kDynamic)));\r\n```\r\n\r\nSee also https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/transpose_conv_test.cc#L129.", "@abattery The suggested method for combining the tests will not work here.  The CUMSUM kernel only has a single registration and the Eval code for that kernel will only call the optimized operator code.", "It would be better to have the reference op version in order to make the reference kernel version visible to users.\r\n\r\nCould you make the changes in the cumsum.cc to have the reference op registration like we have the above transpose conv op?", "With TFLM moving to its own GitHub repository, we are not going to be merging any TFLM specific pull requests in the TensorFlow repository starting today.\r\n\r\nI am closing the current PR but please feel free to open a new PR in https://github.com/tensorflow/tflite-micro/.\r\n\r\nhttps://groups.google.com/a/tensorflow.org/g/micro/c/W4DACgjPmOE\r\n", "This PR is for TfLite, not for TFLM"]}, {"number": 49914, "title": "SafeH2DMemcpy removed in thunk", "body": "I am looking into some profiling issue of XLA. I noticed that in tf2.5 and newer **SafeH2DMemcpy** part is remove from convolution_thunk.cc/custom_call_thunk/convolution_thunk.\r\n\r\nBy looking into old code of convlution_thunk's memcpy part\r\n```python\r\n\r\n  // Write the output tuple.\r\n  const int kNumOutputs = 2;\r\n  auto ptrs = absl::make_unique<void*[]>(kNumOutputs);\r\n  ptrs[0] = result_buffer.opaque();\r\n  ptrs[1] = scratch.opaque();\r\n  se::DeviceMemory<void*> tuple_addr(\r\n      buffer_allocations.GetDeviceAddress(tuple_result_buffer_));\r\n  SafeH2DMemcpy(tuple_addr, std::move(ptrs), kNumOutputs, params.stream,\r\n                params.deferred_host_callbacks);\r\n```\r\nI am wondering\r\n- What are result_buffer, scratch_buffer and tuple_result_buffer here? Are they cpu buffer or gpu buffer?\r\n- Why it does a H2D copy instead of D2H copy? Why result is not from gpu to cpu? \r\n- Why this memcpy is now removed? What is the original purpose of this?\r\n\r\nWould really appreciate any ideas on this.", "comments": []}, {"number": 49913, "title": "TfLite CUMSUM operator missing reference test ", "body": "While porting the CUMSUM kernel op to TFLM, it was discovered that the reference code for the operator in tensorflow/lite/kernels/internal/reference/cumsum.h did not have a dedicated test.  The existing TfLite test for the CUMSUM kernel only tested the optimized version of the operator.  The Eval code for the TfLite CUMSUM kernel only calls the optimized operator code.\r\n\r\nA test for the reference version of the operator has been created.\r\n\r\n", "comments": []}, {"number": 49910, "title": "Syntax warning in lib/python3.9/site-packages/tensorflow/python/keras/benchmarks/benchmark_util.py", "body": "Hi\r\n\r\nAfter moving to python3.9 I get syntax warning in: \r\nlib/python3.9/site-packages/tensorflow/python/keras/benchmarks/benchmark_util.py lines 142, 144, 146\r\n\r\nerror message:\r\npythonVirtualEnv/lib/python3.9/site-packages/tensorflow/python/keras/benchmarks/benchmark_util.py:142: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\n\r\nCode causing error:\r\nif 'x' is None:\r\n    raise ValueError('Input data is required.')\r\n  if 'optimizer' is None:\r\n    raise ValueError('Optimizer is required.')\r\n  if 'loss' is None:\r\n    raise ValueError('Loss function is required.')\r\n\r\n'x' 'optimizer' 'loss' isn't None since it is a string so python3.9 do not like it and raise a warning.\r\nIn order to fix it I removed those lines locally then no longer getting syntax warning.\r\n\r\nCan you fix it? And when?\r\n\r\nThanks", "comments": ["@Nizarazo ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet and the TensorFlow version you are using.\r\n\r\nThanks!", "Hi @tilakrayal \r\n\r\nThanks for your reply.\r\n\r\nI am using tensorflow 2.5.0 release, the code snippet is in my previous comment I will post it again here:\r\nf 'x' is None:\r\nraise ValueError('Input data is required.')\r\nif 'optimizer' is None:\r\nraise ValueError('Optimizer is required.')\r\nif 'loss' is None:\r\nraise ValueError('Loss function is required.')\r\n\r\nThis code is located in tensorflow/python/keras/benchmarks/benchmark_util.py line 142\r\n\r\nAnything else needed?", "Hi @tilakrayal\r\n\r\nWill it be fixed? \r\nif yes when and in which version?\r\n\r\nThanks   ", "@Nizarazo ,\r\n\r\nI was able to execute the above mentioned code without any errors.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/57733590354f2fd55ccb155ce310dd7e/49910.ipynb).\r\n\r\nCan you please provide gist which you are facing issue in executing the code.It helps to analyse the issue.\r\n\r\nThanks!", "Hi\r\n\r\nWe build executable file using pyinstaller and one of our packages is tensorflow so while building the exe, the pyinstaller raises an error due to this code located in tensorflow/keras and as a result it stops building the executable.\r\nSo here is the errors from pyinstaller when it analysis your tensorflow/keras module:\r\n\r\npythonVirtualEnv/lib/python3.9/site-packages/tensorflow/python/keras/benchmarks/benchmark_util.py:142: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\n\r\n  if 'x' is None:\r\n\r\npythonVirtualEnv/lib/python3.9/site-packages/tensorflow/python/keras/benchmarks/benchmark_util.py:144: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\n\r\n  if 'optimizer' is None:\r\n\r\npythonVirtualEnv/lib/python3.9/site-packages/tensorflow/python/keras/benchmarks/benchmark_util.py:146: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\n\r\n  if 'loss' is None:\r\n\r\nYou can reproduce the issue if you run pyinstaller with tensorflow as a requirement in your python environment.\r\n\r\nThanks", "Hi @Saduf2019 \r\n\r\nAny more info is needed or we are all set and waiting for your fix?\r\n\r\nThanks", "Hi @Saduf2019 \r\n\r\nI am still waiting your reply regarding my previous comment, is there any more info needed?\r\n\r\nThanks", "@Nizarazo \r\nWould you want to try with python 3.8 as there are no errors with it.", "Hi @Saduf2019 \r\n\r\nNo because python3.8 doesn't support Big Sur, Tensorflow needs to be supported in python3.9 since it is the first python to support Big Sur 11 please see documentation here:\r\n\r\n\"3.9.1 is the first version of Python to support macOS 11 Big Sur.\" from:\r\nhttps://www.python.org/downloads/release/python-391/\r\n\r\nThanks", "Hi @jvishnuvardhan \r\n\r\nIs there any more info needed for this ticket or we are all set and waiting for the fix?\r\n\r\nThanks", "Hi @jvishnuvardhan \r\n\r\nAny news regarding the fix?\r\n\r\nThanks", "Hi @jvishnuvardhan \r\n\r\nWhat does it mean \"awaiting tensorflower\" status?\r\n\r\nThanks", "@Nizarazo It means it was assigned to a Tensorflow code owner and waiting for the code owner to respond once root-cause was found. Thanks!", "Hi @jvishnuvardhan \r\n\r\nThanks for your reply.\r\nSo when the code owner supposed to respond on that it has been in \"awaiting tensorflower\" state for 17 days?\r\n\r\nThanks", "Hi @jvishnuvardhan \r\n\r\nStill waiting for your reply regarding my previous comment.\r\n\r\nThanks", "Hi @jvishnuvardhan \r\n\r\nI am still waiting for the code owner to fix it any idea what is the status of it? \r\nI have been waiting more than one month already since 16.6.\r\n\r\nThanks"]}, {"number": 49906, "title": "Eager Execution Documentation includes Sections not Describing Relation or relevance to Eager Execution", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/eager\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIn the documentation for Eager Execution, the following sections are included but no detail is provided as to why these sections are related to Eager Execution. A description is required to indicate if these capabilities/operations are only available, operate differently or in what other way are related to Eager Execution. Right now, it is entirely unclear why these sections are located on the Eager Execution page.\r\n\r\n\"Object-based saving\"\r\n\"Summaries and TensorBoard\"\r\n\"Advanced automatic differentiation topics\"\r\n\"Performance\"", "comments": ["Appreciate this @lonerzzz. We'll look into this cc @MarkDaoust ", "@lonerzzz \r\ncould you please refer to the latest page and let us know if this resolves your queries.", "This hasn't been resolved. I feel like the entire content of this doc is covered by the other docs in that section. I'm hoping to delete this, and possibly replace it with a \"quickstart\" guide."]}, {"number": 49900, "title": "No consideration of Activation Functions in the implementation of Initializers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Everything\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): NA\r\n- TensorFlow version (use command below): 2.5\r\n- Python version:\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior** : In the Tensorflow Source Code of neither the [He Initializer](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/initializers/initializers_v2.py#L849-L883), nor the [Glorot Initializer](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/initializers/initializers_v2.py#L711-L752), along with the `Code` of the **`Parent Class`**, [VarianceScaling Initializer](https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/initializers/initializers_v2.py#L430-L531), there is no mention of the dependency on **`Activation Function's`** in the code of **`Initializers`**.\r\n\r\n**Describe the expected behavior** : As per the screenshots shown below, (first one is from the Table 11-1 of the book, **`Hands on Machine Learning with Scikit-Learn and Tensorflow`**, **`Version 2`**, and the bottom one is from the **`First Version`** of the same book), the formula for [Glorot](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal) and [He](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal) Initializers should be dependent on the **`Activation Functions`**. \r\n\r\n![image](https://user-images.githubusercontent.com/48206667/120094668-66661a80-c13f-11eb-8398-8c381f56e182.png)\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/48206667/120095045-4a637880-c141-11eb-9550-c4524bb728d5.png)\r\n\r\n**Standalone code to reproduce the issue** : Source Code of the [He Initializer](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/initializers/initializers_v2.py#L849-L883), [Glorot Initializer](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/initializers/initializers_v2.py#L711-L752), and the `Code` of the **`Parent Class`**, [VarianceScaling Initializer](https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/initializers/initializers_v2.py#L430-L531).", "comments": []}, {"number": 49899, "title": "TfLite C API safety requirements", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/eaa6753876e6a5974258e7244f46bfe8b27f906d/tensorflow/lite/c/c_api.h#L142-L152\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nWhat would happen if the user does not follow the calling order?\r\n\r\nDoes the interpreter perform a runtime safety check?\r\n\r\nCan I read the input tensors before I fill values into them?", "comments": ["When you try to access `TfLiteTensor` objects through the `TfLiteInterpreterGetInputTensor` method before the allocation, the `TfLiteTensor`'s `data` field won't have valid memory address since it hasn't allocated.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> When you try to access `TfLiteTensor` objects through the `TfLiteInterpreterGetInputTensor` method before the allocation, the `TfLiteTensor`'s `data` field won't have valid memory address since it hasn't allocated.\r\n\r\nWhat if I read the tensor immediately after allocation? Is it uninitialized or filled with zeros?"]}, {"number": 49896, "title": "Means of GlorotNormal Initializer and HeNormal Initializer are not zero, respectively", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NA\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Google Colab\r\n- TensorFlow version (use command below): 2.5\r\n- Python version:\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior** : **`Mean`** of the **`Initial Weights`** using **`Glorot Normal Initializer`** is not **`zero`**. Same is the case with **`HeNormal Initializer`**.\r\n\r\n**Describe the expected behavior**: As per the documentation of [Glorot Normal](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal), and [HeNormal](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal), **`mean`** of the **`Normal Distribution`** of the **`Initial Weights`** should be **`zero`**. \r\n\r\n> Draws samples from a truncated normal distribution centered on 0\r\n\r\n**Standalone code to reproduce the issue** : [Colab Gist for Glorot Normal](https://colab.research.google.com/gist/rakeshmothukuru1/a44f7aaa1fd261247a1388c2e2f5a167/glorot_initializer_bug.ipynb)\r\n\r\n[Colab Gist for HeNormal](https://colab.research.google.com/gist/rakeshmothukuru1/c2f6a4896f0a5414581bd438218f2eff/he_normal_initializer_bug.ipynb).", "comments": ["Using a larger number of weights seems to show convergence of the sample mean to 0 (ie, central limit theorem):\r\nhttps://colab.research.google.com/gist/benyeoh/3ffec8ef0262e238918c36e00e11ccd2/glorot_initializer_bug.ipynb\r\nhttps://colab.research.google.com/gist/benyeoh/f56b10487c343cabc3e509eab078fe3e/he_normal_initializer_bug.ipynb\r\n", "@rakeshmothukuru1 ,\r\n\r\nPlease take a look at above @benyeoh  comment and let us know if you are still facing the same issue? Thanks!", "> Using a larger number of weights seems to show convergence of the sample mean to 0 (ie, central limit theorem):\r\n> https://colab.research.google.com/gist/benyeoh/3ffec8ef0262e238918c36e00e11ccd2/glorot_initializer_bug.ipynb\r\n> https://colab.research.google.com/gist/benyeoh/f56b10487c343cabc3e509eab078fe3e/he_normal_initializer_bug.ipynb\r\n\r\n@benyeoh,\r\nThank you for your response. I'm not sure if this is acceptable because we don't always use huge number of **`Weights`**.  The heuristic behind **`Glorot`** and **`He`** **`Initializers`** is to have a **`Mean`** of **`Zero`** and the **`Weights`** should be Initialized accordingly, irrespective of the number of Neurons used.", "@Saduf2019 ,\r\nI was able to reproduce the issue in tf v2.4,v2.5 and nightly.Please find the gist here.[Gist1](https://colab.research.google.com/gist/tilakrayal/fd764d6da6e2799c204acc7e8561f0e4/49896-glorot_initializer_bug.ipynb),[Gist2](https://colab.research.google.com/gist/tilakrayal/f89047ab8e3a7dbde283dab4029e8d48/49896-he_normal_initializer_bug.ipynb)", "> Thank you for your response. I'm not sure if this is acceptable because we don't always use huge number of **`Weights`**. The heuristic behind **`Glorot`** and **`He`** **`Initializers`** is to have a **`Mean`** of **`Zero`** and the **`Weights`** should be Initialized accordingly, irrespective of the number of Neurons used.\r\n\r\n@rakeshmothukuru1 \r\nWell, since you are drawing random samples to initialize the weights (from a truncated normal distribution with an arbitrary scale and 0 mean), then by definition the **sample mean** is unlikely to be exactly 0 especially for a small number of samples. But by the Central Limit Theorem, as the sample size approaches infinity, the sample mean will converge to the true **population mean**, which is 0 in this case.\r\n\r\nFrom my earlier gists, by increasing the number of weights and thus the number of samples, it seems like the sample means were converging to 0.\r\n\r\nIf you want something that always forces the sample mean to be exactly 0, then I suppose you can just manually subtract a bias that is equal to the sample mean.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@benyeoh,\r\nIn your Gist, you were using 4096 Neurons, which is a very big number. We usually use some hundreds of Neurons. So, sorry but I'm still not convinced. \r\n\r\nI want to listen it from Google Engineers too, about this bug. "]}, {"number": 49889, "title": "PiecewiseConstantDecay doesn't work with Wrapping Optimizer on GPUs", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5/2.6\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: 8.2\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using the PiecewiseConstantDecay for an optimizer and wrapping optimizer, like:\r\n```python\r\nlr_fn = PiecewiseConstantDecay()\r\nopt = SGD(lr_fn)\r\nopt = WrapOpt(opt)\r\n```\r\nWe will hit the error of\r\n```\r\nInvalidArgumentError: Cannot assign a device for operation sequential_1/dense_1/Tensordot/ReadVariableOp: Could not satisfy explicit device specification '' because the node {{colocation_node sequential_1/dense_1/Tensordot/ReadVariableOp}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices \r\n```\r\nNote, this error seems to be hit only when PiecewiseConstantDecay schedule is used on GPUs.\r\n\r\n**Describe the expected behavior**\r\n\r\nWe shouldn't see such error when using PiecewiseConstantDecay on GPUs.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nBelow is the colab link and please repro it with ***Runtime=GPU***.\r\nhttps://colab.research.google.com/drive/1QPx4IqQNVpSR-ALfPYbjJjRUffyHo06G?usp=sharing\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers, optimizers, models\r\nprint(tf.__version__)\r\nclass OptimizerWrapper(optimizers.Optimizer):\r\n  def __init__(self, optimizer, name=None, **kwargs):\r\n    super(OptimizerWrapper, self).__init__(name, **kwargs)\r\n    self._optimizer = optimizer\r\n\r\n  def _create_slots(self, var_list):\r\n    self._optimizer._create_slots(var_list)\r\n\r\n  def _resource_apply_dense(self, grad, var):\r\n    return self._optimizer._resource_apply_dense(grad, var)\r\n\r\n  def _resource_apply_sparse(self, grad, var):\r\n    return self._optimizer._resource_apply_sparse(grad, var)\r\n\r\n  def get_config(self):\r\n    return self._optimizer.get_config()\r\n\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(layers.Dense(8))\r\nx = tf.constant(12., shape=(5, 1, 2, 4))\r\nboundaries = [100000, 110000]\r\nvalues = [1.0, 0.5, 0.1]\r\nlearning_rate_fn = optimizers.schedules.PiecewiseConstantDecay(\r\n    boundaries, values)\r\n#learning_rate_fn = optimizers.schedules.ExponentialDecay(\r\n#    0.1, decay_steps=100000, decay_rate=0.96, staircase=True)\r\n#learning_rate_fn = optimizers.schedules.PolynomialDecay(\r\n#    0.1, 10000, 0.01, power=0.5)\r\nopt = optimizers.SGD(learning_rate=learning_rate_fn, momentum=1.0)\r\nopt = OptimizerWrapper(opt)\r\n\r\n@tf.function\r\ndef train_step(x):\r\n  with tf.GradientTape(persistent=True) as tape:\r\n    y = model(x)\r\n    loss = tf.reduce_mean(y)\r\n\r\n  grads = tape.gradient(loss, model.variables)\r\n  opt.apply_gradients(zip(grads, model.variables))\r\n  return loss\r\n\r\nfor i in range(3):\r\n  loss = train_step(x)\r\n  print(\"Loss:\", loss)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-2-354cdd24a945> in <module>()\r\n     45 \r\n     46 for i in range(3):\r\n---> 47   loss = train_step(x)\r\n     48   print(\"Loss:\", loss)\r\n     49 \r\n\r\n5 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError: Cannot assign a device for operation sequential_1/dense_1/Tensordot/ReadVariableOp: Could not satisfy explicit device specification '' because the node {{colocation_node sequential_1/dense_1/Tensordot/ReadVariableOp}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. \r\nColocation Debug Info:\r\nColocation group had the following types and supported devices: \r\nRoot Member(assigned_device_name_index_=1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n```\r\n\r\ncc. @nluehr ", "comments": ["@kaixih ,\r\n\r\nI was able to reproduce the issue in TF v2.4 and v2.5. In nightly i wasn't able to find the issue.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/4b1783af1b8cdd3e233575eea51411e6/49889.ipynb).\r\n\r\nThanks!", "@tilakrayal Thanks for trying this out.\r\n\r\nYes, it seems it is working on the colab with the nightly wheel. However, I can still see the error when I use the nightly-gpu docker on my machine. This is what I have tried:\r\n(1) pull the tensorflow/tensorflow:nightly-gpu: I can see the version: 2.6.0-dev20210601 and still hit the error.\r\n(2) pull the tensorflow/tensorflow:nightly: I can see the version: 2.6.0-dev20210601 and no error.\r\n\r\nSo, I have a feeling that the colab might not use the GPU env for the nightly wheel. Can you try it with docker images? Thanks.", "Some updates:\r\nWhen using the nightly-gpu docker image (ie. the above (1) container), if I disable GPU, the error is gone:\r\n```\r\nCUDA_VISIBLE_DEVICES= python error_repro.py\r\n```\r\nSo, I suspect the colab might not use the GPU for some cases even if the GPU runtime is selected. Please let me know your thoughts. Thanks.", "Hi @tilakrayal , I tweaked your gist code by adding this following code after printing the TF version.\r\n\r\n```python\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\nprint(\"Num GPUs:\", len(physical_devices))\r\n```\r\nIt confirms my findings that the 2.4 and 2.5 shows the GPU number is 1 but the nightly shows the GPU number is 0.\r\n", "@kaixih,\r\nYour code could be run successfully with **`GPU Runtime`** by commenting the **`Wrapper for Optimizer`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/4e719f4b55293ce49aff36103a45a0f9/gh_49889.ipynb) of working code.\r\n\r\nIs there any specific reason for using the **`Wrapper`** for the **`Optimizer`** instead of the **`Optimizer`** ?", "@rmothukuru Yes, I agree the original optimizer works fine. However, we are designing a wrapper optimizer to postprocess the variables after each updating and this design works fine with any other learning schedule or CPU env. Only the PiecewiseConstantSchedule + GPU will hit the error. We think our example is a legit use case and should be working.\r\n\r\nAlso, we found that when using the eager mode (by commenting out the `@tf.function`), this use case works fine. So, I believe it is a TF bug and the implementation of PiecewiseConstantSchedule might trigger some weird situation when using GPU + graph mode. What's your advise?", "Here are some more info that might help:\r\nTo mimic the PiecewiseConstantDecay, I created this\r\n```python\r\nclass SimplePiecewiseConstantDecay(\r\n    tf.keras.optimizers.schedules.LearningRateSchedule):\r\n  def __init__(self, boundaries, values, name=None):\r\n    super(SimplePiecewiseConstantDecay, self).__init__()\r\n    if len(boundaries) != len(values) - 1:\r\n      raise ValueError(\r\n          \"The length of boundaries should be 1 less than the length of values\"\r\n      )\r\n    self.boundaries = boundaries\r\n    self.values = values\r\n    self.name = name\r\n\r\n  def __call__(self, step):\r\n    pred_fn_pairs = []\r\n    pred_fn_pairs.append((step <= self.boundaries[0], lambda: self.values[0]))\r\n    pred_fn_pairs.append((step > self.boundaries[-1], lambda: self.values[-1]))\r\n    for low, high, v in zip(self.boundaries[:-1], self.boundaries[1:],\r\n                            self.values[1:-1]):\r\n      # Need to bind v here; can do this with lambda v=v: ...\r\n      pred = (step > low) & (step <= high)\r\n      pred_fn_pairs.append((pred, lambda v=v: v))\r\n\r\n    # The default isn't needed here because our conditions are mutually\r\n    # exclusive and exhaustive, but tf.case requires it.\r\n    default = lambda: self.values[0]\r\n    return tf.case(pred_fn_pairs, default, exclusive=True)\r\n\r\n  def get_config(self):\r\n    return {\"boundaries\": self.boundaries,\r\n            \"values\": self.values,\r\n            \"name\": self.name}\r\n```\r\nBy using it instead of `optimizers.schedules.PiecewiseConstantDecay` in the original repro, we can still repro the error. From the log, I suspect the TF cannot correctly deal with the device assignment with the `tf.case` op in this use case.", "I just played with the `tf.case` vs `tf.cond` and it confirms only the `tf.case` will hit the error:\r\n```python\r\n  # This doesn't work!\r\n  def __call__(self, step):\r\n    pred_fn_pairs = []\r\n    pred_fn_pairs.append((step <= self.boundaries[0], lambda: self.values[0]))\r\n    pred_fn_pairs.append((step > self.boundaries[-1], lambda: self.values[-1]))\r\n    default = lambda: self.values[0]\r\n    return tf.case(pred_fn_pairs, default, exclusive=True)\r\n```\r\n```python\r\n  # This works fine!\r\n  def __call__(self, step):\r\n    def case0():\r\n      return self.values[0]\r\n    def case1_2():\r\n        return tf.cond(step > self.boundaries[-1], lambda: self.values[-1],\r\n                       lambda: self.values[0])\r\n    return tf.cond(step <= self.boundaries[0], case0, case1_2)\r\n```\r\nSee the colab link https://colab.research.google.com/drive/1KUu7HSX4J7nNvYEaEDXCUZhGgxkoGiWe?usp=sharing.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "No, the issue is not resolved yet.", "Any news @rmothukuru? Thanks.", "I'm not sure what's causing the issue, but delegating `apply_gradients` in `OptimizerWrapper` solves the issue:\r\n\r\n```python\r\nclass OptimizerWrapper(optimizers.Optimizer):\r\n\r\n  ...\r\n\r\n  def apply_gradients(self,\r\n                      grads_and_vars,\r\n                      name=None,\r\n                      experimental_aggregate_gradients=True):\r\n    self._optimizer.apply_gradients(grads_and_vars, name,\r\n                                    experimental_aggregate_gradients)\r\n```\r\n\r\nThis is what `tf.keras.mixed_precision.LossScaleOptimizer` does, which is the only optimizer wrapper within Keras.\r\n\r\nStill, this is a bad error message and this issue should be fixed. /CC @fchollet @tomerk, can either of you take a look or triage?", "Thanks @reedwm for this WAR. It works for this repro script. However, in our real case, it may be not easy for us to delegate the `apply_gradients()` in the wrapper. Roughly, what we are trying to do is that for some vars, we need to prune them before updating. And this logic can be implemented natually in `_resource_apply_dense(self, grad, var)` like:\r\n```python\r\ndef _resource_apply_dense(self, grad, var):\r\n  if var is elegible:\r\n    self._optimizer._resource_apply_dense(...)\r\n    new_var = prune(var)\r\n    return var.assign(new_var)\r\n  else:\r\n    return self._optimizer._resource_apply_dense(...)\r\n```\r\nHowever, with the above delegating solution, it is not simple for us to insert the pruning in front of var updating. So, we are still looking forwards to more info on the root cause and solution of this issue.", "As a workaround, delegating `_prepare` in `OptimizerWrapper` and adding an `apply_state` argument to `_resource_apply_dense` fixes the issue:\r\n\r\n```python\r\nclass OptimizerWrapper(optimizers.Optimizer):\r\n  def __init__(self, optimizer, name=None, **kwargs):\r\n    super(OptimizerWrapper, self).__init__(name, **kwargs)\r\n    self._optimizer = optimizer\r\n\r\n  def _prepare(self, var_list):\r\n    return self._optimizer._prepare(var_list)\r\n\r\n  def _create_slots(self, var_list):\r\n    self._optimizer._create_slots(var_list)\r\n\r\n  def _resource_apply_dense(self, grad, var, apply_state):\r\n    return self._optimizer._resource_apply_dense(grad, var, apply_state)\r\n\r\n  def _resource_apply_sparse(self, grad, var):\r\n    return self._optimizer._resource_apply_sparse(grad, var)\r\n\r\n  def get_config(self):\r\n    return self._optimizer.get_config()\r\n```\r\n\r\nStill, this shouldn't be necessary. Will keep investigating.", "I found the root cause. The issue occurs if an optimizer subclass creates a `tf.string` tensor in `_resource_apply_dense`. The reason is due to [this colocation constraint](https://github.com/keras-team/keras/blob/06b04aeb745c90c5368be02928b08cc7fd592659/keras/optimizer_v2/optimizer_v2.py#L702) in Optimizer. This forces everything in `_resource_apply_dense` to be placed on the GPU if the variable is on the GPU (which is the case for practically every model run on the GPU). However, string tensors can only be placed on the CPU, so the error occurs if a string tensor is created under that colocation constraint, which includes the call to `_resource_apply_dense`.\r\n\r\nAs you noticed, an error occurs when you use a PiecewiseConstantDecay (or any `LearningRateSchedule` that calls `tf.case(..., exclusive=True)`) with certain custom optimizers (such as your OptimizerWrapper). This is because `tf.case` creates a string tensor, since it creates an Assert op which [takes in a string tensor](https://github.com/tensorflow/tensorflow/blob/53c3fc3e5a7cafc57e6644cbbbedea425a7b1db7/tensorflow/python/ops/control_flow_ops.py#L3112). A LearningRateSchedule is called from `_fallback_apply_state`, which is called from `_resource_apply_dense` in many optimizers if the `apply_state` argument is not passed through.\r\n\r\n/CC @fchollet @rohan100jain what do you think the best solution is here? Perhaps we should have the concept of a \"soft colocation constraint\", which allows ops to be placed on the CPU if the colocation constraint cannot be satisfied? Or we can have Keras simply not use a colocation constraint, as [it is deprecated](https://www.tensorflow.org/api_docs/python/tf/compat/v1/colocate_with), and we could instead have grappler or some other optimization to move ops to the GPU if necessary to get optimal performance.", "Also /CC @yuefengz, as the issue occurs with `distribution.extended.colocate_vars_with`."]}, {"number": 49867, "title": "Deploy micro_speech to ESP32 , is running ,but do not get any output,how to modify?where is wrong ", "body": "I (63) boot: Chip Revision: 1\r\nI (64) boot_comm: chip revision: 1, min. bootloader chip revision: 0\r\nI (39) boot: ESP-IDF v4.0.2-442-g41efdb0b3 2nd stage bootloader\r\nI (39) boot: compile time 22:29:55\r\nI (39) boot: Enabling RNG early entropy source...\r\nI (45) qio_mode: Enabling default flash chip QIO\r\nI (50) boot: SPI Speed      : 80MHz\r\nI (54) boot: SPI Mode       : QIO\r\nI (58) boot: SPI Flash Size : 2MB\r\nI (62) boot: Partition Table:\r\nI (66) boot: ## Label            Usage          Type ST Offset   Length\r\nI (73) boot:  0 nvs              WiFi data        01 02 00009000 00006000\r\nI (81) boot:  1 phy_init         RF data          01 01 0000f000 00001000\r\nI (88) boot:  2 factory          factory app      00 00 00010000 00100000\r\nI (96) boot: End of partition table\r\nI (100) boot_comm: chip revision: 1, min. application chip revision: 0\r\nI (107) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x0e314 ( 58132) map\r\nI (132) esp_image: segment 1: paddr=0x0001e33c vaddr=0x3ffb0000 size=0x01cd4 (  7380) load\r\nI (134) esp_image: segment 2: paddr=0x00020018 vaddr=0x400d0018 size=0x23d98 (146840) map\r\n0x400d0018: _stext at ??:?\r\n\r\nI (178) esp_image: segment 3: paddr=0x00043db8 vaddr=0x3ffb1cd4 size=0x002ac (   684) load\r\nI (179) esp_image: segment 4: paddr=0x0004406c vaddr=0x40080000 size=0x00400 (  1024) load\r\n0x40080000: _WindowOverflow4 at /Users/cafe/esp-idf/components/freertos/xtensa_vectors.S:1778\r\n\r\nI (185) esp_image: segment 5: paddr=0x00044474 vaddr=0x40080400 size=0x09a8c ( 39564) load\r\nI (213) boot: Loaded app from partition at offset 0x10000\r\nI (213) boot: Disabling RNG early entropy source...\r\nI (214) cpu_start: Pro cpu up.\r\nI (218) cpu_start: Application information:\r\nI (222) cpu_start: Project name:     micro_speech\r\nI (228) cpu_start: App version:      v1.12.1-57713-gda15b34d114-dirt\r\nI (235) cpu_start: Compile time:     May 28 2021 22:29:50\r\nI (241) cpu_start: ELF file SHA256:  f9f5b5eb91cff7e9...\r\nI (247) cpu_start: ESP-IDF:          v4.0.2-442-g41efdb0b3\r\nI (253) cpu_start: Starting app cpu, entry point is 0x40081064\r\n0x40081064: call_start_cpu1 at /Users/cafe/esp-idf/components/esp32/cpu_start.c:272\r\n\r\nI (0) cpu_start: App cpu up.\r\nI (263) heap_init: Initializing. RAM available for dynamic allocation:\r\nI (270) heap_init: At 3FFAE6E0 len 00001920 (6 KiB): DRAM\r\nI (276) heap_init: At 3FFB6858 len 000297A8 (165 KiB): DRAM\r\nI (283) heap_init: At 3FFE0440 len 00003AE0 (14 KiB): D/IRAM\r\nI (289) heap_init: At 3FFE4350 len 0001BCB0 (111 KiB): D/IRAM\r\nI (295) heap_init: At 40089E8C len 00016174 (88 KiB): IRAM\r\nI (302) cpu_start: Pro cpu start user code\r\nI (319) spi_flash: detected chip: generic\r\nI (319) spi_flash: flash io: qio\r\nW (319) spi_flash: Detected size(4096k) larger than the size in the binary image header(2048k). Using the size in the binary image header.\r\nI (330) cpu_start: Starting scheduler on PRO CPU.\r\nI (0) cpu_start: Starting scheduler on APP CPU.\r\ninit success\r\nI (424) I2S: DMA Malloc info, datalen=blocksize=2400, dma_buf_count=3\r\nI (424) I2S: PLL_D2: Req RATE: 16000, real rate: 16025.000, BITS: 32, CLKM: 39, BCK: 4, MCLK: 4096000.000, SCLK: 1025600.000000, diva: 64, divb: 4\r\nW (474) TF_LITE_AUDIO_PROVIDER: mic data:3200\r\nI (474) TF_LITE_AUDIO_PROVIDER: Audio Recording started\r\n", "comments": []}, {"number": 49854, "title": "tensorflowlite quantized model get incorrect output in tensorflowlite C API", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): \r\n- Tensorflow version (commit SHA if source): r2.4\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Qualcomm Snapdragon XR2 \r\n\r\n**Describe the problem**\r\nI have two tflite model, one is quantized to float16 the other is quantized to full integer. Both model can get correct result on python for PC. But only model quantized to float16 get correct result on tensorflowlite C API. Model quantized to int8 with different input always get the same output. Both model run for CPU on Qualcomm Snapdragon XR2.\r\n\r\nModel quantized to float 16: [link](https://drive.google.com/file/d/119wMGk2Z-Kx4o-Zxji4h-KOmDVuwG3Rq/view?usp=sharing)\r\nModel quantized to full integer: [link](https://drive.google.com/file/d/1oEHtKag7AM_FFNkgyIGg2PsAGt3zERKm/view?usp=sharing)\r\n\r\nThanks\r\n", "comments": []}, {"number": 49853, "title": "React native  runModelOnImage have an error \"Cannot convert between a TensorFlowLite tensor with type UINT8 and a Java object of type [[[[F (which is compatible with the TensorFlowLite type FLOAT32)\"", "body": "When I training data  and export to Tflite on centos7 using Anaconda3 with Tensorflow, it have warning\r\n![image](https://user-images.githubusercontent.com/44739285/119945247-63dcb700-bfbf-11eb-8730-2d50d6bedf12.png)\r\nThe [model.tflite](https://drive.google.com/file/d/1L5KFlHSv1NajUM5Ltpp354O-i8AzKAkH/view?usp=sharing) and [labels.txt](https://drive.google.com/file/d/1eXDEO81Ach45qJTryBWX_O0pupS61GiW/view?usp=sharing) after export.\r\nBelow is my code and the datasets [here](https://drive.google.com/file/d/1SE8DJNU8ctaim2BptUMWZ3dkxPPVSpfT/view?usp=sharing).\r\n```python\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nassert tf.__version__.startswith('2')\r\n\r\nfrom tflite_model_maker import configs\r\nfrom tflite_model_maker import ExportFormat\r\nfrom tflite_model_maker import image_classifier\r\nfrom tflite_model_maker import model_spec\r\nfrom tflite_model_maker import ImageClassifierDataLoader\r\nimport matplotlib.pyplot as plt\r\n\r\ndata = ImageClassifierDataLoader.from_folder('images')\r\ntrain_data, rest_data = data.split(0.8)\r\nvalidation_data, test_data = rest_data.split(0.5)\r\nplt.figure(figsize=(10,10))\r\nfor i, (image, label) in enumerate(data.gen_dataset().unbatch().take(25)):\r\n  plt.subplot(5,5,i+1)\r\n  plt.xticks([])\r\n  plt.yticks([])\r\n  plt.grid(False)\r\n  plt.imshow(image.numpy(), cmap=plt.cm.gray)\r\n  plt.xlabel(data.index_to_label[label.numpy()])\r\nplt.show()\r\n\r\nmodel = image_classifier.create(train_data, validation_data=validation_data)\r\nmodel.summary()\r\nloss, accuracy = model.evaluate(test_data)\r\ndef get_label_color(val1, val2):\r\n  if val1 == val2:\r\n    return 'black'\r\n  else:\r\n    return 'red'\r\nplt.figure(figsize=(20, 20))\r\npredicts = model.predict_top_k(test_data)\r\nfor i, (image, label) in enumerate(test_data.gen_dataset().unbatch().take(100)):\r\n  ax = plt.subplot(10, 10, i+1)\r\n  plt.xticks([])\r\n  plt.yticks([])\r\n  plt.grid(False)\r\n  plt.imshow(image.numpy(), cmap=plt.cm.gray)\r\n\r\n  predict_label = predicts[i][0][0]\r\n  color = get_label_color(predict_label,\r\n                          test_data.index_to_label[label.numpy()])\r\n  ax.xaxis.label.set_color(color)\r\n  plt.xlabel('Predicted: %s' % predict_label)\r\nplt.show()\r\nmodel.export(export_dir='.')\r\nmodel.export(export_dir='./duy_tree', export_format=ExportFormat.LABEL)\r\n```\r\n\r\nSame code above but running on window, after export tflite model can be used.\r\nHelp me, please!", "comments": ["@Duynguyen0897 ,\r\n\r\nCan you please confirm the tensorflow version trying to execute the code.Thanks!", "@tilakrayal\r\n\r\nThe version I'm using is 2.3\r\nBut Same code and version above but running on window, after export tflite model can be used."]}, {"number": 49835, "title": "TFLite on GPU does not support quantised models by default", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/lite/performance/gpu_advanced\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe documentation reads: \"*Android APIs support quantized models by default. To disable, do the following:*\"\r\n\r\nHowever, when I try running my int8 quantised model on mobile GPU, it silently defaults to running on CPU. I'm importing the nightly version as below in build.gradle:\r\n```\r\ndependencies {\r\n    implementation \"androidx.appcompat:appcompat:1.1.0\"\r\n\r\n    implementation \"org.tensorflow:tensorflow-lite:0.0.0-nightly\"\r\n    implementation \"org.tensorflow:tensorflow-lite-support:0.0.0-nightly\"\r\n    implementation \"org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly\"\r\n}\r\n```\r\n\r\nAnd when I do the below in my Android activity:\r\n```\r\ngpuOptions.setQuantizedModelsAllowed(true);\r\n```\r\nit successfully runs on GPU (confirmed by looking at GPU usage, also it runs much faster on GPU). \r\n\r\nThis suggests that the Android APIs actually DON'T support quantised models on GPU by default, and the flag needs to be set manually.\r\n", "comments": ["> ## URL(s) with the issue:\r\n> https://www.tensorflow.org/lite/performance/gpu_advanced\r\n> \r\n> ## Description of issue (what needs changing):\r\n> The documentation reads: \"_Android APIs support quantized models by default. To disable, do the following:_\"\r\n> \r\n> However, when I try running my int8 quantised model on mobile GPU, it silently defaults to running on CPU. I'm importing the nightly version as below in build.gradle:\r\n> \r\n> ```\r\n> dependencies {\r\n>     implementation \"androidx.appcompat:appcompat:1.1.0\"\r\n> \r\n>     implementation \"org.tensorflow:tensorflow-lite:0.0.0-nightly\"\r\n>     implementation \"org.tensorflow:tensorflow-lite-support:0.0.0-nightly\"\r\n>     implementation \"org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly\"\r\n> }\r\n> ```\r\n> \r\n> And when I do the below in my Android activity:\r\n> \r\n> ```\r\n> gpuOptions.setQuantizedModelsAllowed(true);\r\n> ```\r\n> \r\n> it successfully runs on GPU (confirmed by looking at GPU usage, also it runs much faster on GPU).\r\n> \r\n> This suggests that the Android APIs actually DON'T support quantised models on GPU by default, and the flag needs to be set manually.\r\n\r\nAccording to [the gpu delegate Java implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/java/src/main/java/org/tensorflow/lite/gpu/GpuDelegate.java#L90), the support of quantized model is enabled by default in TfLite library.\r\n\r\nAs the tensorflow-lite-support lib is also used, it looks like you are using the tflite via the TfLite task library? If yes, could you detail how it's used? Anyway, to help us debugging, could you share a minimal code piece that we could use to reproduce the issue? Thx!", "It seems only Support library is used (to use Task library, it requires the dependency to Task library specifically). Yeah, the code snippet will help us understand what is going here.", "As a side note, running quantized model on GPU won't have performance benefit compared to running float16 model on GPU, as the weights are dequantized and the inference is being done in float16 (or float32). It also has extra quantize-dequantize between each layers, so the performance overall might even be slower than that of float16 model.", "Apologies for the late reply, I've been called to do other work.\r\n\r\nA couple of notes:\r\n- Indeed I was not using the Task Library. I loaded a custom model trained by myself.\r\n- I'm aware that running quantised models on GPU will just dequantise them. That's not a problem at the moment, we are not optimising for accuracy (yet) and using a single model for GPU and NPU is more convenient.\r\n\r\nBelow is a snippet from my main activity:\r\n```java\r\n  private class RunBenchmarkTask extends AsyncTask<Void, Integer, Void> {\r\n    private long totalTime = 0;\r\n    private long maxTime = 0;\r\n    private long minTime = 999999;\r\n    private int numRuns = 100;\r\n    private int numWarmupRuns = 5;\r\n\r\n    protected void onPreExecute() {\r\n      // Hide settings and button\r\n      View l = findViewById(R.id.settingsLayout);\r\n      l.setVisibility(View.GONE);\r\n\r\n      settingsTextView.setText(String.format(\r\n              \"Model: %s \\nDevice: %s \\nOutput res: %d x %d \\nWarmup runs: %d \\nTimed runs: %d\",\r\n              modelPath, device, getOutputSize(inputHeight), getOutputSize(inputWidth), numWarmupRuns, numRuns));\r\n      resultsTextView.setText(\"Benchmarking...\");\r\n    }\r\n\r\n    protected Void doInBackground(Void... params) {\r\n\r\n      // Load tflite model\r\n      try {\r\n        tfliteModel = FileUtil.loadMappedFile(MainActivity.this, modelPath);\r\n        Log.i(TAG, String.format(\"Model loaded from %s\", modelPath));\r\n      } catch (IOException e) {\r\n        Log.e(TAG, \"Error loading model\", e);\r\n        finish();\r\n      }\r\n\r\n      // Initialise tflite\r\n      tfliteOptions = new Interpreter.Options();\r\n      try {\r\n        switch (device) {\r\n          case CPU:\r\n            break;\r\n          case GPU:\r\n            GpuDelegate.Options gpuOptions = new GpuDelegate.Options();\r\n            // TFLite on GPU documentation says that the below flag is True by default\r\n            // but quantised models won't run on GPU by default\r\n            // TODO github issue\r\n            gpuOptions.setQuantizedModelsAllowed(true);\r\n            gpuDelegate = new GpuDelegate(gpuOptions);\r\n            tfliteOptions.addDelegate(gpuDelegate);\r\n            break;\r\n          case NPU:\r\n            nnApiDelegate = new NnApiDelegate();\r\n            tfliteOptions.addDelegate(nnApiDelegate);\r\n            break;\r\n        }\r\n        tflite = new Interpreter(tfliteModel, tfliteOptions);\r\n\r\n      } catch (Exception e) {\r\n        Log.e(TAG, \"Error initialising tflite interpreter\", e);\r\n        finish();\r\n      }\r\n\r\n      // Tell tflite to use this input size\r\n      int[] inputShape = {batchSize, inputHeight, inputWidth, 3};\r\n      DataType imageDataType = tflite.getInputTensor(0).dataType();\r\n      Log.i(TAG, String.format(\"Input datatype: %s\", imageDataType));\r\n      inputImageBuffer = TensorBuffer.createFixedSize(inputShape, imageDataType);\r\n      tflite.resizeInput(0, inputShape);\r\n\r\n      // Initialise output\r\n      int[] outputShape = {batchSize, getOutputSize(inputHeight), getOutputSize(inputWidth), 3};\r\n      Log.d(TAG, String.format(\"Input shape: %s\", Arrays.toString(inputShape)));\r\n      Log.d(TAG, String.format(\"Calculated output shape: %s\", Arrays.toString(outputShape)));\r\n      outputTensorBuffer = TensorBuffer.createFixedSize(outputShape, imageDataType);\r\n\r\n      // Do some warmup runs\r\n      for (int i=0; i < numWarmupRuns; i++) {\r\n\r\n        rerollInput(inputImageBuffer);\r\n        tflite.run(inputImageBuffer.getBuffer(), outputTensorBuffer.getBuffer().rewind());\r\n      }\r\n\r\n      // Do some timed runs\r\n      for (int i=0; i < numRuns; i++) {\r\n        // Regenerate input tensor with different random values\r\n        rerollInput(inputImageBuffer);\r\n\r\n        // Start counting\r\n        long startTime = SystemClock.uptimeMillis();\r\n\r\n        // Run inference on the decoder\r\n        tflite.run(inputImageBuffer.getBuffer(), outputTensorBuffer.getBuffer().rewind());\r\n\r\n        // Stop counting\r\n        lastProcessingTimeMs = (int) (SystemClock.uptimeMillis() - startTime);\r\n        Log.i(TAG, String.format(\"Decoder inference time = %d ms\", lastProcessingTimeMs));\r\n\r\n        totalTime += lastProcessingTimeMs;\r\n        maxTime = lastProcessingTimeMs > maxTime ? lastProcessingTimeMs : maxTime;\r\n        minTime = lastProcessingTimeMs < minTime ? lastProcessingTimeMs : minTime;\r\n        publishProgress(i, numRuns, lastProcessingTimeMs);\r\n      }\r\n\r\n      tflite.close();\r\n      return null;\r\n    }\r\n\r\n    protected void onProgressUpdate(Integer... progressValues) {\r\n      // progressValues contains current index and total number of iterations\r\n      resultsTextView.setText(String.format(\r\n              \"Benchmarking %d/%d. Time (ms): %d\",\r\n              progressValues[0] + 1, progressValues[1], progressValues[2]));\r\n    }\r\n\r\n    protected void onPostExecute(Void result) {\r\n      float avgTime = (float) totalTime / numRuns;\r\n      Log.i(TAG, String.format(\r\n              \"Inference time (ms): mean = %f, max = %d, min = %d\",\r\n              avgTime, maxTime, minTime));\r\n\r\n      // Show benchmark results on UI\r\n      resultsTextView.setText(String.format(\r\n              \"Inference time (ms): \\nmean = %.3f, max = %d, min = %d\",\r\n              avgTime, maxTime, minTime));\r\n\r\n      // Unhide settings\r\n      View l = findViewById(R.id.settingsLayout);\r\n      l.setVisibility(View.VISIBLE);\r\n\r\n      // Delete stuff\r\n      tflite = null;\r\n    }\r\n  }\r\n```\r\n\r\nPlease let me know if there's something I'm missing, or if you need more information.", "From the [source code](https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/lite/delegates/gpu/java/src/main/java/org/tensorflow/lite/gpu/GpuDelegate.java#L90), quantized models should be enabled as default. And @Cy-r0 is using the nightly build. \r\n\r\nLooped in @srjoglekar246 to take a look.\r\n", "We have [GpuDelegateTest](https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/lite/java/src/test/java/org/tensorflow/lite/gpu/GpuDelegateTest.java#L73) that unit-tests this behavior in our internal tooling, so I am surprised that you don't see the GPU delegate being enabled by default :-/. (Note that the test looks at the ops in the TFLite Interpreter to see if the whole graph has been delegated to GPU). I just re-ran these tests, and they pass.\r\n\r\nTo followup, what do you see in ADB logs from the TFLite runtime when you don't (vs do) set the flag to True explicitly? \r\nAlso, can you try logging the value of `options.quantizedModelsAllowed` through adb (before you explicitly set it to True) as well?"]}, {"number": 49833, "title": "sched_getaffinity fails on systems with more than 1024 cores", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.9\r\n\r\n**Describe the current behavior**\r\n\r\nOn doing `import tensorflow` a warning is shown 2 times:\r\n\r\n```\r\nsched_getaffinity: Invalid argument\r\ncan't determine number of CPU cores: assuming 4\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nTF works correctly on large systems\r\n\r\n**Other info / logs**\r\n\r\nThe reason is, that the default struct for the CPU set contains only space for 1024 cores, so it will fail with EINVAL on systems with more cores, even when they are inactive.\r\nThe solution is more or less simple, by using the macros of glibc to allocate a larger struct. See e.g. the CPython os Module implementation.", "comments": ["@Flamefire,\r\nCan you please elaborate about your issue so that everyone could understand what the issue exactly is? Thanks!", "Sure:\r\nThe call at https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/core/platform/default/port.cc#L75 only works for systems with at most 1024 cores\r\nThis is due to the usage of the default `cpu_set_t` at https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/core/platform/default/port.cc#L74 which has only enough space for 1024 cores.\r\n\r\nHence: When using TF on a system with more than 1024 cores the call to `sched_getaffinity` will fail and TF will fallback to using only 4 cores as indicated by the warning.\r\n\r\nThe solution is to use the macros `CPU_ALLOC_SIZE` and `CPU_ALLOC` provided by glibc to dynamically increase the size of the struct passed to `sched_getaffinity` until there is enough space for all CPUs. See e.g. https://github.com/python/cpython/blob/0fa282c55f1a45765340cb24ed65c90ffe2aa405/Modules/posixmodule.c#L7129", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue is not stale!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue is not stale!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue is not stale!", "@Flamefire,\r\nSorry for the inconvenience caused by the Stale Bot. It doesn't bother you anymore. Thanks! ", "@Flamefire \r\nplease confirm if this is still an issue in latest tf version.", "@Saduf2019 as the problematic code linked at https://github.com/tensorflow/tensorflow/issues/49833#issuecomment-850355351 is still current, yes the issue still persists"]}, {"number": 49740, "title": "precompile model executable binaries", "body": "Hello,\r\n\r\nI'd like to ask you if there is the possibility to have aka ML compiler and execution engine for ARM MCUs (Cortex-M). Is it somehow possible to use for this TF-micro? Is there another way to prepare executable ML code (model) for Cortex-M devices with TensorFlow? As of now, I can get with TF-Lite-Micro as close as possible to MCU ARM Cortex-M class devices and leverage CMSIS-NN optimized kernels for this.", "comments": []}, {"number": 49737, "title": "TimeseriesGenerator with multi-step outputs native support", "body": "**System information**\r\n- TensorFlow version: 2.5.0\r\n- Are you willing to contribute it: No (sorry)\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe feature is a new parameter in the TimeseriesGenerator API, which allows multi-step outputs. So, like the `length` parameter, perhaps a new `output_length` can refer to the length of the outputs (targets).\r\n\r\nI know that it is possible to prepare the target sequence to have multiple steps, but it is not a friendly process for everyone. In addition, there is the repository [time_series_generator](https://github.com/krypton-unite/time_series_generator) that implements this functionality separately from TF.\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes. Adding a new parameter in TimeseriesGenerator API.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone who handles time series and need multi-step outputs out of the box.", "comments": []}, {"number": 49733, "title": "Magic_wand example on Arduino Nano 33: 'class LSM9DS1Class' has no member named 'setContinuousMode' ", "body": "@tensorflow/micro\r\n\r\n**System information**\r\nWindows 10\r\nArduino 1.8.12 IDE\r\nArduino Nano 33 BLE Sense\r\n\r\n**Describe the problem**\r\nWhen trying Magic_wand example from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/magic_wand \r\nduring compilation, the IDE reports the following error:\r\n\r\n```\r\narduino_accelerometer_handler.cpp:50:7: error: 'class LSM9DS1Class' has no member named 'setContinuousMode'\r\n   IMU.setContinuousMode();\r\n```\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nI have followed the steps in the documentation to fix v1.0.0 of the driver. LSM9DS1.cpp looks as follows:\r\n\r\n```\r\nint LSM9DS1Class::begin()\r\n{\r\n  _wire->begin();\r\n\r\n  // reset\r\n  writeRegister(LSM9DS1_ADDRESS, LSM9DS1_CTRL_REG8, 0x05);\r\n  writeRegister(LSM9DS1_ADDRESS_M, LSM9DS1_CTRL_REG2_M, 0x0c);\r\n\r\n  delay(10);\r\n\r\n  if (readRegister(LSM9DS1_ADDRESS, LSM9DS1_WHO_AM_I) != 0x68) {\r\n    end();\r\n\r\n    return 0;\r\n  }\r\n\r\n  if (readRegister(LSM9DS1_ADDRESS_M, LSM9DS1_WHO_AM_I) != 0x3d) {\r\n    end();\r\n\r\n    return 0;\r\n  }\r\n\r\n  writeRegister(LSM9DS1_ADDRESS, LSM9DS1_CTRL_REG1_G, 0x78); // 119 Hz, 2000 dps, 16 Hz BW\r\n  writeRegister(LSM9DS1_ADDRESS, LSM9DS1_CTRL_REG6_XL, 0x70); // 119 Hz, 4G\r\n\r\n  writeRegister(LSM9DS1_ADDRESS_M, LSM9DS1_CTRL_REG1_M, 0xb4); // Temperature compensation enable, medium performance, 20 Hz\r\n  writeRegister(LSM9DS1_ADDRESS_M, LSM9DS1_CTRL_REG2_M, 0x00); // 4 Gauss\r\n  writeRegister(LSM9DS1_ADDRESS_M, LSM9DS1_CTRL_REG3_M, 0x00); // Continuous conversion mode\r\n\r\n  // Enable FIFO (see docs https://www.st.com/resource/en/datasheet/DM00103319.pdf)\r\n  writeRegister(LSM9DS1_ADDRESS, 0x23, 0x02);\r\n  // Set continuous mode\r\n  writeRegister(LSM9DS1_ADDRESS, 0x2E, 0xC0);\r\n  \r\n  return 1;\r\n}\r\n```\r\n\r\nand \r\n\r\n```\r\nint LSM9DS1Class::accelerationAvailable()\r\n{\r\n  //if (readRegister(LSM9DS1_ADDRESS, LSM9DS1_STATUS_REG) & 0x01) {\r\n  //  return 1;\r\n  //}\r\n  // Read FIFO_SRC. If any of the rightmost 8 bits have a value, there is data\r\n  if (readRegister(LSM9DS1_ADDRESS, 0x2F) & 63) {\r\n    return 1;\r\n  }\r\n  return 0;\r\n}\r\n```\r\n\r\nI also tried to use v1.1.0. In that case, no error during compilation and upload, but the Arduino Nano does not seem to run the code properly (no flashing LED, no serial port output). \r\n\r\nI have tried both approaches (v.1.0.0 modified, and v.1.1.0 unmodified) several times, including multiple resets and long waits of the Arduino Nano after upload. ", "comments": ["I'm running into the same issue."]}, {"number": 49727, "title": "Performance of using dicts in tf.data.Dataset.from_generator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 21.04, Colab\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.4.1 and 2.5.0\r\n- Python version: 3.7\r\n\r\nUsing dicts as input structure for a `tf.data.Dataset.from_generator` is significantly (>40%) slower than using tuples. Wrapping the nested dict structure in `tf.nest.flatten` and `tf.nest.pack_sequence_as` is only a bit (<10%) slower than using plain tuples.\r\n\r\nMaybe consider to \"speed-up\" the use of nested structures such as dicts, e.g. by internally flatten and \"unflatten\" the input.\r\n\r\nSee the attached colab code. I included some additional benchmark results on my machines.\r\n\r\nhttps://colab.research.google.com/drive/12pGU80HJXFSh7lqhdsWaTyeeHyWwovPe?usp=sharing\r\n", "comments": ["@ymodak \r\nI was able to reproduce the issue in TF [v2.4](https://colab.research.google.com/gist/tilakrayal/47dea112f28a763f31fc6499c93904fd/49727-2-4.ipynb),[v2.5](https://colab.research.google.com/gist/tilakrayal/e65c7349ce7e2d034523f8e10067b37d/49727-2-5.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/39a161598b1e813450d47525896c7a24/49727-nightly.ipynb).Please find the gist."]}, {"number": 49726, "title": "ParameterServerStrategy worker removing code change", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): No \r\n\r\n\r\nHi, I'm working on dynamic worker adding and removing for ParameterServerStrategy. I noticed that one change in Worker class is necessary to remove worker while training. \r\nThe change is here:\r\n`cluster_coordinator.py`   \r\n```python\r\nclass Worker:\r\n  [...]\r\n  def _process_queue(self):\r\n    \"\"\"Function running in a worker thread to process closure queues.\"\"\"\r\n    self._maybe_delay()\r\n    while self._should_worker_thread_run:\r\n      closure = self._cluster._closure_queue.get()  # pylint: disable=protected-access\r\n      if not self._should_worker_thread_run:\r\n        closure.mark_cancelled()\r\n        return\r\n      if closure is None:\r\n        return\r\n      self._process_closure(closure)\r\n      # To properly stop the worker and preemption threads, it is important that\r\n      # `ClusterCoordinator` object is not held onto so its `__del__` can be\r\n      # called. By removing the reference to the `closure` that has already been\r\n      # processed, we ensure that the `closure` object is released, while\r\n      # getting the next `closure` at above `self._cluster._closure_queue.get()`\r\n      # call.\r\n      del closure\r\n  [...]\r\n```\r\n\r\nCould someone commit it? I'm not familiar with tensorflow contribute requirements, so I think somebody else could do this faster. Thanks in advance.", "comments": ["Looks like you want to add the if block containing `closure.mark_cancelled()`?  Is this the only change? Do you want to send a PR? I am happy to change it as well.", "Hi! Yes, the if block is the only change. I'll be happy if you could do it for me. The contribution polices are too complex to merge such small change. Greetings!"]}, {"number": 49702, "title": "XLA: Cannot get IR on TPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab environment\r\n- TensorFlow installed from (source or binary): Google Colab environment\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nCode is in https://github.com/hjmus/mock/blob/master/TPUs_in_Colab.ipynb. I was trying to get HLO IR when running `tf.function` on TPU, by using `experimental_get_compiler_ir()`, but run into the following error:\r\n```\r\nValueError: No matching device found for '/job:worker/replica:0/task:0/device:TPU:1'\r\n```\r\n\r\n**Describe the expected behavior**\r\nBe able to get IR when running on TPU.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://github.com/hjmus/mock/blob/master/TPUs_in_Colab.ipynb\r\n\r\n**Other info / logs** \r\nThe purpose of this exercise is to check out the IR generated for GSPMD (https://arxiv.org/pdf/2105.04663.pdf).\r\n\r\n#comp:xla #XLA", "comments": ["I was able to reproduce the issue in tf v 2.5 and [nightly](https://colab.research.google.com/gist/tilakrayal/c6e71c51f7b338bb94d634389805d52c/untitled24.ipynb),in v2.4 i am facing different error.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/389dd6505324a21a37a4afacdfc7b3d4/untitled2-4.ipynb).", "@hjmus \r\nCan you please refer to [this comment](https://github.com/tensorflow/tensorflow/issues/36980#issuecomment-607585845) and let us know if it helps.", "@Saduf2019 \r\n\r\nHi Saduf2019, I was already using the suggested API:\r\n```\r\ntry:\r\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\r\n  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\r\nexcept ValueError:\r\n  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\r\n\r\ntf.config.experimental_connect_to_cluster(tpu)  # <<= here\r\ntf.tpu.experimental.initialize_tpu_system(tpu)\r\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n```"]}, {"number": 49603, "title": "Didn't find op for builtin opcode 'EXPAND_DIMS'", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  20H2 (19042.985)\r\n- TensorFlow installed from (source or binary): Binary\r\n- Tensorflow version (commit SHA if source): 2.4.0-alpha\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arduino Nano 33 BLE\r\n\r\n**Describe the problem**\r\nI made a model which uses the following Keras layers:\r\n* Conv1d\r\n* Maxpooling1d\r\n* Flatten\r\n* Dense\r\n\r\nAfter conversion and quantisation I get the following error when attempting to use the model:\r\n```\r\nDidn't find op for builtin opcode 'EXPAND_DIMS' version '1'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?\r\n\r\nFailed to get registration from op code EXPAND_DIMS\r\n\r\nFailed starting model allocation.\r\n\r\nAllocateTensors() failed \r\n```\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nThis is the code that led to the error [from the hello world example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world)\r\n```\r\n  // Map the model into a usable data structure. This doesn't involve any\r\n  // copying or parsing, it's a very lightweight operation.\r\n  model = tflite::GetModel(g_model);\r\n  if (model->version() != TFLITE_SCHEMA_VERSION) {\r\n    TF_LITE_REPORT_ERROR(error_reporter,\r\n                         \"Model provided is schema version %d not equal \"\r\n                         \"to supported version %d.\",\r\n                         model->version(), TFLITE_SCHEMA_VERSION);\r\n    return;\r\n  }\r\n  \r\n  // This pulls in all the operation implementations we need.\r\n  // NOLINTNEXTLINE(runtime-global-variables)\r\n  static tflite::AllOpsResolver resolver;\r\n\r\n  // Build an interpreter to run the model with.\r\n  static tflite::MicroInterpreter static_interpreter(\r\n      model, resolver, tensor_arena, kTensorArenaSize, error_reporter);\r\n  interpreter = &static_interpreter;\r\n\r\n  // Allocate memory from the tensor_arena for the model's tensors.\r\n  TfLiteStatus allocate_status = interpreter->AllocateTensors();\r\n  if (allocate_status != kTfLiteOk) {\r\n    TF_LITE_REPORT_ERROR(error_reporter, \"AllocateTensors() failed\");\r\n    return;\r\n  } \r\n```\r\n", "comments": []}, {"number": 49601, "title": "Model divergence in a TD3 implementation converted from pytorch", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos Big Sur and Ubuntu 18 (colab)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f 2.4.1 (colab) and v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0 (macos)\r\n- Python version: 3.7 (colab), 3.9.5 (macos)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nBased on this [implementation][1] of [TD3][2], I converted different torch-based methods to their tensorflow equivalents. No matter what hyperparameters I use, torch's version will start converging by 50,000 - 100,000 steps which happens within a few minutes however, tensorflow version doesn't show any signs of improvement up to 600,000 steps. Here's a [notebook][3] with both versions and I will include here the converted methods. I'm using gym's [BipedalWalker-v3][4] environment for both versions.\r\n\r\n**What I tried**\r\n\r\nI tried everything from lowering the default learning rate of the paper `0.0003`, hidden unit sizes (i tried 64, 128, 256, 512 and even 400, 300 the paper's defaults nothing works), changed batch size (100, 128, 256, ...), lowered buffer size (default: 1,000,000) to 100,000, 50,000 ... I also tried reusing `tf.GradientTape(persistent=True)` and doing the updates using the same tape, and needless to say, I even re-wrote the code multiple times. NOTHING works.\r\n\r\n**Note** \r\n\r\n`self.max_action` in torch's implementation, equals 1 and you will find it hardcoded in tf's, to clear any confusion you may have.\r\n\r\n**Common hyperparameters**\r\n\r\n - dense units: 256 (different from paper's 400 and 300)\r\n - actor update delay: 2\r\n - exploration noise std: 0.1\r\n - policy noise std: 0.2\r\n - noise clip norm: 0.5\r\n - tau: 0.005\r\n - optimizer: adam\r\n - learning rate: 0.0003\r\n - replay buffer size: 1,000,000\r\n - replay initial size: 25,000\r\n - batch size: 256 (different from paper's 100)\r\n\r\n**Torch actor and critic models**\r\n\r\n    import torch\r\n    import torch.nn as nn\r\n    import torch.nn.functional as F\r\n    \r\n    \r\n    class Actor(nn.Module):\r\n        def __init__(self, state_dim, action_dim, max_action):\r\n            super(Actor, self).__init__()\r\n            self.l1 = nn.Linear(state_dim, 256)\r\n            self.l2 = nn.Linear(256, 256)\r\n            self.l3 = nn.Linear(256, action_dim)\r\n            self.max_action = max_action\r\n    \r\n        def forward(self, state):\r\n            a = F.relu(self.l1(state))\r\n            a = F.relu(self.l2(a))\r\n            return self.max_action * torch.tanh(self.l3(a))\r\n    \r\n    \r\n    class Critic(nn.Module):\r\n        def __init__(self, state_dim, action_dim):\r\n            super(Critic, self).__init__()\r\n            self.l1 = nn.Linear(state_dim + action_dim, 256)\r\n            self.l2 = nn.Linear(256, 256)\r\n            self.l3 = nn.Linear(256, 1)\r\n            self.l4 = nn.Linear(state_dim + action_dim, 256)\r\n            self.l5 = nn.Linear(256, 256)\r\n            self.l6 = nn.Linear(256, 1)\r\n    \r\n        def forward(self, state, action):\r\n            sa = torch.cat([state, action], 1)\r\n            q1 = F.relu(self.l1(sa))\r\n            q1 = F.relu(self.l2(q1))\r\n            q1 = self.l3(q1)\r\n            q2 = F.relu(self.l4(sa))\r\n            q2 = F.relu(self.l5(q2))\r\n            q2 = self.l6(q2)\r\n            return q1, q2\r\n    \r\n        def Q1(self, state, action):\r\n            sa = torch.cat([state, action], 1)\r\n            q1 = F.relu(self.l1(sa))\r\n            q1 = F.relu(self.l2(q1))\r\n            q1 = self.l3(q1)\r\n            return q1\r\n\r\n**Tensorflow actor and critic models**\r\n\r\n    from tensorflow.keras import Model\r\n    from tensorflow.keras.layers import Dense, Input\r\n    \r\n    \r\n    \r\n    def actor(input_shape, output_units):\r\n        x0 = Input(input_shape)\r\n        x = Dense(256, 'relu')(x0)\r\n        x = Dense(256, 'relu')(x)\r\n        output = Dense(output_units, 'tanh')(x)\r\n        return Model(x0, output)\r\n    \r\n    \r\n    def critic(input_shape):\r\n        x0 = Input(input_shape)\r\n        x1 = Dense(256, 'relu')(x0)\r\n        x1 = Dense(256, 'relu')(x1)\r\n        v1 = Dense(1)(x1)\r\n        x2 = Dense(256, 'relu')(x0)\r\n        x2 = Dense(256, 'relu')(x2)\r\n        v2 = Dense(1)(x2)\r\n        return Model(x0, [v1, v2])\r\n\r\n**Torch train step**\r\n\r\n    def train(self, replay_buffer, batch_size=256):\r\n        self.total_it += 1\r\n        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\r\n        with torch.no_grad():\r\n            noise = (torch.randn_like(action) * self.policy_noise).clamp(\r\n                -self.noise_clip, self.noise_clip\r\n            )\r\n            next_action = (self.actor_target(next_state) + noise).clamp(\r\n                -self.max_action, self.max_action\r\n            )\r\n            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\r\n            target_Q = torch.min(target_Q1, target_Q2)\r\n            target_Q = reward + not_done * self.discount * target_Q\r\n        current_Q1, current_Q2 = self.critic(state, action)\r\n        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(\r\n            current_Q2, target_Q\r\n        )\r\n        self.critic_optimizer.zero_grad()\r\n        critic_loss.backward()\r\n        self.critic_optimizer.step()\r\n        if self.total_it % self.policy_freq == 0:\r\n            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\r\n            self.actor_optimizer.zero_grad()\r\n            actor_loss.backward()\r\n            self.actor_optimizer.step()\r\n            for param, target_param in zip(\r\n                self.critic.parameters(), self.critic_target.parameters()\r\n            ):\r\n                target_param.data.copy_(\r\n                    self.tau * param.data + (1 - self.tau) * target_param.data\r\n                )\r\n            for param, target_param in zip(\r\n                self.actor.parameters(), self.actor_target.parameters()\r\n            ):\r\n                target_param.data.copy_(\r\n                    self.tau * param.data + (1 - self.tau) * target_param.data\r\n                )\r\n\r\n**tensorflow training methods**\r\n\r\n    def update_critic_weights(self, states, actions, rewards, dones, new_states):\r\n        policy_noise = tf.random.normal(\r\n            (self.n_envs * self.buffer_batch_size, self.n_actions),\r\n            0,\r\n            self.policy_noise_std,\r\n        )\r\n        policy_noise = tf.clip_by_value(policy_noise, -self.noise_clip, self.noise_clip)\r\n        target_actions = tf.clip_by_value(\r\n            self.target_actor(new_states) + policy_noise, -1, 1\r\n        )\r\n        target_value1, target_value2 = self.target_critic(\r\n            tf.concat([new_states, target_actions], 1)\r\n        )\r\n        target_value = rewards + (1 - dones) * self.gamma * tf.minimum(\r\n            target_value1, target_value2\r\n        )\r\n        with tf.GradientTape() as tape:\r\n            value1, value2 = self.critic(tf.concat([states, actions], 1))\r\n            value_loss = MSE(value1, target_value) + MSE(value2, target_value2)\r\n        self.critic.optimizer.minimize(\r\n            value_loss, self.critic.trainable_variables, tape=tape\r\n        )\r\n\r\n    def update_actor_weights(self, states):\r\n        with tf.GradientTape() as tape:\r\n            actions = self.actor(states)\r\n            loss = -tf.reduce_mean(self.critic([tf.concat([states, actions], 1)])[0])\r\n        self.actor.optimizer.minimize(loss, self.actor.trainable_variables, tape=tape)\r\n\r\n    @tf.function\r\n    def train_step(self):\r\n        self.updates.assign_add(1)\r\n        step_actions = self.actor(self.get_states())\r\n        exploration_noise = tf.random.normal(\r\n            [self.n_actions], 0, self.exploration_noise_std\r\n        )\r\n        step_actions = tf.clip_by_value(step_actions + exploration_noise, -1, 1)\r\n        tf.numpy_function(self.step_envs, [step_actions, False, True], [])\r\n        batch = tf.numpy_function(self.concat_buffer_samples, [], 5 * [tf.float32])\r\n        self.update_critic_weights(*batch)\r\n        if tf.math.equal(tf.math.mod(self.updates, self.actor_delay), 0):\r\n            self.update_actor_weights(batch[0])\r\n            self.sync_target_models()\r\n\r\n    def sync_target_models(self):\r\n        for model, target_model in self.model_groups:\r\n            for var, target_var in zip(\r\n                model.trainable_variables, target_model.trainable_variables\r\n            ):\r\n                target_var.assign(self.tau * var + (1 - self.tau) * target_var)\r\n\r\nThe results which you can reproduce using the [notebook][3]:\r\n\r\n**Torch**\r\n\r\n    time: 0:00:01.928306, steps: 3875, games: 10, fps: 2009.0, mean reward: -105.27, best reward: -82.24\r\n    time: 0:00:06.234093, steps: 13832, games: 20, fps: 2312.0, mean reward: -99.73, best reward: -77.94\r\n    time: 0:00:09.219966, steps: 20691, games: 30, fps: 2297.0, mean reward: -99.37, best reward: -75.26\r\n    time: 0:00:24.854520, steps: 27736, games: 40, fps: 450.0, mean reward: -102.14, best reward: -65.01\r\n    time: 0:00:28.400598, steps: 28444, games: 50, fps: 199.0, mean reward: -103.27, best reward: -65.01\r\n    time: 0:01:24.811760, steps: 39761, games: 60, fps: 200.0, mean reward: -107.92, best reward: -65.01\r\n    time: 0:02:23.950100, steps: 51719, games: 70, fps: 202.0, mean reward: -105.17, best reward: -65.01\r\n    time: 0:03:29.150313, steps: 64853, games: 80, fps: 201.0, mean reward: -103.09, best reward: -65.01\r\n    time: 0:04:27.118459, steps: 76666, games: 90, fps: 203.0, mean reward: -105.1, best reward: -65.01\r\n    time: 0:05:33.525320, steps: 90105, games: 100, fps: 202.0, mean reward: -104.47, best reward: -31.01\r\n    time: 0:06:38.586421, steps: 103162, games: 110, fps: 200.0, mean reward: -102.21, best reward: -31.01\r\n    time: 0:07:50.672645, steps: 117707, games: 120, fps: 201.0, mean reward: -99.69, best reward: -31.01\r\n    time: 0:09:10.076208, steps: 133707, games: 130, fps: 201.0, mean reward: -93.53, best reward: -31.01\r\n    time: 0:10:07.278960, steps: 145180, games: 140, fps: 200.0, mean reward: -88.81, best reward: -30.43\r\n    time: 0:11:04.842319, steps: 156767, games: 150, fps: 201.0, mean reward: -85.12, best reward: -30.43\r\n    time: 0:11:30.127616, steps: 161788, games: 160, fps: 198.0, mean reward: -83.37, best reward: -30.43\r\n    time: 0:11:34.950709, steps: 162755, games: 170, fps: 200.0, mean reward: -86.65, best reward: -30.43\r\n    time: 0:12:06.615783, steps: 169146, games: 180, fps: 201.0, mean reward: -89.97, best reward: -30.43\r\n    time: 0:13:11.028382, steps: 182064, games: 190, fps: 200.0, mean reward: -84.38, best reward: -30.43\r\n    time: 0:14:30.357974, steps: 198064, games: 200, fps: 201.0, mean reward: -79.24, best reward: -30.43\r\n    time: 0:15:49.531968, steps: 214064, games: 210, fps: 202.0, mean reward: -74.81, best reward: -27.15\r\n    time: 0:17:01.859939, steps: 228590, games: 220, fps: 200.0, mean reward: -71.18, best reward: 37.19\r\n\r\n**Tensorflow**\r\n\r\n    time: 0:00:33.882932, steps: 6825, games: 10, fps: 206, mean reward: -81.9, best reward: -68.04\r\n    time: 0:01:30.612159, steps: 18306, games: 20, fps: 201, mean reward: -77.51, best reward: -68.04\r\n    time: 0:01:49.359029, steps: 22075, games: 30, fps: 190, mean reward: -88.67, best reward: -68.04\r\n    time: 0:01:53.394437, steps: 22881, games: 40, fps: 190, mean reward: -94.84, best reward: -68.04\r\n    time: 0:01:56.996669, steps: 23603, games: 50, fps: 205, mean reward: -98.51, best reward: -68.04\r\n    time: 0:02:16.071768, steps: 27402, games: 60, fps: 192, mean reward: -97.91, best reward: -68.04\r\n    time: 0:02:28.969803, steps: 29996, games: 70, fps: 204, mean reward: -100.71, best reward: -68.04\r\n    time: 0:02:32.286386, steps: 30656, games: 80, fps: 203, mean reward: -102.14, best reward: -68.04\r\n    time: 0:02:41.094522, steps: 32407, games: 90, fps: 198, mean reward: -103.45, best reward: -68.04\r\n    time: 0:02:57.489013, steps: 35634, games: 100, fps: 171, mean reward: -104.67, best reward: -68.04\r\n    time: 0:03:15.559559, steps: 39184, games: 110, fps: 199, mean reward: -109.12, best reward: -68.04\r\n    time: 0:03:26.770144, steps: 41374, games: 120, fps: 194, mean reward: -113.48, best reward: -68.04\r\n    time: 0:03:56.878539, steps: 47205, games: 130, fps: 195, mean reward: -114.48, best reward: -68.04\r\n    time: 0:04:37.541307, steps: 55078, games: 140, fps: 194, mean reward: -116.36, best reward: -68.04\r\n    time: 0:05:14.164399, steps: 62089, games: 150, fps: 192, mean reward: -115.31, best reward: -68.04\r\n    time: 0:06:14.816104, steps: 73559, games: 160, fps: 197, mean reward: -118.41, best reward: -68.04\r\n    time: 0:06:51.555758, steps: 80434, games: 170, fps: 185, mean reward: -118.5, best reward: -68.04\r\n    time: 0:07:11.885412, steps: 84218, games: 180, fps: 188, mean reward: -118.14, best reward: -68.04\r\n    time: 0:07:51.662705, steps: 91584, games: 190, fps: 184, mean reward: -119.91, best reward: -68.04\r\n    time: 0:08:12.492280, steps: 95425, games: 200, fps: 181, mean reward: -119.18, best reward: -68.04\r\n    time: 0:08:33.173622, steps: 99274, games: 210, fps: 186, mean reward: -117.43, best reward: -68.04\r\n    time: 0:08:40.747114, steps: 100642, games: 220, fps: 189, mean reward: -116.43, best reward: -68.04\r\n    time: 0:09:09.865704, steps: 105968, games: 230, fps: 185, mean reward: -116.34, best reward: -68.04\r\n\r\nAnd it will keep diverging forever guaranteed, I almost reached 1,000,000 steps and the reward keeps circling around the same average.\r\n\r\n  [1]: https://github.com/sfujim/TD3\r\n  [2]: https://arxiv.org/pdf/1802.09477.pdf\r\n  [3]: https://colab.research.google.com/drive/1gG434fkNOQKXU3-TNOUBA04-44BE7H0Q?usp=sharing\r\n  [4]: https://elegantrl.readthedocs.io/en/latest/examples/BipedalWalker-v3.html\r\n\r\n**Describe the expected behavior**\r\n\r\nGiven that it's almost a copy and paste job from pytorch to tensorflow, the model is expected to work given the same hyperparameters and conditions and it doesn't, so this is a bug, it's not an implementation specific issue unless there is something I don't know that I do and I shouldn't\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you want to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing): I don't know which component that has a bug but there definitely is a bug somewhere, if you point where I may propose a solution / contribute.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nI included a notebook in the description above.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["How i can contribute\r\nplease guide me\r\nI like to solve this problem"]}, {"number": 49520, "title": "Remove duplicated Env::Default to fix gcs issue on Windows", "body": "This PR tries to address the issue raised in #49515 where gcs is not available through tf.io.gfile.GFile interface.\r\n\r\nThe reason was that on Windows -config=monolithic was used in bazel which includes multiple copies of `Env::Default` in different pyd files.\r\n\r\nThis PR removes all duplications to make gcs working on Windows.\r\n\r\nThis PR fixes #49515\r\n\r\n/cc @mihaimaruseac @vnvo2409 @kvignesh1420 @terrytangyuan @burgerkingeater FYI\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @mihaimaruseac for the review. I have updated the PR to change the test to smoke test in:\r\n`tensorflow/tools/ci_build/builds/nightly_release_smoke_test.sh`\r\n\r\nPlease take a look and let me know if this works.", "@yongtang  Can you please resolve conflicts? Thanks!", "@mihaimaruseac @gbaned Thanks for the help. The PR has been rebased to resolve the merge conflict. Can you try re-import again?", "This has a large number of internal targets that fail to link due to `Env::Default` no longer being available at link time.\r\n\r\nNeeds me to go through all these BUILD targets and update", "Can we find a solution that does not require external parties to depend on `//tensorflow/core/platform:env_impl`?\r\n\r\nIt seems even downstream project that want to use `Env::Default()` now need to depend on this internal target.\r\n\r\nThere are around 1000 targets that need to change internally and this likely won't pass, sadly.", "@mihaimaruseac Thanks for the update. I will take a look and see if there are any ways to avoid the downstream projects change.", "@yongtang  Any update on this PR? Please. Thanks!", "Thanks @gbaned for the reminder. Still working on it as last two weeks has been mostly filled with PTOs and some other activities. I will try to get the PR updated really soon.", "Manual import seems needed", "Any update on this? I need this PR to be merged ", "Will probably need to wait for `cc_shared_library` support given that it breaks internal builds in the current form", "Thanks @mihaimaruseac . I think cc_shared_library will also resolve #53234 (grpc upgrade issue) as well."]}, {"number": 49515, "title": "File system is not working on Windows through tf.io.gfile.GFile interface", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): 2.5.0\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nIt looks like on Windows, File system (e.g., gcs) is only working through `tf.io.read_file()` but not working through `tf.io.gfile.GFile().read()`.\r\n\r\nOn Windows, check the following command:\r\n```bash\r\npython3 -c \"import tensorflow as tf;print(tf.version.VERSION);tf.io.read_file('gs://1234567')\" \r\n```\r\nthrows out error of:\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: GCS path doesn't contain an object name: gs://1234567 [Op:ReadFile]\r\n```\r\n\r\nThis indicates GCS file system at least is available.\r\n\r\nOn the other hand, the following command:\r\n```bash\r\npython3 -c \"import tensorflow as tf;print(tf.version.VERSION);tf.io.gfile.GFile('gs://1234567').read()\" \r\n```\r\nthrows out error of:\r\n```\r\ntensorflow.python.framework.errors_impl.UnimplementedError: File system scheme 'gs' not implemented (file: 'gs://1234567')\r\n```\r\n\r\nwhich indicate GCS file system is not even registered.\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing): yes\r\n\r\n**Standalone code to reproduce the issue**\r\nSee description above\r\n\r\nThe issue is due to the duplication of `Env::Default` in multiple places. The issue is exposed on Windows due to `config=monolithic` being passed to bazel. I think I have identified the places that cause the issue. Will submit a PR soon.\r\n\r\n/cc @mihaimaruseac @vnvo2409 @kvignesh1420 @terrytangyuan @burgerkingeater ", "comments": ["The issue will move to closed status once the PR is merged.", "@yongtang \r\nIs this still an issue, could you please try on the latest tf version and let us know.", "@Saduf2019 This is still an issue as #49520 has not been merged yet. (the PR has been approved but has not been imported internally yet). See https://github.com/tensorflow/tensorflow/pull/49520#issuecomment-862739157", "@yongtang\r\nIs this still an issue", "@Saduf2019  PR #49520 has been approved, though it has not been imported and merged yet. Once it is merged this issue can be resolved."]}, {"number": 49514, "title": "image dataset generation functions for image segmentation data", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nImage (semantic) segmentation tasks consist of input image data and target image masks which classify each pixel of the input image. Example: https://www.tensorflow.org/tutorials/images/segmentation\r\n\r\nRight now the Keras image data preprocessing API has convenient functions `image_dataset_from_directory` and `ImageDataGenerator.flow_from_directory` for creating TF Datasets for image data, but these are only suitable for image classification tasks (each image has a single classification).\r\n\r\nUsing these APIs for image segmentation requires some hacky fiddling (I'm currently using this solution from [SO](https://stackoverflow.com/questions/58050113/imagedatagenerator-for-semantic-segmentation)) e.g. creating two ImageDataGenerators with the same seed and `class_mode=None`, and some directory manipulation.\r\n\r\nI would suggest adding something like `class_mode='segmentation'` along with a standard directory format for such tasks.\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt would add `class_mode='segmentation'` to these functions. For ImageDataGenerator, it requires defining whether augmentation is applied to the input image, target mask, and/or both.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone working on image segmentation tasks\r\n\r\n**Any other info**\r\nThis could also affect `flow_from_dataframe()`.", "comments": ["Thanks for opening this issue. I think we should offer a better API for image segmentation use cases (and also related use cases like object detection, keypoint detection, etc).\r\n\r\nThere are 2 subproblems:\r\n\r\n1. Generating a dataset\r\n2. Image data augmentation (same augmentation needs to be applied to inputs and to segmentation masks or target bounding boxes)\r\n\r\nFor 1. I don't think adding new built-in options in `image_dataset_from_directory` is the best option, because the range of possible formats for segmentation masks is large.\r\n\r\nI'd recommend using the utility in a modular way, like so:\r\n\r\n```python\r\nimg_ds = image_dataset_from_directory(flat_img_dir, labels=None, shuffle=None)\r\nsegmentation_mask_ds = image_dataset_from_directory(flat_mask_dir, labels=None, shuffle=None, color_mode='grayscale')  # Assumes the filenames are in the same order\r\nds = Dataset.zip((img_ds, segmentation_mask_ds))\r\n```\r\n\r\nDoes that make sense? You can also add shuffling as long as you provide the same seed in both cases.\r\n\r\nFor data augmentation, we're working on something based on image preprocessing layers."]}, {"number": 49508, "title": "Exception happened in API function: Calling `Model.predict` in graph mode is not supported when the `Model` instance was constructed with eager mode enabled.", "body": "**System information**\r\n- Distribution: Deployed using bentoml\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.4\r\n- Python version: 3.6\r\n\r\nDuring `model.predict()`, the error below  does not happen during single run but happens occasionally during stress test. I used locust.io for stress test at 50 requests per second with around 90% failed requests\r\n```\r\n\"Exception happened in API function: Calling `Model.predict` in graph mode is not supported when the `Model` instance was constructed with eager mode enabled. Please construct your `Model` instance in graph mode or call `Model.predict` with eager mode enabled.\".\r\n```\r\nMy model was deployed in AWS SageMaker endpoint using bentoml for serving.\r\n\r\n\r\n", "comments": ["@davidricafort ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.\r\n\r\nAlso please take a look at this [link](https://stackoverflow.com/questions/64326568/calling-model-predict-in-graph-mode-is-not-supported-when-the-model-instance) with similar error.It helps.\r\n\r\nThanks!", "hi @tilakrayal \r\n\r\nplease see the code snippet below. thank you!\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers.experimental import preprocessing\r\n\u200b\r\n\u200b\r\n# TRAINING DATA\r\ndataframe = pd.DataFrame([[\"43AxPO98MN6ftywe\", \"biPA1S85Edsgl94z\", 1, 30],[\"haGKJlgk834ye1ur\", \"FJ1HcbdPsu4fHFB3\", 4, 100]], columns=[\"user\", \"item\", \"type\", \"label\"])\r\nlabels = dataframe.pop(\"label\")\r\ntrain_ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels)).shuffle(buffer_size=len(dataframe)).batch(1024).cache()\r\n\u200b\r\n\u200b\r\nunique_item_ids = dataframe[\"item\"].unique()\r\nunique_user_ids = dataframe[\"user\"].unique()\r\nembed_size = 8\r\n\u200b\r\n\u200b\r\n# WIDE MODEL\r\nuser_inputs = tf.keras.layers.Input(shape=(1,), name=\"user\", dtype=\"string\")\r\nuser_string =  preprocessing.StringLookup(vocabulary=unique_user_ids, mask_token=None, name=\"user_vocab\")(user_inputs)\r\nuser_embed = tf.keras.layers.Embedding(len(unique_user_ids)+1, embed_size, name=\"user_embed\")(user_string)\r\nuser_embed = tf.keras.layers.Flatten(name=\"user_flatten\")(user_embed)\r\n\u200b\r\nitem_inputs = tf.keras.layers.Input(shape=(1,), name=\"item\", dtype=\"string\")\r\nitem_string =  preprocessing.StringLookup(vocabulary=unique_item_ids, mask_token=None, name=\"item_vocab\")(item_inputs)\r\nitem_embed = tf.keras.layers.Embedding(len(unique_item_ids)+1, embed_size, name=\"item_embed\")(item_string)\r\nitem_embed = tf.keras.layers.Flatten(name=\"item_flatten\")(item_embed)\r\n\u200b\r\nmerged_layer = tf.keras.layers.concatenate([user_embed, item_embed], name=\"cross_product_transformation\")\r\nmerged_layer = tf.keras.layers.Dense(64, activation='relu', name=\"wide_dense_64\")(merged_layer)\r\nmerged_layer = tf.keras.layers.Dense(32, activation='relu', name=\"wide_dense_32\")(merged_layer)\r\npredictions = tf.keras.layers.Dense(1, name=\"wide_output\")(merged_layer)\r\nwide_model = tf.keras.Model(inputs=[user_inputs, item_inputs], outputs=predictions, name=\"wide_model\")\r\n\u200b\r\nwide_model.compile(loss='mse', optimizer='adam', metrics=['binary_crossentropy'])\r\n\u200b\r\n# DEEP MODEL\r\ndeep_inputs = tf.keras.layers.Input(shape=(1,), name=\"type\")\r\n\u200b\r\ndense_128 = tf.keras.layers.Dense(128, name=\"deep_dense_128\")(deep_inputs)\r\ndense_64 = tf.keras.layers.Dense(64, name=\"deep_dense_64\")(dense_128)\r\ndense_32 = tf.keras.layers.Dense(32, name=\"deep_dense_32\")(dense_64)\r\ndropout = tf.keras.layers.Dropout(.35, name=\"deep_dropout\")(dense_32)\r\ndeep_predictions = tf.keras.layers.Dense(1, name=\"deep_output\")(dropout)\r\ndeep_model = tf.keras.Model(inputs=deep_inputs, outputs=deep_predictions, name=\"deep_model\")\r\ndeep_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\r\n\u200b\r\n# CONCAT INTO WIDE AND DEEP\r\nmerged_concat = tf.keras.layers.concatenate([wide_model.output, deep_model.output], name=\"wide_and_deep_concat\")\r\nmerged_out = tf.keras.layers.Dense(1, name=\"wide_and_deep_output\")(merged_concat)\r\ncombined_model = tf.keras.Model(wide_model.input + [deep_model.input], merged_out, name=\"wide_and_deep_model\")\r\n\u200b\r\ncombined_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\r\n\u200b\r\n\u200b\r\n\u200b\r\n# TRAIN THE MODEL\r\ncombined_model.fit(train_ds, epochs=10, batch_size=4096)\r\n\u200b\r\n\u200b\r\n# SAMPLE INPUT\r\nsample = {\r\n    \"user\": np.array([\"adgslva12yvafd\", \"adgslva12yvafd\", \"adgslva12yvafd\"]), \r\n    \"item\": np.array([\"iDAjgfdus9303igh\", \"biPA1S85Edsgl94z\", \"FJ1HcbdPsu4fHFB3\"]), \r\n    \"type\": np.array([6, 4, 1]),\r\n    }\r\n\u200b\r\n# SAMPLE PREDICTIONS\r\npredictions = combined_model.predict(sample)\r\n\u200b\r\n# SAVE THE MODEL\r\nmodel_filepath = \"model_v0\"\r\ncombined_model.save(model_filepath, overwrite=True)\r\n```", "@tilakrayal I tried the link that you sent but there is still an error. here is the error message\r\n```\r\n\\\"Exception happened in API function: Tensor Tensor(\\\"wide_and_deep_output/BiasAdd:0\\\", shape=(None, 1), dtype=float32) is not an element of this graph.\r\n```\r\n\r\nhere is the code snippet:\r\n```\r\ndef __init__(self):\r\n    super().__init__()\r\n    self.filepath = 'model_v0'\r\n    self.my_bucket = 'bucket-name'\r\n    self.download_folder(self.my_bucket, self.filepath)\r\n\u200b\r\n    self.graph = tf.Graph()\r\n    with self.graph.as_default():\r\n        self.model = tf.keras.models.load_model(self.filepath)\r\n        self.model.compile(loss='mse', optimizer=\"adam\", metrics=['mse'])\r\n\u200b\r\n@api(input=JsonInput(), batch=True)\r\ndef predict(self, input_data):\r\n    sample = {k: np.array(v) for k,v in input_data[0].items()}\r\n    \r\n    with self.graph.as_default():\r\n        predictions = self.model.predict(sample)\r\n    \r\n    test_df = pd.DataFrame(predictions)\r\n    test_df[\"item_ids\"] = sample['item']\r\n    test_df.sort_values(0, ascending=False, inplace=True)\r\n    top_3k_predictions = test_df[\"item_ids\"].head(3000).to_list()\r\n        \r\n    return [top_3k_predictions]\r\n```\r\n\r\nI had to load the model upon the instantiation of the class and separate the predict function so that only the predict function will be called when my api is called. Because of this, I had to use the `with self.graph.as_default():` twice", "@davidricafort ,\r\n\r\nI was able to run the code without any issues on TF v2.4. Please find the [gist](https://colab.research.google.com/gist/tilakrayal/ac5dd6009eb1af552ca750db50c6f9d4/49508.ipynb) here.\r\n\r\nCould you please create a virtual environment and test your code again. It helps. Thanks!", "hi @tilakrayal ,\r\n\r\nI'm actually able to run the code and serve my model online but the bug shows when I perform load testing. I served my model using [bentoml](https://github.com/bentoml/BentoML) and my model endpoint is hosted in Amazon SageMaker Endpoint. I perform load testing using [locust](https://locust.io/). On a single invocation of the endpoint, the model will work just fine. But when multiple simultaneous requests are made, the bug starts to appear", "@tilakrayal @Saduf2019 \r\n\r\nI edited the gist notebook you created and added the code for serving the model to bentoml. Here's the link to the [colab notebook](https://colab.research.google.com/drive/12EYpoxE3VlrsHbAU-duOtJYO7eEP2c-8?usp=sharing)\r\n\r\nHosting the served model with ngrok has a limit of 20 requests per minute, thus the issue cannot be replicated but I hope you can try it on your local machine"]}, {"number": 49507, "title": "No enable autoparallel optimizer doc in grappler", "body": "\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/config/optimizer/set_experimental_options#args\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe [document of grappler ](https://www.tensorflow.org/guide/graph_optimization#available_graph_optimizers) says it has an `autoparallel optimizer`, and it supposed to be enabled using `tf.config.optimizer.set_experimental_options`.  While in [`tf.config.optimizer.set_experimental_options`](https://www.tensorflow.org/api_docs/python/tf/config/optimizer/set_experimental_options#args), it does not have the corresponding option (auto_parallel).\r\n\r\n\r\n", "comments": ["@Stonepia \r\n\r\nThank you for the request, we are working on this and will update you at the earliest."]}, {"number": 49505, "title": "AddBuiltin() mismatch ", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n\r\n- OS Platform and Distribution: Linux Ubuntu 20.10\r\n-  TensorFlow version: 2.4.1\r\n-   Python version : 3.8\r\n\r\n**Describe the problem**\r\n\r\nI have this compilation problem:\r\n\r\n`../Core/Src/main.cpp: In function 'int main()':`\r\n`../Core/Src/main.cpp:181:57: error: no matching function for call to 'tflite::MicroMutableOpResolver<1>::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration*)'`\r\n\r\n`tflite::ops::micro::Register_FULLY_CONNECTED());`\r\n                      \r\n`workspace_1.3.0/sine/tensorflow_lite/tensorflow/lite/micro/micro_mutable_op_resolver.h:470:16: note: candidate expects 3 arguments, 2 provided`      \r\n\r\nIn particular I have problems with this snippet of code of main.cpp:\r\n\r\n`static tflite::MicroMutableOpResolver<1> micro_op_resolver;`     \r\n`tflite_status = micro_op_resolver.AddBuiltin(`     \r\n`tflite::BuiltinOperator_FULLY_CONNECTED,`  \r\n`tflite::ops::micro::Register_FULLY_CONNECTED());`                                                                                               \r\n\r\n\r\n\r\n\r\n", "comments": ["I'm reporting the link of the tutorial: https://www.digikey.com/en/maker/projects/tinyml-getting-started-with-tensorflow-lite-for-microcontrollers/c0cdd850f5004b098d263400aa294023", "@Lucy20211,\r\nThe link you provided is very lengthy. Can you please provide a `standalone code` which isolates the problem. Also, please fill all the details of [the Template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=comp%3Amicro&template=70-tflite-micro-issue.md) so that we will have the required information to help you. Thanks!", "Hi, sorry. \r\n\r\n**Describe the problem**\r\n\r\nI'd like to run a simple model (taken from the tutorial above). The code is defined like this:\r\n\r\n                                                                      \r\n\r\n      #include \"main.h\"\r\n      #include <string.h>\r\n      #include \"tensorflow/lite/micro/kernels/micro_ops.h\"\r\n      #include \"tensorflow/lite/micro/micro_error_reporter.h\"\r\n      #include \"tensorflow/lite/micro/micro_interpreter.h\"\r\n      #include \"tensorflow/lite/micro/micro_mutable_op_resolver.h\"\r\n      #include \"tensorflow/lite/version.h\"\r\n      #include \"sine_model.h\"\r\n        \r\n      /* Private variables ---------------------------------------------------------*/\r\n      TIM_HandleTypeDef htim16;\r\n      \r\n      UART_HandleTypeDef huart2;\r\n      \r\n      /* USER CODE BEGIN PV */\r\n      \r\n      // TFLite globals\r\n      namespace {\r\n        tflite::ErrorReporter* error_reporter = nullptr;\r\n        const tflite::Model* model = nullptr;\r\n        tflite::MicroInterpreter* interpreter = nullptr;\r\n        TfLiteTensor* model_input = nullptr;\r\n        TfLiteTensor* model_output = nullptr;\r\n      \r\n        // Create an area of memory to use for input, output, and other TensorFlow\r\n        // arrays. You'll need to adjust this by compiling, running, and looking\r\n        // for errors.\r\n        constexpr int kTensorArenaSize = 2 * 1024;\r\n        __attribute__((aligned(16)))uint8_t tensor_arena[kTensorArenaSize];\r\n      } // namespace\r\n      \r\n      /* USER CODE END PV */\r\n      \r\n      /* Private function prototypes -----------------------------------------------*/\r\n      void SystemClock_Config(void);\r\n      static void MX_GPIO_Init(void);\r\n      static void MX_USART2_UART_Init(void);\r\n      static void MX_TIM16_Init(void);\r\n      /* USER CODE BEGIN PFP */\r\n      \r\n      /* USER CODE END PFP */\r\n      \r\n      /* Private user code ---------------------------------------------------------*/\r\n      /* USER CODE BEGIN 0 */\r\n      \r\n      /* USER CODE END 0 */\r\n      \r\n      /**\r\n        * @brief  The application entry point.\r\n        * @retval int\r\n        */\r\n      int main(void)\r\n      {\r\n        /* USER CODE BEGIN 1 */\r\n        char buf[50];\r\n        int buf_len = 0;\r\n        TfLiteStatus tflite_status;\r\n        uint32_t num_elements;\r\n        uint32_t timestamp;\r\n        float y_val;\r\n      \r\n        /* USER CODE END 1 */\r\n      \r\n        /* MCU Configuration--------------------------------------------------------*/\r\n      \r\n        /* Reset of all peripherals, Initializes the Flash interface and the Systick. */\r\n        HAL_Init();\r\n      \r\n        /* USER CODE BEGIN Init */\r\n      \r\n        /* USER CODE END Init */\r\n      \r\n        /* Configure the system clock */\r\n        SystemClock_Config();\r\n      \r\n        /* USER CODE BEGIN SysInit */\r\n      \r\n        /* USER CODE END SysInit */\r\n      \r\n        /* Initialize all configured peripherals */\r\n        MX_GPIO_Init();\r\n        MX_USART2_UART_Init();\r\n        MX_TIM16_Init();\r\n        /* USER CODE BEGIN 2 */\r\n      \r\n        // Start timer/counter\r\n        HAL_TIM_Base_Start(&htim16);\r\n      \r\n        // Set up logging (modify tensorflow/lite/micro/debug_log.cc)\r\n        static tflite::MicroErrorReporter micro_error_reporter;\r\n        error_reporter = &micro_error_reporter;\r\n      \r\n        // Say something to test error reporter\r\n        error_reporter->Report(\"STM32 TensorFlow Lite test\");\r\n      \r\n        // Map the model into a usable data structure\r\n        model = tflite::GetModel(sine_model);\r\n        if (model->version() != TFLITE_SCHEMA_VERSION)\r\n        {\r\n          error_reporter->Report(\"Model version does not match Schema\");\r\n          while(1);\r\n        }\r\n      \r\n        // Pull in only needed operations (should match NN layers). Template parameter\r\n        // <n> is number of ops to be added. Available ops:\r\n        // tensorflow/lite/micro/kernels/micro_ops.h\r\n        static tflite::MicroMutableOpResolver<1> micro_op_resolver;\r\n      \r\n        // Add dense neural network layer operation\r\n        tflite_status = micro_op_resolver.AddBuiltin(\r\n            tflite::BuiltinOperator_FULLY_CONNECTED,\r\n            tflite::ops::micro::Register_FULLY_CONNECTED());\r\n        if (tflite_status != kTfLiteOk)\r\n        {\r\n          error_reporter->Report(\"Could not add FULLY CONNECTED op\");\r\n          while(1);\r\n        }\r\n      \r\n        // Build an interpreter to run the model with.\r\n        static tflite::MicroInterpreter static_interpreter(\r\n            model, micro_op_resolver, tensor_arena, kTensorArenaSize, error_reporter);\r\n        interpreter = &static_interpreter;\r\n      \r\n        // Allocate memory from the tensor_arena for the model's tensors.\r\n        tflite_status = interpreter->AllocateTensors();\r\n        if (tflite_status != kTfLiteOk)\r\n        {\r\n          error_reporter->Report(\"AllocateTensors() failed\");\r\n          while(1);\r\n        }\r\n      \r\n        // Assign model input and output buffers (tensors) to pointers\r\n        model_input = interpreter->input(0);\r\n        model_output = interpreter->output(0);\r\n      \r\n        // Get number of elements in input tensor\r\n        num_elements = model_input->bytes / sizeof(float);\r\n        buf_len = sprintf(buf, \"Number of input elements: %lu\\r\\n\", num_elements);\r\n        HAL_UART_Transmit(&huart2, (uint8_t *)buf, buf_len, 100);\r\n      \r\n        /* USER CODE END 2 */\r\n      \r\n        /* Infinite loop */\r\n        /* USER CODE BEGIN WHILE */\r\n        while (1)\r\n        {\r\n          // Fill input buffer (use test value)\r\n          for (uint32_t i = 0; i < num_elements; i++)\r\n          {\r\n            model_input->data.f[i] = 2.0f;\r\n          }\r\n      \r\n          // Get current timestamp\r\n          timestamp = htim16.Instance->CNT;\r\n      \r\n          // Run inference\r\n          tflite_status = interpreter->Invoke();\r\n          if (tflite_status != kTfLiteOk)\r\n          {\r\n            error_reporter->Report(\"Invoke failed\");\r\n          }\r\n      \r\n          // Read output (predicted y) of neural network\r\n          y_val = model_output->data.f[0];\r\n      \r\n          // Print output of neural network along with inference time (microseconds)\r\n          buf_len = sprintf(buf,\r\n                            \"Output: %f | Duration: %lu\\r\\n\",\r\n                            y_val,\r\n                            htim16.Instance->CNT - timestamp);\r\n          HAL_UART_Transmit(&huart2, (uint8_t *)buf, buf_len, 100);\r\n      \r\n          // Wait before doing it again\r\n          HAL_Delay(500);\r\n      \r\n          /* USER CODE END WHILE */\r\n      \r\n          /* USER CODE BEGIN 3 */\r\n        }\r\n        /* USER CODE END 3 */\r\n      }\r\n      \r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**   \r\n\r\nWhen I run the program I got this compilation problem:\r\n\r\n        ../Core/Src/main.cpp: In function 'int main()':\r\n        ../Core/Src/main.cpp:181:57: error: no matching function for call to 'tflite::MicroMutableOpResolver<1>::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration*)'\r\n               tflite::ops::micro::Register_FULLY_CONNECTED());\r\n                                                             ^\r\n        \r\n        In file included from ../Core/Src/main.cpp:31:0:\r\n        STM32CubeIDE/workspace_1.3.0/sine/tensorflow_lite/tensorflow/lite/micro/micro_mutable_op_resolver.h:470:16: note: candidate: TfLiteStatus tflite::MicroMutableOpResolver<tOpCount>::AddBuiltin(tflite::BuiltinOperator, const TfLiteRegistration&, tflite::MicroOpResolver::BuiltinParseFunction) [with unsigned int tOpCount = 1; TfLiteStatus = TfLiteStatus; TfLiteRegistration = TfLiteRegistration; tflite::MicroOpResolver::BuiltinParseFunction = TfLiteStatus (*)(const tflite::Operator*, tflite::BuiltinOperator, tflite::ErrorReporter*, tflite::BuiltinDataAllocator*, void**)]\r\n           TfLiteStatus AddBuiltin(tflite::BuiltinOperator op,\r\n                        ^~~~~~~~~~\r\n        STM32CubeIDE/workspace_1.3.0/sine/tensorflow_lite/tensorflow/lite/micro/micro_mutable_op_resolver.h:470:16: note:   candidate expects 3 arguments, 2 provided\r\n        make: *** [Core/Src/subdir.mk:37: Core/Src/main.o] Error 1\r\n        \"make -j2 all\" terminated with exit code 2. Build might be incomplete.\r\n        \r\n\r\n\r\n\r\n\r\n"]}]