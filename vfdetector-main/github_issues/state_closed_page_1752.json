[{"number": 334, "title": "Error: Executing cifar10_train.py on Ubuntu with Cuda", "body": "I just went through the [cifar10 tensorflow tutorial](http://www.tensorflow.org/tutorials/deep_cnn/index.md) on my mac. This tutorial aroused my enthusiasm for NN so I set a Ubuntu Server up in order to train in large scale.\n\nHowever I cant run the cifar10_train.py on my server. I always run into this error:\n\nPath of the tensorflow source code:\n\n```\n~/python/tensorflow/tensorflow/tensorflow/\n```\n\nPath of the tensorflow virtualenv installation:\n\n```\n~/tensorflow/\n```\n\n**Cmd:**\n\n```\nsource ~/tensorflow/bin/activate #activate virtualenv\npython/tensorflow/tensorflow/tensorflow/models/image/cifar10/cifar10_train.py #the raw source code of tensorflow is in ~/python/tensorflow/tensorflow/tensorflow\n```\n\n**Error:**\n\n```\nTraceback (most recent call last):\n  File \"python/tensorflow/tensorflow/tensorflow/models/image/cifar10/cifar10_train.py\", line 28, in <module>\n    import tensorflow.python.platform\n  File \"/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 4, in <module>\n    from tensorflow.python import *\n  File \"/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 22, in <module>\n    from tensorflow.python.client.client_lib import *\n  File \"/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/client_lib.py\", line 35, in <module>\n    from tensorflow.python.client.session import InteractiveSession\n  File \"/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 11, in <module>\n    from tensorflow.python import pywrap_tensorflow as tf_session\n  File \"/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n```\n\nI installed tensorflow just like on my mac in a virtualenv and activated it correctly before executing the script. \nAs by many in other threads suggested I upgraded already six, however I still got the same error.\n\n**Update 1**\nAfter going the issue threads on github/tensorflow through I noticed this is a bug involving cuda. I added these to my path environment:\n\n```\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64\"\nexport CUDA_HOME=/usr/local/cuda\n```\n\nI still run into a error, however it has shortened down to the following:\n\n```\nTraceback (most recent call last):\nFile \"cifar10_train.py\", line 28, in <module>\nimport tensorflow.python.platform\nImportError: No module named tensorflow.python.platform\n```\n\n**Update 2** As someone suggested, I install protobuf via pip. The Error has once again change for some weird reason:\n\n```\nTraceback (most recent call last):\n  File \"cifar10_train.py\", line 28, in <module>\n    import tensorflow.python.platform\n  File \"/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 4, in <module>\n    from tensorflow.python import *\n  File \"/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 13, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n  File \"/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n  File \"/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n  File \"/home/it13095/tensorflow/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 22, in <module>\n    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"d\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tb\\x06proto3')\nTypeError: __init__() got an unexpected keyword argument 'syntax'\n```\n", "comments": ["Are you using python 2? On newer ubuntu releases  (14.04.3 for me), pip defaults to pip3, using python 3\n\n```\npip --version\npip 7.1.2 from /usr/local/lib/python3.4/dist-packages (python 3.4)\n```\n\nSo to correctly install the tensorflow wheel, you'd need to specifically call pip for python 2 with `pip2`\n", "I checked already on that but it seems to be the right package:\n\n```\npip --version\npip 1.5.4 from /usr/lib/python2.7/dist-packages (python 2.7)\n```\n", "What's the protobuf version you're using? I had the exact same issue when I started, see issue https://github.com/tensorflow/tensorflow/issues/4. Maybe this also works for you?\n", "De-duping with https://github.com/tensorflow/tensorflow/issues/4 -- also we have this as a common problem listed on our install website: http://www.tensorflow.org/get_started/os_setup.html#common_install_problems\n"]}, {"number": 333, "title": "Issue with _create_slots: var_list[0].device empty", "body": "I have problem running \n\n```\nloss = crossentropy(activation, y)\ntrain_loss = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n```\n\nwith crossentropy defined in another file as a simple function that return \n\n```\ndef crossentropy(activation, y)\n    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(activation, y))\n```\n\nI got this error:\n\n```\nTraceback (most recent call last):\n  File \"test/test.py\", line 66, in <module>\n    train_loss = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss) # Adam Optimizer\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 167, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 245, in apply_gradients\n    self._create_slots([v for g, v in grads_and_vars if g is not None])\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/adam.py\", line 87, in _create_slots\n    with ops.device(var_list[0].device):\nIndexError: list index out of range\n```\n\nBut this is working: (only call function from another file is not)\n\n```\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(activation, y))\ntrain_loss  = self.optimizer(learning_rate=0.01).minimize(loss)\n```\n\nIt seemed I can't define that expression outside the main thread, I also tried using same graph as default graph, but it doesn't seem to work either. I think this is an issue with graph building.\nIf you ever ran into this problem, thanks for helping!\n", "comments": ["The error indicates that the optimizer could not find any gradients for the variables to optimize.\n\nThis can be caused by several problems:\n 1- The optimizer could not find any variable to optimize.\n 2- The loss does not depend on any variable.\n\nFor 1-: Optimizers look  for trainable variables in the default graph. They use `tf.trainable_variables()` as the list of variables to optimize.  So try calling that function and print its output before calling `.minimize()`.  If you get an empty list then the optimizer won't find variables either.\n\nWhy can't it find variables?\n-  The variables are looked for in the _default graph,_ which is a thread-local  variable.  You mention \"outside the main thread\" which hints that your program is using multiple python threads.  If this is the case you need to pass the  default graph from the main thread to the other thread and install it as the default graph.  See `tf.get_default_graph()` and `Graph.as_default()` in http://tensorflow.org/api_docs/python/framework.md#Graph .\n-  Maybe you have not created any variables?  Or you only create variables with  `trainable=False`?\n\nFor 2-: The optimizer computes gradients of the loss with respect to variables.   If there are  no variables there are no gradients and nothing to optimize.  For example, if you pass a constant to `minimize()` nothing can be done.  Now, the fact that you say it works when called from the same file seems to rule out that possibility, but check anyway.\n\nFinally: The error message clearly needs to be improved.\n", "Closing due to staleness, I think we committed a change a few weeks ago that fixed the error message to be better.  Let us know if that's not true!\n"]}, {"number": 332, "title": "Link error using c api: undefined reference to symbol 'powf@@GLIBC_2.2.5'", "body": "http://theyoungcoder.com/loading-a-tensorflow-graph-with-the-c-api/\nI followed this acticle. \n\n1   cc_binary(\n2   name = \"loader\",\n3   srcs = [\"loader.cc\"],\n4   deps = [\n5   \"//tensorflow/core:tensorflow\",\n6   ]\n7   )\nHere\u2019s the final directory structure:\n\u2022 tensorflow/tensorflow/loader/\n\u2022 tensorflow/tensorflow/loader/loader.cc\n\u2022 tensorflow/tensorflow/loader/BUILD\nCompile & Run\n\u2022 From inside the project folder call\u00a0bazel build\u00a0:loader.\n\u2022 From the repository root, go into\u00a0bazel-bin/tensorflow/loader.\n\u2022 Copy the graph protobuf to\u00a0models/train.pb.\n\nThen run\u00a0./loader\u00a0and check the output!\n\nBut when building I got link error\nSlow read: a 551318010-byte read from /home/users/chenghuige/.cache/bazel/_bazel_chenghuige/56e262ed5e70da1d7ac42e55562c6970/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/core/libkernels.lo took 6143ms.\nINFO: From Linking tensorflow/loader/loader:\n/usr/bin/ld: bazel-out/local_linux-fastbuild/bin/tensorflow/core/libkernels.lo(cwise_op_pow.pic.o): undefined reference to symbol 'powf@@GLIBC_2.2.5'\n/usr/bin/ld: note: 'powf@@GLIBC_2.2.5' is defined in DSO /top/lib/libm.so.6 so try adding it to the linker command line\n/top/lib/libm.so.6: could not read symbols: Invalid operation\ncollect2: error: ld returned 1 exit status\nERROR: /home/users/chenghuige/other/tensorflow/tensorflow/loader/BUILD:1:1: Linking of rule '//tensorflow/loader:loader' failed: gcc failed: error executing command /usr/bin/gcc -o bazel-out/local_linux-fastbuild/bin/tensorflow/loader/loader bazel-out/local_linux-fastbuild/bin/tensorflow/loader/_objs/loader/tensorflow/loader/loader.pic.o -Wl,-whole-archive ... (remaining 45 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/loader:loader failed to build\nUse --verbose_failures to see the command lines of failed build steps.\n", "comments": ["It sounds like you're using an older version of glibc than we support (2.2.5, rather than 2.17). You could try adding `\"-lm\"` to the returned value from `tf_copts()` in `tensorflow.bzl` (see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L40)), but there might be other problems that arise afterwards.\n\nI'd recommend trying to update the version of libc on your machine, or using a Docker container to experiment with a different operating system in a known-good configuration.\n", "@mrry \n ll -t libm.so.6\nlrwxrwxrwx 1 root root 12 Jul  3 17:57 libm.so.6 -> libm-2.18.so\n\nseems libm.so.6 is of version 2.18 ?\n\nAdding -lm will not solve this problem.\nAlso I can use bazel build tensorflow project successfully  but I can not build this example\n", "@mrry: Should we close as won't fix due to the version issue?\n", "I think so. It sounds like a Bazel issue, not picking up the appropriate libraries from a non-standard location.\n"]}, {"number": 331, "title": "1:1 between internal and external commits", "body": "Currently, it seems that the internal commits to TensorFlow are generally exported to the GitHub repository in batches.\n\nWe have some infrastructure set up for bazelbuild/bazel for both importing external patches into the internal repository and pushing internal commits to the GitHub repository, scrubbing any sensitive information from commit messages. Perhaps we can sync offline to see if we can use the tools for TensorFlow and then rewrite the Git history, especially given #294.\n\nRelated: #26\n", "comments": ["Thanks -- we're working on this internally and we're pretty close to something that works for us. De-duping with https://github.com/tensorflow/tensorflow/issues/26\n", "Ah got it. Good to hear!\n", "FYI, we're now writing individual commits for each of our internal changes -- it's working reasonably well so far.\n"]}, {"number": 330, "title": "Link to next tutorial at the bottom of the first tutorial leads to the home page", "body": "On: http://www.tensorflow.org/tutorials/mnist/beginners/index.md\nThe last paragraph:\nWhat matters is that we learned from this model. Still, if you're feeling a bit down about these results, check out the next tutorial where we do a lot better, and learn how to build more sophisticated models using TensorFlow!\n\n'The next tutorial' links to http://www.tensorflow.org/tutorials/index.md which ends up showing me the home page.\n", "comments": ["Not any more.\n"]}, {"number": 329, "title": "Is it possible to plot two scalar summary in one plot?", "body": "Hi, \n\nI have spent some time writing scripts to do stat in Caffe and this let me know Tensorboard is so amazing! It is actually one of the major reasons that makes me switch to tensorflow quickly. \n\nI just wonder could an option be added to plot more than two scalars in the same plot? More specifically. it could allow:\n1. training loss and test loss being plotted together\n2. regularization loss and top level loss to be plotted together.\n3. maybe adding accuracy on the right vertical axis\n\nJust name a few. This would help a lot.\n", "comments": ["Cc @danmane.\n", "Yeah, we definitely want to make comparing different charts easier. No timeline for when we'll have this, though.\n", "Yes, very necessary, now I have to use matplotlib to make such plots.\n", "As a temporary workaround, you can make the plot in matplotlib, and then inject it into a TensorFlow image_summary so that it will display an updated copy in TensorBoard.\n", "....why not I directly go and see the plot made in matplotlib?\n\nMore problems: It is extremely hard to make testing plots in tensorboard. Training plots are OK. However, if I want to make the plots for testing, there will be problems\n\ne.g. During training, I add_summary each 10 steps for 1000 steps. I have 1000 testing values (float numbers) and now I want to plot the difference between my predictions and the 1000 testing values. How do I make the plot in tensorboard? I tried \"1 st testing sample difference->step 1 difference\", \"2nd testing sample diff-> step 2 diff\". However, this way seems not working. The tensorboard only displays few of the testing results, but not all 1000 samples. I do not know what is the problem behind. Are you using a global_step for all of the summaries?\n", "A workaround, you can write your summaries to different directories, and load them as different runs, tensorboard will draw scalars in the same chart as long as they share the same name.\n", "@danmane\nCould you explain how to inject a matplotlib plot into an image_summary. I have tried out a million things but I don't manage to.\nSo far I've got an image as a numpy array, but then how do I add it? Here are my last thoughts:\n\n```\nfor ... training loop ...:\n    sess.run(train_op)\n    img = create_image(...)\n    sum = tf.image_summary(\"foo\",\n                           np.reshape(img, [1, img.shape[0], img.shape[1], img.shape[2]]))\n    summary_writer.add_summary(sum)\n    summary_writer.flush()\n```\n", "@b78 Sorry, I don't have a code example on hand. This would be a good question for StackOverflow, rather than issues.\n\nI'm not prioritizing getting two summaries into one plot, and it's probably too complicated for a pull request, so I'm going to close this issue. Maybe once TensorBoard has plugin support it will make sense to revisit this issue.\n", "@while2  can you explain how to share the same name between two scalars. And can you give me an example?", "@while2  Thank you for the workaround.\r\n@DucVuMinh \r\n\r\n```\r\n    train_graph_location = '/tmp/tensorflow_logs/\r\n    train_writer = tf.summary.FileWriter(graph_location) #Save tensorboard variables\r\n    test_graph_location = tempfile.mkdtemp()\r\n    test_writer = tf.summary.FileWriter(location)\r\n\r\n```\r\nIf you print both the locations, you will get\r\n```\r\n/tmp/tensorflow_logs/train    #location of train_writer\r\n/tmp/tmp82i59t                    #test_writer\r\n```\r\nWhile loading tensorboard,\r\n`tensorboard --logdir=/tmp/`\r\n\r\nYou shoud be able to see two graphs(different colors ) plotted on the same plot.", "@lekhamohan  Thank you for your support. I got it.", "I am creating multiple graph in tensorboard, and would like to have some of them printing multiple scalars ( for comparison purpose). For instance, suppose I have scalar s1,s2,...,s10, and would like s1,s2,s3 to be plotted in one graph, and s4,..., s10 to be plotted in another graph. Is there a way of doing this? Thanks in advance."]}, {"number": 328, "title": "initializing seq2seq embedding with pretrained w2v", "body": "Hi, is there a straightforward way to initialize seq2seq embedding with pretrained w2v?\n\nthe embedding variable is in the embedding_tied_seq2seq (the model I am using). is there anyway to access this variable from translate.py and call .assign() to assign it after random initialisation with all other variables?? is there a global variable name I could call for this embedding?\n", "comments": ["Check out the answer on the google groups here: https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/bH6S98NpIJE\n\nThe short answer given by Rafal is:\n\n```\nYou can initialize it randomly and afterwards do:\nsession.run(embedding.assign(my_word2vec_matrix))\n\nYou need to do separate assigns for the encoder and decoder.\n```\n", "Closing.  For future reference, this kind of question is better on stackoverfow.\n"]}, {"number": 327, "title": "How to find <Python.h> ? compile with bazel will fail not find Python.h", "body": "INFO: Found 1 target...\nINFO: From Compiling tensorflow/python/client/tf_session_helper.cc:\nIn file included from tensorflow/python/client/tf_session_helper.cc:16:0:\n./tensorflow/python/client/tf_session_helper.h:19:20: fatal error: Python.h: No such file or directory\n #include <Python.h>\n\nAcutally I have Python.h in   /SomePath/python2.7/\nBut how to tell bazel ? I tried\nexport CPLUS_INCLUDE_PATH= /SomePath/python2.7/\nStill not work, any suggestions ?\n", "comments": ["See #187.  I think for now you can modify this line:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/BUILD#L702\n\nto point to your python headers folder.  Please confirm this works for you.  We're working on a fix.\n", "Well seems not work for me. Looks like due to I install bazel as root user ? \nBut  ./bazel-0.1.1-installer-linux-x86_64.sh seems only can be installed using root user.\n\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nERROR: /home/users/chenghuige/other/tensorflow/tensorflow/tensorflow.bzl:225:3: in cc_library rule //tensorflow/python:tf_session_helper: The include path '/home/users/chenghuige/.jumbo/include/python2.7' references a path outside of the execution root..\nERROR: /home/users/chenghuige/other/tensorflow/tensorflow/tensorflow.bzl:225:3: in cc_library rule //tensorflow/python:tf_session_helper: The include path '/home/users/chenghuige/.jumbo/include/python2.7' references a path outside of the execution root..\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\n", "Anyway, I just copied python2.7 to /usr/inlude path. \nAnd then will face PyArray_SHAPE not find problem, I took the work around as described in #109 \nNow thie step is ok..\nBut after that\n\n bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\nMon Nov 23 14:16:04 CST 2015 : === Using tmpdir: /tmp/tmp.MWtBU7R88K\n/tmp/tmp.MWtBU7R88K /home/users/chenghuige/other/tensorflow\nMon Nov 23 14:16:04 CST 2015 : === Building wheel\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: setup.py --help [cmd1 cmd2 ...]\n   or: setup.py --help-commands\n   or: setup.py cmd --help\n\nerror: invalid command 'bdist_wheel'\n\nHow to solve this?\n", "Well, after  \"pip install wheel\" everything is fine.\n", "Another thing, import tesnsorflow, seems no tensorflow.**version** exists.\n", "@chenghuige we're working on providing something like tensorflow.version or tensorflow.**version**.\n\nLooks like your other problem has been resolved, so I'm closing this issue.\n", "I noticed that this issue is closed and the problem should be fixed, however, I encountered the same poblem today when trying to build tensorflow serving, and  the solution is set environment variables before running configure. The environment variable is:\r\nPYTHON_INCLUDE_PATH\r\nwhich sholud be set to where Python.h resides.\r\n", "In my case on CentOS 7.2 (bazel-dist 0.5.3 and tensorflow v1.3.0-rc2), also met this issue, thanks for the sharing above, it did save time. \r\nBTW, after the related python libraries installed and PYTHON_INCLUDE_PATH exported, the Bazel needs to be recompiled to include the changes.\r\nFollowing packages have been installed via yum before compiling bazel and tensorflow: java-1.8.0-openjdk-devel, patch, python-devel, python-psycopg2 and numpy.x86_64. ", "I also had this issue and resolved it by uninstalling bazel (which was installed system-wide) and reinstalling as --user.", "Same here, even after setting `PYTHON_INCLUDE_PATH` and re-compile bazel. Built with tensorflow at master (1521eeb) branch with bazel 0.14.0 and bazel 0.10.1.", "Solved it as follows: Do not fill in a soft link when `./configure` asks where your python is. Answer the real python location.", "@Michael-Jing \r\nI encountered the same issue when compiling the tensorflow-serving 1.7.\r\n```\r\nERROR: /home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/tensorflow/python/BUILD:4855:1: C++ compilation of rule '@org_tensorflow//tensorflow/python:framework/fast_tensor_util.so' failed (Exit 1)\r\nbazel-out/k8-opt/genfiles/external/org_tensorflow/tensorflow/python/framework/fast_tensor_util.cpp:4:20: fatal error: Python.h: No such file or directory\r\n #include \"Python.h\"\r\n                    ^\r\ncompilation terminated.\r\nINFO: Elapsed time: 17.580s, Critical Path: 12.44s\r\nFAILED: Build did NOT complete successfully\r\n```\r\nEven after setting the path to Python.h, the error still exists.\r\n```\r\n$ echo $PYTHON_INCLUDE_PATH\r\n/home/web_server/dlpy72/py2.7/include/python2.7\r\n$ ls $PYTHON_INCLUDE_PATH | grep Python\r\nPython-ast.h\r\nPython.h\r\n```\r\n---------------------\r\nSolved by setting the real path instead of soft link.\r\n", "> Solved it as follows: Do not fill in a soft link when ./configure asks where your python is. Answer the real python location.\r\n\r\nThank you. Changing the /usr/bin/python3 to /usr/bin/python3.4m of configure path resolve my problem as I installed python3.4 in the centos7.", "> > Solved it as follows: Do not fill in a soft link when ./configure asks where your python is. Answer the real python location.\r\n> \r\n> Thank you. Changing the /usr/bin/python3 to /usr/bin/python3.4m of configure path resolve my problem as I installed python3.4 in the centos7.\r\n\r\nThis does solve my problem in centos7. Thanks a lot."]}, {"number": 325, "title": "Installation exception in EI Capitan", "body": "Hi,\nI am getting this errors after runing \n\"pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\"\nI am using OSX:10.11.1. Is it a permission problem? should I run sudo?\nThanks in advance!\n\nException:\nTraceback (most recent call last):\n  File \"/Library/Python/2.7/site-packages/pip-1.5.4-py2.7.egg/pip/basecommand.py\", line 122, in main\n    status = self.run(options, args)\n  File \"/Library/Python/2.7/site-packages/pip-1.5.4-py2.7.egg/pip/commands/install.py\", line 283, in run\n    requirement_set.install(install_options, global_options, root=options.root_path)\n  File \"/Library/Python/2.7/site-packages/pip-1.5.4-py2.7.egg/pip/req.py\", line 1435, in install\n    requirement.install(install_options, global_options, _args, *_kwargs)\n  File \"/Library/Python/2.7/site-packages/pip-1.5.4-py2.7.egg/pip/req.py\", line 671, in install\n    self.move_wheel_files(self.source_dir, root=root)\n  File \"/Library/Python/2.7/site-packages/pip-1.5.4-py2.7.egg/pip/req.py\", line 901, in move_wheel_files\n    pycompile=self.pycompile,\n  File \"/Library/Python/2.7/site-packages/pip-1.5.4-py2.7.egg/pip/wheel.py\", line 207, in move_wheel_files\n    clobber(source, lib_dir, True)\n  File \"/Library/Python/2.7/site-packages/pip-1.5.4-py2.7.egg/pip/wheel.py\", line 194, in clobber\n    os.makedirs(destsubdir)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py\", line 157, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/external'\n\nStoring debug log for failure in /var/folders/ky/4cxqzftd7q1844xgkvvb_4dc0000gn/T/tmpcQCpeO\n", "comments": ["It worked for me with the super user permission.\n\nTry sudo.\n", "Tried sudo, a different error:\n\n> Exception:\n> \n> Traceback (most recent call last):\n>   File \"/Library/Python/2.7/site-packages/pip-1.5.4-py2.7.egg/pip/basecommand.py\", line 122, in main\n>     status = self.run(options, args)\n>   File \"/Library/Python/2.7/site-packages/pip-1.5.4-py2.7.egg/pip/commands/install.py\", line 283, in run\n>     requirement_set.install(install_options, global_options, root=options.root_path)\n>   File \"/Library/Python/2.7/site-packages/pip-1.5.4-py2.7.egg/pip/req.py\", line 1431, in install\n>     requirement.uninstall(auto_confirm=True)\n>   File \"/Library/Python/2.7/site-packages/pip-1.5.4-py2.7.egg/pip/req.py\", line 598, in uninstall\n>     paths_to_remove.remove(auto_confirm)\n>   File \"/Library/Python/2.7/site-packages/pip-1.5.4-py2.7.egg/pip/req.py\", line 1836, in remove\n>     renames(path, new_path)\n>   File \"/Library/Python/2.7/site-packages/pip-1.5.4-py2.7.egg/pip/util.py\", line 295, in renames\n>     shutil.move(old, new)\n>   File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 302, in move\n>     copy2(src, real_dst)\n>   File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 131, in copy2\n>     copystat(src, dst)\n>   File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 103, in copystat\n>     os.chflags(dst, st.st_flags)\n> OSError: [Errno 1] Operation not permitted: '/tmp/pip-mLMP2F-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy-1.8.0rc1-py2.7.egg-info'\n> \n> Storing debug log for failure in /Users/Lin/Library/Logs/pip.log\n", "Operation not permitted, is usually a cause due to ownership issue. Try changing it and use the super user again.\n", "If you continue having trouble with sudo, consider using a virtualenv following instructions in the howto [here](http://tensorflow.org/get_started/os_setup.md#virtualenv-based_installation).\n", "I had a similar issue. I believe it's because you are trying to update the system python version. This has issues with a feature Apple introduced called \" System Integrity Protection\". Instead, you should install a user version of Python and use that. \nTry something like this:\nbrew install python\nbrew doctor\n(Read the errors, and fix issues related to python like linking issues) (I had to do 'brew link --overwrite python')\npip install --upgrade numpy\nThen try installing TensorFlow.\n", "I faced the same problem and I found that I can still import tensorflow and have no problem to run some simple code related to tensorflow.\nwill I face other problem later ?\n", "@ad26kt: if you do, let us know.\n"]}, {"number": 324, "title": "TensorBoard Documentation - Confusing and Incorrect in places. (Need End to End Example)", "body": "It would be nice to have a complete, end-to-end working example using TensorBoard.  In addition, some of the documentation appears to be incorrect.\n\nPossible Incorrect Documentation:\nhttp://tensorflow.org/how_tos/summaries_and_tensorboard/index.md\n\n<img width=\"731\" alt=\"screenshot 2015-11-22 10 42 18\" src=\"https://cloud.githubusercontent.com/assets/755710/11324647/bc5560d8-9105-11e5-8293-ddd36cf7c946.png\">\n\nA concise, completely coded example may help. Note, more detail is shown here, but it's still have hard to grasp the full picture.  A complete coded example may help.\n\n``` python\n\n...\nsess.run(init)\n\n# Build the summary operation based on the TF collection of Summaries.\ntf.train.write_graph(sess.graph_def, './tenIrisSave/logsd','graph.pbtxt')\n\ntf.scalar_summary(\"Accuracy:\", tf_accuracy)\ntf.histogram_summary('weights', tf_weight)\ntf.histogram_summary('bias', tf_bias)\ntf.histogram_summary('softmax', tf_softmax)\ntf.histogram_summary('accuracy', tf_accuracy)\n\n\nsummary_op = tf.merge_all_summaries()\nsummary_writer = tf.train.SummaryWriter('./tenIrisSave/logs',sess.graph_def)\n\n# This will not work. You need the full path.\n# tensorboard --logdir=./tenIrisSave/   # BAD!\n# tensorboard --logdir=$(pwd)/tenIrisSave/  # Good!\n...\nfor i in range(100):\n   ...\n    summary_str = sess.run(summary_op,feed_dict={tf_in: x_test, tf_softmax_correct: y_test_onehot})\n    summary_writer.add_summary(summary_str, i)\n\n\n```\n\nThe complete example can be seen here:\nhttps://gist.github.com/mchirico/bcc376fb336b73f24b29\n\nAgreed. The example needs to be cleaned up. The point is to give the user a complete working example that can be run and verified.\n", "comments": ["You are right. sess.graph is a typo there, which should be corrected with sess.graph_def, and sess.run() for session.run(). Actually you can also find some other inconsistencies here and there in the documentation. It takes patience to figure them out yourself. :)\n", "\"It takes patience to figure them out yourself\": definitely! This is especially frustrating for beginners. Why doesn't Google test the code supplied in the documentation first? I spent hours for figuring out those inconsistencies :/\n", "Fixed, the current version of the doc doesn't have these typos and has a full, runnable code example.\n(And the code example is even tested.)\n", "can you link the commit that fixed this issue?\n", "also why has all these 'summaries' be added to display the already existing graph? supposedly the graph exists even with no summaries added, why can't the underlying graph be rendered without summaries? It seems like a duplication of effort from the user perspective\n"]}, {"number": 323, "title": "Tensorflow crashed when using AdamOptimizer", "body": "I tried the cnn in the tutorial for MNIST data, but initialize the parameters with stddev=1 (instead of stddev=0.1).\n\nError message:\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 4\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 4\n0 0.098\nW tensorflow/core/common_runtime/executor.cc:1027] 0x4472710 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: gradients/Relu_grad/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x4472710 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: gradients/Relu_1_grad/Relu_1/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add_1)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x4472710 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: gradients/Relu_2_grad/Relu_2/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add_2)]]\nTraceback (most recent call last):\n  File \"2_cnn.py\", line 67, in <module>\n    session.run(train_step, feed_dict={X: x, Y: y, keep_prob: 0.5})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 345, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 419, in _do_run\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: gradients/Relu_grad/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add)]]\nCaused by op u'gradients/Relu_grad/Relu/CheckNumerics', defined at:\n  File \"2_cnn.py\", line 55, in <module>\n    train_step = tf.train.AdamOptimizer().minimize(loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 165, in minimize\n    gate_gradients=gate_gradients)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 205, in compute_gradients\n    loss, var_list, gate_gradients=(gate_gradients == Optimizer.GATE_OP))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 414, in gradients\n    in_grads = _AsList(grad_fn(op_wrapper, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.py\", line 107, in _ReluGrad\n    t = _VerifyTensor(op.inputs[0], op.name, \"ReluGrad input is not finite.\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.py\", line 100, in _VerifyTensor\n    verify_input = array_ops.check_numerics(t, message=msg)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 48, in check_numerics\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 633, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1710, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 988, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'Relu', defined at:\n  File \"2_cnn.py\", line 32, in <module>\n    h_conv1 = tf.nn.relu(conv2d(X1, w_conv1) + b_conv1)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 506, in relu\n    return _op_def_lib.apply_op(\"Relu\", features=features, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 633, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1710, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 988, in __init__\n    self._traceback = _extract_stack()\n", "comments": ["Your weights are likely diverging. Try reducing the learning rate and/or revert your initialization change.\n", " it also crashed when I did following:\n- same weight initialization as the tutorial\n- change full_connect to 512 units\n- change batch_size to 200\n\nTraining crashed say, at step 3500 (when accuracy is near 99.2%).\n", "Consider increasing the epsilon value of the optimizer.\n", "I am getting the same error on my Neural Network. I get this error when using `AdagradOptimizer`, `AdamOptimizer`, `MomentumOptimizer` and even `GradientDescentOptimizer`. The error happens after the first epoch so the algorithm was already trained with each minibatch at least once. The function I am minimizing is the Root Mean Square error of the output of the networks with the labels. \n\nI am logging the weights max and min and they are between -1 and 1 until the error is raised. If I use a sigmoid activation function the weights will be between -1 and 1 until they get nan values. I have no idea of why this is happening, how can I debug it?\n", "One thing you can do is run with a tiny learning rate, or even zero learning rate. If you still have divergence then, you have a bug in your setup. If not, increase your rate slowly and see if there is a regime in which things train without diverging. It's completely possible to have weights that are in a good range, but activations or gradients going to infinity because of the shape of the loss, or too high a learning rate. It's obviously always a possibility that there is a bug in the optimizers, but in my experience, every single instance of this kind of problem could be traced back to a weirdly wired model, learning rate issues, bad randomization of the input examples, or - in the case of Adam or RMSProp - issues with the epsilon value.\n", "OP: it might help if you can paste your entire code in a gist and report it\nhere.\n\nOn Tuesday, November 24, 2015, Vincent Vanhoucke notifications@github.com\nwrote:\n\n> One thing you can do is run with a tiny learning rate, or even zero\n> learning rate. If you still have divergence then, you have a bug in your\n> setup. If not, increase your rate slowly and see if there is a regime in\n> which things train without diverging. It's completely possible to have\n> weights that are in a good range, but activations or gradients going to\n> infinity because of the shape of the loss, or too high a learning rate.\n> It's obviously always a possibility that there is a bug in the optimizers,\n> but in my experience, every single instance of this kind of problem could\n> be traced back to a weirdly wired model, learning rate issues, bad\n> randomization of the input examples, or - in the case of Adam or RMSProp -\n> issues with the epsilon value.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/323#issuecomment-159294978\n> .\n", "@vincentvanhoucke Thank you for your help. I tried to debug it following your advices:\n\nI ran the algorithm with learning_rate = 0. Didn't raise any error.\nI then tried to run the algorithm with different learning rates (lr) and obtained the following results:\n\n![lr_results](https://dl.dropboxusercontent.com/u/15853805/learning_rates.png)\n\nAll the learning rates, but the `lr = 1E-8`, made the program raise the error. The learning rate 1E-8 didn't learn anything.\n\nBasically I have a neural network with one hidden layer:\n\n``` python\ndef model(data, train=False):\n    hidden = tf.nn.relu(tf.matmul(data, fc1_weights) + fc1_biases)\n    return tf.matmul(hidden, fc2_weights) + fc2_biases\n```\n\nAnd a RMS loss function to optimize:\n\n``` python\ntrain_prediction = model(train_data_node, True)\nloss = tf.reduce_mean(tf.sqrt(tf.pow(((tf.sub(train_prediction,train_labels_node))),2)),name='RMS')\nbatch_index = tf.Variable(0)\nlearning_rate = tf.train.exponential_decay(\n      1E-8,                # Base learning rate.\n      batch_index * BATCH_SIZE,  # Current index into the dataset.\n      train_size,          # Decay step.\n      0.95,                # Decay rate.\n      staircase=True)\noptimizer = tf.train.MomentumOptimizer(learning_rate,0.9).minimize(loss, global_step=batch_index) \n```\n\nMy full code is here: [gist](https://gist.github.com/jpiabrantes/321aff988eaa00dfefcb) .\nIf you want to run it you will also need the data that is [here](/home/joao/Dropbox/data analysis/rossman/results/results1/nn_train.dat\n/home/joao/Dropbox/data analysis/rossman/results/results1/nn_val.dat) and [here](https://dl.dropboxusercontent.com/u/15853805/nn_train.dat).\n", "@jpiabrantes One thing that stands out here is that you use a square root in your loss. That's known to be very unstable numerically for very small values. Can you try 1) not taking the square root or 2) adding a small constant, e.g.: tf.sqrt(1e-4 + ...).\nIf that's indeed the problem, then one possible approach we can provide tooling around is to optionally cap the gradient of the sqrt function, because it's a common use case. Another possibility is to use gradient clipping in general (see: clip_by_norm(), which we should provide better examples for).\nLet me know what happens.\n\nThe other avenue is to tune your learning rate decay. Gradients tend to paradoxically grow in magnitude as training progresses, and sometimes that introduces numerical issues late in training. It is still possible that we have issues with numerical precision somewhere, but my hunch is that these errors are legitimate divergence issues (for some twisted definition of 'legitimate').\n", "@vincentvanhoucke You were right. My problem was due to the loss function being RMS. If I use the loss functions you proposed everything works fine. Thank you for the help. \n", "@vincentvanhoucke: I copied the MNIST Conv Net. Since, I have just a handful of images, each Batch is applying some random changes to the images. Code snippet below. The first iteration is successful. 2nd one gives the above error.\n\n```\ndef preprocessImages(x):\n    retValue = numpy.empty_like(x)\n    for i in range(len(data)):\n    image = x[i]\n    image = tf.reshape(image, [388,190,1])\n    image = tf.image.random_brightness(image, max_delta=63)\n    # Subtract off the mean and divide by the variance of the pixels.\n    float_image = tf.image.per_image_whitening(image)\n    float_image_Mat = sess.run(float_image)\n    retValue[i] = float_image_Mat.reshape((388*190))\nreturn retValue\n\nfor i in range(500):\n    feed_dict={x: preprocessImages(data), y_: labelsAsOneHotVector, keep_prob: 0.5}\n    train_step.run(feed_dict)\n    if i%100 == 0:\n        train_accuracy = accuracy.eval(feed_dict)\n        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n```\n\nFirst iteration works fine! \n\n```\nstep 0, training accuracy 0.680851\nW tensorflow/core/common_runtime/executor.cc:1027] 0x2a1a82e0 Compute status: Invalid     argument: ReluGrad input is not finite. : Tensor had NaN values\n```\n", "@navraj28 diverging when you use SGD can happen for many reasons:\n- your learning rate is too high. Note that the right learning rate can be data dependent.\n- your input data is not well randomized, or highly redundant, which could happen if you're just feeding the same data with few distortions.\n  This bug is about the AdamOptimizer. I am going to close it. Feel free to reopen if you have new evidence of a bug.\n", "It is likely a 0 log(0) issue. See: http://quabr.com/33712178/tensorflow-nan-bug\n", "@lightscalar that solved it for me. Thx.\n", "Thanx lightscalar\n", "cross_entropy = -tf.reduce_sum(y*tf.log(yconv+ 1e-9))\n\nThis works fine\n", "@lightscalar @Muaazbinsaeed Nice - I was stuck on this for awhile and convinced that my activations/weights were somehow exploding.\n", "Thanks @lightscalar, that fixed the issue for me too!!\n", "I had exactly the same problem and you fixed the issue for me too! Thanks @lightscalar !!\n", "thanks @lightscalar\n", "@lightscalar big thanks\n", "@vincentvanhoucke Is there any improvement regarding numerical precisions for `tf.sqrt()` with last versions? Running v0.12 and still getting the mentioned issue.", "@edgarriba that's not an issue with precision of `tf.sqrt` -- square root operation in general is unstable for small values. The bigger underlying issue is that SGD-like algorithms are fundamentally unstable and can produce NaN/Infinity during normal functioning. A stable algorithm would need to do some sanity checks before taking a step, and none of the tf built-in optimizers are doing that.", " @yanchen036 oh I see, thanks to clarify! I hope this feature comes some day since sometimes find such errors during the training is a bit painful.", "We use an optimizer that does a line-search at each step, that makes things more predictable", "Is it an internal code or is already part of TF?", "Sorry, by \"we\" I meant OpenAI, not Google, and we'll be open-sourcing our optimization code soon. However, its tailored for RL problems rather than a generic optimizer. You can use l-BFGS optimizer through the scipy optimizer interface, that one is supposed to be stable, although you incur some extra overhead in transferring data between TF and scipy", "I've come across a similar bug during calculating the L2 norm along a certain dimension of a feature map. The gradients no longer explode after I added small values(1e-10) elementwise to that feature map. Hope this could help someone."]}, {"number": 322, "title": "Allow adding and deleting graph nodes and edges in Tensorboard", "body": "Generating the graph from Python is not an expensive process.  It would greatly lower the barrier to entry if new users & data scientists who are more comfortable with GUIs could modify graph parameters directly from Tensorboard.\n\nIf I understand correctly (per a short tutorial from @rsepassi) you currently cant delete nodes from the graph without regenerating it, however regenerating the graph takes seconds, so it would be nice to allow creation of nodes and edges via the GUI (like Cafe).\n\nI know this is a really high-level feature request, and probably requires a fair amount of work.  Feel free to close it if this would be better discussed on a mailing list instead of Github Issues.\n", "comments": ["Creation won't be a problem atleast on the library's end I guess. As far as deletion is concerned, I have tried hacking around to see if its possible. Technically, it may be simple enough to delete all references to the node in question by deleting it from all graph collections and the Graph instance's inner dictionaries (which are used for methods like 'get_operation_by_name'). However, for complicated structures, you would first have to check if any other Ops depend on the node in question. If yes, deleting it would cause major issues. But I agree its a feature worth having (but with proper checking for dependencies ofcourse).\n", "I'll leave it open for tracking purposes, but if someone wants to tackle this it's probably better to start off as a separate repo.  I'm not sure it makes sense to integrate it into TensorBoard at first, but @danmane may have further thoughts.\n\nIf someone does give this a try, please comment here!\n", "Yeah, we've talked about this internally - it would be really cool to have a GUI for modifying the graph, but it seems like a lot of work and I suspect it would still not be powerful enough for most use cases. For it to be really useful it would need to be embedded in a broader gui-based execution system for TensorFlow, which also sounds very challenging to get right.\n\nAn easier path to fast-feedback graph creation would be to embed the graph visualizer into iPython notebook - you'd still be modifying the graph with code, but with a tighter feedback cycle.\n\nOf course, if someone wants to try building the GUI graph builder, they're welcome to :) and I'd love to see what comes of it.\n", "This seems much higher level than what I was thinking of, but still worth taking inspiration from [Tensorflow Playground](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=8,2&seed=0.82019&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=classification).\n", "Even if there wasn't a GUI for this, having some utility functions for making modified copies of graphs would be useful.\n\nI'm currently trying to use a model that was retrained from the inception model using the image_retraining example and I'm running into issues since there is a totally unnecessary DecodeJpeg Op in the model that isn't supported on android atm (#2570).\n\nIf no-one else volunteers to make these, I might see if I can hack some changes to the copying code, but I'm almost definitely the wrong person to be doing this.\n", "We do not plan to make the graph explorer editable unless compelling reasons arise. One can run TensorFlow in colab and get quick feedback on how the graph looks like.\r\n\r\nIf anyone has ideas on how to extend the graph (which we welcome!), please post issues and in our new repository ([tensorflow/tensorboard](https://github.com/tensorflow/tensorboard/issues))."]}, {"number": 321, "title": "Don't require absolute path when specifying tensorboard logs dir", "body": "When running tensorboard on Mac OS X currently, you have to specify the absolute directory to the logs path in order to get it to run.\n\n``` bash\ntensorboard --logdir /Users/myusername/Code/tensor-play/tf-play/linear_regression/logs/events\n```\n\nIt should accept relative paths (like most unix commands) for convenience.\n\n``` bash\ntensorboard --logdir logs/events\n```\n", "comments": ["My apologies, I didn't find this in search initially, my ticket is a duplicate of this one: https://github.com/tensorflow/tensorflow/issues/117\n"]}, {"number": 320, "title": "__ldg intrinsic unavailable on compute capability <3.2", "body": "I'm not sure if supporting compute 3.0 capability will ever be a priority for the project, it's mainly useful for people who want to train small convnets on AWS g2.2x and g2.8x boxes as they're learning the framework.\n\nA very recent commit 9c3043ff added the read-only data cache load function intrinsic __ldg to two operation defs: conv_ops_gpu_3.cu.cc and bias_op_gpu.cu.cc\n\nAll that's needed to maintain 3.0 support is to pass through __ldg(x) as a pointer -->  *x (I've confirmed this compiles and runs fine on the AWS GPU instances)  One could do this properly in theory with e.g. a generic wrapper:  (this is way too simplistic I'm sure...)\n\n```\ntemplate<typename T>\n__device__ __forceinline__ T ldg(const T* ptr) {\n#if __CUDA_ARCH__ >= 320\n    return __ldg(ptr);\n#else\n    return *ptr;\n#endif\n}\n```\n\nAgain, it's mainly an issue if you want to keep the unsupported TF_UNOFFICIAL_SETTING=1 option working for people who happen to be working with slightly older GPUs.  (BTW thanks for all the amazing work!)\n", "comments": ["https://github.com/tensorflow/tensorflow/commit/854f49bd43588c062b046384f239f64a3d819702 Fixed this a while ago, I believe.\n"]}, {"number": 319, "title": "Argmax, fetching top 5 probabilities", "body": "I'm having an issue with computing the accuracy of my model. I want to see if the top 5 predictions contains the actual label when comparing against the test set.\n\n```\ncorrect_prediction = tf.equal(tf.argmax(activation, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\nprint (\"ACCURACY IS:\")\nprint sess.run(accuracy, feed_dict={x: data.test_set.inputs(), y: data.test_set.targets()})\n```\n\nFrom what I've read in the documentation TensorFlow doesn't seem to have something like fetching the top 5 from `activation` and seeing if one of the items is contained in `argmax(y,1)`\n\nSomething like this would be nice:\n\n```\ncorrect_prediction = tf.contains(tf.argmax(activation, dim, N_VALUES_TO_FETCH), tf.argmax(y, dim)\n```\n\nCan anyone help me with the problem I am facing? Thank you.\n", "comments": ["Check out tf.nn.top_k(input, k, name=None) and tf.nn.in_top_k(predictions, targets, k, name=None).\n", "(let us know if those aren't sufficient)\n", "I would suggest linking to the `tf.nn.top_k` function in the documentation right after `argmin` and `argmax`, potentially under the name of `argsort` (which is what Numpy users would search for).\n", "I've just met this question and I've found one function that is exactly what I want.\r\n\r\n**tf.nn.in_top_k(predictions, targets, k, name=None)** \r\nSays whether the targets are in the top K predictions.\r\nThis outputs a batch_size bool array, an entry out[i] is true if the prediction for the target class is among the top k predictions among all predictions for example i. Note that the behavior of InTopK differs from the TopK op in its handling of ties; if multiple classes have the same prediction value and straddle the top-k boundary, all of those classes are considered to be in the top k."]}, {"number": 318, "title": "TensorFlow with Cuda 7.5 on Ubuntu 15.4?", "body": "Hi,\n\nI have not been able to install GPU TensorFlow on my Ubuntu 15.04 workstation because it seems to require Cuda 7.0, while I am running Cuda 7.5. As far as I can tell there are no Cuda 7.0 installs available for Ubuntu 15.04. Are there any patches/upgrades that will make it possible for me to use TensorFlow on my system? Or do I right now have to downgrade my Ubuntu in order to be able to work with it?\n\nThanks,\n\nBojan\n", "comments": ["You can try the Ubuntu 14.04 deb, or use the tgz installer here:\n\nhttps://developer.nvidia.com/cuda-toolkit-70\n\nunder the \"Linux x86\" tab.\n", "De-duping with https://github.com/tensorflow/tensorflow/issues/20\n"]}, {"number": 316, "title": "Recursively copying elements from one graph to another", "body": "One nifty feature to have in the codebase would be allowing recursive copying of Operation/Tensor/Variable instances from one graph to another. By recursive, I mean that copying the 'top' node (say the trainer for a basic NN) would automatically copy all the required Ops and tensors in the dataflow graph.\nThis could allow users to easily port common components of different frameworks from an 'origin' graph to a 'target' graph. To avoid naming conflicts, we could add the copied elements under a common namespace in the target graph.\n\nI tried writing up some code for it, and since the project doesn't accept pull request as of now, wrote up a blog post about it. I would like the community's feed back on possible issues or applications. Thanks!\n\nHeres a link to the post: https://codesachin.wordpress.com/2015/11/20/recursively-copying-elements-from-one-graph-to-another-in-tensorflow/\n", "comments": ["Thanks for the feature request!  We're also experimenting with support for defining \"functions\" in the graph that would basically allow re-usable components to achieve the same effect.\n", "Thats great! If you guys do enable PRs, you could have a look at my code to see if it can do the job.\n", "Looks like this fell through the cracks.  PRs have been enabled for a while.  @martinwicke: Does this seem appropriate for `contrib`? \n", "@vrv, what's the status of graph functions? We probably should standardize to one way of doing things, although even in the presence of functions functionality like this seems valuable to extract things from an existing graph.\n", "@zffchen78 \n", "I think we accepted this to contrib, closing.\n"]}, {"number": 315, "title": "Use CFFI for interfacing C functions.", "body": "[CFFI](https://cffi.readthedocs.org/en/latest/)  is a big step towards standard practices for making and distributing Python packages with C extension modules.\n\nWith CFFI the maintenance is much easier, and the code is still fast.\nThere are lot of benefits:\n- Abstract the Python version (CPython2, CPython3, PyPy).\n- Better control over when and why the C compilation occurs, and more standard ways to write distutils- or setuptools-based setup.py files.\n- Keep all the Python-related logic in Python so that you don\u2019t need to write much C code.\n\nI'm really looking forward to use [PyPy](http://pypy.org/) with tensorflow.\n", "comments": [":+1:\n", "What about [pybind11](https://github.com/wjakob/pybind11)?\n", "@sherrym is anyone in the core team thinking about this?\n", "Thank you for the suggestion, but it's likely we won't do this, and it is would be undesirable to maintain two sets of Python bindings, closing for now. It also seems PyPy also now has the normal extension API available, relieving that need for this.  \r\n", "OK, I understand your motivation.\r\nJust to clarify PyPy C-Python API  support. That's true that the compatibility is pretty high, but it comes with limitations: speed and GC (it's slow, not jitted and it goes through simulation layers)."]}, {"number": 314, "title": "Replace cuda R2 references with R3", "body": "pulling hack to compile tf against cuda R3 on GPU with 3.0 capability\n", "comments": []}, {"number": 313, "title": "Allow learning rates of specific variables to be scaled", "body": "When repurposing a network, it can be useful to set layer-wise learning rates so that the final layer (which has random weights) does most of the learning initially. A useful pattern for applying learning rate scaling would be tf.stop_gradient(input, name=None), which can be seen as effectively scaling the learning rate by a factor of zero (obviously it also stops computation).\n", "comments": ["You can use different optimizers for any subset of variables in your graph to get per-layer learning rates.\n\nThe 'minimize' function of an optimizer http://www.tensorflow.org/api_docs/python/train.html#optimizers allows you to specify which variables should be updated.  Let us know if this isn't what you need ...\n", "nope, its perfect, thanks\n", "hi guys, I would like to know more about this layer-wise learning rate since I am using this technique for fine-tuning. It seems by using stop_gradient, we can only be able to specify which layer should be prevented from updating. What if I want to give low level layers some low learning rate like 0.0001 and give high level layers high value like 0.01. How am I supposed to do this. Thank you\n", "This should really be posted on StackOverflow -- you'll get more community help there :)\n"]}, {"number": 312, "title": "Relax the requirements for saver checkpoints", "body": "I'm working on testing a few modifications to an existing network including attempting to use it as a teacher in a student/teacher network and swapping its optimizer but I get the following error:\n\n```\nTraceback (most recent call last):\n  File \"knit_net_train_tf.py\", line 40, in <module>\n    saver.restore(sesh, CHECKPOINT_FILE)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 869, in restore\n    sess.run([self._restore_op_name], {self._filename_tensor_name: save_path})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 401, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 477, in _do_run\n    e.code)\ntensorflow.python.framework.errors.NotFoundError: Tensor name \"conv1/biases/Adagrad\" not found in checkpoint files knitnet.0.ckpt\n         [[Node: save/restore_slice_2 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_2/tensor_name, save/restore_slice_2/shape_and_slice)]]\n```\n\nIt appears that the saver requires that every variable that exists in the current graph also be present in the checkpoint and that every variable that that exists in the checkpoint also be in the current graph. I can work around this by making major modifications to the way that my network is constructed but I would be nice to have a feature that says \"don't error out if you don't find this variable, its ok\". Alternatively it appears that I can specify which variables I want to restore, which could also solve the problem, but I have a lot of variable that I haven't explicitly tracked, many of which are created by the optimizer. It would make this feature a lot more useful if there were a function that did \"return all variables that this op depends on\", which I could call on the training variable or the prediction variable and only save those tensors.\n", "comments": ["Agreed on this. Another example could be: I change the dimension (and name) of the last fully-connected layer of some imagenet model, and wanted to load a pre-trained one for some other classification task. An option to ignore the \"tensor not found\" error would be good. (probably with some warning, or return a list of variables successfully restored). \n", "The tooling has improved in 0.8: you can call `tf.all_variables()` to get a list of all of the variables in your model, and use the [`inspect_checkpoint.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/inspect_checkpoint.py) tool to get the list of variables in a checkpoint file, so it should be easier to build the list of variables to restore.\n\nI don't think we'd want to add a less-strict version of `restore()` that ignores missing values, although we'd consider contributions of a tool that integrates the `inspect_checkpoint.py` and `tf.all_variables()` code to make doing this easier.\n", "If so, how can I add a new variable and use them with previously saved variables?\n", "When the checkpoint files are saved with a subset of GPUs, they can't be loaded if new GPUs are used. The error is `tensorflow.python.framework.errors.NotFoundError: Tensor name \"tower_name_gpu_3/fc/Variable_1\" not found in checkpoint files`. How to work around the strict restriction?\n", "@futurely, how do you want to work around this restriction? Do you want to restore all you have, and initialize the rest to what's in the checkpoint? If that's the case, what you can do is this:\n1. Initialize all the variables.\n2. Get all the variables in the checkpoint by using get_variable_to_shape_map() and restore those only.\n\nSee https://github.com/tensorflow/tensorflow/blob/8df836f2611c533c54d2bdaaf107c67afac8d6a5/tensorflow/python/training/saver_test.py#L1214\n\nfor usage example.\n\n@koonyook, how do you want to use a new variable with previously saved variables?\n", "@sherrym , Thanks for the useful information. \nI am adding some new variables to the model serialized on the disk. \nAfter I get the variables with get_variable_to_shape_map(model_path), how do I only restore those variables? Thank you. \n", "I think it's way more user-friendly to allow new names, new shapes, and removed variables (Caffe style). This code seems to get that job done. I call it after running the global init operation.\r\n\r\n```python\r\ndef optimistic_restore(session, save_file):\r\n    reader = tf.train.NewCheckpointReader(save_file)\r\n    saved_shapes = reader.get_variable_to_shape_map()\r\n    var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\r\n            if var.name.split(':')[0] in saved_shapes])\r\n    restore_vars = []\r\n    with tf.variable_scope('', reuse=True):\r\n        for var_name, saved_var_name in var_names:\r\n            curr_var = tf.get_variable(saved_var_name)\r\n            var_shape = curr_var.get_shape().as_list()\r\n            if var_shape == saved_shapes[saved_var_name]:\r\n                restore_vars.append(curr_var)\r\n    saver = tf.train.Saver(restore_vars)\r\n    saver.restore(session, save_file)\r\n```", "@mrry I really don't understand why would not you add an option to restore as much as possible without raising errors when some variable is not present in the checkpoint. It would really make life much easier for a large number of users.\r\n\r\nFor example, it would be possible to save everything using `Saver(tf.global_variables()) `, add some new variables and restore only global variables that were present in the checkpoint. \r\n\r\nI can implement this feature if you agree.  \r\n", "@dandelionmane Thanks for your codes. But I found sometimes it would go wrong due to not-found variables.(I met one with Adam/Wxxx). I made a small change to your codes and it works fine for me.\r\n\r\n\tdef optimistic_restore(session, save_file):\r\n\t\treader = tf.train.NewCheckpointReader(save_file)\r\n\t\tsaved_shapes = reader.get_variable_to_shape_map()\r\n\t\tvar_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\r\n\t\t\t\tif var.name.split(':')[0] in saved_shapes])\r\n\t\trestore_vars = []\r\n            name2var = dict(zip(map(lambda x:x.name.split(':')[0], tf.global_variables()), tf.global_variables()))\r\n\t\twith tf.variable_scope('', reuse=True):\r\n\t\t\tfor var_name, saved_var_name in var_names:\r\n\t\t\t\tcurr_var = name2var[saved_var_name]\r\n\t\t\t\tvar_shape = curr_var.get_shape().as_list()\r\n\t\t\t\tif var_shape == saved_shapes[saved_var_name]:\r\n\t\t\t\t\trestore_vars.append(curr_var)\r\n\t\tsaver = tf.train.Saver(restore_vars)\r\n\t\tsaver.restore(session, save_file)\r\n", "I have a same question. when I restore my LSTM model trained by tensorflow 0.11, the error message told me some variable can not found in checkpoint file, but when I use the  [inspect_checkpoint.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/inspect_checkpoint.py), that variable is existed and have data, I have been confused long time. How can I fixed it?\r\n`NotFoundError (see above for traceback): Tensor name \"bias_fc_end\" not found in checkpoint files /root/PycharmProjects/....../DataOutput1/Hyperparameter2/Hyperparameter_2_RepeatNum_1_Epoch_3.tfmodel\r\n\t [[Node: save/restore_slice_9 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_9/tensor_name, save/restore_slice_9/shape_and_slice)]]\r\n\t [[Node: save/restore_slice_9/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_33_save/restore_slice_9\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n`", "@dandelionmane thanks for your optimistic_restore function; I've tried it successfully. It really should be part of Saver itself. For my purposes I've modified it only slightly to make function return list of variables given checkpoint path, that way it's not concerned with session yet.\r\n\r\n    def optimistic_restore_vars(model_checkpoint_path):\r\n        reader = tf.train.NewCheckpointReader(model_checkpoint_path)\r\n        saved_shapes = reader.get_variable_to_shape_map()\r\n        var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\r\n                            if var.name.split(':')[0] in saved_shapes])\r\n        restore_vars = []\r\n        name2var = dict(zip(map(lambda x:x.name.split(':')[0], tf.global_variables()), tf.global_variables()))\r\n        with tf.variable_scope('', reuse=True):\r\n            for var_name, saved_var_name in var_names:\r\n                curr_var = name2var[saved_var_name]\r\n                var_shape = curr_var.get_shape().as_list()\r\n                if var_shape == saved_shapes[saved_var_name]:\r\n                    restore_vars.append(curr_var)\r\n        return restore_vars\r\n\r\n    ckpt = tf.train.get_checkpoint_state(os.path.dirname(checkpoint + '/checkpoint'))\r\n    saver = tf.train.Saver(max_to_keep=20, var_list = optimistic_restore_vars(ckpt.model_checkpoint_path) if checkpoint else None)", "+1 on making `optimstic_restore` an optional keyword-based behavior of `Saver.restore method` e.g. `saver.restore(optimstic=False)`", "The following codes work for me:\r\n```\r\ninit_assign_op, init_feed_dict = slim.assign_from_checkpoint(\r\n            checkpoint_path, slim.get_variables_to_restore(), ignore_missing_vars=True)\r\nsess.run(init_assign_op, feed_dict=init_feed_dict)\r\n```", "If anyone is interested, I have updated my original code to give more feedback and work for directories as well. I haven't tried the slim solution, but it probably works equally as well, though it may not give as much nice feedback.\r\n\r\n```python\r\ndef restore(session, save_file, raise_if_not_found=False, copy_mismatched_shapes=False):\r\n    if not os.path.exists(save_file) and raise_if_not_found:\r\n        raise Exception('File %s not found' % save_file)\r\n    reader = tf.train.NewCheckpointReader(save_file)\r\n    saved_shapes = reader.get_variable_to_shape_map()\r\n    var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\r\n            if var.name.split(':')[0] in saved_shapes])\r\n    var_name_to_var = {var.name : var for var in tf.global_variables()}\r\n    restore_vars = []\r\n    restored_var_names = set()\r\n    restored_var_new_shape = []\r\n    print('Restoring:')\r\n    with tf.variable_scope(tf.get_variable_scope(), reuse=True):\r\n        for var_name, saved_var_name in var_names:\r\n            if 'global_step' in var_name:\r\n                restored_var_names.add(saved_var_name)\r\n                continue\r\n            curr_var = var_name_to_var[var_name]\r\n            var_shape = curr_var.get_shape().as_list()\r\n            if var_shape == saved_shapes[saved_var_name]:\r\n                restore_vars.append(curr_var)\r\n                print(str(saved_var_name) + ' -> \\t' + str(var_shape) + ' = ' +\r\n                      str(int(np.prod(var_shape) * 4 / 10**6)) + 'MB')\r\n                restored_var_names.add(saved_var_name)\r\n            else:\r\n                print('Shape mismatch for var', saved_var_name, 'expected', var_shape,\r\n                      'got', saved_shapes[saved_var_name])\r\n                restored_var_new_shape.append((saved_var_name, curr_var, reader.get_tensor(saved_var_name)))\r\n                print('bad things')\r\n    ignored_var_names = sorted(list(set(saved_shapes.keys()) - restored_var_names))\r\n    print('\\n')\r\n    if len(ignored_var_names) == 0:\r\n        print('Restored all variables')\r\n    else:\r\n        print('Did not restore:' + '\\n\\t'.join(ignored_var_names))\r\n\r\n    if len(restore_vars) > 0:\r\n        saver = tf.train.Saver(restore_vars)\r\n        saver.restore(session, save_file)\r\n\r\n    if len(restored_var_new_shape) > 0 and copy_mismatched_shapes:\r\n        print('trying to restore misshapen variables')\r\n        assign_ops = []\r\n        for name, kk, vv in restored_var_new_shape:\r\n            copy_sizes = np.minimum(kk.get_shape().as_list(), vv.shape)\r\n            slices = [slice(0,cs) for cs in copy_sizes]\r\n            print('copy shape', name, kk.get_shape().as_list(), '->', copy_sizes.tolist())\r\n            new_arr = session.run(kk)\r\n            new_arr[slices] = vv[slices]\r\n            assign_ops.append(tf.assign(kk, new_arr))\r\n        session.run(assign_ops)\r\n        print('Copying unmatched weights done')\r\n    print('Restored %s' % save_file)\r\n    try:\r\n        start_iter = int(save_file.split('-')[-1])\r\n    except ValueError:\r\n        print('Could not parse start iter, assuming 0')\r\n        start_iter = 0\r\n    return start_iter\r\n\r\n\r\ndef restore_from_dir(sess, folder_path, raise_if_not_found=False, copy_mismatched_shapes=False):\r\n    start_iter = 0\r\n    ckpt = tf.train.get_checkpoint_state(folder_path)\r\n    if ckpt and ckpt.model_checkpoint_path:\r\n        print('Restoring')\r\n        start_iter = restore(sess, ckpt.model_checkpoint_path, raise_if_not_found, copy_mismatched_shapes)\r\n    else:\r\n        if raise_if_not_found:\r\n            raise Exception('No checkpoint to restore in %s' % folder_path)\r\n        else:\r\n            print('No checkpoint to restore in %s' % folder_path)\r\n    return start_iter\r\n```"]}, {"number": 311, "title": "Tensorflow not assigning any tasks to GPU in cifar10 example", "body": "I'm running the `cifar10_multi_gpu_train.py` script with the device placement logging turned on, and I see that all of the operations are being placed on the CPU. When I run it, it outputs:\n\n```\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 8\nI tensorflow/core/common_runtime/direct_session.cc:45] Direct session inter op parallelism threads: 8\nDevice mapping: no known devices.\nI tensorflow/core/common_runtime/direct_session.cc:111] Device mapping:\n\nsoftmax_linear/biases/ExponentialMovingAverage: /job:localhost/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:289] softmax_linear/biases/ExponentialMovingAverage: /job:localhost/replica:0/task:0/cpu:0\n.....\n.....\n```\n\nI would imagine that the line `Device mapping: no known devices.` is supposed to list the devices associated with the current Session, but why does it not have any devices? The script calls \n`tf.device('/gpu:0')`.\n\nMy GPU is Nvidia GeForce GTX 970.\n", "comments": ["Solved this.\n\nI must have accidentally built the python package without the \"--config=cuda\" flag. When I rebuilt with this flag, then pip uninstalled tensorflow, then reinstalled with the newly built package, it worked perfectly.\n", "The same problem occurred when I was running the cifar10_multi_gpu_train.py script with the device placement logging turned on. I have no idea what do you mean when you say \"I must have accidentally built the python package without the \"--config=cuda\" flag\", can you tell which python package you rebuilt?\n", "@GuangmingZhu , on the TensorFlow [Download and Setup page](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html), the command to build the pip package should be:\n\n`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\n\nI just forgot the `--config=cuda`.\n"]}, {"number": 310, "title": "Fast python protocol buffers?", "body": "This might be a better question for the protobuf team, but is there any way to get [fast python protocol buffers](https://developers.google.com/protocol-buffers/docs/reference/python-generated#cpp_impl) with TensorFlow? I have tried `bazel build --copt=-DPYTHON_PROTO2_CPP_IMPL_V2` but running with `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION='cpp'` gives the following error:\n\n``` python\nTraceback (most recent call last):\n  File \"/home/kearnes/git/tensorflow/bazel-bin/deaps/test.runfiles/deaps/test.py\", line 21, in <module>\n    import tensorflow as tf\n  File \"/home/kearnes/git/tensorflow/bazel-bin/deaps/test.runfiles/tensorflow/__init__.py\", line 8, in <module>\n    from tensorflow.python import *\n  File \"/home/kearnes/git/tensorflow/bazel-bin/deaps/test.runfiles/tensorflow/python/__init__.py\", line 24, in <module>\n    raise ImportError(msg)\nImportError: Error importing tensorflow: you should not try to import\n  tensorflow from its source directory; please exit the tensorflow source tree,\n  and relaunch your python interpreter from there.\n  Original ImportError: cannot import name _message\n```\n", "comments": ["This may be caused by incompatible versions of protobuf installed by python.  Are you using the python protobuf package version 3.0.0a3?\n", "@ebrevdo I'm using the version that comes with `git clone --recurse_submodules`:\n\n```\n$ git submodule status \n 55ad57a235c009d0414aed1781072adda0c89137 google/protobuf (v3.0.0-alpha-4-179-g55ad57a)\n```\n", "This is the same version you've got installed in your python interpreter?\nAre you using blaze commands to run the scripts, or are you running python\nyourself?\n\nOn Fri, Nov 20, 2015 at 9:57 AM, Steven Kearnes notifications@github.com\nwrote:\n\n> @ebrevdo https://github.com/ebrevdo I'm using the version that comes\n> with git clone --recurse submodules:\n> \n> $ git submodule status\n>  55ad57a235c009d0414aed1781072adda0c89137 google/protobuf (v3.0.0-alpha-4-179-g55ad57a)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/310#issuecomment-158474448\n> .\n", "I don't have a version installed in my interpreter at all. I'm just running bazel and then running the bazel-bin binary.\n", "This looks like a bug either on the protobuf side or somewhere in the tensorflow/protobuf build interaction.  I was able to replicate; we're working on a solution.\n", "@ebrevdo Any updates on this issue?\n", "Just to confirm; are you running bazel test?  (looks like it)\n\nIt may be that the wrong version of protobuf is being used.  Is this in linux?\n\nTry _also_ installing the protobuf package via pip and making sure you're using the right version in your system python.  Does that help the problem?\n", "I'm running bazel build followed by running the bazel-bin binary. This is on Ubuntu 14.04. It seems that //google/protobuf/python/google/protobuf/pyext/... is not being built at all. Nothing in the BUILD references any of those files. I've been trying to hack the build rules, but that is quickly becoming a rabbit hole.\n\nFollowing your suggestion, I have been able to install protobuf from the branch that comes with tensorflow into my python, but I'm still getting errors like `ImportError: cannot import name _message` when I try to use it with `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp`.\n\nI'll note that `python setup.py test --cpp_implementation` (from the protobuf python install) also gives an error: `AttributeError: 'module' object has no attribute 'python_message'`.\n", "Sounds like this may be a bug in the version of protobuf we check out. Any\nchance you could file a bug there and cross link it here? You can also try\nusing the master HEAD version for our submodule and see the problem has\nbeen fixed.\nOn Nov 30, 2015 3:45 PM, \"Steven Kearnes\" notifications@github.com wrote:\n\n> I'm running bazel build, as described in OP. This is on Ubuntu 14.04. It\n> seems that //google/protobuf/python/google/protobuf/pyext/... is not being\n> built at all. Nothing in the BUILD references any of those files. I've been\n> trying to hack the build rules, but that is quickly becoming a rabbit hole.\n> \n> Following your suggestion, I have been able to install protobuf from the\n> branch that comes with tensorflow into my python, but I'm still getting\n> errors like ImportError: cannot import name _message when I try to use it\n> with PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp.\n> \n> I'll note that python setup.py test --cpp_implementation (from the\n> protobuf python install) also gives an error: AttributeError: 'module'\n> object has no attribute 'python_message'.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/310#issuecomment-160800476\n> .\n", "Sent PR https://github.com/google/protobuf/pull/1029 to google/protobuf.\n", "google/protobuf#1029 was merged; I'll leave the issue open pending update of the protobuf version that comes with TF.\n", "Protobuf version updated in https://github.com/tensorflow/tensorflow/commit/3116631f950a7de755c786fc954dc9cfb295d127. Note that you need to use \n\n```\nbazel build --define=use_fast_cpp_protos=true ...\n```\n", "@ebrevdo @skearnes I found this issue after some searching -- is this flag still required? It's not mentioned anywhere in the documentation; the only mention of fast protos is the pre-compiled protobuf wheel.", "@lemonzi just do this\r\n\r\n`python -c \"from google.protobuf.internal import api_implementation; print(api_implementation._default_implementation_type)\"`\r\n\r\nIf it prints \"cpp\" - good. If you prints \"python\" - bad.", "@yaroslavvb It printed \"python\" after compiling from sources and installing the wheel, both compiling as instructed by the docs and adding the `use_fast_cpp_protos` flag. I had to install an older version of protobuf from the binary builds (https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.0.0-cp27-cp27m-macosx_10_11_x86_64.whl). Since this is a flag for protobuf and not tensorflow, maybe it has to be added internally to the BUILD file for tensorflow?", "I'm seeing \"python\" from `python -c \"from google.protobuf.internal import api_implementation; print(api_implementation._default_implementation_type)\"` using a bazel build of TF 1.2.1.", "FYI downloading and installing the [latest cpp whl](https://pypi.python.org/pypi/protobuf/3.3.0) from pypi fixes the issue for me.\r\n\r\nSee the discussion at https://github.com/google/protobuf/issues/1332", "Also this https://www.tensorflow.org/versions/r0.12/get_started/os_setup#protobuf_library_related_issues", "ImportError                               Traceback (most recent call last)\r\n<ipython-input-5-41389fad42b5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\nE:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\nE:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     50 \r\n     51 # Protocol buffers\r\n---> 52 from tensorflow.core.framework.graph_pb2 import *\r\n     53 from tensorflow.core.framework.node_def_pb2 import *\r\n     54 from tensorflow.core.framework.summary_pb2 import *\r\n\r\nE:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py in <module>()\r\n      4 import sys\r\n      5 _b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))\r\n----> 6 from google.protobuf import descriptor as _descriptor\r\n      7 from google.protobuf import message as _message\r\n      8 from google.protobuf import reflection as _reflection\r\n\r\nImportError: No module named 'google'\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "BTW, it looks like official Mac packages for fast protobuf are no longer maintained, so the link I posted is out of date. You have to build protobuf yourself (see this comment https://github.com/tensorflow/tensorflow/issues/15417#issuecomment-352931701)", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "i am also the problem ,i don't kown how to solve it in window"]}, {"number": 309, "title": "time series example", "body": "is there a multivariate numerical time series example in tensorflow.\n\nMany thanks,\nAndrew\n", "comments": ["+1\n", "+1\n", "+1\n", "@girving if no body is working on that issue , i can start working on it\n", "@mbaddar1: Very cool!\n\n@martinwicke: Do you think this should go in core or the upcoming model zoo?\n", "Everything that doesn't have a full tutorial associated with it should go\ninto the zoo (which we will announce shortly). If you want to write a\ntutorial we'd be happy to merge it into core.\nOn Mon, Jan 25, 2016 at 11:17 Geoffrey Irving notifications@github.com\nwrote:\n\n> @mbaddar1 https://github.com/mbaddar1: Very cool!\n> \n> @martinwicke https://github.com/martinwicke: Do you think this should\n> go in core or the upcoming model zoo?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/309#issuecomment-174626520\n> .\n", "Hi guys!\nApologies but is this inferring that somebody will work on the time series problem?\nMany thanks guys,\nAndrew\n", "and is there a link to zoo?\nCheers,\nAndrew\n", "Zoo: shortly. Nobody is working on a time series example as far as I know.\nOn Mon, Jan 25, 2016 at 17:22 andrewcz notifications@github.com wrote:\n\n> and is there a link to zoo?\n> Cheers,\n> Andrew\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/309#issuecomment-174757369\n> .\n", "Look forward to that.\n", "https://m.reddit.com/r/MachineLearning/comments/3sok8k/tensorflow_basic_rnn_example_with_variable_length/\n\nThought i would post this here. somebody might turn it into a tutorial.\nBest,\nAndrew\n", ":+1:\n", "@martinwicke Any update on Zoo? :tiger: :elephant:", "@allenlavoie how far are we with time series?", "@martinwicke We're looking to open source in Q2. I was actually going to ask you (or someone else who works on Estimators) to look at the API soon.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "New information: probably going to be the (very) beginning of Q3 rather than the end of Q2. Definitely in contrib for 1.3.\r\n\r\nRe-opening because this is still relevant.", "We have some [time series Estimators in contrib](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/timeseries). Feel free to take a look and open specific bugs/feature requests."]}, {"number": 308, "title": "IOError: [Errno socket error] [Errno 60] Operation timed out", "body": "I used tensorflow with Mac. But when running the code convolutional.py, there is some unexpected error.\n\n(cinos)\u279c  ~  python anaconda/envs/cinos/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\n\nTraceback (most recent call last):\n  File \"anaconda/envs/cinos/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\", line 270, in <module>\n    tf.app.run()\n  File \"/Users/kk/anaconda/envs/cinos/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 11, in run\n    sys.exit(main(sys.argv))\n  File \"anaconda/envs/cinos/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\", line 103, in main\n    test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n  File \"anaconda/envs/cinos/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\", line 39, in maybe_download\n    filepath, _ = urllib.urlretrieve(SOURCE_URL + filename, filepath)\n  File \"/Users/kk/anaconda/envs/cinos/lib/python2.7/urllib.py\", line 98, in urlretrieve\n    return opener.retrieve(url, filename, reporthook, data)\n  File \"/Users/kk/anaconda/envs/cinos/lib/python2.7/urllib.py\", line 245, in retrieve\n    fp = self.open(url, data)\n  File \"/Users/kk/anaconda/envs/cinos/lib/python2.7/urllib.py\", line 213, in open\n    return getattr(self, name)(url)\n  File \"/Users/kk/anaconda/envs/cinos/lib/python2.7/urllib.py\", line 351, in open_http\n    errcode, errmsg, headers = h.getreply()\n  File \"/Users/kk/anaconda/envs/cinos/lib/python2.7/httplib.py\", line 1207, in getreply\n    response = self._conn.getresponse()\n  File \"/Users/kk/anaconda/envs/cinos/lib/python2.7/httplib.py\", line 1132, in getresponse\n    response.begin()\n  File \"/Users/kk/anaconda/envs/cinos/lib/python2.7/httplib.py\", line 453, in begin\n    version, status, reason = self._read_status()\n  File \"/Users/kk/anaconda/envs/cinos/lib/python2.7/httplib.py\", line 409, in _read_status\n    line = self.fp.readline(_MAXLINE + 1)\n  File \"/Users/kk/anaconda/envs/cinos/lib/python2.7/socket.py\", line 480, in readline\n    data = self._sock.recv(self._rbufsize)\nIOError: [Errno socket error] [Errno 60] Operation timed out\n", "comments": ["Most likely a transient issue, related to https://github.com/tensorflow/tensorflow/issues/209\n"]}, {"number": 307, "title": "Setting large batch_size in logistic regression method will result output nan", "body": "import tensorflow as tf\nimport numpy as np\nimport melt_dataset\nimport sys\nfrom sklearn.metrics import roc_auc_score\n\ndef init_weights(shape):\n    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n\ndef model(X, w):\n    return 1.0/(1.0 + tf.exp(-(tf.matmul(X, w))))\n# ./logistic_regression.py corpus/feature.normed.rand.12000.0_2.txt corpus/feature.normed.rand.12000.1_2.txt\n# notice if setting batch_size too big here 500 will result in learning turn output nan...  why? @TODO\n\nbatch_size = 100\nlearning_rate = 0.01\nnum_iters = 100\n\nargv = sys.argv \ntrainset = argv[1]\ntestset = argv[2]\n\ntrX, trY = melt_dataset.load_dense_data(trainset)\nprint \"finish loading train set \",trainset\nteX, teY = melt_dataset.load_dense_data(testset)\nprint \"finish loading test set \", testset\n\nnum_features = trX[0].shape[0]\nprint 'num_features: ',num_features \nprint 'trainSet size: ', len(trX)\nprint 'testSet size: ', len(teX)\nprint 'batch_size:', batch_size, ' learning_rate:', learning_rate, ' num_iters:', num_iters\n\nX = tf.placeholder(\"float\", [None, num_features]) # create symbolic variables\nY = tf.placeholder(\"float\", [None, 1])\n\nw = init_weights([num_features, 1]) # like in linear regression, we need a shared variable weight matrix for logistic regression\n\npy_x = model(X, w)\n\ncost = -tf.reduce_sum(Y*tf.log(py_x) + (1 - Y) \\* tf.log(1 - py_x))\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) # construct optimizer\n\npredict_op = py_x\n\nsess = tf.Session()\ninit = tf.initialize_all_variables()\nsess.run(init)\n\nfor i in range(num_iters):\n    predicts, cost_ = sess.run([predict_op, cost], feed_dict={X: teX, Y: teY})\n    print i, 'auc:', roc_auc_score(teY, predicts), 'cost:', cost_\n    for start, end in zip(range(0, len(trX), batch_size), range(batch_size, len(trX), batch_size)):\n            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n\npredicts, cost_ = sess.run([predict_op, cost], feed_dict={X: teX, Y: teY})\nprint 'final ', 'auc:', roc_auc_score(teY, predicts),'cost:', cost_\n\nif  setting batch_size to 100\n0 auc: 0.595120586861 cost: 1422.4\n1 auc: 0.911648704913 cost: 477.082\n2 auc: 0.916009327839 cost: 459.99\n3 auc: 0.918605639188 cost: 460.72\n4 auc: 0.919915596278 cost: 474.58\n5 auc: 0.920871510912 cost: 487.933\n6 auc: 0.921381332049 cost: 500.054\n7 auc: 0.921853388658 cost: 510.762\n\nif setting batch_size to 500\n0 auc: 0.615236099113 cost: 1354.38\n1 auc: 0.560017277272 cost: nan\n2 auc: 0.560017277272 cost: nan\n3 auc: 0.560017277272 cost: nan\n4 auc: 0.560017277272 cost: nan\n5 auc: 0.560017277272 cost: nan\n6 auc: 0.560017277272 cost: nan\n7 auc: 0.560017277272 cost: nan\n8 auc: 0.560017277272 cost: nan\n\nI am a bit curious as I will not face nan for the same train,test data using thenao implemented logistic regression.\n", "comments": ["Well, I find it is due to too large learning rate, for large batch size , need low learning rate, not an issue, close it.\n", "\"tensorflow.python.framework.errors.OutOfRangeError: Nan in summary histogram for: HistogramSummary\"\n\nThis is really hard to understand error. If you want to increase developer happiness using TensorFlow, a more clearer error message could help.\n", "@mxrguspxrt: so we can understand what specific improvements would help, what is your current understanding of that error message? \n", "Current questions for me where  from\n\"tensorflow.python.framework.errors.OutOfRangeError: Nan in summary\nhistogram for: HistogramSummary\".\n\nIs nan out of range? Why I am getting a OutOfRangeError when I increase\nnumber of examples? Maybe a message like \"Your learning rate is too big for\nfinding local minimum\" (what is the real problem?) would be more helpful?\n"]}, {"number": 306, "title": "Strange input image in android demo", "body": "I'm running android camera-based demo application. When I change SAVE_PREVIEW_BITMAP in TensorflowImageListener.java into true, I can save croppedBitmap. However, this saved image is all green with little detail of image.\n\n![preview](https://cloud.githubusercontent.com/assets/13412657/11295381/69658806-8fae-11e5-8db7-9943d1f5a195.png)\n\nWhen the camera is connected, YUV_420_888 format is used, Then plane #0 is always Y, plane #1 is always U (Cb), and plane #2 is always V (Cr).\n\n```\nimageReader =\n            ImageReader.newInstance(\n                largest.getWidth(), largest.getHeight(), ImageFormat.YUV_420_888, /*maxImages*/ 2);\n```\n\nHowever in TensorflowImageListener.java only 0, 2 planes are used.\n\n```\nfinal int[] planeOrder = {0, 2};\nfor (int i = 0; i < planeOrder.length; ++i) {\n  final Plane plane = planes[planeOrder[i]];\n  final ByteBuffer buffer = plane.getBuffer();\n  buffer.rewind();\n  final int readAmount = buffer.remaining();\n  buffer.get(yuvBytes, position, readAmount);\n  position += readAmount;\n}\n```\n\nBy the way I'm using LG G2 and Android version is 5.0.1\n", "comments": ["Hi,\nThis behavior should be fixed in with commit https://github.com/tensorflow/tensorflow/commit/854f49bd43588c062b046384f239f64a3d819702\n\nAn upcoming change will push all of the colorspace conversion into native code as well.\n", "color space conversion change in bf6b536bde7d8060c489b51fedb58968b8cbfd7c\n", "I found that Y channel is OK but U, V channels are all zero because of low android version.\nIt was fixed in Android 5.1.1\n\nhttps://code.google.com/p/android/issues/detail?id=81984\n\nIf Android version can be checked in code and it is below 5.1, please use only Y channel to make input RGB image. Performance is much better.\n", "I see, thanks for the update. Does this occur at all preview resolutions?\n\nOn Fri, Dec 4, 2015 at 2:13 AM, barami98 notifications@github.com wrote:\n\n> I found that Y channel is OK but U, V channels are all zero because of low\n> android version.\n> It was fixed in Android 5.1.1\n> \n> https://code.google.com/p/android/issues/detail?id=81984\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/306#issuecomment-161895881\n> .\n"]}, {"number": 305, "title": "ScatterSub on GPU", "body": "I tried running a CNN with embedding layer on a GPU (AWS), but ran into the same problem as described here: http://stackoverflow.com/questions/33624048/fail-to-run-word-embedding-example-in-tensorflow-tutorial-with-gpus\n\n`ScatterSub` doesn't seem to be supported on GPUs and the device placement fails. It would be great if was :) \n\nIt would be great if the examples worked on GPUs.\n", "comments": ["Agreed, scatter on GPUs would be useful.  Also gather.\n", "I will be great for NLP tasks like machine translation to work and re-train embeddings matrix on GPU. So, gather and scatterSub should work on GPU.\n", "I agree, this functionality would be great to have. Any news on the progress?\n", "Progress here: https://github.com/tensorflow/tensorflow/pull/782\n", "Note that while this is progress, and you can theoretically remove the `with tf.device('/cpu:0')` line, it wouldn't work completely, because many underlying ops are still not GPU enabled (you can see this if you use `with tf.device('/gpu:0')` in `word2vec_basic.py`).  My guess is that because `scatter_sub` is an inline operation, if there's no GPU op, TF just fails; whereas for non-inline operations, TF just knows to copy data back and forth dynamically.  So for something like word2vec_basic.py where a lot of the underlying expensive ops don't have a GPU kernel, you end up using GPU for some and CPU for others, which doesn't speed things up AFAICT.\n", "There are unit tests that are failing on the GPU configuration due to this issue: \n//tensorflow/models/embedding:word2vec_test\n//tensorflow/models/embedding:word2vec_optimized_test\n\nWhen is the ScatterSub support on GPU coming? As fpmchu pointed out, there are a few other ops such as \"NegTrain\", that are not supported on GPU either yet. Is there also plan to get the GPU support for those ops covered? Thanks.\n", "I believe ScatterSub on GPU now is implemented thanks to @fpmchu \n"]}, {"number": 304, "title": "Reduction / Term Rewriting / Optimisation / etc.", "body": "Can you add an optimisation layer which will do a global optimisation based on a pre defined set of rules  to ensure that the resulting DAG is the absolute minimised form (based on the set of rewiring rules) to transform the inputs to the the outputs or effects. Based on the domain or know ledge of the input data you can register new rules and also when adding new operation you should be able to add new rules.\n\nPerhaps this can be part of the DSL (https://github.com/tensorflow/tensorflow/issues/70)\n", "comments": ["Above can be further optimised using the effects and how they propagate and what the effects are on each node. Prima facie list of effects to tract in optimisation:\n- IO effects\n- Computational Operation Resource Consumption effects\n- Memory usage effects (stack / heap allocation, stack / heap de allocation, stack / heap re use,  etc.)\n- Volatility / scheduling / ordering effects\n- Other user defined effects\n\nSo there a balance can be achived between computation, memory, in inevitable system calls, IO operation and buffering (back pressure management), etc. in a formal well defined manner.\n", "Work on performance continues, especially in terms of optimization and balancing memory, cpu, distribution tradeoffs. This issue is extremely broad so so it is not practically actionable as a single issue. Since our roadmaps cover this issue, I am closing this for now.\n"]}, {"number": 303, "title": "Typo in variable name in Variables Getting Started Doc", "body": "Example code begins by defining the counter variable as \"var\" but later on adds to an undefined variable called \"state\"\n", "comments": ["Also, toward the end of the same example, the line `with tf.Session():` caused an error.  I got it to work by using `with tf.Session() as sess:` instead.\n", "Should be fixed now, thanks!\n"]}]