[{"number": 53738, "title": "How to make Keras Layer class to be immutable or is this a bug?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **from https://github.com/ahmedfgad/Mask-RCNN-TF2** with some other TF2 fixes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Windows Pro**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  **n/a**\r\n- TensorFlow installed from (source or binary):  **via pip install**\r\n- TensorFlow version (use command below):  **v2.7.0-rc1-69-gc256c071bb2 2.7.0**\r\n- Python version:   **v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)] on win32**\r\n- Bazel version (if compiling from source):   **n/a**\r\n- GCC/Compiler version (if compiling from source):   **n/a**\r\n- CUDA/cuDNN version:   **cuda_11.2.r11.2/compiler.29373293_0**\r\n- GPU model and memory:   **Quadro 2000 32GB**\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n**No windows version**\r\n\r\n**Describe the current behavior**\r\nThe exception in the log below is symptom not the cause.  The actual cause is the variable used in the `set()` call in method `_insert_layers()` of the class `Functional`\r\n```\r\n    # Insert layers and update other layer attrs.\r\n    layer_set = set(self._self_tracked_trackables) \r\n```\r\nThe `self._self_tracked_trackables` is a list of class instances of type `keras.engine.input_layer.InputLayer`.  Set requires the instances to be immutable and since it gives an error it must not be.  So  the question then becomes why pass something that  will not pass this requirement.   \r\n\r\nWe can find that `self._self_tracked_trackables` is initialized in the code below from method `_init_graph_network` of the same class which gets called when the model is initialized. It can be  seen that self.outputs is what drives the layers output of this `_map_graph_network` call.\r\n```\r\n    # Keep track of the network's nodes and layers.\r\n    nodes, nodes_by_depth, layers, _ = _map_graph_network(\r\n        self.inputs, self.outputs)\r\n    self._network_nodes = nodes\r\n    self._nodes_by_depth = nodes_by_depth\r\n    self._self_tracked_trackables = layers\r\n```\r\nThe model is defined as:\r\n```\r\n            model = KM.Model([input_image, input_image_meta, input_anchors],\r\n                             [detections, mrcnn_class, mrcnn_bbox,\r\n                                 mrcnn_mask, rpn_rois, rpn_class, rpn_bbox],\r\n                             name='mask_rcnn')\r\n```\r\nEach of the outputs are derived from `keras.layers.Layer' class.  So how do  we make Layer to  be non-mutable to be able to use that `set()` call in the keras system?\r\n\r\n**Describe the expected behavior**\r\nNo exception\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):  **no**\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n**Please see https://github.com/ahmedfgad/Mask-RCNN-TF2**\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nException has occurred: TypeError       (note: full exception trace is shown but execution is paused at: _run_module_as_main)\r\nunhashable type: 'ListWrapper'\r\n  File \"D:\\.venvs\\maskrcnn\\Lib\\site-packages\\tensorflow\\python\\training\\tracking\\data_structures.py\", line 672, in __hash__\r\n    raise TypeError(\"unhashable type: 'ListWrapper'\")\r\n  File \"D:\\.venvs\\maskrcnn\\Lib\\site-packages\\keras\\engine\\functional.py\", line 851, in _insert_layers\r\n    layer_set = set(self._self_tracked_trackables)\r\n  File \"D:\\.venvs\\maskrcnn\\Lib\\site-packages\\keras\\engine\\functional.py\", line 908, in _graph_network_add_loss\r\n    self._insert_layers(new_layers, new_nodes)\r\n  File \"D:\\.venvs\\maskrcnn\\Lib\\site-packages\\keras\\engine\\base_layer.py\", line 1579, in add_loss\r\n    self._graph_network_add_loss(symbolic_loss)\r\n  File \"D:\\public\\Mask-RCNN-TF2\\mrcnn\\model.py\", line 2191, in compile\r\n    self.keras_model.add_loss(loss)\r\n  File \"D:\\public\\Mask-RCNN-TF2\\mrcnn\\model.py\", line 2379, in train\r\n    self.compile(learning_rate, self.config.LEARNING_MOMENTUM)\r\n  File \"D:\\public\\Mask-RCNN-TF2\\samples\\shapes\\debug_shapes.py\", line 230, in <module>\r\n    layers='heads')\r\n  File \"C:\\Python\\python37\\Lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Python\\python37\\Lib\\runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"C:\\Python\\python37\\Lib\\runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"C:\\Python\\python37\\Lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Python\\python37\\Lib\\runpy.py\", line 193, in _run_module_as_main (Current frame)\r\n    \"__main__\", mod_spec)\r\n```", "comments": ["@quaesitor-scientiam \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "Moved it to [Keras #15897](https://github.com/keras-team/keras/issues/15897).   Not sure how to close this one.", "@quaesitor-scientiam Thank you for the update!\r\nMoving this issue to closed status as we will track this ticket [Keras #15897](https://github.com/keras-team/keras/issues/15897)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53738\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53738\">No</a>\n"]}, {"number": 53737, "title": "Fix several grammar mistakes in c_api.h", "body": "I scanned over the entirety of c_api.h and found 4 errors that should be addressed. These are all coalesced into a single pull request, and this is not a \u201cone-liner\u201d correcting one error. \r\n\r\nWhen https://github.com/tensorflow/tensorflow/pull/53717 was closed, I was unable to request that it remain open or communicate that I intended to add more commits. If possible, please assign the reviewers from that PR so that it doesn\u2019t seem like I\u2019m trying to bypass their decision.", "comments": ["Thanks"]}, {"number": 53736, "title": "[ROCm] Enable lu_op for ROCm", "body": "Enable/fix the lu_op for ROCm on AMD GPUs.\r\nGetrfBatched on ROCm uses Scalar ** vs const Scalar ** for the packed_triangular_factors_ptrs_base parameter.\r\n\r\n@cheshire @chsigg @deven-amd @stevenireeves @micmelesse ", "comments": ["I created a pr for this a while ago. https://github.com/tensorflow/tensorflow/pull/52099\r\n", "That you did! Sorry, should have rechecked. We do need that one reviewed though and it looks like you addressed the const/not-const comment there as well.\r\n\r\nClosing this one as dup"]}, {"number": 53735, "title": "Padding batches with high ranks (>=6) doesn't work", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04\r\n-   **TensorFlow installed from (source or binary)**: Nvidia docker (nvcr.io/nvidia/tensorflow:21.08-tf2-py3)\r\n-   **TensorFlow version (use command below)**: 2.5 (default installation) / 2.7 (updated with pip), doesn't work for both\r\n-   **Python version**: 3.8.10\r\n-   **CUDA/cuDNN version**: 11.4\r\n\r\n\r\n### Describe the problem\r\nPadding batches with high ranks (>=6) doesn't work and raise an `tensorflow.python.framework.errors_impl.UnimplementedError: CopyElementToLargerSlice Unhandled rank: 6` error.\r\n\r\n### Source code / logs\r\n```python3\r\n# Rank 5 is working\r\nimport tensorflow as tf\r\na = {\"x\": tf.ones([1, 1, 1, 1, 1])}\r\nb = {\"x\": tf.ones([1, 1, 1, 1, 2])}\r\nlabels = [a, b]\r\nds = tf.data.Dataset.from_generator(lambda: iter(labels), output_types={\"x\": tf.float32})\r\nds = ds.padded_batch(2, padded_shapes={\"x\": [None, None, None, None, None]})\r\nlist(ds.take(1))\r\n# [{'x': <tf.Tensor: shape=(2, 1, 1, 1, 1, 2), dtype=float32, \r\n#    numpy=array([[[[[[1., 0.]]]]], [[[[[1., 1.]]]]]], dtype=float32)>}]\r\n\r\n# Rank 6 doesn't\r\nimport tensorflow as tf\r\na = {\"x\": tf.ones([1, 1, 1, 1, 1, 1])}\r\nb = {\"x\": tf.ones([1, 1, 1, 1, 1, 2])}\r\nlabels = [a, b]\r\nds = tf.data.Dataset.from_generator(lambda: iter(labels), output_types={\"x\": tf.float32})\r\nds = ds.padded_batch(2, padded_shapes={\"x\": [None, None, None, None, None, None]})\r\nlist(ds.take(1))\r\n# Throws below error\r\n```\r\n\r\nFull error message:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 800, in __next__\r\n    return self._next_internal()\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 783, in _next_internal\r\n    ret = gen_dataset_ops.iterator_get_next(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2845, in iterator_get_next\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\", line 7107, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.UnimplementedError: CopyElementToLargerSlice Unhandled rank: 6 [Op:IteratorGetNext]\r\n```\r\n", "comments": ["@Saduf2019 ,\r\nI was able to reproduce the issue in tf v2.7,v2.5 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/a6414e3fa844c224f5de5a8f0e6a7d08/53735.ipynb).", "This is because only till Rank 5, case is handled, anything above that rank will throw the unimplemented error. \r\nIs there any specific need for Rank above 5 in general use case scenarios.", "My usecase was 3d-action-recognition, with a shape of [persons, timesteps, x, y, z, joints]\r\n\r\nThe workaround was reshaping the tensor before padding (in this case [x,y,z]->[x\\*y\\*z]) and reshaping it back after batching.", "Thanks for clarifying, is that workaround is sufficient or do you want me to create a fix for Rank till 6.", "I'm currently ok with the workaround. \r\n\r\nIs there so much logic behind the padding step? I didn't thought it was restricted to a maximal rank before.", "It's base implementation is in C++ and you can see the logic here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/batch_util.cc#L386-L395 which is having the defined cases for rank.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53735\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53735\">No</a>\n"]}, {"number": 53734, "title": "Update  tflite::gpu::Delegate  in delegate.cc", "body": "Consider the nullptr input to tflite::gpu::Delegate constructor prevent crash.", "comments": ["Could you rebase the PR ?", "Hi @SunAriesCN Can you please check @terryheo's comments and keep us posted ? Thank you!", "The same change was already merged. Thanks for your contribution."]}, {"number": 53733, "title": "Selete op: analyze tools and int8 support", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): pip\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\nHi, after using tflite in runtime, i meet serval problems and they are related to selete op, e.g. model convert, int8 supprt and reduce library size. So i open this issue. \r\nThanks for you reply.\r\n\r\n1. Is the int8 only support model without selete op.\r\nhere is the doc code, and you can see that selete op model is not support, and it fail too even i add the selete op in supported_ops.\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset\r\n<b>converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]</b>\r\n<b>converter.inference_input_type = tf.int8</b>  # or tf.uint8\r\n<b>converter.inference_output_type = tf.int8</b>  # or tf.uint8\r\ntflite_quant_model = converter.convert()\r\n```\r\n\r\n2. It there any way or tool in python i can quickly find out which keras layer depend on selete op.\r\ni find it's confusing when i want to remove the layer with selete op or replace it. Just like the tflite convert log below, i can only know\r\ntf.FusedBatchNormV3 and  tf.StridedSlice, but how to quickly know where i induce this op?\r\n```\r\nerror: illegal scale: -7.049777e+305\r\nloc(callsite(callsite(\"functional_3/mask_model_input_norm/FusedBatchNormV3@__inference__wrapped_model_7151\" at \"StatefulPartitionedCall@__inference_signature_wrapper_17427\") at \"StatefulPartitionedCall\")): error: 'tf.FusedBatchNormV3' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(\"functional_3/dprnn_block_1/dprnn_layer_2/mask_model_rnn1_norm1/FusedBatchNormV3@__inference__wrapped_model_7151\" at \"StatefulPartitionedCall@__inference_signature_wrapper_17427\") at \"StatefulPartitionedCall\")): error: 'tf.FusedBatchNormV3' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(\"functional_3/tf_op_layer_strided_slice_5/strided_slice_5@__inference__wrapped_model_7151\" at \"StatefulPartitionedCall@__inference_signature_wrapper_17427\") at \"StatefulPartitionedCall\")): error: 'tf.StridedSlice' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(\"functional_3/tf_op_layer_strided_slice_6/strided_slice_6@__inference__wrapped_model_7151\" at \"StatefulPartitionedCall@__inference_signature_wrapper_17427\") at \"StatefulPartitionedCall\")): error: 'tf.StridedSlice' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(\"functional_3/dprnn_block_1/strided_slice@__inference__wrapped_model_7151\" at \"StatefulPartitionedCall@__inference_signature_wrapper_17427\") at \"StatefulPartitionedCall\")): error: 'tf.StridedSlice' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(\"functional_3/dprnn_block_1/strided_slice_1@__inference__wrapped_model_7151\" at \"StatefulPartitionedCall@__inference_signature_wrapper_17427\") at \"StatefulPartitionedCall\")): error: 'tf.StridedSlice' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(\"functional_3/dprnn_block_1/dprnn_layer_2/mask_model_rnn1_norm2/FusedBatchNormV3@__inference__wrapped_model_7151\" at \"StatefulPartitionedCall@__inference_signature_wrapper_17427\") at \"StatefulPartitionedCall\")): error: 'tf.FusedBatchNormV3' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(\"functional_3/dprnn_block_1/dprnn_layer_3/mask_model_rnn2_norm1/FusedBatchNormV3@__inference__wrapped_model_7151\" at \"StatefulPartitionedCall@__inference_signature_wrapper_17427\") at \"StatefulPartitionedCall\")): error: 'tf.FusedBatchNormV3' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(\"functional_3/dprnn_block_1/strided_slice_2@__inference__wrapped_model_7151\" at \"StatefulPartitionedCall@__inference_signature_wrapper_17427\") at \"StatefulPartitionedCall\")): error: 'tf.StridedSlice' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(\"functional_3/dprnn_block_1/strided_slice_3@__inference__wrapped_model_7151\" at \"StatefulPartitionedCall@__inference_signature_wrapper_17427\") at \"StatefulPartitionedCall\")): error: 'tf.StridedSlice' op is neither a custom op nor a flex op\r\nloc(callsite(callsite(\"functional_3/dprnn_block_1/dprnn_layer_3/mask_model_rnn2_norm2/FusedBatchNormV3@__inference__wrapped_model_7151\" at \"StatefulPartitionedCall@__inference_signature_wrapper_17427\") at \"StatefulPartitionedCall\")): error: 'tf.FusedBatchNormV3' op is neither a custom op nor a flex op\r\nerror: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n        tf.FusedBatchNormV3 {data_format = \"NCHW\", device = \"\", epsilon = 1.000000e-03 : f32, exponential_avg_factor = 1.000000e+00 : f32, is_training = true}\r\n        tf.StridedSlice {_cloned = true, begin_mask = 0 : i64, device = \"\", ellipsis_mask = 1 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 2 : i64}\r\n        tf.StridedSlice {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 1 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 2 : i64}\r\nTraceback (most recent call last)\r\n```\r\n3. Why BatchNorm not in buildin op\r\nI may think BN is a buildin op because it is common op in NN, but the log is ```'tf.FusedBatchNormV3' op is neither a custom op nor a flex op```.\r\n", "comments": ["Hi @groovemaxRong ! Could you check these threads and try again ? Link [1](https://stackoverflow.com/a/66040013/11530462), [2](https://github.com/tensorflow/tensorflow/issues/42184), [3](https://www.tensorflow.org/lite/guide/ops_select#run_inference) . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53733\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53733\">No</a>\n"]}, {"number": 53732, "title": "print_selective_registration_header: No module named 'tensorflow.python.platform'", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n- Tensorflow version: r2.3\r\n- TensorFlow installation (pip package or built from source): source\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): source\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\nHi, i follow the tflite doc and want to reduce the library size, but after i build the print_selective_registration_header tool,  i fail to run the bin because some import error is raised, and i try two way to fix it but both fail.\r\nCan you give me some ideas, Many thanks!\r\n\r\n1. try to run print_selective_registration_header **not in tensorflow source dir**.\r\n- i check the pip version has installed and then try to run print_selective_registration_header, but it fails.\r\n```\r\nroot@3110c2e48cdb:~# python3\r\nPython 3.8.10 (default, Nov 26 2021, 20:14:08)\r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2022-01-12 16:57:13.598993: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2022-01-12 16:57:13.599037: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n>>>\r\nroot@3110c2e48cdb:~# /root/dev/tensorflow/tensorflow/bazel-bin/tensorflow/python/tools/print_selective_registration_header\r\nTraceback (most recent call last):\r\n  File \"/root/dev/tensorflow/tensorflow/bazel-bin/tensorflow/python/tools/print_selective_registration_header.runfiles/org_tensorflow/tensorflow/python/tools/print_selective_registration_header.py\", line 41, in <module>\r\n    from tensorflow.python.platform import app\r\nModuleNotFoundError: No module named 'tensorflow.python.platform'\r\n```\r\n\r\n2. try to run print_selective_registration_header **in tensorflow source dir**\r\n```\r\nroot@3110c2e48cdb:~/dev/tensorflow/tensorflow# export PYTHONPATH=`pwd`:$PYTHONPATH\r\nroot@3110c2e48cdb:~/dev/tensorflow/tensorflow# echo $PYTHONPATH\r\n/root/dev/tensorflow/tensorflow:\r\nroot@3110c2e48cdb:~/dev/tensorflow/tensorflow# ./bazel-bin/tensorflow/python/tools/print_selective_registration_header\r\nTraceback (most recent call last):\r\n  File \"/root/dev/tensorflow/tensorflow/./bazel-bin/tensorflow/python/tools/print_selective_registration_header.runfiles/org_tensorflow/tensorflow/python/tools/print_selective_registration_header.py\", line 41, in <module>\r\n    from tensorflow.python.platform import app\r\nModuleNotFoundError: No module named 'tensorflow.python.platform'\r\nroot@3110c2e48cdb:~/dev/tensorflow/tensorflow# cat .tf_configure.bazelrc\r\nbuild --action_env ANDROID_NDK_HOME=${ANDROID_NDK_HOME}/android-ndk-r${NDK_VERSION}\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild --config=xla\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n", "comments": ["@groovemaxRong We could see you are using TF v2.3.0 which is not actively supported ,please refer  this [comment](https://github.com/tensorflow/tensorflow/issues/52915#issuecomment-961288304).Could you please try to upgrade TF version to latest `TF v2.7.0` and let us know the outcome? Thanks! ", "After upgrade TF version to 2.7.0, it fail with other error.  \r\n```bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()```\r\n\r\nBuild CMD:\r\n```\r\nbazel build --config=monolithic --define=with_select_tf_ops=true \\\r\n        --cxxopt=\"-std=c++${CPP_VERSION}\" \\\r\n        -c opt \\\r\n        --define=no_tensorflow_py_deps=true \\\r\n        //tensorflow/lite:libtensorflowlite.so \\\r\n        --verbose_failures\r\n```\r\n\r\nHere is the detailed log.\r\n```\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]:\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.8/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]:\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Invalid selection: -march=native -Wno-sign-compare\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\nbuild --action_env ANDROID_NDK_HOME=${ANDROID_NDK_HOME}/android-ndk-r${NDK_VERSION}\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild:opt --copt=N\r\nbuild:opt --host_copt=N\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\n    deps = [\r\n        \":framework\",\r\n        \":tflite_exported_symbols.lds\",\r\n        \"//tensorflow/lite/delegates/flex:delegate\",\r\n        \":tflite_version_script.lds\",\r\n        \"//tensorflow/lite/kernels:builtin_ops_all_linked\",\r\n    ],\r\n)\r\n\r\ntflite_portable_test_suite()\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=186\r\nINFO: Reading rc options for 'build' from /root/dev/tensorflow/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /root/dev/tensorflow/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from /root/dev/tensorflow/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env ANDROID_NDK_HOME=${ANDROID_NDK_HOME}/android-ndk-r${NDK_VERSION} --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3\r\nINFO: Reading rc options for 'build' from /root/dev/tensorflow/tensorflow/.bazelrc:\r\n  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file /root/dev/tensorflow/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /root/dev/tensorflow/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:monolithic in file /root/dev/tensorflow/tensorflow/.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:linux in file /root/dev/tensorflow/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /root/dev/tensorflow/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/43d6991c2a4cc2ac374e68c029634f2b59ffdfdf.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1596824487 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /root/dev/tensorflow/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /root/dev/tensorflow/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  /root/.cache/bazel/_bazel_root/2c26c1884d3743801d5d0e3029f61ed7/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /root/.cache/bazel/_bazel_root/2c26c1884d3743801d5d0e3029f61ed7/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nINFO: Analyzed target //tensorflow/lite:libtensorflowlite.so (238 packages loaded, 15607 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /root/dev/tensorflow/tensorflow/tensorflow/lite/BUILD:1141:24: Linking of rule '//tensorflow/lite:libtensorflowlite.so' failed (Exit 1): gcc failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/2c26c1884d3743801d5d0e3029f61ed7/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ANDROID_NDK_HOME='${ANDROID_NDK_HOME}/android-ndk-r${NDK_VERSION}' \\\r\n    PATH=/root/dev/ndk/android-ndk-r17c:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/lite/libtensorflowlite.so-2.params)\r\nExecution platform: @local_execution_config_platform//:platform\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::GetDelegateCreators() const'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: previous definition here\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nINFO: Elapsed time: 1396.340s, Critical Path: 364.69s\r\nINFO: 6245 processes: 354 internal, 5891 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53732\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53732\">No</a>\n"]}, {"number": 53731, "title": "Flatten return null with DenseNet121", "body": "python 3.7.9 (windows 11 x64)\r\ntensorflow-gpu 2.7.0\r\n\r\n```\r\nfrom tensorflow.python.keras.applications.densenet import DenseNet121\r\nfrom tensorflow.keras.applications.resnet_v2 import ResNet50V2\r\nfrom tensorflow.keras.layers import Dense, Flatten\r\n\r\ninput_shape = (110, 110, 3)\r\nbase_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\r\n#base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\r\n\r\nx = base_model.output\r\nx = Flatten()(x)\r\n\r\n```\r\n\r\nError: _(if i use ResNet50V2 not happen)_\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"...\\Python37\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"...\\Python37\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 106, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nValueError: Exception encountered when calling layer \"flatten_1\" (type Flatten).\r\n\r\nAttempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\r\n\r\nCall arguments received:\r\n  \u2022 inputs=<KerasTensor: shape=(None, 3, 3, 1024) dtype=float32 (created by layer 'relu')>\r\n\r\n", "comments": ["@bbday ,\r\nPlease take a look at this SO [link](https://stackoverflow.com/questions/44176982/how-does-the-flatten-layer-work-in-keras) and [issue](https://github.com/keras-team/keras/issues/12870) with the similar error.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53731\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53731\">No</a>\n"]}, {"number": 53730, "title": "[TF:TRT] EfficientDet D0 pre-build TRT engine failed. TF 2.7.0, TRT 7.2.3", "body": "Ubuntu 18.04\r\nTensorflow 2.7.0\r\nCuda 11.1.1\r\nTenosrRT 7.2.3.4\r\nCuDNN 8.1.1.33\r\nCuda Compute Capability 7.5\r\nHardware: ec2 g4dn.8xlarge (Tesla Turing T4 Tensor Core)\r\n\r\nTF2 Model is from TensorFlow 2 Detection Model Zoo - [EfficientDet D0 512x512](http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz)\r\n\r\nI tried to convert saved model to TRT model and then pre-build TRT engine. The conversion worked fine but pre-build TRT engine step failed.\r\n\r\nNote: Other models such as \"Faster R-CNN ResNet50 V1 640x640\" and \"SSD MobileNet v2 320x320\" work fine. So, most probably it is not \"cudnn installation issue\" as indicated in the error message below.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n\r\nconversion_params = trt.TrtConversionParams()\r\n\r\nconverter = trt.TrtGraphConverterV2(\r\n    input_saved_model_dir=\"saved_model\",\r\n    conversion_params=conversion_params)\r\n\r\nconverter.convert()\r\n\r\nimport numpy as np\r\ndef my_input_fn():\r\n    input_shapes = [(1,512,512,3)]\r\n    for shape in input_shapes:\r\n        yield [np.zeros(shape).astype(np.uint8)]\r\n\r\nconverter.build(input_fn=my_input_fn)\r\n```\r\nError:\r\n```\r\n>>> converter.build(input_fn=my_input_fn)\r\n2022-01-12 05:55:45.849139: I tensorflow/compiler/tf2tensorrt/common/utils.cc:58] Linked TensorRT version: 7.2.3\r\n2022-01-12 05:55:45.849218: I tensorflow/compiler/tf2tensorrt/common/utils.cc:60] Loaded TensorRT version: 7.2.3\r\n2022-01-12 05:57:02.621885: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)\r\n2022-01-12 05:57:02.630204: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)\r\n2022-01-12 05:57:02.630291: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:945] TF-TRT Warning: Engine creation for StatefulPartitionedCall/EfficientDet-D0/bifpn/TRTEngineOp_0_17 failed. The native segment will be used instead. Reason: INTERNAL: Failed to build TensorRT engine\r\n2022-01-12 05:57:02.630314: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:797] TF-TRT Warning: Engine retrieval for input shapes: [[1,64,64,64], [1,32,32,64]] failed. Running native segment for StatefulPartitionedCall/EfficientDet-D0/bifpn/TRTEngineOp_0_17\r\n2022-01-12 05:57:02.644741: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8101\r\n2022-01-12 05:57:02.647927: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\r\n2022-01-12 05:57:02.778160: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)\r\n2022-01-12 05:57:02.778302: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)\r\n2022-01-12 05:57:02.778392: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:945] TF-TRT Warning: Engine creation for StatefulPartitionedCall/TRTEngineOp_0_19 failed. The native segment will be used instead. Reason: INTERNAL: Failed to build TensorRT engine\r\n2022-01-12 05:57:02.778412: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:797] TF-TRT Warning: Engine retrieval for input shapes: [[1,64,32,32], [1,64,64,64], [1,64,32,32]] failed. Running native segment for StatefulPartitionedCall/TRTEngineOp_0_19\r\n2022-01-12 05:57:02.808375: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)\r\n2022-01-12 05:57:02.808446: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)\r\n2022-01-12 05:57:02.808480: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:945] TF-TRT Warning: Engine creation for StatefulPartitionedCall/WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/TRTEngineOp_0_41 failed. The native segment will be used instead. Reason: INTERNAL: Failed to build TensorRT engine\r\n2022-01-12 05:57:02.808495: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:797] TF-TRT Warning: Engine retrieval for input shapes: [[1,64,64,64]] failed. Running native segment for StatefulPartitionedCall/WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/TRTEngineOp_0_41\r\n2022-01-12 05:57:02.809402: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)\r\n2022-01-12 05:57:02.809463: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)\r\n2022-01-12 05:57:02.809507: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:945] TF-TRT Warning: Engine creation for StatefulPartitionedCall/WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/TRTEngineOp_0_31 failed. The native segment will be used instead. Reason: INTERNAL: Failed to build TensorRT engine\r\n2022-01-12 05:57:02.809522: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:797] TF-TRT Warning: Engine retrieval for input shapes: [[1,64,64,64]] failed. Running native segment for StatefulPartitionedCall/WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/TRTEngineOp_0_31\r\n2022-01-12 05:57:02.821888: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)\r\n2022-01-12 05:57:02.821946: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 1 (Could not initialize cudnn, please check cudnn installation.)\r\n2022-01-12 05:57:02.821980: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:945] TF-TRT Warning: Engine creation for StatefulPartitionedCall/WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/TRTEngineOp_0_46 failed. The native segment will be used instead. Reason: INTERNAL: Failed to build TensorRT engine\r\n2022-01-12 05:57:02.821995: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:797] TF-TRT Warning: Engine retrieval for input shapes: [[1,64,64,64]] failed. Running native segment for StatefulPartitionedCall/WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/TRTEngineOp_0_46\r\n2022-01-12 05:57:02.823642: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)\r\n2022-01-12 05:57:02.823698: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)\r\n2022-01-12 05:57:02.825524: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)\r\n2022-01-12 05:57:02.825598: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger safeContext.cpp (124) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)\r\n2022-01-12 05:57:02.832321: E tensorflow/stream_executor/dnn.cc:764] CUDNN_STATUS_EXECUTION_FAILED\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(5553): 'cudnnPoolingForward( cudnn.handle(), pooling_desc.handle(), &alpha, src_desc.handle(), input_data.opaque(), &beta, dest_desc.handle(), output_data->opaque())'\r\n2022-01-12 05:57:02.833347: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_op.cc:542 : INTERNAL: {{function_node StatefulPartitionedCall/TRTEngineOp_0_21_native_segment}} dnn PoolForward launch failed\r\n\t [[{{node StatefulPartitionedCall/EfficientDet-D0/bifpn/node_23/3_up_lvl_5/input_3_up_lvl_4/downsample_max_x2/MaxPool}}]]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1223, in build\r\n    func(*map(ops.convert_to_tensor, inp))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1707, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/wrap_function.py\", line 249, in _call_impl\r\n    args, kwargs, cancellation_manager)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1725, in _call_impl\r\n    return self._call_with_flat_signature(args, kwargs, cancellation_manager)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1774, in _call_with_flat_signature\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1960, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 603, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) INTERNAL:  dnn PoolForward launch failed\r\n\t [[{{node StatefulPartitionedCall/EfficientDet-D0/bifpn/node_23/3_up_lvl_5/input_3_up_lvl_4/downsample_max_x2/MaxPool}}]]\r\n\t [[StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/TRTEngineOp_0_30/_168]]\r\n  (1) INTERNAL:  dnn PoolForward launch failed\r\n\t [[{{node StatefulPartitionedCall/EfficientDet-D0/bifpn/node_23/3_up_lvl_5/input_3_up_lvl_4/downsample_max_x2/MaxPool}}]]\r\n\t [[StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/non_max_suppression_with_scores_77/NonMaxSuppressionV5/_584]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_pruned_135734]\r\n\r\nFunction call stack:\r\npruned -> pruned\r\n```", "comments": ["Hi @Saduf2019 ! Could you please look at this issue?", "This can happen if you have many processes running and run out of GPU memory, leaving other libraries like CUDNN and CUBLAS no available memory for initialization. Are you running multiple builds at the same time?", "Chris, thank you for the info about possible GPU OOM issue.\r\nTo answer your questions - I run one engine build.\r\n\r\nI was able to solve the issue with TRT engine build for EfficientDet D0 model by setting memory growth config param to True\r\n```\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```\r\nI also checked used GPU memory during the engine build with `nvidia-smi pmon`. At one moment it showed `mem 99%` for 1 second.\r\n```\r\n$ nvidia-smi pmon\r\n# gpu        pid  type    sm   mem   enc   dec   command\r\n# Idx          #   C/G     %     %     %     %   name\r\n    0       8288     C    98    99     -     -   python3.7\r\n```\r\nI also checked peak memory usage after the engine was built. It showed only 4.06 GB peak usage.\r\n```\r\ntf.config.experimental.get_memory_info('GPU:0')\r\n{'current': 53,567,488, 'peak': 4,361,667,840}\r\n```\r\nI'm not sure how `99% mem` reported by nvidia-smi and only 4.06 GB peak usage reported by TF can be explained. GPU has 16GB of memory.\r\n\r\nBut anyway, the problem with TRT engine build is solved. Saved TRT model (with pre-built engine) works and it works slightly faster than the original (non-converted) TF model.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53730\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53730\">No</a>\n"]}, {"number": 53729, "title": "The shell environmental variables are disabled in some actions.run as user local GCC is applied.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6.2\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: under conda env\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: V100 32GiB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI tried to compile TF 2.6.2 from the source with the GCC 7.5.0 compiled by myself under the conda env and failed. I may get messages just like the following one:\r\n```\r\nSUBCOMMAND: # @llvm-project//mlir:SubElementInterfacesIncGen_filegroup___gen_type_interface_defs_1075127115_genrule [action 'TdGenerate external/llvm-project/mlir/include/mlir/IR/SubElementTypeInterfaces.cpp.inc', configuration: 1c2a2a243d1505e141626f6ae2bbc7d9ae7eb8e92e00bd2e48a204719dacf979, execution platform: @local_execution_config_platform//:platform]\r\n(cd /home/sanskrit007/.cache/bazel/_bazel_sanskrit007/5ea07439efe33ae8304ab053db0046f8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen -gen-type-interface-defs external/llvm-project/mlir/include/mlir/IR/SubElementInterfaces.td -I external/llvm-project/mlir/include -I bazel-out/k8-opt/bin/external/llvm-project/mlir/include -I external/llvm-project/ -I bazel-out/k8-opt/bin/external/llvm-project/ -I external/llvm-project/mlir/include/mlir/IR -I bazel-out/k8-opt/bin/external/llvm-project/mlir/include/mlir/IR -o bazel-out/k8-opt/bin/external/llvm-project/mlir/include/mlir/IR/SubElementTypeInterfaces.cpp.inc)\r\nSUBCOMMAND: # @llvm-project//mlir:SubElementInterfacesIncGen_filegroup___gen_attr_interface_decls_-1205848285_genrule [action 'TdGenerate external/llvm-project/mlir/include/mlir/IR/SubElementAttrInterfaces.h.inc', configuration: 1c2a2a243d1505e141626f6ae2bbc7d9ae7eb8e92e00bd2e48a204719dacf979, execution platform: @local_execution_config_platform//:platform]\r\n(cd /home/sanskrit007/.cache/bazel/_bazel_sanskrit007/5ea07439efe33ae8304ab053db0046f8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen -gen-attr-interface-decls external/llvm-project/mlir/include/mlir/IR/SubElementInterfaces.td -I external/llvm-project/mlir/include -I bazel-out/k8-opt/bin/external/llvm-project/mlir/include -I external/llvm-project/ -I bazel-out/k8-opt/bin/external/llvm-project/ -I external/llvm-project/mlir/include/mlir/IR -I bazel-out/k8-opt/bin/external/llvm-project/mlir/include/mlir/IR -o bazel-out/k8-opt/bin/external/llvm-project/mlir/include/mlir/IR/SubElementAttrInterfaces.h.inc)\r\nERROR: /home/sanskrit007/.cache/bazel/_bazel_sanskrit007/5ea07439efe33ae8304ab053db0046f8/external/llvm-project/mlir/BUILD:91:18: TdGenerate external/llvm-project/mlir/include/mlir/IR/SubElementTypeInterfaces.cpp.inc failed (Exit 1): mlir-tblgen failed: error executing command bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen -gen-type-interface-defs external/llvm-project/mlir/include/mlir/IR/SubElementInterfaces.td -I ... (remaining 13 argument(s) skipped)\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen)\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen)\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen)\r\nbazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\nThe complied `mlir-tblgen` cannot get the library directory of my GCC for execution.\r\nThe application of static build is no help as introducing `env BAZEL_LINKOPTS=-static-libstdc++:-static-libgcc BAZEL_LINKLIBS=-l%:libstdc++.a:-lm` that is suggested in [the issue](https://github.com/tensorflow/tensorflow/issues/43311) and [the protocol](https://gist.github.com/jakublipinski/40ba68994fe0092600a05b0060e7d445).\r\n\r\nI am not familiar with Bazel but at last, I found that the default value of `use_default_shell_env` in the `actions.run` is false. Therefore, I added `use_default_shell_env = True,` in two files to solve this problem.\r\nFile 1: third_party/mlir/tblgen.bzl\r\n```\r\n    ctx.actions.run(\r\n        outputs = [ctx.outputs.out],\r\n        inputs = trans_srcs,\r\n        executable = ctx.executable.tblgen,\r\n        arguments = [args],\r\n        use_default_shell_env = True,\r\n        mnemonic = \"TdGenerate\",  # Kythe extractor hook.\r\n    )\r\n```\r\nFile 2: tensorflow/core/kernels/mlir_generated/build_defs.bzl\r\n```\r\n    ctx.actions.run(\r\n        inputs = [ctx.file.mlir_op, ctx.file._tfso],\r\n        outputs = [gpu_bin],\r\n        executable = ctx.executable._tool,\r\n        arguments = cmd_args + [\r\n            \"--tile_sizes=%s\" % tile_sizes,\r\n            \"--max-supported-rank=%s\" % ctx.attr.max_supported_rank,\r\n            \"--arch=%s\" % arch_flag,\r\n            \"--input=%s\" % ctx.file.mlir_op.path,\r\n            \"--output=%s\" % gpu_bin.path,\r\n            \"--enable_ftz=%s\" % (ctx.attr.data_type == \"f32\"),\r\n            \"--cpu_codegen=%s\" % ctx.attr.cpu_codegen,\r\n        ],\r\n        use_default_shell_env = True,\r\n        mnemonic = \"compile\",\r\n    )\r\n```\r\nAlthough the compilation is successful and all environment variables are revealed after applying the above patches, I think maybe there are better ways to solve this problem.", "comments": ["@tirear Could you please try with the latest TF version **2.7.0** and let us know the outcome?Thanks!", "I just compiled TF 2.7.0 from the source without any patch on the same system. The entire process is very smooth! :smile: ", "@tirear Thank you for the update!\r\nGlad it worked for you.\r\nCan you please let us know if we can close this issue as it has been resolved? Thanks!", "Okay, if you also think that my solution is okay, please close this issue. Thanks!", "I just found that I also have to apply similar patches for compiling TF 2.5.2 on the same system.", "Closing this issue as it is fixed in latest version of TensorFlow. Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53729\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53729\">No</a>\n"]}, {"number": 53728, "title": "     ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 1, 4), found shape=(None, 1, 34)", "body": "```\r\n# Reshape \r\nX for model training\r\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\r\ndevX = np.reshape(devX, (devX.shape[0], 1, devX.shape[1]))\r\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\r\n# Running the LSTM model\r\nmodel = Sequential()\r\nmodel.add(LSTM(256, input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences = True))\r\nmodel.add(LSTM(256))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(look_back))\r\nmodel.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\r\nhistory = model.fit(trainX, trainY, epochs=200, batch_size=20, validation_data=(devX, devY), verbose=0, shuffle=True)\r\n```\r\n\r\nValueError                                Traceback (most recent call last)\r\n~\\AppData\\Local\\Temp/ipykernel_33336/3205999125.py in <module>\r\n     10 model.add(Dense(look_back))\r\n     11 model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\r\n---> 12 history = model.fit(trainX, trainY, epochs=200, batch_size=20, validation_data=(devX, devY), verbose=0, shuffle=True)\r\n     13 \r\n     14 \r\n\r\n~\\trade-env\\lib\\site-packages\\keras\\utils\\traceback_utils.py in error_handler(*args, **kwargs)\r\n     65     except Exception as e:  # pylint: disable=broad-except\r\n     66       filtered_tb = _process_traceback_frames(e.__traceback__)\r\n---> 67       raise e.with_traceback(filtered_tb) from None\r\n     68     finally:\r\n     69       del filtered_tb\r\n\r\n~\\trade-env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py in autograph_handler(*args, **kwargs)\r\n   1127           except Exception as e:  # pylint:disable=broad-except\r\n   1128             if hasattr(e, \"ag_error_metadata\"):\r\n-> 1129               raise e.ag_error_metadata.to_exception(e)\r\n   1130             else:\r\n   1131               raise\r\n\r\nValueError: in user code:\r\n\r\n    File \"C:\\Users\\mforeman\\trade-env\\lib\\site-packages\\keras\\engine\\training.py\", line 1366, in test_function  *\r\n        return step_function(self, iterator)\r\n    File \"C:\\Users\\mforeman\\trade-env\\lib\\site-packages\\keras\\engine\\training.py\", line 1356, in step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File \"C:\\Users\\mforeman\\trade-env\\lib\\site-packages\\keras\\engine\\training.py\", line 1349, in run_step  **\r\n        outputs = model.test_step(data)\r\n    File \"C:\\Users\\mforeman\\trade-env\\lib\\site-packages\\keras\\engine\\training.py\", line 1303, in test_step\r\n        y_pred = self(x, training=False)\r\n    File \"C:\\Users\\mforeman\\trade-env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\r\n        raise e.with_traceback(filtered_tb) from None\r\n    File \"C:\\Users\\mforeman\\trade-env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\r\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\r\n\r\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 1, 4), found shape=(None, 1, 34)\r\n\r\n\r\nI can't figure out what is causing the error.  I check the shape of the inputs and it seems to be correct.\r\n\r\n\r\n", "comments": ["@Mrforeman1 ,\r\n In order to expedite the trouble-shooting process, could you please provide a complete code  and the TensorFlow version you are using.\r\n", "Also please take a look at this SO [link](https://stackoverflow.com/questions/63760734/valueerror-input-0-of-layer-sequential-is-incompatible-with-the-layer-expect) and issue [1](https://github.com/tensorflow/tensorflow/issues/50099) and [2](https://github.com/tensorflow/tensorflow/issues/52156) with the similar error.It helps.Thanks!", "I am using tensorflow version 2.7\r\n```\r\n\r\n#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\n# In[3]:\r\n\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pandas_ta as ta\r\n\r\nimport statsmodels\r\nfrom statsmodels.tsa.stattools import coint\r\nimport statsmodels.api as sm\r\nfrom statsmodels.tsa.arima_model import ARIMA\r\n\r\nfrom statsmodels.tsa.vector_ar.var_model import VAR\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom pykalman import KalmanFilter\r\nnp.random.seed(107)\r\nimport pywt\r\n\r\nimport sys\r\n# sys.path.append('/home/ubuntu/auquantoolbox')\r\n# from backtester.dataSource.yahoo_data_source import YahooStockDataSource\r\nfrom datetime import datetime\r\n\r\nfrom math import sqrt\r\nfrom numpy import concatenate\r\nfrom matplotlib import pyplot\r\nfrom datetime import datetime\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom sklearn.preprocessing import LabelEncoder\r\nfrom sklearn.metrics import mean_squared_error\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.layers import LSTM\r\nfrom keras.layers import Dropout\r\nimport plotly.offline as py\r\nimport plotly.graph_objs as go\r\nimport seaborn as sns\r\npy.init_notebook_mode(connected=True)\r\nget_ipython().run_line_magic('matplotlib', 'inline')\r\n\r\n\r\n# # DOWNLOAD DATA FROM YAHOO (OLD)\r\n\r\n# In[4]:\r\n\r\n\r\n# startDateStr = '2008/10/01'\r\n# endDateStr = '2018/10/01'\r\n# cachedFolderName = 'yahooData/'\r\n# dataSetId = 'testPairsTrading'\r\n# instrumentIds = list(set(['ITOT', 'ACWI', 'IWV', 'VT', 'VTI',\r\n#                  'DIA', 'RSP', 'IOO', 'IVV', 'SPY',\r\n#                  'SHE', 'IWM', 'OEF', 'QQQ',\r\n#                  'CVY', 'RPG', 'RPV', 'IWB', 'IWF', \r\n#                  'IWD', 'IVW', 'IVE', 'PKW', \r\n#                  'PRF', 'SDY', 'VV', 'VUG', \r\n#                  'VTV', 'MGC', 'MGK', 'MGV', 'VIG', \r\n#                  'VYM', 'DTN', 'DLN', 'MDY', 'DVY', \r\n#                  'IWR', 'IWP', 'IWS', 'IJH', 'IJK', \r\n#                  'IJJ', 'PDP', 'DON', 'IWC', 'IWM', \r\n#                  'IWO', 'IWN', 'IJR', 'IJT', 'IJS', \r\n#                  'EEB', 'IDV', 'ACWX', 'BKF', 'EFA', \r\n#                  'EFG', 'EFV', 'SCZ', 'EEM', 'PID', \r\n#                  'DWX', 'DEM', 'DGS', 'AAXJ', 'EZU', \r\n#                  'EPP', 'IEV', 'ILF', 'FEZ', 'VGK', \r\n#                  'VPL', 'DFE', 'EWA', 'EWC', 'EWG', \r\n#                  'EWI', 'EWJ', 'EWD', 'EWL', 'EWP', \r\n#                  'EWU', 'DXJ', 'EWZ', 'FXI', 'EWH', \r\n#                  'EWW', 'RSX', 'EWS', 'EWM','EWY', \r\n#                  'EWT', 'EPI', 'XLY', 'IYC', 'ITB', \r\n#                  'XHB', 'VCR','XLP', 'IYK', 'VDC', \r\n#                  'XLE', 'IYE', 'IGE', 'OIH', 'XOP', \r\n#                  'VDE', 'QCLN', 'XLF','IYF', 'KBE', \r\n#                  'KRE', 'VFH']))\r\n# ds = YahooStockDataSource(cachedFolderName=cachedFolderName,\r\n#                             dataSetId=dataSetId,\r\n#                             instrumentIds=instrumentIds,\r\n#                             startDateStr=startDateStr,\r\n#                             endDateStr=endDateStr,\r\n#                             event='history')\r\n# data_close = ds.getBookDataByFeature()['close']\r\n# data_open = ds.getBookDataByFeature()['open']\r\n# data_high = ds.getBookDataByFeature()['high']\r\n# data_low = ds.getBookDataByFeature()['low']\r\n# data_vol = ds.getBookDataByFeature()['volume'] \r\n\r\n\r\n# # DOWNLOAD DATA FROM YAHOO (NEW)\r\n\r\n# In[5]:\r\n\r\n\r\n# Function to download data from Yahoo.\r\n# Can also be used outside the notebook.\r\n# intputs are getYahoo(x,y,z)\r\n# Where:\r\n# x, is a list of strings. ex: x=['SPY','LCID','VGT']\r\n# y, is the start date in format YYYY-MM-DD. ex: 2021-12-31\r\n# z, is the end date in format YYYY-MM-DD. ex: 2022-01-08\r\n# output is a dictionary containing 6 Pandas DataFrames.\r\n# (Open, High, Low, Close, Adj Close, Volume)\r\ndef getYahoo(tickers, start_date, end_date, frequency='1d'):\r\n    import io\r\n    import re\r\n    import requests\r\n    import pandas as pd\r\n    import datetime\r\n    import time\r\n    import numpy as np\r\n    unix_start = str(int(time.mktime(datetime.date(int(start_date[0:4]), int(start_date[5:7]), int(start_date[8:10])).timetuple())))\r\n    unix_end = str(int(time.mktime(datetime.date(int(end_date[0:4]), int(end_date[5:7]), int(end_date[8:10])).timetuple())))\r\n    OHLC = {}\r\n    cookie = ''\r\n    crumb = ''\r\n    s = requests.Session()\r\n    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\r\n    res = s.get('https://finance.yahoo.com/quote/SPY/history', headers=headers)\r\n    cookie = res.cookies['B']\r\n    pattern = re.compile('.*\"CrumbStore\":\\{\"crumb\":\"(?P<crumb>[^\"]+)\"\\}')\r\n    for line in res.text.splitlines():\r\n        m = pattern.match(line)\r\n        if m is not None:\r\n            crumb = m.groupdict()['crumb']\r\n    g_tickers = []\r\n    for x in range(0,len(tickers)):\r\n        ticker = tickers[x] \r\n        url_str = \"https://query1.finance.yahoo.com/v7/finance/download/%s\"\r\n        url_str += \"?period1=%s&period2=%s&interval=%s&events=history&crumb=%s\"\r\n        url = url_str % (ticker, unix_start, unix_end, frequency, crumb)\r\n        try:\r\n            res = s.get(url, headers=headers,cookies={'B': cookie}).text\r\n            OHLC[ticker] = pd.read_csv(io.StringIO(res), index_col=0, error_bad_lines=False,warn_bad_lines=False).replace('null', np.nan).dropna()\r\n            OHLC[ticker].index = pd.to_datetime(OHLC[ticker].index)\r\n            OHLC[ticker] = OHLC[ticker].apply(pd.to_numeric)\r\n            current = x+1\r\n            total = len(tickers)\r\n            pos = str(current)+'/'+str(total)\r\n            g_tickers.append(tickers[x])\r\n            print(pos+' - '+ticker+ \" - Done\")\r\n        except:\r\n            current = x+1\r\n            total = len(tickers)\r\n            pos = str(current)+'/'+str(total)\r\n            print(pos+' - '+ticker+ \" - Error\")\r\n            \r\n    dict_open = {}\r\n    dict_high = {}\r\n    dict_low = {}\r\n    dict_close = {}\r\n    dict_Adjclose = {}\r\n    dict_volume = {}\r\n\r\n    for i in g_tickers:\r\n        try:\r\n            dict_open[i] = OHLC[i][\"Open\"]\r\n            dict_high[i] = OHLC[i][\"High\"]\r\n            dict_low[i] = OHLC[i][\"Low\"]\r\n            dict_close[i] = OHLC[i][\"Close\"]\r\n            dict_Adjclose[i] = OHLC[i][\"Adj Close\"]\r\n            dict_volume[i] = OHLC[i][\"Volume\"]\r\n        except:\r\n            None\r\n            \r\n    Open = pd.DataFrame(dict_open)\r\n    High = pd.DataFrame(dict_high)\r\n    Low = pd.DataFrame(dict_low)\r\n    Close = pd.DataFrame(dict_close)\r\n    Adjclose = pd.DataFrame(dict_Adjclose)\r\n    Volume = pd.DataFrame(dict_volume)\r\n    return {\"Open\":Open,\"High\":High,\"Low\":Low,\"Close\":Close,\"Adj Close\":Adjclose, \"Volume\":Volume}\r\n\r\n\r\n# In[6]:\r\n\r\n\r\nstartDateStr = '2019-01-01'\r\nendDateStr = '2022-01-01'\r\ninstrumentIds = list(set(['KLAC', 'BC','VGT']))\r\n\r\ndata = getYahoo(instrumentIds,startDateStr,endDateStr)\r\ndata_close = data['Close'].dropna()\r\ndata_open = data['Open'].dropna()\r\ndata_high = data['High'].dropna()\r\ndata_low = data['Low'].dropna()\r\ndata_vol = data['Volume'].dropna()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n\r\n\r\n\r\n# # DATA ANALYTICS\r\n\r\n# In[7]:\r\n\r\n\r\nprint(data_close.shape)\r\ndata_high.iloc[:10,:10]\r\n\r\n\r\n# In[8]:\r\n\r\n\r\na = pd.DataFrame({'Close':data_close.iloc[:3,1],\r\n             'Open':data_open.iloc[:3,1],\r\n                 'High':data_high.iloc[:3,1],\r\n                 'Low':data_low.iloc[:3,1],\r\n                 'Volume':data_vol.iloc[:3,1]})\r\na\r\n\r\n\r\n# In[9]:\r\n\r\n\r\n# Checking for any null values in data\r\nd = data_close.isnull().any()\r\nprint(d[d == True])\r\n\r\n\r\n# In[10]:\r\n\r\n\r\n# Create a plot of the time series to visualize the data\r\nfor i in range(data_close.shape[1]):\r\n    S = data_close.iloc[:, i]\r\n    S.plot(figsize=(15,7))\r\n\r\n\r\n# # Generate Co-Integrated Pairs\r\n\r\n# In[11]:\r\n\r\n\r\ndef find_cointegrated_pairs(data):\r\n    n = data.shape[1]\r\n    score_matrix = np.zeros((n, n))\r\n    pvalue_matrix = np.ones((n, n))\r\n    keys = data.keys()\r\n    pairs = {}\r\n    for i in range(n):\r\n        for j in range(i+1, n):\r\n            S1 = data[keys[i]]\r\n            S2 = data[keys[j]]\r\n            result = coint(S1, S2)\r\n            score = result[0]\r\n            pvalue = result[1]\r\n            score_matrix[i, j] = score\r\n            pvalue_matrix[i, j] = pvalue\r\n            if pvalue < 0.05:\r\n                pairs[(keys[i], keys[j])] = result\r\n    return score_matrix, pvalue_matrix, pairs\r\n\r\nscores, pvalues, pairs = find_cointegrated_pairs(data_close)\r\n\r\n\r\n# In[12]:\r\n\r\n\r\nscores\r\n\r\n\r\n# In[13]:\r\n\r\n\r\npvalues\r\n\r\n\r\n# In[14]:\r\n\r\n\r\npairs\r\n\r\n\r\n# In[15]:\r\n\r\n\r\n# Heatmap to show the p-values of the cointegration test\r\n# between each pair of stocks\r\n#m = [0,0.2,0.4,0.6,0.8,1]\r\nsns.heatmap(pvalues,  cmap='RdYlGn_r' \r\n                , mask = (pvalues >= 0.98)\r\n                )\r\npyplot.show()\r\n\r\n\r\n# In[16]:\r\n\r\n\r\npairs_data = {key:value[1]  for (key, value) in pairs.items()}\r\npairs_data = sorted(pairs_data.items(), key=lambda x: x[1])\r\npairs_data[:10]\r\n\r\n\r\n# In[17]:\r\n\r\n\r\npair_data = pd.DataFrame({'S1_close':data_close['KLAC'],'S2_close':data_close['BC']\r\n                         ,'S1_open':data_open['KLAC'],'S2_open':data_open['BC']\r\n                         ,'S1_high':data_high['KLAC'],'S2_high':data_high['BC']\r\n                         ,'S1_low':data_low['KLAC'],'S2_low':data_low['BC']\r\n                         ,'S1_volume':data_vol['KLAC'],'S2_volume':data_vol['BC']})\r\npair_data['S1_close'].plot(figsize=(15,7))\r\npair_data['S2_close'].plot(figsize=(15,7))\r\n\r\n\r\n# ## Adding Technical Indictors\r\n\r\n# In[18]:\r\n\r\n\r\n# add technical indicators\r\n# 1. Momentum Indicators\r\n# Relative Strength Index\r\npair_data['S1_rsi'] = ta.rsi(pair_data['S1_close'], length=14)\r\npair_data['S2_rsi'] = ta.rsi(pair_data['S2_close'], length=14)\r\n#Money Flow Index\r\npair_data['S1_mfi'] = ta.mfi(pair_data['S1_high'], pair_data['S1_low'], \r\n                                                  pair_data['S1_close'], pair_data['S1_volume'], length=14)\r\npair_data['S2_mfi'] = ta.mfi(pair_data['S2_high'], pair_data['S2_low'], \r\n                                                 pair_data['S2_close'], pair_data['S2_volume'], length=14)\r\n\r\n# 2. Volume Indicators\r\n# Accumulation/Distribution Index (ADI)\r\npair_data['S1_adi'] = ta.ad(pair_data['S1_high'], pair_data['S1_low'], pair_data['S1_close'], pair_data['S1_volume'])\r\npair_data['S2_adi'] = ta.ad(pair_data['S2_high'], pair_data['S2_low'], pair_data['S2_close'], pair_data['S2_volume'])\r\n# Volume-price trend (VPT)\r\npair_data['S1_vpt'] = ta.pvt(pair_data['S1_close'], pair_data['S1_volume'])\r\npair_data['S2_vpt'] = ta.pvt(pair_data['S2_close'], pair_data['S2_volume'])\r\n\r\n# 3. Volatility Indicators\r\n# Average True Range (ATR)\r\npair_data['S1_atr'] = ta.atr(pair_data['S1_high'], pair_data['S1_low'], \r\n                                                       pair_data['S1_close'], length=14)\r\npair_data['S2_atr'] = ta.atr(pair_data['S2_high'], pair_data['S2_low'], \r\n                                                       pair_data['S2_close'], length=14)\r\n# Bollinger Bands (BB) N-period simple moving average (MA)\r\npair_data['S2_bb_ma'] = ta.bbands(pair_data['S2_close'], length=20)['BBL_20_2.0']\r\npair_data['S1_bb_ma'] = ta.bbands(pair_data['S1_close'], length=20)['BBL_20_2.0']\r\n\r\n# 4. Trend Indicators\r\n# Average Directlstm_pair_data = pair_data;ional Movement Index (ADX)\r\npair_data['S1_adx'] = ta.adx(pair_data['S1_high'], pair_data['S1_low'], pair_data['S1_close'], length=14)['DMP_14']\r\npair_data['S2_adx'] = ta.adx(pair_data['S2_high'], pair_data['S2_low'], pair_data['S2_close'], length=14)['DMP_14']\r\n# Exponential Moving Average\r\npair_data['S1_ema'] = ta.ema(pair_data['S1_close'], length=14)\r\npair_data['S2_ema'] = ta.ema(pair_data['S2_close'], length=14)\r\n# Moving Average Convergence Divergence (MACD)\r\npair_data['S1_macd'] = ta.macd(pair_data['S1_close'], fast=14,  slow=30, Signal = 9)['MACD_14_30_9']\r\npair_data['S2_macd'] = ta.macd(pair_data['S2_close'], fast=14,  slow=30, Signal = 9)['MACD_14_30_9']\r\n# 5. Other Indicators\r\n# Daily Log Return (DLR)\r\npair_data['S1_dlr'] = ta.log_return(pair_data['S1_close'])\r\npair_data['S2_dlr'] = ta.log_return(pair_data['S2_close'])\r\npd.set_option('display.max_rows', None)\r\npd.set_option('display.max_columns', None)\r\npd.set_option('display.width', None)\r\npd.set_option('display.max_colwidth', -1)\r\n\r\n\r\n# ## Spreads for Ground Truth\r\n\r\n# In[19]:\r\n\r\n\r\nest = sm.OLS(pair_data.S1_close, pair_data.S2_close)\r\nest = est.fit()\r\nalpha = -est.params[0]\r\npair_data['Spread_Close'] = pair_data.S1_close + (pair_data.S2_close * alpha)\r\npair_data['Spread_Close'].plot(figsize=(15,7))\r\npyplot.axhline(pair_data['Spread_Close'].mean())\r\npyplot.legend(['Spread_Close'])\r\n\r\n\r\n# In[20]:\r\n\r\n\r\nest_op = sm.OLS(pair_data.S1_open, pair_data.S2_open)\r\nest_op = est_op.fit()\r\nalpha_op = -est_op.params[0]\r\npair_data['Spread_Open'] = pair_data.S1_open + (pair_data.S2_open * alpha_op)\r\nest_hi = sm.OLS(pair_data.S1_high, pair_data.S2_high)\r\nest_hi = est_hi.fit()\r\nalpha_hi = -est_hi.params[0]\r\npair_data['Spread_High'] = pair_data.S1_high + (pair_data.S2_high * alpha_hi)\r\nest_lo = sm.OLS(pair_data.S1_low, pair_data.S2_low)\r\nest_lo = est_lo.fit()\r\nalpha_lo = -est_lo.params[0]\r\npair_data['Spread_Low'] = pair_data.S1_low + (pair_data.S2_low * alpha_lo)\r\n\r\n\r\n# ## Wavelet Denoising\r\n\r\n# In[21]:\r\n\r\n\r\ndef wav_den(ts_orig):\r\n    (ca, cd) = pywt.dwt(ts_orig, 'db8')\r\n    cat = pywt.threshold(ca, np.std(ca)/8, mode='soft')\r\n    cdt = pywt.threshold(cd, np.std(cd)/8, mode='soft')\r\n    ts_rec = pywt.idwt(cat, cdt, 'db8')\r\n    return ts_rec[1:]\r\n\r\n\r\n# In[22]:\r\n\r\n\r\n#Use MinMaxScaler to normalize Weighted Price to range from 0 to 1\r\ncols = ['Spread_Close', 'Spread_Open', 'Spread_High', 'Spread_Low'#, 'S1_volume',\r\n                                        #  'S2_volume','S1_rsi', 'S2_rsi', 'S1_mfi', 'S2_mfi',\r\n       #'S1_adi', 'S2_adi',\r\n                                         #  'S1_vpt', 'S2_vpt', 'S1_atr', 'S2_atr', 'S1_bb_ma', 'S2_bb_ma', 'S1_adx',\r\n                                         # 'S2_adx', 'S1_ema', 'S2_ema', 'S1_macd', 'S2_macd', 'S1_dlr', 'S2_dlr'\r\n       ]\r\nlstm_pair_data = pair_data;\r\nlsmt_pair_data = pair_data;\r\nlstm_pair_data = pd.DataFrame({'Spread_Close':pair_data['Spread_Close'][30:],\r\n                               'Spread_Open':pair_data['Spread_Open'][30:],\r\n                               'Spread_High':pair_data['Spread_High'][30:],\r\n                               'Spread_Low':pair_data['Spread_Low'][30:],\r\n#                               'S1_volume':pair_data['S1_volume'][30:],\r\n#                               'S2_volume':pair_data['S2_volume'][30:],\r\n#                                'S1_rsi':pair_data['S1_rsi'][30:],\r\n#                                'S2_rsi':pair_data['S2_rsi'][30:],\r\n#                                'S1_mfi':pair_data['S1_mfi'][30:],\r\n#                                'S2_mfi':pair_data['S2_mfi'][30:],\r\n#                                'S1_adi':pair_data['S1_adi'][30:],\r\n#                                'S2_adi':pair_data['S2_adi'][30:],\r\n#                                'S1_vpt':pair_data['S1_vpt'][30:],\r\n#                                'S2_vpt':pair_data['S2_vpt'][30:],\r\n#                                'S1_atr':pair_data['S1_atr'][30:],\r\n#                                'S2_atr':pair_data['S2_atr'][30:],\r\n#                                'S1_bb_ma':pair_data['S1_bb_ma'][30:],\r\n#                                'S2_bb_ma':pair_data['S2_bb_ma'][30:],\r\n#                                'S1_adx':pair_data['S1_adx'][30:],\r\n#                                'S2_adx':pair_data['S2_adx'][30:],\r\n#                                'S1_ema':pair_data['S1_ema'][30:],\r\n#                                'S2_ema':pair_data['S2_ema'][30:],\r\n#                                'S1_macd':pair_data['S1_macd'][30:],\r\n#                               'S2_macd':pair_data['S2_macd'][30:],\r\n#                                'S1_dlr':pair_data['S1_dlr'][30:],\r\n#                                'S2_dlr':pair_data['S2_dlr'][30:]\r\n                              }, columns = cols)\r\n\r\n\r\n# In[23]:\r\n\r\n\r\n\r\ntrain_size = int(len(lsmt_pair_data) * 0.9)\r\ndev_size = int((len(lsmt_pair_data) - train_size) * 0.5) - 30\r\ntest_size = len(lsmt_pair_data) - train_size - dev_size\r\ntrain, dev, test = lsmt_pair_data[0:train_size], lsmt_pair_data[train_size:train_size + dev_size], lsmt_pair_data[train_size + dev_size:len(lsmt_pair_data)]\r\nprint(len(train), len(dev), len(test))\r\n\r\n\r\n# In[24]:\r\n\r\n\r\ntrain_den = pd.DataFrame(columns = cols)\r\nfor col in cols:\r\n    train_den[col] = wav_den(train[col])\r\n\r\n\r\n# ## Prediction Period Setting\r\n\r\n# In[25]:\r\n\r\n\r\nlook_back = 1\r\n\r\n\r\n# In[26]:\r\n\r\n\r\nscaler = MinMaxScaler(feature_range=(0, 1))\r\n# Create function for creating dataset with look back\r\ndef create_dataset(dataset, look_back=1):\r\n    dataX, dataY = [], []\r\n    for i in range(len(dataset) - look_back):\r\n        a = dataset[i, :]\r\n        dataX.append(a)\r\n        dataY.append(dataset[(i+1):(i+1+look_back), 0])\r\n    print(len(dataY))\r\n    return dataX, np.array(scaler.fit_transform(dataX)), dataY, np.array(scaler.fit_transform(dataY))\r\n\r\n# Generate dataset for trainX, trainY, testX, testY\r\ntrainX_untr, trainX, trainY_untr, trainY = create_dataset(train_den.values, look_back)\r\ndevX_untr, devX, devY_untr, devY = create_dataset(dev.values, look_back)\r\ntestX_untr, testX, testY_untr, testY = create_dataset(test.values, look_back)\r\n\r\n\r\n# ## ACCURACY METRIC\r\n\r\n# In[27]:\r\n\r\n\r\ndef acc_metric(true_value, predicted_value):\r\n    acc_met = 0.0\r\n    m = len(true_value)\r\n    for i in range(m):\r\n        acc_met += mean_squared_error(true_value[i], predicted_value[i])\r\n    acc_met /= m\r\n    return np.sqrt(acc_met)\r\n\r\n\r\n# ## Spread Prediction using Kalman Filter\r\n\r\n# In[28]:\r\n\r\n\r\ndef KalmanFilterAverage(x):\r\n  # Construct a Kalman filter\r\n    kf = KalmanFilter(transition_matrices = [1],\r\n    observation_matrices = [1],\r\n    initial_state_mean = 0,\r\n    initial_state_covariance = 1,\r\n    observation_covariance=1,\r\n    transition_covariance=.01)\r\n \r\n  # Use the observed values of the price to get a rolling mean\r\n    state_means, _ = kf.filter(x.values)\r\n    state_means = pd.Series(state_means.flatten(), index=x.index)\r\n    return state_means\r\n \r\n# Kalman filter regression\r\ndef KalmanFilterRegression(x,y):\r\n    delta = 1e-3\r\n    trans_cov = delta / (1 - delta) * np.eye(2) # How much random walk wiggles\r\n    obs_mat = np.expand_dims(np.vstack([[x], [np.ones(len(x))]]).T, axis=1)\r\n \r\n    kf = KalmanFilter(n_dim_obs=1, n_dim_state=2, # y is 1-dimensional, (alpha, beta) is 2-dimensional\r\n    initial_state_mean=[0,0],\r\n    initial_state_covariance=np.ones((2, 2)),\r\n    transition_matrices=np.eye(2),\r\n    observation_matrices=obs_mat,\r\n    observation_covariance=2,\r\n    transition_covariance=trans_cov)\r\n \r\n    # Use the observations y to get running estimates and errors for the state parameters\r\n    state_means, state_covs = kf.filter(y.values)\r\n    return state_means\r\n\r\n\r\n# In[29]:\r\n\r\n\r\ndef normalize(series):\r\n    return (series - np.mean(series)) / np.std(series)\r\n\r\n\r\n# In[30]:\r\n\r\n\r\nstate_means = - KalmanFilterRegression(KalmanFilterAverage(pair_data.S1_close),KalmanFilterAverage(pair_data.S2_close))[:,0]\r\nresults = normalize(pair_data.S1_close + (pair_data.S2_close * state_means))\r\nforecast = results[-len(testX):].values\r\nyhat_KF = forecast\r\nyhat_KF_mse = []\r\nmse = 0.0\r\nif look_back == 1:\r\n    for i in range(len(forecast)):\r\n        temp = []\r\n        temp.append(forecast[i])\r\n        yhat_KF_mse.append(np.array(temp))\r\n    mse = acc_metric(normalize(testY_untr), yhat_KF_mse)\r\nelse:\r\n    mse = 0.0\r\nmse\r\n\r\n\r\n# In[31]:\r\n\r\n\r\nnormalize(pair_data['Spread_Close']).plot(figsize=(15,7))\r\nresults.plot(figsize=(15,7))\r\npyplot.axhline(normalize(pair_data['Spread_Close']).mean(), color='black')\r\npyplot.axhline(1.0, color='red', linestyle='--')\r\npyplot.axhline(-1.0, color='green', linestyle='--')\r\npyplot.legend(['Spread z-score', 'Kalman_Predicted_Spread', 'Mean', '+1', '-1'])\r\npyplot.show()\r\n\r\n\r\n# ## Spread Prediction using ARIMA\r\n\r\n# In[32]:\r\n\r\n\r\nfrom statsmodels.tsa.arima.model import ARIMA\r\n### there some issues with this library\r\n\r\n\r\n# In[33]:\r\n\r\n\r\nyhat_ARIMA = []\r\nyhat_ARIMA_mse = []\r\n#data = lstm_pair_data['Spread_Close'].values\r\n# wrong dataframe assigned\r\nlstm_pair_data = pair_data ### added this line\r\ndata = lstm_pair_data['Spread_Close'].values\r\nfor i in range(train_size+dev_size, len(lstm_pair_data)-1):\r\n    model = ARIMA(data[:i], order=(1,0,0))\r\n    model_fit = model.fit()\r\n    temp = []\r\n    forecast = model_fit.forecast(steps=look_back)\r\n    yhat_ARIMA.append(forecast[0])\r\n    for j in range(len(forecast)):\r\n        temp.append(forecast[j])\r\n    yhat_ARIMA_mse.append(np.array(temp))\r\nmse = 0.0\r\nmse = acc_metric(testY_untr, yhat_ARIMA_mse)\r\nmse\r\n\r\n\r\n# ## Spread Prediction using Vector Auto-Regression Model\r\n\r\n# In[ ]:\r\n\r\n\r\n\r\n\r\n\r\n# In[34]:\r\n\r\n\r\n\r\nyhat_VAR_mse = []\r\nyhat_VAR = []\r\ndata = lstm_pair_data.values\r\nfor i in range(train_size+dev_size, len(lstm_pair_data)-1):\r\n    model = VAR(data[:i,:])\r\n    model_fit = model.fit()\r\n    temp = []\r\n    forecast = model_fit.forecast(model_fit.y, steps=look_back)[:,0]\r\n    yhat_VAR.append(forecast[0])\r\n    for j in range(len(forecast)):\r\n        temp.append(forecast[j])\r\n    yhat_VAR_mse.append(np.array(temp))\r\nmse = 0.0\r\nmse = acc_metric(testY_untr, yhat_VAR_mse)\r\nmse\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n\r\n\r\n\r\n# ## Spread Prediction using Facebook Prophet\r\n\r\n# In[35]:\r\n\r\n\r\n#yhat_FbProphet = []\r\n#yhat_FbProphet_mse = []\r\n#cols = ['ds', 'y']\r\n#fbData = pd.DataFrame({'ds':lstm_pair_data.index,'y':lstm_pair_data.Spread_Close}, columns = cols)\r\n#for i in range(train_size+dev_size+1, len(lstm_pair_data)):\r\n#    model = Prophet(daily_seasonality=True)\r\n#    model.fit(fbData[:i])\r\n#    temp = []\r\n#    future = model.make_future_dataframe(periods=look_back)\r\n#    forecast = (model.predict(future)).yhat[i:].values\r\n#    yhat_FbProphet.append(forecast[0])\r\n#    for j in range(len(forecast)):\r\n#        temp.append(forecast[j])\r\n#    yhat_FbProphet_mse.append(np.array(temp))\r\n#mse = 0.0\r\n#mse = acc_metric(testY_untr, yhat_FbProphet_mse)\r\n#mse\r\n\r\n\r\n# In[36]:\r\n\r\n\r\n# Reshape X for model training\r\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\r\ndevX = np.reshape(devX, (devX.shape[0], 1, devX.shape[1]))\r\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\r\n# Running the LSTM model\r\nmodel = Sequential()\r\nmodel.add(LSTM(256, input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences = True))\r\nmodel.add(LSTM(256))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(look_back))\r\nmodel.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\r\nhistory = model.fit(trainX, trainY, epochs=200, batch_size=20, validation_data=(devX, devY), verbose=0, shuffle=True)\r\n\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nmodel.summary()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Plot line graph to show amount loss according the the epoch\r\npyplot.plot(history.history['loss'], label='train')\r\npyplot.plot(history.history['val_loss'], label='dev')\r\npyplot.legend()\r\npyplot.show()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Make prediction using textX and plotting line graph against testY\r\nyhat = model.predict(testX)\r\npyplot.plot(testY, label='True Value')\r\npyplot.plot(yhat, label='Predicted Value')\r\npyplot.legend()\r\npyplot.show()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\na=pyplot.figure(1)\r\npyplot.plot(scaler.inverse_transform(testY)[35:36][0], label='True Value')\r\npyplot.plot(scaler.inverse_transform(yhat)[35:36][0], label='Predicted Value')\r\npyplot.legend()\r\npyplot.ylabel('Spread')\r\npyplot.xlabel('Ahead Time Steps')\r\npyplot.show()\r\nb=pyplot.figure(1)\r\npyplot.plot(scaler.inverse_transform(testY)[75:76][0], label='True Value')\r\npyplot.plot(scaler.inverse_transform(yhat)[75:76][0], label='Predicted Value')\r\npyplot.legend()\r\npyplot.ylabel('Spread')\r\npyplot.xlabel('Ahead Time Steps')\r\npyplot.show()\r\nc=pyplot.figure(1)\r\npyplot.plot(scaler.inverse_transform(testY)[103:104][0], label='True Value')\r\npyplot.plot(scaler.inverse_transform(yhat)[103:104][0], label='Predicted Value')\r\npyplot.legend()\r\npyplot.ylabel('Spread')\r\npyplot.xlabel('Ahead Time Steps')\r\npyplot.show()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Scaler Inverse Y back to normal value\r\nyhat_LSTM = scaler.inverse_transform(yhat)\r\ntestY_LSTM = scaler.inverse_transform(testY)\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Training and Test error\r\nmse_train = acc_metric(scaler.inverse_transform(trainY), scaler.inverse_transform(model.predict(trainX)))\r\nmse_test = acc_metric(yhat_LSTM, testY_LSTM)\r\nmse_train, mse_test\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\npredictDates = lstm_pair_data.tail(len(testX)).index\r\ntestY_reshape = normalize(testY_LSTM).reshape(len(testY_LSTM))\r\nyhat_reshape = normalize(yhat_LSTM).reshape(len(yhat_LSTM))\r\nkalman_reshape = normalize(yhat_KF).reshape(len(yhat_KF))\r\n#Plot predicted and actual line graph\r\nlayout = go.Layout(\r\n        yaxis=dict(\r\n        title='Spread',\r\n        titlefont=dict(\r\n            family='Arial, sans-serif',\r\n            size=18\r\n        )))\r\nactual_chart = go.Scatter(x=predictDates, y=testY_reshape, name= 'Actual Spread')\r\npredict_chart = go.Scatter(x=predictDates, y=yhat_reshape, name= 'LSTM Predict Spread')\r\npredict_kalman_chart = go.Scatter(x=predictDates, y=kalman_reshape, name= 'Kalman Filter Predict Spread')\r\nfig = go.Figure(data = [predict_kalman_chart, predict_chart, actual_chart], layout = layout)\r\npy.iplot(fig)#([predict_chart, actual_chart])\r\n\r\n\r\n# ## Tuning Hyperparameters using Mean Squared Error as Metric\r\n\r\n# In[ ]:\r\n\r\n\r\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[2]))\r\ndevX = np.reshape(devX, (devX.shape[0], 1, devX.shape[2]))\r\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[2]))\r\n# Tuning Hyperparameters\r\ncols = ['LSTM_Lyr1', 'LSTM_Lyr2', 'Regularization', 'Epochs', 'Batch_Size', 'MSE']\r\nMSE_Results = pd.DataFrame(columns = cols)\r\nLSTM_lyr1 = [16, 32, 128, 256, 512]\r\nLSTM_lyr2 = [16, 32, 128, 256, 512]\r\nepochs = [100, 200, 300, 400, 500]\r\nbatch_size = [20, 40, 60, 80, 100]\r\nregularization = [0.0, 0.1, 0.2, 0.3]\r\nk = 400\r\nl = 100\r\nm = 0.2\r\nfor i in LSTM_lyr1:\r\n    for j in LSTM_lyr2:\r\n        model = Sequential()\r\n        model.add(LSTM(i, input_shape=(trainX.shape[0], trainX.shape[2]), return_sequences = True))\r\n        model.add(LSTM(j))\r\n        model.add(Dropout(m))\r\n        model.add(Dense(look_back))\r\n        model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\r\n        history = model.fit(trainX, trainY, epochs=k, batch_size=l, validation_data=(devX, devY), verbose=0, shuffle=True)\r\n        yhat = model.predict(testX)\r\n        yhat_LSTM = scaler.inverse_transform(yhat)\r\n        testY_LSTM = scaler.inverse_transform(testY)\r\n        mse = acc_metric(testY_LSTM, yhat_LSTM)\r\n        MSE_Results = MSE_Results.append({'LSTM_Lyr1': i,\r\n                                          'LSTM_Lyr2': j,\r\n                                          'Regularization': m,\r\n                                          'Epochs': k,\r\n                                          'Batch_Size': l,\r\n                                          'MSE': mse}, ignore_index=True)\r\nMSE_Results\r\n\r\n\r\n# ## Trading Strategy\r\n\r\n# In[ ]:\r\n\r\n\r\n# Test Data\r\ntest_data = pd.DataFrame({'S1':pair_data['S1_close'].iloc[-len(testX):],'S2':pair_data['S2_close'].iloc[-len(testX):]})\r\ntest_data['Actual_Spread'] = pair_data['Spread_Close'].iloc[-len(testX):]\r\ntest_data['Kalman_Predicted_Spread']  = yhat_KF\r\ntest_data['ARIMA_Predicted_Spread']  = yhat_ARIMA\r\n#test_data['VAR_Predicted_Spread']  = yhat_VAR\r\n#test_data['FBProphet_Predicted_Spread']  = yhat_FbProphet\r\ntest_data['LSTM_Predicted_Spread'] = list(yhat_LSTM[:,0])\r\ndata = test_data['Actual_Spread'] \r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nratios_mavg5 = data.rolling(window=5,\r\n                               center=False).mean()\r\n\r\nratios_mavg60 = data.rolling(window=60,\r\n                               center=False).mean()\r\n\r\nstd_60 = data.rolling(window=60,\r\n                        center=False).std()\r\n\r\nzscore_60_5 = (ratios_mavg5 - ratios_mavg60)/std_60\r\npyplot.figure(figsize=(15,7))\r\npyplot.plot(data.index, data.values)\r\npyplot.plot(ratios_mavg5.index, ratios_mavg5.values)\r\npyplot.plot(ratios_mavg60.index, ratios_mavg60.values)\r\n\r\npyplot.legend(['Spread','5d Ratio MA', '60d Ratio MA'])\r\n\r\npyplot.ylabel('Spread')\r\npyplot.show()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Take a rolling 60 day standard deviation\r\nstd_60 = data.rolling(window=60,center=False).std()\r\nstd_60.name = 'std 60d'\r\n\r\n# Compute the z score for each day\r\nzscore_60_5 = (ratios_mavg5 - ratios_mavg60)/std_60\r\nzscore_60_5.name = 'z-score'\r\n\r\npyplot.figure(figsize=(15,7))\r\nzscore_60_5.plot()\r\npyplot.axhline(0, color='black')\r\npyplot.axhline(1.0, color='red', linestyle='--')\r\npyplot.axhline(-1.0, color='green', linestyle='--')\r\npyplot.legend(['Rolling Spread z-Score', 'Mean', '+1', '-1'])\r\npyplot.show()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Plot the ratios and buy and sell signals from z score\r\npyplot.figure(figsize=(15,7))\r\n\r\ndata[60:].plot()\r\nbuy = data.copy()\r\nsell = data.copy()\r\nbuy[zscore_60_5>-1] = -100\r\nsell[zscore_60_5<1] = -100\r\nbuy[60:].plot(color='g', linestyle='None', marker='^')\r\nsell[60:].plot(color='r', linestyle='None', marker='^')\r\nx1,x2,y1,y2 = pyplot.axis()\r\npyplot.axis((x1,x2,data.min(),data.max()))\r\npyplot.legend(['Spread', 'Buy Signal', 'Sell Signal'])\r\npyplot.show()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Plot the prices and buy and sell signals from z score\r\npyplot.figure(figsize=(18,9))\r\nS1 = test_data.S1\r\nS2 = test_data.S2\r\n\r\nS1[60:].plot(color='b')\r\nS2[60:].plot(color='c')\r\nbuyR = 0*S1.copy()\r\nsellR = 0*S1.copy()\r\n\r\n# When buying the ratio, buy S1 and sell S2\r\nbuyR[buy!=-100] = S1[buy!=-100]\r\nsellR[buy!=-100] = S2[buy!=-100]\r\n# When selling the ratio, sell S1 and buy S2 \r\nbuyR[sell!=-100] = S2[sell!=-100]\r\nsellR[sell!=-100] = S1[sell!=-100]\r\n\r\nbuyR[60:].plot(color='g', linestyle='None', marker='^')\r\nsellR[60:].plot(color='r', linestyle='None', marker='^')\r\nx1,x2,y1,y2 = pyplot.axis()\r\npyplot.axis((x1,x2,min(S1.min(),S2.min()),max(S1.max(),S2.max())))\r\n\r\npyplot.legend(['S1', 'S2', 'Buy Signal', 'Sell Signal'])\r\npyplot.show()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\ndef trade(S1, S2, spread, window1, window2):\r\n    # If window length is 0, algorithm doesn't make sense, so exit\r\n    if (window1 == 0) or (window2 == 0):\r\n        return 0\r\n    # Compute rolling mean and rolling standard deviation\r\n    ma1 = spread.rolling(window=window1, center=False).mean()\r\n    ma2 = spread.rolling(window=window2, center=False).mean()\r\n    std = spread.rolling(window=window2, center=False).std()\r\n    zscore = (ma1 - ma2)/std\r\n    # Simulate trading\r\n    # Start with no money and no positions\r\n    money = 0\r\n    countS1 = 0\r\n    countS2 = 0\r\n    for i in range(len(spread)):\r\n        # Sell short if the z-score is > 1\r\n        if zscore[i] > 1:\r\n            money += S1[i] - S2[i] * spread[i]\r\n            countS1 -= 1\r\n            countS2 += spread[i]\r\n        # Buy long if the z-score is < 1\r\n        elif zscore[i] < -1:\r\n            money -= S1[i] - S2[i] * spread[i]\r\n            countS1 += 1\r\n            countS2 -= spread[i]\r\n        # Clear positions if the z-score between -.5 and .5\r\n        elif abs(zscore[i]) < 0.5:\r\n            money += countS1 * S1[i] - S2[i] * countS2\r\n            countS1 = 0\r\n            countS2 = 0\r\n    return money\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\ndata = test_data['Actual_Spread'] \r\nprofit = trade(test_data['S1'], test_data['S2'], data, 30, 5)\r\nprofit\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Actual Spread\r\n# 91964.78343828625\r\n# Kalman Filter\r\n# 29959.723177883072\r\n# ARIMA\r\n# 64448.09898119516\r\n# VAR\r\n# 64580.88216334411\r\n# FB Prophet\r\n# 49316.91921868784\r\n# LSTM\r\n# 70589.74374960671\r\n\r\n\r\n```\r\n\r\n\r\n\r\n", "@Mrforeman1 ,\r\nThe code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily. Thanks!", "I tried to remove the unimportant calculations.  Let me know if I need to reduce it more.\r\n\r\n```\r\n#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\n# In[3]:\r\n\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pandas_ta as ta\r\n\r\nimport statsmodels\r\nfrom statsmodels.tsa.stattools import coint\r\nimport statsmodels.api as sm\r\nfrom statsmodels.tsa.arima_model import ARIMA\r\n\r\nfrom statsmodels.tsa.vector_ar.var_model import VAR\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom pykalman import KalmanFilter\r\nnp.random.seed(107)\r\nimport pywt\r\n\r\nimport sys\r\n# sys.path.append('/home/ubuntu/auquantoolbox')\r\n# from backtester.dataSource.yahoo_data_source import YahooStockDataSource\r\nfrom datetime import datetime\r\n\r\nfrom math import sqrt\r\nfrom numpy import concatenate\r\nfrom matplotlib import pyplot\r\nfrom datetime import datetime\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom sklearn.preprocessing import LabelEncoder\r\nfrom sklearn.metrics import mean_squared_error\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.layers import LSTM\r\nfrom keras.layers import Dropout\r\nimport plotly.offline as py\r\nimport plotly.graph_objs as go\r\nimport seaborn as sns\r\npy.init_notebook_mode(connected=True)\r\nget_ipython().run_line_magic('matplotlib', 'inline')\r\n\r\n\r\n\r\ndef getYahoo(tickers, start_date, end_date, frequency='1d'):\r\n    import io\r\n    import re\r\n    import requests\r\n    import pandas as pd\r\n    import datetime\r\n    import time\r\n    import numpy as np\r\n    unix_start = str(int(time.mktime(datetime.date(int(start_date[0:4]), int(start_date[5:7]), int(start_date[8:10])).timetuple())))\r\n    unix_end = str(int(time.mktime(datetime.date(int(end_date[0:4]), int(end_date[5:7]), int(end_date[8:10])).timetuple())))\r\n    OHLC = {}\r\n    cookie = ''\r\n    crumb = ''\r\n    s = requests.Session()\r\n    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\r\n    res = s.get('https://finance.yahoo.com/quote/SPY/history', headers=headers)\r\n    cookie = res.cookies['B']\r\n    pattern = re.compile('.*\"CrumbStore\":\\{\"crumb\":\"(?P<crumb>[^\"]+)\"\\}')\r\n    for line in res.text.splitlines():\r\n        m = pattern.match(line)\r\n        if m is not None:\r\n            crumb = m.groupdict()['crumb']\r\n    g_tickers = []\r\n    for x in range(0,len(tickers)):\r\n        ticker = tickers[x] \r\n        url_str = \"https://query1.finance.yahoo.com/v7/finance/download/%s\"\r\n        url_str += \"?period1=%s&period2=%s&interval=%s&events=history&crumb=%s\"\r\n        url = url_str % (ticker, unix_start, unix_end, frequency, crumb)\r\n        try:\r\n            res = s.get(url, headers=headers,cookies={'B': cookie}).text\r\n            OHLC[ticker] = pd.read_csv(io.StringIO(res), index_col=0, error_bad_lines=False,warn_bad_lines=False).replace('null', np.nan).dropna()\r\n            OHLC[ticker].index = pd.to_datetime(OHLC[ticker].index)\r\n            OHLC[ticker] = OHLC[ticker].apply(pd.to_numeric)\r\n            current = x+1\r\n            total = len(tickers)\r\n            pos = str(current)+'/'+str(total)\r\n            g_tickers.append(tickers[x])\r\n            print(pos+' - '+ticker+ \" - Done\")\r\n        except:\r\n            current = x+1\r\n            total = len(tickers)\r\n            pos = str(current)+'/'+str(total)\r\n            print(pos+' - '+ticker+ \" - Error\")\r\n            \r\n    dict_open = {}\r\n    dict_high = {}\r\n    dict_low = {}\r\n    dict_close = {}\r\n    dict_Adjclose = {}\r\n    dict_volume = {}\r\n\r\n    for i in g_tickers:\r\n        try:\r\n            dict_open[i] = OHLC[i][\"Open\"]\r\n            dict_high[i] = OHLC[i][\"High\"]\r\n            dict_low[i] = OHLC[i][\"Low\"]\r\n            dict_close[i] = OHLC[i][\"Close\"]\r\n            dict_Adjclose[i] = OHLC[i][\"Adj Close\"]\r\n            dict_volume[i] = OHLC[i][\"Volume\"]\r\n        except:\r\n            None\r\n            \r\n    Open = pd.DataFrame(dict_open)\r\n    High = pd.DataFrame(dict_high)\r\n    Low = pd.DataFrame(dict_low)\r\n    Close = pd.DataFrame(dict_close)\r\n    Adjclose = pd.DataFrame(dict_Adjclose)\r\n    Volume = pd.DataFrame(dict_volume)\r\n    return {\"Open\":Open,\"High\":High,\"Low\":Low,\"Close\":Close,\"Adj Close\":Adjclose, \"Volume\":Volume}\r\n\r\n\r\n# In[6]:\r\n\r\n\r\nstartDateStr = '2019-01-01'\r\nendDateStr = '2022-01-01'\r\ninstrumentIds = list(set(['KLAC', 'BC','VGT']))\r\n\r\ndata = getYahoo(instrumentIds,startDateStr,endDateStr)\r\ndata_close = data['Close'].dropna()\r\ndata_open = data['Open'].dropna()\r\ndata_high = data['High'].dropna()\r\ndata_low = data['Low'].dropna()\r\ndata_vol = data['Volume'].dropna()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n\r\n\r\n\r\n# # DATA ANALYTICS\r\n\r\n# In[7]:\r\n\r\n\r\nprint(data_close.shape)\r\ndata_high.iloc[:10,:10]\r\n\r\n\r\n# In[8]:\r\n\r\n\r\na = pd.DataFrame({'Close':data_close.iloc[:3,1],\r\n             'Open':data_open.iloc[:3,1],\r\n                 'High':data_high.iloc[:3,1],\r\n                 'Low':data_low.iloc[:3,1],\r\n                 'Volume':data_vol.iloc[:3,1]})\r\na\r\n\r\n\r\n# In[9]:\r\n\r\n\r\n# Checking for any null values in data\r\nd = data_close.isnull().any()\r\nprint(d[d == True])\r\n\r\n\r\n# In[10]:\r\n\r\n\r\n# Create a plot of the time series to visualize the data\r\nfor i in range(data_close.shape[1]):\r\n    S = data_close.iloc[:, i]\r\n    S.plot(figsize=(15,7))\r\n\r\n\r\n# # Generate Co-Integrated Pairs\r\n\r\n# In[11]:\r\n\r\n\r\ndef find_cointegrated_pairs(data):\r\n    n = data.shape[1]\r\n    score_matrix = np.zeros((n, n))\r\n    pvalue_matrix = np.ones((n, n))\r\n    keys = data.keys()\r\n    pairs = {}\r\n    for i in range(n):\r\n        for j in range(i+1, n):\r\n            S1 = data[keys[i]]\r\n            S2 = data[keys[j]]\r\n            result = coint(S1, S2)\r\n            score = result[0]\r\n            pvalue = result[1]\r\n            score_matrix[i, j] = score\r\n            pvalue_matrix[i, j] = pvalue\r\n            if pvalue < 0.05:\r\n                pairs[(keys[i], keys[j])] = result\r\n    return score_matrix, pvalue_matrix, pairs\r\n\r\nscores, pvalues, pairs = find_cointegrated_pairs(data_close)\r\n\r\n\r\n# In[12]:\r\n\r\n\r\nscores\r\n\r\n\r\n# In[13]:\r\n\r\n\r\npvalues\r\n\r\n\r\n# In[14]:\r\n\r\n\r\npairs\r\n\r\n\r\n# In[15]:\r\n\r\n\r\n# Heatmap to show the p-values of the cointegration test\r\n# between each pair of stocks\r\n#m = [0,0.2,0.4,0.6,0.8,1]\r\nsns.heatmap(pvalues,  cmap='RdYlGn_r' \r\n                , mask = (pvalues >= 0.98)\r\n                )\r\npyplot.show()\r\n\r\n\r\n# In[16]:\r\n\r\n\r\npairs_data = {key:value[1]  for (key, value) in pairs.items()}\r\npairs_data = sorted(pairs_data.items(), key=lambda x: x[1])\r\npairs_data[:10]\r\n\r\n\r\n# In[17]:\r\n\r\n\r\npair_data = pd.DataFrame({'S1_close':data_close['KLAC'],'S2_close':data_close['BC']\r\n                         ,'S1_open':data_open['KLAC'],'S2_open':data_open['BC']\r\n                         ,'S1_high':data_high['KLAC'],'S2_high':data_high['BC']\r\n                         ,'S1_low':data_low['KLAC'],'S2_low':data_low['BC']\r\n                         ,'S1_volume':data_vol['KLAC'],'S2_volume':data_vol['BC']})\r\npair_data['S1_close'].plot(figsize=(15,7))\r\npair_data['S2_close'].plot(figsize=(15,7))\r\n\r\n\r\n\r\n# ## Spreads for Ground Truth\r\n\r\n# In[19]:\r\n\r\n\r\nest = sm.OLS(pair_data.S1_close, pair_data.S2_close)\r\nest = est.fit()\r\nalpha = -est.params[0]\r\npair_data['Spread_Close'] = pair_data.S1_close + (pair_data.S2_close * alpha)\r\npair_data['Spread_Close'].plot(figsize=(15,7))\r\npyplot.axhline(pair_data['Spread_Close'].mean())\r\npyplot.legend(['Spread_Close'])\r\n\r\n\r\n# In[20]:\r\n\r\n\r\nest_op = sm.OLS(pair_data.S1_open, pair_data.S2_open)\r\nest_op = est_op.fit()\r\nalpha_op = -est_op.params[0]\r\npair_data['Spread_Open'] = pair_data.S1_open + (pair_data.S2_open * alpha_op)\r\nest_hi = sm.OLS(pair_data.S1_high, pair_data.S2_high)\r\nest_hi = est_hi.fit()\r\nalpha_hi = -est_hi.params[0]\r\npair_data['Spread_High'] = pair_data.S1_high + (pair_data.S2_high * alpha_hi)\r\nest_lo = sm.OLS(pair_data.S1_low, pair_data.S2_low)\r\nest_lo = est_lo.fit()\r\nalpha_lo = -est_lo.params[0]\r\npair_data['Spread_Low'] = pair_data.S1_low + (pair_data.S2_low * alpha_lo)\r\n\r\n\r\n# ## Wavelet Denoising\r\n\r\n# In[21]:\r\n\r\n\r\ndef wav_den(ts_orig):\r\n    (ca, cd) = pywt.dwt(ts_orig, 'db8')\r\n    cat = pywt.threshold(ca, np.std(ca)/8, mode='soft')\r\n    cdt = pywt.threshold(cd, np.std(cd)/8, mode='soft')\r\n    ts_rec = pywt.idwt(cat, cdt, 'db8')\r\n    return ts_rec[1:]\r\n\r\n\r\n# In[22]:\r\n\r\n\r\n#Use MinMaxScaler to normalize Weighted Price to range from 0 to 1\r\ncols = ['Spread_Close', 'Spread_Open', 'Spread_High', 'Spread_Low']\r\nlstm_pair_data = pair_data;\r\nlsmt_pair_data = pair_data;\r\nlstm_pair_data = pd.DataFrame({'Spread_Close':pair_data['Spread_Close'][30:],\r\n                               'Spread_Open':pair_data['Spread_Open'][30:],\r\n                               'Spread_High':pair_data['Spread_High'][30:],\r\n                               'Spread_Low':pair_data['Spread_Low'][30:],\r\n\r\n                              }, columns = cols)\r\n\r\n\r\n# In[23]:\r\n\r\n\r\n\r\ntrain_size = int(len(lsmt_pair_data) * 0.9)\r\ndev_size = int((len(lsmt_pair_data) - train_size) * 0.5) - 30\r\ntest_size = len(lsmt_pair_data) - train_size - dev_size\r\ntrain, dev, test = lsmt_pair_data[0:train_size], lsmt_pair_data[train_size:train_size + dev_size], lsmt_pair_data[train_size + dev_size:len(lsmt_pair_data)]\r\nprint(len(train), len(dev), len(test))\r\n\r\n\r\n# In[24]:\r\n\r\n\r\ntrain_den = pd.DataFrame(columns = cols)\r\nfor col in cols:\r\n    train_den[col] = wav_den(train[col])\r\n\r\n\r\n# ## Prediction Period Setting\r\n\r\n# In[25]:\r\n\r\n\r\nlook_back = 1\r\n\r\n\r\n# In[26]:\r\n\r\n\r\nscaler = MinMaxScaler(feature_range=(0, 1))\r\n# Create function for creating dataset with look back\r\ndef create_dataset(dataset, look_back=1):\r\n    dataX, dataY = [], []\r\n    for i in range(len(dataset) - look_back):\r\n        a = dataset[i, :]\r\n        dataX.append(a)\r\n        dataY.append(dataset[(i+1):(i+1+look_back), 0])\r\n    print(len(dataY))\r\n    return dataX, np.array(scaler.fit_transform(dataX)), dataY, np.array(scaler.fit_transform(dataY))\r\n\r\n# Generate dataset for trainX, trainY, testX, testY\r\ntrainX_untr, trainX, trainY_untr, trainY = create_dataset(train_den.values, look_back)\r\ndevX_untr, devX, devY_untr, devY = create_dataset(dev.values, look_back)\r\ntestX_untr, testX, testY_untr, testY = create_dataset(test.values, look_back)\r\n\r\n\r\n# ## ACCURACY METRIC\r\n\r\n# In[27]:\r\n\r\n\r\ndef acc_metric(true_value, predicted_value):\r\n    acc_met = 0.0\r\n    m = len(true_value)\r\n    for i in range(m):\r\n        acc_met += mean_squared_error(true_value[i], predicted_value[i])\r\n    acc_met /= m\r\n    return np.sqrt(acc_met)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef normalize(series):\r\n    return (series - np.mean(series)) / np.std(series)\r\n\r\n\r\n# In[30]:\r\n\r\n\r\nnormalize(pair_data['Spread_Close']).plot(figsize=(15,7))\r\nresults.plot(figsize=(15,7))\r\npyplot.axhline(normalize(pair_data['Spread_Close']).mean(), color='black')\r\npyplot.axhline(1.0, color='red', linestyle='--')\r\npyplot.axhline(-1.0, color='green', linestyle='--')\r\npyplot.legend(['Spread z-score', 'Kalman_Predicted_Spread', 'Mean', '+1', '-1'])\r\npyplot.show()\r\n\r\n\r\n\r\n\r\n\r\n\r\n# Reshape X for model training\r\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\r\ndevX = np.reshape(devX, (devX.shape[0], 1, devX.shape[1]))\r\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\r\n# Running the LSTM model\r\nmodel = Sequential()\r\nmodel.add(LSTM(256, input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences = True))\r\nmodel.add(LSTM(256))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(look_back))\r\nmodel.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\r\nhistory = model.fit(trainX, trainY, epochs=200, batch_size=20, validation_data=(devX, devY), verbose=0, shuffle=True)\r\n\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\nmodel.summary()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Plot line graph to show amount loss according the the epoch\r\npyplot.plot(history.history['loss'], label='train')\r\npyplot.plot(history.history['val_loss'], label='dev')\r\npyplot.legend()\r\npyplot.show()\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# Make prediction using textX and plotting line graph against testY\r\nyhat = model.predict(testX)\r\npyplot.plot(testY, label='True Value')\r\npyplot.plot(yhat, label='Predicted Value')\r\npyplot.legend()\r\npyplot.show()\r\n\r\n\r\n\r\n# Training and Test error\r\nmse_train = acc_metric(scaler.inverse_transform(trainY), scaler.inverse_transform(model.predict(trainX)))\r\nmse_test = acc_metric(yhat_LSTM, testY_LSTM)\r\nmse_train, mse_test\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\npredictDates = lstm_pair_data.tail(len(testX)).index\r\ntestY_reshape = normalize(testY_LSTM).reshape(len(testY_LSTM))\r\nyhat_reshape = normalize(yhat_LSTM).reshape(len(yhat_LSTM))\r\nkalman_reshape = normalize(yhat_KF).reshape(len(yhat_KF))\r\n#Plot predicted and actual line graph\r\nlayout = go.Layout(\r\n        yaxis=dict(\r\n        title='Spread',\r\n        titlefont=dict(\r\n            family='Arial, sans-serif',\r\n            size=18\r\n        )))\r\nactual_chart = go.Scatter(x=predictDates, y=testY_reshape, name= 'Actual Spread')\r\npredict_chart = go.Scatter(x=predictDates, y=yhat_reshape, name= 'LSTM Predict Spread')\r\npredict_kalman_chart = go.Scatter(x=predictDates, y=kalman_reshape, name= 'Kalman Filter Predict Spread')\r\nfig = go.Figure(data = [predict_kalman_chart, predict_chart, actual_chart], layout = layout)\r\npy.iplot(fig)#([predict_chart, actual_chart])\r\n\r\n\r\n# ## Tuning Hyperparameters using Mean Squared Error as Metric\r\n\r\n# In[ ]:\r\n\r\n\r\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[2]))\r\ndevX = np.reshape(devX, (devX.shape[0], 1, devX.shape[2]))\r\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[2]))\r\n# Tuning Hyperparameters\r\ncols = ['LSTM_Lyr1', 'LSTM_Lyr2', 'Regularization', 'Epochs', 'Batch_Size', 'MSE']\r\nMSE_Results = pd.DataFrame(columns = cols)\r\nLSTM_lyr1 = [16, 32, 128, 256, 512]\r\nLSTM_lyr2 = [16, 32, 128, 256, 512]\r\nepochs = [100, 200, 300, 400, 500]\r\nbatch_size = [20, 40, 60, 80, 100]\r\nregularization = [0.0, 0.1, 0.2, 0.3]\r\nk = 400\r\nl = 100\r\nm = 0.2\r\nfor i in LSTM_lyr1:\r\n    for j in LSTM_lyr2:\r\n        model = Sequential()\r\n        model.add(LSTM(i, input_shape=(trainX.shape[0], trainX.shape[2]), return_sequences = True))\r\n        model.add(LSTM(j))\r\n        model.add(Dropout(m))\r\n        model.add(Dense(look_back))\r\n        model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\r\n        history = model.fit(trainX, trainY, epochs=k, batch_size=l, validation_data=(devX, devY), verbose=0, shuffle=True)\r\n        yhat = model.predict(testX)\r\n        yhat_LSTM = scaler.inverse_transform(yhat)\r\n        testY_LSTM = scaler.inverse_transform(testY)\r\n        mse = acc_metric(testY_LSTM, yhat_LSTM)\r\n        MSE_Results = MSE_Results.append({'LSTM_Lyr1': i,\r\n                                          'LSTM_Lyr2': j,\r\n                                          'Regularization': m,\r\n                                          'Epochs': k,\r\n                                          'Batch_Size': l,\r\n                                          'MSE': mse}, ignore_index=True)\r\nMSE_Results\r\n\r\n\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "@Mrforeman1 ,\r\nI was facing different error while trying to execute the given code.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/dafbeeaf15b91dec610ad63bf66c4836/untitled186.ipynb).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53727, "title": "[INTEL oneDNN] Adding bf16 support for complementary Erf op", "body": null, "comments": ["@ashiqimranintel Can you please sign CLA. Thanks!", "@gbaned, I signed CLA, still it's showing \"Missing CLA from one or more contributors\", can you re-scan it? "]}, {"number": 53724, "title": "How can I uninstall tensorflow built in the v-environment of ubuntu-20.04?", "body": "I have tried both \"sudo pip uninstall tensorflow-2.7.0\" and \"sudo pip uninstall tensorflow\" in both normal environment and the v-environment which I installed the tensorflow before, but they all just display \"WARNING: Skipping tensorflow as it is not installed.\".\r\nI check the \"pip list\" in the v-environment to see whether the tensorflow still exist, but as the picture, it just right there,safe and sound.\r\nSo how can I deal with it to uninstall it? Thanks for your help in advance\r\n<img width=\"364\" alt=\"\u5fae\u4fe1\u56fe\u7247_20220111233045\" src=\"https://user-images.githubusercontent.com/95178820/148972571-8e7f8b1f-7de1-4389-b983-7271125f5515.png\">\r\n<img width=\"479\" alt=\"\u5fae\u4fe1\u56fe\u7247_20220111233050\" src=\"https://user-images.githubusercontent.com/95178820/148972602-72f6fcb2-3976-4292-847a-80ae46456697.png\">\r\n.", "comments": ["@wuliJerry ,\r\nCan you please take a look at this SO [link](https://stackoverflow.com/questions/55366503/problems-cannot-installing-uninstalling-tensorflow) and the issue [1](https://github.com/tensorflow/tensorflow/issues/8785), [2](https://askubuntu.com/questions/764032/how-to-uninstall-tensorflow-completely) and [3](https://sparrow.dev/uninstall-tensorflow-troubleshooting/) with the similar error.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53724\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53724\">No</a>\n"]}, {"number": 53723, "title": "Is there a way I can give a different batch size in each iteration into a fixed batch size in the feed dict?", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Windows 10 Pro\r\n- TensorFlow installed from (source or binary)\r\n- TensorFlow version : 2.6.0 \r\n- Python version: 3.6.13\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model: GeForce RTX 2080 Super\r\n\r\nHi, \r\nI am working on real time project using alexnet model. I am constantly getting frames at a constant FPS value.I need to get a prediction result for each frame. But I cut the objects whose number is not certain in each frame and give them to the trained model in a batch. Like the screenshot below;\r\n\r\n![woring_correctly_loop_2](https://user-images.githubusercontent.com/40995578/148951684-1a84eb42-6710-4465-b82c-e528b1b2b421.png)\r\n\r\nGPU **infrence time** works correctly when there is the same number of objects in each frame, as in the screenshot above. but when there is a different number of objects in each frame, the GPU infrence time returns with too much time. \r\nAs far as I understand, parallel processes set themselves for different batch sizes. I can show you a screenshot of it below ;\r\n\r\n![error_loop_log_image](https://user-images.githubusercontent.com/40995578/148952864-b120e85c-5a7e-40f0-8390-2c29f89b1e6e.png)\r\n\r\nBetween 0.005ms and 0.040 ms is a normal time for me but  values \u200b\u200bother than these are not normal.\r\n\r\n ```\r\n  input_batch = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 227, 227, 1], name='input__batch')\r\n```\r\n\r\n ```\r\n  test_acc_step= sess.run('import/prediction_argmax_model:0', feed_dict={'import/x:0': input_static_batch_size_images})\r\n```\r\nWith this usage, I am sending a **static batch** into the **session feed dict**.\r\n\r\n**Can I give this to static size instead of None and feed it with dynamic batch in each iteration?** Or is there another way to do this?\r\n\r\n\r\n\r\n\r\n", "comments": ["Hi @emrullahpolat ! Attaching relevant [thread](https://stackoverflow.com/questions/50551199/tensorflow-dataset-with-changing-batch-size-to-compute-test-loss-during-training) for reference.\r\nPlease open this issue in TF discussion [forum](https://discuss.tensorflow.org/) for further assistance as there is a larger community there. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53723\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53723\">No</a>\n"]}, {"number": 53722, "title": "Model subclassing: access layers saved as class variables to self.__dict__ for dynamic model definition", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: Linux Ubuntu 20.04 LTS\r\n- TensorFlow installed from binary\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.7.9\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: 2x RTX3090,, 24GB\r\n- Gist with run-able code: https://gist.github.com/MathiesW/2640ac9cf04da4f0f471b0158437c5c6\r\n\r\nHi,\r\nmy goal is to define a dynamically generated FCN model, where I define the amount of filters as a list, and the model is generated with len(filters) Encoder and len(filters) Decoder blocks. I defined my blocks and model according to [this Tensorflow Guide](https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_model_class).\r\n\r\nNow, to achieve my dynamic model class, I have written all class variables into the `model.__dict__`:\r\n```\r\nclass FullyConvolutionalNetworkDynamic(Model, ABC):\r\n    def __init__(self,\r\n                 num_filter: List[int],\r\n                 kernel_size: List[int],\r\n                 strides: List[int],\r\n                 name: str = \"FullyConvolutionalNetwork\",\r\n                 activation: str = \"relu\",\r\n                 data_format: str = \"channels_last\",\r\n                 *args, **kwargs):\r\n        super(FullyConvolutionalNetworkDynamic, self).__init__(name=name, *args, **kwargs)\r\n\r\n        # define symmetric encoder and decoder blocks\r\n        for i, (f, k, s) in enumerate(zip(num_filter, kernel_size, strides)):\r\n            self.__dict__[f\"encoder{i}\"] = EncoderBlock(num_conv=f, kernel=k, stride=s,\r\n                                                        activation=activation,\r\n                                                        name=f\"enc{i}\", data_format=data_format)\r\n            self.__dict__[f\"decoder{i}\"] = DecoderBlock(num_conv=f, kernel=k, stride=s,\r\n                                                        activation=activation,\r\n                                                        name=f\"dec{i}\", data_format=data_format)\r\n\r\n        # output section\r\n        self.conv1x1 = Conv1D(1, 1, strides=1, padding='same', name=\"1x1\", data_format=data_format)\r\n        self.flatten = Flatten()\r\n```\r\nNow that my layers are written to `model.__dict__`, I tried to add them to my call function from here:\r\n```\r\n    @tf.function\r\n    def call(self, x, training=False):\r\n        # encoder path\r\n        for layer in sorted([key for key in self.__dict__ if \"encoder\" in key]):\r\n            x = self.__dict__[layer](x)\r\n\r\n        # decoder path\r\n        for layer in sorted([key for key in self.__dict__ if \"decoder\" in key], reverse=True):\r\n            x = self.__dict__[layer](x)\r\n\r\n        # output section\r\n        x = self.conv1x1(x)\r\n        return self.flatten(x)\r\n```\r\nHowever, when I now create an instance of my model, the model summary states that only the layers defined in the output section are added to the model:\r\n```\r\n>>> model = FullyConvolutionalNetwork(num_filters=[4, 8, 16], kernel_size=[3, 3, 3], strides=[1, 1, 1])\r\n>>> model.build(input_shape=[None, 256, 10])\r\n>>> model.summary()\r\n\r\nModel: \"FullyConvolutionalNetwork\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\n1x1 (Conv1D)                 multiple                  5         \r\n_________________________________________________________________\r\nflatten (Flatten)            multiple                  0         \r\n=================================================================\r\nTotal params: 5\r\nTrainable params: 5\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n\r\n```\r\nI already use the model but \"hard-coded\" to three layers depth on each path. When I try to recreate my hard-coded model with the dynamic approach above, the `model.__dict__` indeed contains my EncoderBlock and DecoderBlock layers, they are just not added to the model. The \"hard-coded\" version works flawlessly! Any help to solve my problem is greatly appreciated! Thank you in advance!\r\n\r\nPS: I have allowed my self to re post my issue here, since the Tensorflow Repository seems much more active than the keras repository.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53722\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53722\">No</a>\n", "Using setattr(self, key, value) instead of writing everything to `self.__dict__` did the trick!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53722\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53722\">No</a>\n"]}, {"number": 53720, "title": "[determinism] Fix indentation in release notes", "body": "NB: this is a PR against the r2.8 branch.\r\n\r\nThis PR fixes indentation that was lost in or after the cherry pick into the r2.8 branch of PR [53465](https://github.com/tensorflow/tensorflow/pull/53465), which was merged into the master branch.\r\n\r\nI wonder if there is a script that auto-formats the release notes, a script which is replacing 2-space indents with 0-space intents, rather than 4-space intents. There may be other indentation losses in the r2.8 release notes file.", "comments": ["There is some autoformatting. We should disable it.\r\n\r\nThank you for the fix."]}, {"number": 53719, "title": "[mhlo] Fix Linalg lowering for DotOp and add a verifier to enforce shape compatibility", "body": null, "comments": ["Seems like at least these are failing:\r\n```\r\n//tensorflow/compiler/mlir/tensorflow/tests:legalize_hlo.mlir.test\r\n//tensorflow/compiler/mlir/xla/tests/translate:import.hlotxt.test\r\n//tensorflow/compiler/mlir/xla/tests/translate:simple.mlir.test\r\n```", "> \r\nThanks. I will fix those later.", "I'm not sure if there are still problems on other critical paths, like tf2xla.", "I need some help, I can't seem to see any error messages printed here.Is this an internal model test?", "I have not rebase on latest code base, Should I rebase on it?", "If there aren't any conflict this should be OK, but we'll see.", "Thanks.", "Seems like I have some tests right now that spell something like: ` %2 = \"mhlo.dot\"(%0, %1) {precision_config = [\"DEFAULT\", \"DEFAULT\"]} : (tensor<256xf32>, tensor<256xf32>) -> tensor<1xf32> `\r\nBut the inferred result is `tensor<f32>`.", "> Seems like I have some tests right now that spell something like: `%2 = \"mhlo.dot\"(%0, %1) {precision_config = [\"DEFAULT\", \"DEFAULT\"]} : (tensor<256xf32>, tensor<256xf32>) -> tensor<1xf32>` But the inferred result is `tensor<f32>`.\r\n\r\nI refer https://www.tensorflow.org/xla/operation_semantics#dot xla doc. vector dot vector will output scalar type.", "XLA accepts: `  %dot.5 = f32[32] dot(f32[16] %Arg_1.2, f32[16,32] %Arg_2.3), lhs_contracting_dims={0}, rhs_contracting_dims={0}`.\r\nI will relax  the verifier to allow vector-matrix for now\r\n", "I have relax the verifier of dot and legal lower vec-mat to linalg::VecmatOp in latest patch."]}, {"number": 53718, "title": "Converter for Tile operation", "body": "- ConvertTile operator is implemented so that both parameters (tensor and multiplier) can be passed as tensors and weights. A unit test with 6 tests and all combinations of passed parameters is provided.\r\n\r\n- New macro `#define CHECK_INPUT_SIZE (size, exp_size, node_def)` unifies the testing of the number of parameters passed. It is used in 8 different places.\r\n\r\n- The helper function template void `AdjustVectorByDims(...)` makes writing tests much easier (especially tests with parameters that can be passed as tensors and weights). For example, today we can use some parameters like tensors with predefined dimensions, but with no real numbers. An attempt to pass the same parameters with the same values as weights leads to the crash. We see a similar crash when the vector providing the values assigned to the tensor is not empty, but its length does not match the number of values defined by the dimensions. `AdjustVectorByDims` fixes these two problems and is called by default in the situations described, but these calls can also be blocked when such failures are intentional.\r\n\r\nNOTE: This PR should be slightly adjusted if it will be merged after\r\n[PR#54230](https://github.com/tensorflow/tensorflow/pull/54230):: Refactoring of ConvertUnary\r\n", "comments": ["Remove this description from the PR description to comment:\r\nThis PR is a replacement for PR# 53698", "@drivanov I assume that you are still working on this as there are comments that aren't addressed yet.", "@bixia, I have addressed the issues, please have a look", "I see a bunch a unrelated files were uploaded, and an existing file op_convert.h is a copied to a different directory and shown as \"new\"? Can you please fix those?", "@bixia1:  Sorry, I have no idea how this happened. Some of these files are not even from Tensorflow but from my MXNET projects. I just fixed it.\r\n\r\n> I see a bunch a unrelated files were uploaded, and an existing file op_convert.h is a copied to a different directory and shown as \"new\"? Can you please fix those?\r\n\r\n", "@bixia1 : I rebased before the squash, therefore I see **Conflicting files**. Is it OK?\r\nOR, perhaps I need to create a new PR.", "Something is not right, it shows this PR change > 2000 files. Would you please check?", "It might have been a wrong-way merge or something like that. \r\n\r\nAt this point it's usually easier to just close it and make a new PR from scratch. "]}, {"number": 53717, "title": "Typo fix", "body": "Shouldn't it be \"TensorFlow *uses* semantic versioning\", not \"TensorFlow *using*\"?", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac \r\n", "@gbaned @mihaimaruseac I'm scanning over `c_api.h` as I try to learn the TensorFlow C API. If I find more errors (e.g. 8 small errors), will this still be considered a \"one-liner\"? I would think that patching all of them in one PR would be cost-effective. Please re-open this PR if my idea is considered worthwhile.", "If you fix multiple typos in a file/directory that's ok. Thank you for the fixes."]}, {"number": 53716, "title": "[ROCm] Enable unique_op for ROCm", "body": "This PR enables the unique_op_gpu (int32 and int64) for ROCM on AMD GPUs.\r\n\r\n@cheshire @chsigg @deven-amd", "comments": ["Should some of the tests be enabled for ROCm as well?", "Thanks @cheshire !  \r\nIt looks like the unique_op_test (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/unique_op_test.cc) is cpu only and is therefore already enabled across the board. Was there any other test you had in mind?", "If there's no GPU test you don't have to write one, but ideally, there really should be.", "I agree! I'll take a look at the existing test, maybe there is an easy way to have it also run on GPU(s). "]}, {"number": 53715, "title": "ci_build: update check_system_libs for numbered workspace files", "body": "This script was not updated when the workspace files were numbered, noticed it when unbundling some more libraries.\r\n\r\nRe-ran against latest main branch and it all passes correctly now", "comments": []}, {"number": 53712, "title": "[MHLO-LINALG] Add lowering of MHLO.RealDynamicSliceOp to Linalg", "body": "-- This commit adds lowering of MHLO.RealDynamicSliceOp to arith/tensor ops\r\n   as part of `hlo-legalize-to-linalg` pass.\r\n\r\nSigned-off-by: Abhishek Varma <abhishek.varma@polymagelabs.com>", "comments": ["Hi @joker-eph \r\nCan you redirect someone to review this PR?", "Yes, trying to :)", "@avarmapml Can you please check @hanhanW's comments and keep us posted ? Thanks!", "Hi @hanhanW , I've addressed all your comments.\r\nYou may re-review now. Thanks.", "Hi @hanhanW \r\nI've addressed all your comments. You may re-review/accept this revision now.", "Hi @hanhanW \r\n\r\nCan you tell me why few checks aren't successful and what is it that's needed to be done from my end?\r\nI'm unable to view them.", "> Hi @hanhanW\r\n> \r\n> Can you tell me why few checks aren't successful and what is it that's needed to be done from my end? I'm unable to view them.\r\n\r\nThere were internal failures. @gbaned I think the internal failures are not related to this PR. We can probably land it by triggering the process again. Can you help on this?", "If you need to make another submit attempt, you can just re-approve the internal CL and it'll retry to submit."]}, {"number": 53711, "title": "(lite) Fix lifetime documentation of TfLiteModel", "body": "Fixes misleading documentation of `TfLiteInterpreterCreate`. It was stating that a model was destroyable immediately after creating an interpreter, see details in [this issue](https://github.com/tensorflow/tensorflow/issues/53628).", "comments": []}, {"number": 53710, "title": "[PluggableDevice] Add DEVICE_DEFAULT registration to dataset ops", "body": null, "comments": ["Hi,\r\n\r\nCould someone review this PR? This is something that we need for the release of our pluggable device plugin.\r\n\r\nThank you!", "See https://github.com/tensorflow/tensorflow/pull/50605#issuecomment-919453092\r\n\r\nIt seems `tf.data` kernels should not use `DEVICE_DEFAULT`", "@mihaimaruseac What should pluggable devices do to support those? As far as I know there's no way from the pluggable device API to handle `tf.data` kernels. Are new APIs similar to what was done for resource tensors required?", "Cc'ing @jsimsa for the answer. IIUC, `tf.data` is meant to be run only on TF native devices be design.", "@penpornk Thank you for the reply. I guess what I'm trying to understand here is what advantages registering those ops for the GPU devices gives them in terms of performance or memory usage. In our TensorFlow 1.15 fork, we registered those ops for our custom device with the same priority as the GPU devices. Since it's not possible with the TF2 pluggable devices, what would be the impact of performance or memory usage of not having a registration for those operators, compared to the native GPU device?"]}, {"number": 53709, "title": "No matching distribution found for tensorflow==2.4.1", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC OS Big Sur 11.6\r\n- TensorFlow installed from (source or binary): via pip \r\n- TensorFlow version: trying to install 2.4.1\r\n- Python version: 3.9.6\r\n- Installed using virtualenv? pip? conda?: pip \r\n\r\n\r\n**Describe the problem**\r\n\r\nVia jupyter notebooks on vs code, trying to run `!pip3 install tensorflow==2.4.1` but getting \r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.4.1 (from versions: 2.5.0rc0, 2.5.0rc1, 2.5.0rc2, 2.5.0rc3, 2.5.0, 2.5.1, 2.5.2, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.8.0rc0)\r\nERROR: No matching distribution found for tensorflow==2.4.1\r\n```\r\n", "comments": ["@zohimchandani Could you please try to latest stable `TF v2.7.0` and refer to [build from source](https://www.tensorflow.org/install/source#macos), [tested build configurations](https://www.tensorflow.org/install/source#macos).Please have a look at the  similar [issue1](https://github.com/tensorflow/tensorflow/issues/47205), [issue2](https://github.com/tensorflow/tensorflow/issues/43526)and  [system requirements](https://www.tensorflow.org/install/pip#system-requirements) .Please let us know if it helps?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "This is because you are trying to install tensorflow 2.4 with python 3.9. As you see [here](https://www.tensorflow.org/install/source_windows#gpu), tensorflow 2.4 is only supported with python 3.6 / 3.7 / 3.8 .\r\n\r\nCreates an environment with one of these python versions and then you could install tensorflow 2.4 .", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53709\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53709\">No</a>\n"]}, {"number": 53707, "title": "TFlite docker build fail using bazel.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3\r\n- Python version: 2.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n\r\n\r\n**Describe the problem**\r\nI had build the tflite successully without using docker, but when i try to use docker to build it automatically. It fails.\r\nMany Thanks for you help!\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nbazel build --config=monolithic --define=with_select_tf_ops=true \\\r\n        --cxxopt=\"-std=c++14\" \\\r\n        -c opt \\\r\n        //tensorflow/lite:libtensorflowlite.so \\\r\n        --verbose_failures\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=186\r\nINFO: Reading rc options for 'build' from /root/dev/tensorflow/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /root/dev/tensorflow/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /root/dev/tensorflow/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env ANDROID_NDK_HOME=${ANDROID_NDK_HOME}/android-ndk-r${NDK_VERSION} --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /root/dev/tensorflow/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /root/dev/tensorflow/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:monolithic in file /root/dev/tensorflow/tensorflow/.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:linux in file /root/dev/tensorflow/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /root/dev/tensorflow/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /root/.cache/bazel/_bazel_root/2c26c1884d3743801d5d0e3029f61ed7/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nWARNING: /root/dev/tensorflow/tensorflow/tensorflow/core/BUILD:1749:1: in linkstatic attribute of cc_library rule //tensorflow/core:lib_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'cc_library', the error might have been caused by the macro implementation\r\nWARNING: /root/dev/tensorflow/tensorflow/tensorflow/core/BUILD:2161:1: in linkstatic attribute of cc_library rule //tensorflow/core:framework_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'tf_cuda_library', the error might have been caused by the macro implementation\r\nWARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Repository aws-checksums instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule third_party_http_archive defined at:\r\n  /root/dev/tensorflow/tensorflow/third_party/repo.bzl:219:28: in <toplevel>\r\nWARNING: Download from https://mirror.tensorflow.org/github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz failed: class javax.net.ssl.SSLHandshakeException No subject alternative DNS name matching mirror.tensorflow.org found.\r\nWARNING: Download from https://github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz failed: class java.io.IOException connect timed out\r\nERROR: An error occurred during the fetch of repository 'aws-checksums':\r\n   java.io.IOException: Error downloading [https://mirror.tensorflow.org/github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz, https://github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz] to /root/.cache/bazel/_bazel_root/2c26c1884d3743801d5d0e3029f61ed7/external/aws-checksums/v0.1.5.tar.gz: connect timed out\r\nINFO: Repository llvm-project instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule tf_http_archive defined at:\r\n  /root/dev/tensorflow/tensorflow/third_party/repo.bzl:134:19: in <toplevel>\r\nINFO: Repository aws-c-common instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule third_party_http_archive defined at:\r\n  /root/dev/tensorflow/tensorflow/third_party/repo.bzl:219:28: in <toplevel>\r\nINFO: Repository aws-c-event-stream instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule third_party_http_archive defined at:\r\n  /root/dev/tensorflow/tensorflow/third_party/repo.bzl:219:28: in <toplevel>\r\nERROR: /root/.cache/bazel/_bazel_root/2c26c1884d3743801d5d0e3029f61ed7/external/aws/BUILD.bazel:12:1: @aws//:aws depends on @aws-checksums//:aws-checksums in repository @aws-checksums which failed to fetch. no such package '@aws-checksums//': java.io.IOException: Error downloading [https://mirror.tensorflow.org/github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz, https://github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz] to /root/.cache/bazel/_bazel_root/2c26c1884d3743801d5d0e3029f61ed7/external/aws-checksums/v0.1.5.tar.gz: connect timed out\r\nERROR: Analysis of target '//tensorflow/lite:libtensorflowlite.so' failed; build aborted: no such package '@aws-checksums//': java.io.IOException: Error downloading [https://mirror.tensorflow.org/github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz, https://github.com/awslabs/aws-checksums/archive/v0.1.5.tar.gz] to /root/.cache/bazel/_bazel_root/2c26c1884d3743801d5d0e3029f61ed7/external/aws-checksums/v0.1.5.tar.gz: connect timed out\r\nINFO: Elapsed time: 169.197s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (209 packages loaded, 12517 targets configured)\r\n```\r\n", "comments": ["@groovemaxRong! Could you check this[ thread ](https://github.com/tensorflow/tensorflow/issues/47348#issuecomment-785056248)and try again ?  @sachinprasadhs ! Could you please look at this issue ?", "yeah, it works! Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53707\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53707\">No</a>\n"]}, {"number": 53706, "title": "Segmentation fault: 11", "body": "pardon me for being new veteran on this. i still cannot figure this error after i try to train the model, it end up with this error. after few search, have a clue that it might be Mac issue.\r\n\r\nMy mac is macOS Catalina and version 10.15.2.\r\n\r\nThank you in advance ^^\r\n\r\n![Screen Shot 2022-01-09 at 10 24 30 PM](https://user-images.githubusercontent.com/15898245/148703921-5db43328-a512-4ba5-a343-ade0aaeaee86.png)\r\n ", "comments": ["@gyurmey \r\nIn order to reproduce the issue reported here, could you please provide the error in text format, complete code , the dataset , tensorflow version you are using?Thanks!", "@sushreebarsa thanks for the reply ^^\r\n\r\nerror is Segmentation fault: 11 \r\ntensorflow version is 2.7.0\r\n", "@gyurmey \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53705, "title": "I have made wrapper of Tensor ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\nLatest Github release\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nGraph\r\n\r\n**Will this change the current api? How?**\r\n\r\nnope\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone\r\n\r\n**Any Other info.**\r\n\r\n\r\nI have started wrapper of Tensorflow using cppflow. I need some help with graph. It is unclear how it works in C API. I am thinking of wrapping C++ api. It works for all swig languages. The question is does C/api suppot the graph enough to use it or should I wrap the C++ API?", "comments": ["If you could give some small example of how to create graph, add tensor ops to it. Then, I can do anything with it.", "I can do anytihng imaginable with tensor, other stuff is not very hard to figure out. ", "I can make a graph and set input and output shape. Problem is I can't add operation to it.", "Ah, now I see how to do it.", "I figured it out so you can close it."]}, {"number": 53704, "title": "Conflict between two libraries of source-built and prebuilt (conda) TensorFlow", "body": "Hi Folks,\r\n\r\nI installed prebuilt TensorFlow (TF) 2.6 using conda and compiled the same version from the source using bazel. I am able to call/import TF in Python but when adding the built shared libraries to the lib env variable (`LD_LIBRARY_PATH`), I got the following error about a conflict between two TF libraries.\r\n\r\nThese codes work well:\r\n\r\n```sh\r\nPython 3.9.0 (default, Nov 15 2020, 14:28:56)\r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2022-01-09 16:31:59.785672: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2022-01-09 16:31:59.785714: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```\r\n\r\nBut these codes yielded the error in question (suppose that shared libs are stored in `/usr/local/tensorflow/lib/`):\r\n\r\n```\r\nexport LD_LIBRARY_PATH=/usr/local/tensorflow/lib/:$LD_LIBRARY_PATH\r\n\r\nPython 3.9.0 (default, Nov 15 2020, 14:28:56)\r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: /home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: descriptor_table_tensorflow_2fcore_2fprotobuf_2fdata_5fservice_2eproto\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"/home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 79, in <module>\r\n    raise ImportError(\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: /home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: descriptor_table_tensorflow_2fcore_2fprotobuf_2fdata_5fservice_2eproto\r\n\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\r\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\r\n>>>\r\n```\r\n", "comments": ["Hi @rangsimanketkaew ! The error is about configuring GPU support . You need to point the respective Cuda files after downloading them in your local machine. You can refer these thread to configure GPU support on your machine.Link [1](url) ,[2 ](https://www.analyticsvidhya.com/blog/2020/11/how-to-download-install-and-use-nvidia-gpu-for-tensorflow-on-windows/) Thank you!", "> Hi @rangsimanketkaew ! The error is about configuring GPU support . You need to point the respective Cuda files after downloading them in your local machine. You can refer these thread to configure GPU support on your machine.Link [1](url) ,[2 ](https://www.analyticsvidhya.com/blog/2020/11/how-to-download-install-and-use-nvidia-gpu-for-tensorflow-on-windows/) Thank you!\r\n\r\nIIUC, the error was at this line `ImportError: /home/rangsiman/miniconda3/envs/tf_cc/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: descriptor_table_tensorflow_2fcore_2fprotobuf_2fdata_5fservice_2eproto` which I think that the conflict is relevant to the protobuf, not GPU support.", "Ok @rangsimanketkaew ! First suggestion on LD_LIBRARY_PATH was upon this[ thread](https://www.tensorflow.org/install/gpu#linux_setup). For the this [error ](https://github.com/tensorflow/tensorflow/issues/53704#issuecomment-1009240177), You can try upgrading the protobuf version . Attaching relevant thread for  [Reference ](https://github.com/tensorflow/tensorflow/issues/12326#issuecomment-346314550). Please don't mix conda installation of Tensorflow with Build from source installation in one environment. Thank you.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi @mohantym Thanks for the suggestions. I've tried upgrading the protobuf to the newer version (4.x) but it didn't work, still get the same error.", "Hi @Saduf2019 ! Could you please look at this issue?", "Could you please try creating a new environment and try only with build from source and let us know if you face any error. Thanks!", "Thanks for the reply. Yes, building from source in a newly created en works for me.", "If your issue is resolved, could you please close this issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53704\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53704\">No</a>\n"]}, {"number": 53703, "title": "unicom lucy", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["asjd jdppojdas", "@luzm123 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53703\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53703\">No</a>\n"]}]