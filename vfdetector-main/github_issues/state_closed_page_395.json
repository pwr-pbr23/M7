[{"number": 42104, "title": "Fixed the Windows build for TF Scala.", "body": "This PR introduces some changes that I found necessary when trying to support a windows build for TF Scala. Specifically, I added a build target for building `tensorflow_framework.lib` and made some changes to export some missing symbols that are used in the TF Scala checkpoint reader.\r\n\r\nI also had a question: is there currently a single bazel build target that can be used for building `tensorflow.dll`, `tensorflow.lib`, `tensorflow_framework.dll`, and `tensorflow_framework.lib`, all at once? Thanks!\r\n\r\ncc @alextp ", "comments": ["Also, there is one remaining linking error that I'm not sure how to fix:\r\n\r\n```\r\nerror LNK2019: unresolved external symbol \"class tensorflow::TensorShapeProtoDefaultTypeInternal tensorflow::_TensorShapeProto_default_instance_\" (?_TensorShapeProto_default_instance_@tensorflow@@3VTensorShapeProtoDefaultTypeInternal@1@A) referenced in function \"private: struct std::pair<class std::unique_ptr<class std::unordered_map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::TensorShape,struct std::hash<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::TensorShape> > >,struct std::default_delete<class std::unordered_map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::TensorShape,struct std::hash<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::TensorShape> > > > >,class std::unique_ptr<class std::unordered_map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,enum tensorflow::DataType,struct std::hash<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,enum tensorflow::DataType> > >,struct std::defau\r\nlt_delete<class std::unordered_map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,enum tensorflow::DataType,struct std::hash<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,enum tensorflow::DataType> > > > > > __cdecl tensorflow::checkpoint::CheckpointReader::BuildV2VarMaps(void)\" (?BuildV2VarMaps@CheckpointReader@checkpoint@tensorflow@@AEAA?AU?$pair@V?$unique_ptr@V?$unordered_map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VTensorShape@tensorflow@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VTensorShape@tensorflow@@@std@@@2@@std@@U?$default_delete@V?$unordered_map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VTensorShape@tensorflow@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@VTensorShape@tensorflow@@@std@@@2@@std@@@2@@std@@V?$unique_ptr@V?$unordered_map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@W4DataType@tensorflow@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@W4DataType@tensorflow@@@std@@@2@@std@@U?$default_delete@V?$unordered_map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@W4DataType@tensorflow@@U?$has\r\nh@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@W4DataType@tensorflow@@@std@@@2@@std@@@2@@2@@std@@XZ) [C:\\Users\\antho\\Development\\Repositories\\tensorflow_scala\\modules\\jni\\target\\native\\windows-x86_64\\build\\tensorflow_jni.vcxproj]\r\n\r\n```", "@alextp sorry I think I dismissed your reviews somehow when I accidentally added and removed some other changes while trying to fix that last linking error.", "Oof protobuf default instances are nasty things. This is some integration with the proto library I don't fully understand.", "Thanks Alex! I got it working by refactoring a bit how I use the bundle reader. :)", "We had to roll this back because it broke tests on windows. Can you look at the failures? They are in the rollback message: https://github.com/tensorflow/tensorflow/commit/04d770b603809a9c89974e77576af238d422fde5"]}, {"number": 42103, "title": "ImportError: DLL load failed with error code 3221225501 while importing _pywrap_ tensorflow_internal in Python3.8", "body": "<em>How i can solve this error while import TensorFlow</em>\r\n\r\n**System information:**\r\n - Python 3.8.4.\r\n - Windows 7 Ultimate 64bit.\r\n - 2GB Ram.\r\n - Intel(R) Celeron(R) CPU N2840 @ 2.16GHz.\r\n - Intel(R) HD Graphics.\r\n\r\n\r\n**Error Text/Logs:**\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"G:\\Users\\Abdo-Prgraming\\AppData\\Roaming\\Python\\Python38\\site-packages\\te\r\nnsorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed with error code 3221225501 while importing _pywrap_\r\ntensorflow_internal\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"G:\\Users\\Abdo-Prgraming\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"G:\\Users\\Abdo-Prgraming\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"G:\\Users\\Abdo-Prgraming\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"G:\\Users\\Abdo-Prgraming\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"G:\\Users\\Abdo-Prgraming\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"G:\\Users\\Abdo-Prgraming\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed with error code 3221225501 while importing _pywrap_tensorflow_internal\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["@AIabdoPr \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\n\r\nTF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.\r\n\r\nIf you have above configuration and using Windows platform -\r\nTry adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\r\nRefer windows [setup guide](https://www.tensorflow.org/install/gpu#windows_setup).", "@AIabdoPr \r\n\r\n I guess this is an issue with mixing two or more versions.\r\n\r\n1.Check multiple tensorflow folder under 'site-packages/` folder and delete them\r\n2.check environmental file and check for path system variable, if the path are not pointing to correct version of software (python), then adjust accordingly\r\n3.Uninstalled tensorflow-->uninstalled python-->restart computer\r\n4. python--> set and check system variables in the environmental file\r\n5.install TF and then check whether everything is working as expected or not.\r\n\r\nAlso, i guess this issue you might not have faced with python 3.7 or 3.6.Can you please see whether you are facing this issue with python 3.6 , 3.7 also or only with python 3.8 \r\n\r\nThanks!", "How i can solve this error while import TensorFlow\r\n\r\n(TF_PY) C:\\Users\\hp>python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.ra\r\nndom.normal([1000, 1000])))\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hp\\anaconda3\\envs\\TF_PY\\lib\\site-packages\\tensorflow\\python\\pyw\r\nrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: Le module sp\u00e9cifi\u00e9 est introuvable.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\hp\\anaconda3\\envs\\TF_PY\\lib\\site-packages\\tensorflow\\__init__.p\r\ny\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\hp\\anaconda3\\envs\\TF_PY\\lib\\site-packages\\tensorflow\\python\\__i\r\nnit__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\hp\\anaconda3\\envs\\TF_PY\\lib\\site-packages\\tensorflow\\python\\eag\r\ner\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\hp\\anaconda3\\envs\\TF_PY\\lib\\site-packages\\tensorflow\\python\\pyw\r\nrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\hp\\anaconda3\\envs\\TF_PY\\lib\\site-packages\\tensorflow\\python\\pyw\r\nrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\hp\\anaconda3\\envs\\TF_PY\\lib\\site-packages\\tensorflow\\python\\pyw\r\nrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: Le module sp\u00e9cifi\u00e9 est introuvable.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42103\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42103\">No</a>\n"]}, {"number": 42102, "title": "Utilize TensorFormat", "body": "", "comments": ["/cc @smit-hinsu for visibility."]}, {"number": 42101, "title": "plot_model not connecting layers between models correctly", "body": "Tested in tf 2.3.0\r\n\r\nWhen building models from other models, the graph generated by `plot_model` does not create arrows between the correct outputs and inputs. See e.g. \r\n\r\n```\r\ninn = tf.keras.Input((1,),name=\"in1\")\r\nd1 = tf.keras.layers.Dense(1, name=\"d1\")(inn)\r\nd2 = tf.keras.layers.Dense(1, name=\"d2\")(inn)\r\nm1 = tf.keras.Model(inputs=inn, outputs=[d1,d2], name=\"model1\")\r\nin2_1 = tf.keras.Input((1,),name=\"in2_1\")\r\nin2_2 = tf.keras.Input((1,),name=\"in2_2\")\r\nm2 = tf.keras.Model(inputs=[in2_1,in2_2],outputs=[in2_1 + in2_2],name=\"model2\")\r\n\r\n# combined model\r\nin0 = tf.keras.Input((1,),name=\"in0\")\r\nm = tf.keras.Model(inputs=in0, outputs=m2(m1(in0)))\r\ntf.keras.utils.plot_model(m, show_shapes=True, expand_nested=True)\r\n```\r\noutputs:\r\n![image](https://user-images.githubusercontent.com/18280829/89551998-7bb23700-d803-11ea-961f-127eb8e71413.png)\r\n\r\nAs you can see, the first output of the first model isn't connected to the first output of the second model\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/1b993bb4c0c6c8cea3643816fe824650/42101-2-3.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/9d299572494a983430de30536ad0d8b2/42101-2-3.ipynb). Please find the attached gist. Thanks!", "I have another issue with plot_model while using it for a double-nested model. I thought this might be the right place to post it. I will remove it if inappropriate. When I create a model_AE from 2 nested models (encoder,decoder), plot_model plots them correctly. But when I further nest model_AE with model_HR into the final end2end model_e2e, the plotted model is incorrect. The arrow goes from input of model_AE to input of model_HR but it should go from output of model_AE to input of model_HR. I have attached the code to reproduce it below and also the generated figure. Tried with Anaconda on Mac (tensorflow 2.0) and Windows (tensorflow 2.1). Any help is appreciated. Thanks.\r\n\r\nUPDATE 1: I verified that the problem is happening only when double-nesting (nesting a nested model). For this, I made the model_AE without nested (encoder, decoder) models and the plot_model worked correctly. As of now, I think this is a bug.\r\n\r\n    import tensorflow as tf\r\n    from tensorflow.keras import layers\r\n    from tensorflow import keras\r\n    \r\n    HR_win_len=200\r\n    in_shape=(400,)\r\n    latent_shape=(16,)\r\n    \r\n    #Simple AE model\r\n    encoder_in = layers.Input(shape=in_shape,name=\"encoder_in\")\r\n    x = layers.Dense(256,activation='relu')(encoder_in)\r\n    encoder_out = layers.Dense(latent_shape[0],activation='relu')(x)\r\n    encoder = keras.Model(encoder_in, encoder_out, name=\"encoder\")\r\n    \r\n    decoder_in = layers.Input(shape=latent_shape,name=\"decoder_in\")\r\n    x = layers.Dense(256,activation='relu')(decoder_in)\r\n    decoder_out = layers.Dense(in_shape[0])(x)\r\n    decoder = keras.Model(decoder_in, decoder_out, name=\"decoder\")\r\n    \r\n    #Connect encoder and decoder and create model_AE\r\n    AE_in = keras.Input(shape=in_shape, name=\"AE_in\")\r\n    sig_hat = decoder(encoder(AE_in))\r\n    model_AE = keras.Model(AE_in, sig_hat, name=\"AE\")\r\n    \r\n    #Create model_HR\r\n    expand_dims = layers.Lambda(lambda x: tf.expand_dims(x,axis=-1), \r\n                                    name='expand_dims')\r\n    rnn = layers.GRU(64, return_sequences=True, return_state=True)\r\n    sig_in = layers.Input(shape=in_shape)\r\n    x = expand_dims(sig_in)\r\n    _, final_state=rnn(x[:,:HR_win_len,:]) #warm-up RNN\r\n    rnn_out, _ = rnn(x[:,HR_win_len:,:],initial_state=final_state)\r\n    HR_hat=layers.Conv1D(filters=1,kernel_size=1, strides=1,padding='same',\r\n                         activation=None,name='Conv_{}'.format(1))(rnn_out)\r\n    HR_hat=layers.Flatten()(HR_hat)\r\n    model_HR = keras.Model(sig_in, HR_hat, name=\"HR\")\r\n    \r\n    #Connect model_AE and model_HR and create model_e2e\r\n    e2e_in = keras.Input(shape=in_shape, name=\"e2e_in\")\r\n    e2e_out=model_HR(model_AE(e2e_in))\r\n    model_e2e = keras.Model(e2e_in, e2e_out, name=\"e2e\")\r\n    \r\n    tf.keras.utils.plot_model(model_e2e,to_file='e2e.png', \r\n    dpi=200, show_shapes=True, show_layer_names=True, expand_nested=True)\r\n[![enter image description here][1]][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/y5TzS.png", "Here to confirm that this is happening for me as well on Tensorflow 2.4.1. Here's MWE:\r\n```\r\nfrom tensorflow import keras\r\n\r\na = keras.Input(shape=(1,), name=\"a\")\r\nb = keras.Input(shape=(1,), name=\"b\")\r\nc = keras.layers.Concatenate(name=\"c\")([a, b])\r\ninner = keras.Model(inputs=[a, b], outputs=c, name=\"inner\")\r\n\r\nx = keras.Input(shape=(1,), name=\"x\")\r\ny = keras.Input(shape=(1,), name=\"y\")\r\nz = inner([x, y])\r\nouter = keras.Model(inputs=[x, y], outputs=z, name=\"outer\")\r\n\r\nkeras.utils.plot_model(outer, show_shapes=True, expand_nested=True)\r\n```\r\n\r\n![model](https://user-images.githubusercontent.com/13689478/109202594-2757b000-7771-11eb-9dd4-a39a34305455.png)\r\n", "Was able to reproduce the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/6a8bfaddbd127216f327724b2d774658/untitled103.ipynb)..Thanks !", "Issue still persists in TF `2.7` and TF-nightly `2.8.0-dev20211120`. Please find the [gist here](https://colab.sandbox.google.com/gist/sanatmpa1/39abf88d40f910bb1aae42640aae283b/untitled103.ipynb).", "@will-cern Agree. This is still an issue. Is it possible for you to open this issue in keras-team/keras repo as the development of Keras moved to that repo.\r\n\r\nKeras development moved to another repository to focus on only keras. Could you please repost this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42101\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42101\">No</a>\n"]}, {"number": 42100, "title": "Add GCS Filesystem Helper", "body": "@mihaimaruseac \r\nThis PR adds some gcs filesystem helper functions we are missing.", "comments": []}, {"number": 42099, "title": "SequenceFeatures cant get embedding with shared_embedding_columns", "body": "```\r\nimport tensorflow as tf # tf.version==1.14.0\r\n\r\ndef test_crossed_column():\r\n    \"\"\" crossed column\u6d4b\u8bd5 \"\"\"\r\n    features = {\r\n        'price': [[0,1], [1,0], [2,1]],\r\n        'color': [[1,2], [2,2], [0,0]]\r\n    }\r\n    price = tf.feature_column.sequence_categorical_column_with_vocabulary_list('price', [1,2], default_value=-1)\r\n    color = tf.feature_column.sequence_categorical_column_with_vocabulary_list('color', [1,2], default_value=-1)\r\n    price = tf.feature_column.shared_embedding_columns([price, color], 4)\r\n    price_layer = tf.keras.experimental.SequenceFeatures(price)\r\n    x = price_layer(features)\r\n    with tf.Session() as session:\r\n        session.run(tf.global_variables_initializer())\r\n        session.run(tf.tables_initializer())\r\n        print(session.run([x]))\r\ntest_crossed_column()\r\n```\r\nwith the code, tf cant get embedding value with shared_embdding_columns, for the reason\r\n```\r\nValueError: Items of feature_columns must be a FeatureColumn. Given (type <class 'tensorflow.python.feature_column.feature_column._SharedEmbeddingColumn'>): _SharedEmbeddingColumn(categorical_column=_SequenceCategoricalColumn(categorical_column=VocabularyListCategoricalColumn(key='price', vocabulary_list=(1, 2), dtype=tf.int64, default_value=-1, num_oov_buckets=0)), dimension=4, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f8fd783a850>, shared_embedding_collection_name='color_price_shared_embedding', ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True).\r\n```\r\nWhy _SharedEmbeddingColumn isn't a type of fc?", "comments": ["@alansplaza \r\nPlease paste the error message (using makrdown formatting around it) instead of screenshotting. Screenshots are not searchable so they don't help in looking for the issue and also don't help other people having the same error from finding about the issue.\r\nAlso please provide complete stand alone code such that we can replicate the issue reported, as i am facing different error, [gist for the same](https://colab.research.google.com/gist/Saduf2019/06c0ade874976589c30c9865fb20db36/untitled330.ipynb).", "> @alansplaza\r\n> Please paste the error message (using makrdown formatting around it) instead of screenshotting. Screenshots are not searchable so they don't help in looking for the issue and also don't help other people having the same error from finding about the issue.\r\n> Also please provide complete stand alone code such that we can replicate the issue reported, as i am facing different error, [gist for the same](https://colab.research.google.com/gist/Saduf2019/06c0ade874976589c30c9865fb20db36/untitled330.ipynb).\r\n\r\nMy tf version is '1.14.0'. Thanks.", "> @alansplaza\r\n> Please paste the error message (using makrdown formatting around it) instead of screenshotting. Screenshots are not searchable so they don't help in looking for the issue and also don't help other people having the same error from finding about the issue.\r\n> Also please provide complete stand alone code such that we can replicate the issue reported, as i am facing different error, [gist for the same](https://colab.research.google.com/gist/Saduf2019/06c0ade874976589c30c9865fb20db36/untitled330.ipynb).\r\n\r\nThe error and complete code is updated.", "\r\nI ran the code on 1.15 and tf nightly , i am able to replicate the issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/a48af59e613e7ee81f4f4f9a0129a6b0/untitled343.ipynb).\r\n\r\n\r\n", "Keras layers like this are known to not work with shared embeddings, even for `DenseFeatures`.\r\nI'd recommend you trying out [keras preprocessing layers](https://github.com/tensorflow/community/blob/master/rfcs/20191212-keras-categorical-inputs.md#workflow-1----official-guide-on-how-to-replace-feature-columns-with-kpl)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42099\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42099\">No</a>\n"]}, {"number": 42097, "title": "BUG: log_cosh alias missing results in round-trip serialization failure", "body": "See https://colab.research.google.com/drive/1AfOBJTHOYngsPEn3Fo4rkeD4kUJG3eil#scrollTo=_pyJctK__Btb\r\n\r\n```python3\r\nfrom tensorflow.keras.metrics import deserialize, serialize\r\n\r\ndeserialize(\"mse\")  # returns tensorflow.python.keras.losses.mean_squared_error\r\n# so let's try deserialize(\"mean_squared_error\")\r\ndeserialize(\"mean_squared_error\") # returns tensorflow.python.keras.losses.mean_squared_error\r\n\r\ndeserialize(\"logcosh\")  # returns tensorflow.python.keras.losses.log_cosh\r\n# so let's try deserialize(\"log_cosh\")\r\ndeserialize(\"log_cosh\") # ValueError: Unknown metric function: log_cosh\r\n\r\ndeserialize(serialize(deserialize(\"mse\")))  # roundtrip works\r\n\r\ndeserialize(serialize(deserialize(\"logcosh\")))  # roundtrip breaks\r\n```", "comments": ["And more generally, is there any way to convert from the shorthand to the \"official\" names for metric/loss functions, or is the best way for me to make a function like:\r\n\r\n```python3\r\ndef get_metric_full_name(name: str) -> str:\r\n    mapping = {\r\n        \"acc\": \"accuracy\",\r\n        \"bce\": \"binary_crossentropy\",\r\n        \"mse\": \"mean_square_error\",\r\n        \"mean_squared_error\": \"mean_square_error\",\r\n        ...\r\n    }\r\n    if name.lower() in mapping.keys():\r\n        return mapping[name.lower()]\r\n    if name in mapping.values():\r\n        return name\r\n    raise ValueError(f\"Unknonw loss/metric '{name}'\")\r\n```\r\n\r\nWhere I'm just copy/pasting the names and hoping they don't change.\r\n\r\nOr maybe this is better (not sure, just asking):\r\n```python3\r\nfrom tensorflow.keras.metrics import deserialize\r\n\r\ndef get_metric_full_name(name: str) -> str:\r\n    # deserialize returns the actual function, then get it's name\r\n    # to keep a single consistent name for the metric\r\n    return getattr(deserialize(name), __name__)\r\n```", "@pavithrasv do you think you could help answer the question in my above comment? \r\n\r\nAnd is there any chance this will be backported to older TF versions?\r\n\r\nThanks!", "@pavithrasv testing with the latest `tf-nightly` it seems that neither `log_cosh` nor `logcosh` are in `tensorflow.keras.metrics.__init__.py`. Apparently that is an auto-generated file. Do we need to add these two to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/api/golden/v2/tensorflow.keras.metrics.pbtxt or something? What am I missing here? Happy to try to help fix anything. Thanks!", "Yes. We currently have `'keras.losses.log_cosh', 'keras.losses.logcosh'` exports for the logcosh function. Can you add `'keras.metrics.log_cosh', 'keras.metrics.logcosh'` to this list?"]}, {"number": 42096, "title": "cross-compile Inference Diff tool", "body": "Hello guys,\r\n\r\nis it posible to cross-compile the [Inference Diff tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/inference_diff) - I mean with bazel ...\r\nIf so, can you please indicate how?\r\n\r\nThank you", "comments": ["Surely yes\r\n```\r\nbazel build -c opt \\\r\n  --config=android_arm64 \\\r\n  //tensorflow/lite/tools/evaluation/tasks/inference_diff:run_eval\r\n```\r\nexample in its REAME.md is to cross-compile it on x86_64 Linux or macOS for arm64 Android devices :-)\r\n\r\nWhat's your target device?", "Target device: ARM64 with Linux", "@peter197321 Is this still an issue? Were you able to build successfully with https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/inference_diff#running-the-binary-on-android?", "I wanna build for aarch64 Linux device and not android.", "`--config elinux_aarch64` is supposed to work, but it doesn't. It seems there is some setting / configuration problem.", "@peter197321 Could you please let us know if the issue still persists ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42096\">No</a>\n"]}, {"number": 42095, "title": "add /hip suffix to find hip path", "body": "Resolves #42081 \r\nFix courtesy of @Imajie\r\n\r\nThis PR helps move along the build when building tensorflow from source.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42095) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42095) for more info**.\n\n<!-- ok -->"]}, {"number": 42094, "title": "Compilation and Linking errors in MSVC Debug builds", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**\r\n- TensorFlow installed from (source or binary): **Source**\r\n- TensorFlow version: **2.3.0**\r\n- Python version: **3.8.5**\r\n- Bazel version (if compiling from source): **3.3.1**\r\n- GCC/Compiler version (if compiling from source): **Visual Studio 2019**\r\n\r\nAlso reproduced in master (4be925e3)\r\n\r\n**Describe the problem**\r\n\r\nMultiple compilation errors when building in Windows Debug mode due to [C4716 ('function' must return a value)](https://docs.microsoft.com/en-us/cpp/error-messages/compiler-warnings/compiler-warning-level-1-c4716?view=vs-2019)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\nbazel build --config=dbg //tensorflow:tensorflow_framework\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nCompilation errors due to C4716 being elevated to an error in debug mode, eg in [`device_base.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/device_base.cc)\r\n\r\n```.py \r\nconst DeviceAttributes& DeviceBase::attributes() const {\r\n  LOG(FATAL) << \"Device does not implement attributes()\";\r\n}\r\n```\r\n\r\nSince AFAIK tensorflow does not use C++ exceptions (I believe it compiles with `-fno-exceptions` on Clang/GCC so I assume this is official policy) this would have to be handled with a `pragma warning (disable: 4217)` somewhere in the code. Will submit a suitable PR when I have time if no one gets to it first.\r\n", "comments": ["Unfortunately there's more to this than simply disabling the warning. MSVC debug builds link in a number of symbols that would normally be excluded by compile-time type checks, meaning the the Windows CPU build will not link.\r\n\r\nFor example, in `spacetodepth_op.cc`\r\n\r\n```.cpp\r\nif (std::is_same<Device, GPUDevice>::value) {\r\n   ...\r\n   functor::SpaceToDepthOpFunctor<GPUDevice, int32, FORMAT_NCHW> functor;\r\n   ...\r\n```\r\n\r\n`functor` would never be instantiated on a CPU build where the `GPUDevice` is not available. In a release build this is not a problem. However, in a debug build the code is still generated and linked, even though it would never be executed - which results in a linking error because the `GPUDevice` instantiation is not available\r\n\r\n```\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,unsigned __int64,1>::operator() ... referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned __int64>::Compute(class tensorflow::OpKernelContext *)\"\r\n```", "I believe this issue can be resolved with\r\n\r\n```.cpp\r\nif constexpr(std::is_same<Device, GPUDevice>::value) {...}\r\n```\r\n\r\nBut requires C++17 support to be enabled. \r\n\r\nAs Bazel has been attempting to link the `tensorflow_cc.dll` for over an hour now, I may just have to give up on the MSVC debug build anyway.", "As much as I would like to help, I never was able to make the debug build work in any OSs with TF.\r\nEspecially on windows, I do not have much experience.", "#42676 might be able to fix this", "This should be done by #42676, I think", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42094\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42094\">No</a>\n"]}, {"number": 42093, "title": "PIP Install incorrct hashes", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Elementary OS 5.1 Hera\r\n- Python version: 3.6\r\n- Installed via pip in a python venv \r\n\r\n**Describe the problem**\r\nUnable to install via pip due to incorrect hashes\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nuser1@XXXX-PC:~/Desktop/Python/ChatBotAI$ python -m venv chatenv python=3.6\r\nuser1@XXXX-PC:~/Desktop/Python/ChatBotAI$ pip install tensorflow\r\n\r\n**Any other info / logs**\r\nCollecting tensorflow\r\n  Using cached tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\r\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\r\n    tensorflow from https://files.pythonhosted.org/packages/16/89/f2d29c2eafc2eeafb17d5634340e06366af904d332341200a49d954bce85/tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl#sha256=92430b6e91f00f38a602c4f547bbbaca598a3a90376f90d5b2acd24bc18fa1d7:\r\n        Expected sha256 92430b6e91f00f38a602c4f547bbbaca598a3a90376f90d5b2acd24bc18fa1d7\r\n             Got        49d42fc43402a2d9fb53aaea084c07c0fdddfd31af0995da463dd31ecc38f41e\r\n\r\n\r\n\r\n", "comments": ["@userwon,\r\nTry add `--no-cache-dir` flag to the command as shown below, and check if it works.\r\n`pip install tensorflow --no-cache-dir`\r\n\r\nReference to similar StackOverflow issues [#1](https://stackoverflow.com/a/40184923), [#2](https://stackoverflow.com/a/40400414).\r\n\r\nThanks!", "That seems to of resolved the issue thank you, enjoy your day :) ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42093\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42093\">No</a>\n"]}, {"number": 42092, "title": "getting input/output dtype as  float32  after converting  keras mnist model to integer quantized tflite model ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 16.04\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (or github SHA if from source): 2.2\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\nimport tensorflow as tf \r\nimport numpy as np\r\n\r\n\r\n\r\n# Load MNIST dataset\r\nmnist = tf.keras.datasets.mnist\r\n\r\n\r\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n\r\n# Normalize the input image so that each pixel value is between 0 to 1.\r\ntrain_images = train_images.astype(np.float32) / 255.0\r\ntest_images = test_images.astype(np.float32) / 255.0\r\n\r\n# Define the model architecture\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.InputLayer(input_shape=(28, 28)),\r\n  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\r\n  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n  tf.keras.layers.Flatten(),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n\r\n# Train the digit classification model\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(\r\n                  from_logits=True),\r\n              metrics=['accuracy'])\r\nmodel.fit(\r\n  train_images,\r\n  train_labels,\r\n  epochs=5,\r\n  validation_data=(test_images, test_labels)\r\n)\r\nmodel.save(\"mnist.h5\")\r\n\r\nmodel_ = tf.keras.models.load_model(\"mnist.h5\")\r\n\r\n\r\n\r\n\r\ndef representative_data_gen():\r\n  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\r\n    # Model has only one input so each data point has one element.\r\n    yield [input_value]\r\n\r\n\r\n\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model_)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\n# Ensure that if any ops can't be quantized, the converter throws an error\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n# Set the input and output tensors to uint8 (APIs added in r2.3)\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\ntflite_model_quant = converter.convert()\r\n\r\n\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\r\ninput_type = interpreter.get_input_details()[0]['dtype']\r\nprint('input: ', input_type)\r\noutput_type = interpreter.get_output_details()[0]['dtype']\r\nprint('output: ', output_type)\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n\r\n<class 'numpy.float32'>\r\n<output:  <class 'numpy.float32'>\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHi \r\nPlease help me to identify this issue, After successful conversion of mnist model to integer quantized tflite model i am getting the input dtype as float32 instead  of uint8.\r\nI followed the same post training interger quantization steps mentioned in the tensorflows official website, but getting input/output dtype  as float32\r\nAttaching the link: https://www.tensorflow.org/lite/performance/post_training_integer_quant\r\n\r\n\r\nI also followed the command line tflite_converter command too,  still getting quantized float model only\r\ncommand used : \r\ntflite_convert --output_file mnist_tflite_cmd.tflite --keras_model_file mnist.h5 --input_arrays \"reshape_input\" --input_shapes \"1,28,28\" --output_arrays \"Identity\" --output_format TFLITE --inference_type QUANTIZED_UINT8 --inference_input_type QUANTIZED_UINT8 \r\n\r\nThanks \r\n\r\n", "comments": ["@shalushajan95 \r\nPlease provide indented code such that we can replicate the issue or if possible share a colab gist with the error reported.", "@Saduf2019 \r\nCode is attached \r\nThan you\r\n[minst_tflite_converter.py.zip](https://github.com/tensorflow/tensorflow/files/5039527/minst_tflite_converter.py.zip)\r\n", "@shalushajan95 \r\nI ran the code shared and i am able to get unit8 [on tf 2.3 and tf nightly]  as expected, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d0f3c92e2b94fee80139339e28075735/untitled335.ipynb).\r\nIn case you are still facing the issue on tf 2.3 please share a colab gist."]}, {"number": 42091, "title": "ValueError: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Code provided by TensorFlow themselves. Tutorial link: https://www.tensorflow.org/tutorials/structured_data/feature_columns \r\n\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Catalina v10.15.4\r\n-   **TensorFlow installed from (source or binary)**: source\r\n-   **TensorFlow version (use command below)**: 2.0.0\r\n-   **Python version**: 3.7.7\r\n\r\n### Describe the problem\r\nBug with Tensorflow code, which I have executed in a Anaconda Environment on Jupyter Notebooks. I have tried running the notebook provided by Tensorflow themselves (link specified above) and was confronted with the following traceback:\r\n\r\n`Epoch 1/10\r\n231/231 [==============================] - 4s 19ms/step - loss: 0.6898 - accuracy: 0.6893\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1609   try:\r\n-> 1610     c_op = c_api.TF_FinishOperation(op_desc)\r\n   1611   except errors.InvalidArgumentError as e:\r\n\r\nInvalidArgumentError: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-39-9b1057f115c9> in <module>\r\n     13 model.fit(train_ds,\r\n     14           validation_data=val_ds,\r\n---> 15           epochs=10)\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    368                       mode=ModeKeys.TEST,\r\n    369                       training_context=eval_context,\r\n--> 370                       total_epochs=1)\r\n    371                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\r\n    372                                  prefix='val_')\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    501       # This is the first call of __call__, so we have to initialize.\r\n    502       initializer_map = object_identity.ObjectIdentityDictionary()\r\n--> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    504     finally:\r\n    505       # At this point we know that the initialization is complete (or less\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    406     self._concrete_stateful_fn = (\r\n    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 408             *args, **kwds))\r\n    409 \r\n    410     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1846     if self.input_signature:\r\n   1847       args, kwargs = None, None\r\n-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1849     return graph_function\r\n   1850 \r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in distributed_function(input_iterator)\r\n     71     strategy = distribution_strategy_context.get_strategy()\r\n     72     outputs = strategy.experimental_run_v2(\r\n---> 73         per_replica_function, args=(model, x, y, sample_weights))\r\n     74     # Out of PerReplica outputs reduce or pick values to return.\r\n     75     all_outputs = dist_utils.unwrap_output_dict(\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)\r\n    758       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\r\n    759                                 convert_by_default=False)\r\n--> 760       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    761 \r\n    762   def reduce(self, reduce_op, value, axis):\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\r\n   1785       kwargs = {}\r\n   1786     with self._container_strategy().scope():\r\n-> 1787       return self._call_for_each_replica(fn, args, kwargs)\r\n   1788 \r\n   1789   def _call_for_each_replica(self, fn, args, kwargs):\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)\r\n   2130         self._container_strategy(),\r\n   2131         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\r\n-> 2132       return fn(*args, **kwargs)\r\n   2133 \r\n   2134   def _reduce_to(self, reduce_op, value, destinations):\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    290   def wrapper(*args, **kwargs):\r\n    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 292       return func(*args, **kwargs)\r\n    293 \r\n    294   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in test_on_batch(model, x, y, sample_weight, reset_metrics)\r\n    319       x, y, sample_weight=sample_weight, extract_tensors_from_dataset=True)\r\n    320 \r\n--> 321   batch_size = array_ops.shape(nest.flatten(x, expand_composites=True)[0])[0]\r\n    322   outputs = training_eager.test_on_batch(\r\n    323       model,\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py in _slice_helper(tensor, slice_spec, var)\r\n    811         ellipsis_mask=ellipsis_mask,\r\n    812         var=var,\r\n--> 813         name=name)\r\n    814 \r\n    815 \r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py in strided_slice(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\r\n    977       ellipsis_mask=ellipsis_mask,\r\n    978       new_axis_mask=new_axis_mask,\r\n--> 979       shrink_axis_mask=shrink_axis_mask)\r\n    980 \r\n    981   parent_name = name\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py in strided_slice(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\r\n  10392                         ellipsis_mask=ellipsis_mask,\r\n  10393                         new_axis_mask=new_axis_mask,\r\n> 10394                         shrink_axis_mask=shrink_axis_mask, name=name)\r\n  10395   _result = _op.outputs[:]\r\n  10396   _inputs_flat = _op.inputs\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    791         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\r\n    792                          input_types=input_types, attrs=attr_protos,\r\n--> 793                          op_def=op_def)\r\n    794       return output_structure, op_def.is_stateful, op\r\n    795 \r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in create_op(***failed resolving arguments***)\r\n    546     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\r\n    547         op_type, inputs, dtypes, input_types, name, attrs, op_def,\r\n--> 548         compute_device)\r\n    549 \r\n    550   def capture(self, tensor, name=None):\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\r\n   3427           input_types=input_types,\r\n   3428           original_op=self._default_original_op,\r\n-> 3429           op_def=op_def)\r\n   3430       self._create_op_helper(ret, compute_device=compute_device)\r\n   3431     return ret\r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1771           op_def, inputs, node_def.attr)\r\n   1772       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\r\n-> 1773                                 control_input_ops)\r\n   1774     # pylint: enable=protected-access\r\n   1775 \r\n\r\n~/miniconda3/envs/cogsci/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1611   except errors.InvalidArgumentError as e:\r\n   1612     # Convert to ValueError for backwards compatibility.\r\n-> 1613     raise ValueError(str(e))\r\n   1614 \r\n   1615   return c_op\r\n\r\nValueError: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.`\r\n", "comments": ["@shepan6 \r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.\r\n\r\nYou can refer similar issue #34850, [link](https://stackoverflow.com/questions/55962508/valueerror-slice-index-0-of-dimension-0-out-of-bounds-while-using-google-colab) and see if it helps you .Thanks!", "Google Colab link:\r\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/feature_columns.ipynb\r\n\r\nIn terms of the solution you have provided: I have tried reduced batch size so it is proportional to the number of sample, and this does not resolve the issue.", "@shepan6 \r\n\r\nI tried in colab with TF nightly version and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/eced86aace6268e56b0eb0502a5763fc/untitled316.ipynb).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "## System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script\r\nprovided in TensorFlow)**: Custom Code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur v11.3\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.7.7\r\n\r\n### Instructions to install\r\nSee [colab](https://colab.research.google.com/drive/1EIHvpQSUTCm8tbrWTu2JtKD1cRFcLT_T?usp=sharing) notebook or script below.\r\n\r\n```\r\nconda create -n venv python=3.7 pip\r\nconda activate venv\r\npip install tensorflow\r\n```\r\n\r\n ### Describe the problem\r\n\r\n`ValueError` is raised when working with RaggerTensors and functions decorated with `tf.function`. More specifically, if I do a `concatenate` on a RaggedTensor inside a `tf.function`, then a `ValueError` is raised. Instead, if I place the `concatenate` outside the `tf.function`, the code exits successfully. See the provided script.\r\n\r\n\r\n### Script with the issue\r\n\r\nI define two functions `f()` and `g()` that perform the same goal: concatenate the tuples in the RaggedTensor.  I named this script `test.py` and ran it with `python test.py`. I placed a print between both calls (which should be equivalent, please correct me if they're not) that demonstrates that the implementation in `f` raises an error while the one in `g` does not. The only difference between them is that the `tf.concat` call is placed outside the `tf.function` or not.\r\n\r\nIn my real use case, the RaggedTensor is created after some pre-processing, as opposed to this case where I already set up the values and row-lengths. I provide this script as a minimal-reproducible example :) \r\n\r\n```python\r\nimport sys\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\n@tf.function\r\ndef f(values, row_lengths):\r\n    b = tf.RaggedTensor.from_row_lengths(\r\n        values=values,\r\n        row_lengths=row_lengths,\r\n    )\r\n\r\n    return tf.concat([x for x in b], axis=0)\r\n\r\n\r\n@tf.function\r\ndef g(values, row_lengths):\r\n    return tf.RaggedTensor.from_row_lengths(\r\n        values=values,\r\n        row_lengths=row_lengths,\r\n    )\r\n\r\n\r\nif __name__ == '__main__':\r\n    values = tf.convert_to_tensor(\r\n        np.array([\r\n            [0, 1],\r\n            [0, 2],\r\n            [1, 0],\r\n            [1, 2],\r\n            [2, 0],\r\n            [2, 1],\r\n        ])\r\n    )\r\n    row_lengths = [2, 2, 2]\r\n\r\n    print(sys.version)\r\n    print(tf.__version__)\r\n    print(tf.concat([x for x in g(values, row_lengths)], axis=0))\r\n    print(\"G Ready\")\r\n    print(f(values, row_lengths))\r\n```\r\n\r\n### Error\r\n```\r\n2021-05-01 00:27:58.195310: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-05-01 00:27:58.195688: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n3.7.10 (default, Feb 26 2021, 10:16:00) \r\n[Clang 10.0.0 ]\r\n2.4.0\r\n2021-05-01 00:27:58.307849: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-05-01 00:27:58.318700: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: RaggedFromRowLengths/RowPartitionFromRowLengths/assert_non_negative/assert_less_equal/Assert/AssertGuard/branch_executed/_8\r\ntf.Tensor(\r\n[[0 1]\r\n [0 2]\r\n [1 0]\r\n [1 2]\r\n [2 0]\r\n [2 1]], shape=(6, 2), dtype=int64)\r\nG Ready\r\nTraceback (most recent call last):\r\n  File \"test_numpy.py\", line 41, in <module>\r\n    print(f(values, row_lengths))\r\n  File \"/Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\r\n    *args, **kwds))\r\n  File \"/Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in user code:\r\n\r\n    test_numpy.py:13 f  *\r\n        return tf.concat([x for x in b], axis=0)\r\n    /Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/ops/ragged/ragged_getitem.py:103 ragged_tensor_getitem\r\n        return _ragged_getitem(self, key)\r\n    /Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/ops/ragged/ragged_getitem.py:186 _ragged_getitem\r\n        row = rt_input.values[starts[row_key]:limits[row_key]]\r\n    /Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1047 _slice_helper\r\n        name=name)\r\n    /Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1219 strided_slice\r\n        shrink_axis_mask=shrink_axis_mask)\r\n    /Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:10479 strided_slice\r\n        shrink_axis_mask=shrink_axis_mask, name=name)\r\n    /Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper\r\n        attrs=attr_protos, op_def=op_def)\r\n    /Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:592 _create_op_internal\r\n        compute_device)\r\n    /Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3536 _create_op_internal\r\n        op_def=op_def)\r\n    /Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:2016 __init__\r\n        control_input_ops, op_def)\r\n    /Users/fernandobperezm/miniconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1856 _create_c_op\r\n        raise ValueError(str(e))\r\n\r\n    ValueError: slice index 3 of dimension 0 out of bounds. for '{{node RaggedGetItem_3/strided_slice_2}} = StridedSlice[Index=DT_INT32, T=DT_INT64, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](RaggedGetItem_3/strided_slice, RaggedGetItem_3/strided_slice_2/stack, RaggedGetItem_3/strided_slice_2/stack_1, RaggedGetItem_3/strided_slice_2/stack_2)' with input shapes: [3], [1], [1], [1] and with computed input tensors: input[1] = <3>, input[2] = <4>, input[3] = <1>.\r\n```"]}, {"number": 42090, "title": "Build Failure of version 2.3.0 in macOS, MacPorts", "body": "Version 2.3.0 fails to build on macOS.\r\n\r\nFirst issue, `Action failed to execute: java.io.IOException: Cannot run program\u2026 error=24, Too many open files`:\r\n```\r\nERROR: /opt/local/var/macports/build/_opt_local_ports_python_py-tensorflow/py37-tensorflow/work/tensorflow-tensorflow-b36436b/tensorflow/core/common_runtime/BUILD:328:11: C++ compilation of rule '//tensorflow/core/common_runtime:collective_executor_mgr' failed (Exit -1): wrapped_clang failed: error executing command \r\n  (cd /opt/local/var/macports/build/_opt_local_ports_python_py-tensorflow/py37-tensorflow/work/e3571a779784f9da03a7824d69817047/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM=MacOSX \\\r\n    APPLE_SDK_VERSION_OVERRIDE=10.15 \\\r\n    PATH=/opt/local/bin:/opt/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin \\\r\n    XCODE_VERSION_OVERRIDE=11.6.0.11E708 \\\r\n  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG '-std=c++11' -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -MD -MF bazel-out/host/bin/tensorflow/core/common_runtime/_objs/collective_executor_mgr/collective_executor_mgr.d -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' '-frandom-seed=bazel-out/host/bin/tensorflow/core/common_runtime/_objs/collective_executor_mgr/collective_executor_mgr.o' -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/MacOSX.platform/Developer/Library/Frameworks '-mmacosx-version-min=10.15' -g0 '-march=x86-64' -g0 '-std=c++14' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/common_runtime/collective_executor_mgr.cc -o bazel-out/host/bin/tensorflow/core/common_runtime/_objs/collective_executor_mgr/collective_executor_mgr.o)\r\nExecution platform: @local_execution_config_platform//:platform. Note: Remote connection/protocol failed with: execution failed\r\nAction failed to execute: java.io.IOException: Cannot run program \"/opt/local/var/macports/build/_opt_local_ports_python_py-tensorflow/py37-tensorflow/work/install/1eb24b6f9fb447fbef56fd6c7521f126/process-wrapper\" (in directory \"/opt/local/var/macports/build/_opt_local_ports_python_py-tensorflow/py37-tensorflow/work/e3571a779784f9da03a7824d69817047/execroot/org_tensorflow\"): error=24, Too many open files\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nThis is with system settings:\r\n```bash\r\n$ ulimit -n\r\n65536\r\n$ launchctl limit maxfiles\r\n\tmaxfiles    65536          200000   \r\n```\r\n\r\nStarting the build again, another issue is encountered, apparently arising from https://github.com/tensorflow/tensorflow/pull/40654.\r\n\r\nSecond issue, `error: no matching function for call to object of type '(lambda at tensorflow/python/lib/core/bfloat16.cc:637:25)`:\r\n```\r\n:info:build tensorflow/python/lib/core/bfloat16.cc:678:8: error: no matching function for call to object of type '(lambda at tensorflow/python/lib/core/bfloat16.cc:637:25)'\r\n:info:build   if (!register_ufunc(\"less_equal\", CompareUFunc<Bfloat16LeFunctor>,\r\n:info:build        ^~~~~~~~~~~~~~\r\n:info:build tensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate function not viable: no overload of 'CompareUFunc' matching 'PyUFuncGenericFunction' (aka 'void (*)(char **, const long *, const long *, void *)') for 2nd argument\r\n:info:build   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n:info:build                         ^\r\n:info:build tensorflow/python/lib/core/bfloat16.cc:682:8: error: no matching function for call to object of type '(lambda at tensorflow/python/lib/core/bfloat16.cc:637:25)'\r\n:info:build   if (!register_ufunc(\"greater_equal\", CompareUFunc<Bfloat16GeFunctor>,\r\n:info:build        ^~~~~~~~~~~~~~\r\n```\r\n\r\nmacOS 10.15.6 19G73\r\nXcode 11.6 11E708 \r\n\r\nRelated:\r\n* https://trac.macports.org/ticket/60960\r\n* https://github.com/macports/macports-ports/pull/7575\r\n* https://github.com/tensorflow/tensorflow/pull/40654\r\n", "comments": ["@essandess,\r\nCould you please provide the exact sequence of commands / steps that you executed before running into the error?\r\n\r\nAlso, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#macos) and check if you have the compatible dependencies installed, Thanks!", "I have the enormous log file. What's the best way to transmit to you?\n\nAlso, this has all dependencies. The MacPorts build was working up until version 2.2.0.", "ERROR: /Users/xxx/oss/tensorflow/tensorflow/python/BUILD:501:1: C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1)\r\ntensorflow/python/lib/core/bfloat16.cc:663:8: error: no matching function for call to object of type '(lambda at tensorflow/python/lib/core/bfloat16.cc:637:25)'\r\n  if (!register_ufunc(\"equal\", CompareUFunc<Bfloat16EqFunctor>,\r\n       ^~~~~~~~~~~~~~\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate function not viable: no overload of 'CompareUFunc' matching 'PyUFuncGenericFunction' (aka 'void (*)(char **, const long *, const long *, void *)') for 2nd argument\r\n  auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n                        ^\r\ntensorflow/python/lib/core/bfloat16.cc:667:8: error: no matching function for call to object of type '(lambda at tensorflow/python/lib/core/bfloat16.cc:637:25)'\r\n  if (!register_ufunc(\"not_equal\", CompareUFunc<Bfloat16NeFunctor>,\r\n       ^~~~~~~~~~~~~~\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate function not viable: no overload of 'CompareUFunc' matching 'PyUFuncGenericFunction' (aka 'void (*)(char **, const long *, const long *, void *)') for 2nd argument\r\n  auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n                        ^\r\ntensorflow/python/lib/core/bfloat16.cc:671:8: error: no matching function for call to object of type '(lambda at tensorflow/python/lib/core/bfloat16.cc:637:25)'\r\n  if (!register_ufunc(\"less\", CompareUFunc<Bfloat16LtFunctor>, compare_types)) {\r\n       ^~~~~~~~~~~~~~\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate function not viable: no overload of 'CompareUFunc' matching 'PyUFuncGenericFunction' (aka 'void (*)(char **, const long *, const long *, void *)') for 2nd argument\r\n  auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n                        ^\r\ntensorflow/python/lib/core/bfloat16.cc:674:8: error: no matching function for call to object of type '(lambda at tensorflow/python/lib/core/bfloat16.cc:637:25)'\r\n  if (!register_ufunc(\"greater\", CompareUFunc<Bfloat16GtFunctor>,\r\n       ^~~~~~~~~~~~~~\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate function not viable: no overload of 'CompareUFunc' matching 'PyUFuncGenericFunction' (aka 'void (*)(char **, const long *, const long *, void *)') for 2nd argument\r\n  auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n                        ^\r\ntensorflow/python/lib/core/bfloat16.cc:678:8: error: no matching function for call to object of type '(lambda at tensorflow/python/lib/core/bfloat16.cc:637:25)'\r\n  if (!register_ufunc(\"less_equal\", CompareUFunc<Bfloat16LeFunctor>,\r\n       ^~~~~~~~~~~~~~\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate function not viable: no overload of 'CompareUFunc' matching 'PyUFuncGenericFunction' (aka 'void (*)(char **, const long *, const long *, void *)') for 2nd argument\r\n  auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n                        ^\r\ntensorflow/python/lib/core/bfloat16.cc:682:8: error: no matching function for call to object of type '(lambda at tensorflow/python/lib/core/bfloat16.cc:637:25)'\r\n  if (!register_ufunc(\"greater_equal\", CompareUFunc<Bfloat16GeFunctor>,\r\n       ^~~~~~~~~~~~~~\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate function not viable: no overload of 'CompareUFunc' matching 'PyUFuncGenericFunction' (aka 'void (*)(char **, const long *, const long *, void *)') for 2nd argument\r\n  auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n                        ^\r\n6 errors generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /Users/xxx/oss/tensorflow/tensorflow/tools/pip_package/BUILD:66:1 C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1)", "> I have the enormous log file. What's the best way to transmit to you?\r\n\r\n@essandess,\r\nYou can copy the logs to a text file and then share the file with us. Thanks!", "@rockspring,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @rockspring,\r\n> Could you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!\r\n\r\nPlease see https://github.com/tensorflow/tensorflow/issues/42479\r\n", "> Please see #42479\r\n\r\n@essandess,\r\nThe message to submit a new issue was addressed to the user @rockspring.\r\n\r\nSince you have already submitted a new issue, can we close this since it is being tracked there? Thanks! \r\n", "> > Please see #42479\r\n> \r\n> @essandess,\r\n> The message to submit a new issue was addressed to the user @rockspring.\r\n> \r\n> Since you have already submitted a new issue, can we close this since it is being tracked there? Thanks!\r\n\r\nYes, closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42090\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42090\">No</a>\n"]}, {"number": 42089, "title": "Fix example formatting in tf.profiler.experimental.server.start", "body": "", "comments": ["I see, thanks. So for consistency, it seems better to make all examples in this file testable. This PR should be re-done."]}, {"number": 42088, "title": "Problem with tf.train.FloatList Precision", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.10\r\n- CUDA/cuDNN version: 10.2\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI want to convert my Pascal VOC to TFRecords.\r\nBut my bounding box values are not the same.\r\nFor example:\r\nmy xmin is:  [0.620390625]\r\nbut the value I get from example is: [0.6203906536102295]\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nxmin = [0.620390625]\r\nfeature_dict = {\r\n    'xmin':\r\n        tf.train.Feature(float_list=tf.train.FloatList(value=xmin)),\r\n}\r\nexample = tf.train.Example(features=tf.train.Features(feature=feature_dict))\r\n\r\nprint(xmin) # [0.620390625]\r\nprint(example.features.feature['xmin'].float_list.value) # [0.6203906536102295]\r\n```\r\nAnd I also try other values, only the value `2.7182817459106445` show in [TFRecord tutorials](https://www.tensorflow.org/tutorials/load_data/tfrecord) would be the same, as below:\r\n\r\n![Screenshot from 2020-08-06 16-57-53](https://user-images.githubusercontent.com/20853096/89513954-ce8ded80-d807-11ea-8c2a-d8ba7a19424a.png)\r\n\r\n**Describe the expected behavior**\r\n\r\nvalue should be the same\r\n\r\n**Standalone code to reproduce the issue**\r\nColab link: https://colab.research.google.com/drive/1X_PnjlxA948OpgXaN2laNzdVRQ-1wuHl", "comments": ["\r\nI am able to replicate the issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/7b67b35ac33d672c9415f843453278a1/untitled329.ipynb)", "@Saduf2019 I don't think it is a bug. @kaka-lin Check https://github.com/tensorflow/tensorflow/issues/12876#issuecomment-328295387", "@kaka-lin\r\nCan you please update as per above comment.", "@Saduf2019 \r\nOK! I would try it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42088\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42088\">No</a>\n", "Because my case is want to confirm `raw_input` is equal `value get from tfrecord` or not\r\nMy workaround is I will storing the data as strings and casting them back.\r\n\r\n@bhack  Thanks !"]}, {"number": 42087, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "I am getting the following error when trying to import tensorflow. I have tried many solutions and non seem to work.\r\n\r\nI have seen Issue #22794, but it does not seem to help.\r\n\r\nVersions -- > python 3.7.4 -- > tensorflow 2.3.0\r\n\r\n> \r\n> runfile('C:/Users/pshad/.spyder-py3/temp.py', wdir='C:/Users/pshad/.spyder-py3')\r\n> Traceback (most recent call last):\r\n> \r\n>   File \"<ipython-input-30-bc8fb4f78acd>\", line 1, in <module>\r\n>     runfile('C:/Users/pshad/.spyder-py3/temp.py', wdir='C:/Users/pshad/.spyder-py3')\r\n> \r\n>   File \"C:\\Users\\pshad\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 827, in runfile\r\n>     execfile(filename, namespace)\r\n> \r\n>   File \"C:\\Users\\pshad\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n>     exec(compile(f.read(), filename, 'exec'), namespace)\r\n> \r\n>   File \"C:/Users/pshad/.spyder-py3/temp.py\", line 9, in <module>\r\n>     import tensorflow as tf\r\n> \r\n>   File \"C:\\Users\\pshad\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n>     from tensorflow.python.tools import module_util as _module_util\r\n> \r\n>   File \"C:\\Users\\pshad\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n>     from tensorflow.python.eager import context\r\n> \r\n>   File \"C:\\Users\\pshad\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n>     from tensorflow.python import pywrap_tfe\r\n> \r\n>   File \"C:\\Users\\pshad\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n> \r\n>   File \"C:\\Users\\pshad\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n>     raise ImportError(msg)\r\n> \r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\pshad\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n>     from tensorflow.python._pywrap_tensorflow_internal import *\r\n> ImportError: DLL load failed: The specified module could not be found.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n\r\nI tried to reinstall the tensorflow, downgrade,  and upgrade as well. Nothing seem to work, unfortunately. ", "comments": ["@SRJaffry \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See[ hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n.Also, please follow the instructions from to install from [Tensorflow website.](https://www.tensorflow.org/install/source_windows)\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issues #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "Thanks, I installed the latest distribution of Visual C++ from the link your shared and the problem seem to solve. Thank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42087\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42087\">No</a>\n"]}, {"number": 42086, "title": "Person detection training doesn't converge. Evaluation showing accuracy of 52% after 1M steps", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Docker installation\r\n- Tensorflow version (commit SHA if source): 1.14.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): None\r\n\r\n**Describe the problem**\r\nI have done the training as exactly as in the document https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection/training_a_model.md with some minor command changes which is not updated in the original document. The training was done for 1Million steps and when evaluated the accuracy was 52%. \r\nThe minor modifications were \r\n1. use the script download_and_convert.py instead of build_visualwakewords_data.py. \r\n2. Changed arguments --input_grayscale to --use_grayscale\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nThe commands i used are as follows all from the path models/research/slim\r\n1. python slim/download_and_convert_data.py \\\r\n    --dataset_name=visualwakewords \\\r\n    --dataset_dir=/tmp/visualwakewords\r\n\r\n2. python slim/train_image_classifier.py     --train_dir=vww_96_grayscale     --dataset_name=visualwakewords     --dataset_split_name=train     --dataset_dir=coco_dataset/     --model_name=mobilenet_v1_025     --preprocessing_name=mobilenet_v1     --train_image_size=96     --use_grayscale=True     --save_summaries_secs=300     --learning_rate=0.045     --label_smoothing=0.1     --learning_rate_decay_factor=0.98     --num_epochs_per_decay=2.5     --moving_average_decay=0.9999     --batch_size=96     --max_number_of_steps=1000000\r\n\r\n3. python slim/eval_image_classifier.py --alsologtostderr checkpoint_path=vww_96_grayscale/model.ckpt-1000000 --dataset_dir=coco_dataset/ --dataset_name=visualwakewords --dataset_split_name=val --model_name=mobilenet_v1_025 --preprocessing_name=mobilenet_v1 --use_grayscale=True --train_image_size=96\r\n\r\nCan someone help me in identifying why it is not converging as stated in the paper? ", "comments": ["Sorry my mistake there was a typo in checkpoint_path argument. Now I am getting 81%. \r\nOne additional change to training_a_model.md. Changed the argument --train_image_size to --eval_image_size. "]}, {"number": 42085, "title": "[TFLite] Error: TfLiteGpuDelegate Init: MUL: 1024x131  cannot be reduced to linear.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 10.0 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel3 \r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (use command below): org.tensorflow:tensorflow-lite-gpu:2.2.0 / 0.0.0-nightly \r\n- Python version: 3.6 \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: X\r\n- GPU model and memory:  Adreno 630\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nHi. I'm trying to serve our model on Pixel3(Android 10), using TFLite's GPU backend. \r\nBut, this error makes our app crash. (Both of tensorflow-lite-gpu:2.2.0 and 0.0.0-nightly)\r\n```\r\n  Process: org.tensorflow.lite.examples.posenet, PID: 30052\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:\r\n    SUM: Operation is not supported.\r\n    First 141 operations will run on the GPU, and the remaining 7 on the CPU.\r\n    TfLiteGpuDelegate Init: MUL: 1024x131  cannot be reduced to linear.\r\n    TfLiteGpuDelegate Prepare: delegate is not initialized\r\n    Node number 148 (TfLiteGpuDelegateV2) failed to prepare.\r\n```\r\nThe error is \"TfLiteGpuDelegate Init: MUL: 1024x131  cannot be reduced to linear.\" \r\nWhat is the matter and what can I do to utilize TfLiteGPUDelegate?    \r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42085\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42085\">No</a>\n", "Hello @cathy-kim , have you solved the problem?", "@zldrobit Actually, No. \r\nI just replaced 'MUL' to 'MATMUL', which is 'FullyConnected' in TFLite.", "@cathy-kim thanks for your confirming.\r\nI see now, and I decide to reconstruct my network.", "@cathy-kim \r\nIs this still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42085\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42085\">No</a>\n", "I found a relevant issue where someone seemed to solve the problem.\r\nhttps://github.com/tensorflow/tensorflow/issues/37102"]}, {"number": 42084, "title": "Tensorflow is BROKE .... ", "body": "Stuck here, tf-nightly is not compatible with CUDA 11.0 and the latest code is broke....\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): One line ?\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): Latest as of this second\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: 11.00\r\n- GPU model and memory: RTX 2070\r\n\r\n```\r\nPython 3.8.2 (default, Jul 16 2020, 14:00:26) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n\r\n2020-08-06 00:18:35.042260: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/xxxxx/.virtualenvs/tensorflowgpu/lib/python3.8/site-packages/tensorflow/__init__.py\", line 433, in <module>\r\n    _ll.load_library(_main_dir)\r\n  File \"/home/xxxx/.virtualenvs/tensorflowgpu/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py\", line 154, in load_library\r\n    py_tf.TF_LoadLibrary(lib)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/xxxxx/.local/lib/python3.8/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\r\n>>> \r\n```", "comments": ["@summa-code \r\nPlease take a look at the [tested build configuration](https://www.tensorflow.org/install/source#gpu) and try building TensorFlow with CUDA 10.1, and check if it works. Thanks!", "No i am not doing for 10.1, because this used to work for 11.0, something is wrong in recent code.", "@summa-code just to get some context: did you manage to build a tf-nightly with CUDA 11 before and now it is broken? Can you run a bisect to identify which nightly has the first failure? We can take it from there.\r\n\r\nWe don't have build nightly with CUDA 11 ourselves, so we can only support CUDA 11 at this time on a best effort basis. \r\n\r\nThere is work being done to switch nightly to CUDA 11 soon, though. Hence, the above bisection can also help there\r\n\r\nThank you", "Oh, I see you're building from source and the failure is recent. Can you please try a `git bisect`?", "i did git clone into a new directory and built... I am still surprised that it is still broke. And i will try to find which night it broke, i do have the nightly built wheel myself here.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nlibtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\r\n```", "Can you post the command you are compiling with? And the output of `git log -n4 --format=oneline`?\r\n\r\nOur dashboard shows builds as being green at the moment", "Ok, the build that i had on 4th of Aug works, but the ones i built after that all have this problem. And i have only enabled CUDA in configuration, not enabling Tensorrt, as i am suspecting that it had some algorithms not implemented. I have only enabled NUMA for the bazel compile command. I will see if linking Tensorrt makes any difference.\r\n\r\n", "Hope that timeline helped ? ", "Still need the command used to build (from a fresh clone and a `bazel clean --expunge`) as well as the output of the `git log` command posted above. `bazel --version` also helps.", "@mihaimaruseac I have rebuilt the system, after i installed 10.x and tested, it reported the same problem. So some other library was messed up by Nvidia update, Damn Nvidia...Now the build looks good and works fine, i will keep digging. Thanks for your follow up. A good weekend worth of work. :-)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42084\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42084\">No</a>\n", "Also incompatible with CUDA 10.2.\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-PCIE...  On   | 00000001:00:00.0 Off |                    0 |\r\n| N/A   27C    P0    24W / 250W |      0MiB / 16160MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n\r\n@summa-code ", "@DjangoPeng NVIDIA says CUDA 10.2 and CUDA 10.1 are compatible. We will skip CUDA 10.2 and go for CUDA 11.", "emm. \r\n\r\nSo I'd better upgrade to CUDA 11?", "The original crash is in fact caused by #42978 and has been fixed by #45399.", "@foxik I have been having the exact same problem as OP. I applied patch #45399 and I still get the same error.", "@danielgrebe Could you post the error happening during `import tensorflow as tf`, please?", "This is the error that I get when I compile tensorflow with Cuda 11.1 and CuDNN 8.\r\nI have tried the following branches and gotten this same error:\r\n\r\n- r2.4\r\n- r2.4 with patch #45399 applied\r\n\r\n\r\nThis is the only tensorflow that I have on my system. Everytime that I install, I make sure to uninstall tf as well.\r\n\r\nI have not tried with any other version of Cuda, as Cuda 11.1 (and 11.2 as of yesterday) is the only version supported on Debian 10. I have tried, without success to install ubuntu packages.\r\n\r\nI have now built nightly without CUDA and it works.\r\nI thought that I had tried compiling tf-nightly, but it turns out that I was getting confused in my files and I was just reinstalling the r2.4 wheel.\r\n\r\n```\r\n2020-12-16 17:50:23.477409: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n/usr/local/lib/python3.8/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\r\n  warnings.warn(msg)\r\nTraceback (most recent call last):\r\n  File \"./fashion_MNIST.py\", line 6, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/tiberius/.local/lib/python3.8/site-packages/tensorflow/__init__.py\", line 435, in <module>\r\n    _ll.load_library(_main_dir)\r\n  File \"/home/tiberius/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py\", line 153, in load_library\r\n    py_tf.TF_LoadLibrary(lib)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb\r\n```", "@summa-code  @ mihaimaruseac\r\n\r\nIt seems like the issue should be reopened.", "It's better to open a new issue and post the full details."]}, {"number": 42083, "title": "Wrong JPEG library version: library is 90, caller expects 80", "body": "I got this error when i call tf.image.decode_jpeg in my program which linked a libjpeg of version 90.\r\nTensorflow : 1.14\r\nOpencv : 3.1.0", "comments": ["@jeshxxx,\r\nTensorFlow v1.14 is not actively supported. Please upgrade to TensorFlow 2.x or v1.15. \r\n\r\nIn order to expedite the trouble-shooting process, please provide the complete code, the dataset you are using and the complete error log. Thanks!", "I was able to use `tf.image.decode_jpeg` without any issues with TF v1.15. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/e82b0db953601c538b6a25f82131b81d/42083.ipynb).\r\n\r\nAlso, take a look at [this thread](https://forum.qt.io/topic/52899/solved-wrong-jpeg-library-version-library-is-90-caller-expects-80) from a similar issue and let us know if it helps. Thanks!", "`TEST(ScalarPreprocessor, SavedModelWithImageInputTest) {\r\n  tensorflow::GraphDef graph_def_;\r\n  tensorflow::SessionOptions opts_;\r\n  tensorflow::Session* session_;\r\n  tensorflow::SavedModelBundle saved_bundle_;\r\n  tensorflow::RunOptions run_options;\r\n  std::string path = \"./saved_model/\";\r\n  std::unordered_set<std::string> saved_model_tags;\r\n  saved_model_tags.insert(\"serve\");\r\n  tensorflow::Status s = LoadSavedModel(opts_, run_options, path, saved_model_tags, &saved_bundle_);\r\n  graph_def_ = saved_bundle_.meta_graph_def.graph_def();\r\n  session_ = saved_bundle_.session.release();\r\n  std::vector<std::pair<std::string, tensorflow::Tensor>> tf_inputs_;\r\n\r\n  std::ifstream t;\r\n  int length;\r\n  t.open(\"test.jpg\", std::ifstream::binary);\r\n  t.seekg(0, std::ios::end);\r\n  length = t.tellg();\r\n  t.seekg(0, std::ios::beg);\r\n  char *buffer = new char[length];\r\n  t.read(buffer, length);\r\n  t.close();\r\n  tensorflow::TensorProto proto;\r\n  proto.set_dtype(tensorflow::DataType::DT_STRING);\r\n  proto.mutable_tensor_shape()->add_dim()->set_size(1);\r\n  proto.add_string_val(buffer, length);\r\n  tensorflow::TensorShape shape;\r\n  tf_inputs_.push_back(std::make_pair(\"ParseExample/ParseExample:0\",\r\n                                      tensorflow::Tensor(tensorflow::DataType::DT_STRING, shape)));\r\n  bool converted = tf_inputs_.back().second.FromProto(proto);\r\n  ASSERT_TRUE(converted);\r\n  std::vector<tensorflow::Tensor> tf_outputs_;\r\n  std::vector<std::string> output_tensor_names;\r\n  output_tensor_names.push_back(\"ArgMax:0\");\r\n  tensorflow::Status status = session_->Run(\r\n      tf_inputs_, output_tensor_names, {}, &tf_outputs_);\r\n  std::cout << status.ToString() << std::endl;\r\n}`\r\nIt works with libjpeg version 80. But I need link OpenCV-3.1.0 which depends on libjpeg version 90.", "When link with libjpeg version 90, it returns status like: Invalid argument: 2 root error(s) found.\r\n  (0) Invalid argument: Invalid JPEG data or crop window, data size 19075\r\n\t [[{{node map/while/DecodeJpeg}}]]\r\n  (1) Invalid argument: Invalid JPEG data or crop window, data size 19075\r\n\t [[{{node map/while/DecodeJpeg}}]]\r\n\t [[map/while/DecodeJpeg/_43]]\r\n0 successful operations.\r\n0 derived errors ignored.", "@jeshxxx,\r\nCould you please check if you are facing the same issue with TF v1.15 or TF v2.3? Thanks!", "Sorry, my mistake. I mean running a tensorflow::GraphDef with DecodeJpeg op. My model is the same as https://github.com/tensorflow/serving/blob/r1.7/tensorflow_serving/example/inception_saved_model.py. And i load it using LoadSavedModel function in my cpp program which linked opencv-3.1.0.", "@jeshxxx We are not clear what you are trying to explain. Can you give us a detailed explanation on what you are trying to accomplish here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42082, "title": "QAT conversion RuntimeError: Quantization not yet supported for op: 'DEQUANTIZE' issue with tf-nightly", "body": "**UPDATE**\r\n\r\n**You can now fully quantize QAT models trained in any TF 2.x version. However, this feature is only available from TF version `2.4.0-rc0` onwards (and will be available in the final TF 2.4 release as well).**\r\n\r\n**You will not require any workaround, i.e, you don't have to use TF 1.x**\r\n\r\nTo verify that your TF version supports this, run the following code and check if runs successfully:\r\n\r\n```\r\nimport tensorflow as tf\r\nassert tf.__version__[:3] == \"2.4\", 'Your TF version ({}), does not support full quantization of QAT models. Upgrade to a TF 2.4 version (2.4.0-rc0, 2.4.0-rc1...2.4) or above'.format(tf.__version__)\r\n```\r\n\r\n\r\n\r\n------------------------------------------------------------------------------------------\r\n**ISSUE**\r\n\r\n**System information**\r\nTensorFlow version (use command below): 2.4.0-dev20200728\r\n\r\n**Describe the current behavior**\r\nError converting quantize aware trained tensorflow model to a fully integer quantized tflite model - error: `RuntimeError: Quantization not yet supported for op: 'DEQUANTIZE'`\r\n\r\n**Describe the expected behavior**\r\nConvert successfully\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://colab.research.google.com/gist/sayakpaul/8c8a1d7c94beca26d93b67d92a90d3f0/qat-bad-accuracy.ipynb", "comments": ["@sayakpaul\r\n\r\nI can reproduce the issue and will get back to you when I resolve this.\r\nhttps://colab.research.google.com/gist/MeghnaNatraj/8458ad508f5355769a980400d4d9d194/qat-bad-accuracy.ipynb\r\n\r\nPossible issue:\r\n If you remove the `TFLITE_BUILTINS_INT8` (don't enforce INT8) -- it works fine. The issue is that the model has 2 consecutive quantize at the beginning and 2 consecutive dequantize at the end (not sure why) -- probably because of the way `tf.keras..mobilenetv2` is structured. \r\n\r\nCouple of things to note (especially as you are involved in creating awesome tutorials! \ud83d\udc4d ):\r\n(The colab gist above has all the final code with the following suggested changes. NOTE: it also has some TODOs where i have simplified the code for faster execution)\r\n\r\n1. Ensure you use the latest `tensorflow_model_optimization` and `tensorflow-datasets` and uninstall `tensorflow` when you install `tf-nightly`\r\n2. Code readability: A) Try to group many similar code sections into one. Sections can be: all imports and initial settings code, all data processing related code, all training related code, all conversion code, etc. B) If your model is for a basic tutorial and it's small, use full paths to keras APIs -- `tf.keras.layers.....` instead of `from tf.keras.layers import *`.  \r\n3. For data generation: Have 3 parts 1) train_raw (loaded from tfds) - data has 3 dimensions 2) train_preprocessed (with all preprocessing steps) - data has 3 dimensions 3) train_data (the final dataset prepared for training, this would have batching shuffle and prefetch function.) - data has 4 dimensions Note: Repeat all 3 for validation data BUT do not shuffle the data for validation (or test dataset.)\r\n4. Representative dataset - should only have 4 dimensions for images. You initially used the batched training data  with shape=(32, 244, 244, 3) and we further add a batch size  in the representative dataset (`tf.expand_dims(train_data_image, 0)`) - as a result the shape increases to 5! (1, 32, 244, 244, 5) This causes some errors which is quite hard to debug  (eg: PAD op dimensions exceeded >=4). You instead want (1, 244, 244, 5) hence we use the `train_preprocessed` data (check the 3rd point above) where the images don't yet have a batch dimension shape (244, 244, 3) for the representative_dataset function.\r\n6. Representative dataset - do not use `next(iter(train_ds..))`. This will make the image and label as a sequential list of items and cause failures. Instead use `for image, _ in train_ds_preprocessed:`\r\n", "Thanks! First of all, the notebook that I had provided to you was meant for reproducing the issue I was facing. Before releasing it publicly, I sure would have modified it a bit. \r\n\r\nA couple of things:\r\n\r\n> uninstall tensorflow when you install tf-nightly\r\n\r\nNot sure about this since when I install `pip install tf-nightly` at the beginning of a Colab session (before doing anything) I have the nightly version gets reflected always. Is there anything specific for which you'd do this? \r\n\r\n> Sections can be: all imports and initial settings code\r\n\r\nI respectfully disagree. I won't put together the pip installs inside the same code block where I am importing dependencies. I try to break longer code blocks some times which you might have seen in my notebook as well. This is my personal preference. If \"all training code\" seems a bit unreadable to me I'd break it into multiple cells and the same applies for \"all conversion code\". \r\n\r\n> If your model is for a basic tutorial and it's small, use full paths to keras APIs -- tf.keras.layers..... instead of from tf.keras.layers import *\r\n\r\nOkay, will keep in mind. But for a bit more complex tutorials/notebooks (in general), I don't think I'd follow it. \r\n\r\n> For data generation\r\n\r\nIn the [original notebook](https://colab.research.google.com/gist/sayakpaul/8c8a1d7c94beca26d93b67d92a90d3f0/qat-bad-accuracy.ipynb), I first loaded the dataset from tfds, visualized it (which I think is a good practice), mapped the resizing step, then mapped the scaling step and batching-shuffling (**_shuffling not for the validation set_**). The only thing I'd change is merging the resizing step and scaling step inside a utility and map them. \r\n\r\n![image](https://user-images.githubusercontent.com/22957388/89483019-bf338380-d7b8-11ea-9f40-48a36dfff12d.png)\r\n\r\nIf you emphasized on the data generation point because I separated the steps into different cells, yes, I won't generally do that. \r\n\r\n> Representative dataset \r\n\r\nAgreed on the point. You might have mistakenly mentioned 5 channels (244, 244, 5) but note that in the flowers' dataset the images come in 3 channels. I also see the problem in the `representative_dataset_gen` utility I used:\r\n\r\n```python\r\nrepresentative_images, _ = next(iter(train_ds))\r\n\r\ndef representative_dataset_gen():\r\n    for image in representative_images:\r\n        yield [tf.expand_dims(image, 0)]\r\n```\r\n\r\nIf I'd have changed it to something like the following I think it should be good.\r\n\r\n```python\r\nrepresentative_images, _ = next(iter(train_ds))\r\n\r\ndef representative_dataset_gen():\r\n    for image in representative_images:\r\n        yield [image]\r\n```\r\n\r\nI can confirm that in this way `image` would have a shape of `(1, 224, 224, 3)`. \r\n\r\nYou might also consider adding these instructions in the documentation. \r\n\r\n> Representative dataset - do not use next(iter(train_ds..)). This will make the image and label as a sequential list of items and cause failures. Instead use for image, _ in train_ds_preprocessed:\r\n\r\nOkay. But what if I'd want to restrict the number of instances in the representative dataset? Because for bigger datasets it's very difficult to have the entire training dataset streamed as the representative dataset. Would you suggest something like the following? \r\n\r\n```python\r\ntrain_ds_unbatched = train_ds.unbatch() # train_ds already batched and preprocessed\r\n\r\ndef representative_dataset_gen():\r\n\tfor i, (image, _) in enumerate(train_ds_unbatched):\r\n\t\tif i==0: # let's say I want 100 samples only\r\n\t\t\tbreak\r\n\t\tyield[image]\r\n```", "Great points! Yes, you can choose what you think works best --- eg: I learnt many new things from your tutorial (loading TFDS datasets with the [:85%]..method! who knew! :))\r\n\r\nFor the representative dataset -- Would `.take()` work? https://www.tensorflow.org/datasets/overview#as_numpy (ignore the `as_numpy()` part... just wanted to show an example usage)", "Yes, `take()` should work as well. Having a note in the documentation on handling large datasets while creating the representative dataset would help. The representative dataset generation can get non-trivial at times and here's [an example](https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Magenta_arbitrary_style_transfer_model_conversion.ipynb) (which I am sure you are already aware of). ", "Do you have any plan solving this? I just encounterd this issue... Here is my minimal reproduing code\r\n- Train\r\n```py\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport tensorflow_model_optimization as tfmot\r\n\r\n# Load MNIST dataset\r\n(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\n# Define the model architecture.\r\nmodel = keras.Sequential([\r\n    keras.layers.InputLayer(input_shape=(28, 28)),\r\n    keras.layers.Flatten(),\r\n    keras.layers.Dense(10)\r\n])\r\n\r\n# Train the digit classification model\r\nmodel.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\r\nmodel.fit(train_images, train_labels, epochs=1, validation_data=(test_images, test_labels))\r\n# 1875/1875 [==============================] - 2s 946us/step - loss: 0.7303 - accuracy: 0.8100 - val_loss: 0.3097 - val_accuracy: 0.9117\r\n\r\n# Train the quantization aware model\r\nq_aware_model = tfmot.quantization.keras.quantize_model(model)\r\nq_aware_model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\r\nq_aware_model.fit(train_images, train_labels, epochs=1, validation_data=(test_images, test_labels))\r\n# 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3107 - accuracy: 0.9136 - val_loss: 0.2824 - val_accuracy: 0.9225\r\n```\r\n- Convert\r\n```py\r\n# Define the representative data.\r\ndef representative_data_gen():\r\n    for input_value in tf.data.Dataset.from_tensor_slices(train_images.astype(\"float32\")).batch(1).take(100):\r\n        yield [input_value]\r\n\r\n# Successful converting from model\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\ntflite_model = converter.convert()\r\n\r\n# Successful converting from model to uint8\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model_quant = converter.convert()\r\n\r\n# Successful converting from q_aware_model\r\nq_converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\r\nq_converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nq_converter.representative_dataset = representative_data_gen\r\nq_tflite_model = q_converter.convert()\r\n\r\n# Fail converting from q_aware_model to uint8\r\nq_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nq_converter.inference_input_type = tf.uint8\r\nq_converter.inference_output_type = tf.uint8\r\nq_tflite_model_quant = q_converter.convert()\r\n```\r\nThrows error\r\n```py\r\nRuntimeError: Quantization not yet supported for op: 'DEQUANTIZE'.\r\n```\r\n- Another test without `tf.lite.OpsSet.TFLITE_BUILTINS_INT8`\r\n```py\r\n# Successful converting from model to uint8\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model_quant = converter.convert()\r\n\r\n# Fail converting from q_aware_model to uint8\r\nq_converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\r\nq_converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nq_converter.representative_dataset = representative_data_gen\r\nq_converter.inference_input_type = tf.uint8\r\nq_converter.inference_output_type = tf.uint8\r\nq_tflite_model_quant = q_converter.convert()\r\n```\r\nThrows error\r\n```py\r\nRuntimeError: Unsupported output type UINT8 for output tensor 'Identity' of type FLOAT32.\r\n```", "You can resolve the second error by using removing all the lines `q_converter.inference_output_type = tf.uint8`. We're currently working on fixing this -- will post an update when it's done.", "Thanks for your update. Ya, removing `q_converter.inference_output_type = tf.uint8` will make it successful, but will leave `output` as `float32`.\r\n```py\r\ninterpreter = tf.lite.Interpreter(model_content=q_tflite_model_quant)\r\nprint('input: ', interpreter.get_input_details()[0]['dtype'])\r\n# input:  <class 'numpy.uint8'>\r\nprint('output: ', interpreter.get_output_details()[0]['dtype'])\r\n# output:  <class 'numpy.float32'>\r\n```", "what about the problem with `RuntimeError: Quantization not yet supported for op: 'DEQUANTIZE' `?\r\nI have also encounter with it when trying to convert my model. Is it because QAT not fully support full integer edge device like coral?", "@MeghnaNatraj I have reconstructed a Resnet model from QAT guide https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide\r\n\r\nThis is my model and I have QAT successfully with it, but when I try to convert fully to uint8 or int8 for edge_tpu. I still got the problem:  `RuntimeError: Quantization not yet supported for op: 'DEQUANTIZE'`\r\n\r\nMy model code:\r\n```\r\nfrom tensorflow import Tensor\r\nfrom tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\r\n                                    Add, AveragePooling2D, Flatten, Dense, concatenate\r\nfrom tensorflow.keras.models import Model, Sequential\r\nimport tensorflow as tf\r\nimport tensorflow_model_optimization as tfmot\r\n\r\nLastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\r\nMovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\r\n\r\nquantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\r\nquantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\r\nquantize_scope = tfmot.quantization.keras.quantize_scope\r\n\r\n\r\nclass DefaultBNQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\r\n    # Configure how to quantize weights.\r\n    def get_weights_and_quantizers(self, layer):\r\n        # return []\r\n        return [(layer.weights[i], LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False)) for i in range(2)]\r\n\r\n    # Configure how to quantize activations.\r\n    def get_activations_and_quantizers(self, layer):\r\n        return []\r\n\r\n    def set_quantize_weights(self, layer, quantize_weights):\r\n#       # Add this line for each item returned in `get_weights_and_quantizers`\r\n#       # , in the same order\r\n        # layer.kernel = quantize_weights[0]\r\n        # print(quantize_weights)\r\n        layer.gamma = quantize_weights[0]\r\n        layer.beta = quantize_weights[1]\r\n        # layer.moving_mean = quantize_weights[2]\r\n        # layer.moving_variance = quantize_weights[3]\r\n        # pass\r\n        \r\n    def set_quantize_activations(self, layer, quantize_activations):\r\n      # Add this line for each item returned in `get_activations_and_quantizers`\r\n      # , in the same order.\r\n        pass\r\n\r\n    # Configure how to quantize outputs (may be equivalent to activations).\r\n    def get_output_quantizers(self, layer):\r\n        return []\r\n\r\n    def get_config(self):\r\n        return {}\r\n\r\ndef relu_bn(inputs: Tensor) -> Tensor:\r\n    relu = ReLU()(inputs)\r\n    bn = quantize_annotate_layer(BatchNormalization(), DefaultBNQuantizeConfig())(relu)\r\n    return bn\r\n\r\ndef residual_block(x: Tensor, downsample: bool, filters: int, kernel_size: int = 3) -> Tensor:\r\n    y = Conv2D(kernel_size=kernel_size,\r\n               strides= (1 if not downsample else 2),\r\n               filters=filters,\r\n               padding=\"same\")(x)\r\n    y = relu_bn(y)\r\n    y = Conv2D(kernel_size=kernel_size,\r\n               strides=1,\r\n               filters=filters,\r\n               padding=\"same\")(y)\r\n\r\n    if downsample:\r\n        x = Conv2D(kernel_size=1,\r\n                   strides=2,\r\n                   filters=filters,\r\n                   padding=\"same\")(x)\r\n    out = Add()([x, y])\r\n    out = relu_bn(out)\r\n    return out\r\n\r\ndef create_res_net_quantize(inputs,embedding_size,quantize=True):\r\n    quantize = True\r\n    num_filters = 64\r\n    t = quantize_annotate_layer(BatchNormalization(), DefaultBNQuantizeConfig())(inputs)\r\n\r\n    t = Conv2D(kernel_size=3,\r\n               strides=1,\r\n               filters=num_filters,\r\n               padding=\"same\")(t)\r\n    t = relu_bn(t)\r\n\r\n    num_blocks_list = [2, 5, 5, 2]\r\n    for i in range(len(num_blocks_list)):\r\n        num_blocks = num_blocks_list[i]\r\n        for j in range(num_blocks):\r\n            t = residual_block(t, downsample=(j==0 and i!=0), filters=num_filters)\r\n        num_filters *= 2\r\n\r\n    t = AveragePooling2D(4)(t)\r\n    t = Flatten()(t)\r\n    outputs = Dense(embedding_size)(t)\r\n\r\n    model = quantize_annotate_model(Model(inputs, outputs))\r\n    with quantize_scope(\r\n      {'DefaultBNQuantizeConfig': DefaultBNQuantizeConfig}):\r\n  # Use `quantize_apply` to actually make the model quantization aware.\r\n        quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\r\n    quant_aware_model.summary()\r\n    return quant_aware_model\r\n```\r\n\r\nI used the same method as @leondgarse did", "Is there any update on this issue? I am facing the same error while trying to convert a QAT model to INT8.", "> Is there any update on this issue? I am facing the same error while trying to convert a QAT model to INT8.\r\n\r\nAs far as my understanding, tf2.0 quantization is not supported Yet for full integer inference. Try QAT for tf1.x and everything is smoothly done", "Thanks @dtlam26 for the response. Strangely, it turns out that for MobileNetV1 the conversion is working fine. For MobileNetV2, as @MeghnaNatraj pointed out in earlier comment, there are two consecutive QUANTIZE and DEQUANTIZE nodes for inputs and outputs respectively which might be causing the issue. Attaching the screenshots.\r\nINPUT:\r\n![image](https://user-images.githubusercontent.com/71460728/93518336-84595b00-f8e1-11ea-9234-ad0d5e0119df.png)\r\nOUTPUT:\r\n![image](https://user-images.githubusercontent.com/71460728/93518371-920ee080-f8e1-11ea-8fe7-5e66a4aaa2d4.png)\r\n", "> Thanks @dtlam26 for the response. Strangely, it turns out that for MobileNetV1 the conversion is working fine. For MobileNetV2, as @MeghnaNatraj pointed out in earlier comment, there are two consecutive QUANTIZE and DEQUANTIZE nodes for inputs and outputs respectively which might be causing the issue. Attaching the screenshots.\r\n> INPUT:\r\n> ![image](https://user-images.githubusercontent.com/71460728/93518336-84595b00-f8e1-11ea-9234-ad0d5e0119df.png)\r\n> OUTPUT:\r\n> ![image](https://user-images.githubusercontent.com/71460728/93518371-920ee080-f8e1-11ea-8fe7-5e66a4aaa2d4.png)\r\n\r\nYes, you can still quantize the model for other type of quantization except int8 (I mean Builtin_int8)\r\nFurthermore, You can check out this link for the detail bypass QAT on tf2: https://github.com/tensorflow/model-optimization/issues/377#issuecomment-625093345\r\nFrom this, by declaring double input to the model, that is why the QAT for mobilenetv2 got double quantize and dequantize", "> You can resolve the second error by using removing all the lines `q_converter.inference_output_type = tf.uint8`. We're currently working on fixing this -- will post an update when it's done.\r\n\r\nAny news ??", "> > Is there any update on this issue? I am facing the same error while trying to convert a QAT model to INT8.\r\n> \r\n> As far as my understanding, tf2.0 quantization is not supported Yet for full integer inference. Try QAT for tf1.x and everything is smoothly done\r\n\r\n@dtlam26 can you point to some resources for QAT for tf1.x and then quanitzation. I am trying the following code (without QAT) but getting some error on TF 1.15:\r\n\r\n\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.models import Sequential\r\nnum_classes = 20\r\n\r\nmodel = Sequential([\r\n  layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(256, 256, 3)),\r\n  layers.MaxPooling2D(),\r\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\r\n  layers.MaxPooling2D(),\r\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\r\n  layers.MaxPooling2D(),\r\n  layers.Flatten(),\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(num_classes)\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(train_generator, epochs=1, steps_per_epoch=100)\r\n\r\nmodel.save('/tmp/temp.h5')\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(\"/tmp/temp.h5\")\r\nconverter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\ninput_arrays = converter.get_input_arrays()\r\nconverter.quantized_input_stats = {input_arrays[0] : (0., 1.)}\r\ntflite_model = converter.convert()\r\n\r\nI am getting below error:\r\nConverterError: See console for info.\r\n2020-09-23 22:55:27.815650: F tensorflow/lite/toco/tooling_util.cc:1734] Array conv2d_3/Relu, which is an input to the MaxPool operator producing the output array max_pooling2d_3/MaxPool, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nFatal Python error: Aborted", "@dtlam26  As you are not using QAT and instead using post-training quantization, you need to provide a  `representative_dataset` in order to quantize the model. \r\n\r\n\r\nModify the steps in your code as:\r\n\r\n```\r\n.\r\n.\r\n\r\nmodel.save('/tmp/temp.h5')\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(\"/tmp/temp.h5\")\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ndef representative_dataset_gen():\r\n  for _ in range(num_calibration_steps):\r\n    # Get sample input data as a numpy array in a method of your choosing.\r\n    yield [input]\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_model = converter.convert()\r\n```\r\n\r\nRefer to [post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) for more information.", "> @dtlam26 As you are not using QAT and instead using post-training quantization, you need to provide a `representative_dataset` in order to quantize the model.\r\n> \r\n> Modify the steps in your code as:\r\n> \r\n> ```\r\n> .\r\n> .\r\n> \r\n> model.save('/tmp/temp.h5')\r\n> \r\n> converter = tf.lite.TFLiteConverter.from_keras_model_file(\"/tmp/temp.h5\")\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> def representative_dataset_gen():\r\n>   for _ in range(num_calibration_steps):\r\n>     # Get sample input data as a numpy array in a method of your choosing.\r\n>     yield [input]\r\n> converter.representative_dataset = representative_dataset_gen\r\n> tflite_model = converter.convert()\r\n> ```\r\n> \r\n> Refer to [post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) for more information.\r\n\r\nYes, I know this for post quantize, but my model is QAT, and it can't inference to int8 on tf2.x. For tf1 it is ok\r\n", "@dtlam26 \r\n> Yes, I know this for post quantize, but my model is QAT, and it can't inference to int8 on tf2.x. For tf1 it is ok\r\n\r\nPlease can you tell me how you are able to perform QAT in tf1 ??", "> > > Is there any update on this issue? I am facing the same error while trying to convert a QAT model to INT8.\r\n> > \r\n> > \r\n> > As far as my understanding, tf2.0 quantization is not supported Yet for full integer inference. Try QAT for tf1.x and everything is smoothly done\r\n> \r\n> @dtlam26 can you point to some resources for QAT for tf1.x and then quanitzation. I am trying the following code (without QAT) but getting some error on TF 1.15:\r\n> \r\n> from tensorflow.keras import layers\r\n> from tensorflow.keras.models import Sequential\r\n> num_classes = 20\r\n> \r\n> model = Sequential([\r\n> layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(256, 256, 3)),\r\n> layers.MaxPooling2D(),\r\n> layers.Conv2D(32, 3, padding='same', activation='relu'),\r\n> layers.MaxPooling2D(),\r\n> layers.Conv2D(64, 3, padding='same', activation='relu'),\r\n> layers.MaxPooling2D(),\r\n> layers.Flatten(),\r\n> layers.Dense(128, activation='relu'),\r\n> layers.Dense(num_classes)\r\n> ])\r\n> model.compile(optimizer='adam',\r\n> loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n> metrics=['accuracy'])\r\n> \r\n> model.fit(train_generator, epochs=1, steps_per_epoch=100)\r\n> \r\n> model.save('/tmp/temp.h5')\r\n> \r\n> converter = tf.lite.TFLiteConverter.from_keras_model_file(\"/tmp/temp.h5\")\r\n> converter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\n> input_arrays = converter.get_input_arrays()\r\n> converter.quantized_input_stats = {input_arrays[0] : (0., 1.)}\r\n> tflite_model = converter.convert()\r\n> \r\n> I am getting below error:\r\n> ConverterError: See console for info.\r\n> 2020-09-23 22:55:27.815650: F tensorflow/lite/toco/tooling_util.cc:1734] Array conv2d_3/Relu, which is an input to the MaxPool operator producing the output array max_pooling2d_3/MaxPool, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\n> Fatal Python error: Aborted\r\n\r\nYou can check out this medium and try to create a training graph and eval graph to QAT 1.x \r\nhttps://medium.com/analytics-vidhya/mobile-inference-b943dc99e29b\r\nThis GitHub is also a good example https://github.com/lusinlu/tensorflow_lite_guide", "> @dtlam26\r\n> \r\n> > Yes, I know this for post quantize, but my model is QAT, and it can't inference to int8 on tf2.x. For tf1 it is ok\r\n> \r\n> Please can you tell me how you are able to perform QAT in tf1 ??\r\n\r\nI have attached the source for example. However, create eval graph will forget the last layer of your model from the graph. You have to add to the graph a dummy part. Example, tf.maximum(output,1e-27) for regression problems", "@dtlam26 Thanks for the resources. @Mattrix00 I also found this notebook that is working for me \r\nhttps://colab.research.google.com/drive/15itdlIyLmXISK6SDAzAFGUgjatfVr0Yq\r\n", "@hangrymoon01, @dtlam26 thank you so much for your help ! ", "Making as resolved due to inactivity. Feel free to reopen it if the issue persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42082\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42082\">No</a>\n", "**UPDATE**\r\n\r\n**You can now fully quantize QAT models trained in any TF 2.x version. However, this feature is only available from TF version `2.4.0-rc0` onwards (and will be available in the final TF 2.4 release as well).**\r\n\r\n**You will not require any workaround, i.e, you don't have to use TF 1.x**\r\n\r\nTo verify that your TF version supports this, run the following code and check if runs successfully:\r\n\r\n```\r\nimport tensorflow as tf\r\nassert tf.__version__[:3] == \"2.4\", 'Your TF version ({}), does not support full quantization of QAT models. Upgrade to a TF 2.4 version (2.4.0-rc0, 2.4.0-rc1...2.4) or above'.format(tf.__version__)\r\n```\r\n", "> **UPDATE**\r\n> \r\n> **You can now fully quantize QAT models trained in any TF 2.x version. However, this feature is only available from TF version `2.4.0-rc0` onwards (and will be available in the final TF 2.4 release as well).**\r\n> \r\n> **You will not require any workaround, i.e, you don't have to use TF 1.x**\r\n> \r\n> To verify that your TF version supports this, run the following code and check if runs successfully:\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> assert tf.__version__[:2] == \"2.4\", 'Your TF version ({}), does not support full quantization of QAT models. Upgrade to a TF 2.4 version (2.4.0-rc0, 2.4.0-rc1...2.4) or above'.format(tf.__version__)\r\n> ```\r\n\r\nFor those of you mindlessly tpying this into your terminal. It should be the following to check compatibility\r\n\r\n```\r\nimport tensorflow as tf\r\nassert tf.__version__[:3] == \"2.4\", 'Your TF version ({}), does not support full quantization of QAT models. Upgrade to a TF 2.4 version (2.4.0-rc0, 2.4.0-rc1...2.4) or above'.format(tf.__version__)\r\n```\r\n", "@msokoloff1 Thank you for that fix! I've updated all the comments above. ", "@MeghnaNatraj using tf 2.4.0rc2 I am still facing the issue \"Quantization not yet supported for op: DEQUANTIZE\". I am simply using the tf.keras.applications vgg16 network with full model quantization from the tfmot library. Do you have an example of this working with tf 2.4?\r\n\r\nJust to make sure I wasn't doing something wrong locally, I get the exact same error if I use the notebook that you linked to above in the original post. The only change I made to the notebook was upgrading to 2.4.0rc0 and I still get the same error.", "@msokoloff1 Could you post a link an end-to-end colab notebook so I can reproduce this issue? ", "@MeghnaNatraj https://colab.research.google.com/gist/msokoloff1/f7d71b73d11adcbcb9f2420503540043/qat-bad-accuracy.ipynb#scrollTo=hMaYuw5AD8qt\r\n\r\nIt is exactly the same notebook that you posted at the start of this issue with the tensorflow version pinned to tf 2.4.0-rc0", "@msokoloff1 @MeghnaNatraj  I faced the similar isuue in tf 2.4.0-rc0. I even tried latest source for tf and tfmot ; but the issue persists. QAT with tk.keras produces quantize and dequantize layers and we are unable to convert them to full integer quantization models, even after using post training quantization on top of it?\r\n\r\nIs there any other workarounds?", "@anilsathyan7 could you also post a colab just as @msokoloff1 did before? It will help us debug this better. \r\n\r\nThere are no workarounds yet, we will get back to you as soon as we find one.", "Here is a modified version of the post training quantization tf example with QAT: https://colab.research.google.com/drive/1aqK5Sd1hy1o55Y1t1MUYQLMgiBwmo2VD?usp=sharing", "I used latest tf-nightly and the error was fixed.\r\n```\r\nRuntimeError: Quantization not yet supported for op: 'DEQUANTIZE'.\r\n```\r\nTensorFlow version: tf-nightly (2.5.0-dev20201130)\r\nMy [gist](https://colab.research.google.com/drive/1qojOz88mQn3Iv85lbZOS_t_62qrzcX8o?usp=sharing)", "Hey @car1hsiung , i think your resulting model is 'quantization aware' but not quantized (e.g. the weights are float32 instead of int8). Your input and outputs are still in float32 format. Can you check with the  other two links ??\r\n", "> Your input and outputs are still in float32 form\r\n\r\nHi @anilsathyan7 , I test the normal and quantized models. **The quantized model is invalid as you mentioned.**\r\n\r\nPlease check the updated [gist](https://colab.research.google.com/drive/1qojOz88mQn3Iv85lbZOS_t_62qrzcX8o?usp=sharing)\r\n\r\n```\r\nRuntimeError: tensorflow/lite/kernels/quantize.cc:113 affine_quantization->scale->size == 1 was not true.Node number 0 (QUANTIZE) failed to prepare.\r\n```\r\n\r\nBTW, the `converter.inference_input_type` and `converter.inference_output_type` can be tf.float32 for quantized model.\r\n\r\nFor example, using float input/output, you will get:\r\n\r\n```\r\n>>> print(interpreter.get_input_details())\r\n[{\r\n\t'name': 'conv2d_input',\r\n\t'index': 0,\r\n\t'shape': array([1, 64, 64, 3], dtype = int32),\r\n\t'shape_signature': array([-1, 64, 64, 3], dtype = int32),\r\n\t'dtype': < class 'numpy.float32' > ,\r\n\t'quantization': (0.0, 0),\r\n\t'quantization_parameters': {\r\n\t\t'scales': array([], dtype = float32),\r\n\t\t'zero_points': array([], dtype = int32),\r\n\t\t'quantized_dimension': 0\r\n\t},\r\n\t'sparsity_parameters': {}\r\n}]\r\n\r\n>>> print(interpreter.get_output_details())\r\n[{\r\n\t'name': 'Identity',\r\n\t'index': 13,\r\n\t'shape': array([1, 12, 12, 64], dtype = int32),\r\n\t'shape_signature': array([-1, 12, 12, 64], dtype = int32),\r\n\t'dtype': < class 'numpy.float32' > ,\r\n\t'quantization': (0.0, 0),\r\n\t'quantization_parameters': {\r\n\t\t'scales': array([], dtype = float32),\r\n\t\t'zero_points': array([], dtype = int32),\r\n\t\t'quantized_dimension': 0\r\n\t},\r\n\t'sparsity_parameters': {}\r\n}]\r\n```\r\n\r\nUsing uint8/int8 you will get:\r\n\r\n```\r\n>>> print(interpreter.get_input_details())\r\n[{\r\n\t'name': 'conv2d_input',\r\n\t'index': 0,\r\n\t'shape': array([1, 64, 64, 3], dtype = int32),\r\n\t'shape_signature': array([-1, 64, 64, 3], dtype = int32),\r\n\t'dtype': < class 'numpy.uint8' > ,\r\n\t'quantization': (3.921568847431445e-09, 127),\r\n\t'quantization_parameters': {\r\n\t\t'scales': array([3.921569e-09], dtype = float32),\r\n\t\t'zero_points': array([127], dtype = int32),\r\n\t\t'quantized_dimension': 0\r\n\t},\r\n\t'sparsity_parameters': {}\r\n}]\r\n\r\n>>> print(interpreter.get_output_details())\r\n[{\r\n\t'name': 'Identity',\r\n\t'index': 13,\r\n\t'shape': array([1, 12, 12, 64], dtype = int32),\r\n\t'shape_signature': array([-1, 12, 12, 64], dtype = int32),\r\n\t'dtype': < class 'numpy.uint8' > ,\r\n\t'quantization': (0.0470588244497776, 128),\r\n\t'quantization_parameters': {\r\n\t\t'scales': array([0.04705882], dtype = float32),\r\n\t\t'zero_points': array([128], dtype = int32),\r\n\t\t'quantized_dimension': 0\r\n\t},\r\n\t'sparsity_parameters': {}\r\n}]\r\n```\r\n\r\nIn other words, you need to quantize input and dequantize output with scales and zero points by yourself if using uint8/int8 input and output.\r\n\r\nI can successfully convert quantize uint8 tflite with **2.4.0-rc1** `converter._experimental_new_quantizer = True`\r\n\r\nPlease check the [tf-2.4-rc1 gist](https://colab.research.google.com/drive/1hPP9SpPB7hVCkT4uxtBF13PABrDWT2V9?usp=sharing)\r\n", "Converting and saving a quantized tflite with QAT using float inputs and outputs was not an issue even in tf 2.3 anyway ...\r\nThe issue was with full integer quantization in QAT, so that we can use them with hardware acclerators(int).\r\n\r\nAlso if you have real train data you need to run [fit/finetune](https://www.tensorflow.org/model_optimization/guide/quantization/training_example#train_and_evaluate_the_model_against_baseline) on the qaware model before conversion to get proper quantized model.\r\n\r\nSetting **converter._experimental_new_quantizer = True**, seems to be key here...\r\nThanks, it worked with **tf-nightly** version also !!!\r\n", "For QAT models, you don't need a representative dataset. Also, full integer quantization support for QAT models (full integer with (default float32)/uint8/int8 input/output) is available from TF 2.4 as shown below:\r\n\r\n[Gist](https://colab.research.google.com/gist/MeghnaNatraj/d8742112d2cabf2a4e67764d255cf0dd/normal-vs-qat-models.ipynb)\r\n\r\n```\r\n!pip uninstall -q -y tensorflow tensorflow-gpu\r\n!pip install tensorflow==2.4\r\n!pip install -q tensorflow-model-optimization\r\n\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_model_optimization as tfmot\r\n\r\ndef get_model(is_qat=False):\r\n  (train_x, train_y) , (_, _) = tf.keras.datasets.mnist.load_data()\r\n  train_x = train_x.astype('float32') / 255\r\n  model = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n    tf.keras.layers.Dense(128,activation='relu'),\r\n    tf.keras.layers.Dense(10)\r\n  ])\r\n  if is_qat:\r\n    model = tfmot.quantization.keras.quantize_model(model)\r\n  model.compile(\r\n      optimizer=tf.keras.optimizers.Adam(0.001),\r\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\r\n  )\r\n  model.fit(train_x, train_y, batch_size=64, epochs=2, verbose=1)\r\n  return model\r\n\r\n## 1. Normal TF Model\r\nmodel = get_model()\r\n\r\n# 1a. Convert normal TF model to INT8 quantized TFLite model (default float32 input/output)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ndef representative_dataset_gen():\r\n    for i in range(10):\r\n        yield [np.random.uniform(low=0.0, high=1.0, size=(1, 28, 28)).astype(np.float32)]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.representative_dataset = representative_dataset_gen\r\nnormal_tf_model_quantized_tflite_model = converter.convert()\r\n\r\n# 1b. Convert normal TF model to INT8 quantized TFLite model (uint8 input/output)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ndef representative_dataset_gen():\r\n    for i in range(10):\r\n        yield [np.random.uniform(low=0.0, high=1.0, size=(1, 28, 28)).astype(np.float32)]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nnormal_tf_model_quantized_with_uint8_io_tflite_model = converter.convert()\r\n\r\n## 2. QAT (Quantize Aware Trained) TF model\r\nqat_model = get_model(is_qat=True)\r\n\r\n# 2a. Convert QAT TF model to INT8 quantized TFLite model (default float32 input/output)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nqat_tf_model_quantized_tflite_model = converter.convert()\r\n\r\n# 2b. Convert QAT TF model to INT8 quantized TFLite model (uint8 input/output)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nqat_tf_model_quantized_with_uint8_io_tflite_model = converter.convert()\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42082\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42082\">No</a>\n", "@MeghnaNatraj In a real training example, should'nt we run a **fit/train** with a train_images_subset between steps 2 and 2.a/2.b in order to maintain accuracy as mentioned in [tf  doc](https://www.tensorflow.org/model_optimization/guide/quantization/training_example#train_and_evaluate_the_model_against_baseline)?", "@anilsathyan7 yes, you would want to train the model actually so that it can adjust to compensate for the information loss (induced for precision loss).", "@anilsathyan7 @sayakpaul yes! Thanks for pointing that out. I've updated the example to also include model training :) ", "I have a little question with the 2.4 quantization, why do the upsampling2d is not supported in tf2 while in tf1 it is acceptable?\r\n`RuntimeError: Layer up_sampling2d_10:<class 'tensorflow.python.keras.layers.convolutional.UpSampling2D'> is not supported. `", "@dtlam26  I found that 8 bit qunatization of Upsampling2d is supported with latest tf-optimization [source](https://github.com/tensorflow/model-optimization/blob/80ccd9a5945ebca22069962edf828caebe213ccf/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_registry.py#L114).", "> @dtlam26 I found that 8 bit qunatization of Upsampling2d is supported with latest tf-optimization [source](https://github.com/tensorflow/model-optimization/blob/80ccd9a5945ebca22069962edf828caebe213ccf/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_registry.py#L114).\r\n\r\nAccording to your source, It seems like they skip the quantization at the upsampling2d layer and only quantize the output as I suppose. This can be created if I custom quantize as well. It is just a surprise when they use to provide quantization on this", "@dtlam26 I'am not sure about the internal implementation; but i was able to get past that error and convert the model to tflite with QAT(Upsample2D Resize Bilinear) and tf-nighlty. The results(accuracy) seems to be fine when i test them on sample images.\r\nAnyway they also mention this:-\r\n\r\n> There are gaps between ResizeBilinear with FakeQuant and\r\n>       # TFLite quantized ResizeBilinear op. It has a bit more quantization\r\n>       # error than other ops in this test now.\r\n\r\n![upsample_quant](https://user-images.githubusercontent.com/1130185/103989690-18637100-51b6-11eb-88b7-ffaadd646cbe.png)\r\n", "> @dtlam26 I'am not sure about the internal implementation; but i was able to get past that error and convert the model to tflite with QAT(Upsample2D Resize Bilinear) and tf-nighlty. The results(accuracy) seems to be fine when i test them on sample images.\r\n> Anyway they also mention this:-\r\n> \r\n> > There are gaps between ResizeBilinear with FakeQuant and\r\n> > # TFLite quantized ResizeBilinear op. It has a bit more quantization\r\n> > # error than other ops in this test now.\r\n> \r\n> ![upsample_quant](https://user-images.githubusercontent.com/1130185/103989690-18637100-51b6-11eb-88b7-ffaadd646cbe.png)\r\n\r\nYes, I can bypass that if I self configure the quantization with no quantization in weights and activations as well. No need for the nightly", "@dtlam26 Can you share demo code/model? ", "> @dtlam26 Can you share demo code/model?\r\n\r\nIt is nothing much as you just need to follow the spec in the file you give me in those [lines](https://github.com/tensorflow/model-optimization/blob/80ccd9a5945ebca22069962edf828caebe213ccf/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_registry.py#L289-L343) as only quantize the output, not the weights and activation. I follow tf guide for custom quantization. You can look into [here](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/quantization/keras/QuantizeConfig)\r\n\r\n```\r\nclass UpSamplingQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\r\n\r\n    def get_weights_and_quantizers(self, layer):\r\n        return []\r\n\r\n    def get_activations_and_quantizers(self, layer):\r\n        return []\r\n\r\n    def set_quantize_weights(self, layer, quantize_weights):\r\n        return\r\n\r\n\r\n    def set_quantize_activations(self, layer, quantize_activations):\r\n        return\r\n\r\n\r\n    def get_output_quantizers(self, layer):\r\n        return [tfmot.quantization.keras.quantizers.MovingAverageQuantizer(\r\n        num_bits=8, per_axis=False, symmetric=False, narrow_range=False)]\r\n\r\n    def get_config(self):\r\n        return {}\r\n```", "@dtlam26  Can you share your minimum working example for upsample layers? I just wanted to know how the 'weights and activations' are supposed to get quantized for Upsample/ResizeBilinear Layers. ", "> @dtlam26 Can you share your minimum working example for upsample layers? I just wanted to know how the 'weights and activations' are supposed to get quantized for Upsample/ResizeBilinear Layers.\r\n\r\nCurrently, If you print out the upsampling layer, There is no weight and activation in this layer. Therefore, you have to skip it. \r\nSame thing I used to do with [BatchNormalize](https://stackoverflow.com/questions/60883928/quantization-aware-training-in-tensorflow-version-2-and-batchnorm-folding/63559933#63559933) when I answered a while ago\r\n\r\nThe way of upsampling a layer is depended on the interpolation and researchers when applying quantization consider this as output quantization. The reason behind you can see in a lot of recent papers that the activation and outputs are heavily related to bias term. Bias gives quantization a hard way to maximize accuracy.\r\n\r\nTherefore, they usually perform the quantization into 2 main streams: weight quantization for kernel/filter and activation quantization for everything that belongs to bias and post bias.\r\n\r\nIn tf1.x You can see this protocol quite clear these protocol in the [contrib](https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/contrib/quantize/python/quantize.py) and their method of inferencing model. They freeze everything in frozen graph and reduce to 8bits, while in tf2.x they use the representation dataset to scale the certainty for the bias term", "I think both in tf 1.x and tf 2.x the basic approach was to train a non-quantized model until convergence and then fine-tune the trained float model with quantization aware training with training data(or subset). for a few more epochs(may be smaller learning rate). Anyway, i got almost (exactly)same accuracy with Upsample2D(ResizeBilinear) in tf2 with QAT, when compared to plain float model(segmentation).Maybe we should train and compare a fixed model with tf1.x and tf2.x and see if there is difference in accuracy.", "> I think both in tf 1.x and tf 2.x the basic approach was to train a non-quantized model until convergence and then fine-tune the trained float model with quantization aware training with training data(or subset). for a few more epochs(may be smaller learning rate). Anyway, i got almost (exactly)same accuracy with Upsample2D(ResizeBilinear) in tf2 with QAT, when compared to plain float model(segmentation).Maybe we should train and compare a fixed model with tf1.x and tf2.x and see if there is difference in accuracy.\r\n\r\nYou are almost right with the QAT as its protocol is *train a non-quantized model until convergence and then fine-tune the trained float model with quantization aware training*. This process will let the quantization bit fit better with the model weights and values. But tensorflow apply the representation dataset with only one purpose of fitting the activation quantization better. This if you use with post quantize can be considered as dynamic quantization. I believe that the tf2,x surely is better as the protocol of training is clear defined and of course, it let the representation dataset adjust for the bias, rather using only MovingAverage ", "@dtlam26 Yes, it was from the official tf documentation only... The original issue was with QAT, so i was referring to that version of quantization anyway.  Also, post training quantization generally gives lesser accuray compared to QAT.", "yes QAT is the best to give out the closest accuracy, but a little time consuming when a lot operator are not yet supported or designed in QAT manner. Depending on the structure of the model, sometimes making a post quantization is quicker and saving a lot of time. Im looking forward for future version of tensorflow quantization as they are making those protocol easier to implement", "In my case, i  was looking for a model(full int) with high accuracy while dealing with segmentation, where the model accuracy is critical. Training time was not an issue(atleast a few epochs). Yes, i also hope that more operators will be supported in the future with keras QAT."]}, {"number": 42081, "title": "[ROCm] Cannot find rocm library hip_hcc", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): 10.1.0\r\n- CUDA/cuDNN version: N/A\r\n- ROCm version: 3.5.0\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen building I am getting the following error:\r\n```\r\nRepository rule rocm_configure defined at:\r\n  /home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl:861:33: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_rocm':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 840\r\n\t\t_create_local_rocm_repository(<1 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 647, in _create_local_rocm_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 449, in _find_libs\r\n\t\t_select_rocm_lib_paths(<3 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 418, in _select_rocm_lib_paths\r\n\t\tauto_configure_fail(<1 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 162, in auto_configure_fail\r\n\t\tfail(<1 more arguments>)\r\n\r\nROCm Configuration Error: Cannot find rocm library hip_hcc\r\nINFO: Repository com_google_protobuf instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule tf_http_archive defined at:\r\n  /home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/repo.bzl:134:34: in <toplevel>\r\nERROR: Skipping '//tensorflow:install_headers': no such package '@local_config_rocm//rocm': Traceback (most recent call last):\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 840\r\n\t\t_create_local_rocm_repository(<1 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 647, in _create_local_rocm_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 449, in _find_libs\r\n\t\t_select_rocm_lib_paths(<3 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 418, in _select_rocm_lib_paths\r\n\t\tauto_configure_fail(<1 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 162, in auto_configure_fail\r\n\t\tfail(<1 more arguments>)\r\n\r\nROCm Configuration Error: Cannot find rocm library hip_hcc\r\nERROR: no such package '@local_config_rocm//rocm': Traceback (most recent call last):\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 840\r\n\t\t_create_local_rocm_repository(<1 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 647, in _create_local_rocm_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 449, in _find_libs\r\n\t\t_select_rocm_lib_paths(<3 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 418, in _select_rocm_lib_paths\r\n\t\tauto_configure_fail(<1 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.2.0-rocm/third_party/gpus/rocm_configure.bzl\", line 162, in auto_configure_fail\r\n\t\tfail(<1 more arguments>)\r\n\r\nROCm Configuration Error: Cannot find rocm library hip_hcc\r\nINFO: Elapsed time: 5.658s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow ... (2 packages)\r\n==> ERROR: A failure occurred in build().\r\n    Aborting...\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. git clone\r\n2. `export TF_NEED_ROCM=1`\r\n3. `./configure`\r\n4. \r\n```\r\n  bazel \\\r\n        build --config=mkl --config=avx2_linux -c opt \\\r\n          //tensorflow:libtensorflow.so \\\r\n          //tensorflow:libtensorflow_cc.so \\\r\n          //tensorflow:install_headers \\\r\n          //tensorflow/tools/pip_package:build_pip_package\r\n      bazel-bin/tensorflow/tools/pip_package/build_pip_package --gpu \"${srcdir}\"/tmpoptrocm\r\n```\r\n\r\nTo be exactly precise I am using the following build script (PKGBUILD):\r\nhttps://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=tensorflow-rocm\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nDownstream issue: https://github.com/rocm-arch/tensorflow-rocm/issues/5\r\n", "comments": ["@acxz \r\n\r\nCan you please refer similar issue #27352 and see if it helps you.Thanks!", "Although the issue there is similar, that issue is not helpful since it does not provide a solution to the problem just tries to go around it by saying use docker and post issue at the tensorflow-rocm repo (a corresponding issue wasn't posted either)", "I opened up the following PR which resolves this for Arch Linux users. https://github.com/tensorflow/tensorflow/pull/42095\r\n\r\nLet's have it go through the CI so that we can ensure it also works for other build configurations.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42081\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42081\">No</a>\n"]}, {"number": 42080, "title": "File \"/home/ray/anaconda3/envs/tf1/lib/python3.8/site-packages/tensorflow/python/feature_column/dense_features.py\", line 30, in <module>     class DenseFeatures(fc._BaseFeaturesLayer):  # pylint: disable=protected-access AttributeError: module 'tensorflow.python.feature_column.feature_column_v2' has no attribute '_BaseFeaturesLayer'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.8.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 11.0 / cuDNN 7.6.5\r\n- GPU model and memory: RTX 2070 super\r\n\r\n\r\n\r\n**Describe the problem**\r\nCan not run tensorboard\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nconda activate tf1 #(where I have tensor installed)\r\npip install -U tensorboard\r\ntensorboard --logdir=models/my_ssd_resnet50_v1_fpn\r\ntensorboard dev --help  #(doesn't work either, gives error narrowed down to title) \r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n2020-08-05 18:01:41.991550: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File \"/home/ray/anaconda3/envs/tf1/bin/tensorboard\", line 5, in <module>\r\n    from tensorboard.main import run_main\r\n  File \"/home/ray/anaconda3/envs/tf1/lib/python3.8/site-packages/tensorboard/main.py\", line 43, in <module>\r\n    from tensorboard import default\r\n  File \"/home/ray/anaconda3/envs/tf1/lib/python3.8/site-packages/tensorboard/default.py\", line 40, in <module>\r\n    from tensorboard.plugins.beholder import beholder_plugin_loader\r\n  File \"/home/ray/anaconda3/envs/tf1/lib/python3.8/site-packages/tensorboard/plugins/beholder/__init__.py\", line 22, in <module>\r\n    from tensorboard.plugins.beholder.beholder import Beholder\r\n  File \"/home/ray/anaconda3/envs/tf1/lib/python3.8/site-packages/tensorboard/plugins/beholder/beholder.py\", line 225, in <module>\r\n    class BeholderHook(tf.estimator.SessionRunHook):\r\n  File \"/home/ray/anaconda3/envs/tf1/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"/home/ray/anaconda3/envs/tf1/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/home/ray/anaconda3/envs/tf1/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/home/ray/anaconda3/envs/tf1/lib/python3.8/site-packages/tensorflow_estimator/__init__.py\", line 10, in <module>\r\n    from tensorflow_estimator._api.v1 import estimator\r\n  File \"/home/ray/anaconda3/envs/tf1/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 10, in <module>\r\n    from tensorflow_estimator._api.v1.estimator import experimental\r\n  File \"/home/ray/anaconda3/envs/tf1/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\", line 10, in <module>\r\n    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\r\n  File \"/home/ray/anaconda3/envs/tf1/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py\", line 23, in <module>\r\n    from tensorflow.python.feature_column import dense_features\r\n  File \"/home/ray/anaconda3/envs/tf1/lib/python3.8/site-packages/tensorflow/python/feature_column/dense_features.py\", line 30, in <module>\r\n    class DenseFeatures(fc._BaseFeaturesLayer):  # pylint: disable=protected-access\r\nAttributeError: module 'tensorflow.python.feature_column.feature_column_v2' has no attribute '_BaseFeaturesLayer'\r\n", "comments": ["@raydjr,\r\nIssues related to Tensorboard are tracked in the Tensorboard repo. Please submit a new issue from [this link](https://github.com/tensorflow/tensorboard/issues/new/choose) and fill in the template,so that we can track the issue there. Thanks!"]}, {"number": 42079, "title": "tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- TensorFlow installed from (source or binary): Anaconda\r\n- TensorFlow version (or github SHA if from source):  tf_nightly_gpu-2.4.0.dev20200805-cp37-cp37m-win_amd64.whl\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nmodel = tf.saved_model.load(saved_model_dir)\r\nconcrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\nconcrete_func.inputs[0].set_shape([1, 300, 300, 3])\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n\r\n\r\n# These lines are necessary for the issue fix https://github.com/tensorflow/tensorflow/issues/41877\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n(tf_gpu_tf_nightly) d:\\TensorFlow\\tensorflowComponentId\\models\\research\\object_detection>python convert_saved_model_to_f\r\nlat_buffer.py\r\n2020-08-05 16:31:38.096415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cudart64_101.dll\r\n2020-08-05 16:31:46.030792: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary nvcuda.dll\r\n2020-08-05 16:31:46.153774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: Quadro P4000 computeCapability: 6.1\r\ncoreClock: 1.48GHz coreCount: 14 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 226.62GiB/s\r\n2020-08-05 16:31:46.176284: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cudart64_101.dll\r\n2020-08-05 16:31:46.371687: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cublas64_10.dll\r\n2020-08-05 16:31:46.632753: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cufft64_10.dll\r\n2020-08-05 16:31:46.734036: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary curand64_10.dll\r\n2020-08-05 16:31:47.053455: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cusolver64_10.dll\r\n2020-08-05 16:31:47.157739: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cusparse64_10.dll\r\n2020-08-05 16:31:47.917165: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cudnn64_7.dll\r\n2020-08-05 16:31:47.930832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-05 16:31:47.966000: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized wit\r\nh oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:\r\n AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-05 16:31:48.030523: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x214141cae40 initialized for\r\nplatform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-05 16:31:48.088275: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default\r\n Version\r\n2020-08-05 16:31:48.136648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: Quadro P4000 computeCapability: 6.1\r\ncoreClock: 1.48GHz coreCount: 14 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 226.62GiB/s\r\n2020-08-05 16:31:48.156650: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cudart64_101.dll\r\n2020-08-05 16:31:48.166699: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cublas64_10.dll\r\n2020-08-05 16:31:48.176341: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cufft64_10.dll\r\n2020-08-05 16:31:48.187558: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary curand64_10.dll\r\n2020-08-05 16:31:48.198596: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cusolver64_10.dll\r\n2020-08-05 16:31:48.207628: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cusparse64_10.dll\r\n2020-08-05 16:31:48.219176: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cudnn64_7.dll\r\n2020-08-05 16:31:48.230180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-05 16:31:48.869564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor\r\nwith strength 1 edge matrix:\r\n2020-08-05 16:31:48.878309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0\r\n2020-08-05 16:31:48.883346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n2020-08-05 16:31:48.888900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:loc\r\nalhost/replica:0/task:0/device:GPU:0 with 6307 MB memory) -> physical GPU (device: 0, name: Quadro P4000, pci bus id: 00\r\n00:01:00.0, compute capability: 6.1)\r\n2020-08-05 16:31:48.914058: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x21455b650a0 initialized for\r\nplatform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-05 16:31:48.924375: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P4000,\r\n Compute Capability 6.1\r\n2020-08-05 16:32:21.722971: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute\r\ncapability >= 0.0): 1\r\n2020-08-05 16:32:21.732162: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-08-05 16:32:21.738833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: Quadro P4000 computeCapability: 6.1\r\ncoreClock: 1.48GHz coreCount: 14 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 226.62GiB/s\r\n2020-08-05 16:32:21.757141: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cudart64_101.dll\r\n2020-08-05 16:32:21.764885: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cublas64_10.dll\r\n2020-08-05 16:32:21.775482: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cufft64_10.dll\r\n2020-08-05 16:32:21.783435: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary curand64_10.dll\r\n2020-08-05 16:32:21.791890: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cusolver64_10.dll\r\n2020-08-05 16:32:21.799672: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cusparse64_10.dll\r\n2020-08-05 16:32:21.813984: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic\r\nlibrary cudnn64_7.dll\r\n2020-08-05 16:32:21.822129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-05 16:32:21.827773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutorwith strength 1 edge matrix:\r\n2020-08-05 16:32:21.839959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0\r\n2020-08-05 16:32:21.844847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n2020-08-05 16:32:21.850558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6307 MB memory) -> physical GPU (device: 0, name: Quadro P4000, pci bus id: 00\r\n00:01:00.0, compute capability: 6.1)\r\n2020-08-05 16:32:23.381950: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:832] Optimization results for grappler item: graph_to_optimize\r\n2020-08-05 16:32:23.392215: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:834]   function_optimizer: Graph size after: 9459 nodes (8971), 11780 edges (11285), time = 283.837ms.\r\n2020-08-05 16:32:23.408641: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:834]   function_optimizer: function_optimizer did nothing. time = 6.568ms.\r\n2020-08-05 16:32:44.552016: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:315] Ignored output_format.\r\n2020-08-05 16:32:44.561287: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:318] Ignored drop_control_dependency.\r\nloc(\"Func/StatefulPartitionedCall/input/_0\"): error: requires all operands and results to have compatible element types\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\clgilbe\\Anaconda3\\envs\\tf_gpu_tf_nightly\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 199, in toco_convert_protos\r\n    enable_mlir_converter)\r\n  File \"C:\\Users\\clgilbe\\Anaconda3\\envs\\tf_gpu_tf_nightly\\lib\\site-packages\\tensorflow\\lite\\python\\wrap_toco.py\", line 38, in wrapped_toco_convert\r\n    enable_mlir_converter)\r\nException: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n<unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x300x300x3x!tf.quint8>) -> tensor<1x300x300x3xui8>\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"convert_saved_model_to_flat_buffer.py\", line 21, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Users\\clgilbe\\Anaconda3\\envs\\tf_gpu_tf_nightly\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 1118,in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"C:\\Users\\clgilbe\\Anaconda3\\envs\\tf_gpu_tf_nightly\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 942, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"C:\\Users\\clgilbe\\Anaconda3\\envs\\tf_gpu_tf_nightly\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 669, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\clgilbe\\Anaconda3\\envs\\tf_gpu_tf_nightly\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 574, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"C:\\Users\\clgilbe\\Anaconda3\\envs\\tf_gpu_tf_nightly\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 202, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n<unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x300x300x3x!tf.quint8>) -> tensor<1x300x300x3xui8>\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nPre-trained model ssd_resnet50_v1_fpn_640x640_coco17_tpu-8 http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\r\n```\r\n\r\n**Failure details**\r\nFailure to convert using instructions for converting a saved model using concrete_functions.  https://www.tensorflow.org/lite/convert/python_api#converting_a_savedmodel_  Similar error to #41877\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title. - NOPE\r\n\r\n**Any other info / logs**\r\n\r\nSee above\r\n", "comments": ["@clgilbe \r\nI ran your code with the changes as per #41877 and do not face any error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/aa58a668ec9ddff9339fa5a12b7f754a/untitled330.ipynb#scrollTo=-COf3oVok4lC)", "@Saduf2019 \r\nHi I tried same code you shared, but I got this exception:\r\n\r\n----\r\n**!pip install tf-nightly**\r\n```\r\nimport tensorflow as tf\r\nsaved_model_dir='ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/'\r\n\r\nmodel = tf.saved_model.load(saved_model_dir)\r\nconcrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\nconcrete_func.inputs[0].set_shape([1, 300, 300, 3])\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n\r\n\r\n# These lines are necessary for the issue fix https://github.com/tensorflow/tensorflow/issues/41877\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\ntflite_model = converter.convert()\r\nopen('tflite_model_coco17o.tflite','wb').write(tflite_model)\r\n```\r\n\r\n---\r\n\r\n> ConverterError: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n> <unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x300x300x3x!tf.quint8>) -> tensor<1x300x300x3xui8>\r\n> \r\n\r\n```\r\nException                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198                                                  debug_info_str,\r\n--> 199                                                  enable_mlir_converter)\r\n    200       return model_str\r\n\r\n6 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n     37       debug_info_str,\r\n---> 38       enable_mlir_converter)\r\n     39 \r\n\r\nException: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n<unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x300x300x3x!tf.quint8>) -> tensor<1x300x300x3xui8>\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-9-2ee2db71f5c7> in <module>()\r\n----> 1 tflite_model = converter.convert()\r\n      2 open('tflite_model_coco17o.tflite','wb').write(tflite_model)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n   1116         Invalid quantization parameters.\r\n   1117     \"\"\"\r\n-> 1118     return super(TFLiteConverterV2, self).convert()\r\n   1119 \r\n   1120 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    940 \r\n    941     return super(TFLiteFrozenGraphConverterV2,\r\n--> 942                  self).convert(graph_def, input_tensors, output_tensors)\r\n    943 \r\n    944 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self, graph_def, input_tensors, output_tensors)\r\n    667         input_tensors=input_tensors,\r\n    668         output_tensors=output_tensors,\r\n--> 669         **converter_kwargs)\r\n    670 \r\n    671     calibrate_and_quantize, flags = quant_mode.quantizer_flags(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    572       input_data.SerializeToString(),\r\n    573       debug_info_str=debug_info_str,\r\n--> 574       enable_mlir_converter=enable_mlir_converter)\r\n    575   return data\r\n    576 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    200       return model_str\r\n    201     except Exception as e:\r\n--> 202       raise ConverterError(str(e))\r\n    203 \r\n    204   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n<unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x300x300x3x!tf.quint8>) -> tensor<1x300x300x3xui8>\r\n```", "@Saduf2019 I agree with @hahmad2008 The error happens on the convert step which I did not see in the gist", "@clgilbe  can you try the solution posted here? https://github.com/tensorflow/tensorflow/issues/42114#issuecomment-671593386\r\n\r\n(save the model and load it again after modifying the input shape?)", "@MeghnaNatraj The suggested solution worked for me to modify the input shape.  Thank you. ", "@clgilbe \r\nPlease move the issue to resolved status if resolved.", "Resolved with solution from #42114 (comment)"]}, {"number": 42078, "title": "Create basic C++ ConcreteFunction class and _build_call_outputs", "body": "Initial C++ migration. \r\nIn the process of running benchmarks.", "comments": ["Approximate benchmarks - mean & [median of means] of 10 trials with 30000 iters each:\r\ndefun_matmul: 174/169 us -> 184/180\r\ndefun_matmul_async: 93/93 us -> 102/102 us\r\ndefun_matmul_with_signature: 175/174 us -> 193/190 us\r\nThe additional time can likely be explained by the fact that the C++ currently simply calls more python, so at least for now there is more latency", "@jonathanchu33 Can you please check Ubuntu Sanity errors? Thanks!", "Was submitted as a CL, pushed as commit 776e040a. The changes reflect up to commit 63facef1 in this PR. Name conflict resolved by commit 9942d2f9."]}, {"number": 42077, "title": "Fix weird dash lines on ImageProjectiveTransformV2", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/41989\r\n\r\nSo, this is before (look at second row):\r\n![Figure_1](https://user-images.githubusercontent.com/10087890/89469309-5d1c5380-d7a3-11ea-8280-159b3b1f785e.png)\r\n\r\nand after :\r\n![Figure_2](https://user-images.githubusercontent.com/10087890/89469319-61e10780-d7a3-11ea-928f-497e6af69145.png)\r\n\r\nAnd the script to generate these images is [here](https://gist.github.com/Smankusors/4a1efd41118fcd21a1abf15c66d1e368)\r\n\r\nLet me know if there's something wrong \ud83d\ude09", "comments": ["huh...? It seems that I need to update unit tests at `tensorflow/python/keras/layers/preprocessing/image_preprocessing.py`?", "> huh...? It seems that I need to update unit tests at `tensorflow/python/keras/layers/preprocessing/image_preprocessing.py`?\r\n\r\nYes", "> huh...? It seems that I need to update unit tests at `tensorflow/python/keras/layers/preprocessing/image_preprocessing.py`?\r\n\r\nIt's at `tensorflow/python/keras/layers/preprocessing/image_preprocessing_test.py`.\r\n\r\nHi @Smankusors, if you feel difficult doing tests or don't mind that I take over your job, I can also fix it in https://github.com/tensorflow/tensorflow/pull/41365. Anyway, feel free to reach out me if there is any problem.", "I haven't validated if this fix is correct, but my implementation in addons is tested against scipy (well, just plugin in the `map_coordinate` function from scipy to TF), and gets the identical results.\r\n\r\nhttps://github.com/tensorflow/addons/pull/1721\r\nhttps://github.com/scipy/scipy/blob/v1.5.2/scipy/ndimage/src/ni_interpolation.c#L43-L119", "> I haven't validated if this fix is correct, but my implementation in addons is tested against scipy (well, just plugin in the `map_coordinate` function from scipy to TF), and gets the identical results.\r\n> \r\n> [tensorflow/addons#1721](https://github.com/tensorflow/addons/pull/1721)\r\n> https://github.com/scipy/scipy/blob/v1.5.2/scipy/ndimage/src/ni_interpolation.c#L43-L119\r\n\r\nI think this is very worth fixing -- would you mind taking it here?", "> It's at `tensorflow/python/keras/layers/preprocessing/image_preprocessing_test.py`.\r\n> \r\n> Hi @Smankusors, if you feel difficult doing tests or don't mind that I take over your job, I can also fix it in #41365. Anyway, feel free to reach out me if there is any problem.\r\n\r\noh yeah damn I pasted a wrong path \ud83d\ude05 \r\n\r\nhmm I will try do this... Should my transform values be added to the unit tests?", "> hmm I will try do this... Should my transform values be added to the unit tests?\r\n\r\nYes! You should fix the original one, and probably add some new test cases that will break in current implementation.", "hmm unsurprisingly this happens on a first test unit (reflect fill, move the image to down by 1 pixel). I also tried `apply_affine_transform` from `keras-preprocessing`. It's.... hmm...\r\n\r\n![reconfirm](https://user-images.githubusercontent.com/10087890/89710392-bacfbc00-d9ac-11ea-9002-29ab133c7e6b.png)\r\n\r\n(second column is my patch)\r\n\r\nIt seems the fix is not as simple as I do \ud83d\ude15 ", "> hmm unsurprisingly this happens on a first test unit (reflect fill, move the image to down by 1 pixel). I also tried `apply_affine_transform` from `keras-preprocessing`. It's.... hmm...\r\n> \r\n> ![reconfirm](https://user-images.githubusercontent.com/10087890/89710392-bacfbc00-d9ac-11ea-9002-29ab133c7e6b.png)\r\n> \r\n> (second column is my patch)\r\n> \r\n> It seems the fix is not as simple as I do \ud83d\ude15\r\n\r\nWhat's `expected_output` here?", "> What's `expected_output` here?\r\n\r\nexpected output is from the existing test unit \r\nhttps://github.com/tensorflow/tensorflow/blob/cdfd1114fd309abc86f4c2b5507abdd3578457fb/tensorflow/python/keras/layers/preprocessing/image_preprocessing_test.py#L713-L718", "I fix it in #41365. It's because the nearest interpolation will round the coordinate. So for example, for \"WRAP\" fill mode, `out_coord = -0.5`, and `len = 4`, `in_coord = 3.5`, and will be rounded to 4. Since this value is larger than `len - 1`, it will be filled with `fill_value_`, resulting in weird dash lines.\r\n\r\nReflect:\r\n![reflect](https://user-images.githubusercontent.com/11615393/89716607-360c8000-d963-11ea-8fdb-41f2038ce39f.jpg)\r\nWrap:\r\n![wrap](https://user-images.githubusercontent.com/11615393/89716610-3ad13400-d963-11ea-9257-ebd69aa594c0.jpg)", "> I fix it in #41365. It's because the nearest interpolation will round the coordinate. So for example, for \"WRAP\" fill mode, `out_coord = -0.5`, and `len = 4`, `in_coord = 3.5`, and will be rounded to 4. Since this value is larger than `len - 1`, it will be filled with `fill_value_`, resulting in weird dash lines.\r\n\r\nnice, I suppose I will close this PR because it doesn't work \ud83d\ude05 "]}, {"number": 42076, "title": "Tensorflow BROKE... for CUDA 11.0", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): Latest\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.00\r\n- GPU model and memory: RTX 2070\r\n\r\n\r\nPython 3.8.2 (default, Jul 16 2020, 14:00:26) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> **import tensorflow as tf**\r\n\r\n2020-08-05 17:27:44.548309: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/xxxxxxx/.virtualenvs/tensorflowgpu/lib/python3.8/site-packages/tensorflow/__init__.py\", line 433, in <module>\r\n    _ll.load_library(_main_dir)\r\n  File \"/home/xxxxxxx/.virtualenvs/tensorflowgpu/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py\", line 154, in load_library\r\n    py_tf.TF_LoadLibrary(lib)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/xxxxxxxx/.local/lib/python3.8/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\r\n", "comments": ["Are you using CUDA 11? Have you compiled the source from master? \r\nSee https://github.com/tensorflow/tensorflow/issues/42047", "Yes, i just did like 30 minutes ago. The latest code basically broke Tensorflow for Cuda 11", "@summa-code\r\n\r\nPlease try building TensorFlow with CUDA 10.1, and check if it works. Thanks!", "@ravikyram Are you serious with your statement ? Come on, give me a break. Tell me officially that Tensorflow is NOT compatible with CUDA 11.0. And i stated that NVIDIA took 10.X out of the view and 11.0 is the latest.\r\n\r\nDude.... what is going on with Tensorflow team ? Should i reach out to someone in Google ? This is crazy. You know what ? This is early in the morning. And your answer is very insulting to tell the least.\r\n\r\nBy the way, Github is owned by Microsoft, Dont you want to have your own bug tracker.. What is wrong with you klingons..", "TensorFlow admittedly has many issues, but lack of CUDA 11 support is not one of them, as far as it seems to me. The tone of @summa-code is wholly inadequate.\r\nI compiled TF 2.3.0 today under the following conditions: Ubuntu 20.04, NVIDIA driver 450.57, CUDA 11.0.2, cuDNN v8.0.2.39, GCC 9.3.0. Absolutely no issues with test code complex enough to demonstrate GPU training functionality. Instructions used for building: https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03.", "@summa-code, each release of TF is released against a specific version of CUDA. There are already issues tracking CUDA 11 support which will likely come in TF 2.4.\r\n\r\nRegarding bug tracking and using GitHub: this is open source project and it has open source tracking. That does not mean that there cannot be a parallel internal tracking.", "@summa-code please follow our [Code of Conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md) and remember to treat everyone in the community (TensorFlow team members included) with respect.", "> each release of TF is released against a specific version of CUDA\r\n\r\nThis seems rather constraining, especially for early adopters. Don't you think this should be rather orthogonal, with the only requirement potentially a *minimum* version tuple of CUDA/cuDNN for a given TF version? ", "@kmhofmann you raise a good point. There are architectural issues which make us able to only test one CUDA/cuDNN tuple. As such, we cannot provide guarantees about other values of this tuple. But, afaik, community builds can build against other versions of CUDA and report issues / send PRs.", "This is a recurrent topic in the history of the project  (2018 or probably also older tickets) https://github.com/tensorflow/tensorflow/issues/22087#issuecomment-420402286\r\n", "@theadactyl and @kmhofmann , it was understandably my frustration that came out after i saw the response. And i know github is opensource but yet run by another for profit corporation. I have spent hours and hours of my time building and testing to solve a specific problem. There are some solutions that are available in MxNET for example. I wanted to give a try with Tensorflow. And for some one to come and tell me to go back to 10.1 for TF 2.4 that used to work before with 11.0 is both frustrating and thought lacked depth. We can't be simply reinstall the whole driver part in the system and rebuild tensor. What i was expecting is to give some thought for these bugs and respond accordingly giving some respect for the times we spend.\r\n\r\nAnd i am completely oblivious to the fact that Microsoft is the defender of OpenSource. Thanks for enlightening me.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42076\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42076\">No</a>\n", "The original crash is in fact caused by #42978 and has been fixed by #45399.", "I was having this problem and wanted to add my fix for any who follow, and add that there may be an issue with Tensorflow 2.4.0.\r\n\r\nWhen installing any version of Tensorflow, always refer to the [chart here](https://www.tensorflow.org/install/source#linux) and check that you're using compatible versions of Tensorflow, your GPU Driver, CUDA, and CudNN.  In my case, I had tried many different approaches but none of them were clearing the undefined symbol error:  Reinstalling tensorflow, pinning packages to get the correct version of CUDA, and finally upgrading from Ubuntu 20.04 to 20.10 entirely.  I had verified using \"nvidia-smi\", \"nvcc --version\", and also the [installation guide here](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html) to confirm that my drivers, CUDA 11, and libcudnn8 were correctly installed and were ostensibly tested and compatible with 2.4.0.  Still, I got OP's error when loading Tensorflow 2.4.0.\r\n\r\nFinally, I tried installing tf-nightly (2.5.0.dev20210105), and it fixed the problem right away.\r\n\r\nThe fact that Tensorflow 2.4.0 showed this undefined symbol error with CUDA 11 and CudNN 8.0 whereas the nightly build worked just fine leads me to believe that there may legitimately be something broken about Tensorflow 2.4.0."]}, {"number": 42074, "title": "Create directory if not exist when saving keras model in h5", "body": "Fixes #41874: raise error when h5 save fail; create directory if not already exist, so that it does not fail when saving to new directory.\r\n", "comments": []}, {"number": 42073, "title": "[INTEL MKL]Changed the DNNL version to 1.5.1", "body": "Made changes to the workspace.bzl and mkldnn_v1.BUILD files to upgrade the DNNL version to 1.5.1 .", "comments": []}]