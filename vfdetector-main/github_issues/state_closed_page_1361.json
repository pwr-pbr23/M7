[{"number": 12245, "title": "Error importing tensorflow-gpu", "body": "\r\n![capture](https://user-images.githubusercontent.com/20387587/29246512-8065cd0a-801a-11e7-8916-0e10817a1dce.PNG)\r\n\r\nI installed Cuda Toolkit, cuDNN 5.1 on windows 8.1\r\nI also installed tensorflow-gpu using pip3\r\nWhen I tried importing tensorflow in python, I got the above error\r\nCan you please help me out?\r\n\r\n\r\n", "comments": ["Install cudNN and if you have already installed and this error occurs, you can use this code to check if cudNN is properly installed. Generally it should work by renaming cudnn64_7.dll to cudnn64_5.dll if you downloaded the latest cudNN\r\n"]}, {"number": 12244, "title": "R0.12 cmake 2017", "body": "", "comments": ["Can one of the admins verify this patch?", "@plsuarez, thanks for your PR! By analyzing the history of the files in this pull request, we identified @gunan, @jart and @vrv to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "sorry my mistake"]}, {"number": 12243, "title": "Feature add provide Java Graph class getter for all operations", "body": "c_api code and tests, Java code and tests, and JNI code to support exposing and using the cardinality of operations contained in a graph and an aggregate of them. This is to support https://github.com/tensorflow/tensorflow/issues/12133\r\n\r\nAs is the stuff of Git sleepover folklore, i stand in the bathroom mirror and softly say @martinwicke three times. ", "comments": ["Can one of the admins verify this patch?", "@quaeler, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @asimshankar and @keveman to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "`I signed it!`", "CLAs look good, thanks!\n\n<!-- ok -->", "(Sorry for the bad cut and paste in my commit message.)", "I don't believe i have the privs to affect any labels on this PR, so i'm textually passing the baton back to @asimshankar here.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Sorry about the formatting issue - will run the formatter on code prior to commit in the future.", "Jenkins, test this please", "Are these build timeouts a Jenkins issue or an issue that got fixed in master since i branched and that i need bring into this branch to make the builds copacetic?", "Jenkins, test this please.", "It appears that there is not consistency in these build failures - this time it's a Linux Python 3 failure and Linux CPU failure ( //tensorflow/contrib/rnn:gru_ops_test ); last time Linux CPU failed with a different earlier test (in time out) so perhaps it would have failed gru_ops_test, but Python 3 passed and it was Windows CMake that had to be killed after nearly two hours.\r\n\r\nWhat's the path forward in this situation?", "FWIW `//tensorflow/contrib/rnn:gru_ops_test` passes locally against this branch (though i had to dance with pip to get it to install python-mock in a non-virtualenv since mock wants a modern version of six and macOS these days SIP-protects the six python libraries for whatever reason.)\r\n", "You can check ci.tensorflow.org and see whether there's an existing failure\nat head. If so, failures may be unrelated. Otherwise the build should be\nconsistent.\n", "The status balls at https://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/ were scene extras in that elevator scene from The Shining. \r\n\r\nWith that plus random tests failing and timing out that touch nothing to do with the meager Java related toe nail clippings of files that i touched, can we assume that there is a larger problem here?", "The py3 and cpu failures are unrelated. This is fine to merge.", "I don't have permission to merge this; could one of you fine gents do the deed?", "Jenkins, test this please", "tensorflow-pull-request-multijob last had a successful build `6 days 23 hr` ago.\r\n\r\nShould i try to merge down from master to hopefully pick up any recent changes that might produce a better build result, in the theory that many of the branches in this week of continual PR build fails stem from a master at a non-building point?\r\n", "Yeah. Keep in mind that I'm going through and bulk merging everything that passes, so it's easier for me to just rerun everything that's stale :)", "Ok - your work is much appreciated - thanks :- )", "Huzzah!"]}, {"number": 12242, "title": "Broken download links of the Windows GPU", "body": "Hi,\r\n\r\n`Windows GPU` download link are broken.", "comments": ["@av8ramit, can you check why the Windows GPU links are broken in [README.md](https://github.com/tensorflow/tensorflow/blob/master/README.md)?", "Trying again after merging a fix for hash mismatch error in cmake: https://github.com/tensorflow/tensorflow/pull/12271", "I think the nightly GPU build is still broken.\r\nWe will keep looking into a fix.", "Fixed!"]}, {"number": 12241, "title": "Revert \"Update metrics_op.py (#12218)\"", "body": "This reverts commit b3e0738a4420d0dce8ebba554ff5c68ee8c228e7.\r\n\r\nReason: broke metrics_op_test, e.g., see:\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=NO_PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/586/console", "comments": ["@caisq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @jhseu and @lukaszkaiser to be potential reviewers.", "cc @alanyee ", "@caisq Ah, this can be fixed via switching the order of the returned tuple. No need to use the deprecated API.", "You can just keep my changes, but in metrics_op.py,\r\n```\r\n  labels, predictions = confusion_matrix.remove_squeezable_dimensions(\r\n      labels, predictions)\r\n```", "@alanyee can you send a PR to fix it?", "@caisq #12240 "]}, {"number": 12240, "title": "Update tensor_util.py and fix metric_ops.py", "body": "Added deprecated note", "comments": ["Can one of the admins verify this patch?", "@alanyee, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo, @tensorflower-gardener and @lukaszkaiser to be potential reviewers."]}, {"number": 12239, "title": "Fix tf.image.central_crop return zero dimension", "body": "This fix tries to address the issue raised in #10315 where tf.image.central_crop will return (0, 0, 3) when input is (240, 320, 3).\r\n\r\nThe issue is that the calculation was done by dividing\r\n```\r\nfraction_offset = int(1 / ((1 - central_fraction) / 2.0))\r\n```\r\nwhich causes deviation when accuracy is not enough.\r\n\r\nThis fix did the calculation in double (and use multiply) until the last step.\r\n\r\nA test case has been added.\r\n\r\nThis fix fixes #10315.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @vrv and @keveman to be potential reviewers.", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please", "Rerun of android http://ci.tensorflow.org/job/tensorflow-pull-requests-android/6612\r\n", "android rerun again http://ci.tensorflow.org/job/tensorflow-pull-requests-android/6673/console", "Trying again. Jenkins, test this please.", "Thanks @drpngx for the help. The Jenkins test failure was caused by `//tensorflow/tools/api/tests:api_compatibility_test` as the `--update_goldens True` was not run. \r\n\r\nI have rebased and updated the PR with goldens updated. Please take a look and sorry for the inconvenience.\r\n", "Thanks! (The api compatibility test was my mistake)\r\n\r\nJenkins, test this please.", "Still a probelm with the golden, can you take a look?", "Thanks @drpngx. The api compatibility error was caused by the recent updates from the other PR:\r\nhttps://github.com/tensorflow/tensorflow/commit/b6f253429c475d1a6f80702feadfff8ff9409156\r\n\r\nI have rebased and updated the PR. Now the golden error should be gone. Please take a look.", "Jenkins, test this please.", "The Jenkins error is `//tensorflow/python/kernel_tests:fft_ops_test`. Not sure about it though I tend to think it is unrelated.", "Yes, we have the fix internally. Pushing that."]}, {"number": 12238, "title": "Use python_bin_path to check for site packages", "body": "This fix tries to fix the issue raised in #12232 where the python invoked the configure.py was used to check the site packages.\r\n\r\nHowever, the site packages should be checked by the provided python_bin_path.\r\n\r\nThis fix fixes the issue. Below is the output after fix:\r\n```\r\nubuntu@ubuntu:~/tmp/tensorflow$ ./configure\r\nYou have bazel 0.5.3 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: python\r\nInvalid python path: python cannot be found.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\nFound possible Python library paths:\r\n/usr/local/lib/python3.5/dist-packages\r\n/usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is /usr/local/lib/python3.5/dist-packages\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]:\r\n\r\n...\r\nubuntu@ubuntu:~/tmp/tensorflow$ ./configure\r\nYou have bazel 0.5.3 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\nFound possible Python library paths:\r\n/usr/local/lib/python2.7/dist-packages\r\n/usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]:\r\n```\r\n\r\nThis fix fixes #12232.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Jenkins, test this please.", "Thanks @yifeif for the review. The PR has been updated with decode added.", "Jenkins, test this please.", "Thanks @yongtang! @tensorflow-jenkins test this please.", "Looks like an infra failure, but just in case @tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please"]}, {"number": 12237, "title": "CUDA_ERROR_OUT_OF_MEMORY", "body": "```\r\n$ python run_atari.py\r\n2017-08-12 08:25:13.924739: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.924772: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.924778: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.924781: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.924785: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.924993: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.925079: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.925109: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.925135: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.925154: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.925180: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.925191: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.925209: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.925249: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.925290: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.930447: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.930465: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.930468: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.930471: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.930473: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.932529: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.932550: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.932555: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.932560: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.932564: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.938520: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.938540: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.938546: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.938550: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.938554: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.943387: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.943402: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.943406: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.943409: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.943412: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.969437: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.969456: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.969460: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.969462: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:13.969465: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-12 08:25:14.793472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:\r\nname: Quadro P400\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.2525\r\npciBusID 0000:01:00.0\r\nTotal memory: 1.94GiB\r\nFree memory: 1.71GiB\r\n2017-08-12 08:25:14.793509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0\r\n2017-08-12 08:25:14.793515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y\r\n2017-08-12 08:25:14.793530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro P400, pci bus id: 0000:01:00.0)\r\n2017-08-12 08:25:14.793544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:\r\nname: Quadro P400\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.2525\r\npciBusID 0000:01:00.0\r\nTotal memory: 1.94GiB\r\nFree memory: 1.71GiB\r\n2017-08-12 08:25:14.793565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0\r\n2017-08-12 08:25:14.793569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y\r\n2017-08-12 08:25:14.793574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro P400, pci bus id: 0000:01:00.0)\r\n2017-08-12 08:25:14.794371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:\r\nname: Quadro P400\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.2525\r\npciBusID 0000:01:00.0\r\nTotal memory: 1.94GiB\r\nFree memory: 1.71GiB\r\n2017-08-12 08:25:14.794393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0\r\n2017-08-12 08:25:14.794400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y\r\n2017-08-12 08:25:14.794414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro P400, pci bus id: 0000:01:00.0)\r\n2017-08-12 08:25:14.797353: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1.52G (1631518720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.798216: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 199.94M (209649664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.798345: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1.37G (1468366848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.802545: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1.23G (1321530112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.803248: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1.11G (1189377024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.803964: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1020.85M (1070439424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.804914: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 918.77M (963395584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.806389: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 826.89M (867056128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n[2017-08-12 08:25:14,806] Making new env: PongNoFrameskip-v4\r\n2017-08-12 08:25:14.807060: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 744.20M (780350464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n[2017-08-12 08:25:14,807] Making new env: PongNoFrameskip-v4\r\n2017-08-12 08:25:14.811390: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 669.78M (702315520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.812196: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 602.80M (632083968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.813940: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 542.52M (568875776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.814633: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 488.27M (511988224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.814680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:\r\nname: Quadro P400\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.2525\r\npciBusID 0000:01:00.0\r\nTotal memory: 1.94GiB\r\nFree memory: 5.94MiB\r\n2017-08-12 08:25:14.814703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0\r\n2017-08-12 08:25:14.814712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y\r\n2017-08-12 08:25:14.814728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro P400, pci bus id: 0000:01:00.0)\r\n2017-08-12 08:25:14.818246: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 439.44M (460789504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.818910: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 7.94M (8323072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.819004: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 395.50M (414710528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.819713: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 7.14M (7490816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.820048: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 355.95M (373239552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.820746: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 6.43M (6741760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.822546: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 320.35M (335915776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.823119: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 5.79M (6067712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.823313: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 288.32M (302324224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.823724: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 5.21M (5460992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.823876: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 259.49M (272091904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.828059: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 4.69M (4914944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.828254: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 233.54M (244882944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.830112: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 4.22M (4423680 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.830394: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 210.18M (220394752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.831054: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 189.17M (198355456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.834707: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 170.25M (178520064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.835248: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 153.22M (160668160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.835705: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 137.90M (144601344 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n[2017-08-12 08:25:14,835] Making new env: PongNoFrameskip-v4\r\n2017-08-12 08:25:14.837829: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 124.11M (130141440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.838592: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 111.70M (117127424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.840518: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 100.53M (105414912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.841699: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 90.48M (94873600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.845234: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 81.43M (85386240 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.845857: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 73.29M (76847616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.846513: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 65.96M (69163008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.847151: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 59.36M (62246912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.849240: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 53.43M (56022272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.850945: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 48.08M (50420224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.851940: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 43.28M (45378304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.855851: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 38.95M (40840704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.856374: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 35.05M (36756736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.857234: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 31.55M (33081088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.857762: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 28.39M (29773056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.858341: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 25.55M (26795776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.858817: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 23.00M (24116224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.859718: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 20.70M (21704704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.860385: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 18.63M (19534336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:14.860857: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 16.77M (17581056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n[2017-08-12 08:25:14,863] Making new env: PongNoFrameskip-v4\r\n2017-08-12 08:25:15.081855: E tensorflow/core/common_runtime/direct_session.cc:138] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN\r\nTraceback (most recent call last):\r\n  File \"run_atari.py\", line 55, in <module>\r\n    main()\r\n  File \"run_atari.py\", line 52, in main\r\n    train('PongNoFrameskip-v4', num_timesteps=40e6, seed=0, num_cpu=8)\r\n  File \"run_atari.py\", line 24, in train\r\n    sess = U.single_threaded_session()\r\n  File \"/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py\", line 233, in single_threaded_session\r\n    return make_session(1)\r\n  File \"/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py\", line 228, in make_session\r\n    return tf.Session(config=tf_config)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1292, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 562, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n2017-08-12 08:25:15.099150: E tensorflow/core/common_runtime/direct_session.cc:138] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN\r\nTraceback (most recent call last):\r\n  File \"run_atari.py\", line 55, in <module>\r\n    main()\r\n  File \"run_atari.py\", line 52, in main\r\n    train('PongNoFrameskip-v4', num_timesteps=40e6, seed=0, num_cpu=8)\r\n  File \"run_atari.py\", line 24, in train\r\n    sess = U.single_threaded_session()\r\n  File \"/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py\", line 233, in single_threaded_session\r\n    return make_session(1)\r\n  File \"/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py\", line 228, in make_session\r\n    return tf.Session(config=tf_config)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1292, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 562, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n2017-08-12 08:25:15.121723: E tensorflow/core/common_runtime/direct_session.cc:138] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 2086797312\r\nTraceback (most recent call last):\r\n  File \"run_atari.py\", line 55, in <module>\r\n    main()\r\n  File \"run_atari.py\", line 52, in main\r\n    train('PongNoFrameskip-v4', num_timesteps=40e6, seed=0, num_cpu=8)\r\n  File \"run_atari.py\", line 24, in train\r\n    sess = U.single_threaded_session()\r\n  File \"/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py\", line 233, in single_threaded_session\r\n    return make_session(1)\r\n  File \"/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py\", line 228, in make_session\r\n    return tf.Session(config=tf_config)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1292, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 562, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n2017-08-12 08:25:15.138143: E tensorflow/core/common_runtime/direct_session.cc:138] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN\r\nTraceback (most recent call last):\r\n  File \"run_atari.py\", line 55, in <module>\r\n    main()\r\n  File \"run_atari.py\", line 52, in main\r\n    train('PongNoFrameskip-v4', num_timesteps=40e6, seed=0, num_cpu=8)\r\n  File \"run_atari.py\", line 24, in train\r\n    sess = U.single_threaded_session()\r\n  File \"/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py\", line 233, in single_threaded_session\r\n    return make_session(1)\r\n  File \"/nohome/jaan/abhishek/code/baselines/baselines/common/tf_util.py\", line 228, in make_session\r\n    return tf.Session(config=tf_config)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1292, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 562, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\n[2017-08-12 08:25:16,309] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\n[2017-08-12 08:25:16,310] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\n[2017-08-12 08:25:16,344] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\n[2017-08-12 08:25:16,344] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\n[2017-08-12 08:25:16,373] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\n[2017-08-12 08:25:16,373] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\n[2017-08-12 08:25:16,385] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\n[2017-08-12 08:25:16,385] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\n2017-08-12 08:25:16.507902: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1.51G (1617277696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-08-12 08:25:16.508376: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1.51G (1617277696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nJob 1, 'python run_atari.py' has stopped\r\n```\r\n\r\nIt fails, even when the memory is there!\r\n```\r\nTotal memory: 1.94GiB\r\nFree memory: 1.71GiB\r\n2017-08-12 08:25:14.794393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0\r\n2017-08-12 08:25:14.794400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y\r\n2017-08-12 08:25:14.794414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro P400, pci bus id: 0000:01:00.0)\r\n2017-08-12 08:25:14.797353: E tensorflow/stream_executor/cuda/cuda_driver.cc:924] failed to allocate 1.52G (1631518720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n```\r\n\r\n\r\n```\r\n$ nvidia-smi\r\nSat Aug 12 08:34:26 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro P400         Off  | 0000:01:00.0      On |                  N/A |\r\n| 41%   50C    P8    12W /  N/A |    116MiB /  1990MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K40c          Off  | 0000:03:00.0     Off |                    0 |\r\n| 26%   49C    P8    21W / 235W |      0MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla K40c          Off  | 0000:04:00.0     Off |                    0 |\r\n| 28%   54C    P8    30W / 235W |      0MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1835    G   /usr/lib/xorg/Xorg                              60MiB |\r\n|    0      1946    G   gnome-shell                                     54MiB |\r\n+-----------------------------------------------------------------------------+\r\n```", "comments": ["Is the run_atari.py file [this file](https://github.com/openai/baselines/blob/master/baselines/trpo_mpi/run_atari.py)? If so, you probably want to file this issue with that project."]}, {"number": 12236, "title": "\"/gpu:0/stream\" not showing up on timeline", "body": "I am generating timeline with TensorFlow r1.3.\r\nI can only see `/job:localhost/replica:0/task:0/cpu:0` and `/job:localhost/replica:0/task:0/gpu:0`.\r\nBut I can't find `/gpu:0/stream`\r\n\r\nAny idea?\r\n\r\n![image](https://user-images.githubusercontent.com/425637/29240963-bb7047dc-7f24-11e7-9cd3-2422c97487b0.png)\r\n\r\nMy code to regenerate the timeline.\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import timeline\r\n\r\ndim = 6000\r\nn = 5\r\n\r\nwith tf.device('/gpu:0'):\r\n    X, Y, Z, _X, _Y = [], [], [], [], []\r\n    for i in range(n):\r\n        X.append(tf.random_uniform([dim, dim], 0, 10, name='X' + str(i)))\r\n        Y.append(tf.random_uniform([dim, dim], 0, 10, name='Y' + str(i)))\r\n        _X.append(tf.placeholder(dtype=tf.float32, shape=[dim, dim]))\r\n        _Y.append(tf.placeholder(dtype=tf.float32, shape=[dim, dim]))\r\n        Z.append(tf.matmul(_X[i], _Y[i]))\r\n    W = tf.ones([dim, dim])\r\n    for i in range(n):\r\n        W = tf.matmul(W, Z[i])\r\n\r\nsess = tf.Session(config=tf.ConfigProto(\r\n                      graph_options=tf.GraphOptions(build_cost_model=1),\r\n                      gpu_options=tf.GPUOptions(allow_growth=True)))\r\nsess.run(tf.global_variables_initializer())\r\nX_, Y_ = sess.run([X, Y])\r\n\r\nrun_metadata = tf.RunMetadata()\r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n\r\nW_ = sess.run(W,\r\n              {_i: i_ for _i, i_ in zip(_X + _Y, X_ + Y_)},\r\n              options=run_options,\r\n              run_metadata=run_metadata)\r\n\r\ntf.profiler.profile(\r\n    tf.get_default_graph(),\r\n    run_meta=run_metadata,\r\n    options=(tf.profiler.ProfileOptionBuilder(\r\n        tf.profiler.ProfileOptionBuilder.time_and_memory()\r\n    ).build()))\r\n\r\ntl = timeline.Timeline(run_metadata.step_stats)\r\nwith open('/tmp/timeline.json', 'w') as f:\r\n    f.write(tl.generate_chrome_trace_format(show_dataflow=True,\r\n                                            show_memory=True))\r\n```", "comments": ["I cannot reproduce on 1.3 RC2 or on head. I have a \"/gpu:0/stream:all Compute\" section when I run the code and load `/tmp/timeline.json` into Chrome.\r\n\r\n@prb12, any ideas what the issue is?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I can no longer reproduce it on r1.4. Close it now."]}, {"number": 12235, "title": "Feature: add safe mode for tf.cast", "body": "tf.cast allows to cast float value to int without any warnning. It seems quite unsafe and data will be truncated.\r\n\r\n```python\r\nIn [34]: tf.__version__\r\nOut[34]: '1.2.1'\r\n\r\nIn [35]: sess.run(tf.cast(tf.constant(100000, tf.float32), tf.int16\r\n))\r\nOut[35]: -31072\r\n```\r\n\r\nPerhaps  it will be better, like [astype](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.astype.html) in numpy, to add a `casting` to support both safe and unsafe casting mode.\r\n\r\nI can work on it if possible.\r\n", "comments": ["Same problem for `tf.to_xxx` methods:\r\n\r\n```python\r\nIn [37]: sess.run(tf.to_int32(tf.constant(10000000000, tf.float64)))\r\nOut[37]: -2147483648\r\n```", "If I understand correctly, you can use tf.saturate_cast() instead of cast().\r\n\r\n**Result by cast()**\r\n![pre11](https://user-images.githubusercontent.com/28051014/29648843-56756302-88cb-11e7-9717-5bfd745a58ed.png)\r\n![pre12](https://user-images.githubusercontent.com/28051014/29648845-57bfd90e-88cb-11e7-8123-50aacf6a4d21.png)\r\n\r\n**Result by saturate_cast()**\r\n![pre13](https://user-images.githubusercontent.com/28051014/29648849-5958d02c-88cb-11e7-854a-c6ada219d995.png)\r\n![pre14](https://user-images.githubusercontent.com/28051014/29648850-5a77ff32-88cb-11e7-87f8-62472dca5254.png)\r\n", "Thanks for your suggestion, @Edwin222 . `saturate_cast` is a good alternative."]}, {"number": 12234, "title": "Lack of documentation in tf.decode_raw", "body": "Hi, \r\ni need to load arrays stored in tfrecords files, so i need to convert the raw string/byte data to number, but i don't understand the output i get using `tf.decode_raw`. Unfortunately function documentation doesn't help me.\r\n\r\nHere an example of the strange (for me) behavior of this function\r\n\r\n```\r\nimport numpy\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\n\r\n\r\narray = numpy.array([0.1, 0.2, 0.4, 0.8, 0.9, 1.1])\r\nprint(array.tobytes())\r\nprint(numpy.fromstring(array.tobytes()))\r\n\r\ntensoraw = tf.constant(array.tobytes())\r\n\r\nprint(sess.run(tensoraw))\r\nprint(sess.run(tf.decode_raw(tensoraw, tf.float32)))\r\n\r\nrawArray = sess.run(tensoraw)\r\ndecodedArray = sess.run(tf.decode_raw(tensoraw, tf.float32))\r\nprint(numpy.fromstring(rawArray))\r\nprint(numpy.fromstring(decodedArray))\r\n```\r\nwith this output\r\n```\r\nb'\\x9a\\x99\\x99\\x99\\x99\\x99\\xb9?\\x9a\\x99\\x99\\x99\\x99\\x99\\xc9?\\x9a\\x99\\x99\\x99\\x99\\x99\\xd9?\\x9a\\x99\\x99\\x99\\x99\\x99\\xe9?\\xcd\\xcc\\xcc\\xcc\\xcc\\xcc\\xec?\\x9a\\x99\\x99\\x99\\x99\\x99\\xf1?'\r\n\r\n[ 0.1  0.2  0.4  0.8  0.9  1.1]\r\n\r\nb'\\x9a\\x99\\x99\\x99\\x99\\x99\\xb9?\\x9a\\x99\\x99\\x99\\x99\\x99\\xc9?\\x9a\\x99\\x99\\x99\\x99\\x99\\xd9?\\x9a\\x99\\x99\\x99\\x99\\x99\\xe9?\\xcd\\xcc\\xcc\\xcc\\xcc\\xcc\\xec?\\x9a\\x99\\x99\\x99\\x99\\x99\\xf1?'\r\n\r\n[ -1.58818684e-23   1.44999993e+00  -1.58818684e-23   1.57499993e+00\r\n  -1.58818684e-23   1.69999993e+00  -1.58818684e-23   1.82499993e+00\r\n  -1.07374184e+08   1.84999990e+00  -1.58818684e-23   1.88749993e+00]\r\n\r\n[ 0.1  0.2  0.4  0.8  0.9  1.1]\r\n\r\n[ 0.1  0.2  0.4  0.8  0.9  1.1]\r\n```\r\n\r\nThe strangest thing for me is that numpy.fromstring applied both to the raw tensor and the decoded tensor (evaluated) gives the same output.\r\nThank you.", "comments": ["`decode_raw` in converting a string to an array of numbers, in a datatype specified.\r\n\r\nnumpy's `tobytes` is producing a string of bytes, 8 bytes per value to represent the default numpy float value (64 bit) - so you end up with 48 bytes in your string.\r\n\r\nAt that point, calling `decode_raw` with float32 as the desired data type is producing 12 values instead of the original 6 (since you've halved the bytes per value). If you called the operation with `tf.uint8` you'd see 48 values - each byte as an int value - and if you called it with `tf.float64` you'd get your original array values back (because you're just converting back to the original datatype.)\r\n\r\nHope this helps.\r\n\r\n", "Oh ok! This is because numpy arrays have float64 data type as default!\r\nWith `array = numpy.array([0.1, 0.2, 0.4, 0.8, 0.9, 1.1]).astype(numpy.float32)` i have back the original values with `tf.decode_raw(tensoraw, tf.float32)` (you don't say? :) )\r\n\r\nThank you so much. I don't know if the issue can be closed, or if the `decode_raw` documentation still needs a better description of the \"Returns:\" section.", "Closing since it seems the issue was not with the documentation, but with confusion over the fact the default numpy type is float64, not float32. Feel free to reopen if you still think the documentation is unclear."]}, {"number": 12233, "title": "add new op for tensorflow in windows os", "body": "Hello,\r\nhow can create new op in window os (a example or manual).\r\n\r\nthe same link in  linux ( https://www.tensorflow.org/extend/adding_an_op#build_the_op_library)\r\n\r\nThanks.\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12232, "title": "Default Python lib paths are not matched with the Python exe user inputted when 'configure'", "body": "Here is the output of 'configure':\r\n```\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\nFound possible Python library paths:\r\n/usr/local/lib/python2.7/dist-packages\r\n/usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is /usr/local/lib/python2.7/dist-packages\r\n...\r\n```\r\nIn above,  user typed `Python3` for the location but the library paths found are with `Python2`. \r\n\r\n### 1. Env\r\n  1.1 OS: Ubuntu 16.04 LTS (64 bit) with CUDA 8\r\n  1.2 Bazel Build label: 0.5.3\r\n  1.3 Python:\r\n        - Python 2.7 and Python 3.5 are installed\r\n        - Default `python` cmd in this OS is `python2.7`\r\n  1.4 Tensorflow `git rev-parse HEAD`:  863d7e7f0202cf5b513f2e3e691e7686562c8d74\r\n### 2. Desc\r\n  Using `python3 configure.py` to start configuration instead of `./configure` would save the trouble directly, but may not preferable to others.\r\n  \r\n", "comments": ["Added a PR #12238 to address the issue."]}, {"number": 12231, "title": "Tensorflow inception speed issue", "body": "hello this is Rohan Naik,\r\ni developed an app which interact with tensorflow running on server and shows parking spaces available for your car.\r\ni am using inception model which was retrained and modified to process live video and tell the users where parking slots are available.\r\nThe issue is it takes a 10 seconds to process 8 thread which run the image recognition function. Is there any way i can make it run faster.\r\nIs \"tensorflow slim\" model can process the images faster?\r\n[code.txt](https://github.com/tensorflow/tensorflow/files/1219473/code.txt)\r\n\r\n", "comments": ["If your concern is speed, I saw a ~25% improvement when I switched to c++ over python for image classification (not training). I know in theory the heavy lifting in python is handled by compiled c/c++ binaries, but in practice it looks like there is still some slowdown.\r\n\r\nAlso, you may see substantial speedup if you use a GPU, assuming you are running many images through the classifier.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12230, "title": "(Reopened PR) Add handle_ties arg to in_top_k Op to deal with ties.(#10767)", "body": "Reopened PR for [10767](https://github.com/tensorflow/tensorflow/issues/10767)\r\nAdd handle_ties argument to in_top_k Op to make the Op more clear.", "comments": ["Can one of the admins verify this patch?", "@nolanliou, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @vrv to be potential reviewers.", "@martinwicke please review it if you have time. thanks.", "Few questions: Is the shape function using the new information? Was the shape function wrong before?\r\n\r\nThe new default changes behavior (although it has strictly stronger guarantees, so I think it's fine). But we should call this out very explcitly in relnotes. Can you add relnotes for this change?\r\n\r\nShould we make a similar change for topk so the two are aligned again?", "@martinwicke \r\n1. The shape function is right before and this change don't affect the input and output shape.\r\n2. where is the relnotes?  sorry, I cannot find it.\r\n3.  `If two elements are equal, the lower-index element appears first.`, The topk strategy is different from InTopK, Is align necessary?", "I had a couple questions about the SAMPLE method and the intended use:\r\n\r\n1. would it be useful to have a mode that matches top_k exactly, so that any class returning true here would be included in the equivalent top_k call?\r\n\r\n2. what is SAMPLE used for?  My understanding of the code is that it doesn't guarantee exactly k classes return true.  Is that right?", "@cwhipkey \r\n1. So it is necessary to align with topk. \r\n2. SAMPLE means we choose some elements from the ties for guarantee exactly k classes. the result k classes may contain the object or not.\r\n\r\nActually, there is not a perfect way to deal with the tie situation. you could have a look at the [issue](https://github.com/tensorflow/tensorflow/issues/10767).", "I think aligning with top_k would be good. How hard would that be?"]}, {"number": 12229, "title": "Fix error caused in MonitoredSession.close()", "body": "This fix tries to address the error raised in #12224:\r\n```\r\n>>> import tensorflow as tf\r\n>>> with tf.train.MonitoredTrainingSession() as sess:\r\n...   sess.close()\r\n...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 534, in __exit__\r\n    self._close_internal(exception_type)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 569, in _close_internal\r\n    self._sess.close()\r\nAttributeError: 'NoneType' object has no attribute 'close'\r\n```\r\n\r\nThe error was caused by double call on `_internal_close()`.\r\n\r\nThis fix fixes the issue by skipping the error at the end of the `close()`.\r\n\r\nThis fix fixes #12224\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Also, it would be good to understand under what circumstances `self._sess` can be `None`. Maybe there is a bug elsewhere that should be fixed. If not, a unit test is needed to cover the case.", "Thanks @caisq for the review. The PR has been updated. A test case has also been added.", "@ispirmustafa Thanks for the review. The PR has been updated with error message outputted. Please take a look."]}, {"number": 12228, "title": "Allow mixed ints and Dimensions for reshape shape param", "body": "CF issue #11974 \r\n\r\nI think this should fix issue with PR #12127 by mapping the dimensions to a new list, avoiding any attempt to assign to the shape param if it is a tuple.", "comments": ["Can one of the admins verify this patch?", "@pmccarter, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @aselle and @keveman to be potential reviewers.", "Hi, pmccarter. PR #12127 has been revised in the case of tuple argument, see [commit](https://github.com/tensorflow/tensorflow/pull/12127/commits/74e89e4c693251743bd28b941712317a233be6af). If possible, could you take a review? Thanks.", "Closing this out in favor of #12127"]}, {"number": 12227, "title": "Invalid argument: NodeDef mentions attr 'Tshape' not in Op<name=Reshape; signature=tensor:T, shape:int32 -> output:T; attr=T:type>;", "body": "I got this kind of error message once my Android app launced.\r\n\r\nError message\r\n\r\n    08-12 10:28:32.307 25515-25533/? E/native: executor.cc:334 Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'Tshape' not in Op<name=Reshape; signature=tensor:T, shape:int32 -> output:T; attr=T:type>; NodeDef: pool_3/_reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool_3, pool_3/_reshape/shape)\r\n                                               \t [[Node: pool_3/_reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool_3, pool_3/_reshape/shape)]]\r\n    08-12 10:28:32.352 25515-25533/? I/native: tensorflow_jni.cc:299 End computing. Ran in 786ms (786ms avg over 1 runs)\r\n    08-12 10:28:32.357 25515-25533/? A/native: tensorflow_jni.cc:304 Error during inference: Invalid argument: NodeDef mentions attr 'Tshape' not in Op<name=Reshape; signature=tensor:T, shape:int32 -> output:T; attr=T:type>; NodeDef: pool_3/_reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool_3, pool_3/_reshape/shape)\r\n                                               \t [[Node: pool_3/_reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool_3, pool_3/_reshape/shape)]]\r\n    08-12 10:28:32.357 25515-25533/? A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 25533 (InferenceThread)\r\n\r\n\r\nThese two line of codes that possibility produced the error message\r\n\r\n1. https://github.com/datomnurdin/AndroidTensorFlowBirdExample/blob/master/app/src/main/jni/include/tensorflow/core/common_runtime/executor.cc\r\n\r\n2. https://github.com/datomnurdin/AndroidTensorFlowBirdExample/blob/master/app/src/main/jni/tensorflow_jni.cc\r\n\r\nPlease advice. Thank you.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@datomnurdin any update about this issue? I am having the same error"]}, {"number": 12226, "title": "Fix issues due to old numpy versions in dist_test and gcs_test", "body": "Previously numpy was installed with apt-get, leading to a version too\r\nold to be compatible with the recently-added autograd dependency of\r\ntensorflow. This change set fixes that.", "comments": ["@caisq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener and @andrewharp to be potential reviewers.", "Test failures are all unrelated.", "Jenkins, test this please."]}, {"number": 12225, "title": "request: Make tensorflow checkpoints portable", "body": "### System information\r\nNot applicable, this is a general feature request.\r\n\r\n### Describe the problem\r\nFeature request: make checkpoint files portable.\r\nCheckpoint files are not portable, because they use absolute paths. If I move the directory containing a trained graph, and then try to restore from a checkpoint, I get \"unable to match files\" errors because tensorflow does not know to look in the checkpoint directory for checkpoint files.", "comments": ["`tf.train.Saver(save_relative_paths=True)`"]}, {"number": 12224, "title": "MonitoredSession has no `close`", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Minimum custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: mac os 10.12.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nBug: MonitoredSession has no `close` but it should have according to https://www.tensorflow.org/api_docs/python/tf/train/MonitoredSession#close\r\n\r\nI think in some cases, self._sess is not initialized.\r\n\r\n### Source code / logs\r\nminimum code to reproduce:\r\nwith tf.train.MonitoredTrainingSession() as sess:\r\n  sess.close()\r\n\r\ncompared to working code using simple session:\r\nwith tf.Session() as sess:\r\n  sess.close()", "comments": ["I see it in the current master - private class _MonitoredSession implements it here https://github.com/tensorflow/tensorflow/blob/863d7e7f0202cf5b513f2e3e691e7686562c8d74/tensorflow/python/training/monitored_session.py#L525\r\nand MonitoredSession subclasses that class here https://github.com/tensorflow/tensorflow/blob/863d7e7f0202cf5b513f2e3e691e7686562c8d74/tensorflow/python/training/monitored_session.py#L589\r\n", "The issue is not about the existence of `close()` as `close()` has already been defined. The issue is that:\r\n```\r\nwith tf.train.MonitoredTrainingSession() as sess:\r\n    sess.close()\r\n```\r\nactually invokes `_close_internal()` twice. One in the `sess.close()` call and another in the `__exit__()` call at the end of the with statement. The second call triggered a None:\r\n```\r\n>>> import tensorflow as tf\r\n>>> with tf.train.MonitoredTrainingSession() as sess:\r\n...   sess.close()\r\n... \r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 534, in __exit__\r\n    self._close_internal(exception_type)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 569, in _close_internal\r\n    self._sess.close()\r\nAttributeError: 'NoneType' object has no attribute 'close'\r\n```\r\n\r\nCreated a PR #12229 to fix the exception issue.", "@yongtang Thanks. Your explanation makes sense to me."]}, {"number": 12223, "title": "Branch 165019969", "body": "Needed a CP for 1.3", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @caisq, @petewarden and @tensorflower-gardener to be potential reviewers.", "The device_setter_test failure looks like a real bug:\r\n\r\nERROR: testBasic (__main__.RandomStrategyTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/bazel_pip/tensorflow/contrib/training/device_setter_test.runfiles/org_tensorflow/bazel_pip/tensorflow/contrib/training/python/training/device_setter_test.py\", line 42, in testBasic\r\n    u = variables.Variable(array_ops.zeros([2, 2]))\r\n  File \"/workspace/pip_test/venv/lib/python3.4/site-packages/tensorflow/python/ops/variables.py\", line 208, in __init__\r\n    constraint=constraint)\r\n  File \"/workspace/pip_test/venv/lib/python3.4/site-packages/tensorflow/python/ops/variables.py\", line 326, in _init_from_args\r\n    name=name)\r\n  File \"/workspace/pip_test/venv/lib/python3.4/site-packages/tensorflow/python/ops/state_ops.py\", line 131, in variable_op_v2\r\n    shared_name=shared_name)\r\n  File \"/workspace/pip_test/venv/lib/python3.4/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 910, in _variable_v2\r\n    shared_name=shared_name, name=name)\r\n  File \"/workspace/pip_test/venv/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/workspace/pip_test/venv/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2669, in create_op\r\n    self._apply_device_functions(ret)\r\n  File \"/workspace/pip_test/venv/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 3438, in _apply_device_functions\r\n    op._set_device(device_function(op))  # pylint: disable=protected-access\r\n  File \"/workspace/pip_test/venv/lib/python3.4/site-packages/tensorflow/python/training/device_setter.py\", line 114, in device_function\r\n    ps_device.task = self._ps_strategy(op)\r\n  File \"/workspace/pip_test/venv/lib/python3.4/site-packages/tensorflow/contrib/training/python/training/device_setter.py\", line 51, in __call__\r\n    n = int(hashlib.md5(key).hexdigest(), 16)\r\nTypeError: Unicode-objects must be encoded before hashing\r\n", "Cmake has a hash mismatch which is weird, and metric_ops test is failing.", "Maybe just disable metric_ops and ask the author to fix it?\r\nLet me look at the Cmake error. (I ignored it since it seems perpetually broken :-( )", "@av8ramit something is fishy. The hash of the mirror and the original do not match:\r\n\r\n http://mirror.bazel.build/github.com/NVlabs/cub/archive/1.6.4.zip  => 966d0c4f41e2bdc81aebf9ccfbf0baffaac5a74f00b826b06f4dee79b2bb8cee\r\n\r\n https://github.com/NVlabs/cub/archive/1.6.4.zip => b35162e70fdfee4af4926b46dc3fbb89e06c57bb5b2e72bf6cc47e1f58cc2c75\r\n\r\n@jart @ebrevdo do you know how this could be?\r\n", "@av8ramit considering that the test catches the hash problem (whenever we fall back to the original) and it was already present before this push, I think it's fine to merge this.", "@rmlarsen I updated the cmake hash to match the file it downloads. The mirror link and cmake link have diverged which is why regular bazel is fine.", "Jenkins, test this please.", "Jenkins, test this please.", "Looks like we still have some build issues on GPU.\r\nMaybe we should create a new push?", "Yeah and I've deviated a couple commits now anyways. I'm fine closing and waiting for a new push."]}, {"number": 12222, "title": "Feature: Add dependency version", "body": "Tensor examples : \r\ntensorflow/tensorflow/examples/adding_an_op/\r\n\r\ndo not come with version of TensorFlow.\r\n\r\nSo, as tensorFlow API is still changing a lot, breaks appear when:\r\n    using older-examples  with new TF\r\n    new examples with older TF.\r\n\r\nPlease provide versionning for example scripts:\r\n\r\n1) Example of versioning in the comments of the example script:\r\n(to ensure we can reproduce it) : \r\n'''\r\nScript MNIST\r\nTensorFlow  1.0.1\r\nNumpy   1.9.1\r\n\r\n\r\n'''\r\n\r\n2) Please provide different version of exampleof scripts:\r\n   Major current version t\r\n   Major version t-1\r\n   Major version t-2\r\n\r\nQuestion:  Can you include example scripts as part of regression tests ?\r\n\r\n\r\n\r\n\r\n------------------------\r\n\r\n", "comments": ["The TensorFlow Python API should not change within a major version. Since we haven't released TF 2.0 yet, that means that all post-1.0 op-loading code should work.\r\n\r\nWe still haven't developed an official stable API for writing ops. However, in practice, this API has remained stable (at least AFAIK). Are you experiencing custom op instability between post-1.0 TF versions?", "Yes, some new functions concerning sparse input (feature_colums) work in 1.2 but not in 1.0.\r\n\r\nFor usage examples (ie regression type),\r\nwhich is outside of core TF, would it be possible to add at end of the script (maybe automatically) when the script is successfully run :\r\nOS /Os version\r\nTF version.\r\nPandas, numpy, scipy versions\r\n\r\nWe need to manage regular breaks with new API functions and need to run regression for each version and manage mutiple TF version at same time....\r\n\r\nIt becomes heavy to integrate recent features (but very useful).\r\n\r\n\r\nAlso having a roadmap over 1 year for API changes will really help for integration.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Ah sorry, I misspoke above. The API should remain backwards compatible, but you're right we may add new features that won't work in older versions of TF.\r\n\r\nI believe our documentation at tensorflow.org has documentation for all TF versions starting at r0.12 (https://www.tensorflow.org/versions/). Which usage examples are you referring to?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 12221, "title": "problem in imprting the tensorflow in phython", "body": "Hi,\r\nI installed tensorflow cpu version but I got the following error. actually, I am using python 3.5.2 \r\nso please anyone can help to solve this problem! \r\n\r\nPMicrosoft Windows [Version 6.1.7601]\r\nCopyright (c) 2009 Microsoft Corporation.  All rights reserved.\r\n\r\nC:\\Windows\\system32>python\r\nPython 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Install\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n", "comments": ["I see you are trying to install on Windows, @mrry has a great installation troubleshooter script that can help pinpoint issues available here: [mrry's Self-check Script](https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c)", "Thank you very much Jamie, it is work!  The problem was missing the\nmsvcp140.dll. for cpu version, I just add it to the path\n\nOn Mon, Aug 14, 2017 at 6:30 AM, Jamie Cooke <notifications@github.com>\nwrote:\n\n> I see you are trying to install on Windows, @mrry\n> <https://github.com/mrry> has a great installation troubleshooter script\n> that can help pinpoint issues available here: mrry's Self-check Script\n> <https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12221#issuecomment-322189430>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AdgMHLd_FrdR6GMRZLUNmGpg6A2RYWMeks5sYEvcgaJpZM4O0-R_>\n> .\n>\n", "Thanks for your help @jubjamie and @linaunlv for confirming the fix!", "Thank you  @mrry <https://github.com/mrry> I appreciate!\n\nOn Aug 15, 2017 1:36 PM, \"Derek Murray\" <notifications@github.com> wrote:\n\n> Closed #12221 <https://github.com/tensorflow/tensorflow/issues/12221>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12221#event-1207146003>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AdgMHOurFJZI8p5YYPHCwUD0NQsU0Tn5ks5sYgE-gaJpZM4O0-R_>\n> .\n>\n", "@mrry if you get a chance can you fix the syntax error in your check script. Stray ) on line 110. Means that when i refer people I dont have to talk them through the fix for that either :D", "Thanks for pointing that out Jamie! Just updated the gist."]}, {"number": 12220, "title": "Anyone working with tensorflow in visual studio ? I am facing some issues integrating tensorflow in vs2017. I can run the code of tensorflow in cmd but couldn't run it on vs. I hope someone comes up with a solution. Thanks in advance.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 12219, "title": "Reverting a commit that is causing compilation errors.", "body": "MKL optimized Tensorflow does not support EIGEN_USE_MKL_ALL currently.", "comments": ["Can one of the admins verify this patch?", "@agramesh1, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener and @suiyuan2009 to be potential reviewers."]}, {"number": 12218, "title": "Update metrics_op.py", "body": "Replaced contrib with direct equivalents outside of contrib", "comments": ["Can one of the admins verify this patch?"]}, {"number": 12217, "title": "Branch 164943597", "body": "", "comments": []}, {"number": 12216, "title": "Added the EMNIST dataset", "body": "This PR adds EMNIST to the datasets. EMNIST is an extension of the MNIST dataset but it also adds Characters, and more numbers. It can be found in the paper here: https://arxiv.org/abs/1702.05373v1", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "The problem in the initial test was down to it taking too long, but this is mainly caused by where the data is pulled from, as well as the size of the data. This problem wasn't really a concern for the MNIST set as it only had at most 70,000 images and labels, where as every set in the EMNIST is bigger, and can be up to 800,000 images and labels.", "@vincentvanhoucke is this a dataset we are interested in adding? I hadn't heard of it before this.", "@nealwu I don't know what's our policy in regards to accepting new datasets into TF.\r\nThis one only has 2 citations thus far: https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8838276906904533917&as_sdt=5", "Doesn't seem noteworthy enough to include then. Going to close this. If others think it is worth including, feel free to reopen."]}]