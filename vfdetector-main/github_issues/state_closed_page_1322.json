[{"number": 13441, "title": "tf.biderectional_dynamic_rnn get stuck when running the graph", "body": "I built a 1-layer bidirectional RNN with 128 hidden nodes using the `output = tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, inputs, seqlens, tf.float32, is_tuple=True, time_major=True)` interface. I run the network with input data size of 57x285x4608 ([time_step x batch_size x num_feature]) but get stuck in the `_outputs = sess.run(outputs, feeds)`. The system does not indicate any resource exhausted. When I reduce the time_step to 31, the network runs successfully. When I only reduce the number of 3rd dimension to 512, it still fails to work. It seems there is some constraints on the input sequence length.\r\n\r\nAny idea on this problem?\r\n\r\nI run this program on Nvidia DGX Server with 4 Tesla P100 GPUs. The OS is ubuntu 14.04.", "comments": ["Can you please give us your full reproduction information, including code? ", "Hi @cy89 , below is the code that reproduces the error. You can copy and run directly on your PC. The system I use is ubuntu 14.04 with Python3.4. The Tensorflow version is v1.1. \r\nThis phenomena is that the program get stuck at `sess.run(optimizer, feeds)`. But when I comment the batch normalization operation `logits = tf.contrib.layers.batch_norm(logits, is_training=True)` and `tmp = tf.contrib.layers.batch_norm(tmp, is_training=True)`, it run correctly.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\nimport math\r\nimport time\r\nimport nets.CTPNetCL_MSRA_TD500 as ctp\r\nimport scipy.io as io\r\nimport cv2\r\nimport os\r\nimport utils.preprocessing as pp\r\nfrom cmath import isnan\r\n\r\ndef weight_variable(shape, mean=0, stddev=1):\r\n    initial = tf.truncated_normal(shape, mean=mean, stddev=stddev)\r\n    return tf.Variable(initial)\r\n\r\n\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.01, shape=shape)\r\n    return tf.Variable(initial)\r\n\r\ndef birnn_1d_layer(inputs, seq_len, num_hidden=None, is_truple=True, cell_type='LSTM', time_major=True, scope=None):\r\n    if cell_type == 'LSTM': \r\n        forward_cell = tf.contrib.rnn.core_rnn_cell.LSTMCell(num_hidden, state_is_tuple=is_truple)\r\n        backward_cell = tf.contrib.rnn.core_rnn_cell.LSTMCell(num_hidden, state_is_tuple=is_truple)\r\n    elif cell_type == 'GRU':\r\n        forward_cell = tf.contrib.rnn.GRUCell(num_hidden)    \r\n        backward_cell = tf.contrib.rnn.GRUCell(num_hidden)\r\n    else :\r\n        exit(\"birnn_1d_layer: Unrecognized cell type.\")\r\n    \r\n    output_tuple = tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, inputs, seq_len, dtype=tf.float32, time_major=time_major, scope=scope)\r\n    outputs = tf.concat(output_tuple[0], 2)\r\n    return outputs\r\n\r\nH = 768\r\nW = 768\r\nC = 3\r\nbatch_size = 1\r\nstride = 16\r\nlearning_rate = 0.0001\r\ndecay_rate = 0.9\r\nsave_period = 3\r\nprob_period = 3\r\ndecay_steps = 1000\r\n\r\ninputs = tf.placeholder(tf.float32, [batch_size, H, W, C])\r\nlabels = tf.placeholder(tf.float32, [batch_size, H/stride, W/stride, 2])\r\n\r\nweight1 = weight_variable([3, 3, 3, 4608])\r\nb1 = weight_variable([4608])\r\ntmp = tf.nn.conv2d(inputs, weight1, strides=(1,16,16,1), padding='SAME')\r\ntmp = tf.nn.bias_add(tmp, b1)\r\ntmp = tf.contrib.layers.batch_norm(tmp, is_training=True)\r\ntmp = tf.nn.relu(tmp)\r\nshape = np.shape(tmp)\r\n# row major\r\ntmp = tf.transpose(tmp, [2, 0, 1, 3])\r\n# reshape to RNN\r\ntmp = tf.reshape(tmp, [shape[2].value, -1, shape[3].value])\r\n# feed to RNN\r\nseqlens = np.array([shape[2].value] *(shape[0].value*shape[1].value))\r\noutputs = birnn_1d_layer(tmp, seqlens, num_hidden=256)\r\n# reshape back to 2D feature map\r\noutputs = tf.transpose(tf.reshape(outputs, [shape[2].value, shape[0].value, shape[1].value, -1]), [1,2,0,3])\r\n# outputs to Logits\r\nweight2 = weight_variable([3, 3, np.shape(outputs)[3].value, 2])\r\nb2 = weight_variable([2])\r\nlogits = tf.nn.conv2d(outputs, weight2, strides=(1,1,1,1), padding='SAME')\r\nlogits = tf.nn.bias_add(logits, b2)\r\nlogits = tf.contrib.layers.batch_norm(logits, is_training=True)\r\n# loss\r\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, dim=-1)\r\n# cost\r\ncost = tf.reduce_mean(loss)\r\n\r\nglobal_step = tf.Variable(0, trainable=False)\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)).minimize(cost, global_step=global_step)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\n# testing data\r\n_inputs = np.random.randn(batch_size, H, W, C)\r\n_labels_positive = np.random.randint(0, 1, [batch_size, np.int32(H/stride), np.int32(W/stride)]);\r\n_labels_negative = 1 - _labels_positive\r\n_labels = np.stack((_labels_negative, _labels_positive), -1)\r\n\r\nfeeds = {inputs: _inputs, labels: _labels}\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    sess.run(optimizer, feeds)\r\n```\r\nThe log message  before getting stuck is shown below:\r\n```\r\nUsing TensorFlow backend.\r\nloading dataset 'MSRA_TD500'.\r\nloading finished: 2.849345s\r\nloading vgg-16 pretrained model from /home/tensor_v1/project/CRNN-Tensorflow/CRNN/nets/vgg16/vgg16.npy.\r\nnpy file loaded\r\n2017-10-09 21:48:58.425429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-10-09 21:48:58.425860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\r\npciBusID 0000:03:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.76GiB\r\n2017-10-09 21:48:58.425934: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0xbcaf9d0\r\n2017-10-09 21:48:58.643478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-10-09 21:48:58.643903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\r\npciBusID 0000:04:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\n2017-10-09 21:48:58.644864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 \r\n2017-10-09 21:48:58.644880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y \r\n2017-10-09 21:48:58.644886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y \r\n2017-10-09 21:48:58.644897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0)\r\n2017-10-09 21:48:58.644904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)\r\n\r\n```", "When I run the code on TensorFlow 1.3, I get the error `AttributeError: 'module' object has no attribute 'core_rnn_cell'`. @ZichuanLIU, can you update the program for TensorFlow 1.3, which is the latest version, and see if the problem still occurs?", "Sure. Will update it to the new version.\n\n\n\nGet Outlook for iOS<https://aka.ms/o0ukef>\n________________________________\nFrom: Reed <notifications@github.com>\nSent: Thursday, October 12, 2017 11:22:42 AM\nTo: tensorflow/tensorflow\nCc: #LIU ZICHUAN#; Mention\nSubject: Re: [tensorflow/tensorflow] tf.biderectional_dynamic_rnn get stuck when running the graph (#13441)\n\n\nWhen I run the code on TensorFlow 1.3, I get the error AttributeError: 'module' object has no attribute 'core_rnn_cell'. @ZichuanLIU<https://github.com/zichuanliu>, can you update the program for TensorFlow 1.3, which is the latest version, and see if the problem still occurs?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/13441#issuecomment-336009710>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AVGmIEMYq8oe5dCS3Y861XdXAqfUObRnks5srYYCgaJpZM4PqY9Z>.\n", "@reedwm I have update the program, which is shown below: \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\nimport math\r\nimport time\r\n# import cv2\r\nimport os\r\n\r\n\r\ndef weight_variable(shape, mean=0, stddev=1):\r\n    initial = tf.truncated_normal(shape, mean=mean, stddev=stddev)\r\n    return tf.Variable(initial)\r\n\r\n\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.01, shape=shape)\r\n    return tf.Variable(initial)\r\n\r\ndef birnn_1d_layer(inputs, seq_len, num_hidden=None, is_truple=True, cell_type='LSTM', time_major=True, scope=None):\r\n    if cell_type == 'LSTM': \r\n        forward_cell = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=is_truple)\r\n        backward_cell = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=is_truple)\r\n    elif cell_type == 'GRU':\r\n        forward_cell = tf.contrib.rnn.GRUCell(num_hidden)    \r\n        backward_cell = tf.contrib.rnn.GRUCell(num_hidden)\r\n    else :\r\n        exit(\"birnn_1d_layer: Unrecognized cell type.\")\r\n    \r\n    output_tuple = tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, inputs, seq_len, dtype=tf.float32, time_major=time_major, scope=scope)\r\n    outputs = tf.concat(output_tuple[0], 2)\r\n    return outputs\r\n\r\nH = 768\r\nW = 768\r\nC = 3\r\nbatch_size = 1\r\nstride = 16\r\nlearning_rate = 0.0001\r\ndecay_rate = 0.9\r\nsave_period = 3\r\nprob_period = 3\r\ndecay_steps = 1000\r\n\r\ninputs = tf.placeholder(tf.float32, [batch_size, H, W, C])\r\nlabels = tf.placeholder(tf.float32, [batch_size, H/stride, W/stride, 2])\r\n# vgg = vgg16.Vgg16()\r\n# vgg.build(inputs)\r\n# tmp = vgg.outputs\r\n\r\n\r\nweight1 = weight_variable([3, 3, 3, 4608])\r\nb1 = weight_variable([4608])\r\ntmp = tf.nn.conv2d(inputs, weight1, strides=(1,16,16,1), padding='SAME')\r\ntmp = tf.nn.bias_add(tmp, b1)\r\ntmp = tf.contrib.layers.batch_norm(tmp, is_training=True)\r\ntmp = tf.nn.relu(tmp)\r\nshape = np.shape(tmp)\r\n# row major\r\ntmp = tf.transpose(tmp, [2, 0, 1, 3])\r\n# reshape to RNN\r\ntmp = tf.reshape(tmp, [shape[2].value, -1, shape[3].value])\r\n# feed to RNN\r\nseqlens = np.array([shape[2].value] *(shape[0].value*shape[1].value))\r\noutputs = birnn_1d_layer(tmp, seqlens, num_hidden=256)\r\n# reshape back to 2D feature map\r\noutputs = tf.transpose(tf.reshape(outputs, [shape[2].value, shape[0].value, shape[1].value, -1]), [1,2,0,3])\r\n# outputs to Logits\r\nweight2 = weight_variable([3, 3, np.shape(outputs)[3].value, 2])\r\nb2 = weight_variable([2])\r\nlogits = tf.nn.conv2d(outputs, weight2, strides=(1,1,1,1), padding='SAME')\r\nlogits = tf.nn.bias_add(logits, b2)\r\n# logits = tf.layers.batch_normalization(logits, training=True)\r\nlogits = tf.contrib.layers.batch_norm(logits, is_training=True)\r\n# loss\r\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, dim=-1)\r\n# cost\r\ncost = tf.reduce_mean(loss)\r\n\r\nglobal_step = tf.Variable(0, trainable=False)\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)).minimize(cost, global_step=global_step)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\n# testing data\r\n_inputs = np.random.randn(batch_size, H, W, C)\r\n_labels_positive = np.random.randint(0, 1, [batch_size, np.int32(H/stride), np.int32(W/stride)]);\r\n_labels_negative = 1 - _labels_positive\r\n_labels = np.stack((_labels_negative, _labels_positive), -1)\r\n\r\nfeeds = {inputs: _inputs, labels: _labels}\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    sess.run(optimizer, feeds)\r\nprint('finished.')\r\n\r\n```", "I am also having a similar problem for RNNs, and I can't use batch norm because of it. It seems to be due to the tf.control_dependencies statement, it is waiting to update something that it is not getting. Any insight?\r\n\r\n", "/CC @ebrevdo, do you know what the problem is?", "Can you print the list of update_ops that you get?  It's possible some of them are actually *inside* the while_loop and that's creating the problem.  If I remember correctly, the batch norm code in contrib handles this by default by not creating any additional update ops.  @sguada ", "(more specifically, we should never be able to call session.run on a tensor/op that's inside a frame...  which is a bit of a bug.  fyi @mrry another case!)", "hmm; i may have been hasty!  the batch norm happens outside the dynamic rnn itself.", "can you please print the update ops, and also try running w/ optimisations disabled?  e.g. via\r\n\r\n```\r\nconfig = tf.ConfigProto(graph_options=tf.GraphOptions(\r\n  optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))\r\nsess = tf.Session(config=config)\r\n```", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Closing due to lack of activity. @ZichuanLIU Feel free to reopen if you have answers to @ebrevdo's questions.", "This is still an issue for us. We don't have a short example yet, but we can try to generate one. How did you solve it @LiuZichuan ? "]}, {"number": 13440, "title": "Fix typos", "body": "This PR fixes some typos: `Explicitely`, `ouput`, `Dimensiton`, `occurences`, `partiton`, `unknwon`, and `Summmary`.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 13439, "title": "Keras has much better gradients calculated than native TF", "body": "Hi,\r\nI am not sure if this is a bug in some TF function or Keras has just some clever ways to pull things off.\r\nI was prototyping a simple logistic regression model with Keras and trying to write the exact same model with TF to reproduce the result. However, there's something unexplainable to me that Keras always has much better gradients calculated than TF does when I use mini-batch SGD.\r\n\r\ntensorflow==1.2.1\r\nKeras==2.0.8\r\nGPU: Tesla P40\r\n\r\nKeras version:\r\n```python\r\ndef custom_objective(y_true, y_pred):\r\n    loss = tf.reduce_mean(-(y_true*tf.log(y_pred)+((1.0-y_true)*tf.log(1.0-y_pred))))\r\n    return loss\r\nmodel = Sequential()\r\nmodel.add(Dense(1,input_dim=2440000, activation='sigmoid', bias_initializer='zeros', kernel_initializer='zeros'))\r\nsgd = tf.train.GradientDescentOptimizer(0.5)\r\nmodel.compile(loss=custom_objective, optimizer=sgd)\r\nmodel.fit_generator(generator, steps_per_epoch=1, epochs=1, callbacks=[ival], max_queue_size=10, workers=1, use_multiprocessing=False, initial_epoch=0)\r\n```\r\nTF version:\r\n```python\r\ndef linear(x, n_input, n_output, name=None):\r\n    with tf.variable_scope(name or 'fc'):\r\n        W = tf.get_variable(\r\n            name = \"W\",\r\n            # shape = [n_input, n_output],\r\n            dtype=tf.float32,\r\n            # initializer=tf.contrib.layers.xavier_initializer())\r\n            initializer=tf.zeros(shape=[n_input,n_output]))\r\n        b = tf.get_variable(\r\n            name='bias',\r\n            shape=[n_output],\r\n            dtype=tf.float32,\r\n            initializer=tf.constant_initializer(0.0))\r\n        if not isinstance(x, tf.SparseTensor):\r\n            h = tf.nn.bias_add(\r\n                tf.matmul(x, W),\r\n                b,\r\n                name='h')\r\n        else:\r\n            h = tf.nn.bias_add(\r\n                tf.sparse_tensor_dense_matmul(x, W),\r\n                b,\r\n                name='h')\r\n    return h, W, b\r\n\r\ntf.reset_default_graph()\r\nX_shape = tf.placeholder(tf.int64, shape=[2], name=\"X_shape\")\r\nX_indices = tf.placeholder(tf.int64, name=\"X_indices\")\r\nX_values = tf.placeholder(tf.float32, shape=[None], name=\"X_values\")\r\ny = tf.placeholder(dtype=tf.float32, name=\"y\")\r\nH = tf.SparseTensor(indices=X_indices, values=X_values, dense_shape=X_shape)\r\nlogit, w, b = linear(H, 2440000, 1, name=\"output_layer\")\r\ny_pred = tf.nn.sigmoid(logit)\r\ntrain_error = -(y*tf.log(y_pred) + ((1.0 - y) * tf.log(1.0-y_pred)))\r\nloss = tf.reduce_mean(train_error)\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)\r\ngvs = optimizer.compute_gradients(loss,[w,b])\r\ntrain_op = optimizer.apply_gradients(gvs)\r\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True), graph=tf.get_default_graph())\r\nsess.run(tf.global_variables_initializer())\r\n```\r\n\r\nTL;DR\r\nKeras has better gradients calculated/updates than TF.\r\n\r\nBoth version implements a vanilla logistic regression, with **same native TF optimizer**, **same user defined cross entropy** and **same data generator**(except for Keras accepts a dense matrix and TF accepts sparse matrix.tocoo()), **same learning rate**, **same zero initializer for both w and b**.\r\nSimple calculus can show that if the first batch contains all NEGATIVE examples, the gradient for b in the first update must be exactly 0.5.\r\n\r\nIf a batch has very few examples (e.g 1-9), both version produce an exact gradient of 0.5 for b.\r\nWhen sample size goes above 9, Keras starts to have a way better gradients calculated for both b and w. For example, with sample size 10, Keras calculates 0.50000006 for b and TF gives 0.49999988. With sample size 12, Keras gives 0.49999994 but TF gives 0.50000012. Though both give wrong gradient, Keras is always better, not to mentions the weights gradients. Also trying casting the loss to float16, 32 or 64 won't make the gradient as good as Keras'.\r\n\r\nThe accumulated differences after 100 batches of training makes TF's model worse than Keras' in terms of AUC.\r\n\r\nAt this stage I am not sure where I should look for so I resort to the community to help me with this \"unexplainable\" phenomena. Any suggestion will be much appreciated.\r\n\r\nOscar\r\n", "comments": ["Did you try it with tf.float64?\r\nKeras is probably using float32 too, but maybe not?", "Hi @georgh yes I did try cast the loss to float16, 32 and 64. Float16 gave the worst result and 32,64 gave same result and it was worse than Keras', in terms of AUC.\r\nAnd yes I think Keras is using float32 for most of its calculations.", "The use of sparse already means that the two snippets are not equivalent on a limited-precision computer. Then you can't expect them to have the same numerical precision.", "@ppwwyyxx \r\nThank you for your response.\r\nThe weird thing is:\r\n1. If using batch size 1, I will have identical results even after 1000 batches.\r\n2. I did try using dense tensor to see if I can get rid of the differences, but I got the same outcome as with sparse tensor.\r\n\r\nUsing tf.float64 for both w and b  gave me a slightly better result than Keras, but only on the first batch, then things spiraled out after 1st batch and never converged while Keras did.\r\n\r\nI know I can just make TF's learning rate smaller and get a convergence, but\r\nI really want to know what's making Keras so good (or what's making the my TF version so bad) as they are both essentially using TF.", "I cannot reproduce your problem locally where both give the same precision.\r\nWhen using dense tensor the two code is still not guaranteed to be equivalent -- at least they may not generate the same graph and run with the same session config. This will depend on exact versions of both libraries to tell.", "Thank you @ppwwyyxx for running the code. Knowing your results is different than mine coudl be very helpful with future research.\r\nWhat you just said about the graph or session config or anything else is exactly what I wanted to dig out. I believe those \"magics\" would be very helpful for future implementations. :)", "@OscarDPan can you provide a full example that computes and prints the gradients? Also, to provide a minimal example, the example should use `tf.gradients` instead of using an optimizer. Thanks!", "Hi @reedwm,\r\n\r\nMy apologies for the late response. I haven't been able to access to my aws instance lately but I will provide the samples ASAP. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "My apologies. This is still an issue. I just haven't been able to going back to the problem and dig deeper. Should I close the issue for now and reopen it later?\r\n", "Sure, I'll close the issue for now."]}, {"number": 13438, "title": "The problem in saving and restoring LSTM models", "body": "I have trained a LSTM model and saved it as a small part of another model. the key code as follows:\r\n```python\r\n       initializer = tf.truncated_normal_initializer()\r\n        with tf.variable_scope('RNN_model', reuse=None, initializer=initializer):\r\n            train_rnn = RNNmodel.LSTMmodel(True, RNNmodel.TRAIN_BATCH_SIZE, RNNmodel.NUM_STEP)\r\n        with tf.variable_scope('RNN_model', reuse=True, initializer=initializer):\r\n            test_rnn = RNNmodel.LSTMmodel(False, RNNmodel.EVAL_BATCH_SIZE, RNNmodel.NUM_STEP)\r\n        init = tf.global_variables_initializer()\r\n        sess.run(init)\r\n        # train here\r\n        saver.save(sess, saver_path)\r\n```\r\n\r\neverything looks normal when i restore it `saver.restore(sess, saver_path)`, but when i run the epoch and reach to the code `sess.run(op)`, it throws an error saying that\r\n> FailedPreconditionError (see above for traceback): Attempting to use uninitialized value RNN_model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/kernel\r\n\r\nand i checked out the ckpt files and found that there was not any variables about LSTM cells so i wonder if `Saver().save` could save the variables in LSTM cells.Or i just had a wrong practice.\r\n", "comments": ["The LSTM models key code as follow:\r\n```python\r\nclass LSTMmodel(object):\r\n    def __init__(self, is_train, batch_size, num_step):\r\n        self.num_step = num_step\r\n        self.batch_size = batch_size\r\n\r\n        self.input_data = tf.placeholder(tf.float32, (batch_size, num_step, n_input))\r\n        self.target = tf.placeholder(tf.float32, (batch_size, n_class))\r\n\r\n        lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(front_d_hidden, forget_bias=1.0)\r\n        lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(front_d_hidden, forget_bias=1.0)\r\n        lstm_cell_3 = tf.contrib.rnn.BasicLSTMCell(front_d_hidden, forget_bias=1.0)\r\n\r\n        if is_train:\r\n            lstm_cell_1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_1, output_keep_prob=1.0)\r\n            lstm_cell_2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_2, output_keep_prob=1.0)\r\n            lstm_cell_3 = tf.contrib.rnn.DropoutWrapper(lstm_cell_3, output_keep_prob=1.0)\r\n        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2, lstm_cell_3])\r\n\r\n        self.initiate_state = cell.zero_state(batch_size, tf.float32)\r\n        if is_train:\r\n            self.input_data = tf.nn.dropout(self.input_data, KEEP_PROB)\r\n        outputs = []\r\n        state = self.initiate_state # state variable\r\n        with tf.variable_scope('RNN'):\r\n            for time_step in range(num_step):\r\n                if time_step > 0:\r\n                    tf.get_variable_scope().reuse_variables()\r\n                batch_step = self.skeleInputLayer(self.input_data[:, time_step, :])\r\n                cells_output, state = cell(batch_step, state)\r\n                outputs.append(cells_output)\r\n        last_output = outputs[-1]\r\n\r\n        self.logits = self.softmax_output(last_output)\r\n        self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.target)\r\n\r\n        self.distribution = tf.nn.softmax(logits=self.logits)\r\n        self.argmax_target = tf.argmax(self.distribution, axis=1)\r\n\r\n        self.loss = tf.reduce_sum(self.cross_entropy, name='loss')/batch_size\r\n        self.final_state = state\r\n\r\n        if not is_train:\r\n            return\r\n\r\n        trainable_variables = tf.trainable_variables()\r\n        raw_grad = tf.gradients(self.loss, trainable_variables)\r\n        grads, _ = tf.clip_by_global_norm(raw_grad, 10)\r\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\r\n        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))\r\n```", "figure it out! according to the [link] method, create the saver after the define of LSTM models, i solved the problem\r\n\r\n\r\n[link]: https://stackoverflow.com/questions/40442098/saving-and-restoring-a-trained-lstm-in-tensor-flow"]}, {"number": 13437, "title": "Sin family identities for y=x yield bad gradients", "body": "I'm writing a custom continuous piecewise function. At some point the function becomes an identity of f(x) = x, but while the loss decreases, the accuracy does not improve. Simply swapping in an \"x\" in the below code does cause everything to work smoothly.\r\n\r\nOriginally suspected tf.where as that has NaN gradient troubles, so I rewrote an equivalent function using boolean_mask. Still the same issue. I also attempted to trim values to prevent NaN propagation. A simplified version of the code is below (the troublesome statement in question being tf.cos(i*tf.acos(x)), which equals x):\r\n\r\n    i = 1\r\n\r\n    location_value = tf.stack([\r\n        tf.less_equal(tf.abs(x), 1), # between_neg_1_and_1\r\n        tf.greater(x, 1), # greater_than_1\r\n        tf.less(x, -1), # less_than_neg_1\r\n        tf.is_nan(x)] \r\n    , -1)\r\n\r\n    res = tf.stack([\r\n        tf.cos(i*tf.acos(tf.minimum(tf.maximum(x, -1), 1))),\r\n        x,\r\n        x,\r\n        0*x]\r\n    , -1)\r\n\r\n    out_shape = x.get_shape().as_list()\r\n    out_shape[0] = batch_size\r\n\r\n    res = tf.reshape(tf.boolean_mask(res, location_value), out_shape)\r\n\r\n\r\n\r\n------------------------\r\n\r\n== cat /etc/issue ===============================================\r\nLinux tyler-desktop 4.2.0-42-generic #49~14.04.1-Ubuntu SMP Wed Jun 29 20:22:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"14.04.5 LTS, Trusty Tahr\"\r\nVERSION_ID=\"14.04\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 4.8.5-2ubuntu1~14.04.1) 4.8.5\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux tyler-desktop 4.2.0-42-generic #49~14.04.1-Ubuntu SMP Wed Jun 29 20:22:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.0)\r\nprotobuf (3.1.0.post1)\r\ntensorflow-gpu (0.12.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 0.12.1\r\ntf.GIT_VERSION = v0.12.0-10-g4d924e7-dirty\r\ntf.COMPILER_VERSION = v0.12.0-10-g4d924e7-dirty\r\nSanity check: array([1], dtype=int32)\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /root/torch/install/lib:/root/torch/install/lib:/usr/local/cuda-8.0/lib64:/usr/lib:/usr/openwin/lib:/usr/dt/lib:/X11.6/lib:/X11.5/lib:/uva/lib:/gnu/lib:/usr/local/cuda/lib64:/usr/local/cuda:/usr/bin/g++\r\nDYLD_LIBRARY_PATH /root/torch/install/lib:/root/torch/install/lib:\r\n\r\n== nvidia-smi ===================================================\r\nSun Oct  1 18:05:01 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    Off  | 0000:01:00.0      On |                  N/A |\r\n|  0%   37C    P0    40W / 230W |    728MiB /  8110MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1137    G   /usr/bin/X                                     370MiB |\r\n|    0      1761    G   compiz                                         243MiB |\r\n|    0      2645    G   /usr/lib/firefox/firefox                         1MiB |\r\n|    0      3024    G   ...ble-features=DocumentWriteEvaluator<Disal   111MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n", "comments": ["You might try writing a custom gradient that does equivalent logic to compute the gradient without relying on automatic differentiation. I'd guess the discontinuities in representation are causing problems. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "This is no longer a pressing issue. Thanks for the help!\n\nOn Dec 19, 2017 8:15 PM, \"Alfred\" <notifications@github.com> wrote:\n\n> It has been 14 days with no activity and the awaiting response label was\n> assigned. Is this still an issue? Please update the label and/or status\n> accordingly.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13437#issuecomment-352935144>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AF4UHxYaRm657fdNv_Tk8x7tCPPYSR3Lks5tCF9vgaJpZM4PqEVf>\n> .\n>\n", "Closing this as this is not an issue anymore. [Here](https://colab.research.google.com/gist/jvishnuvardhan/69024812f786beac95b4a9431bc895a0/35011.ipynb) is a gist for a reference. If the issue persists with newer TF versions, please open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/13437\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/13437\">No</a>\n"]}, {"number": 13436, "title": "\"Variable rnn/basic_rnn_cell/kernel already exists, disallowed.\" error while defining dynamic_rnn", "body": "I was writing a simple code to define an RNN and the code goes thus:\r\n\r\n```\r\nn_steps = 28\r\nn_inputs = 28\r\nn_neurons = 150\r\nn_outputs = 10\r\nn_epochs = 100\r\nbatch_sz = 150\r\nl_rate = 0.001\r\n\r\nX0 = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\r\nY0 = tf.placeholder(tf.int32, [None])\r\ninit_state = tf.zeros([n_steps, n_inputs])\r\n\r\nbasic_r_cell = rnn.BasicRNNCell(num_units = n_neurons)\r\nouputs, states = tf.nn.dynamic_rnn(basic_r_cell, X0, initial_state = init_state)\r\n\r\nlogits = layers.fully_connected(states, n_outputs, activation_fn = None)\r\n```\r\n\r\nExecuting the above code gave the below error with traceback:\r\n\r\n```\r\n> ---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-67-05674d7f7864> in <module>()\r\n     16 \r\n     17 basic_r_cell = rnn.BasicRNNCell(num_units = n_neurons)\r\n---> 18 ouputs, states = tf.nn.dynamic_rnn(basic_r_cell, X0, initial_state = init_state)\r\n     19 \r\n     20 logits = layers.fully_connected(states, n_outputs, activation_fn = None)\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\r\n    572         swap_memory=swap_memory,\r\n    573         sequence_length=sequence_length,\r\n--> 574         dtype=dtype)\r\n    575 \r\n    576     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\r\n    735       loop_vars=(time, output_ta, state),\r\n    736       parallel_iterations=parallel_iterations,\r\n--> 737       swap_memory=swap_memory)\r\n    738 \r\n    739   # Unpack final output if not using output tuples.\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\r\n   2768     context = WhileContext(parallel_iterations, back_prop, swap_memory, name)\r\n   2769     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, context)\r\n-> 2770     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n   2771     return result\r\n   2772 \r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\r\n   2597       self.Enter()\r\n   2598       original_body_result, exit_vars = self._BuildLoop(\r\n-> 2599           pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2600     finally:\r\n   2601       self.Exit()\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2547         structure=original_loop_vars,\r\n   2548         flat_sequence=vars_for_body_with_tensor_arrays)\r\n-> 2549     body_result = body(*packed_vars_for_body)\r\n   2550     if not nest.is_sequence(body_result):\r\n   2551       body_result = [body_result]\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in _time_step(time, output_ta_t, state)\r\n    720           skip_conditionals=True)\r\n    721     else:\r\n--> 722       (output, new_state) = call_cell()\r\n    723 \r\n    724     # Pack state if using state tuples\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in <lambda>()\r\n    706 \r\n    707     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\r\n--> 708     call_cell = lambda: cell(input_t, state)\r\n    709 \r\n    710     if sequence_length is not None:\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in __call__(self, inputs, state, scope)\r\n    178       with vs.variable_scope(vs.get_variable_scope(),\r\n    179                              custom_getter=self._rnn_get_variable):\r\n--> 180         return super(RNNCell, self).__call__(inputs, state)\r\n    181 \r\n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\r\n    439         # Check input assumptions set after layer building, e.g. input shape.\r\n    440         self._assert_input_compatibility(inputs)\r\n--> 441         outputs = self.call(inputs, *args, **kwargs)\r\n    442 \r\n    443         # Apply activity regularization.\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in call(self, inputs, state)\r\n    256   def call(self, inputs, state):\r\n    257     \"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\r\n--> 258     output = self._activation(_linear([inputs, state], self._num_units, True))\r\n    259     return output, output\r\n    260 \r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _linear(args, output_size, bias, bias_initializer, kernel_initializer)\r\n   1015         _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\r\n   1016         dtype=dtype,\r\n-> 1017         initializer=kernel_initializer)\r\n   1018     if len(args) == 1:\r\n   1019       res = math_ops.matmul(args[0], weights)\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n   1063       collections=collections, caching_device=caching_device,\r\n   1064       partitioner=partitioner, validate_shape=validate_shape,\r\n-> 1065       use_resource=use_resource, custom_getter=custom_getter)\r\n   1066 get_variable_or_local_docstring = (\r\n   1067     \"\"\"%s\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n    960           collections=collections, caching_device=caching_device,\r\n    961           partitioner=partitioner, validate_shape=validate_shape,\r\n--> 962           use_resource=use_resource, custom_getter=custom_getter)\r\n    963 \r\n    964   def _get_partitioned_variable(self,\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n    358           reuse=reuse, trainable=trainable, collections=collections,\r\n    359           caching_device=caching_device, partitioner=partitioner,\r\n--> 360           validate_shape=validate_shape, use_resource=use_resource)\r\n    361     else:\r\n    362       return _true_getter(\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _rnn_get_variable(self, getter, *args, **kwargs)\r\n    181 \r\n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n--> 183     variable = getter(*args, **kwargs)\r\n    184     trainable = (variable in tf_variables.trainable_variables() or\r\n    185                  (isinstance(variable, tf_variables.PartitionedVariable) and\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\r\n    350           trainable=trainable, collections=collections,\r\n    351           caching_device=caching_device, validate_shape=validate_shape,\r\n--> 352           use_resource=use_resource)\r\n    353 \r\n    354     if custom_getter is not None:\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\r\n    662                          \" Did you mean to set reuse=True in VarScope? \"\r\n    663                          \"Originally defined at:\\n\\n%s\" % (\r\n--> 664                              name, \"\".join(traceback.format_list(tb))))\r\n    665       found_var = self._vars[name]\r\n    666       if not shape.is_compatible_with(found_var.get_shape()):\r\n\r\nValueError: Variable rnn/basic_rnn_cell/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\r\n\r\n  File \"c:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n  File \"c:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"c:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n```", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 226 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "Use tf.reset_default_graph() on the header file section and re-run the code. Hope it will help.\r\nEvery time clear all variables and run . if u are using spyder. "]}, {"number": 13435, "title": "Branch 170594836", "body": "", "comments": ["Jenkins, test this please.", "@drpngx the CLA bot hasn't commented on the PR for some reason. Is there a way to force trigger it?", "Usually it comes after a while. Not sure why it got stock. Maybe it's a large CL.", "Is that a flaky test (`monitored_session_test`)?\r\n\r\n```\r\nINFO: From Testing //tensorflow/python:monitored_session_test:\r\n==================== Test output for //tensorflow/python:monitored_session_test:\r\n2017-10-01 22:47:36.584234: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n...................WARNING:tensorflow:From /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/monitored_session_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session_test.py:1207: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.get_or_create_global_step\r\n..........F................................................\r\n======================================================================\r\nFAIL: test_recover_and_retry_on_aborted_error (__main__.MonitoredSessionTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/monitored_session_test.runfiles/org_tensorflow/tensorflow/python/training/monitored_session_test.py\", line 1198, in test_recover_and_retry_on_aborted_error\r\n    self.assertEqual(3, session.run(do_step))\r\nAssertionError: 3 != 2\r\n```", "I think so as it didn't happen in the previous run of the tests.", "Jenkins, test this please.", "@sb2nov @drpngx Is this PR ready to merge?", "We're investigating why the CLA doesn't trigger.", "The CLA was manually approved on the PR. All changes are coming from internal code so have been committed by Googlers so it should be safe to merge this.", "@willnorris looks like we ran into a bug with the CLA bot.\r\nWe will merge the change now, but any idea why CLA bot did not respond on this change?\r\nI checked and bot is working fine on all other PRs. Maybe too many commits in this PR?", "b/67294573 mentions that the PR was timing out due to large number of commits."]}, {"number": 13434, "title": "Tensorflow does NOT utilize the memory from two GPUs in Windows 10", "body": "Tensorflow Version 1.3.0\r\nOS: Windows 10\r\nGPUs: Nvidia Quadro M4000 * 2 with 8G GPU memory for each\r\nGPU modes: one for WDDM, one for TCC\r\n\r\nI tested the official codes at https://github.com/tensorflow/models/blob/master/official/resnet/imagenet_main.py\r\n\r\nI just add the GPU constraints in the main function as:\r\n\r\n\tdef main(unused_argv):\r\n\t  os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\r\n\r\n          # For this line, visible_divice_list set to only \"0\" and \"0, 1\" can only support the same batch_size\r\n\t  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0, 1')) \r\n\r\n\t  resnet_classifier = tf.estimator.Estimator(\r\n\t\t  model_fn=imagenet_model_fn, model_dir=FLAGS.model_dir,\r\n\t\t  config=tf.contrib.learn.RunConfig(session_config=config))\r\n\r\n\t  for cycle in range(FLAGS.train_steps // FLAGS.steps_per_eval):\r\n\t\ttensors_to_log = {\r\n\t\t\t'learning_rate': 'learning_rate',\r\n\t\t\t'cross_entropy': 'cross_entropy',\r\n\t\t\t'train_accuracy': 'train_accuracy'\r\n\t\t}\r\n\r\n\t\tlogging_hook = tf.train.LoggingTensorHook(\r\n\t\t\ttensors=tensors_to_log, every_n_iter=100)\r\n\r\n\t\tprint('Starting a training cycle.')\r\n\t\tresnet_classifier.train(\r\n\t\t\tinput_fn=lambda: input_fn(tf.estimator.ModeKeys.TRAIN),\r\n\t\t\tsteps=FLAGS.first_cycle_steps or FLAGS.steps_per_eval,\r\n\t\t\thooks=[logging_hook])\r\n\t\tFLAGS.first_cycle_steps = None\r\n\r\n\t\tprint('Starting to evaluate.')\r\n\t\teval_results = resnet_classifier.evaluate(\r\n\t\t  input_fn=lambda: input_fn(tf.estimator.ModeKeys.EVAL))\r\n\t\tprint(eval_results)\r\n\r\nIn the training process, if I set the visible device list to \"0, 1\" or \"0\" only, both can run successfully with batch_size=48, but BOTH failed with batch_size=49! This indicates that the second GPU's memory is not utilized, as batch size could not be bigger when using two GPUs. I have use Nvidia-smi to confirm that only one or two GPUs are used in the above experiments.\r\n\r\nMy questions are:\r\n1. Is there any way that I can use bigger batch_size when using two GPUs?\r\n2. If the answer for Q1 is No in Windows, is there any way to do it in Linux? I am not familiar with Linux. In Linux, can I set all GPUs to TCC mode? Will the batch size be bigger when two GPUs are both in TCC mode? \r\n\r\nThank you.", "comments": ["Very important question! Because batch size is everything in deep learning! But nobody cares... Why?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nThat said, I believe there is a misunderstanding here. TensorFlow does not automatically split a batch onto all available GPUs.  If you want to use multiple GPUs, at this time you must have your `model_fn` specify how to split the computation across multiple GPUs. See https://www.tensorflow.org/tutorials/using_gpu#using_multiple_gpus\r\n\r\nHope", "@asimshankar Thank you for providing the link; however, I am still not clear about how to modify my model_fn to use larger batch size by fully utilizing two GPU memories. Is there any existing examples of codes? I mean the real model_fn examples rather than just assigning a few variables example. Thank you so much for your help.", "@ybsave : You might find some interesting information here: https://www.tensorflow.org/performance/performance_models#variable_distribution_and_gradient_aggregation \r\n\r\nWe're making the process smoother, but at this point stackoverflow and/or the link above\r\n\r\nHope that helps", "@asimshankar Thank you very much. These materials seem really helpful.", "I have tried to distributed the data batch on two GPUs and there is NaN loss error now. Would there be any possible cause for this? It runs well before (using one GPU only). But now even I set the _DEVICE_LIST to one GPU only, it still produce the NaN loss error. \r\n\r\nMy modified codes are:\r\n\r\n\tdef imagenet_model_fn(features, labels, mode):\r\n\t  tf.summary.image('images', features, max_outputs=6)\r\n\r\n\t  with tf.device('/cpu:0'):\r\n\t\tsplit_batch = tf.split(features, len(_DEVICE_LIST))\r\n\t\tsplit_labels = tf.split(labels, len(_DEVICE_LIST))\r\n\r\n\t\tall_predictions = {\r\n\t\t  'classes': [],\r\n\t\t  'probabilities': []\r\n\t\t}\r\n\t\tall_cross_entropy = []\r\n\t\tall_reg_loss = []\r\n\r\n\t\twith tf.variable_scope(tf.get_variable_scope()):\r\n\t\t  for dev_idx, (device, device_features, device_labels) in enumerate(zip(\r\n\t\t\t_DEVICE_LIST, split_batch, split_labels)):\r\n\t\t\twith tf.device(device):\r\n\t\t\t  with tf.name_scope('device_%d' % dev_idx):\r\n\t\t\t\tlogits = network(inputs=device_features,\r\n\t\t\t\t\t\t\t\t is_training=(mode == tf.estimator.ModeKeys.TRAIN))\r\n\r\n\t\t\t\ttf.get_variable_scope().reuse_variables()\r\n\t\t\t\tall_predictions['classes'].append(tf.argmax(logits, axis=1))\r\n\t\t\t\tall_predictions['probabilities'].append(tf.nn.softmax(logits))\r\n\t\t\t\t\r\n\t\t\t\tif mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\t\t\t# Calculate loss, which includes softmax cross entropy and L2 regularization.\r\n\t\t\t\t  cross_entropy = tf.losses.softmax_cross_entropy(\r\n\t\t\t\t\tlogits=logits, onehot_labels=device_labels)\r\n\t\t\t\t  reg_loss = FLAGS.weight_decay * tf.add_n(\r\n\t\t\t\t\t[tf.nn.l2_loss(v) for v in tf.trainable_variables()])\r\n\t\t\t  \r\n\t\t\t\t  all_cross_entropy.append(cross_entropy)\r\n\t\t\t\t  all_reg_loss.append(reg_loss)\r\n\r\n\t  \r\n\t\tall_predictions['classes'] = tf.reshape(all_predictions['classes'], [-1])\r\n\t\tall_predictions['probabilities'] = tf.reshape(\r\n\t\t  all_predictions['probabilities'], [-1])\r\n\r\n\t\ttotal_cross_entropy = tf.add_n(all_cross_entropy)\r\n\t\ttotal_reg_loss = tf.add_n(all_reg_loss)\r\n\t\ttotal_loss = total_cross_entropy + total_reg_loss\r\n\r\n\t\ttf.identity(total_cross_entropy, name='cross_entropy')\r\n\t\ttf.summary.scalar('cross_entropy', total_cross_entropy)\r\n\t\ttf.summary.scalar('reg_loss', total_reg_loss)\r\n\t\ttf.summary.scalar('total_loss', total_loss)\r\n\r\n\t\tif mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\t  global_step = tf.train.get_or_create_global_step()\r\n\t\t  boundaries = [\r\n\t\t\tint(batches_per_epoch * epoch) for epoch in [30, 60, 120, 150]]\r\n\t\t  values = [\r\n\t\t\t_INITIAL_LEARNING_RATE * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]\r\n\t\t  learning_rate = tf.train.piecewise_constant(\r\n\t\t\ttf.cast(global_step, tf.int32), boundaries, values)\r\n\r\n\t\t  tf.identity(learning_rate, name='learning_rate')\r\n\t\t  tf.summary.scalar('learning_rate', learning_rate)\r\n\r\n\t\t  optimizer = tf.train.MomentumOptimizer(\r\n\t\t\tlearning_rate=learning_rate,\r\n\t\t\tmomentum=_MOMENTUM)\r\n\r\n\t\t# Batch norm requires update_ops to be added as a train_op dependency.\r\n\t\t  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n\t\t  with tf.control_dependencies(update_ops):\r\n\t\t\ttrain_op = optimizer.minimize(total_loss, global_step)\r\n\t\telse:\r\n\t\t  train_op = None\r\n\r\n\t\treturn tf.estimator.EstimatorSpec(\r\n\t\t  mode=mode,\r\n\t\t  predictions=all_predictions,\r\n\t\t  loss=total_loss,\r\n\t\t  train_op=train_op)\r\n\r\nThe error message is:\r\n\r\n\tINFO:tensorflow:Saving checkpoints for 1 into F:\\projects\\DeepLearning\\TensorFlow\\Models\\ImageNet\\resnet_101_imagenet_augmented\\temp\\model.ckpt.\r\n\tINFO:tensorflow:learning_rate = 0.003125, cross_entropy = 14.394\r\n\tINFO:tensorflow:loss = 30.0782, step = 1\r\n\tERROR:tensorflow:Model diverged with loss = NaN.\r\n\tTraceback (most recent call last):\r\n\t  File \"imagenet_main.py\", line 321, in <module>\r\n\t\ttf.app.run()\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n\t\t_sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n\t  File \"imagenet_main.py\", line 310, in main\r\n\t\thooks=[logging_hook])\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 241, in train\r\n\t\tloss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 686, in _train_model\r\n\t\t_, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 518, in run\r\n\t\trun_metadata=run_metadata)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 862, in run\r\n\t\trun_metadata=run_metadata)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 818, in run\r\n\t\treturn self._sess.run(*args, **kwargs)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 980, in run\r\n\t\trun_metadata=run_metadata))\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\basic_session_run_hooks.py\", line 551, in after_run\r\n\t\traise NanLossDuringTrainingError\r\n\ttensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.", "Could you be a bit more specific on what the problem is?\r\nIn particular, please fill out all the details asked for in the new issue template, which include lots of metadata about your setup and instructions to reproduce the problem (for example, a single command line that one could run to reproduce the problem).\r\n\r\nFrom what is described, it is hard to determine what the problem may be. For example, what error are you getting? How big are the input tensors?  Are there any other live tensors/variables etc. that are holding on to memory in GPU (hard to tell since I can't see the full code that's running this)? Is it possible that your `features` and `labels` tensors are placed on CPU and you're running out of host memory?\r\n\r\nFor example, to demonstrate that you can do more with two GPUs than one, consider the following:\r\n\r\n```python\r\nGPU_MEMORY_BYTES = 8 * 2**30 # Assuming your GPU has 8GB of memory, adjust accordingly\r\n\r\n# Number of float32 elements (4 bytes) that consume 80% of GPU memory\r\nNUM_ELEMS = (8 * GPU_MEMORY_BYTES / 10) / 4\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n  t0 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU0's memory\r\n  s0 = tf.reduce_sum(t0)\r\nwith tf.device(\"/gpu:1\"):\r\n  t1 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU1's memory\r\n  s1 = tf.reduce_sum(t1)\r\ns = tf.add(s0, s1)\r\n\r\nwith tf.Session() as sess:\r\n  print(sess.run(s))\r\n```\r\nComputing `s` requires more than one GPU worth of memory, but with the computation and input split between two GPUs, this will succeed. As opposed to trying to do the following on a single GPU:\r\n\r\n```\r\nwith tf.device(\"/gpu:0\"):\r\n  t = tf.ones([2 * NUM_ELEMS])\r\n  s = tf.reduced_sum(t)\r\nwith tf.Session() as sess:\r\n  print(sess.run(s)) # This should fail since it consumes more memory than exists in the GPU\r\n```\r\n ", "@asimshankar Thanks for your explanation. The codes I used are exactly the same as in https://github.com/tensorflow/models/blob/master/official/resnet/imagenet_main.py.\r\n\r\nI only modified the \"resnet_model_fn\" for only a few lines (I labeled below).\r\n\r\n\tdef resnet_model_fn(features, labels, mode):\r\n\t  \"\"\" Our model_fn for ResNet to be used with our Estimator.\"\"\"\r\n\t  tf.summary.image('images', features, max_outputs=6)\r\n\r\n\t  with tf.device('/cpu:0'):    #modified by me\r\n\t\tall_cross_entropy = []   #modified by me\r\n\t\twith tf.device('/gpu:0'):  #modified by me\r\n\t\t  logits = network(\r\n\t\t\tinputs=features, is_training=(mode == tf.estimator.ModeKeys.TRAIN))\r\n\t\t  \r\n\t\t  predictions = {\r\n\t\t\t'classes': tf.argmax(logits, axis=1),\r\n\t\t\t'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\r\n\t\t  }\r\n\r\n\t\t  if mode == tf.estimator.ModeKeys.PREDICT:\r\n\t\t\treturn tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\r\n\r\n\t\t  # Calculate loss, which includes softmax cross entropy and L2 regularization.\r\n\t\t  cross_entropy = tf.losses.softmax_cross_entropy(\r\n\t\t\tlogits=logits, onehot_labels=labels)\r\n\r\n\t\t  all_cross_entropy.append(cross_entropy)   #modified by me\r\n\r\n\t\ttotal_entropy = tf.add_n(all_cross_entropy)  #modified by me\r\n\t\t# Create a tensor named cross_entropy for logging purposes.\r\n\t\ttf.identity(total_entropy, name='cross_entropy')  #modified by me\r\n\t\ttf.summary.scalar('cross_entropy', total_entropy)  #modified by me\r\n\r\n\t\t# Add weight decay to the loss. We perform weight decay on all trainable\r\n\t\t# variables, which includes batch norm beta and gamma variables.\r\n\t\treg_loss = FLAGS.weight_decay * tf.add_n(\r\n\t\t  [tf.nn.l2_loss(v) for v in tf.trainable_variables()])\r\n\t\ttf.summary.scalar('reg_loss', reg_loss)\r\n\r\n\t\tloss = total_entropy + reg_loss  #modified by me\r\n\t\ttf.summary.scalar('total_loss', loss)\r\n\r\n\t\tif mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\t  global_step = tf.train.get_or_create_global_step()\r\n\r\n\t\t  # Multiply the learning rate by 0.1 at 30, 60, 120, and 150 epochs.\r\n\t\t  boundaries = [\r\n\t\t\tint(batches_per_epoch * epoch) for epoch in [30, 60, 120, 150]]\r\n\t\t  values = [\r\n\t\t\t_INITIAL_LEARNING_RATE * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]\r\n\t\t  learning_rate = tf.train.piecewise_constant(\r\n\t\t\ttf.cast(global_step, tf.int32), boundaries, values)\r\n\r\n\t\t  # Create a tensor named learning_rate for logging purposes.\r\n\t\t  tf.identity(learning_rate, name='learning_rate')\r\n\t\t  tf.summary.scalar('learning_rate', learning_rate)\r\n\r\n\t\t  optimizer = tf.train.MomentumOptimizer(\r\n\t\t\tlearning_rate=learning_rate,\r\n\t\t\tmomentum=_MOMENTUM)\r\n\t\t  \r\n\t\t  # Batch norm requires update_ops to be added as a train_op dependency.\r\n\t\t  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n\t\t  with tf.control_dependencies(update_ops):\r\n\t\t\ttrain_op = optimizer.minimize(loss, global_step)\r\n\t\telse:\r\n\t\t  train_op = None\r\n\r\n\t\taccuracy = tf.metrics.accuracy(\r\n\t\t  tf.argmax(labels, axis=1), predictions['classes'])\r\n\t\tmetrics = {'accuracy': accuracy}\r\n\r\n\t\t# Create a tensor named train_accuracy for logging purposes.\r\n\t\ttf.identity(accuracy[1], name='train_accuracy')\r\n\t\ttf.summary.scalar('train_accuracy', accuracy[1])\r\n\r\n\t\treturn tf.estimator.EstimatorSpec(\r\n\t\t  mode=mode,\r\n\t\t  predictions=predictions,\r\n\t\t  loss=loss,\r\n\t\t  train_op=train_op,\r\n\t\t  eval_metric_ops=metrics)\r\n\r\nHowever, the error still shows:\r\n\tTraceback (most recent call last):\r\n\t  File \"imagenet_main_multi-gpu.py\", line 396, in <module>\r\n\t\ttf.app.run()\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n\t\t_sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n\t  File \"imagenet_main_multi-gpu.py\", line 385, in main\r\n\t\thooks=[logging_hook])\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 241, in train\r\n\t\tloss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 686, in _train_model\r\n\t\t_, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 518, in run\r\n\t\trun_metadata=run_metadata)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 862, in run\r\n\t\trun_metadata=run_metadata)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 818, in run\r\n\t\treturn self._sess.run(*args, **kwargs)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 980, in run\r\n\t\trun_metadata=run_metadata))\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\basic_session_run_hooks.py\", line 551, in after_run\r\n\t\traise NanLossDuringTrainingError\r\n\ttensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.\r\n\r\nIf I remove my added cpu, gpu appointment, the codes have no problem to run. Is it possibly because some communications problem between CPU & GPU? What should I do for correct one?\r\n\r\nI tried hardly to search for examples (multi-GPU using Estimator); however, I can only find how to distribute different layers of network at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/multiple_gpu.py. But I cannot find any example of splitting data into multi-GPU using Estimator. Would you please give me some help? Thank you so much.", "@ybsave : I'm confused, I thought the issue was filed about GPU memory utilization, while the error you describe now seems orthogonal?\r\n\r\nIf that is indeed the case, please file a separate issue/stackoverflow question about that so we can keep discussion on this thread focused on the question raised at the beginning (and mentioned in the summary description)", "@asimshankar Yes, this seems a separate issue. I will submit another new issue. The reason for that is when I utilized two GPUs according to your suggestion, it does work and can support bigger batch size, but a NaN error occurs. After tracing back, I find the minimum modification of codes (above) already leads to the same error. I will update this multi-GPU issue later after the new problem solved. Thank you.", "I finally made a successful running codes which now use double batch size on both GPUs now. I have double checked that both GPUs are in use now. Thanks for all the help. The revised codes are:\r\n\r\n\tdef average_gradients(tower_grads):\r\n\t  \"\"\"Calculate the average gradient for each shared variable across all towers.\r\n\t  Note that this function provides a synchronization point across all towers.\r\n\t  Args:\r\n\t\ttower_grads: List of lists of (gradient, variable) tuples. The outer list\r\n\t\t  is over individual gradients. The inner list is over the gradient\r\n\t\t  calculation for each tower.\r\n\t  Returns:\r\n\t\t List of pairs of (gradient, variable) where the gradient has been averaged\r\n\t\t across all towers.\r\n\t  \"\"\"\r\n\t  average_grads = []\r\n\t  for grad_and_vars in zip(*tower_grads):\r\n\t\t# Note that each grad_and_vars looks like the following:\r\n\t\t#   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\r\n\t\tgrads = []\r\n\t\tfor g, _ in grad_and_vars:\r\n\t\t  # Add 0 dimension to the gradients to represent the tower.\r\n\t\t  expanded_g = tf.expand_dims(g, 0)\r\n\r\n\t\t  # Append on a 'tower' dimension which we will average over below.\r\n\t\t  grads.append(expanded_g)\r\n\r\n\t\t# Average over the 'tower' dimension.\r\n\t\tgrad = tf.concat(axis=0, values=grads)\r\n\t\tgrad = tf.reduce_mean(grad, 0)\r\n\r\n\t\t# Keep in mind that the Variables are redundant because they are shared\r\n\t\t# across towers. So .. we will just return the first tower's pointer to\r\n\t\t# the Variable.\r\n\t\tv = grad_and_vars[0][1]\r\n\t\tgrad_and_var = (grad, v)\r\n\t\taverage_grads.append(grad_and_var)\r\n\t  return average_grads\r\n\r\n\r\n\tdef imagenet_model_fn(features, labels, mode):\r\n\t  \"\"\" Our model_fn for ResNet to be used with our Estimator.\"\"\"\r\n\t  tf.summary.image('images', features, max_outputs=6)\r\n\r\n\t  with tf.device('/cpu:0'):\r\n\t\tsplit_batch = tf.split(features, len(_DEVICE_LIST))\r\n\t\tsplit_labels = tf.split(labels, len(_DEVICE_LIST))\r\n\t\t\r\n\t\tif mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\t  global_step = tf.train.get_or_create_global_step()\r\n\r\n\t\t  # Multiply the learning rate by 0.1 at 30, 60, 120, and 150 epochs.\r\n\t\t  boundaries = [\r\n\t\t\tint(batches_per_epoch * epoch) for epoch in [30, 60, 120, 150]]\r\n\t\t  values = [\r\n\t\t\t_INITIAL_LEARNING_RATE * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]\r\n\t\t  learning_rate = tf.train.piecewise_constant(\r\n\t\t\ttf.cast(global_step, tf.int32), boundaries, values)\r\n\r\n\t\t  # Create a tensor named learning_rate for logging purposes.\r\n\t\t  tf.identity(learning_rate, name='learning_rate')\r\n\t\t  tf.summary.scalar('learning_rate', learning_rate)\r\n\r\n\t\t  optimizer = tf.train.MomentumOptimizer(\r\n\t\t\tlearning_rate=learning_rate,\r\n\t\t\tmomentum=_MOMENTUM)\r\n\r\n\t\ttower_grads = []\r\n\t\ttower_cross_entropy = []\r\n\t\ttower_reg_loss = []\r\n\t\ttower_accuracy = []\r\n\r\n\t\twith tf.variable_scope(tf.get_variable_scope()):\r\n\t\t  for dev_idx, (device, device_features, device_labels) in enumerate(zip(\r\n\t\t\t_DEVICE_LIST, split_batch, split_labels)):\r\n\t\t\twith tf.device(device):\r\n\t\t\t  with tf.name_scope('device_%d' % dev_idx):\r\n\t\t\t\tlogits = network(inputs=device_features,\r\n\t\t\t\t\t\t\t\t is_training=(mode == tf.estimator.ModeKeys.TRAIN))\r\n\r\n\t\t\t\ttf.get_variable_scope().reuse_variables()\r\n\t\t  \r\n\t\t\t\tpredictions = {\r\n\t\t\t\t  'classes': tf.argmax(logits, axis=1),\r\n\t\t\t\t  'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\r\n\t\t\t\t}\r\n\r\n\t\t\t\tcross_entropy = tf.losses.softmax_cross_entropy(\r\n\t\t\t\t  logits=logits, onehot_labels=device_labels)\r\n\t\t\t\ttower_cross_entropy.append(cross_entropy)\r\n\r\n\t\t\t\treg_loss = 0.5 * FLAGS.weight_decay * tf.add_n(\r\n\t\t\t\t  [tf.nn.l2_loss(v) for v in tf.trainable_variables()])\r\n\t\t\t\ttower_reg_loss.append(reg_loss)            \r\n\r\n\t\t\t\tloss = cross_entropy + reg_loss\r\n\r\n\t\t\t\tif mode == tf.estimator.ModeKeys.TRAIN:          \r\n\t\t\t\t  grads = optimizer.compute_gradients(loss)\r\n\t\t\t\t  tower_grads.append(grads)\r\n\r\n\t\t\t\taccuracy = tf.metrics.accuracy(\r\n\t\t\t\t  tf.argmax(device_labels, axis=1), predictions['classes'])\r\n\t\t\t\tmetrics = {'accuracy': accuracy}\r\n\t\t\t\ttower_accuracy.append(accuracy[1])\r\n\r\n\t\tcross_entropy = tf.add_n(tower_cross_entropy)\r\n\t\ttf.identity(cross_entropy, name='cross_entropy')\r\n\t\ttf.summary.scalar('cross_entropy', cross_entropy)\r\n\r\n\t\treg_loss = tf.add_n(tower_reg_loss)\r\n\t\ttf.summary.scalar('reg_loss', reg_loss)\r\n\r\n\t\tloss = cross_entropy + reg_loss\r\n\t\ttf.summary.scalar('total_loss', loss)\r\n\t\t  \r\n\t\tif mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\t  accuracy = tf.reduce_mean(tower_accuracy)\r\n\t\t  tf.identity(accuracy, name='train_accuracy')\r\n\t\t  tf.summary.scalar('train_accuracy', accuracy)\r\n\t\t  \r\n\t\t  # Batch norm requires update_ops to be added as a train_op dependency.\r\n\t\t  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n\t\t  with tf.control_dependencies(update_ops):\r\n\t\t\tgrads = average_gradients(tower_grads)\r\n\t\t\ttrain_op = optimizer.apply_gradients(grads, global_step=global_step)\r\n\t\telse:\r\n\t\t  train_op = None\r\n\r\n\t\treturn tf.estimator.EstimatorSpec(\r\n\t\t  mode=mode,\r\n\t\t  predictions=predictions,\r\n\t\t  loss=loss,\r\n\t\t  train_op=train_op,\r\n\t\t  eval_metric_ops=metrics)", "Why does nobody want to include that code to the standard tensorflow package to make it work on multiple GPUs automatically???", "@zyavrik\r\nBecause. Because of customization and it's different for scenarios. How about you tell us a foolproof idea that would be the most efficient. Submit it through a feature request.\r\n\r\nThere is no \"correct way\" of doing things. I guess you can say tensorflow kinda automatically does it for you, just that you have to select which one suits best for your needs. See https://www.tensorflow.org/guide/distributed_training"]}, {"number": 13433, "title": "Bug: tf.Variable uses always twice the memory (on the CPU)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: tested on both\r\n- **TensorFlow version (use command below)**: 1.3 for pip / 0cfb16e025b3d20e8c8aca431fc0887814817c44 for self-compiled\r\n- **Python version**: Python 3.4.3 [GCC 4.9.2] on linux\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **CUDA/cuDNN version**: not used\r\n- **GPU model and memory**: not used\r\n\r\n### Describe the problem\r\nEvery tf.variable occupies always twice the necessary memory: \r\nOnce for the tf.constant vector that is created for the initializer and once as the persistend storage.\r\nCode to reproduce:\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_VLOG_LEVEL'] = '100' #print all\r\nimport tensorflow as tf\r\n\r\nruns = 1\r\nN = int(1024 * 1024 * 1.1)\r\nM = int(1024 / 8)\r\nprint(\"testing allocation of {:.2f} MB\".format(N*M*8. / 1024 / 1024))\r\n\r\n#does not matter which version you use:\r\nv = tf.Variable(tf.ones([M, N], tf.float64), name=\"var1\")\r\n#v = tf.get_variable(shape=(M,N), initializer=tf.ones_initializer, dtype=tf.float64, name='var1', trainable=False)\r\n\r\ninit = tf.global_variables_initializer()\r\nfor i in range(runs):\r\n      print(\"start session\")\r\n      with tf.Session() as sess:\r\n            print(\"start init\")\r\n            sess.run(init)\r\n```\r\n\r\nIn my self-compiled version the output contains the following lines:\r\n```\r\n tensorflow/core/common_runtime/bfc_allocator.cc:133] Extending allocation by 2.00GiB bytes.\r\n tensorflow/core/common_runtime/bfc_allocator.cc:137] Total allocated bytes: 4.00GiB\r\n```\r\nFor the pip version, the current allocation is not displayed, but observing htop during the execution or using mprof reveals the same.\r\n\r\nThe issue seems to occur because assign does not reuse the memory of tf.ones but instead allocates additional memory. Related lines: [assign_op.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/assign_op.h#L79-L86)\r\ncontext->forward_input returns null in this case, because the memory of tf.ones has a ref count of 2.\r\n(I dont know why) see [op_kernel.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op_kernel.cc#L455)\r\ninput->RefCountIsOne() is therefore false.\r\n\r\nI tried to comment out the input->RefCountIsOne() check, but then it still doesn't work because of the \r\noutput_attr.IsEqualOrLessRestrictiveThan() check.\r\nIf you remove this check too, the memory usage finaly drops to the expected value, the memory of tf.ones is reused.  \r\nBut this is not a real solution, because I don't know how this would effect other operations and it seems to break the memory freeing.\r\n\r\nI think this bug is quiet serious, because it affects nearly all computations. \r\nIs this an already known bug?\r\n\r\n", "comments": ["There was an op added to initialize variable to zeros without requiring extra memory\r\nhttps://github.com/tensorflow/tensorflow/pull/4077\r\n\r\nI guess if the initialization tensor doesn't have any other consumers, it makes sense to forward it's memory instead of copying, @alextp might know if that's a possibility", "@rmlarsen any idea why the refcount of ones would be 2 here? It should be one.", "@yaroslavvb Thanks a lot! Thats exactly what I wanted to achieve.\r\nMay I ask why this initializer is hidden at contrib/framework and is not replacing the normal process for initialization? The commit you are referring to is older then one year.\r\n\r\nFor all visitors, this is how you should init all \"big\" variables you plan to fill later step by step:\r\n\r\n```\r\n#does not matter which version you use:\r\nv = tf.Variable(tf.ones([M, N], tf.float64), name=\"var1\")\r\n#v = tf.get_variable(shape=(M,N), initializer=tf.ones_initializer, dtype=tf.float64, name='var1', trainable=False)\r\nvar_zero = tf.contrib.framework.zero_initializer(v)\r\n\r\nfor i in range(runs):\r\n      print(\"start session\")\r\n      with tf.Session() as sess:\r\n            print(\"start init\")\r\n            sess.run(var_zero)\r\n```\r\n", "Not sure. I'll take a look.", "I agree with @georgh:\r\nI believe my deep learning work is pretty down-to-earth but already two times in past several months I came across the urgent need of sending the training (and testing) dataset to the gpu memory.\r\nA simple minded way in Keras is seting up a large fixed embedding layer, but then a \"protobuf error\" appears when the size exceeds 2GB (the limitation seem artificial to me, but I admit, I don't see into the core of tensorflow at all). \r\nThen I tried to work in pure tensorflow in the sense of this post but it turns out that unless I use some hardly documented hack, I can use only half of the gpu memory (I don't understand why other dozens of people are not complaining here:)).", "Actually this situation has been improved and you should be able to\ninitialize a tf.Variable with a non-constant initializer without using\ntwice the memory. Constant initializers are troublesome (constants overall\nare quite broken in tf).\n\nOn Wed, Dec 13, 2017 at 5:10 PM, marek-krcal <notifications@github.com>\nwrote:\n\n> I agree with @georgh <https://github.com/georgh>:\n> I believe my deep learning work is pretty down-to-earth but already two\n> times in past several months I came across the urgent need of sending the\n> training (and testing) dataset to the gpu memory.\n> A simple minded way in Keras is seting up a large fixed embedding layer,\n> but then a \"protobuf error\" appears when the size exceeds 2GB (the\n> limitation seem artificial to me, but I admit, I don't see into the core of\n> tensorflow at all).\n> Then I tried to work in pure tensorflow in the sense of this post but it\n> turns out that unless I use some hardly documented hack, I can use only\n> half of the gpu memory (I don't understand why other dozens of people are\n> not complaining here:)).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13433#issuecomment-351456846>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxS9tVEk9GCSKAkYA31lTLUYjnxcAks5tAAUigaJpZM4Pp-Qh>\n> .\n>\n\n\n\n-- \n - Alex\n", "Thanks for a prompt reply. What exactly is \"non-constant\" initializer?\r\nwhen I tried: \r\n`\r\nw = tf.Variable(tf.random_uniform((2000,1000000),dtype=dtype),dtype=\"float32\")\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n`\r\ngives me ResourceExhaustedError (OOM) on GTX 1080 with 11GB of memory (tensor is 8GB large).\r\n\r\n(Sorry for for perhaps a silly question, I am a complete layman in TF)", "Interesting, this should work. I'll debug.\n\nOn Thu, Dec 14, 2017 at 12:43 PM, marek-krcal <notifications@github.com>\nwrote:\n\n> Thanks for a prompt reply. What exactly is \"non-constant\" initializer?\n> when I tried:\n> w = tf.Variable(tf.random_uniform((2000,1000000),dtype=dtype),\n> trainable=False,dtype='float32') with tf.Session() as sess:\n> sess.run(tf.global_variables_initializer())\n> gives me ResourceExhaustedError (OOM) on GTX 1080 with 11GB of memory\n> (tensor is 8GB large).\n>\n> (Sorry for for perhaps a silly question, I am a complete layman in TF)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13433#issuecomment-351699495>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxYcX9sEPydhHY9ZeS0hBWSNLT5BEks5tARf9gaJpZM4Pp-Qh>\n> .\n>\n\n\n\n-- \n - Alex\n", "BTW, you can use \"mem_util.py\" from this [repo](https://github.com/yaroslavvb/chain_constant_memory/) to simplify debugging memory issues.\r\n\r\nAs an example, run this file https://github.com/yaroslavvb/stuff/blob/master/double_memory_bug.py\r\n\r\nThis prints memory allocation timeline\r\n```\r\nPrinting timeline for /gpu:0\r\n     0          0          0 _SOURCE\r\n   457          0          0 Variable\r\n   499          0          0 random_uniform/shape\r\n   521          0          0 random_uniform/sub\r\n   529          0          0 random_uniform/min\r\n   555  400000000  400000000 random_uniform/RandomUniform\r\n417117  400000000          0 random_uniform/mul\r\n417185  400000000          0 random_uniform\r\n417243  800000000  400000000 Variable/Assign\r\n417289  400000000 -400000000 random_uniform/RandomUniform\r\n417304  400000000          0 init\r\n```\r\n\r\nIt extracts numbers from run_metadata...memory_stats entries, so here you can see Assign allocated memory rather than forward", "@rmlarsen want to take a look and see why the copy elision is not triggering here?", "When using `var_zero = tf.contrib.framework.zero_initializer()` I am able to allocate roughly all the 12GB of the GPU memory. However, once I start filling that variable by data, I get \"ResourceExhaustedError: OOM...\". The error happens as long as my variable is roughly above 8GB:\r\n```\r\ndtype = 'float32'\r\nfeatures = np.ones((4000000,538),dtype)\r\nshape = features.shape\r\nprint(features.nbytes) #prints 8608000000\r\nfeatures_var = tf.Variable(tf.ones(shape,dtype=dtype),trainable=False,dtype=dtype)\r\nefficient_init = tf.contrib.framework.zero_initializer(features_var)\r\nprint('initializing data variable features_var by zeros')\r\nsess.run(efficient_init)\r\nsess.run(tf.assign(features_var[:1000,:],features[:1000,:]))\r\n```\r\n(The same error happens if I try to fill the features_var through an assignment from a placeholder.)\r\nOnce I set the features to have the shape=(3900000,538), the error disappears.\r\n\r\nHaving 8GB of data on GPU is already nice, but I am wondering if there is a way of how to make use of the remaining 4GB?", "A different issue on a very related note:\r\nMy features are good enough when rounded to half-precision (float16) - actually I get even slightly better outcomes probably due to a regularization effect.\r\nI would like to enjoy the half memory footprint of such dataset. However, once my variable slightly exceeds some number around 4GB, I get an error while assigning at that the offset=number:\r\n```\r\ndtype = 'float16'\r\nfeatures = np.ones((4000000,538),dtype)\r\nshape = features.shape\r\nprint(features.nbytes) #prints 4304000000\r\nfeatures_var = tf.Variable(tf.ones(shape,dtype=dtype),trainable=False,dtype=dtype)\r\nefficient_init = tf.contrib.framework.zero_initializer(features_var)\r\nprint('initializing data variable features_var by zeros')\r\nsess.run(efficient_init)\r\nsess.run(tf.assign(features_var[:1000,:],features[:1000,:])) #this is OK\r\nsess.run(tf.assign(features_var[3900000:3901000,:],features[:1000,:])) #up to this point still OK\r\nsess.run(tf.assign(features_var[3990000:3991000,:],features[-1000:,:]))#this throws an error\r\nsess.run(tf.assign(features_var[-1000:,:],features[-1000:,:])) #this throws an error\r\n```\r\nMy Jupyter kernel fails with no error printed and some lines containing `Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS` in my jupyter logfile.\r\n\r\nWith features having the shape=(3900000,538), everything goes fine. I.e., afterall the limit on the number of datapoints turns out to be (almost) the same as if I used single precision (float32).\r\n\r\nIf for any of the above problems the \"memory allocation timeline\" from \"mem_utils\" of @yaroslavvb would help, I could try to produce that as well.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I don't want to turn this thread into a monologue, but I think I do have one more thing worth a short mention: a simple workaround to my 8GB-limit problem is to make two data variables. I did not try to push the limits but I definitely managed to get beyond the 8GB.  (I also did not try whether I could fit more data by using half precision floats)\r\nIt is a bit ackward to have the dataset split among two variables - I need additional tf.cond operation at the beginning of my model - but it's not a disaster either.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 121 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 136 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 151 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 166 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 181 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "If you use resource variables this issue has been fixed. To use those, either do `tf.Variable(..., use_resource=True)`, `tf.get_variable(..., use_resource=True)` or add a line saying `tf.get_variable_scope().set_use_resource(True)` somewhere before you start building graphs."]}, {"number": 13432, "title": "An exception has occurred, use %tb to see the full traceback. and low accuracy", "body": "![tf issue](https://user-images.githubusercontent.com/22562558/31056287-282fb44e-a69d-11e7-800b-af2a8ab41f12.png)\r\n\r\nThe accuracy I get for sklearn and tensorflow is 0.428571. And after that I get a line that says: An exception has occurred, use %tb to see the full traceback.\r\n\r\nWhat should I do? What is the accuracy that I am supposed to get?\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 13431, "title": "Windows nightly build Dataset.from_generator fails with pyfunc error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows-7\r\n- **TensorFlow installed from (source or binary)**:pip\r\n- **TensorFlow version**:1.4.0-dev20170929\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:-\r\n- **CUDA/cuDNN version**:-\r\n- **GPU model and memory**:-\r\n- **Exact command to reproduce**:see below\r\n\r\n### Describe the problem\r\nAs described in the SO question https://stackoverflow.com/q/46511328/281545 the code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nDataset = tf.contrib.data.Dataset\r\nit2 = Dataset.range(5).make_one_shot_iterator()\r\n\r\n# Dataset.from_generator need tensorflow > 1.3 !\r\nwith tf.Session() as sess:\r\n    print(tf.__version__)\r\n    def _dataset_generator():\r\n        while True:\r\n            try:\r\n                yield sess.run(it2.get_next())\r\n            except tf.errors.OutOfRangeError:\r\n                return\r\n    das_dataset = Dataset.from_generator(_dataset_generator, tf.int64)\r\n    das_dataset_it = das_dataset.make_one_shot_iterator()\r\n    while True:\r\n        try:\r\n            print(sess.run(das_dataset_it.get_next()))\r\n        except tf.errors.OutOfRangeError:\r\n            break\r\n```\r\n\r\nfails with:\r\n\r\n```\r\nC:\\Dropbox\\_\\PyCharmVirtual\\TF-Nigthly-2\\Scripts\\python.exe C:/Dropbox/eclipse_workspaces/python/zebra/so_46511328_from_generator.py\r\n1.4.0-dev20170929\r\n2017-10-01 16:41:41.978576: W C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows\\PY\\35\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: 0-th value returned by pyfunc_0 is int32, but expects int64\r\n\t [[Node: PyFunc = PyFunc[Tin=[], Tout=[DT_INT64], token=\"pyfunc_0\"]()]]\r\nTraceback (most recent call last):\r\n  File \"C:\\Dropbox\\_\\PyCharmVirtual\\TF-Nigthly-2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Dropbox\\_\\PyCharmVirtual\\TF-Nigthly-2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\_\\Python35\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Dropbox\\_\\PyCharmVirtual\\TF-Nigthly-2\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 467, in raise_exception_on_not_ok_status\r\n    c_api.TF_GetCode(status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 0-th value returned by pyfunc_0 is int32, but expects int64\r\n\t [[Node: PyFunc = PyFunc[Tin=[], Tout=[DT_INT64], token=\"pyfunc_0\"]()]]\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[<unknown>], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](OneShotIterator_1)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Dropbox/eclipse_workspaces/python/zebra/so_46511328_from_generator.py\", line 19, in <module>\r\n    print(sess.run(das_dataset_it.get_next()))\r\n  File \"C:\\Dropbox\\_\\PyCharmVirtual\\TF-Nigthly-2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Dropbox\\_\\PyCharmVirtual\\TF-Nigthly-2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Dropbox\\_\\PyCharmVirtual\\TF-Nigthly-2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"C:\\Dropbox\\_\\PyCharmVirtual\\TF-Nigthly-2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 0-th value returned by pyfunc_0 is int32, but expects int64\r\n\t [[Node: PyFunc = PyFunc[Tin=[], Tout=[DT_INT64], token=\"pyfunc_0\"]()]]\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[<unknown>], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](OneShotIterator_1)]]\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nThat's a problem in windows nightly - installing the nightly on an Ubuntu machine works:\r\n\r\n```\r\n$ pipenv run python3 so_46511328_from_generator.py\r\n2017-10-01 13:34:21.840423: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n1.4.0-dev20170929\r\n0\r\n1\r\n2\r\n3\r\n4\r\n2017-10-01 13:34:21.903201: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: StopIteration: Iteration finished.\r\n```\r\n\r\n**EDIT:** Maybe related to https://github.com/tensorflow/tensorflow/issues/8196 ?\r\n", "comments": ["Yes, I think is a result of `np.array(x)` returning a different array type (when `x` is a Python `int`) on Windows and Linux. If I remember correctly, on Windows the array will have type `np.int32` and on Linux the array will have type `np.int64`. This behavior in `Dataset.from_generator()` is inherited from `tf.py_func()`, which performs the NumPy conversion automatically (as discussed in #8196). To write platform independent code that handles this case, you should explicitly wrap the return value in a NumPy array.", "On second thoughts... I think this is really easy to fix. I'm working on a patch."]}, {"number": 13430, "title": "Unstable numerics in MvNormal KL", "body": "https://github.com/tensorflow/tensorflow/blame/0cfb16e025b3d20e8c8aca431fc0887814817c44/tensorflow/contrib/distributions/python/ops/mvn_linear_operator.py#L302\r\n\r\nThis line should look like this:\r\n\r\n```python\r\ndef squared_frobenius_norm(x):                                          \r\n    \"\"\"Helper to make KL calculation slightly more readable. And Stable!\r\n    square(sqrt(sqnorm)) = sqnorm                                                 \r\n    \"\"\"                                                                 \r\n    # http://mathworld.wolfram.com/FrobeniusNorm.html                   \r\n    return tf.reduce_sum(x * tf.conj(x), axis=[-2, -1], keep_dims=False)\r\n```\r\n\r\nIt yields NaN in grad KL(q||p) if q=p ", "comments": ["@jvdillon @langmore : Mind taking a look?", "Thanks for alerting me. I will take a look!", "Fixed. Should hit master tomorrow or the day after.  Thanks for your suggested fix! Not sure why linalg.norm doesnt work, that seems like the real bug.", "It doesn't work because gradient is unstable function and graph is not simplified "]}, {"number": 13429, "title": "Name/variable scopes of tensorflow.python.layers.base.Layer", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nThis is a tiny code that creates layers and connects them in series.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers.base import Layer\r\n\r\n\r\nclass A(Layer):\r\n    def build(self, input_shape):\r\n        self.v = self.add_variable('v', (), tf.float32)\r\n        self.built = True\r\n    \r\n    def call(self, inputs):\r\n        return self.v * inputs\r\n    \r\n\r\n# Case 1\r\nwith tf.Graph().as_default() as graph:\r\n    x = tf.placeholder(tf.float32, (), 'x')\r\n   \r\n    out = x\r\n    out = A()(out)\r\n    out = A()(out)\r\n    out = A()(out)\r\n    \r\n    tf.summary.FileWriter('/tmp/tensorboard/1', graph=graph).close()\r\n\r\n# Case 2\r\nwith tf.Graph().as_default() as graph:\r\n    x = tf.placeholder(tf.float32, (), 'x')\r\n   \r\n    out = x\r\n    out = A(name='a')(out)\r\n    out = A(name='a_1')(out)\r\n    out = A(name='a_2')(out)\r\n    \r\n    tf.summary.FileWriter('/tmp/tensorboard/2', graph=graph).close()\r\n\r\n# Case 3\r\nwith tf.Graph().as_default() as graph:\r\n    x = tf.placeholder(tf.float32, (), 'x')\r\n   \r\n    out = x\r\n    out = A(name='a_1')(out)\r\n    out = A(name='a_2')(out)\r\n    out = A(name='a_3')(out)\r\n    \r\n    tf.summary.FileWriter('/tmp/tensorboard/3', graph=graph).close()\r\n```\r\n\r\n#### Results\r\n\r\nOther than case 3, an unexpected graph is generated.\r\nIs this a bug?\r\n\r\n- Case 1\r\n\r\n<img alt=\"result\" height=\"450\" src=\"https://user-images.githubusercontent.com/7009040/31054076-b290a772-a6e5-11e7-8e6a-2d97420b5e0e.png\">\r\n\r\n- Case 2\r\n\r\n<img alt=\"result\" height=\"450\" src=\"https://user-images.githubusercontent.com/7009040/31054078-b2a7ca7e-a6e5-11e7-9a17-805fd56e6ea7.png\">\r\n\r\n- Case 3\r\n\r\n<img alt=\"result\" height=\"450\" src=\"https://user-images.githubusercontent.com/7009040/31054077-b291b7c0-a6e5-11e7-9256-be7c986094ec.png\">", "comments": ["Interesting, I just wonder whether the same problem occurs with builtin layer, say `Dense`? Could you make a similar experiment for `Dense` and `dense`?", "I replaced `A` by `tensorflow.python.layers.core.Dense` and obtained similar results.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers.core import Dense\r\n\r\n\r\ndef A(**kwargs):\r\n    return Dense(units=3, **kwargs)\r\n    \r\n\r\n# Case 1\r\nwith tf.Graph().as_default() as graph:\r\n    x = tf.placeholder(tf.float32, (None, 3), 'x')\r\n   \r\n    out = x\r\n    out = A()(out)\r\n    out = A()(out)\r\n    out = A()(out)\r\n    \r\n    tf.summary.FileWriter('/tmp/tensorboard/1', graph=graph).close()\r\n\r\n# Case 2\r\nwith tf.Graph().as_default() as graph:\r\n    x = tf.placeholder(tf.float32, (None, 3), 'x')\r\n   \r\n    out = x\r\n    out = A(name='a')(out)\r\n    out = A(name='a_1')(out)\r\n    out = A(name='a_2')(out)\r\n    \r\n    tf.summary.FileWriter('/tmp/tensorboard/2', graph=graph).close()\r\n\r\n# Case 3\r\nwith tf.Graph().as_default() as graph:\r\n    x = tf.placeholder(tf.float32, (None, 3), 'x')\r\n   \r\n    out = x\r\n    out = A(name='a_1')(out)\r\n    out = A(name='a_2')(out)\r\n    out = A(name='a_3')(out)\r\n    \r\n    tf.summary.FileWriter('/tmp/tensorboard/3', graph=graph).close()\r\n```\r\n\r\n- Case 1\r\n<img alt=\"result\" height=\"450\" src=\"https://user-images.githubusercontent.com/7009040/31216908-2454165c-a9f0-11e7-9812-d85b3b4b1b2d.png\"/>\r\n\r\n- Case 2\r\n<img alt=\"result\" height=\"450\" src=\"https://user-images.githubusercontent.com/7009040/31216909-25709ed4-a9f0-11e7-9033-3770d0f22d71.png\"/>\r\n\r\n- Case 3\r\n<img alt=\"result\" height=\"450\" src=\"https://user-images.githubusercontent.com/7009040/31216915-2a201a90-a9f0-11e7-9418-9b8705ed20b6.png\"/>\r\n\r\n", "Thank you very much, @kzm4269 . Since `dense` might be slightly different with `Dense` , could you test `dense` as well? ", "OK.\r\nI have tried the following code but the result was exactly the same.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef A(**kwargs):\r\n    def layer(*args):\r\n        return tf.layers.dense(*args, units=3, **kwargs)\r\n    \r\n    return layer\r\n\r\n#...\r\n```\r\n", "https://github.com/tensorflow/tensorflow/blob/b58c89129894618ab28a023fb2b2e610bbbdcf81/tensorflow/python/layers/base.py#L527-L530\r\n\r\n```\r\nself._scope: dense\r\nself._scope.original_name_scope: dense/\r\n\r\nself._scope: dense_1\r\nself._scope.original_name_scope: dense_2/\r\n\r\nself._scope: dense_2\r\nself._scope.original_name_scope: dense_3/\r\n```\r\n\r\nAs we known, `get_variable(xxx)` only uses `variable_scope` as prefix.\r\nI believe that the bug is from the inconsistency between `variable_scope` and `name_scope`, which really surprises me.\r\nI guess that it is critical to find why `name_scope` jumps over `dense_1`.\r\n", "Hi, @kzm4269 , just in case, could you test it on [nightly pip packages](https://github.com/tensorflow/tensorflow#installation)?", "I create a hotfix #13518 for the issue. Results look good after patched. However, the patch might introduce some side effects, I'll finish the PR after holiday.\r\n\r\ncase 1:\r\n![1](https://user-images.githubusercontent.com/1112263/31264979-eaa4e68e-aa9e-11e7-8053-ad375289b03b.png)\r\n\r\n\r\ncase 2:\r\n![2](https://user-images.githubusercontent.com/1112263/31264989-fb821422-aa9e-11e7-92df-cdb6ff2e5ebd.png)\r\n\r\n\r\ncase 3:\r\n![3](https://user-images.githubusercontent.com/1112263/31264991-008f5ae2-aa9f-11e7-8bac-65860c59c875.png)\r\n\r\n\r\n", "Thank you @facaiy!", "Hi, @facaiy .\r\n\r\nI think that the bug caused by this line:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/97bcc65e4c218ec9e1af1c9353cb8aa0d84b7014/tensorflow/python/ops/variable_scope.py#L1785\r\n\r\nThis code was written to reuse the name scope, but since `name_scope` is relative, so a new unique name scope will be created. Properly it should be written like the following code:\r\n\r\n```python\r\n        name_scope = self._name_or_scope.name + \"/\"\r\n```", "Thanks you for suggestion, @kzm4269 . I found that it is more complicated and subtle than I expected when `name_scope` intertwines with `variable_scope`. As far as I see now, the behavior (create a new name_scope when variable_scope is opened) is on purpose by tensorflow, see #6189. I cannot believe it...", "Hi, @lukaszkaiser , let's discuss here. (cc @fchollet , @ebrevdo who I believe might be interested in the issue.) \r\n\r\nI found that the bug roots from an unexpected behavior from `variable_scope`, which always creates a new `name_scope` when it is opened (or re-entered). [see code here](https://github.com/tensorflow/tensorflow/blob/97bcc65e4c218ec9e1af1c9353cb8aa0d84b7014/tensorflow/python/ops/variable_scope.py#L1785). This is a simple example from #6189:\r\n\r\n```python\r\nwith tf.variable_scope('a') as scope:\r\n    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))\r\nwith tf.variable_scope(scope):\r\n    w = v * 2\r\nprint(w.name)    # expected: a/mul:0\r\n#result:  'a_1/mul:0'\r\n```\r\n\r\nAt the first glance, I believe it is a bug (that's what PR #13518 is opened to fix) until those test cases are found:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/a1ab2a3b5263c535bfece377f1bdd77c7ade3240/tensorflow/python/kernel_tests/variable_scope_test.py#L364-L371\r\n\r\nThe confusing behavior seems designed on purpose for tensorflow, and introduced from 79d872ef79a9a9f57dfcd82e3068b1d8bd9fe392 (if I\u2019m not mistaken with `git blame`).  Could @lukaszkaiser, if you know, give more details? Might we correct the unexpected behavior? Thanks.", "To help clarify our problem, I write a simpler version for `tf.layers.Layer`.  Please notice that  `a_1` name scope has been created in 1st layer. That's why for 2nd layer, name scope must start from `a_2`, while variable scope is from `a_1` (inconsistent scopes).\r\n\r\n### code\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef print_tensors():\r\n    print(tf.constant([1], name=\"t_const\"))\r\n    print(tf.Variable([1], name=\"t_var\"))\r\n\r\ndef print_scope():\r\n    print(\"variable_scope: {}, name_scope: {}\".format(tf.get_variable_scope().name, tf.contrib.framework.get_name_scope()))\r\n\r\ndef layer(name):\r\n    # `base._set_scope` method\r\n    with tf.variable_scope(None, default_name=name) as scope:\r\n        s = scope\r\n        print_scope()\r\n\r\n    # `base.add_variable` and `base.__call__` method\r\n    with tf.variable_scope(s) as ss:\r\n        print(\"---------------------\")\r\n        print(\"before using original_name_scope:\")\r\n        print_scope()\r\n        with tf.name_scope(ss.original_name_scope) as n:\r\n            print(\"after using original_name_scope:\")\r\n            print_scope()\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    print(\"1st layer:\")\r\n    a = layer(\"a\")\r\n    print(\"\\n===============================\\n\")\r\n    print(\"2nd layer:\")\r\n    a_1 = layer(\"a\")\r\n    print(\"\\n===============================\\n\")\r\n    print(\"3rd layer:\")\r\n    a_2 = layer(\"a\")\r\n```\r\n\r\n### logs\r\n```\r\n1st layer:\r\nvariable_scope: a, name_scope: a\r\n---------------------\r\nbefore using original_name_scope:\r\nvariable_scope: a, name_scope: a_1\r\nafter using original_name_scope:\r\nvariable_scope: a, name_scope: a\r\n\r\n===============================\r\n\r\n2nd layer:\r\nvariable_scope: a_1, name_scope: a_2\r\n---------------------\r\nbefore using original_name_scope:\r\nvariable_scope: a_1, name_scope: a_1_1\r\nafter using original_name_scope:\r\nvariable_scope: a_1, name_scope: a_2\r\n\r\n===============================\r\n\r\n3rd layer:\r\nvariable_scope: a_2, name_scope: a_3\r\n---------------------\r\nbefore using original_name_scope:\r\nvariable_scope: a_2, name_scope: a_2_1\r\nafter using original_name_scope:\r\nvariable_scope: a_2, name_scope: a_3\r\n```", "@ebrevdo, can you take a look at this issue?", "Hi @reedwm \r\n\r\nThe re-opening of name-scope when re-entering a variable scope is entirely on puropse. In this example:\r\n\r\n```\r\nwith tf.variable_scope('a') as scope:\r\n    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))\r\nwith tf.variable_scope(scope):\r\n    w = v * 2\r\nprint(w.name)    # expected: a/mul:0\r\n#result:  'a_1/mul:0'\r\n```\r\n\r\nthe current result, `a_1/mul:0`, is entirely expected. Imagine you're sharing a large subgraph, a large LSTM cell or a whole net. When you re-enter the scope of that graph, you'd rather add a `_1` to the root scope than add it to every single op, right? (Note that every op in the graph must have a unique name, so the `_1` will appear somewhere if you're re-creating a subgraph.)\r\n\r\nThe problem you note appears in a situation where you re-enter the scope not to re-create a subgraph but to continue some construction. This used to be more rare than reusing before, but with layers it's becoming more and more common, am I understanding this right? In any case, I think there is a simple way that was designed to do what you want, namely jump back to the original name scope, it's through `scope.original_name_scope`. So you can just rewrite your above example like this:\r\n\r\n```\r\nwith tf.variable_scope('a') as scope:\r\n    v = tf.get_variable('v', [], initializer=tf.constant_initializer(42., tf.float32))\r\nwith tf.variable_scope(scope):\r\n   with tf.name_scope(scope.original_name_scope):\r\n     w = v * 2\r\nprint(w.name)\r\n```\r\n\r\nDoes that work? I thought we were doing that everywhere in Layer class where it makes sense, but now I'm less sure. Can you check? I think this should make it possible to just make changes in Layer and not touch variable_scope, but maybe there's a bug somewhere?", "Hi, @lukaszkaiser. Could you take a look at my tiny example above, which indeed uses scope.original_name_scope in layer. Unfortunately, it couldn't solve the mismatch problem here. ", "Which one do you mean? Sorry, could you paste it again?", "To help clarify our problem, I write a simpler version for `tf.layers.Layer`.  Please notice that  `a_1` name scope has been created in 1st layer. That's why for 2nd layer, name scope must start from `a_2`, while variable scope is from `a_1` (inconsistent scopes).\r\n\r\n### code\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef print_tensors():\r\n    print(tf.constant([1], name=\"t_const\"))\r\n    print(tf.Variable([1], name=\"t_var\"))\r\n\r\ndef print_scope():\r\n    print(\"variable_scope: {}, name_scope: {}\".format(tf.get_variable_scope().name, tf.contrib.framework.get_name_scope()))\r\n\r\ndef layer(name):\r\n    # `base._set_scope` method\r\n    with tf.variable_scope(None, default_name=name) as scope:\r\n        s = scope\r\n        print_scope()\r\n\r\n    # `base.add_variable` and `base.__call__` method\r\n    with tf.variable_scope(s) as ss:\r\n        print(\"---------------------\")\r\n        print(\"before using original_name_scope:\")\r\n        print_scope()\r\n        with tf.name_scope(ss.original_name_scope) as n:\r\n            print(\"after using original_name_scope:\")\r\n            print_scope()\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    print(\"1st layer:\")\r\n    a = layer(\"a\")\r\n    print(\"\\n===============================\\n\")\r\n    print(\"2nd layer:\")\r\n    a_1 = layer(\"a\")\r\n    print(\"\\n===============================\\n\")\r\n    print(\"3rd layer:\")\r\n    a_2 = layer(\"a\")\r\n```\r\n\r\n### logs\r\n```\r\n1st layer:\r\nvariable_scope: a, name_scope: a\r\n---------------------\r\nbefore using original_name_scope:\r\nvariable_scope: a, name_scope: a_1\r\nafter using original_name_scope:\r\nvariable_scope: a, name_scope: a\r\n\r\n===============================\r\n\r\n2nd layer:\r\nvariable_scope: a_1, name_scope: a_2\r\n---------------------\r\nbefore using original_name_scope:\r\nvariable_scope: a_1, name_scope: a_1_1\r\nafter using original_name_scope:\r\nvariable_scope: a_1, name_scope: a_2\r\n\r\n===============================\r\n\r\n3rd layer:\r\nvariable_scope: a_2, name_scope: a_3\r\n---------------------\r\nbefore using original_name_scope:\r\nvariable_scope: a_2, name_scope: a_2_1\r\nafter using original_name_scope:\r\nvariable_scope: a_2, name_scope: a_3\r\n```", "@lukaszkaiser name scope jumps from a to a_2 for 2nd layer, though original_name_scope is used ", "Ah, this is a different problem, I see. Here is the summary: the first time you create a scope with default_name everything works fine, right? The second time, it creates a new variable_scope name `a_1` (it's done here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L1827).\r\n\r\nThe problem after that is that `name_scope` cannot re-open the scope `a_1` any more because it was already used in the previous call. So it increments it again, creating `a_2`, the new name scope. Which is preserved by `original_name_scope`.\r\n\r\nSo the problem would be that by entering a variable scope we create a name scope that's grabbing a name, even if it's empty and there's another re-opened name_scope just after that. One solution could be to try to prevent that somehow by checking things in the name_scope handler. Or adding functionality to clean up the reserved names if name_scope is empty. In all cases, it seems like it could be handled by changes to name_scope without touching variable_scope -- which is good, since this doesn't risk breaking compatibility.", "Yes, exactly right! ", "@lukaszkaiser How about adding a new parameter, say `auxiliary_name_scope`, for ` tf.variable_scope`:\r\n+ `auxiliary_name_scope=True`, create new name scope as before by default.\r\n+ `auxiliary_name_scope=None` or `auxiliary_name_scope=False`, don't create any name scope.\r\n+ `auxiliary_name_scope=some_name_scope`, reuse it if `some_name_scope`  is `name_scope`.\r\n\r\nIf so, we can write one line code to reuse both variable_scope and its name_scope:\r\n\r\n```python\r\nwith tf.variable_scope(s, auxiliary_name_scope=s.original_name_scope) as ss:\r\n        # do something\r\n```\r\nor\r\n```python\r\nwith tf.variable_scope(s, auxiliary_name_scope=None) as ss:\r\n     with tf.name_scope(ss.original_name_scope) as n:\r\n          # do something\r\n```", "That looks fine to me provided Martin and others agree to the API change.", "OK. Since the change is easy to implement, I'd like to propose a PR later for further discussion.", "Hi, all. #15471 is added to resolve the issue, could you take a look and comment? Thanks."]}, {"number": 13428, "title": "Branch 170584354 - 2", "body": "", "comments": []}, {"number": 13427, "title": "Fix typos", "body": "This PR fixes some typos: `correclty`, `lenght`, `identifer`, and `obejct`.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13426, "title": "//tensorflow/contrib/android:libtensorflow_inference.so build fails when compiling @protobuf//:protobuf", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Docker image gcr.io/tensorflow/tensorflow:1.3.0-devel\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.5.0\r\n\r\nOutput from `tf_env_collect.sh` is at the end of this report.\r\n\r\n### Describe the problem\r\nI'm trying to follow the instructions in [print_selective_registration_header.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/print_selective_registration_header.py#L15) to create a smaller TensorFlow binary size.\r\n\r\nPart of those instructions involves building ` //tensorflow/contrib/android:libtensorflow_inference.so`, but that build fails every time.\r\n\r\nI'm doing this in the `gcr.io/tensorflow/tensorflow:1.3.0-devel` Docker container (not sure if this is appropriate because I can't find documentation explaining what each container is for).  I tried to use the `1.3.0` container, but that doesn't contain the TensorFlow repo or `git`.\r\n\r\n### Source code / logs\r\n\r\nHere are the steps I took (the first steps succeeded so I have not included their output):\r\n```\r\n$ docker run -it -v $HOME/TF:/TF gcr.io/tensorflow/tensorflow:1.3.0-devel bash\r\n\r\n# bazel build tensorflow/python/tools:print_selective_registration_header\r\n\r\n# bazel-bin/tensorflow/python/tools/print_selective_registration_header --graphs=/TF/mnist_model_graph.pb > ops_to_register.h\r\n\r\n# bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" //tensorflow/contrib/android:libtensorflow_inference.so     --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --verbose_failures\r\nINFO: Reading 'startup' options from /etc/bazel.bazelrc: --batch\r\nWARNING: /tensorflow/tensorflow/core/BUILD:935:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\n<snip repeated warning>\r\nWARNING: /tensorflow/tensorflow/core/BUILD:935:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nINFO: Found 1 target...\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/protobuf/BUILD:133:1: C++ compilation of rule '@protobuf//:protobuf' failed: false failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /bin/false -DSELECTIVE_REGISTRATION -DSUPPORT_SELECTIVE_REGISTRATION -MD -MF bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/io/printer.pic.d '-frandom-seed=bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/io/printer.pic.o' -fPIC -iquote external/protobuf -iquote bazel-out/stub_armeabi-v7a-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/stub_armeabi-v7a-opt/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/stub_armeabi-v7a-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -c external/protobuf/src/google/protobuf/io/printer.cc -o bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/io/printer.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nINFO: Elapsed time: 202.538s, Critical Path: 11.70s\r\n```\r\nI also tried (with the same result):\r\n - `bazel clean` before the failing build step.\r\n - Removing the ops_to_register.h file and the `--copt` parameters.\r\n\r\nHere's my full environment info (which shows an error when running pywrap_tensorflow_internal):\r\n```\r\n# cat tf_env.txt\r\n\r\n== cat /etc/issue ===============================================\r\nLinux 813a49ffd3e0 4.9.27-moby #1 SMP Thu May 11 04:01:18 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 813a49ffd3e0 4.9.27-moby #1 SMP Thu May 11 04:01:18 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.2)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\nImportError: No module named pywrap_tensorflow_internal\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tools/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```", "comments": ["@andrewharp @petewarden might have some suggestions.", "@daj: Can you try adding --config=monolithic to your build command and see if that fixes the issue? It may be a docker issue, and we fixed something similar with the nightly build in f97195c6f936ee3edd9ad2620c091b742bb45476 by adding this flag.", "I tried all the steps from beginning to end with a fresh 1.3.0-devel container, and did a `git pull` for the `r1.3` branch.  I reproduced the problem again, then added the `--config=monolithic` flag, and hit a very similar looking problem (I'm pretty sure I've seen the `compilation of rule '@protobuf//:protobuf_lite'` error earlier too). \r\n```\r\n# bazel build --local_resources 4096,4.0,1.0 -j 2 -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" //tensorflow/contrib/android:libtensorflow_inference.so     --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --verbose_failures --config=monolithic\r\n<snip>\r\nINFO: Found 1 target...\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/protobuf/BUILD:93:1: C++ compilation of rule '@protobuf//:protobuf_lite' failed: false failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /bin/false -DSELECTIVE_REGISTRATION -DSUPPORT_SELECTIVE_REGISTRATION -MD -MF bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/message_lite.pic.d '-frandom-seed=bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/message_lite.pic.o' -fPIC -iquote external/protobuf -iquote bazel-out/stub_armeabi-v7a-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/stub_armeabi-v7a-opt/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/stub_armeabi-v7a-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -c external/protobuf/src/google/protobuf/message_lite.cc -o bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/message_lite.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\n```\r\n\r\nIn case it helps, here's the content of the `ops_to_register.h` file:\r\n```\r\n# cat ops_to_register.h \r\n// This file was autogenerated by print_selective_registration_header.py\r\n#ifndef OPS_TO_REGISTER\r\n#define OPS_TO_REGISTER\r\n\r\n    namespace {\r\n      constexpr const char* skip(const char* x) {\r\n        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;\r\n      }\r\n\r\n      constexpr bool isequal(const char* x, const char* y) {\r\n        return (*skip(x) && *skip(y))\r\n                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))\r\n                   : (!*skip(x) && !*skip(y));\r\n      }\r\n\r\n      template<int N>\r\n      struct find_in {\r\n        static constexpr bool f(const char* x, const char* const y[N]) {\r\n          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);\r\n        }\r\n      };\r\n\r\n      template<>\r\n      struct find_in<0> {\r\n        static constexpr bool f(const char* x, const char* const y[]) {\r\n          return false;\r\n        }\r\n      };\r\n    }  // end namespace\r\n    constexpr const char* kNecessaryOpKernelClasses[] = {\r\n\"BinaryOp<CPUDevice, functor::add<float>>\",\r\n\"BiasOp<CPUDevice, float>\",\r\n\"ConstantOp\",\r\n\"Conv2DOp<CPUDevice, float>\",\r\n\"MatMulOp<CPUDevice, float, false >\",\r\n\"MaxPoolingOp<CPUDevice, float>\",\r\n\"NoOp\",\r\n\"PlaceholderOp\",\r\n\"ReluOp<CPUDevice, float>\",\r\n\"ReshapeOp\",\r\n\"SoftmaxOp<CPUDevice, float>\",\r\n\"RecvOp\",\r\n\"SendOp\",\r\n};\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))\r\n\r\nconstexpr inline bool ShouldRegisterOp(const char op[]) {\r\n  return false\r\n     || isequal(op, \"Add\")\r\n     || isequal(op, \"BiasAdd\")\r\n     || isequal(op, \"Const\")\r\n     || isequal(op, \"Conv2D\")\r\n     || isequal(op, \"MatMul\")\r\n     || isequal(op, \"MaxPool\")\r\n     || isequal(op, \"NoOp\")\r\n     || isequal(op, \"Placeholder\")\r\n     || isequal(op, \"Relu\")\r\n     || isequal(op, \"Reshape\")\r\n     || isequal(op, \"Softmax\")\r\n     || isequal(op, \"_Recv\")\r\n     || isequal(op, \"_Send\")\r\n  ;\r\n}\r\n#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)\r\n\r\n#define SHOULD_REGISTER_OP_GRADIENT false\r\n#endif\r\n```", "Have you edited your tensorflow/WORKSPACE file for android to make sure that the paths are correct for your NDK/SDK installs?\r\n\r\nThe direct reason for failure is that it's running /bin/false. This typically happens when bazel can't find a necessary binary, such as one of the Android build tools. This unfortunately leads to some pretty unintuitive error messages.", "Thanks, that was the problem!  I keep forgetting that the `gcr.io/tensorflow/tensorflow` Docker images don't have all the necessary configuration already in place for Android builds.\r\n\r\nI have now successfully reduced libtensorflow_inference.so from 9.7MB to 2.5MB for the arm64-v8a architecture.  I've documented this process [in this Medium article](https://medium.com/@daj/how-to-shrink-the-tensorflow-android-inference-library-cb698facf758) and [this commit](https://github.com/daj/AndroidTensorFlowMNISTExample/commit/6e1b15ff6d3182be0e665c0456f356ffc2f62514).\r\n\r\nI have a couple of ideas for improvements:\r\n\r\n1) Please can you maintain a Docker image with the SDK+NDK+WORKSPACE configuration already in place?  \r\n  - I think any Android developer using a TensorFlow model will need to optimize libtensorflow_inference.so using selective registration to reduce their binary size.  A pre-built Docker image would be very useful for them (and probably easier to document).\r\n  - As an example, I created my own Docker container here (though this is probably too big):\r\nhttps://hub.docker.com/r/danjarvis/tensorflow-android/  Tag 1.3.0 contains print_selective_registration_header pre-built.\r\n\r\n2) Please can we improve the error messages when the SDK/NDK/WORKSPACE are missing?  I imagine many developers will make the same mistake as me. :-)\r\n\r\nFor either of these, I'd be happy to attempt a pull request if you could give me some pointers of where to start. Thanks!\r\n\r\n\r\n", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I've modified the tags to reflect @daj's ideas for improvements.\r\n\r\nNote also that newer versions of TensorFlow also have an SDK/NDK configuration-helper built into the `configure` script. ", "Hi @daj ,\r\nWe see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/13426\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/13426\">No</a>\n"]}, {"number": 13425, "title": "Initialize fetchTensors to fix NullPointerException", "body": "fetchTensors should not be null. This was broken by the refactoring in https://github.com/tensorflow/tensorflow/commit/244b8d6b0767c0fb63e58e56f58d03bd97c27822", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it.", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "Jenkins, test this please"]}, {"number": 13424, "title": "Support weight_column for time series regressor", "body": "", "comments": ["Right now the Estimators operate on a single series. What is getting weighted here? Parts of the series?", "I guess I should've spent some more time on the input pipeline. I was\nassuming those estimators support multiple series (for example multivariate\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/timeseries/examples/multivariate.py)\nso I could use part of the series (one of the columns in the data)  as\nweights. Please correct me if I misunderstood something here.\n\nOn Mon, Oct 2, 2017 at 12:30 PM Allen Lavoie <notifications@github.com>\nwrote:\n\n> Right now the Estimators operate on a single series. What is getting\n> weighted here? Parts of the series?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13424#issuecomment-333604561>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEEnSkHTCM6oIyNiILVSBtx7mlutSR6Cks5soR2NgaJpZM4Ppre5>\n> .\n>\n", "Ah, so the idea is that each feature of the series can have its own weight? I suppose that could be useful. Could you add a unit test or two?", "I am basically trying to complete feature parity against the heads used by\nnon-ts estimators, e.g.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/canned/head.py#L786\n\nRight now it's using the shared _weights() function in canned/head.py to\nextract the weight column from the list of columns from input.\n\nOn Mon, Oct 2, 2017 at 2:14 PM Allen Lavoie <notifications@github.com>\nwrote:\n\n> Ah, so the idea is that each feature of the series can have its own\n> weight? I suppose that could be useful. Could you add a unit test or two?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13424#issuecomment-333635284>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEEnSl95JtwH4Jx5Z0SgcKI-fWGTXls2ks5soTYYgaJpZM4Ppre5>\n> .\n>\n", "Got distracted by my projects recently. I am going to close this one for now until I found more time. "]}, {"number": 13423, "title": "In what cases a OpKernel instance may be accessed concurrently?", "body": "In the documentation for adding new op, it is said that Compute method of OpKernel should be thread safe, because instances of your OpKernel may be accessed concurrently. I want to know in what cases an instance may be accessed concurrently. If the graph has no loop or any other control flow,does this guarantee that every OpKernel instance is accessed only once for each forward/backward pass?\r\n\r\nThank you.", "comments": ["I think the issue here is that multiple session.run calls can run in parallel and reuse the same OpKernel", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13422, "title": "Branch 170584354", "body": "", "comments": ["@gunan is there a way to trigger the CLA bot ? ", "I signed it!!", "@sb2nov I think there is probably some configuration issue with your account re: CLAs? I don't have that issue when I submit changes -- the CLA bot accepts immediately without me asking.", "It was fine for other PRs so I don't think that is the issue. Also it didn't ever post the message about asking for CLA on this PR so might be something else.", "Jenkins, test this please.", "Jenkins, test this please.", "@martinwicke the CLA bot got stuck on this PR. Is it possible to merge this without that.", "@sb2nov could you try commenting `I signed it!` with single exclamation mark.\r\nWhat is curious is CLA bot never reported anything on this PR.", "I signed it!", "This is really curious. Let us wait for tomorrow, I can manually verify CLAs, and override CLAbot tomorrow morning if we cannot find a way to get it to work.", "@gunan the CLA bot is timing out in the check which is causing this issue. I think we can manually merge this as all the commits are from Googlers.\r\n\r\nA good future lesson to make merges smaller and resolving broken syncs faster.", "Got manual approval on https://github.com/tensorflow/tensorflow/pull/13435 so closing this."]}, {"number": 13421, "title": "Java API Generics Phase 3.", "body": "- Added the utility Tensors class.\r\n- Updated tests to use it.\r\n- Updated scripts for generating Tensors.java and the types directory.\r\n  Note that these are still run manually, but remain helpful because\r\n  maintaining so many methods and their documentation is a headache.\r\n- Added missing checking for attempts to create tensors from arrays of boxed\r\n  primitives, and a test case.", "comments": ["Can one of the admins verify this patch?", "@asimshankar Java Generics Phase 3 PR started.\r\n\r\nSome issues to consider:\r\n- Do we want to change the type parameter for strings?  It's genuinely unclear to me how often encoding mistakes will happen and how much TFString would actually help. The idea of getting more opinions sounds good; I can go either way.\r\n- Should the scripts for code generation be ported to some other language? They are not part of the build process, so maybe it's not so critical.\r\n- For convenience, we will probably want parallel methods in `op.Constant` for creating constant Tensors, folding together `Constant.createWithTensor` + `Tensors.create`. I'm thinking that can go into a later PR.", "Jenkins, test this please.", "Thanks for the PR. A few quick things:\r\n\r\n- RE: scripts to generate: Yes, we should port to another language (or heck, use an `AnnotationProcessor` :)). But can defer that for now because, as you point out, the generation scripts are not part of the build process (and have to be manually invoked). However, I didn't see the generated `Tensors.java` in this PR, shouldn't that be included too?\r\n\r\n- RE: Merging `Constant.createWithTensor` and `Tensors.create` sounds like a great idea, but again, something that we can do later on.\r\n\r\nPlease update the PR with the generated `Tensors.java`.\r\nThanks\r\n", "Thanks @andrewcmyers ! Much appreciated.", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 13420, "title": "Fix bug: variables outside won't update in DNNLinearCombinedRegressor", "body": "This fix is an proposal for #13419 \r\n\r\n### What changes were proposed in this pull request?\r\n\r\nThere are two solutions to fix the problem in my opinion:\r\n+ all variables except of linear is optimized by dnn.\r\n+ which one of optimizers does the variables left (except of linear and dnn) use is selected by user, perhaps a new argument is needed.\r\n\r\nThe PR is implemented by the first one for simplicity.\r\n\r\n### How to test\r\n\r\n+ [x] add test case: I have no idea of how to write it, hence any suggestions will be appreciated.\r\n+ [ ] pass all tests.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Hi, the failure is unrelated. Could you retest it? Anyway, I'll try to add a test case later.", "Hi, @ ispirmustafa . I agree with you that `input_fn` is not recommend to create variable, perhaps `Dataset` will be better or not. Anyway, it is not important. What I concerned is that the limit might be too strict (for custom).\r\n\r\nFor example, In practice, sentence features are not uncommon to make some preprocessing. We might split them, and build a shallow convolution for these features. So what is the best solution for the case?\r\n+ hack `input_fn`: easy, however rebuild need be careful (export model and load model).\r\n+ inherit `_FeatureColumn`: private class, and more code.\r\n+ create custom `Estimator`: more code.\r\n\r\nAs far as I known, feature engineering is fundamental to the traditional application of machine learning. However, it seems that we, unfortunately, cannot make some complicate preprocession for now. That's why I hope to removing the restriction. Could you give some suggestions? Thank you.", "How about creating a `Lambda` feature column, like `Lambda` layer of keras ?  It seems easier to customize our pre-procession."]}, {"number": 13419, "title": "BUG: variables outside won't update in DNNLinearCombinedRegressor", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.11.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n\r\n### Describe the problem\r\n\r\nVariables outside won't update for `DNNLinearCombinedRegressor`, while everything is OK for `DNNRegressor`.\r\n\r\nThe bug stems from that only variables in dnn / linear scope will be updated in the code below:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/107cc777af7880c140d089e44ad898a6ba929286/tensorflow/python/estimator/canned/dnn_linear_combined.py#L214-L227\r\n\r\n\r\n### Source code / logs\r\n\r\nThis is a tiny code to see whether `w` is updated or not.\r\n\r\n```python\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column as fc\r\nfrom tensorflow.python.summary import summary\r\n\r\ntf.logging.set_verbosity(tf.logging.DEBUG)\r\n\r\n\r\nBATCH_SIZE = 4\r\n\r\n\r\ndef input_fn():\r\n    x = tf.constant(np.random.randn(BATCH_SIZE, 4), dtype=tf.float32)\r\n\r\n    w = tf.Variable(np.array([1, 2, 3, 4]).reshape((4, 1)), dtype=tf.float32, name=\"test/w\")\r\n    summary.scalar(\"test/w_0_0\", w[0][0])\r\n    summary.scalar(\"test/w_1_0\", w[1][0])\r\n    summary.scalar(\"test/w_2_0\", w[2][0])\r\n    summary.scalar(\"test/w_3_0\", w[3][0])\r\n\r\n    y = tf.matmul(x, w)\r\n    label = tf.constant(np.random.randint(0, 1, size=(BATCH_SIZE,)))\r\n\r\n    return {\"y\": y}, label\r\n\r\nf = fc.numeric_column(\"y\")\r\n\r\ndef gen_estimator(cls, model_dir):\r\n    if cls == \"dnn\":\r\n        return tf.estimator.DNNRegressor(\r\n                feature_columns=[f],\r\n                hidden_units=[2],\r\n                model_dir=model_dir)\r\n    else:\r\n        return tf.estimator.DNNLinearCombinedRegressor(\r\n                dnn_feature_columns=[f],\r\n                dnn_hidden_units=[2],\r\n                model_dir=model_dir)\r\n\r\ngen_estimator(\"dnn\", model_dir=\"/tmp/tf/facai/test_dnn\").train(input_fn, steps=1000)\r\ngen_estimator(\"deep_and_wide\", model_dir=\"/tmp/tf/facai/test_wide_and_deep\").train(input_fn, steps=1000)\r\n```\r\n\r\n##### Results\r\n\r\n+ For `DNNRegressor`, `w` variables are updated:\r\n\r\n<img width=\"688\" alt=\"dnn\" src=\"https://user-images.githubusercontent.com/1112263/31045300-20944e50-a613-11e7-9f5d-611b5fde5de8.png\">\r\n\r\n+ For `DNNLinearCombinedRegressor`, `w` keeps constant:\r\n\r\n<img width=\"690\" alt=\"deep_and_wide\" src=\"https://user-images.githubusercontent.com/1112263/31045305-5a821192-a613-11e7-8610-7a6d312e2552.png\">\r\n\r\n\r\n", "comments": ["Hi, @ispirmustafa . I am not sure whether I understand correctly, could you take a look (and verify)? Thanks.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Hi @facaiy ,\r\nIt is not recommended to have variables within input_fn. "]}, {"number": 13418, "title": "BUG: variable won't update in input_fn (or outside Estimator)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.11.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: \r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nSometimes we will preprocess our data before feeding them into `Estimator`. For example, text data will be split or truncated at first, and then we might create a shallow convolution layer for it. However, it seems that those variables, if created, won't update in training.\r\n\r\nI create a tiny code below by using `input_fn` to clarify my question: variable `w` seems its initial value after training.\r\n\r\n\r\n### Source code / logs\r\n\r\ncode:\r\n\r\n```python\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column as fc\r\nfrom tensorflow.python.summary import summary\r\n\r\ntf.logging.set_verbosity(tf.logging.DEBUG)\r\n\r\n\r\nBATCH_SIZE=4\r\n\r\n\r\ndef input_fn():\r\n    x = tf.constant(np.random.randn(BATCH_SIZE, 4), dtype=tf.float32)\r\n\r\n    w = tf.Variable(np.array([1, 2, 3, 4]).reshape((4, 1)), dtype=tf.float32, name=\"test/w\")\r\n    summary.scalar(\"test/w[0][0]\", w[0][0])\r\n    summary.scalar(\"test/w[1][0]\", w[1][0])\r\n    summary.scalar(\"test/w[2][0]\", w[2][0])\r\n    summary.scalar(\"test/w[3][0]\", w[3][0])\r\n\r\n    y = tf.to_int64(tf.matmul(x, w))\r\n    label = tf.constant(np.random.randint(0, 1, size=(BATCH_SIZE,)))\r\n\r\n    return {\"y\": y}, label\r\n\r\nf = fc.embedding_column(\r\n        fc.categorical_column_with_hash_bucket(\"y\", 4, dtype=tf.int64),\r\n        dimension=2)\r\n\r\ne = tf.estimator.DNNRegressor(\r\n        feature_columns=[f],\r\n        hidden_units=[2],\r\n        model_dir=\"/tmp/tf/facai/test\")\r\n\r\ne.train(input_fn, steps=1000)\r\n```\r\n\r\nlogs:\r\n```\r\n~/Downloads \u276f\u276f\u276f python test.py\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_model_dir': '/tmp/tf/facai/test', '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_tf_random_seed': 1, '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_session_config': None, '_save_checkpoints_steps': None, '_save_summary_steps': 100}\r\nINFO:tensorflow:Summary name test/w[0][0] is illegal; using test/w_0__0_ instead.\r\nINFO:tensorflow:Summary name test/w[1][0] is illegal; using test/w_1__0_ instead.\r\nINFO:tensorflow:Summary name test/w[2][0] is illegal; using test/w_2__0_ instead.\r\nINFO:tensorflow:Summary name test/w[3][0] is illegal; using test/w_3__0_ instead.\r\nDEBUG:tensorflow:Transforming feature_column _HashedCategoricalColumn(key='y', hash_bucket_size=4, dtype=tf.int64).\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Saving checkpoints for 1 into /tmp/tf/facai/test/model.ckpt.\r\nINFO:tensorflow:step = 1, loss = 18.9489\r\nINFO:tensorflow:global_step/sec: 723.222\r\nINFO:tensorflow:step = 101, loss = 0.187707 (0.138 sec)\r\nINFO:tensorflow:global_step/sec: 817.997\r\nINFO:tensorflow:step = 201, loss = 0.0705907 (0.123 sec)\r\nINFO:tensorflow:global_step/sec: 777.309\r\nINFO:tensorflow:step = 301, loss = 0.037501 (0.128 sec)\r\nINFO:tensorflow:global_step/sec: 795.153\r\nINFO:tensorflow:step = 401, loss = 0.0219456 (0.126 sec)\r\nINFO:tensorflow:global_step/sec: 810.99\r\nINFO:tensorflow:step = 501, loss = 0.0132351 (0.123 sec)\r\nINFO:tensorflow:global_step/sec: 744.186\r\nINFO:tensorflow:step = 601, loss = 0.00807175 (0.134 sec)\r\nINFO:tensorflow:global_step/sec: 778.229\r\nINFO:tensorflow:step = 701, loss = 0.00494827 (0.129 sec)\r\nINFO:tensorflow:global_step/sec: 760.138\r\nINFO:tensorflow:step = 801, loss = 0.00304232 (0.131 sec)\r\nINFO:tensorflow:global_step/sec: 830.641\r\nINFO:tensorflow:step = 901, loss = 0.00187403 (0.120 sec)\r\nINFO:tensorflow:Saving checkpoints for 1000 into /tmp/tf/facai/test/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 0.00116151.\r\n```\r\n\r\n<img width=\"701\" alt=\"screen shot 2017-09-30 at 5 40 29 pm\" src=\"https://user-images.githubusercontent.com/1112263/31044707-d92a2ba4-a606-11e7-93c5-457ac8658141.png\">\r\n\r\n\r\n", "comments": ["Sorry, I found that something is wrong with my code, hence close it."]}, {"number": 13417, "title": "Is it wrong design of slim.learning.train?", "body": "Look at this [issue](https://github.com/tensorflow/tensorflow/issues/13342),  see my commit.", "comments": ["I list some problem which may be fixed:\r\n1.  slim.learning.train may need to add parameter `wait_for_checkpoint`, otherwise the parameter `wait_for_checkpoint` of `SessionManager`'s prepare_session is always `False`."]}, {"number": 13416, "title": "Install tensorflow-gpu in Ubuntun16.04 meet some problem", "body": "Successfully installed tensorflow-gpu-1.3.0\r\nt91@ubuntu:~/pengyulong$ python \r\nPython 3.5.2 (default, Nov 17 2016, 17:05:23) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/t91/pengyulong/tensorflow/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/t91/pengyulong/tensorflow/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/t91/pengyulong/tensorflow/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/t91/pengyulong/tensorflow/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\nP.S:run this Machine configure list:GPU:P100*4,cuda9.0,cudnn7.0", "comments": ["As you can read in the [installation docs](https://www.tensorflow.org/install/install_linux#nvidia_requirements_to_run_tensorflow_with_gpu_support) TF expects CUDA 8.0 and cuDNN 6.0 as of now.\r\n\r\nThat also can be inferred by the error log which makes reference to the CUDA version it's looking for \"libcusolver.so.**8.0**\". ", "How to install libcusolver.so.8.0?"]}, {"number": 13415, "title": "fix typo", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13414, "title": "updated instructions for libcupti for CUDA Toolkit >= 8.0", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 13413, "title": "V1.3.1 undefined reference to `clock_gettime' Error", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nReaHat 6.5 with self compiled gcc 4.8.2 installed locally\r\n- **TensorFlow installed from (source or binary)**:\r\nCompile from source\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n2.7.14 compiled from source\r\n- **Bazel version (if compiling from source)**:\r\n5.4.0 Compiled from source\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\nbazel build //tensorflow/examples/label_image:label_image\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI working on Redhat 6.5, with gcc 4.8.2 compiled from source and installed locally, binutiles like ld, as were not updated, glic was 2.12.\r\n\r\nI managed to compile tensorflow 1.3.1 shared C++ libs libtensorflow_cc.so, but when Iinking to my projects, it encounted with \"undefined reference to `clock_gettime' error. This error can also be reproduced by compiling label image examples with command \r\n``` \r\nbazel build //tensorflow/examples/label_image:label_image\r\n``` \r\nHere are the error details:\r\n\r\n/home/xxxxxxx/opensource/tensorflow/tensorflow/tensorflow/examples/label_image/BUILD:10:1: Linking of rule '//tensorflow/examples/label_image:label_image' failed (Exit 1).\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/libattention_ops.lo(attention_ops.o): In function `void Eigen::(anonymous namespace)::GlimpseExtractionOp<long>::eval<Eigen::TensorLayoutSwapOp<Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const>, Eigen::TensorMap<Eigen::Tensor<float, 4, 0, long>, 0, Eigen::MakePointer>, Eigen::ThreadPoolDevice>(Eigen::TensorLayoutSwapOp<Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const&, Eigen::TensorMap<Eigen::Tensor<float, 4, 0, long>, 0, Eigen::MakePointer>&, Eigen::ThreadPoolDevice const&) const':\r\nattention_ops.cc:(.text.unlikely._ZNK5Eigen12_GLOBAL__N_119GlimpseExtractionOpIlE4evalINS_18TensorLayoutSwapOpIKNS_9TensorMapINS_6TensorIKfLi4ELi1ElEELi16ENS_11MakePointerEEEEENS5_INS6_IfLi4ELi0ElEELi0ES9_EENS_16ThreadPoolDeviceEEEvRKT_RT0_RKT1_+0xbb): undefined reference to `clock_gettime'\r\nattention_ops.cc:(.text.unlikely._ZNK5Eigen12_GLOBAL__N_119GlimpseExtractionOpIlE4evalINS_18TensorLayoutSwapOpIKNS_9TensorMapINS_6TensorIKfLi4ELi1ElEELi16ENS_11MakePointerEEEEENS5_INS6_IfLi4ELi0ElEELi0ES9_EENS_16ThreadPoolDeviceEEEvRKT_RT0_RKT1_+0xdf): undefined reference to `clock_gettime'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/examples/label_image:label_image failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n\r\n\r\nI compiled python lib on the exactly the same environment. I had this error when running \" import tensorflow\". The error was fixed by modifying  \"return []\" to 'return [\"-lrt\"] ' in tensorflow/tensorflow.bzl line 975.\r\n\r\nI tried the same trick for label_image example, but it didn't work out.\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@javiribera please take a look.", "@channingxiao I encountered the same issue on RHEL6 in tensorflow 1.3.1. You need to link explicitly the librt library in RHEL6, since it uses glibc 2.12, while Google apparently uses a glibc newer than glibc 2.17, which changed the location of the time_* functions from librt to glibc itself. See [here](https://blog.flameeyes.eu/2012/12/glibc-2-17-what-s-going-to-be-a-trouble/) for more details.\r\n\r\nThe work-around is to change:\r\n\r\n```\r\n        \"//conditions:default\": [\r\n            \"-z defs\",\r\n            \"-s\",\r\n            \"-Wl,--version-script\",  #  This line must be directly followed by LINKER_VERSION_SCRIPT\r\n            LINKER_VERSION_SCRIPT,\r\n        ],\r\n```\r\n\r\nto\r\n\r\n```\r\n        \"//conditions:default\": [\r\n            \"-lrt\",\r\n            \"-z defs\",\r\n            \"-s\",\r\n            \"-Wl,--version-script\",  #  This line must be directly followed by LINKER_VERSION_SCRIPT\r\n            LINKER_VERSION_SCRIPT,\r\n        ],\r\n```\r\n\r\nin tensorflow/java/BUILD.\r\n\r\nLet me know if it works for you.\r\n\r\nI'm happy to make a PR like #10717 (which fixed the same problem for the C++ build on RHEL6) to fix this problem.", "Try to build  tensorflow add `-linkopt='-lrt'`:\r\n```\r\nbazel build  --linkopt='-lrt' -c opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nhttps://github.com/wangyum/Anaconda/blob/master/doc/installing-tensorflow-from-sources.md#package-tensorflow", "@wangyum\uff0cthat's work ,thank you ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Thank you! It helped me on gentoo\r\n`bazel build --linkopt='-lrt' -c opt //tensorflow:libtensorflow_cc.so --verbose_failures`", "Thanks @galv @wangyum \r\nClosing this out. If someone would like to make a PR, feel free to send one.\r\nThanks!"]}, {"number": 13412, "title": "Branch 170208694 2", "body": "", "comments": []}]