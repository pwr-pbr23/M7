[{"number": 28872, "title": "added two examples for math.argmax", "body": "", "comments": ["@yifeif can you please help merge this PR.", "The 2.0 branch will be recut, we don't need to merge this there. Instead, the change should be made to master.", "Closing as there is no activity and the change should be against master anyway"]}, {"number": 28871, "title": " Error parsing ApiDef file tensorflow/core/api_def/base_api/api_def_RefEnter.pbtxt", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu16.04\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:2.0\r\n- Python version:2.7\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):23.0 \r\n- GCC/Compiler version (if compiling from source):5.4\r\n- CUDA/cuDNN version:10.0/7.4\r\n- GPU model and memory:GPU\r\n\r\n**Describe the problem**\r\n\r\nbazel create bazel-out/k8- pt/genfiles/tensorflow/python/parsing_ops_pygenrule.genrule_script.sh\r\nhowever, this shell file could not be run correctly.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --config=opt --config=cuda --config=monolithic //tensorflow/python:parsing_ops_pygenrule --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nERROR: /home/xiesiyuan/Desktop/tensorflow2.0/tensorflow/tensorflow/python/BUILD:1933:1: Executing genrule //tensorflow/python:parsing_ops_pygenrule failed (Aborted): **bash failed: error executing command /bin/bash bazel-out/k8- opt/genfiles/tensorflow/python/parsing_ops_pygenrule.genrule_script.sh**\r\n2019-05-21 01:11:26.706445: F tensorflow/python/framework/python_op_gen_main.cc:123] Non-OK-status: api_def_map.LoadFileList(env, api_files) status: Invalid argument: Error parsing ApiDef file tensorflow/core/api_def/base_api/api_def_RefEnter.pbtxt: 5(4): Expected identifier, got:\r\n\r\nbazel-out/k8-opt/genfiles/tensorflow/python/parsing_ops_pygenrule.genrule_script.sh: line 2: 16961 Aborted  ", "comments": ["I run bazel-out/k8-opt/genfiles/tensorflow/python/parsing_ops_pygenrule.genrule_script.sh\r\ncould not parsing any words to .py file.", "This ERROR could be easily and quickly repeated if you type \r\n`\"bazel build --config=opt --config=cuda --config=monolithic //tensorflow/python:parsing_ops_pygenrule --cxxopt=\"-GLIBCXX_USE_CXX11_ABI=0\"`\r\nThis is very big bug of tensorflow 2.0.", "I am not the right assignee for this. Please reassign.", "@xiesiyuan Is this still an issue in 2.0.0b1? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I have meet the same problem... Could you please share the method to solve it? @xiesiyuan "]}, {"number": 28870, "title": "[ROCm] Adding ROCm support for the \"resize_bilinear\" op", "body": "This PR adds ROCm support for the \"resize_bilinear\" op\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n--------------------------\r\n\r\n@tatianashp , @whchung", "comments": []}, {"number": 28869, "title": "[ROCm] Adding ROCm support for the \"resize_nearest_neighbor\" op", "body": "This PR adds ROCm support for the \"resize_nearest_neighbor\" op\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n--------------------------\r\n\r\n@tatianashp , @whchung", "comments": []}, {"number": 28868, "title": "tf.summary.image does not work with keras layers in tf 2.0 due to EagerExecution issues", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Official tf docker \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):No\r\n- TensorFlow version (use command below):2.0.0-alpha0\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory:Titan V\r\n\r\nI use tf.keras subclassing, but when I want to track the some images in my network, the error at the bottom of the post occurs. Initially,  I think the reason for the error is, that `tf.summary.image` becomes a normal Tensor as input, but needs an eagerTensor. If I create a random tensor in the summary function and evaluate the type it is an eagerTensor, but if I use a Tensor from a `keras.layers.Layer` class it evaluates to a normal Tensor and throws the error (you can try it in your code). \r\n\r\nFirst I thought that the `tf.dataset` returns a non-eager tensor, but if I use a numpy array as input, the same error occurs as well. So the error might be caused by the `keras.layer.Layer` class.\r\n\r\nThe error occurs on GPU and CPU.\r\n\r\n\r\n**Working example**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninp_arr_without_ds = np.random.rand(2, 200, 200, 3)\r\n\r\ninp_arr_4_ds = np.random.rand(2, 2, 200, 200, 3)\r\ntf_ds = tf.data.Dataset.from_tensor_slices((inp_arr_4_ds, inp_arr_4_ds))\r\ntf_ds = tf_ds.map(lambda x, y: (x, y)).repeat(10).shuffle(10)\r\n\r\nclass random_model(tf.keras.Model):\r\n    \r\n    def __init__(self, name):\r\n        super(random_model, self).__init__(name=name)\r\n        self.conv_1 = tf.keras.layers.Conv2D(3, [3, 3], padding=\"same\")\r\n        self.tf_board_writer = tf.summary.create_file_writer(\"test\")\r\n        self.img_callback = [tf.keras.callbacks.LambdaCallback(on_epoch_end=self.save_img)]\r\n        \r\n    def call(self, inputs):\r\n        self.initialized_layer = self.conv_1(inputs)\r\n        return self.initialized_layer\r\n    \r\n    def save_img(self, epochs, logs):\r\n        with self.tf_board_writer.as_default():\r\n            # Does not work: type: class 'tensorflow.python.framework.ops.Tensor' \r\n            tf.summary.image(\"image\", self.initialized_layer, step=epochs)\r\n            \r\n            # Does work type: class 'tensorflow.python.framework.ops.EagerTensor'\r\n            #tf.summary.image(\"image\", tf.random.uniform([2, 200, 200, 3]), step=epochs) \r\n            \r\n    def compile_model(self):\r\n        self.compile(tf.optimizers.Adam(0.001), tf.losses.mean_absolute_error)\r\n    \r\n    def fit_model_with_ds(self, ds):\r\n        self.fit(ds, callbacks=self.img_callback)\r\n        \r\n    def fit_model_with_array(self, x, y):\r\n        self.fit(x, y, callbacks=self.img_callback)\r\n\r\nprint(tf.__version__)        \r\n\r\n# Both do not work\r\nnon_ds_model = random_model(\"non_ds\") \r\nnon_ds_model.compile_model()\r\nnon_ds_model.fit_model_with_array(inp_arr_without_ds, inp_arr_without_ds)\r\n            \r\ntf_ds_model = random_model(\"tf_ds\")\r\ntf_ds_model.compile_model()\r\ntf_ds_model.fit_model_with_ds(tf_ds)\r\n```\r\n\r\n\r\n    \r\n    \r\n\r\n\r\n> **Error Message**\r\n```\r\n> 2.0.0-alpha0\r\n> \r\n> ---------------------------------------------------------------------------\r\n> TypeError                                 Traceback (most recent call last)\r\n> <ipython-input-25-0b5a4b30a43a> in <module>\r\n>      39 non_ds_model = random_model(\"non_ds\")\r\n>      40 non_ds_model.compile_model()\r\n> ---> 41 non_ds_model.fit_model_with_array(inp_arr_without_ds, inp_arr_without_ds)\r\n>      42 \r\n>      43 tf_ds_model = random_model(\"tf_ds\")\r\n> \r\n> <ipython-input-25-0b5a4b30a43a> in fit_model_with_array(self, x, y)\r\n>      32 \r\n>      33     def fit_model_with_array(self, x, y):\r\n> ---> 34         self.fit(x, y, callbacks=self.img_callback)\r\n>      35 \r\n>      36 print(tf.__version__)\r\n> \r\n> /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n>     871           validation_steps=validation_steps,\r\n>     872           validation_freq=validation_freq,\r\n> --> 873           steps_name='steps_per_epoch')\r\n>     874 \r\n>     875   def evaluate(self,\r\n> \r\n> /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n>     406     if mode == ModeKeys.TRAIN:\r\n>     407       # Epochs only apply to `fit`.\r\n> --> 408       callbacks.on_epoch_end(epoch, epoch_logs)\r\n>     409     progbar.on_epoch_end(epoch, epoch_logs)\r\n>     410 \r\n> \r\n> /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_end(self, epoch, logs)\r\n>     288     logs = logs or {}\r\n>     289     for callback in self.callbacks:\r\n> --> 290       callback.on_epoch_end(epoch, logs)\r\n>     291 \r\n>     292   def on_train_batch_begin(self, batch, logs=None):\r\n> \r\n> <ipython-input-25-0b5a4b30a43a> in save_img(self, epochs, logs)\r\n>      20         with self.tf_board_writer.as_default():\r\n>      21             # Does not work: type: class 'tensorflow.python.framework.ops.Tensor'\r\n> ---> 22             tf.summary.image(\"image\", self.initialized_layer, step=epochs)\r\n>      23 \r\n>      24             # Does work type: class 'tensorflow.python.framework.ops.EagerTensor'\r\n> \r\n> /usr/local/lib/python3.5/dist-packages/tensorboard/plugins/image/summary_v2.py in image(name, data, step, max_outputs, description)\r\n>      71     encoded_images = tf.map_fn(tf.image.encode_png, limited_images,\r\n>      72                                dtype=tf.string,\r\n> ---> 73                                name='encode_each_image')\r\n>      74     # Workaround for map_fn returning float dtype for an empty elems input.\r\n>      75     encoded_images = tf.cond(\r\n> \r\n> /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/map_fn.py in map_fn(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)\r\n>     226                                      dynamic_size=False,\r\n>     227                                      infer_shape=True)\r\n> --> 228         for elem in elems_flat]\r\n>     229     # Unpack elements\r\n>     230     elems_ta = [\r\n> \r\n> /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/map_fn.py in <listcomp>(.0)\r\n>     226                                      dynamic_size=False,\r\n>     227                                      infer_shape=True)\r\n> --> 228         for elem in elems_flat]\r\n>     229     # Unpack elements\r\n>     230     elems_ta = [\r\n> \r\n> /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/tensor_array_ops.py in __init__(self, dtype, size, dynamic_size, clear_after_read, tensor_array_name, handle, flow, infer_shape, element_shape, colocate_with_first_write_call, name)\r\n>    1036         element_shape=element_shape,\r\n>    1037         colocate_with_first_write_call=colocate_with_first_write_call,\r\n> -> 1038         name=name)\r\n>    1039 \r\n>    1040     self._implementation.parent = weakref.ref(self)\r\n> \r\n> /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/tensor_array_ops.py in __init__(***failed resolving arguments***)\r\n>     742     if isinstance(size, ops.EagerTensor):\r\n>     743       size = size.numpy()\r\n> --> 744     self._tensor_array = [None for _ in range(size)]\r\n>     745 \r\n>     746   @property\r\n> \r\n> TypeError: 'Tensor' object cannot be interpreted as an integer\r\n> \r\n\r\n```\r\n", "comments": ["@2649 Ran the provided code, it says NameError: name 'map_ds' is not defined. Please provide full reproducible code to investigate it further.", "@muddham Sorry, my bad. This was an old variable. Now you can run it.", "@muddham I investigated the problem further and when you evaluate `tf.executing_eagerly()` in every function, only the `call()` function evaluates to `False`. I am not sure if this is expected or not?\r\n\r\nI also checked all `tf.keras.base_layer` descendant and their `dynamic `attribute, which all evaluate to False. When I set it to True for every instance via the `**kwargs` it throws the following error: \r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-40-7c7316d7e638> in <module>\r\n     43 # Both do not work\r\n     44 non_ds_model = random_model(\"non_ds\")\r\n---> 45 non_ds_model.compile_model()\r\n     46 non_ds_model.fit_model_with_array(inp_arr_without_ds, inp_arr_without_ds)\r\n     47 \r\n\r\n<ipython-input-40-7c7316d7e638> in compile_model(self)\r\n     31 \r\n     32     def compile_model(self):\r\n---> 33         self.compile(tf.optimizers.Adam(0.001), tf.losses.mean_absolute_error, target_tensor=tf.random.uniform([2, 200, 200, 3]))\r\n     34 \r\n     35     def fit_model_with_ds(self, ds):\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    454     self._setattr_tracking = False  # pylint: disable=protected-access\r\n    455     try:\r\n--> 456       result = method(self, *args, **kwargs)\r\n    457     finally:\r\n    458       self._setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    278     self.sample_weight_mode = sample_weight_mode\r\n    279     self._compile_weighted_metrics = weighted_metrics\r\n--> 280     if self.run_eagerly and target_tensors is not None:\r\n    281       raise ValueError(\r\n    282           'target_tensors argument is not supported when '\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in run_eagerly(self)\r\n    506     else:\r\n    507       if not context.executing_eagerly():\r\n--> 508         raise ValueError('Your model contains layers that can only be '\r\n    509                          'successfully run in eager execution (layers '\r\n    510                          'constructed with `dynamic=True`). '\r\n\r\nValueError: Your model contains layers that can only be successfully run in eager execution (layers constructed with `dynamic=True`). You must enable eager execution with `tf.enable_eager_execution()`.\r\n```", "@2649 Able to reproduce the issue with th provided code.\r\n\r\nTypeError: 'Tensor' object cannot be interpreted as an integer", "Any ideas?", "@2649 I think there is some issue with your callback code that is giving the error. If I remove the callback function from `model.fit` then there is no error. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/98cea0e3381567496b1dad4228fe4601/tf28868_keras_layers.ipynb). Thanks!", "@jvishnuvardhan Yes this is correct. I already pointed it out in the first post. The tf 2.0 documentation recommends to use `tf.summary.image` to log image files. This function excepts an eager tensor. However, when you use keras to train your model,  it will build a static graph and the summary will throw the mentioned error. ", "Thanks for reporting.\r\nThis is by design. tf.keras in 2.0 automatically wrap the call in tf.function (to accelerate computation), so output is symbolic tensor, which then you used for callbacks.\r\nTo make this work, there are two approaches:\r\n1) use the regular model fit with Tensorboard callback, or tf.print (whichever works in your case)\r\n2) compile the model with run_eagerly=True, so that output is EagerTensor instead of symbolic tensor.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28868\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28868\">No</a>\n", "This issue is not resolved for me. I use the regular fit function with the tensorboard callback, but I still get the same error. Setting the run_eagerly parameter also does not help. I use the output from a convolutional layer as input to the image summary operation.\r\n\r\nIs there anything else I should be aware of to make this work?", "I just switched from the keras compile and fit method to a custom training loop with gradient tape and wrap the computational parts in a `tf.function`. These are only few more lines of code, but you are very flexible. I'm very satisfied with this functionality. In between the `tf.function` calls you can use the `tf.summary` api, because it returns eager tensors. I used this [tutorial](https://www.tensorflow.org/beta/tutorials/generative/cvae).\r\n\r\nBut I can understand, that the keras training procedure is not suited for eager summary, because the focus is different.", "@mlippie If there is something not working with model.fit and callback, can you file another bug and include your code snippet?", "Hi @tanzhenyu,\r\nI tested with tensorflow-gpu 2.0.0-rc0 and still have this problem while using `tf.summary.image` with a `tf.keras.layer` of a model\r\n`TypeError: 'Tensor' object cannot be interpreted as an integer`\r\nI follow the [Using TensorBoard in Notebooks](https://www.tensorflow.org/tensorboard/r2/tensorboard_in_notebooks) . You can check [this notebook file](https://colab.research.google.com/drive/1HIe4EnwOomI0tC9vsVUcMX-zdbpV0QFU)\r\n\r\nIn #32034 , I mentioned there was the easiest way to log image in keras layer using tensorflow version tf.1x \r\nNow in tf.2x , everything is changed so I can't find the easist way to log image while training in tensorboard. ", "@shaolinkhoa \r\nHave you solved this problem?", "@fengyang0317 \r\nI'm sorry, I couldn't, therefore, I'm using Pytorch now.", "This issue should not be closed. Still experiencing this exact issue and the suggestions by @tanzhenyu are not fixes.", "Still experiencing this issue in TF2.1 and TF2.2", "Just ran into this issue, tf.summary.image seems to not work with symbolic tensors. I'm trying to display output from a layer in my network."]}, {"number": 28867, "title": "Sync find_cuda_config.py with master (fixes CUDA 10.1 build)", "body": "Linux build on r1.14 branch currently fails with CUDA 10.1 (issue #28865).\r\n\r\nThis is already fixed on master in third_party/gpus/find_cuda_config.py, so this pull request just syncs r1.14 with master for this file.\r\n", "comments": []}, {"number": 28866, "title": "Fix the output dtype issue in graph_test_utils", "body": "This PR fixes the output dtype issue in graph_test_utils.", "comments": ["@jsimsa Could you help review this PR when you have time?"]}, {"number": 28865, "title": "TF1.14 compile still fails with CUDA 10.1 (\"Could not find any cublas_api.h\")", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: r1.14 tag\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: venv\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 8.2.1\r\n- CUDA/cuDNN version: 10.1/7.5.1\r\n- GPU model and memory: V100\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nCUDA build fails because cublas_api.h is now under /usr/include and not /usr/local/cuda/include. (And the r1.14 find_cuda_config.py script ignores TF_CUDA_PATHS when searching for cublas.)\r\n\r\nThis problem is fixed on master. Someone just needs to bring third_party/gpu/find_cuda_config.py to the r1.14 branch.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nenv TF_CUDA_PATHS=/usr/local/cuda,/usr ./configure\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\n\r\nCuda Configuration Error: Failed to run find_cuda_config.py: Could not find any cublas_api.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\n        '/usr/local/cuda'\r\n\r\nINFO: Elapsed time: 0.468s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\n", "comments": ["@plopresti Can you please follow the steps [install TF from sources](https://www.tensorflow.org/install/source#tested_build_configurations). Please install 10.0 CUDA version and let us know how it progresses. Thanks!", "@plopresti Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Reopening the issue just because I faced the same problem on Windows.\r\n\r\n**System Information**\r\n- *OS Platform*: Windows 10\r\n- *TensorFlow installed from (source or binary)*: Source\r\n- *TensorFlow version*: master at 2fd2eff6cb\r\n- *Python version*: 3.6.8\r\n- *Bazel version (if compiling from source)*: 0.26.1\r\n- *GCC/Compiler version (if compiling from source)*: MSVC Tools 19.21\r\n- *CUDA/cuDNN version*: 10.1/7.6.1\r\n- *GPU model and memory*: Quadro P1000 4GB\r\n\r\n**Describe the problem**\r\nDuring the configuration process I specified the CUDA paths with as `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1`. When Bazel launches `cuda_configure.bzl`, the `find_cuda_config.py` script is not able to locate the libraries because the paths are purged from the single back slashes.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI was following the steps provided on the [Build from source on Windows](https://www.tensorflow.org/install/source_windows) page. The error raised after launching the bazel compilation.\r\n\r\n**Any other info/logs**\r\n```\r\nReading rc options for 'build' from d:\\tensorflow\\.tf_configure.bazelrc:\r\n'build' options:\r\n    --action_env PYTHON_BIN_PATH=C:/Program Files/Python36/python.exe\r\n    --action_env PYTHON_LIB_PATH=C:/Program Files/Python36/lib/site-packages\r\n    --python_path=C:/Program Files/Python36/python.exe\r\n    --action_env TF_CUDA_VERSION=10.1\r\n    --action_env TF_CUDNN_VERSION=7.6.1\r\n    --action_env TF_CUDA_PATHS=C:Program FilesNVIDIA GPU Computing ToolkitCUDAv10.1,C:localcudnn-10.1-v7.6.1cuda\r\n    --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    --config=cuda\r\n    --config monolithic\r\n    --copt=-w\r\n    --host_copt=-w\r\n    --copt=-DWIN32_LEAN_AND_MEAN\r\n    --host_copt=-DWIN32_LEAN_AND_MEAN\r\n    --copt=-DNOGDI\r\n    --host_copt=-DNOGDI\r\n    --verbose_failures\r\n    --distinct_host_configuration=false\r\n    --define=override_eigen_strong_inline=true\r\n    --action_env TF_CONFIGURE_IOS=0\r\n```\r\n\r\nI moved forward adding an additional back slash to the paths I was specifying.", "You may instead use backslashes, as in all other paths.\r\nWorked for me.", "What worked for me was editing the `find_cuda_config.py` file. I went to line 455, after the script updated variable base_paths by filtering paths that existed via `os.path.exists(path)`, and then I added a `base_paths.append(\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.1\")` line, explicit adding to base_paths the CUDA path in my Windows 10. After that, `bazel build --config=v2 //tensorflow/tools/pip_package:build_pip_package` stopped complaining that it didn't find `cublas_api.h` file.\r\n\r\nObservation: I had to run `configure.cmd` again, and even it started to find CUDA and cuDNN automatically, without the need to explicit inform their paths. Changing `find_cuda_config.py` also fixes `configure.cmd` calls.", "Adding BAZEL_VC environment variable for the VC++ build tool location solved it for me:\r\nBAZEL_VC\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC", "I have faced with the same issue:\r\n\r\nFor such path `C:\\tools\\cuda,C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1`:\r\n```console\r\nINFO: Repository local_config_cuda instantiated at:\r\n  C:/tensorflow/WORKSPACE:19:16: in <toplevel>\r\n  C:/tensorflow/tensorflow/workspace.bzl:98:19: in tf_repositories\r\nRepository rule cuda_configure defined at:\r\n  C:/tensorflow/third_party/gpus/cuda_configure.bzl:1399:33: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369, column 38, in _cuda_autoconf_impl\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 955, column 35, in _create_local_cuda_repository\r\n                cuda_config = _get_cuda_config(repository_ctx, find_cuda_config_script)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 657, column 30, in _get_cuda_config\r\n                config = find_cuda_config(repository_ctx, find_cuda_config_script, [\"cuda\", \"cudnn\"])\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 635, column 41, in find_cuda_config\r\n                exec_result = _exec_find_cuda_config(repository_ctx, script_path, cuda_libraries)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 629, column 19, in _exec_find_cuda_config\r\n                return execute(repository_ctx, [python_bin, \"-c\", decompress_and_execute_cmd])\r\n        File \"C:/tensorflow/third_party/remote_config/common.bzl\", line 208, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\nCould not find any cudnn.h, cudnn_version.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\n        'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1'\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Repository command failed\r\nCould not find any cudnn.h, cudnn_version.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\n        'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1'\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Repository command failed\r\nCould not find any cudnn.h, cudnn_version.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\n        'C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1'\r\nINFO: Elapsed time: 0.381s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```\r\n\r\nAnd for such path `C:\\\\tools\\\\cuda,C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.1`:\r\n```console\r\nINFO: Repository local_config_cuda instantiated at:\r\n  C:/tensorflow/WORKSPACE:19:16: in <toplevel>\r\n  C:/tensorflow/tensorflow/workspace.bzl:98:19: in tf_repositories\r\nRepository rule cuda_configure defined at:\r\n  C:/tensorflow/third_party/gpus/cuda_configure.bzl:1399:33: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369, column 38, in _cuda_autoconf_impl\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1214, column 56, in _create_local_cuda_repository\r\n                host_compiler_includes + _cuda_include_path(\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 363, column 32, in _cuda_include_path\r\n                inc_entries.append(realpath(repository_ctx, cuda_config.cuda_toolkit_path + \"/include\"))\r\n        File \"C:/tensorflow/third_party/remote_config/common.bzl\", line 268, column 19, in realpath\r\n                return execute(repository_ctx, [bash_bin, \"-c\", \"realpath \\\"%s\\\"\" % path]).stdout.strip()\r\n        File \"C:/tensorflow/third_party/remote_config/common.bzl\", line 208, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\nrealpath: 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include': No such file or directory\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Repository command failed\r\nrealpath: 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include': No such file or directory\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Repository command failed\r\nrealpath: 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include': No such file or directory\r\nINFO: Elapsed time: 1.268s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```\r\n\r\nWhy it so complicated to compile tensorflow for Windows ???\r\n\r\nIt seems for me that it is bunch of issues in python script on Windows ((", "@AndreyPlotkinOr @ayrtondenner @elvetian\r\n\r\nI have faced with another issue:\r\n\r\n```console\r\nIf the definition of 'com_google_protobuf' was updated, verify that the hashes were also updated.\r\nERROR: An error occurred during the fetch of repository 'com_google_protobuf':\r\n   Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 110, column 25, in _tf_http_archive\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 68, column 32, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"C:/tensorflow/third_party/repo.bzl\", line 52, column 13, in _execute_and_check_ret_code\r\n                fail((\"Non-zero return code({1}) when executing '{0}':\\n\" + \"Stdout: {2}\\n\" +\r\nError in fail: Non-zero return code(2) when executing 'C:\\WINDOWS\\system32\\bash.exe -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/{user}/_bazel_{user}/xv6zejqw/external/com_google_protobuf\" \"-i\" \"C:/tensorflow/third_party/protobuf/protobuf.patch\"':\r\nStdout:\r\nStderr: patch: **** Can't change to directory C:/users/{user}/_bazel_{user}/xv6zejqw/external/com_google_protobuf : No such file or directory\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@com_google_protobuf//': Non-zero return code(2) when executing 'C:\\WINDOWS\\system32\\bash.exe -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/{user}/_bazel_{user}/xv6zejqw/external/com_google_protobuf\" \"-i\" \"C:/tensorflow/third_party/protobuf/protobuf.patch\"':\r\nStdout:\r\nStderr: patch: **** Can't change to directory C:/users/{user}/_bazel_{user}/xv6zejqw/external/com_google_protobuf : No such file or directory\r\n```\r\n\r\nIt is pretty strange because when I run the following command manually it works:\r\n'C:\\WINDOWS\\system32\\bash.exe -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/{user}/_bazel_{user}/xv6zejqw/external/com_google_protobuf\" \"-i\" \"C:/tensorflow/third_party/protobuf/protobuf.patch\"'\r\n\r\nBut build still fails ...\r\n\r\nI have found the similar issue https://stackoverflow.com/questions/60449959/error-arises-when-build-tensorflow-from-source-on-windows-with-bazel, but unfortunately it is without answer ..."]}, {"number": 28864, "title": "[ROCm] Adding ROCm support for the random ops", "body": "This PR adds ROCm support for the random ops\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n------------------------\r\n\r\n@tatianashp , @whchung", "comments": []}, {"number": 28863, "title": "Build Tensorflow Lite from source on Yocto Linux", "body": "I have built a custom 32 bit Linux with [Yocto project](https://www.yoctoproject.org/), now i have a gcc toolchain that full support on c and c++ standard library, can i build Tensorflow Lite from source? Which dependencies will i have to install for a succesful build?\r\n", "comments": ["I did the same using tensorflow/lite/tools/make/Makefile on x86, ppc64le, and s390.\r\nUse the dependencies as listed in download_dependencies.sh.\r\nA few full tensorflow files are needed:\r\ntensorflow/workspace.bzl is only needed for\r\ntensorflow/lite/tools/make/download_dependencies.sh\r\n(to pick up some download URLs)\r\n\r\ntensorflow/core/util/ seems only needed to build benchmark_model\r\nbenchmark_model.h:\r\n#include \"tensorflow/core/util/stats_calculator.h\"\r\n\r\nHere is a listing of the files needed in tensorflow (apart from lite):\r\n<pre>\r\n$ ls -la\r\n       total 60\r\n       drwxrwxr-x  4 geert geert  4096 May 15 15:26 .\r\n       drwxrwxr-x  4 geert geert  4096 May 16 09:48 ..\r\n       drwxrwxr-x  5 geert geert  4096 Apr 19 09:41 core\r\n       drwxrwxr-x 22 geert geert  4096 May 13 13:52 lite\r\n       -rwxrwxr-x  1 geert geert 44212 Apr 19 09:36 workspace.bzl\r\n\r\n$ ls -laR core\r\n       core:\r\n       total 20\r\n       drwxrwxr-x 5 geert geert 4096 Apr 19 09:41 .\r\n       drwxrwxr-x 4 geert geert 4096 May 15 15:26 ..\r\n       drwxrwxr-x 2 geert geert 4096 Apr 18 17:19 kernels\r\n       drwxrwxr-x 2 geert geert 4096 Apr 18 17:15 public\r\n       drwxrwxr-x 2 geert geert 4096 May 13 13:54 util\r\n\r\n       core/kernels:\r\n       total 72\r\n       drwxrwxr-x 2 geert geert  4096 Apr 18 17:19 .\r\n       drwxrwxr-x 5 geert geert  4096 Apr 19 09:41 ..\r\n       -rw-rw-r-- 1 geert geert 62142 Mar 15 17:26 eigen_spatial_convolutions-inl.h\r\n\r\n       core/public:\r\n       total 16\r\n       drwxrwxr-x 2 geert geert 4096 Apr 18 17:15 .\r\n       drwxrwxr-x 5 geert geert 4096 Apr 19 09:41 ..\r\n       -rw-rw-r-- 1 geert geert 6088 Mar 15 17:27 version.h\r\n\r\n       core/util:\r\n       total 32\r\n       drwxrwxr-x 2 geert geert 4096 May 13 13:54 .\r\n       drwxrwxr-x 5 geert geert 4096 Apr 19 09:41 ..\r\n       -rw-rw-r-- 1 geert geert 9732 Mar 15 17:27 stats_calculator.cc\r\n       -rw-rw-r-- 1 geert geert 5724 Mar 15 17:27 stats_calculator.h\r\n       -rw-rw-r-- 1 geert geert 1397 Mar 15 17:27 stat_summarizer_options.h\r\n</pre>", "it's amazing, thanks @geert56 , i will try with my toolchain now.", "@geert56 : Thanks you for providing the support\r\n@nguyenhuy3588 : Let us know if you are able to resolve it. Thanks!", "Hi @achandraa will i need to separately compile library dependencies downloaded by ownload_dependencies.sh?\r\n\r\nI successfully compile object file for the source in tensorflow/lite but got some undefined reference error when creating libtensorflow-lite.a, like so:\r\n\r\n> /home/xxxx/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include/flatbuffers/base.h:356: undefined reference to `flatbuffers::ClassicLocale::instance_' \r\n/home/xxxx/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a(if.o): In function `flexbuffers::Indirect(unsigned char const*, unsigned char)':\r\n", "I face this problem when compile with ubuntu default environtment too.\r\n\r\nForgot to mention, i change the CXX_FLAGS from:\r\n\r\n> CXXFLAGS := -O3 -DNDEBUG -fPIC\r\nCXXFLAGS += $(EXTRA_CXXFLAGS)\r\n\r\nto only this:\r\n\r\n> CXXFLAGS := -O3 -DNDEBUG -fPIC -pipe -g -feliminate-unused-debug-types\r\n\r\nfor not typing so many each time but i think this's not a problem.", "> Hi @achandraa will i need to separately compile library dependencies downloaded by ownload_dependencies.sh?\r\n> \r\n> I successfully compile object file for the source in tensorflow/lite but got some undefined reference error when creating libtensorflow-lite.a, like so:\r\n> \r\n> > /home/xxxx/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include/flatbuffers/base.h:356: undefined reference to `flatbuffers::ClassicLocale::instance_'  /home/xxxx/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a(if.o): In function `flexbuffers::Indirect(unsigned char const*, unsigned char)':\r\n\r\nHello\uff0cDid you solved this problem\uff1f", "> > Hi @achandraa will i need to separately compile library dependencies downloaded by ownload_dependencies.sh?\r\n> > I successfully compile object file for the source in tensorflow/lite but got some undefined reference error when creating libtensorflow-lite.a, like so:\r\n> > > /home/xxxx/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include/flatbuffers/base.h:356: undefined reference to `flatbuffers::ClassicLocale::instance_'  /home/xxxx/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a(if.o): In function `flexbuffers::Indirect(unsigned char const*, unsigned char)':\r\n> \r\n> Hello\uff0cDid you solved this problem\uff1f\r\n\r\nSame problem for me. If someone has a patch, I can test it.\r\n", "> > > Hi @achandraa will i need to separately compile library dependencies downloaded by ownload_dependencies.sh?\r\n> > > I successfully compile object file for the source in tensorflow/lite but got some undefined reference error when creating libtensorflow-lite.a, like so:\r\n> > > > /home/xxxx/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include/flatbuffers/base.h:356: undefined reference to `flatbuffers::ClassicLocale::instance_'  /home/xxxx/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a(if.o): In function `flexbuffers::Indirect(unsigned char const*, unsigned char)':\r\n> > \r\n> > \r\n> > Hello\uff0cDid you solved this problem\uff1f\r\n> \r\n> Same problem for me. If someone has a patch, I can test it.\r\n\r\nI still can't solve it, waiting for Tensorflow for help.", "Hi,\r\nBuilding tensorflow lite for ARM64 with the below link\r\nhttps://www.tensorflow.org/lite/guide/build_arm64\r\nDependencies are resolved.\r\nChanged the Makefile to build for arm(./tensorflow/lite/tools/make/build_aarch64_lib.sh)\r\nMakefile Changes:\r\nTARGET := aarch64\r\nTARGET_ARCH := armv8-a\r\nwhen i run this file ./tensorflow/lite/tools/make/build_aarch64_lib.sh getting the below error.\r\n\r\n/home/xxxxx/ARMNN/tensorflow_lite_build/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/benchmark-lib.a(test_main.o): In function `main':\r\ntest_main.cc:(.text.startup+0x48): undefined reference to `testing::InitGoogleTest(int*, char**)'\r\ntest_main.cc:(.text.startup+0x250): undefined reference to `tflite::SingleOpModel::SetForceUseNnapi(bool)'\r\ntest_main.cc:(.text.startup+0x25c): undefined reference to `testing::UnitTest::GetInstance()'\r\ntest_main.cc:(.text.startup+0x260): undefined reference to `testing::UnitTest::Run()'\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:284: recipe for target '/home/xxxxx/ARMNN/tensorflow_lite_build/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/benchmark_model' failed\r\nmake: *** [/home/xxxxx/ARMNN/tensorflow_lite_build/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/benchmark_model] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\n/home/xxxxx/ARMNN/tensorflow_lite_build/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(while.o): In function `tflite::ops::custom::while_kernel::Init(TfLiteContext*, char const*, unsigned long)':\r\nwhile.cc:(.text+0x1d04): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nwhile.cc:(.text+0x1d18): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nwhile.cc:(.text+0x1d6c): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nwhile.cc:(.text+0x1d78): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n/home/xxxxxx/ARMNN/tensorflow_lite_build/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(audio_spectrogram.o): In function `tflite::ops::custom::audio_spectrogram::Init(TfLiteContext*, char const*, unsigned long)':\r\naudio_spectrogram.cc:(.text+0xf00): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n/home/xxxxxx/ARMNN/tensorflow_lite_build/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(audio_spectrogram.o):audio_spectrogram.cc:(.text+0xf0c): more undefined references to `flatbuffers::ClassicLocale::instance_' follow\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:270: recipe for target '/home/aiiec/ARMNN/tensorflow_lite_build/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal' failed\r\nmake: *** [/home/xxxxx/ARMNN/tensorflow_lite_build/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal] Error 1\r\n\r\nPlease give your inputs how to resolve it.\r\nBy going in deep analysis the issue is with google test library. Please find the below link to build it.\r\nhttps://stackoverflow.com/questions/13513905/how-to-setup-googletest-as-a-shared-library-on-linux\r\nChanged the Makefile by including \"-lgtest\" at line 60\r\nCXXFLAGS := -O3 -DNDEBUG -fPIC -flax-vector-conversions -fomit-frame-pointer -lgtest\r\n\r\nError:\r\n/usr/lib/gcc-cross/aarch64-linux-gnu/5/../../../../aarch64-linux-gnu/bin/ld: cannot find -lgtest\r\ncollect2: error: ld returned 1 exit status\r\n\r\nPlease give some information to overcome this issue.\r\n\r\nThanks in advance. ", "I'm also getting this flatbuffers::ClassicLocale::instance_ error.  This error comes up on both my Raspberry Pi & my ODroid N2 ARM64 boards.  I use the same build instructions as @thotaram for ARM64 & the pip wheel instructions for my Raspberry Pi: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package", "Hi @achandraa \r\nDid you resolve the below issue\r\nwhile.cc:(.text+0x1d04): undefined reference to flatbuffers::ClassicLocale::instance_' while.cc:(.text+0x1d18): undefined reference to flatbuffers::ClassicLocale::instance_'\r\nwhile.cc:(.text+0x1d6c): undefined reference to flatbuffers::ClassicLocale::instance_' while.cc:(.text+0x1d78): undefined reference to flatbuffers::ClassicLocale::instance_'\r\n\r\nCan you please help me how to resolve it.\r\n\r\nThanks.", "I had the same problem, solved it by adding\r\n/tensorflow/lite/tools/make/downloads/flatbuffers/src/idl_gen_cpp.cpp\r\n/tensorflow/lite/tools/make/downloads/flatbuffers/src/code_generators.cpp\r\n/tensorflow/lite/tools/make/downloads/flatbuffers/src/idl_gen_general.cpp\r\n/tensorflow/lite/tools/make/downloads/flatbuffers/src/idl_parser.cpp\r\n/tensorflow/lite/tools/make/downloads/flatbuffers/src/reflection.cpp\r\n/tensorflow/lite/tools/make/downloads/flatbuffers/src/util.cpp\r\nto my app sources", "@kn0t3k \r\nHi,\r\nThank you for your feedback.\r\nCan you please send the tensorflow source code path where to add these .cpp files.\r\n", "> \r\n> \r\n> @kn0t3k\r\n> Hi,\r\n> Thank you for your feedback.\r\n> Can you please send the tensorflow source code path where to add these .cpp files.\r\n\r\nIts not about tensorflow build, you have to add those file to your app CMakeLists.txt or whatever your using, they contain the neccesary symbols\r\n\r\nthe path is relative to tensorflow root dir", "@thotaram, my workaround is to install `libflatbuffers` on the system and apply this patch to tensorflow-lite to use the system `libflatbuffers`:\r\n```\r\n--- tensorflow-1.13.1/tensorflow/lite/tools/make/Makefile.orig  2019-06-04 13:13:08.329080620 +0200\r\n+++ tensorflow-1.13.1/tensorflow/lite/tools/make/Makefile       2019-06-04 16:05:13.325963284 +0200\r\n@@ -38,11 +38,12 @@ INCLUDES := \\\r\n -I$(OBJDIR)\r\n # This is at the end so any globally-installed frameworks like protobuf don't\r\n # override local versions in the source tree.\r\n-INCLUDES += -I/usr/local/include\r\n+INCLUDES += -I/usr/include\r\n \r\n # These are the default libraries needed, but they can be added to or\r\n # overridden by the platform-specific settings in target makefiles.\r\n LIBS := \\\r\n+-lflatbuffers \\\r\n -lstdc++ \\\r\n -lpthread \\\r\n -lm \\\r\n```", "@ggardet \r\nThank you for your prompt response.\r\n\r\nI have build the flatbuffers source code and copied the library files to /usr/lib.\r\nand included \r\n LIBS := \\\r\n+-lflatbuffers \\\r\n\r\nStill am getting the error: undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n\r\nI think if you cross build(ARM64) the tensorflow lite source code, Can you please help me or any document you have followed to build it.\r\n\r\n", "> Hi @achandraa\r\n> Did you resolve the below issue\r\n> while.cc:(.text+0x1d04): undefined reference to flatbuffers::ClassicLocale::instance_' while.cc:(.text+0x1d18): undefined reference to flatbuffers::ClassicLocale::instance_'\r\n> while.cc:(.text+0x1d6c): undefined reference to flatbuffers::ClassicLocale::instance_' while.cc:(.text+0x1d78): undefined reference to flatbuffers::ClassicLocale::instance_'\r\n> \r\n> Can you please help me how to resolve it.\r\n> \r\n> Thanks.\r\n\r\n\r\n@aselle \r\nI face with the same problem, please help", "@kn0t3k @ggardet \r\nHave the same issue. Could you please explain your workaround a bit more in detail? From your answer its not very clear how to resolve that issue.\r\nThanks in advance", "Hello @vizero1 \r\nPlease look for more detail of #29626. I think it can help you to resolve some problem.", "@TrungLM13 so you could only compile tf lite with version v1.12.2(with the workaround of disabling NNAPI)?", "> @TrungLM13 so you could only compile tf lite with version v1.12.2(with the workaround of disabling NNAPI)?\r\n\r\n@vizero1 I updated my anwer, v1.12.1\r\n", "Hello All,\r\nHas any one tried building the entire tensorflow source code instead of tensorflow lite?\r\nCould you please provide any web link or steps to cross compile entire tensorflow source code for ARM64.\r\nThanks in advance.\r\n", "I have instructions for building tensorflow v1.4 (not lite) for android, @thotaram:\r\nhttps://github.com/arnaldog12/Tensorflow-Build-MT/tree/master/v1.4/android\r\n\r\nI hope it helps.", "Hi All,\r\nTF lite is build successfully.\r\nWhile importing the tensorflow on board, am getting the below error.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named 'tensorflow'\r\n\r\nI would like to know how to import interpreter function?\r\ntf.lite.interpreter instead is there any way to use tflite interpreter directly? because \"import tensorflow\" is not working.\r\n\r\nAny one tried running label_image.py files on target board?\r\nThanks inadvance.", "@thotaram See https://www.tensorflow.org/lite/guide/python#run_an_inference_using_tflite_runtime", "@ nguyenhuy3588\r\nCould you please verify on the latest tf version and let us know if this is still an issue as many bugs have been fixed since this has been reported.\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28863\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28863\">No</a>\n"]}, {"number": 28862, "title": "[ROCm] Adding ROCm support for the matrix_triangular_solve op", "body": "This PR adds ROCm support for the matrix_triangular_solve op\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n------------------------\r\n\r\n@tatianashp , @whchung \r\n\r\n\r\n\r\n\r\n", "comments": []}, {"number": 28861, "title": "module 'tensorflow._api.v1.keras.layers' has no attribute 'Lamdba'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n`classifier_layer = layers.Lamdba(classifier_layer, input_shape = IMAGE_SIZE*[3]) \r\nclassifier_model = tf.keras.Sequential([classifier_layer]) classifier_mode.summary()`\r\n- OS Platform and \r\nDistribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro, 64bit, build number 17763.503\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7\r\n- Using Visual studios as a compiler\r\n- CUDA/cuDNN version: cant find my CUDA/cuDNN version.\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n**Describe the current behavior**\r\nGiving ma a attribute error.\r\n\r\n**Describe the expected behavior**\r\nNot getting the attribute error\r\n\r\n**Code to reproduce the issue**\r\n`classifier_layer = layers.Lamdba(classifier_layer, input_shape = IMAGE_SIZE*[3]) classifier_model = tf.keras.Sequential([classifier_layer]) classifier_mode.summary()`\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\njust compile it.\r\n\r\n**Other info / logs**\r\n`classifier_layer = layers.Lamdba(classifier_layer, input_shape = IMAGE_SIZE*[3]) \r\nclassifier_model = tf.keras.Sequential([classifier_layer]) classifier_mode.summary()`\r\nmodule 'tensorflow._api.v1.keras.layers' has no attribute 'Lamdba'\r\n", "comments": ["@Krum3L Code snippet you provided looks incomplete, In order to expedite the trouble-shooting process, please provide a minimal code snippet to reproduce the issue reported here. Thanks!", "Hello,\r\nI made a pastebin since i dont mind sharing the code (currently just learning and you can call it \"copy pasting\" the code forom the tensorflow tutorials)\r\nHeres the code: https://pastebin.com/8QaWkqr5", "@Krum3L Thanks for the code snippet. I am able to reproduce the issue reported here.  ", "@Krum3L I checked your code and found lot of typos that is why the code is not running properly. I found your are trying to learn this [tutorial](https://www.tensorflow.org/tutorials/images/hub_with_keras). I ran the code in the Google colab and it worked without any issue. \r\n\r\nWhen you run if you see any issue, please post it in [TF_Hub_repo]( https://github.com/tensorflow/hub/issues) so that your issue will get more visibility and faster resolution. Thanks!", "Hello,\r\nWell, its good to hear that its only typo's and notthing with the code itself... i wrote alot of that day and probably had alot of typos because of that...\r\nSorry for the trouble i have caused."]}, {"number": 28860, "title": "tf.data.Dataset::cache files are twice the size compared to original records", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1 / 1.14.1-dev / 2.0.0-nightly\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n\r\nFiles generated by `tf.data.Dataset::cache` consume considerably more disk space than the original TFRecord files. E.g. MNIST from `tensorflow_datasets` uses 21MB split into 10 files.  `tf.data.Dataset::cache` generates a single file with size 45MB.\r\nIs there a specific reason why `tf.data.Dataset::cache` files use a lot more disk space and don't split the files into chunks?\r\nThis might be acceptable for MNIST size datasets but when using ImageNet `tf.data.Dataset::cache` will create a single cache file with well above 350GB vs 144GB of original TFRecords.\r\n\r\n**Describe the expected behavior**\r\n`tf.data.Dataset::cache` should produce a similar file compared to the input TFRecord files.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf \r\nimport tensorflow_datasets as tfds \r\n\r\ntf.enable_eager_execution() # if version 1.x\r\n\r\ndata = tfds.load(\"mnist\", split=tfds.Split.TRAIN) \r\ncached = data.cache(\"cache/mnist\").shuffle(100).repeat().batch(128) \r\n \r\nfor feat in cached: \r\n    print(\"iterating\")\r\n```\r\n\r\n**Other info / logs**\r\nOriginal TFRecords:\r\n```console\r\n$ ll tensorflow_datasets/mnist/1.0.0/\r\ntotal 50008\r\ndrwxr-xr-x  15 lukasgeiger  staff   480B Mar 26 00:23 .\r\ndrwxr-xr-x   3 lukasgeiger  staff    96B Mar 26 00:23 ..\r\n-rw-r--r--@  1 lukasgeiger  staff   2.0K Mar 26 00:23 dataset_info.json\r\n-rw-r--r--   1 lukasgeiger  staff    48B Mar 26 00:23 image.image.json\r\n-rw-r--r--   1 lukasgeiger  staff   3.2M Mar 26 00:23 mnist-test.tfrecord-00000-of-00001\r\n-rw-r--r--   1 lukasgeiger  staff   1.9M Mar 26 00:23 mnist-train.tfrecord-00000-of-00010\r\n-rw-r--r--   1 lukasgeiger  staff   1.9M Mar 26 00:23 mnist-train.tfrecord-00001-of-00010\r\n-rw-r--r--   1 lukasgeiger  staff   1.9M Mar 26 00:23 mnist-train.tfrecord-00002-of-00010\r\n-rw-r--r--   1 lukasgeiger  staff   1.9M Mar 26 00:23 mnist-train.tfrecord-00003-of-00010\r\n-rw-r--r--   1 lukasgeiger  staff   1.9M Mar 26 00:23 mnist-train.tfrecord-00004-of-00010\r\n-rw-r--r--   1 lukasgeiger  staff   1.9M Mar 26 00:23 mnist-train.tfrecord-00005-of-00010\r\n-rw-r--r--   1 lukasgeiger  staff   1.9M Mar 26 00:23 mnist-train.tfrecord-00006-of-00010\r\n-rw-r--r--   1 lukasgeiger  staff   1.9M Mar 26 00:23 mnist-train.tfrecord-00007-of-00010\r\n-rw-r--r--   1 lukasgeiger  staff   1.9M Mar 26 00:23 mnist-train.tfrecord-00008-of-00010\r\n-rw-r--r--   1 lukasgeiger  staff   1.9M Mar 26 00:23 mnist-train.tfrecord-00009-of-00010\r\n```\r\nCached TFRecords:\r\n```console\r\n$ ll cache/\r\ntotal 100960\r\ndrwxr-xr-x    4 lukasgeiger  staff   128B May 20 09:45 .\r\ndrwxr-xr-x@ 113 lukasgeiger  staff   3.5K May 20 10:14 ..\r\n-rw-r--r--    1 lukasgeiger  staff    45M May 20 09:45 mnist.data-00000-of-00001\r\n-rw-r--r--    1 lukasgeiger  staff   3.2M May 20 09:45 mnist.index\r\n```", "comments": ["This is actually not a bug in `tf.data` but rather a limitation of TensorFlow Datasets. With TensorFlow Datasets version 1.1.0 this can be mitigated by skipping the internal decoding and caching the encoded version of the dataset. For more information see https://github.com/tensorflow/datasets/blob/master/docs/decode.md#filtershuffle-dataset-before-images-get-decoded", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28860\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28860\">No</a>\n"]}, {"number": 28859, "title": "TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(7, 7, 3, 64), dtype=float32) is not an element of this graph.", "body": "**### This is my code in python** \r\n\r\nfrom flask import Flask\r\nfrom flask_restful import Api, Resource, reqparse\r\n\r\napp = Flask(__name__)\r\napi = Api(app)\r\n\r\nimport os\r\nimport json\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.image as mpimg\r\nimport logging\r\nimport tensorflow as tf\r\nfrom keras import backend as K \r\n\r\nfrom collections import Counter\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nfrom keras.models import load_model\r\n\r\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img\r\nfrom scipy.misc import imresize\r\n\r\nfrom PIL import Image\r\n\r\n\r\nusers = [\r\n    {\r\n        \"name\": \"Nicholas\",\r\n        \"age\": 42,\r\n        \"occupation\": \"Network Engineer\",\r\n       \r\n    },\r\n    {\r\n        \"name\": \"Elvin\",\r\n        \"age\": 32,\r\n        \"occupation\": \"Doctor\"\r\n       \r\n    },\r\n    {\r\n        \"name\": \"Jass\",\r\n        \"age\": 22,\r\n        \"occupation\": \"Web Developer\"\r\n      \r\n    }\r\n]\r\n\r\nclass User(Resource):\r\n    def get(self, name):\r\n        for user in users:\r\n            if(name == user[\"name\"]):\r\n                return user, 200\r\n        return \"User not found\", 404\r\n\r\n    def post(self, name):\r\n        K.clear_session()\r\n        parser = reqparse.RequestParser()\r\n        parser.add_argument(\"image\")\r\n        #get the image\r\n        model = load_model('C:/Users/ASUS/Downloads/mse-04-0.0877.h5')\r\n        img_height, img_width, channels = 350, 350, 3\r\n\r\n        img = load_img('C:/xampp/htdocs/faceD/image/logo.png')\r\n\r\n        img = imresize(img, size=(img_height, img_width))\t  \r\n        test_x = img_to_array(img).reshape(img_height, img_width, channels) \r\n        test_x = test_x / 255.\r\n        test_x = test_x.reshape((1,) + test_x.shape)\r\n        predicted = model.predict(test_x)\r\n\r\n        return{\r\n            \"name\": \"Nicholas\",\r\n            \"age\": 42,\r\n            \"occupation\": \"Network Engineer\",\r\n            \"prediction\": str(predicted[0][0])\r\n\r\n    }, 201\r\n\r\n\r\n    def put(self, name):\r\n        parser = reqparse.RequestParser()\r\n        parser.add_argument(\"age\")\r\n        parser.add_argument(\"occupation\")\r\n        args = parser.parse_args()\r\n\r\n        for user in users:\r\n            if(name == user[\"name\"]):\r\n                user[\"age\"] = args[\"age\"]\r\n                user[\"occupation\"] = args[\"occupation\"]\r\n                user[\"prediction\"] = args[\"str(predicted[0][0])\"]\r\n                return user, 200\r\n        \r\n        user = {\r\n            \"name\": name,\r\n            \"age\": args[\"age\"],\r\n            \"occupation\": args[\"occupation\"],\r\n            \"prediction\": args[\"str(predicted[0][0])\"]\r\n        }\r\n        users.append(user)\r\n        return user, 201\r\n\r\n    def delete(self, name):\r\n        global users\r\n        users = [user for user in users if user[\"name\"] != name]\r\n        return \"{} is deleted.\".format(name), 200\r\n      \r\napi.add_resource(User, \"/user/<string:name>\")\r\n\r\napp.run(debug=True)\r\n\r\n\r\n\r\n**### _All the error_**\r\n\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2309, in __call__\r\n    return self.wsgi_app(environ, start_response)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2295, in wsgi_app\r\n    response = self.handle_exception(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 269, in error_router\r\n    return original_handler(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1741, in handle_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 34, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2292, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1815, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 269, in error_router\r\n    return original_handler(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1718, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 34, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1813, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 458, in wrapper\r\n    resp = resource(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\views.py\", line 88, in view\r\n    return self.dispatch_request(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 573, in dispatch_request\r\n    resp = meth(*args, **kwargs)\r\n  File \"C:\\xampp\\htdocs\\faceD\\app.py\", line 61, in post\r\n    model = load_model('C:/Users/ASUS/Downloads/mse-04-0.0877.h5')\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\", line 419, in load_model\r\n    model = _deserialize_model(f, custom_objects, compile)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\", line 287, in _deserialize_model\r\n    K.batch_set_value(weight_value_tuples)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2470, in batch_set_value\r\n    get_session().run(assign_ops, feed_dict=feed_dict)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1095, in _run\r\n    'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\r\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(7, 7, 3, 64), dtype=float32) is not an element of this graph.\r\n127.0.0.1 - - [20/May/2019 21:21:32] \"\u001b[1m\u001b[35mPOST /user/Nicholas HTTP/1.1\u001b[0m\" 500 -\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2309, in __call__\r\n    return self.wsgi_app(environ, start_response)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2295, in wsgi_app\r\n    response = self.handle_exception(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 269, in error_router\r\n    return original_handler(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1741, in handle_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 34, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2292, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1815, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 269, in error_router\r\n    return original_handler(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1718, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 34, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1813, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 458, in wrapper\r\n    resp = resource(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\views.py\", line 88, in view\r\n    return self.dispatch_request(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 573, in dispatch_request\r\n    resp = meth(*args, **kwargs)\r\n  File \"C:\\xampp\\htdocs\\faceD\\app.py\", line 70, in post\r\n    predicted = model.predict(test_x)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1169, in predict\r\n    steps=steps)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\", line 294, in predict_loop\r\n    batch_outs = f(ins_batch)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2715, in __call__\r\n    return self._call(inputs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2675, in _call\r\n    fetched = self._callable_fn(*array_vals)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1439, in __call__\r\n    run_metadata_ptr)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value dense_1/bias\r\n         [[{{node dense_1/bias/read}}]]\r\n127.0.0.1 - - [20/May/2019 21:21:33] \"\u001b[1m\u001b[35mPOST /user/Nicholas HTTP/1.1\u001b[0m\" 500 -\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2309, in __call__\r\n    return self.wsgi_app(environ, start_response)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2295, in wsgi_app\r\n    response = self.handle_exception(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 269, in error_router\r\n    return original_handler(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1741, in handle_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 34, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2292, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1815, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 269, in error_router\r\n    return original_handler(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1718, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 34, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1813, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 458, in wrapper\r\n    resp = resource(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\views.py\", line 88, in view\r\n    return self.dispatch_request(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 573, in dispatch_request\r\n    resp = meth(*args, **kwargs)\r\n  File \"C:\\xampp\\htdocs\\faceD\\app.py\", line 61, in post\r\n    model = load_model('C:/Users/ASUS/Downloads/mse-04-0.0877.h5')\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\", line 419, in load_model\r\n    model = _deserialize_model(f, custom_objects, compile)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\", line 287, in _deserialize_model\r\n    K.batch_set_value(weight_value_tuples)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2470, in batch_set_value\r\n    get_session().run(assign_ops, feed_dict=feed_dict)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1095, in _run\r\n    'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\r\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(7, 7, 3, 64), dtype=float32) is not an element of this graph.", "comments": ["@yensan0512 Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "@yensan0512 Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28858, "title": "How to set up TensorFlow in GPU machine only using CPU By Java code\uff1f", "body": "\r\n\r\n\r\ntensorflow version: 1.8.0 Java\r\n\r\n```\r\n        System.out.println(\"setting config\");\r\n        ConfigProto.Builder configBuilder=ConfigProto.newBuilder()\r\n                .setAllowSoftPlacement(true)\r\n                .setLogDevicePlacement(true)\r\n                .clearLogDevicePlacement()\r\n                //.setInterOpParallelismThreads(1)\r\n        ;\r\n        ConfigProto config;\r\n        string visibleDevices=\"-1\";\r\n\r\n        config=configBuilder.setGpuOptions(GPUOptions.newBuilder()\r\n                .clearVisibleDeviceList()\r\n                .setVisibleDeviceList(visibleDevices)\r\n                .setAllowGrowth(true)\r\n        )\r\n        .build();\r\n        System.out.println(\"setting session\");\r\n        this.sess=new Session( this.graph, config.toByteArray() );\r\n```\r\nthis visibleDevices=\"-1\" throw a error:\r\n```\r\n2019-05-20 20:01:08.472129: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n2019-05-20 20:01:10.441081: E tensorflow/core/common_runtime/direct_session.cc:154] Invalid argument: 'visible_device_list' listed an invalid GPU id '-1' but visible device count is 3\r\nException in thread \"main\" org.tensorflow.TensorFlowException: Failed to create session.\r\n        at org.tensorflow.Session.allocate2(Native Method)\r\n        at org.tensorflow.Session.<init>(Session.java:70)\r\n```\r\n\r\n\r\nthis visibleDevices=\"0\" Indicates that the 0th GPU is used, but my GPU is full and I want to use the CPU to make predictions.\r\n\r\n", "comments": ["I had solved the problem. thank you.", "Glad it resolved. Thanks!", "CUDA_VISIBLE_DEVICES\r\nmodify the environment variables of the current program through Java built-in.\r\n\r\nc++  import <stdlib.h> setenv(\u201c{Name}={Value}\u201d)\r\n\r\nsetenv can modify the environment variables of the current program.\r\n"]}, {"number": 28857, "title": "How can I use same tf.Session() in another function as opened session??", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there and can provide suggestion quickly for such issues. Thanks!\r\n"]}, {"number": 28856, "title": "TFLiteConverterV2 has no attribute 'from_saved_model'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 16.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): **v1.12.0-9492-g2c319fb415 2.0.0-alpha0**\r\n- Python version: **3.6**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: **9.0/7.1.4**\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen running `tf.lite.TFLiteConverter.from_saved_model(saved_model_path)`, an error occurs as described in the title.\r\n\r\n**Describe the expected behavior**\r\nThe TF lite converter should work properly.\r\n\r\n**Code to reproduce the issue**\r\nJust follow the sample code in [here](https://www.tensorflow.org/lite/r2/convert/python_api)\r\n```\r\nimport tensorflow as tf\r\nroot = tf.train.Checkpoint()\r\nroot.v1 = tf.Variable(3.)\r\nroot.v2 = tf.Variable(2.)\r\nroot.f = tf.function(lambda x: root.v1 * root.v2 * x)\r\n\r\nexport_dir = '/tmp/test_saved_model'\r\ninput_data = tf.constant(1., shape=[1, 1])\r\nto_save = root.f.get_concrete_function(input_data)\r\ntf.saved_model.save(root, export_dir, to_save)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\r\ntflite_model = converter.convert()\r\n```", "comments": ["`from_saved_model` wasn't added until after the alpha was released. Details are available in the comment here: https://www.tensorflow.org/lite/r2/convert/python_api. Can you upgrade to `tf-nightly-2.0-preview` and rerun your code?", "@gargn I originally compiled tf-2.0 from source and had the same issue. Then I switched to tf-1.12. Here is the version of my tf-2.0: `v2.0.0-alpha0-4-g2c2d508 2.0.0-alpha0`. ", "I installed last night's `tf-nightly-2.0-preview` using `pip install tf-nightly-2.0-preview` (specifically `2.0.0.dev20190529`) and ran the code you provided without error.\r\n\r\nThe 2.0 alpha branch you mentioned was cut in early February and has a slightly different API for `from_saved_model` as the documentation indicates.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28856\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28856\">No</a>\n"]}, {"number": 28855, "title": "Low performance and detection on Google Coral DevBoard", "body": "<em>Just started working on google coral devboard. As I seen in blogs and official doc I need to convert FLOAT 32 bit neural network to QUANTIZED_UNIT8 i.e. 8bit neural network. </em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 19.04):\r\n- Google Dev Board:\r\n- TensorFlow installed from (source or binary):\r\n- Edge TPU\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nAs I run the official demo of the google dev board getting very low accuracy while fps is quite good, But the accuracy is no furthur than 50-60 %. \r\nThe model is official and included into dev board itself. \r\nCan any one have any idea to improve accuracy or it will be same because of 8 bit neural network.", "comments": ["@rahuladream Just to verify, Did you follow this [link](https://coral.withgoogle.com/docs/dev-board/get-started/). ?. Thanks!", "> @rahuladream Just to verify, Did you follow this [link](https://coral.withgoogle.com/docs/dev-board/get-started/). ?. Thanks!\r\n\r\nYes, I did \r\nThe official one also have low accuracy", "I haven't worked on this area, but I've pinged the Coral team to ask for the right owner.", "@rahuladream can you please send your query at coral-support@google.com so that your question is visible to larger audience and we would be able to help you appropriately?", "@rahuladream Did you get a chance to look at @manoj7410's comment. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28855\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28855\">No</a>\n"]}, {"number": 28854, "title": "Error loading a TensorRT optimised graph", "body": "I was able to convert a frozen model using the tensorRT API on a Nvidia Tesla P100 on Debian 9 using the command\r\n\r\n```\r\ntrt_graph = trt.create_inference_graph(\r\n    input_graph_def=saved_graph,\r\n    outputs=output_names[0:1],\r\n    max_batch_size=1,\r\n    max_workspace_size_bytes=5000000000,\r\n    precision_mode='FP16',\r\n    is_dynamic_op=True\r\n)\r\n```\r\n\r\nI am able to load the graph on the same system. However, when I try to load the graph on my local system which has an Nvidia GeForce GTX 1050M I get the following error.\r\n\r\n```\r\n  File \"/home/fuzzybatman/.local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/fuzzybatman/.local/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 426, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'TRTEngineOp' in binary running on ceph. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n```\r\n\r\nIs it because my GPU lacks support for TensorRT?", "comments": ["Just to verify did you get chance to have a look on #22360. Which TensorFlow version you are using?", "Thank you for the response. I am using TF r.1.13\r\nI am loading it in python and I did try adding `import tensorflow.contrib.tensorrt`\r\n\r\nOn a side note, I posted [this](https://devtalk.nvidia.com/default/topic/1052148/tensorrt/gpu-requirements-to-run-create_inference_graph-using-tensorrt-trt-in-tensorflow-/post/5340837/#5340837) question on nvidia devtalk and they answered\r\n\r\n#### A generated TensorRT PLAN is valid for a specific GPU \u2014 more precisely, a specific CUDA Compute Capability. For example, if you generate a PLAN for an NVIDIA P4 (compute capability 6.1) you can\u2019t use that PLAN on an NVIDIA Tesla V100 (compute capability 7.0).\r\n\r\nThis is quite confusing because there are articles online on optimising on one GPU and running on another.", "Just noticed that there is a TF version mismatch between the one on my system (1.13) and the one on the GCP VM (1.12). Does this affect the result?", "Tried again with a new model. Same error.", "Which CUDA/cuDNN version you are using ?", "On my local system it is CUDA 10.1 and cuDNN 7.4.2\r\nOn the VM it is CUDA 10.0 and cuDNN 7.4.1", "Please help us with some more info as in whether you are getting this error on TensorFlow installed on your GCP VM or on local system. Which operating system you are using and whether you have installed TensorFlow from source or binary. If you are unclear about the template, you can refer this [link](https://github.com/tensorflow/tensorflow/issues/new/choose). Also kindly verify whether you have followed the instruction from [TensorFlow website](https://www.tensorflow.org/install/source) based on information provided in the template. Thanks!  ", "Hi. I run the `create_inference_graph` method on the VM \r\n- Debian 9\r\n- CUDA 10.0\r\n- cuDNN 7.4.1\r\n- TF 1.13\r\n- Nvidia Tesla P100 [16GB] (compute capability 6.0)\r\n\r\nI try loading the graph for inference on the VM and it works fine.\r\n\r\nI try loading the graph on my local system\r\n- Fedora 30\r\n- CUDA 10.1\r\n- cuDNN 7.4.2\r\n- TF 1.13\r\n- Nvidia GeForce GTX1050M [4GB] (compute capability 6.1)\r\n\r\nand get the error\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'TRTEngineOp' in binary running on ceph. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n```\r\n\r\nI did not build TF from source. I installed it using pip3 in the terminal.\r\n\r\n`pip3 install tensorflow-gpu --user`\r\n\r\nAccording to a moderator on the nvidia devtalk forum\r\n\r\n#### A generated TensorRT PLAN is valid for a specific GPU \u2014 more precisely, a specific CUDA Compute Capability. For example, if you generate a PLAN for an NVIDIA P4 (compute capability 6.1) you can\u2019t use that PLAN on an NVIDIA Tesla V100 (compute capability 7.0).", "Hi @fuzzyBatman could you try adding:\r\n```\r\nfrom tensorflow.contrib.tensorrt.python.ops import trt_engine_op\r\n```\r\nto your loading script to see if it works?\r\nThanks.", "@aaroey Same error.\r\n\r\nDoes the GPU choice not affect this?", "@fuzzyBatman could you share your full script, I'll try and let you know.", "I have a TF frozen graph (.pb extension). I load it and run the `create_inference_graph` method on the GCP VM which has an Nvidia Tesla P100 (16 GB) GPU.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import graph_io\r\nfrom tensorflow.contrib import tensorrt as trt\r\n\r\ndef get_frozen_graph(graph_file):\r\n    \"\"\"Read Frozen Graph file from disk.\"\"\"\r\n\r\n    with tf.gfile.GFile(graph_file, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    tf.import_graph_def(graph_def, name='')\r\n    return graph_def\r\n\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.6)))\r\nsess.run(tf.global_variables_initializer())\r\n\r\nsaved_graph = get_frozen_graph(pbfile)\r\n\r\n# Comment the following when loading the TensorRT graph.\r\nprint('Creating trt inference graph')\r\n\r\ntrt_graph = trt.create_inference_graph(\r\n    input_graph_def=saved_graph,\r\n    outputs=output_names[0:1],\r\n    max_batch_size=1,\r\n    max_workspace_size_bytes=4000000000,\r\n    precision_mode='FP16',\r\n    minimum_segment_size=2\r\n)\r\n\r\ngraph_io.write_graph(trt_graph, \"./train_log/faster_rcnn_fpn/\", \"frcnn_trt.pb\", as_text=False)\r\n```\r\n\r\nThe above script provides the file `frcnn_trt.pb`. Now I use the same `get_frozen_graph` procedure as above, using _frcnn_trt.pb_, with rest of the code commented out. This works on the same VM but fails on my local system that has an Nvidia GeForce GTX1050M (4 GB) GPU.", "@fuzzyBatman sorry I was not able to get to this. Thanks for the scripts, it looks legit to me. By `This works on the same VM` did you mean that in your VM you can run the TRT converted graph `frcnn_trt.pb`? By `but fails on my local system` did you mean it failed with `TRTEngineOp not found` error? I can imagine it'll fail with some error because TRT engines are not portable, meaning you'd better run the converted graph on a machine that has the same type of GPU as the one on which you  ran the conversion.\r\n\r\nAlso, 1.15.0rc1 is out and 1.15.0 will be out soon, you may want to try with that. Also feel free to provide the `pbfile` and I'll try your script with that. Thanks.", "@fuzzyBatman We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. we will get you the right help.Thanks!", "Hi! I stopped working on that project a year ago", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28854\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28854\">No</a>\n"]}, {"number": 28853, "title": "Handle multiple channel audio files", "body": "Addresses [#22838](https://github.com/tensorflow/tensorflow/issues/22838).\r\n\r\nThe old code only works for single channel audio files. For a multiple channel audio file, there will be problems described in the issue.\r\n\r\nThis PR add handling for multiple channel audio files. Now the code works for both single and multiple channel audio files. But there is an open question in the implementation that need to be discussed: for a multiple channel audio file, it only export the first channel as a png file. I don't know if this is the behavior we want. Do we want to export each channel as a png file, or export all channel data as a single png file. Thanks!", "comments": ["@petewarden  Hi, Can you please review this PR ?", "Can you add a test that fails before this PR is added, and is fixed by it, in tensorflow/core/kernels/spectrogram_test.cc? That would help document this functionality.\r\n\r\nI'm happy to have just the first channel be written out to the PNG, since that's only for debugging purposes.", "Can one of the admins verify this patch?", "@petewarden Can you please take a look on this PR? Thanks!", "@petewarden Thanks for the comments, I will add a test case soon", "@petewarden can you help to create the expected output file for the attached wav file? The wav file has two channels, and is about 1s of music audio.\r\n\r\nTo add a new test case, I need to add a new wav file whose channel is 2.\r\n\r\nIn the README(tensorflow/core/kernels/spectrogram_test_data/README), it says:\r\nThe CSV spectrogram files in this directory are generated from the\r\nmatlab code in ./matlab/GenerateTestData.m\r\nTo save space in the repo, you'll then need to convert them into a binary packed\r\nformat using the convert_test_data.cc command line tool.\r\n\r\nSo I think I can't do it myself.\r\n\r\n[short_test_segment_2.wav.zip](https://github.com/tensorflow/tensorflow/files/3587535/short_test_segment_2.wav.zip)\r\n\r\n", "> Format using the convert_test_data.cc command line tool.  So I think I can't do it myself.\r\n\r\nThe tool code is here: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/spectrogram_convert_test_data.cc\r\n\r\nThis build rule creating it:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/7829641a2a3083b091782e306c476de7cbf7bc25/tensorflow/core/kernels/BUILD#L5811\r\n\r\nSo if you're setup to build tensorflow from source this command might do it:\r\n\r\n```\r\nbazel run //tensorflow/core/kernels:spectrogram_convert_test_data -- <args>\r\n```\r\n\r\nThere's also this file group you probably need to add it too:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/7829641a2a3083b091782e306c476de7cbf7bc25/tensorflow/core/kernels/BUILD#L5773", "> The CSV spectrogram files in this directory are generated from the\r\n> matlab code in ./matlab/GenerateTestData.m\r\n\r\nThe problem is that I don't have the matlab tool that converts a wav file to a csv file, which is then fed to the 'convert_test_data.cc' tool.", "This is still missing tests", "@astropeak Can you please check mihaimaruseac's comments and keep us posted. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I won't be able to work on this PR anymore. As said before, to add the test case, a matlab tool is required to generate the reference file. But that matlab tool seems an internal tool and is unavailable.\r\nBut anyhow I don't have time to work on this. Anyone interested in could continue working on this. If no one will work on this, maybe closing the PR will be an option."]}, {"number": 28852, "title": "after quantization aware training, add operation of resdural block lack min/max value, why? anyone can help?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.12/1.13\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.2\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nafter quantization aware training, when convert to tflite, Array slim_mobilenetv2/Add, which is an input to the Conv operator producing the output array slim_mobilenetv2/Conv_6/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nFatal Python error: Aborted\r\n\r\n**Describe the expected behavior**\r\ncan't convert to tflite?There should have min/max value?\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["anyone can help?", "The main reason is FakeQuant() ops are not placed everywhere needed. It was a known limitation with the old quantization rewriter and as a result we're working on a new keras based API. Stay tuned!\r\n\r\nMeanwhile, please take a look at post-training integer quantization:\r\nhttps://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba\r\n\r\nFeel free to open other issues if you run into problem with it. Thanks!"]}, {"number": 28851, "title": "Keras Custom Conv2D layer does not work (`None` gradient issue)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\n```python\r\nclass CustomConv2D(Conv2D):\r\n    def __init__(self,\r\n                 filters,\r\n                 kernel_size,\r\n                 strides=(1, 1),\r\n                 padding='valid',\r\n                 dilation_rate=(1, 1),\r\n                 activation=None,\r\n                 use_bias=True,\r\n                 kernel_initializer=tf.keras.initializers.TruncatedNormal(0.0, 0.01),\r\n                 bias_initializer='zeros',\r\n                 kernel_regularizer=None,\r\n                 bias_regularizer=None,\r\n                 activity_regularizer=None,\r\n                 kernel_constraint=None,\r\n                 bias_constraint=None,\r\n                 **kwargs):\r\n        \r\n        self.pad_type = padding.lower()\r\n        super(CustomConv2D, self).__init__(filters,\r\n                                           kernel_size,\r\n                                           strides,\r\n                                           self.pad_type if self.pad_type in ['valid', 'same'] else 'valid',\r\n                                           'channels_last',\r\n                                           dilation_rate,\r\n                                           activation,\r\n                                           use_bias,\r\n                                           kernel_initializer,\r\n                                           bias_initializer,\r\n                                           activity_regularizer,\r\n                                           kernel_constraint,\r\n                                           bias_constraint,\r\n                                           **kwargs)\r\n    \r\n    def call(self, input):\r\n        if self.pad_type in ['symmetric', 'reflect']:\r\n            input_rows = tf.shape(input)[2]\r\n            filter_rows = self.kernel_size[0]\r\n            out_rows = (input_rows + self.strides[0] - 1) // self.strides[0]\r\n            padding_rows = tf.maximum(0, (out_rows - 1) * self.strides[0] +\r\n                                      (filter_rows - 1) * self.dilation_rate[0] + 1 - input_rows)\r\n            rows_odd = tf.mod(padding_rows, 2)\r\n\r\n            input_cols = tf.shape(input)[3]\r\n            filter_cols = self.kernel_size[1]\r\n            out_cols = (input_cols + self.strides[1] - 1) // self.strides[1]\r\n            padding_cols = tf.maximum(0, (out_cols - 1) * self.strides[1] +\r\n                                      (filter_cols - 1) * self.dilation_rate[1] + 1 - input_cols)\r\n            cols_odd = tf.mod(padding_cols, 2)\r\n\r\n            output = tf.pad(input, [[0, 0], [padding_rows // 2, padding_rows // 2 + rows_odd],\r\n                            [padding_cols // 2, padding_cols // 2 + cols_odd], [0, 0]], mode=self.pad_type)\r\n\r\n            return K.conv2d(output,\r\n                            self.kernel,\r\n                            strides=self.strides,\r\n                            padding='valid',\r\n                            data_format=self.data_format,\r\n                            dilation_rate=self.dilation_rate)\r\n\r\n        elif self.pad_type in ['same', 'valid']:\r\n            return K.conv2d(input,\r\n                            self.kernel,\r\n                            strides=self.strides,\r\n                            padding=self.padding,\r\n                            data_format=self.data_format,\r\n                            dilation_rate=self.dilation_rate)\r\n```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  - Windows 10 & Linux Ubuntu 16.04 LTS\r\n- TensorFlow installed from (source or binary):\r\n  - pip install tensorflow (cpu version)\r\n- TensorFlow version (use command below):\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): Not related\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Not related\r\n- GPU model and memory: Not related\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nMy custom conv2d layer shows the following error message.\r\n```bash\r\nValueError: An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.\r\n```\r\nI'm not using any common ops (K.argmax, Kround, etc.) explicitly in \r\n```python\r\nelif self.pad_type in ['same', 'valid']:\r\n            return K.conv2d(input,\r\n                            self.kernel,\r\n                            strides=self.strides,\r\n                            padding=self.padding,\r\n                            data_format=self.data_format,\r\n                            dilation_rate=self.dilation_rate)\r\n```\r\nWhat am I missing?\r\n\r\n**Describe the expected behavior**\r\nJust working\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\ninputs = Input(shape=(None, None, 1))\r\nx = inputs\r\nx = CustomConv2D(filters=1, kernel_size=(k, k), padding='same')(x)\r\nmodel = Model(inputs=inputs, outputs=x)\r\noptimizer = Adam(config.learning_rate, config.momentum)\r\nmodel.compile(loss='mean_squared_error',\r\n                         optimizer=optimizer)\r\nmodel.fit_generator(batch_gen, epochs=100, steps_per_epoch=10)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Will it be possible to provide us the minimal code snippet that can give us the error you are facing. Let us also know which TensorFlow version you are using. Thanks!", "Closing the issue due to lack of recent activities. Please feel free to come back in case you are still stuck. Thanks!"]}, {"number": 28850, "title": "Handle checking equality of unknown Tensorshapes", "body": "Hi,\r\n\r\nThis PR adds code for handling unknown TensorShape when checking if one is equal to the other. \r\n\r\nReference: `Not Equal` condition has this check but `equal to` is missing it. I assume that we are not handling unknown Tensorshape based on how it is being handled in 'Not Equal to`\r\n\r\nLines added:\r\n```\r\n    if self.rank is None or other.rank is None:\r\n      raise ValueError(\"The equality of unknown TensorShapes is undefined.\")\r\n```\r\n", "comments": ["Thanks for the review. Closing the issue."]}, {"number": 28849, "title": "Python3 type annotation does not work with @tf.function + for loop -> tf.while_loop conversion", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0 alpha\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nIf you use python3 type annotation such as x:tf.Tensor = tf.constant(0) (I aliased tf.Tensor for various shape to keep my sanity for reinforcement learning problems) in a @tf.function and the function contains a for loop to be translated to tf.while_loop (that doesn't even have to use the tensor that's annotated), the code will fail as if you did not turn on eager execution.\r\n\r\n**Describe the expected behavior**\r\nPython3 type hinting should not fail the code.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n@tf.function\r\ndef tf_for_tf_break():\r\n    x: tf.Tensor = tf.constant(0)\r\n    for i in tf.range(5):\r\n        x += i\r\n    return x\r\n\r\nprint(tf_for_tf_break())\r\n```\r\n\r\n**Other info / logs**\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0519 21:35:32.992043 140297958307648 tf_logging.py:161] Entity <function tf_for_tf_break at 0x7f99a9edde18> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: AttributeError during conversion: 'NoneType' object has no attribute '_fields'\r\nTraceback (most recent call last):\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 393, in function_to_graph\r\n    node = node_to_graph(node, context)\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 436, in node_to_graph\r\n    node = converter.standard_analysis(node, context, is_initial=True)\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/core/converter.py\", line 493, in standard_analysis\r\n    graphs = cfg.build(node)\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/cfg.py\", line 813, in build\r\n    visitor.visit(node)\r\n  File \"/usr/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/cfg.py\", line 672, in visit_FunctionDef\r\n    self.visit(stmt)\r\n  File \"/usr/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/usr/lib/python3.6/ast.py\", line 257, in generic_visit\r\n    for field, value in iter_fields(node):\r\n  File \"/usr/lib/python3.6/ast.py\", line 171, in iter_fields\r\n    for field in node._fields:\r\nAttributeError: 'NoneType' object has no attribute '_fields'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 369, in converted_call\r\n    experimental_partial_types=partial_types)\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 513, in to_graph\r\n    arg_values, arg_types)\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 190, in entity_to_graph\r\n    node, name, ns = function_to_graph(o, program_ctx, arg_values, arg_types)\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 396, in function_to_graph\r\n    raise errors.InternalError('conversion', e)\r\ntensorflow.python.autograph.pyct.errors.InternalError: AttributeError during conversion: 'NoneType' object has no attribute '_fields'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jackshi/MagneticAccelerator/descrete_optimization/tf_scratch.py\", line 12, in <module>\r\n    print(tf_for_tf_break())\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 426, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 370, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1313, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1580, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1512, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 694, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 317, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 686, in wrapper\r\n    ), args, kwargs)\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 390, in converted_call\r\n    return _call_unconverted(f, args, kwargs)\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 188, in _call_unconverted\r\n    return f(*args, **kwargs)\r\n  File \"/home/jackshi/MagneticAccelerator/descrete_optimization/tf_scratch.py\", line 7, in tf_for_tf_break\r\n    for i in tf.range(5):\r\n  File \"/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 449, in __iter__\r\n    \"Tensor objects are only iterable when eager execution is \"\r\nTypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.\r\n", "comments": ["I am able to reproduce the issue on colab with TF 2.0alpha . This is the error I got TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28849\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28849\">No</a>\n"]}, {"number": 28848, "title": "ImportError: DLL load failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.5.1\r\n- GPU model and memory:RTX2070\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm installing tf from source using bazel and I'm trying to use CUDA, when I build pip package by using: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \r\n\r\n**Any other info / logs**\r\n\r\nERROR: E:/tensorflow/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/dylan/_bazel_dylan/4ejpfwyr/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin\r\n    SET PYTHON_BIN_PATH=C:/Users/dylan/AppData/Local/Programs/Python/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/dylan/AppData/Local/Programs/Python/Python36/lib/site-packages\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  C:/msys64/usr/bin/bash.exe bazel-out/x64_windows-opt/genfiles/tensorflow/tf_python_api_gen_v1.genrule_script.sh\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\dylan\\AppData\\Local\\Temp\\Bazel.runfiles_gllyv2tm\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\dylan\\AppData\\Local\\Temp\\Bazel.runfiles_gllyv2tm\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\dylan\\AppData\\Local\\Temp\\Bazel.runfiles_gllyv2tm\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\dylan\\AppData\\Local\\Temp\\Bazel.runfiles_gllyv2tm\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\dylan\\AppData\\Local\\Temp\\Bazel.runfiles_gllyv2tm\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"\\\\?\\C:\\Users\\dylan\\AppData\\Local\\Temp\\Bazel.runfiles_gllyv2tm\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\dylan\\AppData\\Local\\Temp\\Bazel.runfiles_gllyv2tm\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\dylan\\AppData\\Local\\Temp\\Bazel.runfiles_gllyv2tm\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\dylan\\AppData\\Local\\Temp\\Bazel.runfiles_gllyv2tm\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2207.386s, Critical Path: 378.51s\r\nINFO: 4635 processes: 4635 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Does it look like set path issue?", "Did you follow the instruction from [TensorFlow website](https://www.tensorflow.org/install/source_windows). You can have a look on similar issue #22794 and #10033 . let us know if that helps. Looks like you need to add path for CUDA and CuDNN. ", "> Did you follow the instruction from [TensorFlow website](https://www.tensorflow.org/install/source_windows). You can have a look on similar issue #22794 and #10033 . let us know if that helps. Looks like you need to add path for CUDA and CuDNN.\r\n\r\nHi, thanks for reply. I have tried multiple solutions from previous similar issues, but it still gives the same error. And their error doesn't display the list of SET operations as well. The only difference I can see is i'm using cuda 10.0, but rtx 2070 requires cuda 10 i think. I have set all required path in system environment variable:\r\nPath = c:\\bazel;c:\\msys64\\usr\\bin;C:\\tools\\cuda\\bin;\r\nC:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\;C:\\Users\\dylan\\AppData\\Local\\Programs\\Python\\Python36;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\extras\\CUPTI\\libx64;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\r\n\r\nBAZEL_SH = c:\\msys64\\usr\\bin\\bash.exe\r\nBAZEL_VC = C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\r\nCUDA_PATH = C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nCUDA_PATH_V10_0 = C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\nJAVA_HOME = C:\\Program Files\\Java\\jdk1.8.0_201\r\n\r\n", "I have successfully installed tensorflow with gpu support through anaconda. But I don't know what's wrong with my build from source approach still. But I can see anaconda installs cuda 10.0.1 and cudnn 7.5 with tensorflow 1.13.1", "Just to verify did you follow exactly the same instructions from [TensorFlow website](https://www.tensorflow.org/install/source_windows) (with GPU support). Thanks!", "> Just to verify did you follow exactly the same instructions from [TensorFlow website](https://www.tensorflow.org/install/source_windows) (with GPU support). Thanks!\r\n\r\nYes, i did it exactly the same way. And used the same version of package if mentioned. But there are some packages version not specified, they might be the issue then. ", "Open ...\\Lib\\site-packages\\tensorflow\\python\\\\_pywrap_tensorflow_internal.pyd use [Dependency Walker](http://www.dependencywalker.com/), it will show you the DLL dependency tree, you will find which DLL cause the problem. You can then install the needed CUDA or vcredist version, or add some path to system variable.\r\n\r\nHere is the dependency tree look like:\r\n![image](https://user-images.githubusercontent.com/10413780/58224686-43831d80-7d51-11e9-9610-9fa5820ffa7e.png)", "@tianjiaq : Give a try to fengyoulin's suggestion and let us know if that does not help. Thanks!", "@tianjiaq. I am working on TF 1.13.1 and got same issue during the build from source.\r\nHow did you solve the problem?   thank you.", "> @tianjiaq. I am working on TF 1.13.1 and got same issue during the build from source.\r\n> How did you solve the problem? thank you.\r\n\r\nI installed with Anaconda and get install versions cuda 10.0.1 and cudnn 7.5. You can try these versions first, if not then you can try fengyoulin's solution. Please let me know if it works!", "I have solved problem. My issue is nvcuda.dll missing during the cuda installation. \r\nThen I got nvcuda.dll and the related files from others. \r\nThanks @tianjiaq and @fengyoulin.", "Got same problem,\r\nAs [Tensorflow website\r\n![dll](https://user-images.githubusercontent.com/33541116/71784822-fc1b7a00-301f-11ea-80f4-d9dc484e7872.PNG)\r\n](https://www.tensorflow.org/install/pip?lang=python3)\r\nI did installed Microsoft Visual c++ 2015 Redistributable but again not working,\r\n\r\n(base) C:\\Users\\SAMAN>python\r\n**Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] :: Ana\r\nconda, Inc. on win32**\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap\r\n_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap\r\n_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap\r\n_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code 3221225501\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line\r\n 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\",\r\n line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line\r\n 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line\r\n 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import\r\n_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init\r\n__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap\r\n_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap\r\n_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap\r\n_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap\r\n_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code 3221225501\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>"]}, {"number": 28847, "title": "how to use tf.contrib.image.rotate in tensorflow 1.1", "body": "Here, we want to use  **tensorflow.contrib.image.rotate** function in our code, it works well in the tensorflow 1.8 version, but does not works well in the tensorflow 1.1 version. Also, if we remove  the **tensorflow.contrib.image.rotate** function , it also works well in the 1.1 tensorflow.  So how can I use the tensorflow.contrib.image.rotate in tensorflow 1.1 version.\r\n\r\nThe tensorflow.contrib.image.rotate has changed with the tensorflow library. In 1.1 version, Click [tf.contrib.image.rotate](https://github.com/tensorflow/docs/blob/r1.1/site/en/api_docs/python/tf/contrib/image/rotate.md) \r\n```\r\nrotate(\r\n    images,\r\n    angles\r\n)\r\n\r\n```\r\nand in 1.8 version, click [tf.contrib.image.rotate](https://github.com/tensorflow/docs/blob/r1.8/site/en/api_docs/python/tf/contrib/image/rotate.md). \r\n```\r\ntf.contrib.image.rotate(\r\n    images,\r\n    angles,\r\n    interpolation='NEAREST',\r\n    name=None\r\n)\r\n```\r\nSo we remove the **interpolation='BILINEAR** parameters.  \r\n\r\nHere is some of my code:\r\n```\r\n# coding=utf-8\r\n\r\nimport os\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.image import rotate as images_rotate\r\n\r\nslim = tf.contrib.slim\r\n\r\ndef image_rotation(x):\r\n  rands = tf.truncated_normal([tf.shape(x)[0]], stddev=0.05)\r\n  return images_rotate(x, rands)\r\n\r\ndef graph(x, y, i, x_max, x_min, grad):\r\n    eps = 2.0 * 32 / 255.0\r\n    num_iter = 15\r\n    alpha = eps / num_iter\r\n    momentum = 1\r\n    num_classes = 110\r\n\r\n    with slim.arg_scope(inception_v1.inception_v1_arg_scope()):\r\n        logits_v1, end_points_v1 = inception_v1.inception_v1(\r\n            image_rotation(x), num_classes=num_classes, is_training=False)\r\n\r\n    one_hot = tf.one_hot(y, num_classes)\r\n    logits = logits_v1\r\n\r\n    cross_entropy = tf.losses.softmax_cross_entropy(one_hot,\r\n                                                    logits,\r\n                                                    label_smoothing=0.0,\r\n                                                    weights=1.0)\r\n    noise = tf.gradients(cross_entropy, x)[0]\r\n\r\n    kernel = gkern(7, 4).astype(np.float32)\r\n    stack_kernel = np.stack([kernel, kernel, kernel]).swapaxes(2, 0)\r\n    stack_kernel = np.expand_dims(stack_kernel, 3)\r\n\r\n    noise = tf.nn.depthwise_conv2d(noise, stack_kernel, strides=[1, 1, 1, 1], padding='SAME')\r\n\r\n    x = x + 2 * tf.sign(noise)\r\n    return x, y, i, x_max, x_min, noise\r\n\r\n\r\ndef stop(x, y, i, x_max, x_min, grad):\r\n  num_iter = FLAGS.num_iter\r\n  return tf.less(i, num_iter)\r\n\r\n\r\ndef main(input_dir, output_dir):\r\n\r\n    eps = 2 * 32 / 255.0\r\n    batch_shape = [1, 224, 224, 3]\r\n\r\n    with tf.Graph().as_default():\r\n        # Prepare graph\r\n        x_input = tf.placeholder(tf.float32, shape=batch_shape)\r\n        x_max = tf.clip_by_value(x_input + eps, -1.0, 1.0)\r\n        x_min = tf.clip_by_value(x_input - eps, -1.0, 1.0)\r\n\r\n        y = tf.constant(np.zeros([FLAGS.batch_size]), tf.int64)\r\n        i = tf.cast(tf.constant(0), tf.float32)\r\n\r\n        grad = tf.zeros(shape=batch_shape)\r\n\r\n        x_adv, _, _, _, _, _ = tf.while_loop(stop, graph, [x_input, y, i, x_max, x_min, grad])\r\n\r\n```\r\n\r\nOur environment is\uff1a\r\n> 1. cuda8.0 \r\n> 2. tensorflow1.1\r\n\r\nAnd the problems is attached here:\r\n```\r\nTraceback (most recent call last):\r\n  File \"attack_iter_notarge_attack_Grad_Rand.py\", line 352, in <module>\r\n    tf.app.run()\r\n  File \"/home/xuchao/anaconda3/envs/tensorflow1.1/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"attack_iter_notarge_attack_Grad_Rand.py\", line 343, in main\r\n    notarget_grad_rand_attack(FLAGS.input_dir, FLAGS.output_dir)\r\n  File \"attack_iter_notarge_attack_Grad_Rand.py\", line 303, in notarget_grad_rand_attack\r\n    x_adv, _, _, _, _, _ = tf.while_loop(stop, caad18_grad_rand_notarget_graph, [x_input, y, i, x_max, x_min, grad])\r\n  File \"/home/xuchao/anaconda3/envs/tensorflow1.1/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/home/xuchao/anaconda3/envs/tensorflow1.1/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/xuchao/anaconda3/envs/tensorflow1.1/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"attack_iter_notarge_attack_Grad_Rand.py\", line 258, in caad18_grad_rand_notarget_graph\r\n    noise = tf.nn.depthwise_conv2d(noise, stack_kernel, strides=[1, 1, 1, 1], padding='SAME')\r\n  File \"/home/xuchao/anaconda3/envs/tensorflow1.1/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py\", line 372, in depthwise_conv2d\r\n    input = ops.convert_to_tensor(input, name=\"tensor_in\")\r\n  File \"/home/xuchao/anaconda3/envs/tensorflow1.1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 639, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/home/xuchao/anaconda3/envs/tensorflow1.1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 704, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/xuchao/anaconda3/envs/tensorflow1.1/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 113, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/xuchao/anaconda3/envs/tensorflow1.1/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 102, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/home/xuchao/anaconda3/envs/tensorflow1.1/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 360, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n```\r\n\r\nI have searched for the problems on the website, and someones say it caused by the tf.contrib.image.rotate function can't work in the GPU environment in the tensorflow 1.1 version, so, I modify the image_rotation with:\r\n```\r\ndef image_rotation(x):\r\n  rands = tf.truncated_normal([tf.shape(x)[0]], stddev=0.05)\r\n  with tf.device('/cpu:0'):\r\n        input_tensor = tf.contrib.image.rotate(x, rands)\r\n  return input_tensor\r\n```\r\nBut it doesn't work well for me, look forward to your help. Thank you!", "comments": ["@Alxemade Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "After using tensorflow1.2 it works fine."]}, {"number": 28846, "title": "TF 2.0 crossed_column  on Windows fails with SystemError: <built-in function TFE_Py_FastPathExecute> returned a result with an error set", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0\r\n- Python version: 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe code snippets works fine on Colab but gives the following error on Windows:\r\n\r\n```\r\nOverflowError: Python int too large to convert to C long\r\nThe above exception was the direct cause of the following exception:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-6-6cfdea12e863>\", line 44, in <module>\r\n    demo(feature_column.indicator_column(crossed_feature))\r\n  File \"<ipython-input-6-6cfdea12e863>\", line 36, in demo\r\n    print(feature_layer(example_batch))\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 660, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 473, in call\r\n    self._state_manager)\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 4391, in get_dense_tensor\r\n    return transformation_cache.get(self, state_manager)\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 2573, in get\r\n    transformed = column.transform_feature(self, state_manager)\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 4330, in transform_feature\r\n    transformation_cache, state_manager)\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 4184, in get_sparse_tensors\r\n    transformation_cache.get(self, state_manager), None)\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 2573, in get\r\n    transformed = column.transform_feature(self, state_manager)\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\", line 4145, in transform_feature\r\n    hash_key=self.hash_key)\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py\", line 564, in sparse_cross_hashed\r\n    name=name)\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py\", line 617, in _sparse_cross_internal\r\n    name=name)\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_sparse_ops.py\", line 1372, in sparse_cross\r\n    \"internal_type\", internal_type)\r\nSystemError: <built-in function TFE_Py_FastPathExecute> returned a result with an error set\r\n```\r\nExecuting:\r\n```\r\nimport sys\r\nsys.maxsize\r\n```\r\n\r\ngives:\r\n`9223372036854775807`\r\n\r\n**Describe the expected behavior**\r\nSame output as running on Colab:\r\n\r\n![image](https://user-images.githubusercontent.com/2398765/57988793-d5c2c180-7ac4-11e9-8b33-02f3cdb22abc.png)\r\n\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport pandas as pd\r\n\r\nfrom tensorflow import feature_column\r\nfrom tensorflow.keras import layers\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\ndataframe = pd.read_csv(URL)\r\ndataframe.head()\r\n\r\ntrain, test = train_test_split(dataframe, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\n\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('target')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\n\r\n\r\nbatch_size = 5 # A small batch sized is used for demonstration purposes\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nexample_batch = next(iter(train_ds))[0]\r\n\r\n# A utility method to create a feature column\r\n# and to transform a batch of data\r\ndef demo(feature_column):\r\n  feature_layer = layers.DenseFeatures(feature_column)\r\n  print(feature_layer(example_batch))\r\n\r\nage = feature_column.numeric_column(\"age\")\r\nage_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\nthal = feature_column.categorical_column_with_vocabulary_list(\r\n      'thal', ['fixed', 'normal', 'reversible'])\r\n\r\ncrossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=3)\r\ndemo(feature_column.indicator_column(crossed_feature))\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@hsm207 I tried reproducing the issue on my system but the code executed without any error. Can you try once again and let us know if that still gives error. Thanks!", "@gadagashwini yes, I still get the same error. Is there additional information I can provide to help you diagnose this issue?", "I have the same issue following the same 'Classify structured data' tutorial. Same characteristics as the original poster, only I'm running tf on GPU.", "Same here.", "Same here on 2.0 beta version", "I get the same error about the \"Built-in function ...  returned a result with an error set\" but refers to the function 'TFE_Py_TapeWatch'.\r\nI find that all functions with 'TFE' are defined in pywrap_tensorflow_internal. So I try to import it on tape but the result is the same as before.\r\nVersion: tensorflow-gpu== 2.1 beta\r\nPlatform: Google Colab", "> I get the same error about the \"Built-in function ... returned a result with an error set\" but refers to the function 'TFE_Py_TapeWatch'.\r\n> I find that all functions with 'TFE' are defined in pywrap_tensorflow_internal. So I try to import it on tape but the result is the same as before.\r\n> Version: tensorflow-gpu== 2.1 beta\r\n> Platform: Google Colab\r\n\r\nI am having the same error... any solutions?", "I am also having the same error in the \"Classify structured data\" guide for tf 2.0. \r\nversion 2.0.0 beta1\r\nPlatform: Jupyter Lab\r\nWindows.\r\n\r\nI also had a similar issue on another model and both are listing gen_sparse_ops.py as the last script in the traceback.", "I tried on Colab as well on local system with Tensorflow 2.0.0.rc0. It is working as expected, can you please try with latest TF version and check. \r\n\r\nPTAL colab [gist here](https://colab.sandbox.google.com/gist/gadagashwini/fadf44ecddae701e7f051fe01c049c04/untitled146.ipynb) and Jupyter notebook gist here \r\n[#28846.ipynb.tar.gz](https://github.com/tensorflow/tensorflow/files/3608766/28846.ipynb.tar.gz)\r\n.Let us know how it progresses. Thanks!", "I tried running the code again after upgrading to `v2.0.0-rc0-101-gd2d2566eef 2.0.0-rc1` and now the error is gone. Thanks @gadagashwini ", "Glad it is working.\r\nClosing the issue. Please feel free to reopen if still issue persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28846\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28846\">No</a>\n", "@hsm207  I tried to upgrade with `pip install --upgrade tensorflow==2.0.0rc1`, but problem existed stilly. Could you tell me how to upgrade to `v2.0.0-rc0-101-gd2d2566eef 2.0.0-rc1`. Thanks!", "> @hsm207  I tried to upgrade with pip install --upgrade tensorflow==2.0.0rc1, but problem existed stilly. Could you tell me how to upgrade to v2.0.0-rc0-101-gd2d2566eef 2.0.0-rc1. Thanks!\r\n\r\n@JerichoHy I did not do an upgrade. I created a new conda environment and ran `pip install tensorflow==2.0.0-rc1`", "> @hsm207  I tried to upgrade with pip install --upgrade tensorflow==2.0.0rc1, but problem existed stilly. Could you tell me how to upgrade to v2.0.0-rc0-101-gd2d2566eef 2.0.0-rc1. Thanks!\r\n> \r\n> @JerichoHy I did not do an upgrade. I created a new conda environment and ran pip install tensorflow==2.0.0-rc1\r\n\r\nThanks.", "@JerichoHy I created a new conda environment with `tensorflow==2.0.0-rc1` installed but still get the same error. \r\n\r\n'OverflowError: Python int too large to convert to C long'\r\nfollowed by traceback down to\r\n\r\n'~\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_sparse_ops.py in sparse_cross(indices, values, shapes, dense_inputs, hashed_output, num_buckets, hash_key, out_type, internal_type, name)\r\n   1147         shapes, dense_inputs, \"hashed_output\", hashed_output, \"num_buckets\",\r\n   1148         num_buckets, \"hash_key\", hash_key, \"out_type\", out_type,\r\n-> 1149         \"internal_type\", internal_type)\r\n   1150       _result = _SparseCrossOutput._make(_result)\r\n   1151       return _result\r\n\r\nSystemError: <built-in function TFE_Py_FastPathExecute> returned a result with an error set'\r\n\r\nHappy to provide more info.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28846\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28846\">No</a>\n", "@Akoopie Please post a new issue describing your problem and provide all relevant information from the template. Thanks", "@ymodak The issue is `OverflowError: Python int too large to convert to C long` error after running this line of [feature_columns.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/feature_columns.ipynb) codes on the windows OS.\r\n`demo(feature_column.indicator_column(crossed_feature))`", "This error is closed but it still happening in windows 10.\r\n\r\n```\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\nOverflowError: Python int too large to convert to C long\r\n\r\n```\r\n", "`import tensorflow as tf #tf.__version__: '2.1.0'\r\n\r\nfrom tensorflow import feature_column\r\nfrom tensorflow.keras import layers\r\n\r\ndata = {'marks': [55,21,63,88,74,54,95,41,84,52],\r\n        'grade': ['average','poor','average','good','good','average','good','average','good','average'],\r\n        'point': ['c','f','c+','b+','b','c','a','d+','b+','c']}\r\n        \r\ndef demo(feature_column):\r\n  feature_layer = layers.DenseFeatures(feature_column)\r\n  print(feature_layer(data).numpy())        \r\n\r\nmarks = feature_column.numeric_column(\"marks\")  \r\nmarks_buckets = feature_column.bucketized_column(marks, boundaries=[30,40,50,60,70,80,90])\r\n\r\ngrade = feature_column.categorical_column_with_vocabulary_list(\r\n    'grade', ['poor', 'average', 'good']\r\n)\r\n\r\ncrossed_feature = feature_column.crossed_column([marks_buckets, grade], hash_bucket_size=10)\r\ndemo(feature_column.indicator_column(crossed_feature))`", "---------------------------------------------------------------------------\r\nOverflowError                             Traceback (most recent call last)\r\nOverflowError: Python int too large to convert to C long\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nSystemError                               Traceback (most recent call last)\r\n<ipython-input-33-d48208d43111> in <module>\r\n     21 \r\n     22 crossed_feature = feature_column.crossed_column([marks_buckets, grade], hash_bucket_size=10)\r\n---> 23 demo(feature_column.indicator_column(crossed_feature))\r\n\r\n<ipython-input-33-d48208d43111> in demo(feature_column)\r\n     11 def demo(feature_column):\r\n     12   feature_layer = layers.DenseFeatures(feature_column)\r\n---> 13   print(feature_layer(data).numpy())\r\n     14 \r\n     15 marks = feature_column.numeric_column(\"marks\")\r\n\r\nC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    820           with base_layer_utils.autocast_context_manager(\r\n    821               self._compute_dtype):\r\n--> 822             outputs = self.call(cast_inputs, *args, **kwargs)\r\n    823           self._handle_activity_regularization(inputs, outputs)\r\n    824           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\nC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\dense_features.py in call(self, features, cols_to_output_tensors)\r\n    133       with ops.name_scope(column.name):\r\n    134         tensor = column.get_dense_tensor(transformation_cache,\r\n--> 135                                          self._state_manager)\r\n    136         processed_tensors = self._process_dense_tensor(column, tensor)\r\n    137         if cols_to_output_tensors is not None:\r\n\r\nC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py in get_dense_tensor(self, transformation_cache, state_manager)\r\n   4349     # Feature has been already transformed. Return the intermediate\r\n   4350     # representation created by transform_feature.\r\n-> 4351     return transformation_cache.get(self, state_manager)\r\n   4352 \r\n   4353   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\r\n\r\nC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py in get(self, key, state_manager)\r\n   2613     column = key\r\n   2614     logging.debug('Transforming feature_column %s.', column)\r\n-> 2615     transformed = column.transform_feature(self, state_manager)\r\n   2616     if transformed is None:\r\n   2617       raise ValueError('Column {} is not supported.'.format(column.name))\r\n\r\nC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py in transform_feature(self, transformation_cache, state_manager)\r\n   4288     \"\"\"\r\n   4289     id_weight_pair = self.categorical_column.get_sparse_tensors(\r\n-> 4290         transformation_cache, state_manager)\r\n   4291     return self._transform_id_weight_pair(id_weight_pair)\r\n   4292 \r\n\r\nC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py in get_sparse_tensors(self, transformation_cache, state_manager)\r\n   4140     \"\"\"See `CategoricalColumn` base class.\"\"\"\r\n   4141     return CategoricalColumn.IdWeightPair(\r\n-> 4142         transformation_cache.get(self, state_manager), None)\r\n   4143 \r\n   4144   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\r\n\r\nC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py in get(self, key, state_manager)\r\n   2613     column = key\r\n   2614     logging.debug('Transforming feature_column %s.', column)\r\n-> 2615     transformed = column.transform_feature(self, state_manager)\r\n   2616     if transformed is None:\r\n   2617       raise ValueError('Column {} is not supported.'.format(column.name))\r\n\r\nC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py in transform_feature(self, transformation_cache, state_manager)\r\n   4101         inputs=feature_tensors,\r\n   4102         num_buckets=self.hash_bucket_size,\r\n-> 4103         hash_key=self.hash_key)\r\n   4104 \r\n   4105   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\r\n\r\nC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\sparse_ops.py in sparse_cross_hashed(inputs, num_buckets, hash_key, name)\r\n    596       num_buckets=num_buckets,\r\n    597       hash_key=hash_key,\r\n--> 598       name=name)\r\n    599 \r\n    600 \r\n\r\nC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\sparse_ops.py in _sparse_cross_internal(inputs, hashed_output, num_buckets, hash_key, name)\r\n    649       out_type=out_type,\r\n    650       internal_type=internal_type,\r\n--> 651       name=name)\r\n    652 \r\n    653   return sparse_tensor.SparseTensor(indices_out, values_out, shape_out)\r\n\r\nC:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_sparse_ops.py in sparse_cross(indices, values, shapes, dense_inputs, hashed_output, num_buckets, hash_key, out_type, internal_type, name)\r\n   1046         \"hashed_output\", hashed_output, \"num_buckets\", num_buckets,\r\n   1047         \"hash_key\", hash_key, \"out_type\", out_type, \"internal_type\",\r\n-> 1048         internal_type)\r\n   1049       _result = _SparseCrossOutput._make(_result)\r\n   1050       return _result\r\n\r\nSystemError: built-in function TFE_Py_FastPathExecute returned a result with an error set\r\n", "import tensorflow as tf # tf.__version__: 1.13.1 #run my code on https://www.katacoda.com/courses/tensorflow/playground\r\nfrom tensorflow import feature_column\r\n\r\nsess=tf.Session()#appended\r\n\r\ndata = {'marks': [55,21,63,88,74,54,95,41,84,52],\r\n        'grade': ['average','poor','average','good','good','average','good','average','good','average'],\r\n        'point': ['c','f','c+','b+','b','c','a','d+','b+','c']}\r\n        \r\n      \r\nmarks = feature_column.numeric_column(\"marks\")  \r\nmarks_buckets = feature_column.bucketized_column(marks, boundaries=[30,40,50,60,70,80,90])\r\n\r\ngrade = feature_column.categorical_column_with_vocabulary_list(\r\n    'grade', ['poor', 'average', 'good']\r\n)\r\n\r\ncrossed_feature = feature_column.crossed_column([marks_buckets, grade], hash_bucket_size=10)\r\n\r\ninputs = tf.feature_column.input_layer(data, [feature_column.indicator_column(crossed_feature)])\r\ninit = tf.global_variables_initializer()\r\n\r\ninit = tf.global_variables_initializer()\r\nsess.run(tf.tables_initializer())\r\nsess.run(init)\r\noutputs=sess.run(inputs)\r\nprint(outputs)\r\n\r\n![image](https://user-images.githubusercontent.com/36342491/90229569-215a3d00-dde6-11ea-8992-60ce8d00963e.png)\r\n\r\n# I just changed my code for running on tensorflow version 1.13.1 from  https://www.katacoda.com/courses/tensorflow/playground", "I'm getting the same error on Windows OS 10, tensorflow version 2.1.0 on gpu in conda environment using jupyter lab. final error message: `<built-in function TFE_Py_FastPathExecute> returned a result with an error set `. Is there any fix/workaround for it yet?\r\n "]}, {"number": 28845, "title": "GPU visible device selection causes segfault in tf-nightly-gpu", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): tf-nightly-gpu==1.14.1.dev20190519\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe following code causes a segfault:\r\n```python\r\nimport tensorflow as tf\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.visible_device_list = '0'\r\nsess = tf.Session(config=config)\r\nprint(sess)\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo segfault.  GPU pinning should work.\r\n\r\ncc @jaingaurav", "comments": ["It looks like https://github.com/tensorflow/tensorflow/pull/28885 pulled whatever caused this issue into r1.14. Could you make sure the fix gets picked into r1.14 as well?\r\n\r\ncc @annarev ", "I tried gdb on this and saw it failed at `\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n0x00007fff2608c6fa in stream_executor::StreamExecutorMemoryAllocator::StreamExecutorMemoryAllocator(stream_executor::Platform const*, absl::Span<stream_executor::StreamExecutor* const>) () from /.env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so`\r\n@skye @hawkinsp could this be related to the recent StreamExecutorMemoryAllocator changes?", "@alsrgv Could you verify that https://github.com/tensorflow/tensorflow/commit/fc2863425d742585b6e7d8b0ae9d008d2bfbe72f fixes the issue? I'll be cherry-picking it into r1.14.", "@cheshire, the fix works, thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28845\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28845\">No</a>\n", "Thanks @cheshire!"]}, {"number": 28844, "title": "TF2.0 leaking memory when input_shape is specified in keras layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): there is a custom MWE code snippet included in the issue\r\n- OS Platform and Distribution: Linux Elementary Loki (Ubuntu 16.04)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0.0-alpha0\r\n- Python version: 3.7.1\r\n\r\n**Describe the current behavior**\r\nGenerating several models with no input shape specified results in normal memory occupancy:\r\n\r\n```python\r\nimport time\r\n\r\nfrom tensorflow import keras\r\n\r\n\r\nfor _ in range(100):\r\n    model = keras.models.Sequential()\r\n    model.add(keras.layers.Dense(120, activation='relu'))\r\n    model.compile(loss='binary_crossentropy',\r\n                  optimizer=keras.optimizers.SGD())\r\n    time.sleep(0.1)\r\n```\r\n![no_shape](https://user-images.githubusercontent.com/472900/57986674-e39a2780-7a6f-11e9-909f-ac244abbcc4b.png)\r\n\r\nThis is the behavior expected when the input shape is specified. However, when multiple models are generated within the same program life cycle, the specification of an input shape seems to produce a memory leak.\r\n\r\nIf the input shape is specified when the models are created, they pile up in memory, without being destroyed. Also the execution time seems to increase quite a lot.\r\n\r\n```python\r\nimport time\r\n\r\nfrom tensorflow import keras\r\n\r\n\r\nfor _ in range(100):\r\n    model = keras.models.Sequential()\r\n    model.add(keras.layers.Dense(120, activation='relu', input_shape=(10, 10)))\r\n    model.compile(loss='binary_crossentropy',\r\n                  optimizer=keras.optimizers.SGD())\r\n    time.sleep(0.1)\r\n```\r\n\r\n![shape](https://user-images.githubusercontent.com/472900/57986711-67541400-7a70-11e9-9743-5528ece4417b.png)\r\n\r\n**Describe the expected behavior**\r\nModel generation should behave the same from a memory point of view regardless of whether the input shape is specified or not.\r\n\r\n**Code to reproduce the issue**\r\nIt is enough to execute the code snippets included in the issue. In order to generate the memory occupancy, the `memory_profiler` package can be used. Assuming the code snippet is pasted in a file called `test.py`, run:\r\n\r\n```\r\n$ mprof run --include-children python test.py\r\n$ mprof plot\r\n```\r\n\r\n**Other info / logs**\r\nI generated a list of all the objects for the 2 code snippets, using `Pympler`.\r\n\r\n```python\r\nfrom pympler import muppy\r\nfrom pympler import summary\r\n\r\n...\r\n\r\nall_objects = muppy.get_objects()\r\noccupancy = summary.summarize(all_objects)\r\nsummary.print_(occupancy)\r\n```\r\n\r\nFor the models generated without input shape, here is the result:\r\n\r\n```\r\n                                                                   types |   # objects |   total size\r\n======================================================================== | =========== | ============\r\n                                                             <class 'str |       79278 |     14.02 MB\r\n                                                            <class 'dict |       14468 |      6.93 MB\r\n                                                            <class 'code |       25252 |      3.49 MB\r\n                                                            <class 'type |        2582 |      2.57 MB\r\n                                                            <class 'list |        8573 |    944.06 KB\r\n                                                           <class 'tuple |       12079 |    796.21 KB\r\n                                                             <class 'set |         732 |    466.12 KB\r\n                                                         <class 'weakref |        4475 |    349.61 KB\r\n                                                     <class 'abc.ABCMeta |         341 |    346.27 KB\r\n                     <class 'tensorflow.core.framework.op_def_pb2.ArgDef |        3822 |    328.45 KB\r\n  <class 'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType |         369 |    320.16 KB\r\n                                                            <class 'cell |        5921 |    277.55 KB\r\n                                                     function (__init__) |        1694 |    224.98 KB\r\n                                                        <class 'property |        2466 |    192.66 KB\r\n                                              <class 'wrapper_descriptor |        2353 |    183.83 KB\r\n```\r\n\r\nAnd here is the snapshot for the models generated with the input shape.\r\n\r\n```\r\n                                                           types |   # objects |   total size\r\n================================================================ | =========== | ============\r\n                                                   <class 'tuple |      296304 |     26.77 MB\r\n                                                    <class 'dict |       71296 |     16.14 MB\r\n                                                     <class 'str |       84828 |     14.54 MB\r\n                                                     <class 'int |      395032 |     10.55 MB\r\n                                                    <class 'list |       98027 |      9.89 MB\r\n                                                    <class 'code |       25281 |      3.49 MB\r\n                                                    <class 'type |        2587 |      2.58 MB\r\n                                                     <class 'set |        2944 |    975.00 KB\r\n               <class 'tensorflow.python.framework.ops.Operation |       14100 |    771.09 KB\r\n    <class 'tensorflow.python.framework.ops.Operation._InputList |       14100 |    771.09 KB\r\n                  <class 'tensorflow.python.framework.ops.Tensor |       14100 |    771.09 KB\r\n  <class 'tensorflow.python.pywrap_tensorflow_internal.TF_Output |       13200 |    721.88 KB\r\n                                            <class 'SwigPyObject |       14100 |    660.94 KB\r\n                                 <class 'collections.OrderedDict |        1262 |    622.22 KB\r\n                                                 <class 'weakref |        6127 |    478.67 KB\r\n```\r\n\r\nEdit: typo in `keras.models`\r\nEdit: typo in `keras.layers`", "comments": ["@b3by I ran the code, but did not see any output, does this need any modifications in the code, if not please suggest me to reproduce the issue", "Unless you look at your system manager or htop, the way you can see the memory leak is by plotting the memory consumption using `memory_profiler`. Did you execute the script by running the following?\r\n\r\n```\r\nmprof run --include-children python test.py && mprof plot\r\n```\r\n\r\nIt should open up a matplotlib plot with the memory taken by the script.", "This is expected behavior, as the model has to be rebuilt if no input shape is available. You can use [clear_session](https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session) to clear the current Keras session.", "I just tried to include\r\n\r\n```tf.keras.backend.clear_session()```\r\n\r\ninto the test script (both before and after the sleep), but no luck with that, the memory leak is still there. Now if this was really an expected behaviour, that would mean the bug is actually in the `clear_session` method. I understand your point, but I just can't wrap my mind around the idea that old model instances are not destroyed just because the new ones have to be rebuilt.", "To follow up on what @karmel said:\r\n\r\nWhen you make a Sequential model without input shape specified there isn't actually enough information at compile time to create the weights. (For instance in the dense layer above, there's no way to know that the kernel needs to be shape [10, 120] and it can't allocate a [???, 120] Variable.) So Sequential defers weight creation until it sees the first batch, at which point it can create variables. So the lack of memory growth is simply reflecting the fact that the example doesn't reach the point to which allocation is deferred.\r\n\r\nOn the other hand, when input shape is provided then Sequential actually can allocate weights before seeing the first batch. The reason that those weights aren't freed when the model goes out of scope is that unless otherwise specified, all tensors used by keras share the same global graph. It has the nice property of allowing mixing and matching of models (this is why you can compose models from other models, for instance), but it also means that this graph holds references to weights and therefore blocks their garbage collection even beyond the life of the python Model object.\r\n\r\n`tf.keras.backend.clear_session` destroys the underlying global graph, and therefore allows variables to be freed. The reason you're seeing different behavior is that there was a bug where clear_session didn't work in 2.0, and the fix (https://github.com/tensorflow/tensorflow/commit/5956c7e9c44e23cd1a006df872ae468201fdb600) was added after the alpha cut. If you try `tf-nightly-2.0-preview` it should do the right thing.\r\n\r\nI've replicated your example (which was stellar, by the way. top notch repro.) and confirmed that clear_session works as intended in the latest nightly build: https://gist.github.com/robieta/474aa37a76ba242d9f5ea2e5284c9fab\r\n\r\nWe're also actively looking into the broader issue of making keras more gc friendly, so stay tuned.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28844\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28844\">No</a>\n", "@robieta spot on. Brilliant explanation and excellent notebook, thank you very much. The shared global graph explains things, I was just unlucky to stump in a bug (fixed in nightly) that was concealed in a normal behaviour that I took for a bug. I already patched my code so that I spawn a new process every time I train a new fold, but I might try with the nightly version and see if my whole project breaks.", "will this fix work in beta1?"]}, {"number": 28843, "title": "TF2.0 DocSprint Prep - Error with Dataset Example", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe Example for \"from_generator\" fails.\r\n\r\n### Clear description\r\n\r\nThis example uses tf.enable_eager_execution() which gives the following error\r\nAttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'\r\n\r\nIf you remove this line of code the example runs but identifies this code is deprecated in TF V2.\r\n\r\n### Correct links\r\n\r\nI do not see a link to the source.\r\n\r\n### Parameters defined\r\n\r\nThe parameters are defined but the example only uses three of the four parameters.  The last parameter is optional but not shown how to be used.\r\n\r\n### Returns defined\r\n\r\nThe return code is defined correctly as a dataset.\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nThere is a usage example.  This PR is regarding the example.\r\n\r\n### Request visuals, if applicable\r\n\r\nThere are not example.\r\n\r\n### Submit a pull request?\r\nyes\r\nhttps://github.com/tensorflow/tensorflow/pull/28842\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\nyes\r\nhttps://github.com/tensorflow/tensorflow/pull/28842\r\n", "comments": ["@netskink : Thank you for bringing this out and raising PR. Eager execution is enabled by default in TF2.0", "You are welcome achandraa.  Yesterday there was discussion regarding the branches.  Is 2.0 updates going into master branch or r2.0 branch?", "Yes you are right. The updates for 2.0 are going to r2.0 branch as of now. ", "This is fixed now. Thanks!\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#for_example_3"]}]