[{"number": 45981, "title": "Running custom tflite model, segfault only on CPU, tflite 2.4.0 built from sources", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 8.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Meizu 16th\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0, also 5a8dc94c30a\r\n- Python version: no \r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n**Describe the current behavior**\r\ntrying to run custom [converted](https://github.com/pb-julian/liteface) tflite [model](https://www.dropbox.com/s/akxeqp99jvsd6z7/model-MobileFaceNet-arcface-ms1m-refine-v1.zip?dl=0) from insigthface on android. It works with standard tflite.aar from jcenter. Also works with built from sources libtensorflowlite_gpu_delegate.so. But when i tried to run on CPU with built from source libtensorflowlite.so, i got segfault.\r\n\r\n**Describe the expected behavior**\r\nExpected to work with manually built libtensorflowlite.so\r\n\r\n**Standalone code to reproduce the issue**\r\n- *.so files was built with docker from repo and [guide](https://www.tensorflow.org/lite/guide/build_android) in site.\r\n- libtensorflowlite.so was built with commands: `bazel build tensorflow/lite:libtensorflowlite.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config=android --cpu=arm64-v8a` and  `bazel build tensorflow/lite:libtensorflowlite.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config=android --cpu=armeabi-v7a`\r\n- code for running was adapted from [sample of usage](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_c)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ntraceback:\r\n<img width=\"1250\" alt=\"\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 2020-12-27 \u0432 05 13 36\" src=\"https://user-images.githubusercontent.com/32731602/103161348-56e64c80-4802-11eb-87de-e40365ea69e0.png\">\r\n\r\n", "comments": ["Why are you building .so instead of .aar file? As you mentioned, it worked with the standard .aar file from jCenter so it might be easier for you to just build the .aar file.\r\nCan you try following command:\r\n```\r\nbazel build -c opt --config=android_arm64 \\\r\n  --crosstool_top=//external:android/crosstool \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  //tensorflow/lite/java:tensorflow-lite\r\n```", "Hi @thaink,\r\n\r\nOther code in my project written on C++, so i need C API for tflite. Also, jCenter's aar is slow, *.so complied from sources runs faster.\r\n\r\nYes, java aar variant works, and that proofs that problem are not in model.\r\n\r\nBut cpp *.so crashes. What is wrong?", "Got it.\r\nInstead of \"--config=android --cpu=arm64-v8a\", can you try --config=android_arm64?\r\nBeside \"-cpu\", it also set \"-fat_apk_cpu\". \"Bazel needs to have --cpu and --fat_apk_cpu both set to the target CPU to build transient dependencies correctly.\"\r\n", "@thaink \r\nYes, i tried to use together \"--cpu=armeabi-v7a --fat_apk_cpu=armeabi-v7a\", also for separate build tried \"--cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\". Also tried to add \"--config=android_arm64\" and \"--config=android_arm\" and tried to use only --config param.\r\n\r\nresults are same, still crashes.", "@hfnvbh Can you provide a code to reproduce this.\r\nI cannot find the model from your link.", " @thaink \r\n[model.tflite.zip](https://github.com/tensorflow/tensorflow/files/5780045/model.tflite.zip)\r\n\r\n```cpp\r\n// Load the model\r\nstd::unique_ptr<tflite::FlatBufferModel> model =\r\n    tflite::FlatBufferModel::BuildFromFile(\"model.tflite\");\r\n\r\n// Build the interpreter\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n\r\n// Resize input tensors, if desired.\r\ninterpreter->AllocateTensors();\r\n\r\nfloat* input_tensor = interpreter->typed_input_tensor<float>(0);\r\nstd::vector<float> input;\r\ninput.resize(112 * 112 * 3)\r\nstd::memcpy(input_tensor, input.data(), 112 * 112 * 3 * sizeof(float));\r\n\r\ninterpreter->Invoke();\r\n\r\nfloat* output = interpreter->typed_output_tensor<float>(0);\r\n```\r\n\r\n", "I just tried your code on my android phone. It works just fine.", "@thaink \r\nthat interesting. Could you please share your .so files, i will check it in my phone too.", "The lib file is here https://github.com/thaink/upload_files/tree/main/TF_PR45981\r\nBy the way, the error log you show doesn't look like a linking issue.\r\nIf you face the error, could you upload the full log?", "Hi @thaink .\r\n\r\nI made some experiments and want to clarify situation\r\nYes, you are correct, if just run that code in single function it's works with your and my *.so builds.\r\n\r\nBut when i separate code for two functions: init and run and call it in third function, i got crash even with your build.\r\n\r\nHere is code for reproduce:\r\n```cpp\r\n\r\nvoid tfinit(std::unique_ptr<tflite::Interpreter> &interpreter) {\r\n    // Load the model\r\n    std::unique_ptr<tflite::FlatBufferModel> model =\r\n            tflite::FlatBufferModel::BuildFromFile(\"/storage/emulated/0/model.tflite\");\r\n\r\n// Build the interpreter\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    tflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n\r\n// Resize input tensors, if desired.\r\n    interpreter->AllocateTensors();\r\n}\r\n\r\nvoid tfrun (std::unique_ptr<tflite::Interpreter> &interpreter) {\r\n    float* input_tensor = interpreter->typed_input_tensor<float>(0);\r\n    std::vector<float> input;\r\n    input.resize(112 * 112 * 3);\r\n    std::copy(input.data(), input.data() + 112 * 112 * 3 , input_tensor);\r\n\r\n    interpreter->Invoke();\r\n\r\n    float* output = interpreter->typed_output_tensor<float>(0);\r\n}\r\n\r\n//function runs others two\r\nJNIEXPORT jboolean JNICALL JNI_FUNC(initTfModel)(JNIEnv* env, jobject thiz) {\r\n    std::unique_ptr<tflite::Interpreter> interpreter;\r\n    tfinit(interpreter);\r\n    tfrun(interpreter);\r\n    return true;\r\n}\r\n``` \r\n\r\nhere is logs for crash:\r\n[crash.log](https://github.com/tensorflow/tensorflow/files/5811702/crash.log)\r\n\r\nBut if i add GPU delegate to init and make at as written below, it will work without crashes\r\n```cpp\r\nvoid tfinit(std::unique_ptr<tflite::Interpreter> &interpreter) {\r\n    // Load the model\r\n    std::unique_ptr<tflite::FlatBufferModel> model =\r\n            tflite::FlatBufferModel::BuildFromFile(\"/storage/emulated/0/model.tflite\");\r\n\r\n// Build the interpreter\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    tflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n\r\n    auto delegateOptions = TfLiteGpuDelegateOptionsV2Default();\r\n    delegateOptions.inference_priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY;\r\n\r\n    std::unique_ptr<TfLiteDelegate, std::function<void(TfLiteDelegate*)>> delegate(\r\n            TfLiteGpuDelegateV2Create(&delegateOptions),\r\n    [](TfLiteDelegate* d) { TfLiteGpuDelegateV2Delete(d); }\r\n    );\r\n\r\n    interpreter->ModifyGraphWithDelegate(std::move(delegate));\r\n\r\n// Resize input tensors, if desired.\r\n    interpreter->AllocateTensors();\r\n}\r\n```\r\n\r\n", "Looks like `tflite::Interpreter` reuse tensors that are belong to tflite::Model.\r\nSo when your program exit the tfinit, the `tflite::FlatBufferModel` object is deallocated, causing those tensors in `tflite::Interpreter` become invalid.\r\n\r\nYou can fix it by moving the content of tfinit to JNI_FUNC. This way, you still can invoke the model multiple times. Or define the  `tflite::FlatBufferModel` object in JNI_FUNC and pass it to tfinit.\r\n\r\n@jdduke do you think this is error-prone and should be fixed?", "@thaink \r\n\r\nThank you, i also found notes from api reference about it. The fact that GPU version stable working even after `tflite::FlatBufferModel`  destruction confused me.\r\n\r\nSo, my problem solved, thank you very much.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45981\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45981\">No</a>\n", "> @jdduke do you think this is error-prone and should be fixed?\r\n\r\nIt does require a bit more care on the client side, but it has been this way from the beginning and is fully documented. We could offer a convenience path which passes ownership of `FlatBufferModel` to the `Interpreter`. Relatively few clients shared a `FlatBufferModel` instance between `Interpreters` (which was a good motivation for the existing API that allows sharing)."]}, {"number": 45980, "title": "TF2.3; tensorflow.python.framework.errors_impl.UnknownError: Failed to rename; : Access is denied. ; Input/output error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Conda; TF 2.3; \r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nStackoverflow: https://stackoverflow.com/questions/65461750/tensorflow-python-framework-errors-impl-unknownerror-failed-to-rename-access\r\nI am not able to download and load tensorflow dataset on my Windows 10 machine. It works okay on Google colab. \r\n\r\n\r\n**Describe the expected behavior**\r\nI should be able to download the datasets. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow_datasets as tfds\r\ndatasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True\r\n```\r\n\r\n**Error:**\r\n\r\n\r\n```\r\n\r\nWriting...:   0%|          | 0/2500 [00:00<?, ? examples/s]\r\nShuffling...:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 18/20 [00:01<00:00, 14.15 shard/s]\r\nReading...: 0 examples [00:00, ? examples/s]\r\n                                            \r\nWriting...:   0%|          | 0/2500 [00:00<?, ? examples/s]\r\n                                                           \r\nReading...: 0 examples [00:00, ? examples/s]\r\n                                            \r\nWriting...:   0%|          | 0/2500 [00:00<?, ? examples/s]\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-3b586bfe81d7>\", line 3, in <module>\r\n    datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py\", line 52, in disallow_positional_args_dec\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\registered.py\", line 300, in load\r\n    dbuilder.download_and_prepare(**download_and_prepare_kwargs)\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py\", line 52, in disallow_positional_args_dec\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 307, in download_and_prepare\r\n    self.info.write_to_directory(self._data_dir)\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\contextlib.py\", line 119, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py\", line 200, in incomplete_dir\r\n    tf.io.gfile.rename(tmp_dir, dirname)\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 546, in rename_v2\r\n    compat.as_bytes(src), compat.as_bytes(dst), overwrite)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to rename: C:\\Users\\User\\tensorflow_datasets\\imdb_reviews\\plain_text\\0.1.0.incomplete5JQVCL to: C:\\Users\\User\\tensorflow_datasets\\imdb_reviews\\plain_text\\0.1.0 : Access is denied.\r\n; Input/output error\r\n```\r\n\r\n**Conda list**\r\n\r\n```\r\ntensorboard               2.3.0              pyh4dce500_0\r\ntensorboard-plugin-wit    1.6.0                      py_0\r\ntensorflow                2.3.0           mkl_py37h3bad0a6_0\r\ntensorflow-base           2.3.0           eigen_py37h17acbac_0\r\ntensorflow-datasets       1.2.0                    py37_0\r\ntensorflow-estimator      2.3.0              pyheb71bc4_0\r\ntensorflow-metadata       0.14.0             pyhe6710b0_1\r\ntensorflow-mkl            2.3.0                h93d2e19_0\r\n```", "comments": ["@gmatalongthewatchtower \r\nI ran the code shared on tf nightly and did not face any issues, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d8d644ec98eee653cdd7186e9e2fd092/untitled488.ipynb).", "@Saduf2019 : this is an issue on Windows 10. Google Colab uses Linux. I don't see this issue on Google Colab. If you Google this error, many people are facing the issue. ", "Here's a sample github issue thread: https://github.com/tensorflow/tensorflow/issues/41380. There are way too many issues on this topic.", "Are you able to load [tf keras datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data)?\r\n```python\r\ntf.keras.datasets.imdb.load_data()\r\n```", "Yes, that works well without any issues. I got a bunch of deprecation warnings, but there was no issue with the code.", "Thanks for confirming. Can you please raise this issue on TensorFlow Datasets repository as well?\r\nhttps://github.com/tensorflow/datasets/issues\r\ncc @Conchylicultor ", "I am also getting this error on: Windows 10; TF 2.3; Python 3.7\r\n\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to rename: path\\trial_5a095c02600a30dc086a9efe046b1272\\checkpoints\\epoch_0\\checkpoint_temp/part-00000-of-00001.data-00000-of-00001 to: path\\trial_5a095c02600a30dc086a9efe046b1272\\checkpoints\\epoch_0\\checkpoint.data-00000-of-00001 : Access is denied. \r\n; Input/output error [Op:MergeV2Checkpoints]\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45980\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45980\">No</a>\n", "This works with TF 2.4.1 and TF Datasets 4.0.1"]}, {"number": 45979, "title": "tf.keras.preprocessing.image.save_img() flips dimensions of images created from tf.keras.preprocessing.image_dataset_from_directory()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.1\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.8.6\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: Intel Iris 1536 MB\r\n\r\n**Describe the current behavior**\r\n\r\n-> A directory hierarchy with the following format is created\r\n\r\n/Project\r\n|-- data\r\n|    -- class-1\r\n|        -- image1.jpg\r\n|        -- image2.jpg\r\n|        -- ...\r\n|    -- class-2\r\n|        -- image1.jpg\r\n|        -- image2.jpg\r\n|-- venv (tensorflow, pillow)\r\n|    -- ...\r\n|-- bug.py\r\n\r\nin which the images have uneven dimensions (ie. 640x360 as opposed to 256x256)\r\n\r\n-> A dataset is imported from a directory using tf.keras.preprocessing.image_dataset_from_directory()\r\n-> A single image is taken from the dataset using next(dataset)\r\n-> The image is saved using tf.keras.preprocessing.image.save_img()\r\n\r\n**Unexpected Behavior:** the resulting image has dimensions 360x640 instead of 640x360 and is heavily warped\r\n\r\n**Describe the expected behavior**\r\n\r\nThe image should have the same dimensions as when it was imported ie. 640x360\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nGithub repo (couldn't figure out how to get it into a Jupyter notebook)\r\nhttps://github.com/atw1020/tensorflowBug\r\n\r\n**Other info / logs**\r\n\r\n\r\n", "comments": ["Realized this was a bug with my own code, I was inputting my dimensions incorrectly. In the Image_dataset_from_directory() function call I was passing in image_size=(640, 360) instead of (360, 640). This caused the warping which I blamed on the function rather than the incorrect order of parameters", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45979\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45979\">No</a>\n"]}, {"number": 45978, "title": "TF 2.4: Metrics not recognized as stateful in ProgbarLogger callback", "body": "**System information**\r\n- The vanilla example in https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric has the issue\r\n- OS Platform and Distribution: Linux Ubuntu 21.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.7.9\r\n- CUDA/cuDNN version: 11.0/8.0.5.39\r\n- GPU model and memory: GeForce RTX 2080 Mobile\r\n\r\n**Describe the current behavior**\r\nAlthough the metric in the example, `CategoricalAccuracy`, is calculated correctly, it is not being recognized as a stateful metric by the `ProgbarLogger` callback. For more complicated metrics, that require proper handling by the `ProgbarLogger` callback, this causes errors such as `ValueError: operands could not be broadcast together with shapes (19,11) (38,11)` since they are concatenated instead of using the stateful metric value.\r\n\r\n**Describe the expected behavior**\r\nIf the automatic addition of the `ProgbarLogger` callback did recognize the metrics added to the model, it would have been seen in the `ProgbarLogger.stateful_metrics` attribute, but it is currently always empty.\r\n\r\n**Standalone code to reproduce the issue**\r\nIn the code example below, the `ProgbarLogger` is manually created and metrics are added to the constructor. Hence, the automatic creation of the `ProgbarLogger` is disabled and the `ProgbarLogger.stateful_metrics` will contain the `CategoricalAccuracy` metric and properly calculate the progress in a stateful manner.\r\n\r\n```python\r\nimport types\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.callbacks import ProgbarLogger\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\r\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\r\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\r\n\r\nmetrics = [tf.keras.metrics.CategoricalAccuracy()]\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),\r\n              loss=tf.keras.losses.CategoricalCrossentropy(),\r\n              metrics=metrics)\r\n\r\nnp.random.seed(0)\r\ntf.random.set_seed(0)\r\ndata = np.random.random((1000, 32))\r\nlabels = np.random.random((1000, 10))\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((data, labels))\r\ndataset = dataset.batch(32)\r\n\r\nmetric_names = [\r\n    m.lower() if isinstance(m, str) else\r\n    m.__name__ if isinstance(m, types.FunctionType) else\r\n    m.name for m in metrics]\r\ncallbacks = [ProgbarLogger(\r\n    count_mode='steps',\r\n    stateful_metrics=metric_names)]\r\n\r\nresult = model.fit(dataset, callbacks=callbacks, epochs=10)\r\n\r\n```\r\n", "comments": ["@DrAA,\r\nI did not face any errors while running the given code snippet and the linked TensorFlow documentation example. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/7de9e1facb8ea70e68e06ca5edde4275/45978.ipynb).\r\n\r\n\r\n\r\n> this causes errors such as `ValueError: operands could not be broadcast together`\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet to reproduce the issue reported here. Thanks!", "Sure, I added a metric from a codebase where I experience the problem, called `ReservoirHistogram`. I added a cell to you [colab notebook](https://colab.research.google.com/gist/amahendrakar/7de9e1facb8ea70e68e06ca5edde4275/45978.ipynb), and also pasted that below. Running the code should demonstrate the problem, and just switch the `if False` statement to `if True`  to see that the hotfix works.\r\n\r\n```python\r\n#!/usr/bin/env python\r\nimport types\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\r\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\r\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\r\n\r\n\r\nclass ReservoirHistogram(tf.metrics.Metric):\r\n\r\n    def __init__(self,\r\n                 name='histogram',\r\n                 reservoir_size=300,\r\n                 reservoir_shape=(10,),\r\n                 dtype=tf.float32,\r\n                 **kwargs):\r\n        super().__init__(name=name, dtype=dtype, **kwargs)\r\n        self.reservoir: tf.Variable = self.add_weight(\r\n            name='reservior',\r\n            shape=(reservoir_size,) + reservoir_shape,\r\n            initializer='zeros',\r\n            dtype=self.dtype)\r\n\r\n        self.current_index: tf.Variable = self.add_weight(\r\n            name='current_index', shape=(), dtype=tf.int32,\r\n            initializer=tf.constant_initializer(0))\r\n\r\n    def update_state(self, y_true, y_pred, sample_weight=None):\r\n        batch_size = tf.shape(y_pred)[0]\r\n        reservoir_size = tf.cast(tf.shape(self.reservoir)[0], tf.int32)\r\n        r_index = self.current_index % reservoir_size\r\n        batch_part = tf.minimum(batch_size, reservoir_size - r_index)\r\n        batch_random = tf.random.uniform(shape=(batch_part,), maxval=1.)\r\n        self.current_index.assign_add(batch_part)\r\n        reservoir_prob = (1. if self.current_index < reservoir_size\r\n                          else tf.cast(reservoir_size / self.current_index,\r\n                                       dtype=tf.float32))\r\n        batch_mask = batch_random <= reservoir_prob\r\n        batch_mask = tf.tile(\r\n            input=tf.expand_dims(tf.cast(batch_mask, tf.float32), axis=1),\r\n            multiples=tf.concat(((1,), tf.shape(self.reservoir)[1:]), axis=0))\r\n        old_values = self.reservoir[r_index: r_index + batch_part]\r\n        batch_values = y_pred[:batch_part]\r\n        # transform values to have a zero-centered distribution\r\n        batch_values = tf.clip_by_value(tf.where(\r\n            batch_values > 0.5, batch_values - 1, batch_values),\r\n            clip_value_min=-0.1, clip_value_max=0.1)\r\n        new_values = old_values * (1 - batch_mask) + batch_values * batch_mask\r\n        new_reservoir = tf.concat(\r\n            (self.reservoir[:r_index],\r\n             new_values,\r\n             self.reservoir[r_index + batch_part:]),\r\n            axis=0)\r\n        self.reservoir.assign(new_reservoir)\r\n\r\n    def result(self):\r\n        used_size = tf.minimum(self.current_index, tf.shape(self.reservoir)[0])\r\n        return self.reservoir[:used_size]\r\n\r\n    def reset_states(self):\r\n        self.reservoir.assign(tf.zeros(shape=tf.shape(self.reservoir)))\r\n        self.current_index.assign(0)\r\n\r\n\r\nmetrics = [tf.keras.metrics.CategoricalAccuracy(), ReservoirHistogram()]\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),\r\n              loss=tf.keras.losses.CategoricalCrossentropy(),\r\n              metrics=metrics)\r\n\r\nnp.random.seed(0)\r\ntf.random.set_seed(0)\r\ndata = np.random.random((1000, 32))\r\nlabels = np.random.random((1000, 10))\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((data, labels))\r\ndataset = dataset.batch(32)\r\n\r\nif False:  # switch to True to demonstrate that the hotfix works\r\n    metric_names = [\r\n        m.lower() if isinstance(m, str) else\r\n        m.__name__ if isinstance(m, types.FunctionType) else\r\n        m.name for m in metrics]\r\n    callbacks = [tf.keras.callbacks.ProgbarLogger(\r\n        count_mode='steps',\r\n        stateful_metrics=metric_names)]\r\n    result = model.fit(dataset, callbacks=callbacks, epochs=10)\r\nelse:\r\n    result = model.fit(dataset, epochs=10)\r\n```", "Was able to run the code without any errors on [TF v2.3](https://colab.research.google.com/gist/amahendrakar/b01a2a3654163ea4edc2b9eafb282017/45978-2-3.ipynb).\r\n\r\nHowever with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/5a14072b48aefa4288f95d0a7dca7599/45978.ipynb#scrollTo=0pauDHBLSFyP) and TF-nightly, I'm facing an error stating `ValueError: operands could not be broadcast together with shapes (32,10) (64,10) (32,10)`. Please check the linked gist for reference. Thanks!", "Yes, that sounds similar to my experience. Worked well for me in 2.3 but\nnot in 2.4. Any additional information or help that I could provide?\n\nBest regards,\n\nAnders Arpteg, Ph.D.\n\nHead of Research, peltarion.com\n\nFounder, Agent Central AB\n\nCell: +46 (0)709 389020\nhttps://www.linkedin.com/in/andersarpteg/\n\n\nOn Tue, Jan 5, 2021 at 5:15 PM Abhilash Mahendrakar <\nnotifications@github.com> wrote:\n\n> Was able to run the code without any errors on TF v2.3\n> <https://colab.research.google.com/gist/amahendrakar/b01a2a3654163ea4edc2b9eafb282017/45978-2-3.ipynb>\n> .\n>\n> However with TF v2.4\n> <https://colab.research.google.com/gist/amahendrakar/5a14072b48aefa4288f95d0a7dca7599/45978.ipynb#scrollTo=0pauDHBLSFyP>\n> and TF-nightly, I'm facing an error stating ValueError: operands could\n> not be broadcast together with shapes (32,10) (64,10) (32,10). Please\n> check the linked gist for reference. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/45978#issuecomment-754736771>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIH767HI2KTP7VUPVWW6XTSYM3LLANCNFSM4VKB4AXA>\n> .\n>\n", "@DrAA Is this still an issue for you? I ran your code with `tf-nightly` and I don't see any error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/794a831d94911b5e13712e7f666314df/45978.ipynb)\r\n\r\nCan you please check and confirm whether the issue is still persisting with `tf-nightly`. Thanks!", "Yes, I can confirm that the issue has been resolved in the latest tf-nightly. Great! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45978\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45978\">No</a>\n"]}, {"number": 45977, "title": "MaskRCNN TensorFlow Lite Inference Issue. No output from TFLite Model.", "body": "**System information**\r\n- OS Platform and Distribution ( Ubuntu 18.04.5 LTS (GNU/Linux 5.4.0-1034-azure x86_64)):\r\n- TensorFlow installed from (source- Pip Install):\r\n- TensorFlow version (2.3.0):\r\n\r\n\r\n**Command used to run the converter**\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\nconverter.allow_custom_ops = True\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\n\r\nconverter.optimizations = [ tf.lite.Optimize.DEFAULT ]\r\n\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**link to Jupyter notebook and tflite model**\r\n\r\nhttps://drive.google.com/drive/folders/1pTB33fTSo5ENzevobTvuG7hN4YmiCPF_?usp=sharing\r\n\r\n\r\n**Commands used for inference**\r\n```\r\n### Load the TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"model_2.3.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n### Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n### Test the model on random input data.\r\ninput_data_1 = np.array(np.random.random_sample(input_details[0]['shape']), dtype=np.float32)\r\ninput_data_2 = np.array(np.random.random_sample(input_details[1]['shape']), dtype=np.float32)\r\ninput_data_3 = np.array(np.random.random_sample(input_details[2]['shape']), dtype=np.float32)\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], input_data_1)\r\ninterpreter.set_tensor(input_details[1]['index'], input_data_2)\r\ninterpreter.set_tensor(input_details[2]['index'], input_data_3)\r\n\r\ninterpreter.invoke() ---> Kernel is getting stuck here. No output. I am executing the code from jupyter.\r\n\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\nNo output in Jupyter. \r\n\r\nSegmentation fault (core dumped) -- When executed in command line.\r\n\r\n**Failure details**\r\n\r\nConversion is successful. But there is no output from model.\r\n\r\nCould you guys please provide some ideas? I am stuck here and don't know how to proceed!\r\n", "comments": ["I ran the model with tf-nightly version through the shared notebook. It took a few minutes to complete the invoke() part but I could get the result.\r\n\r\n```\r\nPrediction results: [[[0.2833773  0.28597987 0.03956655 0.5136417 ]\r\n  [0.38100398 0.45283458 0.66197014 0.6441412 ]\r\n  [0.17763907 0.53368616 0.48083413 0.1984492 ]\r\n  ...\r\n  [0.55963373 0.806476   0.32869753 0.5672232 ]\r\n  [0.7234199  0.45786685 0.3743322  0.25172007]\r\n  [0.662722   0.8848254  0.75358796 0.9891881 ]]]\r\n```", "> I ran the model with tf-nightly version through the shared notebook. It took a few minutes to complete the invoke() part but I could get the result.\r\n> \r\n> ```\r\n> Prediction results: [[[0.2833773  0.28597987 0.03956655 0.5136417 ]\r\n>   [0.38100398 0.45283458 0.66197014 0.6441412 ]\r\n>   [0.17763907 0.53368616 0.48083413 0.1984492 ]\r\n>   ...\r\n>   [0.55963373 0.806476   0.32869753 0.5672232 ]\r\n>   [0.7234199  0.45786685 0.3743322  0.25172007]\r\n>   [0.662722   0.8848254  0.75358796 0.9891881 ]]]\r\n> ```\r\n\r\nHi abattery, Thanks a ton for checking the code and responding.\r\n\r\nHave you done any modifications in the code? Are you using the same model which I shared?\r\n\r\nI tried tf-nightly(cpu), but the system freezed and jupyter kernel died after sometime.\r\n\r\nDid you try the GPU version? I have to change my CUDA to 11 for trying tf-nightly in gpu.\r\n\r\nCould you please share the system details where you tried this code?\r\n", "I ran the shared model on the shared colab without any modifications after installing tf-nightly version via ```!pip install tf-nightly```. \r\n", "I think the model is quite big and slow to run. However, the model itself is runnable.", "Thanks again. I am able to execute the code in Colab and get the results. It took a long time though(~10 minutes). I will check how to improve the inference time. Please suggest if you have any ideas.\r\n\r\nOnce again, Thank you very much for helping \ud83d\udc4d ", "If you need bounding boxes, you can check out the SSD models on the [TF2 detection zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). Mask-RCNN is not very well supported on TFLite, mainly because the models in the TF1 detection zoo are too large to perform well on-device.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45977\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45977\">No</a>\n", "Hi @abattery @jiks-hue I have tried to run my tflite model with tf-nightly multiple times (with the above code). On colab the runtime gets disconnected and locally I get a segmentation fault. Is there any reason it is not working for me?"]}, {"number": 45976, "title": " No OpKernel was registered to support Op 'VarHandleOp' used by {{node dense/kernel}}with these attrs: [shape=[1,1], shared_name=\"dense/kernel\", _class=[\"loc:@dense/kernel\"], dtype=DT_FLOAT, container=\"\"]", "body": "I have created a tensorflow 1.13.1 model and try to used it in an android app. When I \"init\" the variables running   sess.runner().addTarget(\"init\").run(); I got this error: No OpKernel was registered to support Op 'VarHandleOp' used by {{node dense/kernel}}with these attrs: [shape=[1,1], shared_name=\"dense/kernel\", _class=[\"loc:@dense/kernel\"], dtype=DT_FLOAT, container=\"\"]\r\n\r\nThis is the code I used to create the graph.pb file:\r\n\r\nmodel = tf.keras.models.Sequential([tf.keras.layers.Dense(1, input_shape=(1, )),\r\ntf.keras.layers.Dense(25, activation=tf.keras.activations.relu),  \r\ntf.keras.layers.Dense(1, activation=tf.keras.activations.relu)])\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.mean_squared_error)\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n\r\ninit = tf.global_variables_initializer()\r\nsaver_def = tf.train.Saver().as_saver_def()\r\nwith open('graph.pb', 'wb') as f:\r\n  f.write(tf.get_default_graph().as_graph_def().SerializeToString())\r\n\r\nprint('Operation to initialize variables:       ', init.name)\r\nprint('Tensor to be fed for checkpoint filename:', saver_def.filename_tensor_name)\r\nprint('Operation to save a checkpoint:          ', saver_def.save_tensor_name)\r\nprint('Operation to restore a checkpoint:       ', saver_def.restore_op_name)\r\nprint('Trainable variables: ', tf.trainable_variables())\r\n\r\nOperation to initialize variables:        init\r\nTensor to be fed for checkpoint filename: save/Const:0\r\nOperation to save a checkpoint:           save/control_dependency:0\r\nOperation to restore a checkpoint:        save/restore_all\r\nTrainable variables:  [<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32>, <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32>, <tf.Variable 'dense_1/kernel:0' shape=(1, 25) dtype=float32>, <tf.Variable 'dense_1/bias:0' shape=(25,) dtype=float32>, <tf.Variable 'dense_2/kernel:0' shape=(25, 1) dtype=float32>, <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32>]\r\n\r\nThe tensorflow android version is:  'org.tensorflow:tensorflow-android:1.13.1'\r\n\r\nIt worked with other linnear regression model but I dont know what is wrong,\r\n\r\nRegards,\r\nAlejandro.", "comments": ["@alejandroaguileraalcalde-ing \r\n\r\nCan you try with latest stable version 2.4 or nightly version and see if the issue still persists. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45975, "title": "tf.debugging.assert_type raising error for wrong reason", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary (pip install)\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n```\r\n>>> tf.debugging.assert_type(tf.constant(0.0), tf_type=(tf.float32,))\r\nTypeError:  tensor must be of type (tf.float32,)\r\n```\r\nis incorrect.\r\n\r\n**Describe the expected behavior**\r\nError should be raised because of incorrect argument type for `tf_type` (which needs to be a tf float type, not an iterable of them), not because tensor is not of type `(tf.float32,)`", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/e97a3e0f3be52cfb7dbb5b4051d9d0fd/45975.ipynb). Thanks!", "Added a PR #46013 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45975\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45975\">No</a>\n"]}, {"number": 45973, "title": "grpc 1.32.0 doesn't support python 3.9 in windows 10, dependency should be updated to 1.34.0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.9.1\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source):  Visual Studio 2019\r\n- CUDA/cuDNN version: 11.2 / 8.0.5\r\n- GPU model and memory:\r\nRTX 3070 GDDR6 8GB\r\n\r\n\r\n**Describe the problem**\r\ncannot build grpcio 1.32.0\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build and install\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\ngrpcio 1.32.0 does not support python 3.9.1 in Windows 10\r\n\r\ntensorflow dependency should be updated as with grpcio 1.34.0", "comments": ["We don't have yet an official build for 3.9 but we plan to have one next quarter. When we get this,we will also update the dependencies.\r\n\r\nCurrently, protobuf, grpc and gast are the dependencies that need to be updated to have 100% tests green.", "I'am trying the grpcio upgrade with https://github.com/tensorflow/tensorflow/pull/46019", "I think that we could close this as dup of https://github.com/tensorflow/tensorflow/issues/44485", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45973\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45973\">No</a>\n"]}, {"number": 45972, "title": "Android Benchmark tool Build Failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: master branch\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: GTX 1660 and 6GB RAM\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nBuild failed when building android benchmark tool for tflite models.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI just ran this command \r\n\r\n> bazel build -c opt --config=android_arm64 tensorflow/lite/tools/benchmark/android:benchmark_model\r\n\r\n**Any other info / logs**\r\n> INFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=204\r\nINFO: Reading rc options for 'build' from /home/ram/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/ram/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file /home/ram/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/ram/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:android_arm64 in file /home/ram/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\nINFO: Found applicable config definition build:android in file /home/ram/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nERROR: /home/ram/.cache/bazel/_bazel_ram/a1828cafbf783c13dba0538b4a59550c/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'\r\nERROR: Analysis of target '//tensorflow/lite/tools/benchmark/android:benchmark_model' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\nINFO: Elapsed time: 0.180s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (2 packages loaded, 1 target configured)\r\n\r\n", "comments": ["> _Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template_\r\n> \r\n> **System information**\r\n> \r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n> * TensorFlow installed from (source or binary): Source\r\n> * TensorFlow version: master branch\r\n> * Python version: 3.8\r\n> * Installed using virtualenv? pip? conda?:\r\n> * Bazel version (if compiling from source): 3.1.0\r\n> * GCC/Compiler version (if compiling from source): 7.5.0\r\n> * CUDA/cuDNN version:\r\n> * GPU model and memory: GTX 1660 and 6GB RAM\r\n> \r\n> **Describe the problem**\r\n> \r\n> Build failed when building android benchmark tool for tflite models.\r\n> \r\n> **Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n> I just ran this command\r\n> \r\n> > bazel build -c opt --config=android_arm64 tensorflow/lite/tools/benchmark/android:benchmark_model\r\n> \r\n> **Any other info / logs**\r\n> \r\n> > INFO: Options provided by the client:\r\n> > Inherited 'common' options: --isatty=1 --terminal_columns=204\r\n> > INFO: Reading rc options for 'build' from /home/ram/tensorflow/.bazelrc:\r\n> > Inherited 'common' options: --experimental_repo_remote_exec\r\n> > INFO: Reading rc options for 'build' from /home/ram/tensorflow/.bazelrc:\r\n> > 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\n> > INFO: Found applicable config definition build:short_logs in file /home/ram/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\n> > INFO: Found applicable config definition build:v2 in file /home/ram/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n> > INFO: Found applicable config definition build:android_arm64 in file /home/ram/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n> > INFO: Found applicable config definition build:android in file /home/ram/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\n> > ERROR: /home/ram/.cache/bazel/_bazel_ram/a1828cafbf783c13dba0538b4a59550c/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'\r\n> > ERROR: Analysis of target '//tensorflow/lite/tools/benchmark/android:benchmark_model' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\n\r\nIt looks like your overall building environment isn't set properly. Please follow https://www.tensorflow.org/lite/guide/build_android for details. \r\n\r\nOr one could use pre-built binaries mentioned in https://www.tensorflow.org/lite/performance/measurement as you've discovered in https://github.com/tensorflow/tensorflow/issues/45986\r\n\r\n> > INFO: Elapsed time: 0.180s\r\n> > INFO: 0 processes.\r\n> > FAILED: Build did NOT complete successfully (2 packages loaded, 1 target configured)\r\n\r\n", "Yeah after raising this issue I got working binary from Tensorflow website. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45972\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45972\">No</a>\n"]}, {"number": 45971, "title": "Attempting to use uninitialized value ", "body": "Hello, \r\nI have a trained tensorflow model which is trained by tensorflow v1.14 and the model is saved as meta graph model. What I'm trying to do is to convert this model to Keras since I cannot work with the model with SHAP package. The Shap package doesn't support the meta graph tensorflow model. So I want to convert it to Keras. I have done this with another TF model with VGG architecture and this works perfectly fine.  For this purpose I create the architecture in Keras and then I set the weight and bias values to the new model. \r\nNow I have a trained model with Resnet-18 architecture. But the problem is when I want to get the weights I get this error: \r\n\r\nAttempting to use uninitialized value scale2/block1/A/weights\r\n\t [[{{node _retval_scale2/block1/A/weights_0_0}}]]\r\n\r\nThis works fine when I want to get the non-nested layer's values. \r\n\r\n\r\n```\r\nwith tf.compat.v1.Session() as sess, tf.device('/device:gpu:0'):\r\n    saver = tf.compat.v1.train.import_meta_graph('./snapshot-110.meta')\r\n    saver.restore(sess,tf.train.latest_checkpoint('./'))\r\n    graph = tf.compat.v1.get_default_graph()\r\n\r\n    vars_global = tf.compat.v1.global_variables()\r\n    sess.as_default()\r\n    model_vars = {}\r\n    for var in vars_global:\r\n        model_vars[var.name] = var.eval()\r\n```", "comments": ["@haniyeka \r\nWe see that you are using an old version of tf 1.x is not supported, can you please try on 2.x and let us know if you face any issues.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45969, "title": "Apollo3 project upload issue ", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Windows 10\r\n- Arduino IDE\r\n- TensorFlow Lite 2.1\r\n- Sparkfun Apollo3 blue\r\n\r\nI make all the steps correct from the sparkfun site to i can run the apollo3 from arduino ide.\r\n\r\nWhen i upload the micro speech project into apollo3 from Arduino IDE i get this message #include \"tensorflow/lite/c/c_api_internal.h\"\r\nHow i can resolve this ?\r\n", "comments": ["sorry the full error message is 'tensorflow/lite/c/c_api_internal.h' file not found", "@aloizo03,\r\nCould you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same error.\r\n\r\nAlso in order to expedite the trouble-shooting process, please provide the exact sequence of commands / steps that you executed before running into the error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45969\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45969\">No</a>\n"]}, {"number": 45968, "title": "Package build failure", "body": "The package build using \r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n\r\nfails with the message: \r\n\r\nERROR: \r\n//tensorflow/tools/pip_package: licenses depends on @aws//:LICENSE in repository @aws which failed to fetch. no such package '@aws//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz, https://github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz] to /private/var/tmp/_bazel_kevinlano/58ba076afb76e123cd689325196ab4ac/external/aws/temp6281506492855719619/1.7.336.tar.gz: Tried to reconnect at offset 15,240,407 but server didn't support it\r\n\r\n\r\n", "comments": ["having the same error on Mac OS 10.15.7 with Docker 20.10.0.\r\nbranch: r2.4\r\n\r\nafter running the build command\r\n`tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\\r\n    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh`", "This is likely a connectivity issue. Does retrying solve the issue?", "No, I tried it again on an EC2 instance (Ubuntu OS) and got the same error.", "What is the Bazel version and what version of code you try to build (tag/commit)", "I follow the instructions at https://www.tensorflow.org/install/source_rpi#python-3.7 for **Python 3.7**:\r\n\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\\r\n    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n```\r\n\r\nI try building **version 2.4**:\r\n\r\n`git checkout r2.4`\r\n\r\nBazel version in the Docker image is **3.1.0**:\r\n\r\n```\r\n# bazel version\r\nBuild label: 3.1.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Apr 22 10:32:27 2020 (1587551547)\r\nBuild timestamp: 1587551547\r\nBuild timestamp as int: 1587551547\r\n```\r\n", "@kevinlano \r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45968\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45968\">No</a>\n"]}, {"number": 45967, "title": "tflite android gpu delegate init error, what does it mean?  Init: MUL: 1x1088x1x1  cannot be reduced to linear.", "body": "@tensorflow/micro\r\n\r\n\r\n\r\n**Describe the problem**\r\nerror log:\r\njava.lang.IllegalArgumentException: Internal error: Failed to apply delegate: \r\nTfLiteGpuDelegate Init: MUL: 1x1088x1x1  cannot be reduced to linear.\r\nTfLiteGpuDelegate Prepare: delegate is not initialized\r\n Node number 23608 (TfLiteGpuDelegateV2) failed to prepare.\r\n\r\nwhen i run my own model convert from pytorch to TFLite.\r\n\r\ndevice: HUAWEI Mate20\r\ndependence:\r\nimplementation('org.tensorflow:tensorflow-lite:0.0.0-nightly')\r\nimplementation('org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly')\r\nimplementation('org.tensorflow:tensorflow-lite-support:0.0.0-nightly')\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nWhen I run the model on the android GPU\r\n", "comments": ["@uchihaltachi \r\n\r\nPlease, share simple standalone code to reproduce the issue in our environment. Thanks!", "Closing this issue since it's addressed in another [thread](https://github.com/tensorflow/tensorflow/issues/37102#issuecomment-754889708). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45967\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45967\">No</a>\n"]}, {"number": 45966, "title": "sparse_categorical_crossentropy and temporal sample weight", "body": "I am training a (BERT-like) language model with Keras, using _sparse_categorical_crossentropy_ loss. As with any training of language models, I have input sequences of different lengths, meaning that I need to add padding at the ends of most input sequences in order for them to fit a uniform batch size. When it comes to the outputs, however, I do not want to force the model to learn unnecessary padding tokens; rather, I'd just like it to ignore the outputs corresponding to positions where the input has a padding token.\r\n\r\nThe most sensible solution I was able to think of for doing that was using a _sample_weight_ with zeros at positions corresponding to padding in the input, and ones at all other (actual) positions. This requires setting _sample_weight_mode = 'temporal'_.\r\n\r\nThe problem is that _sparse_categorical_crossentropy_ doesn't seem to support that, and I get the following error:\r\n_**ValueError: Found a sample_weight array for an input with shape (8108, 512). Timestep-wise sample weighting (use of sample_weight_mode=\"temporal\") is restricted to outputs that are at least 3D, i.e. that have a time dimension.**_\r\n\r\nIs there a way of using temporal sample weight with _sparse_categorical_crossentropy_  in Keras? \r\nAlternatively, is there another approach I should consider to achieve what I want, or any workaround I could resort to (for making Keras ignore positions with padding in the input during training)?\r\n\r\nThank you!", "comments": ["@nadavbra,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "**I use TensorFlow 2.0.0.**\r\n\r\nHere is a simple toy example reproducing the problem:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ndataset_size = 3200\r\nseq_len = 20\r\nvocab_size = 10\r\n\r\nX = np.random.randint(0, vocab_size, (dataset_size, seq_len))\r\nY = X\r\n\r\ninput_layer = keras.layers.Input(shape = (seq_len,), dtype = np.int32)\r\nembedding_layer = keras.layers.Embedding(vocab_size, 5)(input_layer)\r\noutput_layer = keras.layers.Dense(vocab_size, activation = 'softmax')(embedding_layer)\r\n\r\nmodel = keras.models.Model(inputs = input_layer, outputs = output_layer)\r\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer = keras.optimizers.Adam(), sample_weight_mode = 'temporal')\r\nmodel.fit(X, Y)\r\n```\r\n\r\nRunning this gives me the error:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-0ba618c51df7> in <module>\r\n     16 model = keras.models.Model(inputs = input_layer, outputs = output_layer)\r\n     17 model.compile(loss = 'sparse_categorical_crossentropy', optimizer = keras.optimizers.Adam(), sample_weight_mode = 'temporal')\r\n---> 18 model.fit(X, Y)\r\n\r\n/usr/local/tensorflow/avx-avx2-gpu/2.0.0/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\n/usr/local/tensorflow/avx-avx2-gpu/2.0.0/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    640         steps=steps_per_epoch,\r\n    641         validation_split=validation_split,\r\n--> 642         shuffle=shuffle)\r\n    643 \r\n    644     if validation_data:\r\n\r\n/usr/local/tensorflow/avx-avx2-gpu/2.0.0/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2528           training_utils.standardize_weights(ref, sw, cw, mode)\r\n   2529           for (ref, sw, cw, mode) in zip(y, sample_weights, class_weights,\r\n-> 2530                                          feed_sample_weight_modes)\r\n   2531       ]\r\n   2532       # Check that all arrays have the same length.\r\n\r\n/usr/local/tensorflow/avx-avx2-gpu/2.0.0/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in <listcomp>(.0)\r\n   2527       sample_weights = [\r\n   2528           training_utils.standardize_weights(ref, sw, cw, mode)\r\n-> 2529           for (ref, sw, cw, mode) in zip(y, sample_weights, class_weights,\r\n   2530                                          feed_sample_weight_modes)\r\n   2531       ]\r\n\r\n/usr/local/tensorflow/avx-avx2-gpu/2.0.0/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py in standardize_weights(y, sample_weight, class_weight, sample_weight_mode)\r\n    884     if len(y.shape) < 3:\r\n    885       raise ValueError('Found a sample_weight array for '\r\n--> 886                        'an input with shape ' + str(y.shape) + '. '\r\n    887                        'Timestep-wise sample weighting (use of '\r\n    888                        'sample_weight_mode=\"temporal\") is restricted to '\r\n\r\nValueError: Found a sample_weight array for an input with shape (3200, 20). Timestep-wise sample weighting (use of sample_weight_mode=\"temporal\") is restricted to outputs that are at least 3D, i.e. that have a time dimension.\r\n```\r\n\r\nNote that everything works fine when removing the `sample_weight_mode = 'temporal'` option.", "@nadavbra,\r\nI was able to reproduce the error with [TF v2.0](https://colab.research.google.com/gist/amahendrakar/1b34de0904e872a39ec7d02f4177b1cf/45966-2-0.ipynb). \r\n\r\nHowever, the error seems to be fixed with the latest stable version [v2.4](https://colab.research.google.com/gist/amahendrakar/595533f687d7b7a0a60895e5e30280a1/45966.ipynb). I was able to run the code without any issues with it. Please check the linked gist for reference. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45966\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45966\">No</a>\n"]}, {"number": 45965, "title": "Download dependent 404\uff08Build did NOT complete successfully\uff09", "body": "INFO: Found applicable config definition build:linux in file /home/hortor/Desktop/data1/work/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/hortor/Desktop/data1/work/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\nDEBUG: Repository io_bazel_rules_go instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /home/hortor/.cache/bazel/_bazel_hortor/c4d98c77c9b8b7dcb6bae627296ef8a4/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /home/hortor/.cache/bazel/_bazel_hortor/c4d98c77c9b8b7dcb6bae627296ef8a4/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nWARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow/tools/lib_package:libtensorflow (190 packages loaded, 20055 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/hortor/.cache/bazel/_bazel_hortor/c4d98c77c9b8b7dcb6bae627296ef8a4/external/aws/BUILD.bazel:12:1: C++ compilation of rule '@aws//:aws' failed (Exit 1): gcc failed: error executing command \r\n  (cd /home/hortor/.cache/bazel/_bazel_hortor/c4d98c77c9b8b7dcb6bae627296ef8a4/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/external/aws/_objs/aws/S3Client.pic.d '-frandom-seed=bazel-out/host/bin/external/aws/_objs/aws/S3Client.pic.o' -fPIC -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/host/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/host/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/host/bin/external/aws-checksums -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/host/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/host/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/host/bin/external/aws-checksums/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -g0 -w '-march=native' -g0 '-std=c++14' -DENABLE_OPENSSL_ENCRYPTION '-DAWS_SDK_VERSION_MAJOR=1' '-DAWS_SDK_VERSION_MINOR=7' '-DAWS_SDK_VERSION_PATCH=266' -DOPENSSL_IS_BORINGSSL -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/aws/aws-cpp-sdk-s3/source/S3Client.cpp -o bazel-out/host/bin/external/aws/_objs/aws/S3Client.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\ngcc: fatal error: Killed signal terminated program cc1plus\r\ncompilation terminated.\r\nTarget //tensorflow/tools/lib_package:libtensorflow failed to build\r\nINFO: Elapsed time: 2784.337s, Critical Path: 123.30s\r\nINFO: 3070 processes: 3070 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["having the same error on Mac OS 10.15.7 with Docker 20.10.0.\r\nbranch: r2.4\r\n\r\nafter running the build command\r\n`tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\ tensorflow/tools/ci_build/pi/build_raspberry_pi.sh`", "@smartbooks \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, system info, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45965\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45965\">No</a>\n", "issue still exists "]}, {"number": 45963, "title": "NotFoundError: '_MklMatMul'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nc0-deeplearning-tf2-2-4-tpu-v20201215-debian-10\r\nDescription\r\nGoogle, Deep Learning Image: TensorFlow 2.4, m60 TPU, A debian-10 Linux based image with TensorFlow 2.4 pre-installed.\r\nLocation\r\nasia (Asia Pacific), eu (European Union), us (United States)\r\nLabels\r\nrelease : m60\r\nCreation time\r\nDec 16, 2020, 5:10:12 PM UTC-08:00\r\nFamily\r\ntf2-2-4-tpu-debian-10\r\nEncryption type\r\nGoogle managed\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nAfter initializing the tpu - I'm using version 2.4 on the tpu as well, tf.matmul doesn't work anymore.\r\n\r\nI'm initializing the tpu with:\r\n\r\n```\r\ndef get_strategy():\r\n\r\n    try:\r\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver('tpu-3')\r\n        print('Running on TPU ', tpu.master())\r\n    except:\r\n        tpu = None\r\n\r\n    if tpu:\r\n        tf.config.experimental_connect_to_cluster(tpu)\r\n        tf.tpu.experimental.initialize_tpu_system(tpu)\r\n        strategy = tf.distribute.TPUStrategy(tpu)\r\n    \r\n    else:\r\n        strategy = tf.distribute.get_strategy()\r\n        for d in tf.config.list_physical_devices():\r\n            print(d)\r\n            \r\n    return strategy\r\n\r\nstrategy = get_strategy()\r\n```\r\n\r\nHere is the full error:\r\n\r\n```\r\nNotFoundError: '_MklMatMul' is neither a type of a primitive operation nor a name of a function registered in binary running on n-1f5a3a66-w-0. Make sure the operation or function is registered in the binary running in this process.\r\n```\r\n\r\nIt works fine on the cpu before the tpu is initialized.", "comments": ["Reverting to 2.2 works", "@CalebEverett,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "Also, please go through [this comment](https://github.com/tensorflow/tensorflow/issues/45317#issuecomment-738273941) from similar issue #45317 and let us know if it helps. Thanks!", "1. Create a GCP VM - from the console I selected a linux deeplearning image and then the debian 10 one optimized for tpu. Here is the source image: `c0-deeplearning-tf2-2-4-tpu-v20201215-debian-10`\r\n2. Create a v3-8 tpu with version 2.4.0 of the software installed\r\n3. Import tensorflow and run a tf.matmul calculation:\r\n```\r\nimport tensorflow as tf\r\ntf.matmul(tf.ones((4,4)), tf.ones((4,4)))\r\n\r\n\r\n<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\r\narray([[4., 4., 4., 4.],\r\n       [4., 4., 4., 4.],\r\n       [4., 4., 4., 4.],\r\n       [4., 4., 4., 4.]], dtype=float32)>\r\n```\r\nInitialize tpu\r\n```\r\n    try:\r\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver('tpu-3')\r\n        print('Running on TPU ', tpu.master())\r\n    except:\r\n        tpu = None\r\n\r\n    if tpu:\r\n        tf.config.experimental_connect_to_cluster(tpu)\r\n        tf.tpu.experimental.initialize_tpu_system(tpu)\r\n        strategy = tf.distribute.TPUStrategy(tpu)\r\n    \r\n    else:\r\n        strategy = tf.distribute.get_strategy()\r\n        for d in tf.config.list_physical_devices():\r\n            print(d)\r\n\r\n\r\nRunning on TPU  grpc://10.211.162.226:8470\r\nINFO:tensorflow:Initializing the TPU system: tpu-3\r\nINFO:tensorflow:Initializing the TPU system: tpu-3\r\nINFO:tensorflow:Clearing out eager caches\r\nINFO:tensorflow:Clearing out eager caches\r\nINFO:tensorflow:Finished initializing TPU system.\r\nINFO:tensorflow:Finished initializing TPU system.\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\n```\r\n\r\n4. Run the tf.matmul calculation again\r\n```\r\ntf.matmul(tf.ones((4,4)), tf.ones((4,4)))\r\n\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n/opt/conda/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj)\r\n    700                 type_pprinters=self.type_printers,\r\n    701                 deferred_pprinters=self.deferred_printers)\r\n--> 702             printer.pretty(obj)\r\n    703             printer.flush()\r\n    704             return stream.getvalue()\r\n\r\n/opt/conda/lib/python3.7/site-packages/IPython/lib/pretty.py in pretty(self, obj)\r\n    392                         if cls is not object \\\r\n    393                                 and callable(cls.__dict__.get('__repr__')):\r\n--> 394                             return _repr_pprint(obj, self, cycle)\r\n    395 \r\n    396             return _default_pprint(obj, self, cycle)\r\n\r\n/opt/conda/lib/python3.7/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)\r\n    698     \"\"\"A pprint that just redirects to the normal repr function.\"\"\"\r\n    699     # Find newlines and replace them with p.break_()\r\n--> 700     output = repr(obj)\r\n    701     lines = output.splitlines()\r\n    702     with p.group():\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in __repr__(self)\r\n   1015   def __repr__(self):\r\n   1016     return \"<tf.Tensor: shape=%s, dtype=%s, numpy=%s>\" % (\r\n-> 1017         self.shape, self.dtype.name, numpy_text(self, is_repr=True))\r\n   1018 \r\n   1019   def __len__(self):\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in shape(self)\r\n   1173         # `_tensor_shape` is declared and defined in the definition of\r\n   1174         # `EagerTensor`, in C.\r\n-> 1175         self._tensor_shape = tensor_shape.TensorShape(self._shape_tuple())\r\n   1176       except core._NotOkStatusException as e:\r\n   1177         six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n\r\nNotFoundError: '_MklMatMul' is neither a type of a primitive operation nor a name of a function registered in binary running on n-1f5a3a66-w-0. Make sure the operation or function is registered in the binary running in this process.\r\n```", "@CalebEverett,\r\nI was able to run the code without any issues on TF v2.4, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/76ddd34f8853e0025d174121e2b7650d/45963.ipynb).\r\n\r\nCould you please try running the code in a new virtual environment and check if you are facing the same error. Thanks!", "That looks like its working - thank you.", "@CalebEverett,\r\nThank you for the update. Closing the issue as it is resolved. Please feel free to re-open if necessary.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45963\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45963\">No</a>\n", "Problem still persists and it lies in the image provided by Google Cloud itself. The solution is to always be defensive to any pre-installed tensorflow distribution and install one manually.\r\n\r\nQuick fix is:\r\n`pip install --upgrade --force-reinstall tensorflow`"]}, {"number": 45961, "title": "Programm ends Drasicly", "body": "Works fine with other Code\r\n\r\nI try to run a convolution script for my network, whan i was running it on my CPU it was fine, then i installed CUDA and CUDNN and now it stopped working. I tried searching for the issue got a few got a few fixes, but it still dosn't work. Now it only says:\r\n2020-12-24 14:31:49.491561: I\r\nand not: W or E ; so i guess thats a good thing?\r\n\r\n**System information**\r\n- OS: Windows 10\r\n- TensorFlow: 2.4.0\r\n- Python version: 3.8\r\n- Installed using: pip install tensorflow-gpu\r\n- CUDA/cuDNN version: 11.0 / 8.0.5.39\r\n- GPU model and memory: 1050 Mobile 4GB\r\n\r\nIm using pycharm IDE:\r\n\r\nOutput:\r\n2020-12-24 14:31:42.522978: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n3-conv-64-nodes-0-dense-1608816708\r\n2020-12-24 14:31:48.503990: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-24 14:31:48.505733: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-12-24 14:31:49.421165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1\r\ncoreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-12-24 14:31:49.421594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-24 14:31:49.455737: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-24 14:31:49.456022: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-24 14:31:49.462110: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-24 14:31:49.467640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-24 14:31:49.480366: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-24 14:31:49.484829: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-24 14:31:49.486190: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-24 14:31:49.486542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-12-24 14:31:49.487316: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-24 14:31:49.488676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1\r\ncoreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-12-24 14:31:49.489674: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-24 14:31:49.490059: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-24 14:31:49.490445: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-24 14:31:49.490811: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-24 14:31:49.491164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-24 14:31:49.491561: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-24 14:31:49.491986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-24 14:31:49.492414: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-24 14:31:49.492930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-12-24 14:31:50.647138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-24 14:31:50.647372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-12-24 14:31:50.647516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-12-24 14:31:50.647835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2989 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-12-24 14:31:50.649096: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-24 14:31:51.001350: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\r\n2020-12-24 14:31:51.001582: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\r\n2020-12-24 14:31:51.001816: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs\r\n2020-12-24 14:31:51.004339: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cupti64_110.dll\r\n2020-12-24 14:31:51.158879: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\r\n2020-12-24 14:31:51.159215: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\r\n2020-12-24 14:31:52.099653: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\nEpoch 1/10\r\n2020-12-24 14:31:53.954017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-24 14:31:54.522501: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-24 14:31:54.544413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n\r\nProcess finished with exit code -1073740791 (0xC0000409)\r\n\r\n\r\n(If you'd need any additional information ask)\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45961\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45961\">No</a>\n"]}, {"number": 45960, "title": "A long period of GPU/CPU idle time in timeline", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux version 3.10.0\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tensorflow-gpu==1.15\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: Tesla-V100, 16G\r\n\r\n**Describe the current behavior**\r\nThere is a long period of GPU and CPU idle time in timeline. The model is trained in one GPU and there is no problem of network transmission.\r\n\r\n**Describe the expected behavior**\r\nThere should not be such a long period of idle time.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n![\u5c4f\u5e55\u5feb\u7167 2020-12-24 \u4e0b\u53488 17 36](https://user-images.githubusercontent.com/5723913/103089708-c2c18d00-4629-11eb-9059-43aeb8b20136.png)\r\n\r\n", "comments": ["@lonway,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@lonway Did you manage to solve this?"]}, {"number": 45959, "title": "Nontrainable Custom Convolution at the start of model", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\nI am creating a CNN model ,but at the start I want my image to be passed through high pass filter(based on a matrix convolution) how to do that,or How to add a custom non trainable Convolution layer with a constant matrix at the start of model\r\n.Please refer GNCNN model , how to do the HIgh pass filter part in keras , using flow from directory method", "comments": ["@876arham,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45959\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45959\">No</a>\n"]}, {"number": 45957, "title": "Is it possible to build tf2.4 with cuda 10.0?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 10.0.130, cudnn 7.5\r\n- GPU model and memory: K80\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI would like to know if is it possible to build newest tensorflow against cuda 10.0? Is there any workaround with cuda 10.0? It looks like libcublasLt.so.10.0 is missing in cuda10.0 ...\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nDuring the build I get following error:\r\n\r\n    $: bazel build //tensorflow/tools/pip_package:build_pip_package\r\n    ....\r\n    Repository command failed\r\n    No library found under: /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublasLt.so.10.0\r\n    INFO: Elapsed time: 1.836s\r\n    INFO: 0 processes.\r\n    FAILED: Build did NOT complete successfully (0 packages loaded)\r\n        currently loading: tensorflow/tools/pip_package\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Cospel,\r\nEvery TensorFlow release is compatible with a certain CUDA and cuDNN version. For more information, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu). \r\n\r\nTensorFlow v2.4 is compatible with CUDA 11.0 and cuDNN 8, hence you will face issues while building with CUDA 10.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45957\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45957\">No</a>\n"]}, {"number": 45956, "title": "Using Tensorflow 2.3 in r (conda environment) to reproduce the examples of Platypus R-package", "body": "the example included in Platypus package: https://github.com/maju116/platypus/blob/yolo3_fix/examples/Blood%20Cell%20Detection/Blood-Cell-Detection.md\r\n\r\nmy session trying to reproduce the result:\r\n> history <- blood_yolo %>%\r\n+   yolo3_fit_generator(\r\n+     generator = train_blood_yolo_generator,\r\n+     epochs = 3,\r\n+     steps_per_epoch = 3,\r\n+     validation_generator = valid_blood_yolo_generator,\r\n+     validation_steps_per_epoch = 9,\r\n+     model_filepath = \"development/BCCD/blood_w.hdf5\",\r\n+     save_best_only = TRUE,\r\n+     monitor = \"val_loss\"\r\n+   )\r\nWARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x0000024526D8AB80> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x00000245244990D0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x00000245244E0EE0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x0000024561F4C280> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x0000024561F97D30> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <function make_python_function.<locals>.python_function at 0x0000024562187C10> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert", "comments": ["@konstanto21,\r\nYou can safely ignore the warnings and suppress the messages by changing the log level at the start of the program  \r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\nimport tensorflow as tf\r\n```\r\n\r\nor you can set the [autograph verbosity](https://www.tensorflow.org/api_docs/python/tf/autograph/set_verbosity\r\n) level using the below the below code \r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nos.environ['AUTOGRAPH_VERBOSITY'] = 1\r\n```\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45956\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45956\">No</a>\n"]}, {"number": 45955, "title": "Tf.contrib.layers.Bias_add fails to operate due to uninitialized value", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from source:\r\n- TensorFlow version (use command below): 1.14/1.15\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10\r\n\r\n\r\n**Describe the current behavior**\r\nI want to add bias layers to slicing parts of a reshape layer. I conduct it with `tf.contrib.layers.bias_add` and then concatenate them back. However, I can't do that and it keeps raising the bug of uninitialized for the `Bias_add op` even I have given it\r\n\r\nThe code I use is below:\r\n\r\n   ```\r\n input = keras.Input(shape=[shape, shape, 256])\r\n\r\n    kernel_init = tf.keras.initializers.RandomNormal(0.0, 0.01)\r\n\r\n    # seg_layer = _seg_layer(output_filters,num, f_l,kernel_init,bias_init)\r\n    head = keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)(input)\r\n    head = keras.layers.ReLU()(head)\r\n    for _ in range(3):\r\n        head = keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)(head)\r\n        head = keras.layers.ReLU()(head)\r\n    head = keras.layers.Conv2D(\r\n        output_filters,\r\n        3,\r\n        1,\r\n        padding=\"same\",\r\n        kernel_initializer=kernel_init,\r\n        use_bias=False,\r\n    )(head)\r\n    # head = tf.identity(head)\r\n    head = keras.layers.Reshape([-1, int(output_filters/9) ])(head)\r\n    slicing = int(f_l/num)\r\n    segment = []\r\n    for i in range(num):\r\n        tempt = head[:,slicing*i:slicing*(i+1),:]\r\n        tempt = tf.contrib.layers.bias_add(\r\n            tempt,\r\n            activation_fn=None,\r\n            initializer=tf.zeros_initializer(),\r\n            regularizer=None,\r\n            reuse=None,\r\n            variables_collections=None,\r\n            outputs_collections=None,\r\n            trainable=True,\r\n            data_format='NHWC',\r\n            scope=None\r\n        )\r\n        segment.append(tempt)\r\n    output = keras.layers.Concatenate(axis=1)(segment)\r\n    model = keras.models.Model(input,output)\r\n```\r\n\r\nError:\r\n```\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found.\r\n  (0) Failed precondition: Attempting to use uninitialized value BiasAdd/biases\r\n         [[{{node BiasAdd/biases/read}}]]\r\n         [[BiasAdd/biases/read/_687]]\r\n  (1) Failed precondition: Attempting to use uninitialized value BiasAdd/biases\r\n         [[{{node BiasAdd/biases/read}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```", "comments": ["@dtlam26 \r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue. \r\n\r\nIn TF 2.4 you can try with `tf.nn.bias_add` as contrib functions will not be supported in TF 2.x\r\nThanks!", "Does the `tf.nn.bias_add` give fixed bias value for the tensor? Because according to the description of both 1.15 and 2.4, I am not sure they can learn", "@dtlam26 \r\n\r\nCan you please refer the [link](https://www.tensorflow.org/agents/api_docs/python/tf_agents/keras_layers/bias_layer?hl=fr) and see if it help you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45955\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45955\">No</a>\n"]}, {"number": 45954, "title": "AttributeError: module 'tensorflow._api.v1.config' has no attribute 'list_physical_devices'", "body": "c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\r\n  return f(*args, **kwds)\r\nc:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\r\n  return f(*args, **kwds)\r\nc:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\r\n  return f(*args, **kwds)\r\nc:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\r\n  return f(*args, **kwds)\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Gwinivac\\.conda\\envs\\chatboty\\Scripts\\rasa.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\site-packages\\rasa\\__main__.py\", line 115, in main\r\n    rasa.telemetry.initialize_error_reporting()\r\n  File \"c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\site-packages\\rasa\\telemetry.py\", line 230, in decorated\r\n    return f(*args, **kwargs)\r\n  File \"c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\site-packages\\rasa\\telemetry.py\", line 651, in initialize_error_reporting\r\n    default_context = _default_context_fields()\r\n  File \"c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\site-packages\\rasa\\telemetry.py\", line 480, in _default_context_fields\r\n    \"gpu\": len(tf.config.list_physical_devices(\"GPU\")),\r\n  File \"c:\\users\\gwinivac\\.conda\\envs\\chatboty\\lib\\site-packages\\tensorflow_core\\python\\util\\module_wrapper.py\", line 193, in __getattr__\r\n    attr = getattr(self._tfmw_wrapped_module, name)\r\nAttributeError: module 'tensorflow._api.v1.config' has no attribute 'list_physical_devices'\r\n\r\n\r\nrunning on:\r\ntensorflow version: 1.15.0\r\ninstalled using conda\r\n\r\n", "comments": ["@Mathias-Godwin,\r\n[tf.config.list_physical_devices](https://www.tensorflow.org/api_docs/python/tf/config/list_physical_devices) is a TensorFlow 2.x API, hence you are facing an error while running it on TensorFlow 1.x. Please take a look at [this gist](https://colab.research.google.com/gist/amahendrakar/8f878f458560dd7e00730128dde29fdf/45954.ipynb) for reference. \r\n\r\nAlso, I'd suggest you to update TensorFlow to v2.4 as TensorFlow 1.x is not actively supported. Thanks!", "> @Mathias-Godwin,\r\n> [tf.config.list_physical_devices](https://www.tensorflow.org/api_docs/python/tf/config/list_physical_devices) is a TensorFlow 2.x API, hence you are facing an error while running it on TensorFlow 1.x. Please take a look at [this gist](https://colab.research.google.com/gist/amahendrakar/8f878f458560dd7e00730128dde29fdf/45954.ipynb) for reference.\r\n> \r\n> Also, I'd suggest you to update TensorFlow to v2.4 as TensorFlow 1.x is not actively supported. Thanks!\r\n\r\nI found out a temporary solution\r\nhttps://github.com/RasaHQ/rasa/issues/7634#issuecomment-751234632\r\n\r\nAlso, Thanks for the update!", "@Mathias-Godwin,\r\nGlad the issue is resolved. Closing the issue, please feel free to re-open if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45954\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45954\">No</a>\n", "Use `tf.compat.v1.config.experimental.set_visible_devices` for TF 1.15 or below, instead of `tf.config.set_visible_devices` which is for TF >= 2.0", "On tf 1.15.0 Found it here: `tf.compat.v1.config.experimental.list_physical_devices('GPU')`"]}, {"number": 45953, "title": "Tensorflow Lite speed -> mobile vs desktop", "body": "As I am currently building a mobile application with pretty heavy duty object detection, I switched to tensorflow lite as a way to speed up computing. Everything is lightning fast as expected, but I am seeing 99% of the computation time being overpowered by the interpretation.invoke() function which is obviously critical for inference as described in https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter. \r\n\r\nWith over 1400 images to loop my model through, I am seeing the invoke() function take roughly 1 sec for each image extending the total inference time to over 20 min. One extremely important detail to note is that this is run through xcode on my desktop computer and not a mobile device. After extensive research, I have discovered to expect better performance on a mobile device with tensorflow lite.\r\n\r\nMy question is - are the performance gains from tensorflow lite on a mobile device exponentially faster than desktop? If my run time on desktop is over 20 min, is it even possible for this to decrease to a few minutes on mobile? Tons of forums out there are concluding tflite is obviously faster on mobile than desktop, but I am looking for logistics here. Generally HOW MUCH FASTER? Please help! Thanks!!", "comments": ["Great questions here @jrash33. As you rightly pointed out you should expect pretty good speedups with an optimized TensorFlow Lite model even on a commodity mobile phone. \r\n\r\nNow, coming to how much that all really depends on the phone you are running inference on and also the type of model you are using.\r\n\r\nType of model because if you are using the TFLite GPU Delegate then you need to ensure your model does not have dynamic-shaped inputs and also, you would need to minimize the number of ops not supported by the GPU delegate. TFLite GPU delegate works the best when your model is generally in float16 but they have recently added support for integer models. Phones come with different GPU chipsets sometimes and that can vary the extent of speed-up. \r\n\r\n[Here's](https://github.com/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/DeepLabV3/DeepLab_TFLite_COCO.ipynb) an example that shows how to carefully design the inputs and outputs in your TFLite model if you would run it on GPU delegate.\r\n\r\nThat said, if you want to get an idea of how faster your model would run on a mobile device I would suggest checking out the [Benchmarking tool](https://www.tensorflow.org/lite/performance/measurement). It gives you a fair idea of different inference statistics such as memory usage, average inference latency, and many more. \r\n\r\nNow, from my personal experience, I have noticed at least 10x speedups on style transfer models (with a MobileNet backend) and 5x speedups on MobileDets all on my humble Realme Pro 2. \r\n\r\nI hope that helps. ", "this helps a ton! Thank you - I will checkout the benchmarking tool and try to dig deeper into different types of models. I am not too familiar with tflite and am actually just using the tflite file provided to me in autoML from google cloud. I found some scripts online to help me execute inference in swift but was disappointed in the performance as explained earlier. It sounds to me from your response like running this on mobile should fix the issue to the extent that I had hoped. I will definitely be optimizing the model and using the benchmarking tool you recommended, but for now, I think I am good to proceed. Thanks!", "Sure thing! Please feel free to continue posting your questions here. I will try my best to answer them from my experience. ", "The open-gl based GPU delegate can be runnable on the desktop. That might help accelerating the speed on the desktop. @terryheo", "Could you point to some relevant resources that talk about this? @abattery ", "Actually it's OpenCL not OpenGL. You can build x86_64 binary with OpenCL supports.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake#opencl_gpu_delegate\r\n\r\nFYI, it's still experimental and only verified with NVidia CUDA OpenCL 1.2.", "Interesting. Any information on the amount of speedup one might expect? I understand this is highly model specific. Also, what would be the recommendation for accelerating inference with commodity CPUs?", "Regarding GPU performance, check this article.\r\nhttps://blog.tensorflow.org/2019/01/tensorflow-lite-now-faster-with-mobile.html\r\n\r\nAlso you can apply quantization which usually gets at least 2x improvement.\r\nhttps://www.tensorflow.org/lite/performance/model_optimization\r\n\r\nAlso it's worth to read.\r\nhttps://www.tensorflow.org/lite/performance/best_practices\r\n", "@terryheo I am aware of the above-mentioned stuff. If I am not wrong all of them are highly mobile-specific. Since that is not under question (I am quite aware of TensorFlow Lite's capability of accelerating mobile-specific models as I already mentioned [here](https://github.com/tensorflow/tensorflow/issues/45953#issuecomment-750821307)) I wanted to understand the practices one should follow when the inference runtime is based on common NVIDIA GPUs or Intel CPUs and so on. \r\n\r\nFor GPUs, you mentioned OpenCL related building [here](https://github.com/tensorflow/tensorflow/issues/45953#issuecomment-751565725). But what if I am using an Intel CPU during inference? Are there any recommendations for that? Also, in these cases what is the typical speed-up one might expect? \r\n\r\nLet me know if anything is unclear. ", "@terryheo any further inputs based on my comments above? ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45952, "title": "tf.read_file have different speed to read the same images", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):docker image: `docker pull tensorflow/tensorflow:1.13.1-gpu-py3`\r\n- TensorFlow version (use command below):1.13.1\r\n- Python version:3.5.2\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory:1080TI/11G\r\n\r\n**Describe the current behavior**\r\nHi,\r\nI had pull tf1.13.1 docker image from dockerhub: \r\n\r\n```sh\r\n$ docker pull tensorflow/tensorflow:1.13.1-gpu-py3\r\n```\r\nI use `tf.data.TextLineDataset` to load data from HHD, here is my code:\r\n\r\n```python\r\ndef _parse_function(data):\r\n    data_str_split = tf.string_split([data]).values\r\n    filename = data_str_split[0]\r\n    image_string = tf.read_file(filename)\r\n    return image_string\r\n\r\ndef input_fn(is_training, filename, params):\r\n    batch_size = params['batch_size']\r\n    num_parallel_calls = params['num_parallel_calls']\r\n    if num_parallel_calls is None:\r\n        num_parallel_calls = tf.data.experimental.AUTOTUNE\r\n    parse_fn = lambda data: _parse_function(data)    # anchor\r\n    dataset = tf.data.TextLineDataset(filename)\r\n    if is_training:\r\n        dataset = dataset.apply(tf.data.experimental.map_and_batch(\r\n            map_func=parse_fn, batch_size=batch_size, num_parallel_calls=num_parallel_calls))\r\n        dataset = dataset.shuffle(300)\r\n    else:\r\n        dataset = dataset.apply(tf.data.experimental.map_and_batch(\r\n            map_func=parse_fn, batch_size=batch_size, num_parallel_calls=num_parallel_calls))\r\n\r\n    # create reinitializable iterator from dataset\r\n    iterator = dataset.make_initializable_iterator()\r\n    images = iterator.get_next()\r\n    iterator_init_op = iterator.initializer\r\n    inputs = {'images': images, 'iterator_init_op': iterator_init_op}\r\n    return inputs\r\n```\r\nI tested the reading speed of the first file: `total.txt`(about 4,000,000 lines), the reading speed is about 2000\uff5e3000it/s(if i test it a second time, it will up to 8000~9000it/s):\r\n```python\r\nif __name__ == \"__main__\":\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n    params = {\r\n        'batch_size': 32,\r\n        'num_parallel_calls': 64\r\n    }\r\n\r\n    test_txt = '${ROOT}/total.txt'\r\n    # test_txt = '${ROOT}/train.txt'\r\n    # test_txt = '${ROOT}/valid.txt'\r\n    test_inputs = input_fn(True, test_txt, params)\r\n\r\n    import time\r\n    ts = time.time()\r\n    last_processed = 0\r\n    num_processed = 0\r\n    with tf.Session() as sess:\r\n        sess.run(test_inputs['iterator_init_op'])\r\n        while True:\r\n            try:\r\n                images = sess.run(test_inputs['images'])\r\n            except tf.errors.OutOfRangeError:\r\n                sess.run(test_inputs['iterator_init_op'])\r\n                continue\r\n            num_processed += images.shape[0]\r\n            tn = time.time()\r\n            if tn-ts > 1.0:\r\n                print(\"\\rnum_parallel_calls[{}]:{:5.2f}it/s\".format(params['num_parallel_calls'],\r\n                                                                    (num_processed-last_processed)/(tn-ts)), end='')\r\n                last_processed = num_processed\r\n                ts = tn\r\n```\r\nThen, I shuffled the file `total.txt` and splited to `train.txt/valid.txt`, and test the reading speed,\r\nbut the speed drops to **100it/s**!\r\n\r\nThe 2 txt file pointed to the same images, why is there such a big difference?\r\n\r\nCan you tell me how to solve this problem, I have tried to split the `train.txt` to several small `.txt` file, but it's useless.\r\n\r\nThanks a lot!", "comments": ["@ShiquanYu \r\n\r\nYou are seeing the same behavior with latest TF  2.4, Nightly versions?\r\n\r\nIf the issue still persists in latest versions please share simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "Hi, @ravikyram \r\n\r\nI'm sorry but i need to run it in tf1.x, because I need to deploy the model to an embedded device which only support tf1.x's operators and model.\r\n\r\nIs there any way to solve this problem in version tf1.x? Otherwise, I have to use tfrecord instead.\r\n\r\nBut this problem is too strange, the cpu is idle, the disk is idle, the memory is enough, but it's still can't load data from the disk.\r\nEach picture has only 2kb~10kb size, the maximum loading speed is only 1Mb/s?\r\n\r\nThanks!", "Hi, @ravikyram \r\n\r\nThanks for your reply, I have found the reason, maybe because of the `shuffle` operation raise this problem.\r\n\r\nI used cv2 to read image from HHD, it's same speed with tf.read_file. If I shuffle the data, the HHD need to cost more time on `seek time`.\r\n\r\nThanks a lot!"]}, {"number": 45951, "title": "fix the hyperlink in the doc", "body": "the text and link are reversed.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45951) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 45950, "title": "Tensorflow does not use the GPU during training with eager execution, despite manual activation", "body": "**NOTE** This issue is being the same issue [here](https://github.com/tensorflow/tensorflow/issues/45546) and it's being re-opened because this [person](https://github.com/sanjoy) decided to close it without resolution / discussing his decision, so there we go again ...\r\n\r\n<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux (Google colab)):\r\n- TensorFlow installed using `pip install tensorflow-gpu`:\r\n- TensorFlow version (2.3.1):\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1.243\r\n- GPU model and memory: Tesla T4 computeCapability: 7.5 coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI'm experiencing a very slow training time when running a third party [script](https://github.com/marload/DeepRL-TensorFlow2/blob/master/DQN/DQN_Discrete.py) which is the same(0% difference between GPU and CPU). I use the following command for activating the GPU on google colab after installing `tensorflow-gpu` using `pip`:\r\n\r\n```\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nif len(physical_devices) > 0:\r\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```\r\nI add the lines above in `main()` in the script I referred to earlier and I use [wandb](https://github.com/wandb/client) for monitoring the training. Here are the [graphs](https://drive.google.com/file/d/1Fgn7tlm6HBUyZIcPelVjxNz_RWvY7QU_/view?usp=sharing) within a few minutes of training showing 0% GPU utilization.\r\n\r\n**Describe the expected behavior**\r\n\r\nA fast performance which results in a remarkable difference in speeds (CPU vs GPU) and GPU utilization above 0% if the metrics are accurate and if they are not, I'm still experiencing the same speed when run on CPU or GPU.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport wandb\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nimport gym\r\nimport argparse\r\nimport numpy as np\r\nfrom collections import deque\r\nimport random\r\n\r\ntf.keras.backend.set_floatx('float64')\r\nwandb.init(name='DQN', project=\"deep-rl-tf2\")\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--gamma', type=float, default=0.95)\r\nparser.add_argument('--lr', type=float, default=0.005)\r\nparser.add_argument('--batch_size', type=int, default=32)\r\nparser.add_argument('--eps', type=float, default=1.0)\r\nparser.add_argument('--eps_decay', type=float, default=0.995)\r\nparser.add_argument('--eps_min', type=float, default=0.01)\r\n\r\nargs = parser.parse_args()\r\n\r\nclass ReplayBuffer:\r\n    def __init__(self, capacity=10000):\r\n        self.buffer = deque(maxlen=capacity)\r\n    \r\n    def put(self, state, action, reward, next_state, done):\r\n        self.buffer.append([state, action, reward, next_state, done])\r\n    \r\n    def sample(self):\r\n        sample = random.sample(self.buffer, args.batch_size)\r\n        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\r\n        states = np.array(states).reshape(args.batch_size, -1)\r\n        next_states = np.array(next_states).reshape(args.batch_size, -1)\r\n        return states, actions, rewards, next_states, done\r\n    \r\n    def size(self):\r\n        return len(self.buffer)\r\n\r\nclass ActionStateModel:\r\n    def __init__(self, state_dim, aciton_dim):\r\n        self.state_dim  = state_dim\r\n        self.action_dim = aciton_dim\r\n        self.epsilon = args.eps\r\n        \r\n        self.model = self.create_model()\r\n    \r\n    def create_model(self):\r\n        model = tf.keras.Sequential([\r\n            Input((self.state_dim,)),\r\n            Dense(32, activation='relu'),\r\n            Dense(16, activation='relu'),\r\n            Dense(self.action_dim)\r\n        ])\r\n        model.compile(loss='mse', optimizer=Adam(args.lr))\r\n        return model\r\n    \r\n    def predict(self, state):\r\n        return self.model.predict(state)\r\n    \r\n    def get_action(self, state):\r\n        state = np.reshape(state, [1, self.state_dim])\r\n        self.epsilon *= args.eps_decay\r\n        self.epsilon = max(self.epsilon, args.eps_min)\r\n        q_value = self.predict(state)[0]\r\n        if np.random.random() < self.epsilon:\r\n            return random.randint(0, self.action_dim-1)\r\n        return np.argmax(q_value)\r\n\r\n    def train(self, states, targets):\r\n        self.model.fit(states, targets, epochs=1, verbose=0)\r\n    \r\n\r\nclass Agent:\r\n    def __init__(self, env):\r\n        self.env = env\r\n        self.state_dim = self.env.observation_space.shape[0]\r\n        self.action_dim = self.env.action_space.n\r\n\r\n        self.model = ActionStateModel(self.state_dim, self.action_dim)\r\n        self.target_model = ActionStateModel(self.state_dim, self.action_dim)\r\n        self.target_update()\r\n\r\n        self.buffer = ReplayBuffer()\r\n\r\n    def target_update(self):\r\n        weights = self.model.model.get_weights()\r\n        self.target_model.model.set_weights(weights)\r\n    \r\n    def replay(self):\r\n        for _ in range(10):\r\n            states, actions, rewards, next_states, done = self.buffer.sample()\r\n            targets = self.target_model.predict(states)\r\n            next_q_values = self.target_model.predict(next_states).max(axis=1)\r\n            targets[range(args.batch_size), actions] = rewards + (1-done) * next_q_values * args.gamma\r\n            self.model.train(states, targets)\r\n    \r\n    def train(self, max_episodes=1000):\r\n        for ep in range(max_episodes):\r\n            done, total_reward = False, 0\r\n            state = self.env.reset()\r\n            while not done:\r\n                action = self.model.get_action(state)\r\n                next_state, reward, done, _ = self.env.step(action)\r\n                self.buffer.put(state, action, reward*0.01, next_state, done)\r\n                total_reward += reward\r\n                state = next_state\r\n            if self.buffer.size() >= args.batch_size:\r\n                self.replay()\r\n            self.target_update()\r\n            print('EP{} EpisodeReward={}'.format(ep, total_reward))\r\n            wandb.log({'Reward': total_reward})\r\n\r\n\r\ndef main():\r\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\n    if len(physical_devices) > 0:\r\n        tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n    env = gym.make('CartPole-v1')\r\n    agent = Agent(env)\r\n    agent.train(max_episodes=1000)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Sorry for the confusion, I've reopened the original issue.", "@emadboctorx,\r\nThe original issue has been re-opened. Can we please close this issue since it is already being tracked there. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45950\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45950\">No</a>\n"]}, {"number": 45948, "title": "https://youtu.be/IGpHWo0ySBU", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45948\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45948\">No</a>\n", "@santop29  Please read [code of conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md) before posting any queries. The issue tracker is for Tensorflow feature requests or bug reports. Please refrain from posting blank issues or issues that are not relevant to the topic.  If any functionality or feature does not work to your satisfaction, please let us know, as each feedback is valuable to us. Tensorflow team will be happy to address them."]}, {"number": 45947, "title": "https://youtu.be/IGpHWo0ySBU", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["spam", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45947\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45947\">No</a>\n", "@santop29  Please read [code of conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md) before posting any queries. The issue tracker is for Tensorflow feature requests or bug reports. Please refrain from posting blank issues or issues that are not relevant to the topic.  If any functionality or feature does not work to your satisfaction, please let us know, as each feedback is valuable to us. Tensorflow team will be happy to address them."]}, {"number": 45945, "title": "SavedModel: KeyError on concrete_functions ", "body": "\r\n**System information**\r\n-Google Colab\r\n-Tensorflow version 2.4.0\r\n\r\nI wrote encoder_network, decoder_network using Keras Functional API and saved in SavedModel Format.\r\nBut when I use 'saved_model_cli show --dir model  --all', it shows those errors\r\n\r\n2020-12-23 17:23:49.847954: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['__saved_model_init_op']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['__saved_model_init_op'] tensor_info:\r\n        dtype: DT_INVALID\r\n        shape: unknown_rank\r\n        name: NoOp\r\n  Method name is: \r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['input_2'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 110, 470, 3)\r\n        name: serving_default_input_2:0\r\n    inputs['input_3'] tensor_info:\r\n        dtype: DT_INT32\r\n        shape: (-1, -1)\r\n        name: serving_default_input_3:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['decoder_network'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, -1, 37)\r\n        name: StatefulPartitionedCall:0\r\n  Method name is: tensorflow/serving/predict\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/saved_model_cli\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py\", line 1185, in main\r\n    args.func(args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py\", line 715, in show\r\n    _show_all(args.dir)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py\", line 307, in _show_all\r\n    _show_defined_functions(saved_model_dir)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py\", line 187, in _show_defined_functions\r\n    trackable_object = load.load(saved_model_dir)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 603, in load\r\n    return load_internal(export_dir, tags, options)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 633, in load_internal\r\n    ckpt_options)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 130, in __init__\r\n    self._load_all()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 141, in _load_all\r\n    self._load_nodes()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 283, in _load_nodes\r\n    node, setter = self._recreate(proto, node_id)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 393, in _recreate\r\n    return factory[kind]()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 382, in <lambda>\r\n    \"function\": lambda: self._recreate_function(proto.function),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 421, in _recreate_function\r\n    proto, self._concrete_functions), setattr\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py\", line 261, in recreate_function\r\n    concrete_function_objects.append(concrete_functions[concrete_function_name])\r\nKeyError: '__inference_functional_3_layer_call_fn_718866'\r\n\r\n\r\nDo you have any hint how to fix this ? What am I doing wrong here ? is it because I am using Keras Functional API?", "comments": ["@Bilgee \r\n\r\nPlease, share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@ravikyram Colab link: https://colab.research.google.com/drive/1o00sUhX93xtY4eLo7U8Nrz1xpaevHrz8?usp=sharing\r\n\r\nThank you. ", "@Bilgee \r\n\r\nPlease, grant me the access for the colab link. Thanks!", "@ravikyram I am sorry. It is done.", "@Bilgee \r\n\r\nI have tried in colab with TF version 2.4 and was able to reproduce the issue.In TF nightly version i am seeing the different error message. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/55b2ae3622369cb62d64413402ecb792/untitled583.ipynb). Thanks!", "@ravikyram Thank you. I am trying to run the model in C++ using SavedModel format. But it failed, maybe because I could not save my model correctly in SavedModel Format.", "@ravikyram  @ymodak any updates?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45945\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45945\">No</a>\n", "@ymodak Why did you close this issue? I think it is a bug. If a model is saved correctly, saved_model_cli must shows the right results, right?. But right now, we cannot save the model in TF format correctly. ", "Does your model contain custom objects? If yes then you may want to switch to Keras Model Subclassing API."]}]