[{"number": 49523, "title": "Validate (and ensure validation sticks) inputs for `MatrixTriangularSolve`.", "body": "PiperOrigin-RevId: 370282444\nChange-Id: Iaed61a0b0727cc42c830658b72eb69f785f48dc5", "comments": []}, {"number": 49522, "title": "Validate (and ensure validation sticks) inputs for `MatrixTriangularSolve`.", "body": "PiperOrigin-RevId: 370282444\nChange-Id: Iaed61a0b0727cc42c830658b72eb69f785f48dc5", "comments": []}, {"number": 49521, "title": "ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.", "body": "<em>This is a issue similar to the issues #35226 and #36392.</em>\r\n\r\n**System information**\r\n- I am testing  a code available at  https://github.com/hehefan/PointRNN on my system(which is a slurm cluster):\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.3\r\n- CUDA/cuDNN version: 10.1 / 7.6\r\n\r\n**Traceback**\r\n```\r\n`Use `tf.keras.layers.Conv1D` instead.\r\nTraceback (most recent call last):\r\n  File \"dist_arg.py\", line 119, in <module>\r\n    model = create_model()\r\n  File \"dist_arg.py\", line 106, in create_model\r\n    model = Model(batch_size=args.batch_size,\r\n  File \"/home/srijan.singh/convert_PointRNN_2/models/dist_arg.py\", line 158, in __init__\r\n    gradients = tape.gradient(self.loss, params)\r\n  File \"/home/srijan.singh/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\", line 1051, in gradient\r\n    flat_sources = [_handle_or_self(x) for x in flat_sources]\r\n  File \"/home/srijan.singh/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\", line 1051, in <listcomp>\r\n    flat_sources = [_handle_or_self(x) for x in flat_sources]\r\n  File \"/home/srijan.singh/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\", line 729, in _handle_or_self\r\n    return x.handle\r\n  File \"/home/srijan.singh/.local/lib/python3.8/site-packages/tensorflow/python/distribute/values.py\", line 571, in handle\r\n    raise ValueError(\"`handle` is not available outside the replica context\"\r\nValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.`\r\n```\r\n**dist.arg.py**\r\n```\r\nmport os\r\nimport sys\r\nimport io\r\nfrom datetime import datetime\r\nimport argparse\r\nimport numpy as np\r\nfrom PIL import Image\r\nimport tensorflow as tf\r\n#import matplotlib.pyplot as plt\r\n#from mpl_toolkits.mplot3d import axes3d, Axes3D\r\nfrom PIL import Image\r\nimport time\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nBATCH_SIZE_PER_REPLICA = 2\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\nBUFFER_SIZE = 8\r\nEPOCHS = 200000\r\n\r\nimport models.dist_arg as models\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--data-dir', default='data/argo-5m', help='Dataset directory [default: data/argo-5m]')\r\nparser.add_argument('--dataset', default='argo', help='Dataset. argo or nu [default: argo]')\r\nparser.add_argument('--batch-size', type=int, default=8, help='Batch Size during training [default: 4]')\r\nparser.add_argument('--num-iters', type=int, default=200000, help='Iterations to run [default: 200000]')\r\nparser.add_argument('--save-iters', type=int, default=1000, help='Iterations to save checkpoints [default: 1000]')\r\nparser.add_argument('--learning-rate', type=float, default=1e-5, help='Learning rate [default: 1e-5]')\r\nparser.add_argument('--max-gradient-norm', type=float, default=5.0, help='Clip gradients to this norm [default: 5.0].')\r\nparser.add_argument('--seq-length', type=int, default=10, help='Length of sequence [default: 10]')\r\nparser.add_argument('--num-points', type=int, default=1024, help='Number of points [default: 1024]')\r\nparser.add_argument('--num-samples', type=int, default=8, help='Number of samples [default: 8]')\r\nparser.add_argument('--unit', type=str, default='pointrnn', help='Unit. pointrnn, pointgru or pointlstm [default: pointlstm]')\r\nparser.add_argument('--alpha', type=float, default=1.0, help='Weigh on CD loss [default: 1.0]')\r\nparser.add_argument('--beta', type=float, default=1.0, help='Weigh on EMD loss [default: 1.0]')\r\nparser.add_argument('--log-dir', default='outputs', help='Log dir [default: outputs]')\r\n\r\nargs = parser.parse_args()\r\nnp.random.seed(999)\r\ntf.compat.v1.set_random_seed(999)\r\nprint ('-'*60)\r\nprint (\"arguments_parsed\")\r\nprint ('-'*60)\r\nprint (tf.distribute.get_strategy())\r\n\r\nargs.log_dir += '/%s-%s'%(args.dataset, args.unit)\r\n\r\nif args.dataset == 'argo':\r\n    from datasets.argo_nu import Argoverse as Dataset\r\nif args.dataset == 'nu':\r\n    from datasets.argo_nu import nuScenes as Dataset\r\n\r\ntrain_dataset = Dataset(root=args.data_dir,\r\n                        seq_length=args.seq_length,\r\n                        num_points=args.num_points,\r\n                        train=True)\r\n\r\n\r\n\r\nprint ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n\r\ndef get_batch(dataset, batch_size):\r\n    batch_data = []\r\n    for i in range(batch_size):\r\n        sample = dataset[0]\r\n        batch_data.append(sample)\r\n    return np.stack(batch_data, axis=0)\r\n\r\nbatch_data = tf.convert_to_tensor(get_batch(dataset=train_dataset, batch_size=args.batch_size))\r\ndataset = tf.data.Dataset.from_tensor_slices(batch_data).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\r\ntrain_dist_dataset = strategy.experimental_distribute_dataset(dataset)\r\nprint(type(batch_data), batch_data.shape, dataset.element_spec)\r\n\r\nprint ('-'*60)\r\nprint (\"distributed dataset\")\r\n\r\ndef create_model():\r\n    model_name = 'Point' + args.unit[5:].upper()\r\n    Model = getattr(models, model_name)\r\n    model = Model(batch_size=args.batch_size,\r\n              seq_length=args.seq_length,\r\n              num_points=args.num_points,\r\n              num_samples=args.num_samples,\r\n              knn=True,\r\n              alpha=args.alpha,\r\n              beta=args.beta,\r\n              learning_rate=args.learning_rate,\r\n              max_gradient_norm=args.max_gradient_norm,\r\n              is_training=True)\r\n    return model\r\n\r\nwith strategy.scope():\r\n  model = create_model()\r\n\r\nprint ('-'*60)\r\nprint (\"model_made\")\r\n\r\n@tf.function\r\ndef distributed_train_step(dataset_inputs):\r\n\r\n        los, cd, emd, step, summary, predictions, _ = strategy.experimental_run_v2([model.loss, model.cd, model.emd, model.global_step, summary_op, model.predicted_frames, model.train_op], args=(dataset_inputs,))\r\n        return strategy.reduce(tf.distribute.ReduceOp.SUM, los, axis=None)\r\n\r\nfor epoch in range(args.num_iters):\r\n        # TRAIN LOOP\r\n        total_loss = 0.0\r\n        num_batches = 0\r\n        for x in train_dist_dataset:\r\n                total_loss += distributed_train_step(x)\r\n                num_batches += 1\r\n        train_loss = total_loss / num_batches\r\n\r\n```\r\nIt calls the **model/dist_arg.py** which leads to the issue observed.\r\n\r\n**models/dist.arg.py**\r\n```\r\nimport os\r\nimport sys\r\nimport tensorflow as tf\r\n\r\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\r\nROOT_DIR = os.path.dirname(BASE_DIR)\r\nsys.path.append(os.path.join(ROOT_DIR, 'modules'))\r\nsys.path.append(os.path.join(ROOT_DIR, 'modules/tf_ops/nn_distance'))\r\nsys.path.append(os.path.join(ROOT_DIR, 'modules/tf_ops/approxmatch'))\r\n\r\nfrom pointnet2 import *\r\nfrom pointrnn_cell_impl import *\r\nimport tf_nndistance\r\nimport tf_approxmatch\r\n\"\"\"\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\"\"\"\r\nstrategy = tf.distribute.get_strategy()\r\n\r\nBATCH_SIZE_PER_REPLICA = 2\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\nBUFFER_SIZE = 8\r\nEPOCHS = 200000\r\n\r\nstrategy = tf.distribute.get_strategy()\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nclass PointRNN(object):\r\n    def __init__(self, batch_size, seq_length, num_points=1024, num_samples=8, knn=False, alpha=1.0, beta=1.0, learning_rate=0.001, max_gradient_norm=5.0, is_training=False):\r\n\r\n        self.global_step = tf.Variable(0, trainable=False)\r\n\r\n        self.inputs = tf.compat.v1.placeholder(tf.float32, [batch_size, seq_length, num_points, 3])\r\n        frames = tf.split(value=self.inputs, num_or_size_splits=seq_length, axis=1)\r\n        frames = [tf.squeeze(input=frame, axis=[1]) for frame in frames]\r\n\r\n        cell1 = PointRNNCell(radius=1.0+1e-6, nsample=3*num_samples, out_channels=128, knn=knn, pooling='max')\r\n        cell2 = PointRNNCell(radius=2.0+1e-6, nsample=2*num_samples, out_channels=256, knn=knn, pooling='max')\r\n        cell3 = PointRNNCell(radius=4.0+1e-6, nsample=1*num_samples, out_channels=512, knn=knn, pooling='max')\r\n\r\n        # context\r\n        states1 = None\r\n        states2 = None\r\n        states3 = None\r\n        for i in range(int(seq_length/2)):\r\n            # 512\r\n            xyz1, _, _, _ = sample_and_group(int(num_points/2), radius=0.5+1e-6, nsample=num_samples, xyz=frames[i], points=None, knn=False, use_xyz=False)\r\n            with tf.compat.v1.variable_scope('encoder_1', reuse=tf.compat.v1.AUTO_REUSE) as scope:\r\n                states1 = cell1((xyz1, None), states1)\r\n                s_xyz1, s_feat1 = states1\r\n            # 256\r\n            xyz2, feat2, _, _ = sample_and_group(int(num_points/2/2), radius=1.0+1e-6, nsample=num_samples, xyz=s_xyz1, points=s_feat1, knn=False, use_xyz=False)\r\n            feat2 = tf.reduce_max(input_tensor=feat2, axis=[2], keepdims=False, name='maxpool')\r\n            with tf.compat.v1.variable_scope('encoder_2', reuse=tf.compat.v1.AUTO_REUSE) as scope:\r\n                states2 = cell2((xyz2, feat2), states2)\r\n                s_xyz2, s_feat2 = states2\r\n            # 128\r\n            xyz3, feat3, _, _ = sample_and_group(int(num_points/2/2/2), radius=2.0+1e-6, nsample=num_samples, xyz=s_xyz2, points=s_feat2, knn=False, use_xyz=False)\r\n            feat3 = tf.reduce_max(input_tensor=feat3, axis=[2], keepdims=False, name='maxpool')\r\n            with tf.compat.v1.variable_scope('encoder_3', reuse=tf.compat.v1.AUTO_REUSE) as scope:\r\n                states3 = cell3((xyz3, feat3), states3)\r\n\r\n        # prediction\r\n        predicted_motions = []\r\n        predicted_frames = []\r\n        input_frame = frames[int(seq_length/2)-1]\r\n        for i in range(int(seq_length/2), seq_length):\r\n            # 512\r\n            xyz1, _, _, _ = sample_and_group(int(num_points/2), radius=0.5+1e-6, nsample=num_samples, xyz=input_frame, points=None, knn=False, use_xyz=False)\r\n            with tf.compat.v1.variable_scope('decoder_1', reuse=tf.compat.v1.AUTO_REUSE) as scope:\r\n                states1 = cell1((xyz1, None), states1)\r\n                s_xyz1, s_feat1 = states1\r\n            # 256\r\n            xyz2, feat2, _, _ = sample_and_group(int(num_points/2/2), radius=1.0+1e-6, nsample=num_samples, xyz=s_xyz1, points=s_feat1, knn=False, use_xyz=False)\r\n            feat2 = tf.reduce_max(input_tensor=feat2, axis=[2], keepdims=False, name='maxpool')\r\n            with tf.compat.v1.variable_scope('decoder_2', reuse=tf.compat.v1.AUTO_REUSE) as scope:\r\n                states2 = cell2((xyz2, feat2), states2)\r\n                s_xyz2, s_feat2 = states2\r\n            # 128\r\n            xyz3, feat3, _, _ = sample_and_group(int(num_points/2/2/2), radius=2.0+1e-6, nsample=num_samples, xyz=s_xyz2, points=s_feat2, knn=False, use_xyz=False)\r\n            feat3 = tf.reduce_max(input_tensor=feat3, axis=[2], keepdims=False, name='maxpool')\r\n            with tf.compat.v1.variable_scope('decoder_3', reuse=tf.compat.v1.AUTO_REUSE) as scope:\r\n                states3 = cell3((xyz3, feat3), states3)\r\n                s_xyz3, s_feat3 = states3\r\n\r\n\r\n            with tf.compat.v1.variable_scope('fp', reuse=tf.compat.v1.AUTO_REUSE) as scope:\r\n                l2_feat = pointnet_fp_module(xyz2,\r\n                                             xyz3,\r\n                                             s_feat2,\r\n                                             s_feat3,\r\n                                             mlp=[256],\r\n                                             last_mlp_activation=True,\r\n                                             scope='fp2')\r\n                l1_feat = pointnet_fp_module(xyz1,\r\n                                             xyz2,\r\n                                             s_feat1,\r\n                                             l2_feat,\r\n                                             mlp=[256],\r\n                                             last_mlp_activation=True,\r\n                                             scope='fp1')\r\n                l0_feat = pointnet_fp_module(input_frame,\r\n                                             xyz1,\r\n                                             None,\r\n                                             l1_feat,\r\n                                             mlp=[256],\r\n                                            xyz1,\r\n                                             None,\r\n                                             l1_feat,\r\n                                             mlp=[256],\r\n                                             last_mlp_activation=True,\r\n                                             scope='fp0')\r\n\r\n            with tf.compat.v1.variable_scope('fc', reuse=tf.compat.v1.AUTO_REUSE) as scope:\r\n                predicted_motion = tf.compat.v1.layers.conv1d(inputs=l0_feat, filters=128, kernel_size=1, strides=1, padding='valid', data_format='channels_last', activation=tf.nn.relu, name='fc1')\r\n                predicted_motion = tf.compat.v1.layers.conv1d(inputs=predicted_motion, filters=3, kernel_size=1, strides=1, padding='valid', data_format='channels_last', activation=None, name='fc2')\r\n\r\n            predicted_motions.append(predicted_motion)\r\n            input_frame += predicted_motion\r\n            predicted_frames.append(input_frame)\r\n\r\n        # loss\r\n        if is_training:\r\n            with tf.GradientTape() as tape:\r\n                optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\r\n                self.loss = self.emd = self.cd = 0\r\n                for i in range(int(seq_length/2)):\r\n                    match = tf_approxmatch.approx_match(frames[i+int(seq_length/2)], predicted_frames[i])\r\n                    #input_emd = tf_approxmatch.match_cost(frames[i+int(seq_length/2)], predicted_frames[i], match)\r\n                    emd_distance = tf_approxmatch.match_cost(frames[i+int(seq_length/2)], predicted_frames[i], match)\r\n                    ####################################################\r\n                    #print (i, match, input_emd, emd_distance)\r\n                    loss_emd = emd_distance\r\n                    self.emd += loss_emd\r\n\r\n                    dists_forward, _, dists_backward, _ = tf_nndistance.nn_distance(predicted_frames[i], frames[i+int(seq_length/2)])\r\n                    #loss_cd = tf.reduce_mean(input_tensor=dists_forward+dists_backward)\r\n                    loss_cd = dists_forward + dists_backward\r\n                    ######################################################\r\n                    self.cd += loss_cd\r\n\r\n                    self.loss += (alpha*loss_cd + beta*loss_emd)\r\n\r\n                self.cd /= int(seq_length/2)\r\n                self.emd /= (int(seq_length/2)*num_points)\r\n\r\n                self.loss /= int(seq_length/2)\r\n\r\n                self.lais /= (int(seq_length/2)*num_points)\r\n\r\n                self.loss = tf.nn.compute_average_loss(self.loss, global_batch_size=GLOBAL_BATCH_SIZE)\r\n\r\n            params = tf.compat.v1.trainable_variables()\r\n            gradients = tape.gradient(self.loss, params)\r\n            #gradients = tf.gradients(ys=self.loss, xs=params)\r\n            #clipped_gradients, norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\r\n            #self.train_op = tf.compat.v1.train.AdamOptimizer(learning_rate).apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\r\n            #self.train_op = optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n            self.train_op = optimizer(learning_rate).apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\r\n\r\n        self.predicted_motions = tf.stack(values=predicted_motions, axis=1)\r\n        self.predicted_frames = tf.stack(values=predicted_frames, axis=1)\r\n        self.saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables(), max_to_keep=1)\r\n\r\n```\r\n\r\nThe trace mentions the line \r\n```\r\ngradients = tape.gradient(self.loss, params)\r\n```\r\nin the **models/dist_arg.py** (the second file shown above).\r\n\r\nI have used \r\n```\r\nstrategy.experimental_run_v2([model.loss, model.cd, model.emd, model.lais, model.global_step, summary_op, model.predicted_frames, model.train_op], args=(dataset_inputs,)) \r\n```\r\nas mentioned in similar issues and enclosed my loss computation part within\r\n\r\n``` \r\nused tf.GradientTape() as tape:\r\n```\r\nand also commented\r\n``` \r\nclipped_gradients, norm = tf.clip_by_global_norm(gradients, max_gradient_norm)  \r\n```\r\nas it was also mentioned that normalization was causing an error,  though the model was different(CNN).\r\n\r\nCan someone please find any error on my part(as i have added/changed some lines of code to make it run on multiple gpus) or is it a tensorflow error?", "comments": ["@all-over-the-place \r\nCan you please share a colab gist with the issue reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49521\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49521\">No</a>\n"]}, {"number": 49519, "title": "Added missing reshape folding for MEAN op to TFLite MLIR converter", "body": "Added optimizing pattern for folding RESHAPE op in MEAN op when the former is following the latter with `keep_dims=false` for re-inserting reduced dimensions to TFLite MLIR converter. Apparently it is supposed to be there before, but was missing for some reason. Without the proposed fix MLIR converter may produce a very inefficient model when quantization is used, because `keep_dims=true` is an [essential condition](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/reduce.cc#L316) for using optimized reduce kernel instead of reference one.", "comments": []}, {"number": 49518, "title": "Tensorflow 2.5.0 Windows - Cannot import name 'keras' from partially initialized module 'tensorflow'", "body": "Python returns an error when I run script \r\n`Cannot import name 'keras' from partially initialized module 'tensorflow' (most likely due to a circular import) (C:\\Users\\ytigiev\\source\\repos\\tensorflow\\tensorflow.py)\r\n`\r\n\r\nOS - Wn 10 64 bit\r\nIDE - VS 2019 16.9.6 + env (with Python 3.8.10 - 64 bit. (Downloaded from https://www.python.org/downloads/windows/))\r\n\r\nSource code from https://www.tensorflow.org/api_docs/python/tf/keras (Is this sample code actual for tensorflow 2.5.0?)\r\n\r\nI just copied first strings - \"import\"\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport os\r\nimport PIL\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.models import Sequential\r\n\r\n\r\nrequirements.txt\r\n\r\nabsl-py==0.12.0\r\nastunparse==1.6.3\r\ncachetools==4.2.2\r\ncertifi==2020.12.5\r\nchardet==4.0.0\r\ncycler==0.10.0\r\nflatbuffers==2.0\r\ngast==0.4.0\r\ngoogle-auth==2.0.0.dev0\r\ngoogle-auth-oauthlib==0.4.4\r\ngoogle-pasta==0.2.0\r\ngrpcio==1.38.0\r\nh5py==3.2.1\r\nidna==3.1\r\nKeras==2.4.3\r\nkeras-nightly==2.5.0.dev2021032900\r\nKeras-Preprocessing==1.1.2\r\nkiwisolver==1.3.1\r\nMarkdown==3.3.4\r\nmatplotlib==3.4.2\r\nnumpy==1.20.3\r\noauthlib==3.1.0\r\nopt-einsum==3.3.0\r\nPillow==8.2.0\r\npip==21.1.2\r\nprotobuf==3.17.0\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\npyparsing==2.4.7\r\npython-dateutil==2.8.1\r\nPyYAML==5.4.1\r\nrequests==2.25.1\r\nrequests-oauthlib==1.3.0\r\nrsa==4.7.2\r\nscipy==1.6.3\r\nsetuptools==57.0.0\r\nsix==1.16.0\r\ntensorboard==2.5.0\r\ntensorboard-data-server==0.6.1\r\ntensorboard-plugin-wit==1.8.0\r\ntensorflow==2.5.0\r\ntensorflow-estimator==2.5.0\r\ntermcolor==1.1.0\r\ntyping-extensions==3.10.0.0\r\nurllib3==1.26.4\r\nWerkzeug==2.0.1\r\nwheel==0.36.2\r\nwrapt==1.12.1", "comments": ["@YuriyTigiev \r\n\r\nCould you please try to create all the route files and import them in main file.If you import everything at the top it would result in a circular import error, and let me if it helps.\r\n\r\nand also refer similar [#44822](https://github.com/tensorflow/tensorflow/issues/44822), [#40265](https://github.com/tensorflow/tensorflow/issues/40265) issues.Thanks", "> @YuriyTigiev\r\n> \r\n> Could you please try to create model and route files and import them in main file.If you import everything at the top it would result in a circular import error, and let me if it helps.\r\n> \r\n> and also refer similar [#44822](https://github.com/tensorflow/tensorflow/issues/44822), [#40265](https://github.com/tensorflow/tensorflow/issues/40265) issues.Thanks\r\n\r\nSorry, \r\nI don't understand what do you mean ? - \"create model and route files and import them in main file\"\r\n\r\nCould you send me a step by step instructions? \r\n\r\n", "@YuriyTigiev \r\n\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "The code snippet below. The second record raises error. \r\nThe .py file doesn't contain any other strings of code.\r\nIssue with import keras.  I have tested this code with python 3.7 and 3.8 with the same result.\r\nFor reproduce error \r\n1) Create empty console python project in Visual Studio Community 2019 \r\n2) Add env with python 3.7  or 3.8 (both generates the same error)\r\n3) Install all modules from requirments.txt \r\n4) Put this 4 strings \r\n5) Run it.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.models import Sequential\r\n```\r\n", "@YuriyTigiev \r\nUsually A circular import occurs when two or more modules depend on each other. This is due to the fact that each module is defined in terms of the other .Please make sure you followed the steps below\r\n\r\n1.import inside the function. \r\n2.sysconfig is imported from the tensorflow module, so having this  \r\n3.import at the top would cause a circular import, resulting in  \r\n4.the tensorflow module missing symbols that come after sysconfig.  \r\n\r\nfor more information refer [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/sysconfig.py) ,find these steps [here](https://screenshot.googleplex.com/86FiqVnyAdFEJVN).Please let us know if it helps.Thanks\r\n\r\n", "> @YuriyTigiev\r\n> Usually A circular import occurs when two or more modules depend on each other. This is due to the fact that each module is defined in terms of the other .Please make sure you followed the steps below\r\n> \r\n> 1.import inside the function.\r\n> 2.sysconfig is imported from the tensorflow module, so having this\r\n> 3.import at the top would cause a circular import, resulting in\r\n> 4.the tensorflow module missing symbols that come after sysconfig.\r\n> \r\n> for more information refer [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/sysconfig.py) ,find these steps [here](https://screenshot.googleplex.com/86FiqVnyAdFEJVN).Please let us know if it helps.Thanks\r\n\r\nPlease, provide a correct example how to use import tensorflow. \r\n\r\nAnd based on your comments does it mean that the import in the example  from the https://www.tensorflow.org/api_docs/python/tf/keras doesn't defined correctly?\r\n\r\nI'm trying to run this example in visual studio community 2019 with tenserflow 2.5.0  \r\n\r\n", "I reinstalled visual studio community. \r\nNow everything works properly,\r\nThank you for attention.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49518\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49518\">No</a>\n"]}, {"number": 49517, "title": "DOC: Fix Capitalization", "body": "- [x] Docs have been updated (for bug fixes/features)\r\n\r\ndocs update => proper capitalisation would be better. \ud83d\udc4d\r\n\r\n### Other information:\r\nSigned-off-by: Ayushman Singh Chauhan <ascb508@gmail.com>\r\nI have read the Contribution guidelines, but it's for the learning purpose :)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49517) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac "]}, {"number": 49516, "title": "Add note about automatic input expansion", "body": "As discussed in https://github.com/tensorflow/tensorflow/issues/42046 this documents the fact that TensorFlow automatically expands inputs to at least 2D when calling functions on Keras models (this actually happens in `model.train_step`, `model.test_step`, and `model.predict_step`, but I suspect those are used less frequently than `model.fit()`, etc).\r\n\r\nThis should hopefully reduce confusion between\r\n\r\n`outputs = model(inputs)  # allows 1D inputs`\r\n\r\nand\r\n\r\n`outputs = model.predict(inputs)  # will automatically expand 1D inputs to 2D`", "comments": ["Thanks for the PR! We're actually in the process of removing this behavior right now. It will be gone in TF 2.7."]}, {"number": 49513, "title": "TFLM: Only disable ScopedMicroProfiler for BUILD_TYPE=release", "body": "* Enable ScopedMicroProfiler for release builds with logs.\r\n* Add cortex_m_generic specific implementation of timer functions.\r\n\r\nThis is a fix for: https://github.com/tensorflow/tensorflow/issues/43958", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 49512, "title": "I have never had success with Tensorflow (or lite version) build with cmake or bazel on windows", "body": "i downloaded source from here [https://github.com/tensorflow/tensorflow/archive/refs/tags/v2.5.0.zip](url)\r\n\r\n**why tensorflow doesn't directly provide static and dynamic windows library, why do we have to build it manually**\r\n\r\nfrom this link [https://www.tensorflow.org/install/lang_c](url)\r\n\r\ni can easily download tensorflowlite library\r\nbut it is **only x64 version**, whereas **i need x86 and x64** because i want to make an application that runs universally on **windows x86 and x64 versions**\r\n\r\n**because tensorflow only provides the x64 version** [https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.5.0.zip](url)\r\n\r\ni am not satisfied, **i want both x86 and x64 version** for my application to run on all windows versions universally\r\n\r\nthat's why I want to build it manually through the source provided [https://github.com/tensorflow/tensorflow](url)\r\n\r\n- i downloaded tensorflow version 2.5.0 [https://github.com/tensorflow/tensorflow/archive/refs/tags/v2.5.0.zip](url)\r\n- i downloaded cmake gui x64 version 3.20.2 [https://cmake.org/download/](url)\r\n- i downloaded bazel x64 version 4.1.0 [https://github.com/bazelbuild/bazel/releases](url)\r\n\r\nI did tensorflow zip extraction to **D:\\tensorflow \\tensorflow-2.5.0**\r\ni make BUILD folder to **D:\\tensorflow\\BUILD**\r\n\r\ni run cmake gui\r\n- \"Where is the source code\" set to **D:\\tensorflow\\tensorflow-2.5.0\\tensorflow\\lite**\r\n- \"Where to build the binaries\" set to **D:/tensorflow/BUILD**\r\ncmake configue is\r\n- visual studio 15 2017\r\n- x64 use default native compiler\r\n**D:\\tensorflow\\tensorflow-2.5.0\\tensorflow\\lite** **has CMakeLists.txt**\r\nthe error detail is :\r\n**Setting build type to Release, for debug builds use'-DCMAKE_BUILD_TYPE=Debug'.\r\nSelecting Windows SDK version 10.0.17134.0 to target Windows 6.1.7601.\r\nThe C compiler identification is MSVC 19.15.26732.1\r\nThe CXX compiler identification is MSVC 19.15.26732.1\r\nDetecting C compiler ABI info\r\nDetecting C compiler ABI info - done\r\nCheck for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.15.26726/bin/Hostx86/x64/cl.exe - skipped\r\nDetecting C compile features\r\nDetecting C compile features - done\r\nDetecting CXX compiler ABI info\r\nDetecting CXX compiler ABI info - done\r\nCheck for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.15.26726/bin/Hostx86/x64/cl.exe - skipped\r\nDetecting CXX compile features\r\nDetecting CXX compile features - done\r\n-- Selecting Windows SDK version 10.0.17134.0 to target Windows 6.1.7601.\r\nCMake Error at C:/Program Files/CMake/share/cmake-3.20/Modules/ExternalProject.cmake:2633 (message):\r\n  error: could not find git for clone of abseil-cpp-populate\r\nCall Stack (most recent call first):\r\n  C:/Program Files/CMake/share/cmake-3.20/Modules/ExternalProject.cmake:3681 (_ep_add_download_command)\r\n  CMakeLists.txt:22 (ExternalProject_Add)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"D:/tensorflow/BUILD/_deps/abseil-cpp-subbuild/CMakeFiles/CMakeOutput.log\".\r\n\r\nCMake Error at C:/Program Files/CMake/share/cmake-3.20/Modules/FetchContent.cmake:1000 (message):\r\n  CMake step for abseil-cpp failed: 1\r\nCall Stack (most recent call first):\r\n  C:/Program Files/CMake/share/cmake-3.20/Modules/FetchContent.cmake:1141:EVAL:2 (__FetchContent_directPopulate)\r\n  C:/Program Files/CMake/share/cmake-3.20/Modules/FetchContent.cmake:1141 (cmake_language)\r\n  tools/cmake/modules/OverridableFetchContent.cmake:531 (FetchContent_Populate)\r\n  tools/cmake/modules/abseil-cpp.cmake:34 (OverridableFetchContent_Populate)\r\n  tools/cmake/modules/Findabsl.cmake:18 (include)\r\n  CMakeLists.txt:127 (find_package)\r\n\r\n\r\nConfiguring incomplete, errors occurred!\r\nSee also \"D:/tensorflow/BUILD/CMakeFiles/CMakeOutput.log\".**\r\n\r\n**and then**\r\n- TFLITE_ENABLE_FLEX  = unchecked **(i dont need it)**\r\n- TFLITE_ENABLE_GPU  = unchecked **(i dont need it)**\r\n- TFLITE_ENABLE_MMAP  = unchecked **(i dont need it)**\r\n- TFLITE_ENABLE_NNAAPI = unchecked **(i dont need it)**\r\n- TFLITE_ENABLE_RESOURCE  = unchecked **(i dont need it)**\r\n- TFLITE_ENABLE_RUY  = unchecked **(i dont need it)**\r\n- TFLITE_ENABLE_XNNPACK  = unchecked **(i dont need it)**\r\ni reconfigure but still failed.\r\n\r\nWHAT I WaNT IS, I WANT **TENSORFLOW LlTE STATIC LIBRARY BOTH x86 AND x64**\r\ni dont want my app only working on x64, i want my app run unviversally both x86 and x64\r\n\r\nplease just tell me **\"the right way to build tensorflow lite\"**, because it says \"**CMake step for abseil-cpp failed: 1**\" do I have to download the abseil-cpp library. Thank you\r\n\r\ni want STATIC library not DYNAMIC library\r\ni need LITE version", "comments": ["@terryheo could you take a look?", "if you give me a static library, make sure you have both x86 and x64, and make sure builded with **/MT** Not **/MD**", "For Win32 (32bit Windows) build, you need https://github.com/tensorflow/tensorflow/pull/49171 (which will be merged soon)\r\nBTW, you need to configure your build environment with https://www.tensorflow.org/install/source_windows", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49512\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49512\">No</a>\n"]}, {"number": 49511, "title": "Fix JNI memleak problem for java Tensor<String> reference invoked by tensor_jni.cc", "body": "env->GetObjectArrayElement should be followed by env->DeleteLocalRef, which to release the local reference.", "comments": ["Any comments on this PR? Please. Thanks!"]}, {"number": 49510, "title": "Fix ubsan in bazelrc", "body": "", "comments": []}, {"number": 49509, "title": "tf.image.sobel_edges not supported in TPU", "body": "Hello,\r\n\r\nI am training my deep neural network in TPU on google colab using tensorflow 2 and keras. I have added an L2 loss between the sobel edges of prediction and ground truth images. But, when I am calling `model.fit()` function, this `tf.image.sobel_edges` function raises errors. I believe that the function is not supported while compiling the graph in TPU. \r\n\r\n1. If I am correct then is there any alternate way to compute edges in TPU. \r\n2. And if I not correct then can you please point me to correct use of `tf.image.sobel_edges` function in TPU.\r\nThank you in advance.", "comments": ["Have you already tried with https://www.tensorflow.org/io/api_docs/python/tfio/experimental/filter/sobel ?", "@vinits5 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the Tensorflow version, complete code and dataset to reproduce the issue reported here.\r\n\r\nThanks!", "This https://www.tensorflow.org/io/api_docs/python/tfio/experimental/filter/sobel ? worked. Thanks @bhack .", "/cc @yongtang @mihaimaruseac  @theadactyl  I think that we could improve on handling not homegenous duplicates in the ecosystem like this one."]}, {"number": 49506, "title": "File \"...\", line 6, in <module> import tensorflow.compat.v1 as tf ModuleNotFoundError: No module named 'tensorflow'", "body": "When I push run the program shown me that. How can i solve it, plz and thank you everyone", "comments": ["![image](https://user-images.githubusercontent.com/84584319/119275877-cb060e80-bc41-11eb-9a83-a9575afecea6.png)\r\nWhen I push run the program shown me \"  File \"...\", line 6, in <module> import tensorflow.compat.v1 as tf ModuleNotFoundError: No module named 'tensorflow'  \". How can i solve it, plz and thank you everyone (I use Python 3.8)", "@fronrfron11 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the Tensorflow version you are trying and also please take a look at this [link](https://github.com/tensorflow/tensorflow/issues/38800) with similar error.It helps.\r\n\r\nThanks ", "From stack trace looks like TF is not correctly installed in your system. Can you mention what steps did you take to install TF?\r\nSee https://www.tensorflow.org/install/pip for installation.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49504, "title": "Replace --extra-index-url to --index-url to prevent typosquatting", "body": "The tflite_runtime package is not in PyPI and its name is available. With --extra-index-url, if the version is higher, pip will infer to PyPI. This may result in typosquatting.\r\n\r\nThis is a minor fix.", "comments": []}, {"number": 49503, "title": "tf.data.Dataset.from_tensor_slices support more types than int32", "body": "Currently, `tf.data.Dataset.from_tensor_slices` transforms any slice into `int32`. If working with `uint8` data, this leads to an unnecessary and unexpected data type shift leading to extremely enlarged memory use.", "comments": ["@pesekon2 \r\nThank you for the request, we are working on this and will update you at the earliest.", "@pesekon2 Can you please create a simple standalone code to reproduce the issue? I tried slicing `uint8` data in [this gist](https://colab.research.google.com/gist/jvishnuvardhan/d10b5d5511feb2085eeff987b6de0116/untitled992.ipynb) but couldn't see unexpected data type shift. May be i am missing something here. \r\n\r\nMore details and a standalone code will help in resolving the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49501, "title": "Unable to create TFLite LSTM Model with SND format (time_major = true", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installation (pip package or built from source):\r\n- TensorFlow library (version, if pip package or github SHA, if built from source):\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n#### Option A: Reference colab notebooks\r\n\r\n1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.\r\n2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).\r\n\r\n```\r\n(You can paste links or attach files by dragging & dropping them below)\r\n- Provide links to your updated versions of the above two colab notebooks.\r\n- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.\r\n```\r\n\r\n#### Option B: Paste your code here or provide a link to a custom end-to-end colab\r\n\r\n```\r\n(You can paste links or attach files by dragging & dropping them below)\r\n- Include code to invoke the TFLite Converter Python API and the errors.\r\n- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.\r\n```\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\n- Model produces wrong results and/or has lesser accuracy.\r\n- Model produces correct results, but it is slower than expected.\r\n\r\n### 4. (optional) RNN conversion support\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n### 5. (optional) Any other info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Dupilicate"]}, {"number": 49500, "title": "ModelCheckpoint does not check the value to monitor from logs modified with custom callback ", "body": "I created a custom callback that adds an additional metrics to the logs after each epoch during training. In a Modelcheckpoint callback, I put the additional metric to be monitored. But the Modelcheckpoint does not save according to the added metrics.\r\nSample code:\r\n```\r\nclass LossCallback(tf.keras.callbacks.Callback):\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        logs['new_loss']=logs['loss']+logs['val_loss']\r\n\r\nsave = tf.keras.callbacks.ModelCheckpoint(\r\n        'model.h5', monitor='new_loss', verbose=1, save_best_only=True,\r\n        save_weights_only=True, mode='min', save_freq='epoch')\r\n\r\nmodel.fit(\r\n       data, \r\n        epochs=10, \r\n        callbacks = [LossCallback(),save], \r\n        steps_per_epoch=100,\r\n        validation_data=val_data, \r\n    )\r\n```\r\nIs there any way to get ModelCheckpoint to access the updated logs and save accordingly?", "comments": ["@BishmoyPaul \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/cloab gist to reproduce the issue faced].\r\n Thanks!\r\n\r\n\r\n\r\n", "Issue fixed after adding `self._supports_tf_logs = True` after initializing the callback. Sample code given below :\r\n```\r\nclass LossCallback(tf.keras.callbacks.Callback):\r\n    def __init__(self):\r\n        super(LossCallback, self).__init__()\r\n        self._supports_tf_logs = True  ## Adding this fixed my issue\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        logs['new_loss']=logs['loss']+logs['val_loss']\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49500\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49500\">No</a>\n"]}, {"number": 49499, "title": "R2.1", "body": "test", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49499) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 49498, "title": "Update parse_entity error message.", "body": "`tf.autograph.do_not_convert` is still in `experimental` (as of `2.6.0.dev20210522`), this PR fixes the error message so users know to use `tf.autograph.experimental.do_not_convert`.", "comments": ["Will do @8bitmp3, thanks!", "Thanks for updating the error message @n2cholas and congrats on the award \ud83d\udc4d "]}, {"number": 49497, "title": "Remove the AMDGPU due to duplicated dependency", "body": "The PR #49163 has moved the \"llvm:AMDGPUCodeGen\" dependency into the \"if_rocm_is_configured\". However, in the merge commit (commit id: 013bf86), this dependency appears twice for ROCm builds. This makes the build to fail for AMD GPUs.", "comments": ["It seems that meanwhile this PR was being reviewed, commit  23fbdd1 reverted PR #49163. With this change, the build works fine but still misses the point of PR #49163. Please advise. @sanjoy, @gbaned ", "@cheshire Can you PTAL?  We probably should remove the copybara rule that's adding the dependency into `llvm:AMDGPUCodeGen` unconditionally.", "I thought I've approved?\n\nOn Tue, Jun 8, 2021 at 6:55 AM Fr\u00e9d\u00e9ric Bastien ***@***.***>\nwrote:\n\n> Any update on this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/49497#issuecomment-856934277>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH3JCNCUKN342WELXXDTRZDPZANCNFSM45LJBR7A>\n> .\n>\n", "@cheshire I missed that. I deleted my comment in hope that you do not see it. Sorry for the noise.", "@reza-amd  Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned Could you please take a look again. ", "Sorry, it seems the exact same thing happened again. It looks like we need to adjust something internally to prevent this bad merge, I am looking into it.", "> Sorry, it seems the exact same thing happened again. It looks like we need to adjust something internally to prevent this bad merge, I am looking into it.\r\n\r\nShould be fixed by 9929dd2386f0decaf75b7b1cc9d751d0b581d418"]}, {"number": 49496, "title": "RNN integer input error", "body": "```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx_1 = [ i for i in range(20) ]\r\nx_2 = np.array(x_1)\r\nx_3 = x_2.reshape([1,20,1])\r\n\r\nrnn = tf.keras.layers.SimpleRNN(1)\r\nrnn(x_3)\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-59-799b31aab8d4> in <module>()\r\n      7 \r\n      8 rnn = tf.keras.layers.SimpleRNN(1)\r\n----> 9 rnn(x_3)\r\n\r\n15 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)\r\n    658 \r\n    659     if initial_state is None and constants is None:\r\n--> 660       return super(RNN, self).__call__(inputs, **kwargs)\r\n    661 \r\n    662     # If any of `initial_state` or `constants` are specified and are Keras\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n   1010         with autocast_variable.enable_auto_cast_variables(\r\n   1011             self._compute_dtype_object):\r\n-> 1012           outputs = call_fn(inputs, *args, **kwargs)\r\n   1013 \r\n   1014         if self._activity_regularizer:\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent.py in call(self, inputs, mask, training, initial_state)\r\n   1571     self._maybe_reset_cell_dropout_mask(self.cell)\r\n   1572     return super(SimpleRNN, self).call(\r\n-> 1573         inputs, mask=mask, training=training, initial_state=initial_state)\r\n   1574 \r\n   1575   @property\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent.py in call(self, inputs, mask, training, initial_state, constants)\r\n    802         input_length=row_lengths if row_lengths is not None else timesteps,\r\n    803         time_major=self.time_major,\r\n--> 804         zero_output_for_mask=self.zero_output_for_mask)\r\n    805 \r\n    806     if self.stateful:\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py in rnn(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\r\n   4347     # the value is discarded.\r\n   4348     output_time_zero, _ = step_function(\r\n-> 4349         input_time_zero, tuple(initial_states) + tuple(constants))\r\n   4350     output_ta = tuple(\r\n   4351         tensor_array_ops.TensorArray(\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent.py in step(inputs, states)\r\n    788       def step(inputs, states):\r\n    789         states = states[0] if len(states) == 1 and is_tf_rnn_cell else states\r\n--> 790         output, new_states = cell_call_fn(inputs, states, **kwargs)\r\n    791         if not nest.is_nested(new_states):\r\n    792           new_states = [new_states]\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n   1010         with autocast_variable.enable_auto_cast_variables(\r\n   1011             self._compute_dtype_object):\r\n-> 1012           outputs = call_fn(inputs, *args, **kwargs)\r\n   1013 \r\n   1014         if self._activity_regularizer:\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent.py in call(self, inputs, states, training)\r\n   1369       h = K.dot(inputs * dp_mask, self.kernel)\r\n   1370     else:\r\n-> 1371       h = K.dot(inputs, self.kernel)\r\n   1372     if self.bias is not None:\r\n   1373       h = K.bias_add(h, self.bias)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py in dot(x, y)\r\n   1896     out = sparse_ops.sparse_tensor_dense_matmul(x, y)\r\n   1897   else:\r\n-> 1898     out = math_ops.matmul(x, y)\r\n   1899   return out\r\n   1900 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\r\n   3313     else:\r\n   3314       return gen_math_ops.mat_mul(\r\n-> 3315           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n   3316 \r\n   3317 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py in mat_mul(a, b, transpose_a, transpose_b, name)\r\n   5530       return _result\r\n   5531     except _core._NotOkStatusException as e:\r\n-> 5532       _ops.raise_from_not_ok_status(e, name)\r\n   5533     except _core._FallbackException:\r\n   5534       pass\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6860   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6861   # pylint: disable=protected-access\r\n-> 6862   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6863   # pylint: enable=protected-access\r\n   6864 \r\n\r\n/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: cannot compute MatMul as input #1(zero-based) was expected to be a int64 tensor but is a float tensor [Op:MatMul]\r\n\r\n------------------\r\n\r\nThis is a dtype problem. Change int to float can solve the problem.\r\nBut I don't know why. Also, the final error message does not make sense.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx_1 = [ float(i) for i in range(20) ]\r\nx_2 = np.array(x_1)\r\nx_3 = x_2.reshape([1,20,1])\r\n\r\nrnn = tf.keras.layers.SimpleRNN(1)\r\nrnn(x_3)\r\n\r\n```\r\n\r\n", "comments": ["This behavior is documented in the example of simple rnn. The input is explicitly converted to type float.\r\nSee https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN#examples", "@ymodak You are right. So this is the reason why I report it as a bug. RNN should handle the dtype problem.", "@Joocheol ,\r\n\r\nCan you please provide the tensorflow version which you are trying to execute the mentioned code.Thanks!", "@ymodak 2.4.1 \r\nI run it on the colab.", "@Joocheol This is an intended behavior since TF 2 keras layers default dtype is `float32`.\r\nSee https://www.tensorflow.org/api_docs/python/tf/keras/backend/floatx\r\nThus you need to explicitly set the data type to `float32` as mentioned in the usage examples of Keras layers SimpleRNN and [SimpleRNNCell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNNCell#examples).\r\n", "@ymodak OK. I will accept it. Thank you for your clarification.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49496\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49496\">No</a>\n"]}, {"number": 49495, "title": "Port TFL kernel Gather to TFL Micro", "body": "PR5 for issue #45196. This PR aims to finish porting GATHER from TFL to TFLM.\r\n\r\nNotes:\r\n1. For input/output tensors, only data types float and int8_t are supported;\r\n2. For position tensors, only data types int32_t and are supported;\r\n3. The reference implementation in lite/kernels/internal/reference/gather.h is not used;\r\n4. The TFLM Gather kernel supports batch_dims.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 49494, "title": "Tutorial returning 404 on small part", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/keras/regression?hl=nb\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nUnder ther part:\r\n\r\n```\r\nhist = pd.DataFrame(history.history)\r\nhist['epoch'] = history.epoch\r\nhist.tail()\r\n```\r\n\r\nParts of the tutorial returns a imbeded site which is a 404 page.\r\n\r\n### Correct links\r\n\r\nYes\r\n\r\n### Parameters defined\r\n\r\nNot relevant\r\n\r\n### Returns defined\r\n\r\nNot relevant\r\n\r\n### Raises listed and defined\r\n\r\nNot relevant\r\n\r\n### Usage example\r\n\r\nNot relevant\r\n\r\n### Request visuals, if applicable\r\n\r\n![Bug report](https://user-images.githubusercontent.com/22838996/119238071-99f1e500-bb40-11eb-8cef-4e7a90674a17.PNG)\r\n\r\n\r\n### Submit a pull request?\r\n\r\nNot relevant\r\n", "comments": ["@Iskers \r\nThis has been fixed, please refer to the page and confirm and move this to closed status as resolved.", "It looks like it should and has been resolved."]}, {"number": 49491, "title": "Fix typo in documentation.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49491) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@mcourteaux Thanks for the PR!\r\nWould you please submit this PR to the https://github.com/keras-team/keras repo?\r\nThe Keras directory under tensorflow is a staled copy.\r\n\r\nThanks.", "This is merged to keras repo.\r\nhttps://github.com/keras-team/keras/commit/c883c0a0e0c0fef186a7028861a24641ec2d214a"]}, {"number": 49489, "title": "'is_tf_type' is deprecated in tensor_util.py what else  may i use? thanks.", "body": "**System information*\r\n--Google Colab\r\n--Tensorflow v 2.4.1\r\n--Python 3.7.10\r\n--runtime (no hardware acceleration)\r\n\r\n\r\n**Describe the current behavior**\r\nI'm trying to do inference from a model I trained for object detection.\r\nI successfully exported the model.\r\n\r\nI'm using the python code to load this new model and do inference but my code keeps crashing at this part:\r\n\r\nfrom object_detection.utils import label_map_util\r\n\r\nand this is the error code:\r\n\r\n\r\n```\r\n 192 from tensorflow.python.framework.tensor_util import MakeNdarray as make_ndarray\r\n    193 from tensorflow.python.framework.tensor_util import constant_value as get_static_value\r\n--> 194 from tensorflow.python.framework.tensor_util import is_tf_type as is_tensor\r\n    195 from tensorflow.python.framework.tensor_util import make_tensor_proto\r\n    196 from tensorflow.python.framework.type_spec import TypeSpec\r\n\r\nImportError: cannot import name 'is_tf_type' from 'tensorflow.python.framework.tensor_util' \r\n\r\n```\r\n\r\nso I checked \r\n\r\ntensor_util.py (tensorflow/python/framework/tensor_util.py)\r\n\r\nand found that 'is_tf_type' is to be deprecated. The function is still there however.\r\nIs there any other way to load and use/do inference with an exported saved model? without using  'is_tf_type'?\r\n\r\nThanks in advance. \r\n\r\nhere is the code I am using by the way:\r\n\r\n```\r\n#Import the required libraries for Object detection infernece\r\nimport time\r\nimport tensorflow as tf\r\nimport object_detection.utils \r\nfrom object_detection.utils import label_map_util\r\nfrom object_detection.utils import visualization_utils as viz_utils\r\nimport os\r\nimport cv2\r\nimport matplotlib.pyplot as plt\r\n%matplotlib inline# setting min confidence threshold\r\nMIN_CONF_THRESH=.6#Loading the exported model from saved_model directory\r\n\r\n# PATH_TO_SAVED_MODEL =r'Custom_OD\\Workspace\\exported_model\\saved_model'\r\nPATH_TO_SAVED_MODEL =r'exported_model\\saved_model'\r\n\r\nprint('Loading model...', end='')\r\nstart_time = time.time()# LOAD SAVED MODEL AND BUILD DETECTION FUNCTION\r\ndetect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\r\nend_time = time.time()\r\nelapsed_time = end_time - start_time\r\nprint('Done! Took {} seconds'.format(elapsed_time))# LOAD LABEL MAP DATA\r\n\r\n# PATH_TO_LABELS=r'\\Custom_OD\\Workspace\\Annotations\\label_map.pbtxt'\r\nPATH_TO_LABELS=r'annotations\\label_map.pbtxt'\r\n\r\ncategory_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)#Image file for inference\r\n\r\n\r\n# IMAGE_PATH=r'\\Custom_OD\\Workspace\\images\\test\\00178.jpg'def load_image_into_numpy_array(path):\r\nIMAGE_PATH=r'images\\test\\testimage_(1).jpg'\r\ndef load_image_into_numpy_array(path):\r\n    \"\"\"Load an image from file into a numpy array.\r\n    Puts image into numpy array of shape (height, width, channels), where channels=3 for RGB to feed into tensorflow graph.\r\n    Args:\r\n      path: the file path to the image\r\n    Returns:\r\n      uint8 numpy array with shape (img_height, img_width, 3)\r\n    \"\"\"\r\n    return np.array(cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB))\r\nimage_np = load_image_into_numpy_array(IMAGE_PATH)# Running the infernce on the image specified in the  image path\r\n# The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\r\ninput_tensor = tf.convert_to_tensor(image_np)\r\n# The model expects a batch of images, so add an axis with `tf.newaxis`.\r\ninput_tensor = input_tensor[tf.newaxis, ...]\r\ndetections = detect_fn(input_tensor)\r\n\r\n# All outputs are batches tensors.\r\n# Convert to numpy arrays, and take index [0] to remove the batch dimension.\r\n# We're only interested in the first num_detections.\r\nnum_detections = int(detections.pop('num_detections'))\r\ndetections = {key: value[0, :num_detections].numpy()\r\n               for key, value in detections.items()}\r\ndetections['num_detections'] = num_detections# detection_classes should be ints.\r\ndetections['detection_classes'] = detections['detection_classes'].astype(np.int64)#print(detections['detection_classes'])\r\nimage_np_with_detections = image_np.copy()\r\nviz_utils.visualize_boxes_and_labels_on_image_array(\r\n      image_np_with_detections,\r\n      detections['detection_boxes'],\r\n      detections['detection_classes'],\r\n      detections['detection_scores'],\r\n      category_index,\r\n      use_normalized_coordinates=True,\r\n      max_boxes_to_draw=200,\r\n      min_score_thresh=MIN_CONF_THRESH,\r\n      agnostic_mode=False)\r\nplt.figure()\r\nplt.imshow(image_np_with_detections)\r\nprint('Done')\r\nplt.show()\r\n\r\n```\r\nwhich I found at this tutorial online \r\nhttps://medium.com/analytics-vidhya/tensorflow-2-object-detection-api-using-custom-dataset-745f30278446 \\\r\n\r\nI went to the tensorflow api docs at \r\nhttps://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/auto_examples/plot_object_detection_checkpoint.html \\\r\nbut  it uses from object_detection.utils import label_map_util as well and gives me the same error.\r\n\r\n\r\nAny advice would be much appreciated. Thanks in advance.\r\n", "comments": ["just wanted to update... I redid everything (recompiled protobuff, re-installed the detection api and re tested that it runs....) and everything works now.... the only difference.... I switched the runtime of google colab to gpu ..... just thought I'd put this here in case anyone had similar issues with colab....and the object detection api. thanks again...", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49489\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49489\">No</a>\n"]}, {"number": 49488, "title": "Doc for custom_gradient to compute gradients for trainable params and computing over batch", "body": "Fixes #26270 .\r\n\r\nThanks to @tsbertalan for gist. I took help of that and soon will be raising PR to `tensorflow/examples` repo for adding the `custom_gradient` example for polynomial.\r\n\r\ncc @alextp , @dynamicwebpaige , @tsbertalan .", "comments": ["The build failure is not because of the changes in this PR.\r\nIt because of bazel build failure and the error is something like this:\r\n`'@llvm-project//llvm:AMDGPUCodeGen' is duplicated in the 'deps' attribute of rule 'llvm_gpu_backend'`", "@allenlavoie , Thanks for feedback. I have modified the docstring as per your feedback. Please review.", "@allenlavoie , updated as per your feedback. Please review.", "@allenlavoie , i have updated commit. Can you please review again?", "@allenlavoie , Can you please approve again? It was failing doctest.", "@allenlavoie , extremely sorry. I don't know why i put ``` based docstring which was deprecated. I am not able to run doctests in my local that is why these problems. \r\nCan you please approve?", "@allenlavoie , @gbaned - This PR was approved 12 days ago but hasn't merged yet. Is there anything pending or failing for this?", "@ashutosh1919 there was some problems with our internal tool , we will process this soon. Thanks for your patience."]}, {"number": 49487, "title": "Error while installing tf-nightly in Jupyter Notebook", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: tf-nightly\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the problem**\r\nWhen trying to install **`tf-nightly`** in the **`Jupyter Notebook`** of my **`PC`**, I'm encountering errors. It is working fine in **`Google Colab`**, by the way.\r\n\r\nErrors are:\r\n\r\n```python\r\nERROR: tf-nightly-2-0-preview 2.0.0.dev20190601 has requirement tb-nightly<1.15.0a0,>=1.14.0a0, but you'll have tb-nightly 2.4.0a20201022 which is incompatible.\r\nERROR: tensorflow 2.5.0 has requirement gast==0.4.0, but you'll have gast 0.3.3 which is incompatible.\r\nERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\r\nERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\r\nERROR: tensorflow-transform 0.24.1 has requirement tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2, but you'll have tensorflow 2.5.0 which is incompatible.\r\nERROR: tensorflow-text 2.3.0 has requirement tensorflow<2.4,>=2.3.0, but you'll have tensorflow 2.5.0 which is incompatible.\r\nERROR: tensorflow-gpu 2.0.0b1 has requirement tb-nightly<1.14.0a20190604,>=1.14.0a20190603, but you'll have tb-nightly 2.4.0a20201022 which is incompatible.\r\nERROR: tensorflow-gpu 2.0.0b1 has requirement tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501, but you'll have tf-estimator-nightly 2.4.0.dev2020090201 which is incompatible.\r\n```\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`!pip install tf-nightly`\r\n\r\n**Any other info / logs**\r\nComplete Error Log is present in this attachment,\r\n[Error_Log.txt](https://github.com/tensorflow/tensorflow/files/6525736/Error_Log.txt)\r\n", "comments": ["@worldpeaceaspirer , I think you don't have CUDA support and that is what required for `tensorflow-gpu` it is trying to install.", "@worldpeaceaspirer \r\nCan you please try below commands and let us know, there clearly has been incompatibility issues, please verify [system requirements](https://www.tensorflow.org/install/pip):\r\nPlease refer to similar [gast issue here](https://github.com/tensorflow/tensorflow/issues/38734):\r\npip install --upgrade pip\r\npip install gast==0.2.2\r\npip install tensorflow\r\n\r\n- for numpy refer to:[link](https://github.com/tensorflow/models/issues/9200), please verify an dupgrade/downgrade as per error log and let us know.\r\n- ALso you may uninstall the version you have and stat fresh with tf nightly on new environment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49487\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49487\">No</a>\n", "@Saduf2019,\r\nSorry for the delayed response. My problem is not with installing **`Tensorflow`**, but with installing **`TF-Nightly`**. So, your answer didn't help me to resolve my problem. Thanks!", "@worldpeaceaspirer \r\nPlease try with stable version and let us know."]}, {"number": 49486, "title": "Suppress TF-32 warning print when disabled by TF's own API", "body": "Suppress TF's warning message about the use of TensorFloat-32 when TensorFloat-32 is disabled by its own API,  `tf.config.experimental.enable_tensor_float_32_execution(false)`.  Note that, environment variable `NVIDIA_TF32_OVERRIDE=0` will indeed disable TF32 evaluation, but it couldn't be peeked by TF at this moment. So no effect is expected from `NVIDIA_TF32_OVERRIDE=0`.", "comments": []}, {"number": 49485, "title": "Revert \"Revert: PR #48758: TFLM: Remove uint8 support for CMSIS-NN kernels\"", "body": "This reverts commit 6d8a184710164c7bcc41d3bd29fe4ef46836b6c9.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @mansnils @freddan80 ", "@advaitjain this is hard to reproduce but I have seen a problem in another context, which I suspect is the same as this. If so then this should hopefully fix the issue.\r\nBasically the fix is to use memcpy to interpret the bytes of an object as a value of a different type, as proposed here: https://en.cppreference.com/w/cpp/language/reinterpret_cast.\r\nI have a patch but I am not able to push to this PR (is it even possible?). Besides the new repo will be taken into use tomorrow.. should we create a new PR?", "I will create a new PR in the new repo.", "> I will create a new PR in the new repo.\r\n\r\nthanks!", "> @advaitjain this is hard to reproduce but I have seen a problem in another context, which I suspect is the same as this. If so then this should hopefully fix the issue.\r\n> Basically the fix is to use memcpy to interpret the bytes of an object as a value of a different type, as proposed here: [en.cppreference.com/w/cpp/language/reinterpret_cast](https://en.cppreference.com/w/cpp/language/reinterpret_cast).\r\n> I have a patch but I am not able to push to this PR (is it even possible?). Besides the new repo will be taken into use tomorrow.. should we create a new PR?\r\n\r\nGood to know. Only maintainers can push to incoming PRs so you won't be able to push to a PR that I created. The purpose was to have a shared state to see the errors. Having you make a new PR would be great.", "closing this PR in favor of the PR that @mansnils will create in https://github.com/tensorflow/tflite-micro/\r\n"]}, {"number": 49483, "title": "Disable TensorFloat-32 warning print when disabled by TF's own API", "body": "Suppress TF's warning message in `cuda_blas.cc` about the use of TensorFloat-32 if it's disabled by TF's own API. ", "comments": []}]