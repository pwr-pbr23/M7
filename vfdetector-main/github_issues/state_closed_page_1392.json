[{"number": 11301, "title": "Blas SGEMM launch failed", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I am using Resnet code from tensorflow models with some modifications.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 24\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: ('v1.2.0-rc2-21-g12f033d', '1.2.0')\r\n- **Python version**: 2.7\r\n- **CUDA/cuDNN version**: cuda_8.0.61-cudnnv5\r\n- **GPU model and memory**: Nvidia Titan X, 12 GB\r\n\r\nI am training a Resnet model on my own data from scratch. After successfully running for 65700 steps, it crashed with the following error:\r\n\r\n2017-06-30 16:27:50.302438: E tensorflow/stream_executor/cuda/cuda_blas.cc:543] failed to run cuBLAS routine cublasSgemm_v2: CUBLAS_STATUS_EXECUTION_FAILED\r\nTraceback (most recent call last):\r\n  File \"resnet_main.py\", line 178, in <module>\r\n    if z%100 == 0:\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"resnet_main.py\", line 171, in main\r\n    tf.logging.info('Loading checkpoint %s', ckpt_state.model_checkpoint_path)\r\n  File \"resnet_main.py\", line 90, in train\r\n    mon_sess.run(model.train_op)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 505, in run\r\n    run_metadata=run_metadata)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 842, in run\r\n    run_metadata=run_metadata)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 798, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 952, in run\r\n    run_metadata=run_metadata)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 798, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : m=12544, n=1024, k=256\r\n\t [[Node: unit_3_2/sub3/conv3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](unit_3_2/sub3/leaky_relu, unit_3_2/sub3/conv3/DW/read)]]\r\n\t [[Node: train_step/update/_6858 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_15509_train_step/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'unit_3_2/sub3/conv3/Conv2D', defined at:\r\n  File \"resnet_main.py\", line 178, in <module>\r\n    if z%100 == 0:\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"resnet_main.py\", line 171, in main\r\n    tf.logging.info('Loading checkpoint %s', ckpt_state.model_checkpoint_path)\r\n  File \"resnet_main.py\", line 31, in train\r\n    model.build_graph()\r\n  File \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 42, in build_graph\r\n    self._build_model()\r\n  File \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 95, in _build_model\r\n    x = res_func(x, filters[3], filters[3], self._stride_arr(1), False)\r\n  File \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 245, in _bottleneck_residual\r\n    x = self._conv('conv3', x, 1, out_filter / 4, out_filter, [1, 1, 1, 1])\r\n  File \"/s/chopin/l/grad/prady/PycharmProjects/GestureRecognition/frame/resnet_model.py\", line 273, in _conv\r\n    return tf.nn.conv2d(x, kernel, strides, padding='SAME')\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 399, in conv2d\r\n    data_format=data_format, name=name)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/s/chopin/l/grad/prady/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInternalError (see above for traceback): Blas SGEMM launch failed : m=12544, n=1024, k=256\r\n\t [[Node: unit_3_2/sub3/conv3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](unit_3_2/sub3/leaky_relu, unit_3_2/sub3/conv3/DW/read)]]\r\n\t [[Node: train_step/update/_6858 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_15509_train_step/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n2017-06-30 16:27:55.680213: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x564291240800: CUDA_ERROR_MISALIGNED_ADDRESS\r\n\r\nOn another machine with exact configuration, it is still running successfully (425,000 steps as of now). I don't understand why it crashed on one machine after running for 65,700 steps.", "comments": ["I'm sorry, but Fedora isn't a supported configuration for us. \r\nIt seems worth asking: can you restart from a checkpoint, and roll past the 67000 step failure point? Or otherwise repeat the failure? ", "@cy89 how about getting `Blas GEMM launch failed` with Ubuntu 16.04?\r\n\r\nFull log: [errors/Embedding error 2.txt](https://github.com/TheShellLand/tenderflow/blob/master/keras/errors/Embedding%20error%202.txt)", "having the same problem on ubuntu 16.04 only when using webcam instead of input video and only when using GPU and not only the CPU. Any ideas?", "Did anyone fix this? \r\n\r\nWe are getting this error (Blas SGEMM launch failed) while trying to learn word2vec embeddings. The code is pretty much the one in the TF tutorial (https://www.tensorflow.org/tutorials/word2vec). It runs without any problem on a regular GPU machine. However, we are trying to run it in a CodaLab setup (http://codalab.org/). The TF docker image is the official one from https://hub.docker.com/r/tensorflow/tensorflow/ specified as: tensorflow/tensorflow:1.0.0-gpu\r\n\r\nThe full error can be seen here: \r\nhttps://codalab.rollme.ml.cmu.edu/bundles/0x3646a0f2dc614a6a971cbeadf753cf15/", "Closing due to lack of recent activity. if you are still facing this problem, please create a new issue using one of the issue templates. Thanks!\r\n"]}, {"number": 11300, "title": "State of GPU support for commercial Android devices", "body": "Question has been asked in  [this thread](https://github.com/tensorflow/tensorflow/issues/663) but got no answer. Just curious what's the plans of TF team to support Android GPUs (or no plan at all) for commercial devices such as Nexus. I understand OpenCL is not included in Android now, but we still have RenderScript. And [some work](https://github.com/nesl/RSTensorFlow/tree/matmul_and_conv/tensorflow/contrib/android_renderscript_ops) has already been put to this end.\r\n\r\nIt would be great to know it from TF developers.", "comments": ["@petewarden and @andrewharp: can you offer any guidance?", "Most mobile acceleration work being done currently is for the Hexagon DSP, which can offer an order of magnitude speedup. @satok16 has been working on this and may be able to offer more details.\r\n\r\nWe may also support CUDA on Android in the future, but this would be limited to select Nvidia devices like the TX1/TX2.", "@andrewharp Thanks Andre for your reply. So is there any workable toy that I can play with now on any Android GPU? And I saw TF supports OpenCL now (as I configure it it asks me if I want to add OpenCL support). If I have a OpenCL library on my Android device, is it possible that TF can work without further modification?", "@PKUEcho See [master/tensorflow/contrib/hvx](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx) for instructions on running HVX-accelerated TF on supported devices.\r\n\r\nWe don't currently have internal plans to add OpenCL support on Android, but if the community wants to add it that would be great. I have no idea what sort of issues might be run into though.\r\n\r\nOtherwise I'd suggest the HVX option or waiting on CUDA support.", "Thanks @andrewharp for your reply!", "@PKUEcho For deep learning framework with mobile phone GPU support, you can have a look at [MACE](https://github.com/xiaomi/mace) and [some benchmark results](https://github.com/XiaoMi/mace/issues/1)."]}, {"number": 11299, "title": "undefined reference to 'soc_interface_AllocateInOutNodeBuffers'  Hexagon build failed with latest commit", "body": "OS: Ubuntu 16.04 64bits\r\n Android Version: 7.1 (Nougat)\r\n NDK Version: android-ndk-r12b\r\n HEXAGON SDK: 3.1\r\n nnlib source: https://source.codeaurora.org/quic/hexagon_nn/nnlib\r\n\r\nI have cloned to git commit id - 43a819e1386195f7010f8ca9c74ed96ab81913d8\r\n\r\nwhen I try to build executable for hexagon,\r\n`CC_PREFIX=${CC_PREFIX} NDK_ROOT=${NDK_ROOT} \"${BUILD_ALL_ANDROID_PATH}\" -x \"${GEN_LIBS_DIR}\" -s \"${TF_ROOT_DIR}/tensorflow/contrib/makefile/sub_makefiles/hexagon_graph_execution/Makefile.in\" -t hexagon_graph_execution`\r\n\r\nI am getting build error,\r\n```\r\ntensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(hexagon_control_wrapper.o):hexagon_control_wrapper.cc:function tensorflow::HexagonControlWrapper::Init(tensorflow::RemoteFusedGraphExecuteInfo const&): error: undefined reference to 'soc_interface_AllocateInOutNodeBuffers'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\nbuild changes are moved partially out of `tensorflow/contrib/makefile/sub_makefiles/hexagon_graph_execution/Makefile.in`\r\n\r\nis this the possible cause for error.\r\n\r\nI am not sure I can pull out latest commit and build over that, but right now I am stuck at build fail.\r\nHelp me with a fix .", "comments": ["@Satok16 provide pointers. thanks", "Is this issue still open?", "Sorry, I'm on it.  Will be fixed asap.", "@satok16, keep me posted.", "It's fixed last week.", "@satok16 , I missed it, I guess this is the commit id-06d25a7e, will check with this, \r\nthanks."]}, {"number": 11298, "title": "fix beam search for windows and enable seq2seq unit tests", "body": "fix for https://github.com/tensorflow/tensorflow/issues/11277\r\n\r\nall unit tests under contrib/seq2seq are now enabled and passing.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.\r\n\r\n", "@caisq it looks like there is a flaky test (`drop_stale_gradient_optimizer_test`):\r\n\r\n```\r\nERROR: test3WorkersStaleness0 (__main__.DropStaleGradientOptimizerTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/bazel_pip/tensorflow/contrib/opt/drop_stale_gradient_optimizer_test.runfiles/org_tensorflow/bazel_pip/tensorflow/contrib/opt/python/training/drop_stale_gradient_optimizer_test.py\", line 227, in test3WorkersStaleness0\r\n    sessions, graphs, train_ops = _get_workers(num_workers, 0)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/bazel_pip/tensorflow/contrib/opt/drop_stale_gradient_optimizer_test.runfiles/org_tensorflow/bazel_pip/tensorflow/contrib/opt/python/training/drop_stale_gradient_optimizer_test.py\", line 46, in _get_workers\r\n    server_lib.Server(cs, job_name='ps', task_index=0, start=True)\r\n  File \"/workspace/pip_test/venv/lib/python3.4/site-packages/tensorflow/python/training/server_lib.py\", line 145, in __init__\r\n    self._server_def.SerializeToString(), status)\r\n  File \"/usr/lib/python3.4/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/workspace/pip_test/venv/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\r\n\r\n----------------------------------------------------------------------\r\n```", "@drpngx Yes, I think it's an unrelated flake."]}, {"number": 11297, "title": "Matrix Inverse in TensorFlow", "body": "I have a problem related to calculating matrix inverse in TensorFlow python interface version 1.1.0 on Linux. What I'm now trying to do is that, I have an input vector as `tensorflow.float64`, say `S` and a value `V`. I augment the vector `S` to be polynomial fashion in the form ![A=[ 1, S, S^2, S^3 ]](https://latex.codecogs.com/gif.download?A%3D%5B%201%2C%20S%2C%20S%5E2%2C%20S%5E3%20%5D) and want to do a regression on `V`. I choose to compute the linear regression myself instead of using the infrastructure from tensorflow, where the regression is conducted as ![\\beta=(A^TA)^{-1}A^T\\times V](https://latex.codecogs.com/gif.download?%5Cbeta%3D%28A%5ETA%29%5E%7B-1%7DA%5ET%5Ctimes%20V). The problem occurs at the ![(A^TA)^{-1}](https://latex.codecogs.com/gif.download?%28A%5ETA%29%5E%7B-1%7D) step, where the inverse multiply the original matrix does not give an identity. However, if I feed the ![A^TA](https://latex.codecogs.com/gif.download?A%5ETA) as a constant matrix containing the same value as the preprocessed input, the result is really the inverse of itself.\r\n\r\nThe code below is a runnable version with parameter `control=True` turns on the constant input matrix version where the inverse behaves correctly. Three matrices are output by running the program, the original matrix, the \"inverse\" by `tf.matrix_inverse`, and the multiplication of the \"inverse\" with the original matrix aiming to recover an identify. `control=False` gives the same original matrix as `control=True` run, however, the recovered \"identity\" is not correct with `control=False`. I suspect something wrong with the data flow during preprocessing. However, limited by my experience with TensorFlow, I cannot spot it. Would you mind a help why the `tf.matrix_inverse` does not work as expected?\r\n\r\n\r\n    import tensorflow as tf\r\n    import pprint\r\n    \r\n    def matrixInverse( control=False ):\r\n        '''Compute inverse of a matrix.\r\n    \r\n    Parameters\r\n    ----------\r\n    control : bool\r\n        whether to use control group or not.\r\n        '''\r\n        X = tf.constant( [ [100. , 100., 100., 100.],\r\n             [ 101.75497118 ,  92.84824314 ,  95.09528336 , 103.24955959],\r\n             [ 92.33287485 ,  95.86868862 ,  84.70664178 , 107.9505686 ],\r\n             [ 85.86109085 ,  99.05621029 ,  94.24396596 , 119.60257907] ], dtype=tf.float64 )\r\n    \r\n        # extract input X\r\n        s = tf.slice( X, [ 2, 0 ], [ 1, 4 ])\r\n        s = tf.squeeze(s)\r\n        s1 = tf.multiply( tf.ones( 4, dtype=tf.float64 ), s )\r\n        s2 = tf.multiply( s, s )\r\n        s3 = tf.multiply( tf.multiply( s, s ), s )\r\n    \r\n        A = tf.concat( [ tf.ones( 4, dtype=tf.float64 ), s1, s2, s3 ], 0 )\r\n        A = tf.reshape( A, [ 4, 4 ] )\r\n    \r\n        # filter only the first element in the selected row\r\n        itm = tf.constant( [ True, False, False, False ], dtype=tf.bool )\r\n    \r\n        A = tf.boolean_mask( tf.transpose(A), itm )\r\n    \r\n        if control:\r\n            ATA = tf.constant([[  1.00000000e+00,   9.23328748e+01,   8.52535978e+03,   7.87170977e+05],\r\n                         [  9.23328748e+01,   8.52535978e+03,   7.87170977e+05,   7.26817593e+07],\r\n                         [  8.52535978e+03,   7.87170977e+05,   7.26817593e+07,   6.71091579e+09],\r\n                         [  7.87170977e+05,   7.26817593e+07,   6.71091579e+09,   6.19638148e+11]], dtype = tf.float64)\r\n        else:\r\n            ATA = tf.matmul( tf.transpose( A ), A )\r\n    \r\n        inverseATA = tf.matrix_inverse( ATA )\r\n    \r\n        sess = tf.Session()\r\n        pprint.pprint( sess.run( [ ATA, inverseATA, tf.matmul( ATA, inverseATA ) ] ) )", "comments": ["It looks in the given case, the matrix is not invertible. However, why for the same input to `matrix_inverse` the output is different? The forward computation should be deterministic, right?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11296, "title": "No OpKernel was registered to support Op 'LesseEqual' with these attrs on Android", "body": "On Android I am trying to load a tensorflow graph which I have frozen by using convert_variables_to_constants however I am getting:\r\n\r\ncom.example.trio.tensordemo E/TensorflowDebug: \r\njava.lang.IllegalArgumentException: No OpKernel was registered to support Op 'LessEqual' with these attrs.  Registered devices: [CPU], \r\nRegistered kernels:\r\n<no registered kernels>\r\n[[Node: bidirectional_rnn/fw/fw/LessEqual = LessEqual[T=DT_INT32, _device=\"/device:GPU:0\"](bidirectional_rnn/fw/fw/Max, bidirectional_rnn/fw/fw/LessEqual/y)]]\r\nThis is what the node looks like:\r\n\r\nname: \"bidirectional_rnn/fw/fw/LessEqual_1\"\r\nop: \"LessEqual\"\r\ninput: \"bidirectional_rnn/fw/fw/Max\"\r\ninput: \"bidirectional_rnn/fw/fw/LessEqual_1/y\"\r\ndevice: \"/device:CPU:0\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_INT32\r\n  }\r\n}\r\nThe tensorflow version I trained and loaded in Android are both r1.2.\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nAs an aside, is there any chance that you've registered the kernel on GPU, but the node is trying to find the CPU variant, so that's why you're not finding the correctly registered kernel?"]}, {"number": 11295, "title": "Different batch size different result with inception_v2", "body": "I user inception_v2 as a base network for classification. During training, the batchsize=128. During testing, if the batchsize=128, everything is ok. However, if the batchsize is smaller than 128 the results are different. And the precision declines as the batchsize drops. If the batchsize=1, the network will failed. I also used inception_v3 and inception_v1, the same problems appered. However, if the base network is replaced with Alex network (tensorflow), everything goes well. \r\nI also replace the inception_v2 with vgg (slim), and everything goes well. \r\nThe bug is associated with inception_v1~v3.  I think I have not used inception_v2 properly. Did anyone encounter similar problems?", "comments": ["_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Overflow._\r\n\r\nIt looks like you are asking about what Batch Size is. It is expected that reducing the batch size will reduce the accuracy. This is because you are giving your model a limited view of the data. But this is ok because we go over all the batches. There is a trade off with batch size and other performance aspects.\r\n\r\nSee this question for information https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network", "@jubjamie Hi Jubjamie, thanks for your reply. However, my question is not about how to select the batch size during training, but about the batch size during testing. I found the output are complete different when the batch size are different during testing.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@andydavis1 Thanks, I will.", "@JackieLeeTHU11 I meet the same problem. Have you solved it? Thanks", "Do you update moving_mean and moving_variance variables in batch normalization during optimization?\r\nI had the same problem with resnet and I added this in training, then it worked fine:\r\n`update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)`\r\n` with tf.control_dependencies(update_ops):`\r\n    `train_op = opt.minimize(loss)`", "@JackieLeeTHU11  Did you solve the problem?", "hmm, if its about batch size in **testing** it looks indeed a problem with tensorflow. why is this closed?", "It's not a problem in tensorflow, just lacking of well documentation. Batch normalization variables are not updated in the graph automatically during optimization if you don't explicitly add them to dependency as I wrote in my earlier comment. That's why this only happens in inception and resnet. The other networks don't have batch norm as far as I know.", "@azadef It still doesn't work to me. Can you share me your script?", "I solved this problem. As @azadef said, the problem is because of batch norm. We should set is_training=True in training and False in testing. Also, use train_op = slim.learning.create_train_op(). instead of train_op = optimizer.minimize()."]}, {"number": 11293, "title": "tf.reverse doesn't support tensor with unknown shape", "body": "### System information\r\n   ubuntu 14.04\r\n   tensorflow installed from binary\r\n   tensorflow version: tensorflow-gpu  1.2.1\r\n   python version: python 2.7\r\n   CUDA/CuDNN version: cuda 8.0/ CUDNN 5.1\r\n   GPU model\r\n   \r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\n### Describe the problem\r\nHey tensorflow community. I want to feed an unknown shape tensor into tf.placeholder. Then it is reversed. But it reports errors\r\na = tf.placeholder(dtype = tf.int64,shape = [None])\r\nb = tf.reverse(a,axis = 0)\r\n\r\n### Source code / logs\r\n![1](https://user-images.githubusercontent.com/15981416/27864587-c449c5d4-61c1-11e7-852d-fa8911f9e429.png)\r\n![2](https://user-images.githubusercontent.com/15981416/27864596-ca4d3790-61c1-11e7-9568-bc822a9917bb.png)\r\n\r\n\r\n", "comments": ["The docs for `tf.reverse()` indicate (perhaps too subtly...) that the `axis` argument must be one-dimensional, so you should instead pass `axis=[0]`. The following code works as intended:\r\n\r\n```python\r\na = tf.placeholder(dtype=tf.int64, shape=[None])\r\nb = tf.reverse(a, axis=[0])\r\nsess = tf.Session()\r\nprint sess.run(b, feed_dict={a: [1, 2, 3]})  # Prints \"[3, 2, 1]\".\r\n```", "@mrry thank you mrry."]}, {"number": 11292, "title": "Return output_graph_def in tools/freeze_graph.py", "body": "It makes sense that `def freeze_graph` works with files for both the input graph and frozen output graph.\r\n\r\nIt does not make sense that `def freeze_graph_with_def_protos` only uses a tf.GraphDef for the input, but doesn't actually return the frozen graph (and instead only writes it to a file).\r\n\r\nThis pull request changes `freeze_graph_with_def_protos` to optionally write the output to file (for backwards-compatibility) when `output_graph` is set as usual, but also returns the GraphDef protobuf to make the API more symmetric.\r\n\r\nOtherwise you have to write and read from a tempfile just to use freeze_graph, which is a hassle.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11291, "title": "R0.12", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "It doesn't look like a valid request. Please start over and open a new one. Thanks!"]}, {"number": 11290, "title": "Restoring SavedModel in //tensorflow/c:c_api_test fails ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version**: 'v1.2.1-0-gb4957ff', '1.2.1'\r\n- **Python version**: 2.7.12\r\n- **Bazel version**: 0.4.5\r\n- **CUDA/cuDNN version**: No GPU\r\n- **GPU model and memory**: No GPU\r\n- **Exact command to reproduce**: bazel test //tensorflow/c:c_api_test\r\n\r\n### The problem:\r\nWhile executing c_api_test from c module on a big endian machine, it fails while restoring SavedModel(tensorflow/cc/saved_model/testdata/half_plus_two/00000123). There is a warning displayed `Reading a bundle with different endianness from the reader`. \r\n\r\nSimilarly, other tests from cc, java module which read this SavedModel from testdata also fail due to endianness mismatch.\r\n\r\nWhat would be appropriate way to handle this failure on big endian?\r\nIs there a way to convert the above SavedModel to Big Endian while reading it in the tests?\r\n\r\n### Test Logs:\r\n```\r\n[ RUN      ] CAPI.SavedModel\r\n2017-06-29 09:04:33.999662: I tensorflow/cc/saved_model/loader.cc:226] Loading SavedModel from: <HOME>/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/c/c_api_test.runfiles/org_tensorflow/tensorflow/cc/saved_model/testdata/half_plus_two/00000123\r\n2017-06-29 09:04:34.039733: I tensorflow/cc/saved_model/loader.cc:145] Restoring SavedModel bundle.\r\n2017-06-29 09:04:34.079006: W tensorflow/core/framework/op_kernel.cc:1158] Unimplemented: Reading a bundle with different endianness from the reader\r\n2017-06-29 09:04:34.079311: W tensorflow/core/framework/op_kernel.cc:1158] Unimplemented: Reading a bundle with different endianness from the reader\r\n2017-06-29 09:04:34.079363: W tensorflow/core/framework/op_kernel.cc:1158] Unimplemented: Reading a bundle with different endianness from the reader\r\n2017-06-29 09:04:34.079708: I tensorflow/cc/saved_model/loader.cc:274] Loading SavedModel: fail. Took 136931 microseconds.\r\ntensorflow/c/c_api_test.cc:1198: Failure\r\n      Expected: TF_OK\r\n      Which is: 0\r\nTo be equal to: TF_GetCode(s)\r\n      Which is: 12\r\nReading a bundle with different endianness from the reader\r\n         [[Node: save/RestoreV2 = RestoreV2[_output_shapes=[[]], dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_1, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n```", "comments": ["There's a suggestion of a workaround for Endianness conversion in #2619. Also some discussion of how to set and handle the IS_LITTLE_ENDIAN flag.  \r\nI think we're in the unfortunate situation of not being able to test big-endian code. We're happy to accept patches that add big-endian support. ", "@cy89 Thanks for your reply. I have checked the flag used to detect underlying platform.  \r\nThe flag `kLittleEndian `declared [here ](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/core/platform/cpu_info.h#L29) can be used to check for big endian However, the `c_api_test`  fails on the SavedModel from test data which is in raw format and we are not sure about an appropriate way to convert this raw data suitable for big endian. \r\n", "Further analysis on this test shows that there is a check for endianness.  In case of BIG endian, test fails with \"`Unimplemented: Reading a bundle with different endianness from the reader`\". \r\nI tried to bypass this check and allowed the execution further for this test however it gives wrong output. \r\nThe tensor bundle write/read seems to be working for big endian so now suspecting that the test data for big endian systems can be uploaded separately. But not sure how to generate the test data. \r\nCan you please confirm this understanding is correct? Also, could you let us know the steps which will generate test data. \r\n@aselle @poxvoculi Could you please provide your inputs on this? \r\n\r\n", "@asimshankar: Do you know who's in a position to fix c_api_test.cc?", "I think the larger question raised by this is what our position should be regarding serialization formats. Regenerating the testdata would make this test pass, but we'd still have issues where say a SavedModel produced by a little endian machine will be unreadable on a big endian machine and vice-versa. Similarly, any tensors sent over RPCs across between machines with different architectures would likely fail.\r\n\r\nA more principled fix would be something that perhaps includes the encoding format in the serialization and translates if necessary when deserializing. Something like that, I believe, would be the right way to go about ensuring TensorFlow is big-endian friendly.\r\n\r\nThat said, for specifically this test, you could regenerate a saved model in big-endian format in the testdata directory by running https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/saved_model/saved_model_half_plus_two.py , committing the output and changing `c_api_test.cc` to use one path or the other based on the architecture it's running on.\r\n\r\nUnassigning myself as both changes outlined above is not something that I, or anyone else on the TensorFlow team (I think), will be getting to in the near future. But we'd be happy to accept PRs.", "@asimshankar: Model serialization is outside my area of expertise, but I saw this related issue last week: #11908  Why is model save/restore sensitive to endianness?  Are we not saving in protobuf format?", "@poxvoculi : I'm not entirely sure about the history behind this as it predates my involvement in the project. Perhaps @zffchen78 has some historical knowledge?\r\n\r\nBut looking at the code, when we write out a tensor to disk (see [`tensor_bundle.cc`](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/util/tensor_bundle/tensor_bundle.cc#L138)), it seems we're dumping the raw contents of the `TensorBuffer`, which will be in the endianness of the architecture it was on. Even when we dump to a proto ([`Tensor::AsProtoTensorContent`](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/tensor.cc#L727)), the [memory buffer is being used as-is](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/tensor.cc#L122)). And in neither do we make a note of the endianness of the host when writing it out.\r\n\r\nIt doesn't look like we use the endian-safe routines in `coding.cc` for tensor content.", "@asimshankar @poxvoculi Thank you for your inputs. \r\n\r\nWe could use the script [saved_model_half_plus_two.py](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/examples/saved_model/saved_model_half_plus_two.py) (referred in previous comment) to regenerate a saved model testdata in big-endian format (specifically s390x).  With the changes in source code` test/BUILD` files, now below tests are passing:\r\n```\r\n//tensorflow/c:c_api_test\r\n//tensorflow/cc/saved_model:loader_test\r\n//tensorflow/java:SavedModelBundleTest\r\n//tensorflow/contrib/session_bundle:bundle_shim_py_test\r\n//tensorflow/contrib/session_bundle:bundle_shim_test\r\n```\r\n\r\nOn further execution of test modules like` tensorflow/contrib, tensorflow/tools` I found that there are few tests which are failing with same reason. i.e. testdata in little endian format. \r\n```\r\n//tensorflow/python/feature_column:feature_column_test\r\n//tensorflow/tools/tfprof/internal:tfprof_show_test\r\n//tensorflow/tools/tfprof/internal:tfprof_stats_test\r\n//tensorflow/tools/tfprof/internal:tfprof_tensor_test\r\n//tensorflow/contrib/session_bundle:session_bundle_py_test\r\n//tensorflow/contrib/session_bundle:session_bundle_test\r\n```\r\nWe could also generate partial[ testdata for session bundle](https://github.com/tensorflow/tensorflow/tree/v1.2.1/tensorflow/contrib/session_bundle/testdata/half_plus_two/00000123) by executing script https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/contrib/session_bundle/example/export_half_plus_two.py used by session_bundle tests. \r\n\r\nNow,  we need to generate testdata used by below modules as well: \r\n1. `tensorflow/session_bundle/testdata/half_plus_two_ckpt_v2`\r\nhttps://github.com/tensorflow/tensorflow/tree/v1.2.1/tensorflow/contrib/session_bundle/testdata/half_plus_two_ckpt_v2/00000123\r\n2. `tensorflow/python/feature_column `\r\nhttps://github.com/tensorflow/tensorflow/tree/v1.2.1/tensorflow/python/feature_column/testdata\r\n3. ` tensorflow/tools/tfprof/internal/testdata`\r\nhttps://github.com/tensorflow/tensorflow/tree/v1.2.1/tensorflow/tools/tfprof/internal/testdata\r\n\r\n\r\nI tried to search for scripts/commands that could generate this testdata however didn't get much information. Am I missing something here?\r\n Could you please provide us pointers?\r\n\r\n\r\n\r\n", "@lilao can you provide guidance on session_bundle/testdata?\r\n\r\n@ispirmustafa can you provide guidance on feature_column_test?\r\n\r\n@panyx0718 can you advise on tprof/internal/testdata?", "tprof/internal/testdata is a tiny model in python/profiler/model_analyzer_test.py. However, there is no script to automatically regenerate the data. And regenerating the data would require change of test results (due to time/memory dynamics).\r\n\r\nThe tests don't affect save/restore model. So, it should be safe to disable in your environment.", "Same here, there is no script for feature_column_test and you can disable feature_column tests for your environment.", "While we can skip tests/update testdata, this does mean that the serialized tensors will only be usable on the architecture that created it, which is a sad situation to be in.\r\n\r\n@namrata-ibm : Would you have the bandwidth to contribute a more principled fix where we could record the endinaness of the creator in the checkpoint and do the conversion if necessary when we load the checkpoint? ", "@asimshankar, agree with you that this needs a better fix. However right now some other functionality needs attention on BE. Focusing on that first to make v1.2.1 available for BE and then consider this.\r\n\r\nFor now, skipping these tests for BE and accordingly raising a PR.\r\n\r\n@asimshankar , @poxvoculi , @panyx0718 , @ispirmustafa  Thank you for your guidance.", "Looks liked fixed by https://github.com/tensorflow/tensorflow/pull/28490\r\nThis issue can be closed", "I'm still facing an error in loading a model on s390x from a protobuf file is still there. I think #41652 is related to his issue.\r\n\r\n**CC**: @Nayana-ibm @asimshankar "]}, {"number": 11289, "title": "argument --learning_rate: conflicting option string: --learning_rate", "body": "Hi everyone,\r\n\r\nI'm currently learning how to user Tensorflow, but when I'm running the fully_connected_feed.py file from tensorflow website. It returns the following error: \r\nargument --learning_rate: conflicting option string: --learning_rate\r\n\r\nDoes someone know how to fix the issue?\r\n\r\nThis is the entire Error message: \r\n\r\n\r\n  File \"<ipython-input-8-db6c9214eb7b>\", line 1, in <module>\r\n    debugfile('C:/Users/X188068/Desktop/Merck/Vevey Deviation/fully connected feed__Mechanics 101.py', wdir='C:/Users/X188068/Desktop/Merck/Vevey Deviation')\r\n\r\n  File \"C:\\Users\\X188068\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 888, in debugfile\r\n    debugger.run(\"runfile(%r, args=%r, wdir=%r)\" % (filename, args, wdir))\r\n\r\n  File \"C:\\Users\\X188068\\AppData\\Local\\Continuum\\Anaconda3\\lib\\bdb.py\", line 431, in run\r\n    exec(cmd, globals, locals)\r\n\r\n  File \"<string>\", line 1, in <module>\r\n\r\n  File \"C:\\Users\\X188068\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 866, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\Users\\X188068\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 102, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"c:/users/x188068/desktop/merck/vevey deviation/fully connected feed__mechanics 101.py\", line 25, in <module>\r\n    flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\r\n\r\n  File \"C:\\Users\\X188068\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py\", line 132, in DEFINE_float\r\n    _define_helper(flag_name, default_value, docstring, float)\r\n\r\n  File \"C:\\Users\\X188068\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py\", line 65, in _define_helper\r\n    type=flagtype)\r\n\r\n  File \"C:\\Users\\X188068\\AppData\\Local\\Continuum\\Anaconda3\\lib\\argparse.py\", line 1348, in add_argument\r\n    return self._add_action(action)\r\n\r\n  File \"C:\\Users\\X188068\\AppData\\Local\\Continuum\\Anaconda3\\lib\\argparse.py\", line 1711, in _add_action\r\n    self._optionals._add_action(action)\r\n\r\n  File \"C:\\Users\\X188068\\AppData\\Local\\Continuum\\Anaconda3\\lib\\argparse.py\", line 1552, in _add_action\r\n    action = super(_ArgumentGroup, self)._add_action(action)\r\n\r\n  File \"C:\\Users\\X188068\\AppData\\Local\\Continuum\\Anaconda3\\lib\\argparse.py\", line 1362, in _add_action\r\n    self._check_conflict(action)\r\n\r\n  File \"C:\\Users\\X188068\\AppData\\Local\\Continuum\\Anaconda3\\lib\\argparse.py\", line 1501, in _check_conflict\r\n    conflict_handler(action, confl_optionals)\r\n\r\n  File \"C:\\Users\\X188068\\AppData\\Local\\Continuum\\Anaconda3\\lib\\argparse.py\", line 1510, in _handle_conflict_error\r\n    raise ArgumentError(action, message % conflict_string)\r\n\r\nArgumentError: argument --learning_rate: conflicting option string: --learning_rate\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11288, "title": "Fix typos", "body": "This PR fixes some typos: `parition`, `partiton`, `grpah`, `excuted`, `definitons`, and `operaiton`.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11287, "title": "Missing definition in Getting Started With TensorFlow guide", "body": "\r\n### System information\r\n- macOS 10.12.5\r\n- installed from binary\r\n- Tensorflow v1.2.0-1751-g43a819e13 1.2.1\r\n- Python 3.6.1\r\n- Bazel 0.5.2-homebrew\r\n\r\n### Describe the problem\r\nThe custom model section of the [\"Getting Started With TensorFlow\" guide](https://www.tensorflow.org/get_started/get_started) doesn't define \"eval_input_fn\".\r\n\r\nI ran across the error when I copied the code line by line and ran it on my machine. I fixed it by adding the following definition after \"input_fn\" is defined:\r\n\r\n```\r\neval_input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\":x_eval}, y_eval,\r\n                                                   4, num_epochs = 1000);\r\n```\r\n\r\n### Source code / logs\r\nCode in Guide:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n# Declare list of features, we only have one real-valued feature\r\ndef model(features, labels, mode):\r\n  # Build a linear model and predict values\r\n  W = tf.get_variable(\"W\", [1], dtype=tf.float64)\r\n  b = tf.get_variable(\"b\", [1], dtype=tf.float64)\r\n  y = W*features['x'] + b\r\n  # Loss sub-graph\r\n  loss = tf.reduce_sum(tf.square(y - labels))\r\n  # Training sub-graph\r\n  global_step = tf.train.get_global_step()\r\n  optimizer = tf.train.GradientDescentOptimizer(0.01)\r\n  train = tf.group(optimizer.minimize(loss),\r\n                   tf.assign_add(global_step, 1))\r\n  # ModelFnOps connects subgraphs we built to the\r\n  # appropriate functionality.\r\n  return tf.contrib.learn.ModelFnOps(\r\n      mode=mode, predictions=y,\r\n      loss=loss,\r\n      train_op=train)\r\n\r\nestimator = tf.contrib.learn.Estimator(model_fn=model)\r\n# define our data sets\r\nx_train = np.array([1., 2., 3., 4.])\r\ny_train = np.array([0., -1., -2., -3.])\r\nx_eval = np.array([2., 5., 8., 1.])\r\ny_eval = np.array([-1.01, -4.1, -7, 0.])\r\ninput_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x_train}, y_train, 4, num_epochs=1000)\r\n\r\n# WHERE I ADDED THE DEFINITION\r\n\r\n# train\r\nestimator.fit(input_fn=input_fn, steps=1000)\r\n# Here we evaluate how well our model did. \r\ntrain_loss = estimator.evaluate(input_fn=input_fn)\r\neval_loss = estimator.evaluate(input_fn=eval_input_fn)\r\nprint(\"train loss: %r\"% train_loss)\r\nprint(\"eval loss: %r\"% eval_loss)\r\n```\r\n\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"custom_model.py\", line 37, in <module>\r\n    eval_loss = estimator.evaluate(input_fn=eval_input_fn);\r\nNameError: name 'eval_input_fn' is not defined\r\n```", "comments": ["This is a duplicate of #11123 but I think the information is probably relevant and useful for that issue too.", "As #11123 is closed and I see the fix also on https://www.tensorflow.org/get_started/get_started I guess this issue can be closed.", "Yeah, it can, thanks."]}, {"number": 11286, "title": "tensorboard fetching sprite image... parsing metadata takes forever", "body": "When i study tensorboard embedding, My code refers #6322 @danielgordon10. Whatever my datasets size is(I modify `batch_size` , `epoches` and `embedding data_szie`, but it doesn't works ), `localhost:6006#embedding` takes forever and the browser  shows always `fetching sprite image... parsing metadata`. What should i do? Could someone give me any ideas?\r\n", "comments": ["The reason why my hardware is old. When i change a new machine, it displays normally. "]}, {"number": 11285, "title": "Fix quantization tutorial", "body": "Fix [this tutorial on quantization](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md).\r\n\r\n1. Remove outdated dependencies, as pointed out in #10463.\r\n\r\n1. The change made in #10592 was incomplete. The [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md#how-can-you-quantize-your-models) on how to run `transform_graph` to quantize a graph should follow [this tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#eight-bit-calculations) about `graph_transforms`, but the argument `--inputs='Mul'` is missing.\r\n\r\n@petewarden I'm also wondering when/how will [the tutorial on tensorflow.org](https://www.tensorflow.org/performance/quantization) get updated? I'm assuming it should be the same as `quantization.md` that I'm fixing?\r\n\r\nRelated to #11181.\r\n\r\nThanks!", "comments": ["Can one of the admins verify this patch?", "@saxenasaurabh does that look right?\r\n\r\nJenkins, test this please.", "This looks correct, and if it's wrong I'm sure we'll get another PR to fix it :P"]}, {"number": 11284, "title": "Error when build tfprof, bazel version 0.5.2", "body": "Hi, I want to try with tfprof, but failed to build the tool. The error information is as follow. \r\n\r\n### System information\r\n- **OS Platform and Distribution**:  Linux Ubuntu 14.04\r\n- **Bazel version (if compiling from source)**: 0.5.2\r\n- **Exact command to reproduce**: ```bazel build --config opt tensorflow/tools/tfprof/...```\r\n\r\n```\r\n~/tensorflow-src$ ~/bin/bazel build --config opt tensorflow/tools/tfprof/...\r\nExtracting Bazel installation...\r\n...........\r\nWARNING: Config values are not defined in any .rc file: opt\r\nERROR: /home/feigao/tensorflow-src/tensorflow/core/BUILD:1415:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /home/feigao/tensorflow-src/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /home/feigao/tensorflow-src/tensorflow/core/BUILD:1415:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /home/feigao/tensorflow-src/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /home/feigao/tensorflow-src/tensorflow/core/BUILD:1415:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /home/feigao/tensorflow-src/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: Analysis of target '//tensorflow/tools/tfprof/internal:tfprof_tensor_test' failed; build aborted.\r\n```", "comments": ["Which branch are you in?\r\n\r\nCheckout released branch, e.g. `v1.2.0`, and retry.", "@ScorpioCPH I was using master branch. I also tried with v1.2.0, and it also failed with same errors.", "Do you run `./configure` before bazel buiild?", "Please see #4279, which seems relevant. Sorry we haven't figured out better error messages."]}, {"number": 11283, "title": "grappler swap_to_host bug fix", "body": "I have already solved the problem mentioned in the following issue:\r\n https://github.com/tensorflow/tensorflow/issues/11163\r\n\r\nThis fix actually is trivial and could please take a look at it to see whether it is ready to be merged?", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11282, "title": "grappler swap bug fix", "body": "I have already solved the problem mentioned in the following issue and tested in my local environment:\r\n   https://github.com/tensorflow/tensorflow/issues/11163\r\n\r\nThe code change is actually trivial and could you please take a look to see whether it is ready for being merged? \r\n\r\nBTW, I am already applying for the CLA. \r\n\r\nThanks\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 11281, "title": "Failed to load the native TensorFlow runtime.", "body": "### Describe the problem\r\nI'm not sure what I did incorrectly. I opened up the anaconda prompts and followed the steps here: https://gist.github.com/jeffgreenca/28e0fe58644b8af48f97a3e18fe08302\r\n\r\n### Source code / logs\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-2-41389fad42b5>\", line 1, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"/sscc/home/s/snu8359/.conda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n\r\n  File \"/sscc/home/s/snu8359/.conda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"/sscc/home/s/snu8359/.conda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/sscc/home/s/snu8359/.conda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/sscc/home/s/snu8359/.conda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/sscc/home/s/snu8359/.conda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/sscc/home/s/snu8359/.conda/envs/tensorflow/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/sscc/home/s/snu8359/.conda/envs/tensorflow/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /sscc/home/s/snu8359/.conda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "comments": ["You are using tensorflow on a platform that is too old for the current binary. You need one that has `glibc-2.14` or later. At this point, your options are to upgrade your system or build tensorflow yourself."]}, {"number": 11280, "title": "Can't access gs:// logfiles using tensorboard", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\nUbuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: \r\npip install\r\n- **TensorFlow version (use command below)**:\r\n1.2.1\r\n- **Python version**:  \r\n3.6\r\n- **Exact command to reproduce**:\r\n`tensorboard --logdir=gs://mybucket\r\n`\r\n\r\n### Describe the problem\r\nWhen trying to run tensorboard from a google cloud storage bucket the following error occurs:\r\n\r\n`tensorflow.python.framework.errors_impl.UnimplementedError: File system scheme gs not implemented\r\n`\r\n\r\nEven after running gs authentication\r\n`gcloud auth application-default login`\r\n\r\n\r\n### Source code / logs\r\n\r\nI was following [this guide](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/running_pets.md) on training a pet object detector", "comments": ["@rinugun any idea?", "@slandersson does the technique in #4480 help?", "This can happen if you build TensorFlow from source and answer \"No\" to the question \"Do you wish to build TensorFlow with Google Cloud Platform support?\". However you indicated that you installed a pre-built TensorFlow via `pip install` which should presumably have this option enabled. Adding @jhseu to help.", "I verified that gs is included in our builds. Maybe you're using the wrong version of tensorboard? You should use `pip3 install` for Python 3.\r\n\r\nMaybe try to `pip uninstall tensorflow` first and `pip3 install` TF.\r\n\r\nClosing because I manually verified gs paths work with the latest builds.", "I also encountered such problem, installed both version (python 2 and 3) with conda.", "I am also facing the same issue:\r\n\r\nSystem information:\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n\r\nTensorFlow installed from (source or binary):\r\n`pip3 install tensorflow`\r\n\r\nTensorFlow version (use command below):\r\n`1.3.0`\r\n\r\nPython version:\r\n`3.5.2`\r\n\r\nExact command to reproduce:\r\n`tensorboard --logdir=gs://mybucket`\r\n\r\n", "@jhseu It looks like this was closed prematurely? Since @yatendragoel is reproing with tf1.3 (which has a recent TensorBoard).", "@RedAlice64 are you on Windows too? It's known not to work on Windows.", "I'm facing the same problem, on Debian. ", "Facing the same issue, on MacOS", "I still can't reproduce this issue on both the Linux and MacOS with pip install tensorflow.", "I can reproduce this on MacOS 10.13, python 3.6.3.\r\n\r\n```\r\n$ python -V\r\nPython 3.6.3 :: Anaconda, Inc.\r\n\r\n$ pip install tensorflow tensorboard\r\n\r\n$ pip freeze|grep tensor\r\ntensorboard==1.0.0a6\r\ntensorflow==1.4.1\r\ntensorflow-tensorboard==0.4.0rc3\r\n```\r\n\r\n@jhseu what's your setup?", "It seems that the problem comes from installing the package called `tensorboard`. If I just go `pip install tensorflow`, it installs `tensorflow` and `tensorflow-tensorboard` which works fine.\r\n\r\nSo: Avoiding the package `tensorboard` and only installing `tensorflow` solved this for me. ", "/CC @jart for package collisions.", "Sorry if I'm asking in the wrong place, but there is some workaround to run tensorboard from windows pointing to a cloud storage? Or I must wait for the fix? I have the same error described here and no clue!", "It sounds like you need to install tensorflow, which includes tensorboard, or just tensorboard standalone, but not both.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Yes the `tensorboard` package was created by a third party and we're going to be moving `tensorflow-tensorboard` over to `tensorboard` on PyPi in the next week or so.", "Did #14856 fully add in GCS support in Windows?\r\n\r\nBoth 1.5.0rc1 and nightly (1.6.0.dev20180117) still complain that gs scheme is not implemented.", "I'm experiencing this issue trying to go through this tutorial running_pets.md.\r\n\r\nWindows 10\r\nTensorFlow: 1.6.0-dev20180126\r\n\r\nTried different TensorFlow versions and everything on this issue, any other ideas?", "I am facing the same challenge. Have not tried TF1.7 or TF1.6 for the issue.  \r\n\r\nAny resolution for this?", "TF1.7 has the same issue in the packages", "I don't know off the top of my head if TensorFlow is built with GCS support out of the box. cc: @gunan It might be necessary to build TensorFlow from source and enable that support when you run `./configure`. This shouldn't be tensorboard related.", "GCS support should have been enabled by default on our packages.\r\n@case540 @av8ramit to confirm.", "Windows has GCS disabled by default. Is this something that should be changed?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/configure.py#L1466", "Ah you are correct. On windows it is not as simple as just flipping a flag because GCS depends on curl, and we have not set the curl build properly yet. So on windows cloud builds are off.\r\nThanks for catching this Michael!", "Its there a workaround for this issue in windows ? ( besides downloading the output to local )", "At the moment, no.\r\nHowever, if anyone is willing to add GCS support on windows, I am happy to review the PR.", "I'd ideally like to get GCS / AWS / etc. support in TensorBoard, most of the time, without having to `import tensorflow`. I know with Linux I can invoke a `gsutil` subprocess. Is there something like that on Windows? How hard could it be?", "It looks like google cloud SDK binaries for windows are available:\nhttps://cloud.google.com/sdk/downloads\nSo subprocessing may work, but I am reluctant to make the GCS support for\nTF on windows to go through subprocessing.\n\nOn Wed, May 9, 2018 at 10:23 AM Justine Tunney <notifications@github.com>\nwrote:\n\n> Reopened #11280 <https://github.com/tensorflow/tensorflow/issues/11280>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11280#event-1618147755>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOV9j5P6yU-MVgn4L4r_f7GoheZcYks5twyYJgaJpZM4ONrM7>\n> .\n>\n", "Unassigning because TensorBoard probably does not need this.\r\n\r\nRunning TensorBoard on a Windows PC to ingest gigabytes of event logs from Google Cloud Storage is almost certainly a mistake. It can result in bandwidth charges. Both from your ISP and GCP. If you have a Windows PC and use GCS, please run TensorBoard on GCE in the same zone. For example:\r\n\r\n```sh\r\n# Create VM\r\ngcloud config set compute/zone us-central1-a\r\ngcloud compute instances create tensorboard\r\ngcloud compute ssh tensorboard -- -T sudo /bin/sh <<EOF\r\napt-get update\r\napt-get install python-setuptools\r\neasy_install -U pip\r\npip install -U tensorflow\r\nEOF\r\n\r\n# Run TensorBoard remotely on VM and ask SSH to port tunnel (don't edit GCP firewall).\r\ngcloud compute ssh tensorboard -- -L 6006:localhost:6006 tensorboard --logdir=gs://bucket/experiment\r\n\r\n# Show TensorBoard\r\ngoogle-chrome http://localhost:6006/\r\n```", "I'm facing the same issue. Does anyone has solution for this? Thank you", "I have created an issue that describes the requested feature cleanly.\r\nI am closing this issue as the duplicate of #19297\r\n", "+1 to what Justine said, but if anyone wants to add support for GCS on Windows, feel free to send me a pull request.", "So it means I should not use Google AI Platform to train my model because I cannot visualize it through tensorboard on Windows for now?", "Bandwidth concerns aside, I seem to be able to run tensorboard in WSL on Windows 10.", "I had to first run:\r\n\r\n`gcloud auth application-default login`\r\n\r\nand then launch tensorboard.\r\n\r\nhttps://stackoverflow.com/questions/40830085/tensorboard-can-not-read-summaries-on-google-cloud-storage"]}, {"number": 11279, "title": "Want to build tensorflow for android but stuck at this problem.", "body": "OS Platform and Distribution : Windows 10\r\nBazel version :0.5.2\r\n\r\nC:\\Users\\dulam\\tensorflow>bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so\r\nERROR: C:/users/dulam/tensorflow/tensorflow/core/BUILD:1415:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by C:/users/dulam/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: C:/users/dulam/tensorflow/tensorflow/core/BUILD:1415:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by C:/users/dulam/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: C:/users/dulam/tensorflow/tensorflow/core/BUILD:1415:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by C:/users/dulam/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: Analysis of target '//tensorflow/contrib/android:libtensorflow_inference.so' failed; build aborted.", "comments": ["For windows, it's best that you use the CMake solution right now, in `contrib/cmake`.", "I'm new to this, can you give me a reference link that explains the procedure you just said. @drpngx ", "Oh, wait, do you want to cross-compile from windows to android?", "I want to train a classifier on my windows machine and use it in an android application.", "Ah, OK. I'd recommend downloading the binary for windows, building your model, then downloading the binary for android, then copying the binaries over. There is no HOWTO AFAIK.\r\n\r\n@mhyttsten might have a clue. If not, that might be a good document to have.", "You can just copy the binaries into the project. I just got them from the nightly and put them into my libs folder in my project.  I would recommend looking at the android demo app here for more information: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android\r\n\r\n@drpngx when I started with the android stuff a few months ago it was incredibly painful getting it to work, especially on windows. I think a proper howto on getting your android project set up with Tensorflow would be handy. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/README.md is a good start but I think some clear documentation and/or tutorial would be much appreciated!", "assigned to add documentation for a how-to on this.", "@RohitDulam It's currently not possible to build Android TF on Windows directly using Bazel (this is a Bazel limitation). See https://github.com/tensorflow/tensorflow/issues/6385 for details and workarounds. The preferred method of using TF on Android is the [AAR distribution](https://github.com/tensorflow/tensorflow/issues/6385#issuecomment-299913647) which allows integration with a few lines of configuration. You can also manually download the prebuilt binaries as others suggest.\r\n\r\n@jubjamie Agreed that a more comprehensive official walkthrough would be useful, this is something we plan on adding.\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "We seem to have revamped the build to split it into multiple libraries. Please check with the newest version to see if the problem persists.", "Right -- I think everything current about building for Android is in https://www.tensorflow.org/mobile/ now.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing since it seems to be working now.", "Hi,\r\n I'm on windows. The android example runs pretty straight forward, however using this build the native libraries are pre-built and stripped of symbols and I can't use them to link other native C++ code that uses either the C++ or C TF APIs.\r\n Do you have any suggestions other than use a different operation system for the compilation?\r\n\r\nThanks.", "@barmazal you might try the Linux for Windows workaround outlined in #6385 to build the custom binaries you need", "drpngx i am building .so file using bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a and i faced this probelm form two weeks\r\nERROR: C:/users/sabir-pc/_bazel_sabir-pc/zp6kp3ov/external/com_google_absl/absl/strings/BUILD.bazel:32:1: C++ compilation of rule '@com_google_absl//absl/strings:strings' failed (Exit 1)\r\nIn file included from external/com_google_absl/absl/strings/str_cat.cc:15:\r\nIn file included from external/com_google_absl\\absl/strings/str_cat.h:62:\r\nIn file included from external/com_google_absl\\absl/strings/numbers.h:37:\r\nIn file included from external/com_google_absl\\absl/numeric/int128.h:29:\r\nIn file included from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include\\cmath:305:\r\nIn file included from external/androidndk/ndk/sources/android/support/include\\math.h:32:\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include\\math.h:1302:93: error: no member named 'log2f' in the global namespace\r\ninline _LIBCPP_INLINE_VISIBILITY float       log2(float __lcpp_x) _NOEXCEPT       {return ::log2f(__lcpp_x);}\r\n                                                                                          ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include\\math.h:1303:93: error: no member named 'log2l' in the global namespace\r\ninline _LIBCPP_INLINE_VISIBILITY long double log2(long double __lcpp_x) _NOEXCEPT {return ::log2l(__lcpp_x);}\r\n                                                                                          ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include\\math.h:1308:38: error: call to 'log2' is ambiguous\r\nlog2(_A1 __lcpp_x) _NOEXCEPT {return ::log2((double)__lcpp_x);}\r\n                                     ^~~~~~\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include\\math.h:1302:46: note: candidate function\r\ninline _LIBCPP_INLINE_VISIBILITY float       log2(float __lcpp_x) _NOEXCEPT       {return ::log2f(__lcpp_x);}\r\n                                             ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include\\math.h:1303:46: note: candidate function\r\ninline _LIBCPP_INLINE_VISIBILITY long double log2(long double __lcpp_x) _NOEXCEPT {return ::log2l(__lcpp_x);}\r\n                                             ^\r\n3 errors generated.\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 212.615s, Critical Path: 18.57s\r\nINFO: 213 processes: 213 local.\r\nFAILED: Build did NOT complete successfully\r\nplease help i am stuck in my FYP\r\n\r\n", "i am building libtensorflow_inference.so file using build but it will give me the error\r\ncompilation of rule '@com_google_absl//absl/strings:strings' failed (Exit 1)\r\nplease tensorflow member help me"]}, {"number": 11278, "title": "Examples in 'Getting started with tensor flow' use deprecated methods", "body": "https://www.tensorflow.org/get_started/get_started \r\nBasic Usage example output:\r\n\r\nWARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\pocherka\\AppData\\Local\\Temp\\tmpaq48b_3u\r\nWARNING:tensorflow:From C:\\Users\\pocherka\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nWARNING:tensorflow:From C:\\Users\\pocherka\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nWARNING:tensorflow:From C:\\Users\\pocherka\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\ntrain loss: {'global_step': 1000, 'loss': 2.3828899e-11}\r\neval loss: {'global_step': 1000, 'loss': 0.0025255322}sic usage\r\n\r\n", "comments": ["There is a PR against #11159 to fix that.", "Closing as a duplicate of #11159. Please reopen if that PR doesn't fix the issue."]}, {"number": 11277, "title": "Tensorflow Windows Bug with BeamSearchDecoder", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Pip Binary \r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 5.1\r\n- **GPU model and memory**: Nvidia Gtx 1080 (8 GB)\r\n\r\n### Describe the problem\r\n\r\nTensorflow on Windows gives an error (\"Multiple OpKernel registrations match NodeDef\") when I try to use BeamSearchDecoder in the seq2seq module. The same code works well on Ubuntu. Someone else on stackoverflow also had the [same issue](https://stackoverflow.com/questions/44535111/error-when-restoring-model-multiple-opkernel-registrations-match-nodedef). I have attached a snippet of code that generates the same error on my computer (original code is much bigger).\r\n\r\n\r\n### Source code to reproduce the problem\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nsess = tf.Session()\r\ncell1 = tf.contrib.rnn.BasicLSTMCell(90)\r\nword_embedding = tf.convert_to_tensor(np.eye(1,1),dtype=tf.float32)\r\ndecoder = tf.contrib.seq2seq.BeamSearchDecoder(\r\n                                                        cell=cell1,\r\n                                                        embedding=lambda inputs : tf.nn.embedding_lookup(word_embedding, inputs),\r\n                                                        start_tokens=[1],\r\n                                                        end_token=2,\r\n                                                        initial_state=cell1.zero_state(batch_size=3,dtype=tf.float32),\r\n                                                        beam_width=3\r\n)\r\noutputs_decoding, last_state_decoding, _ = tf.contrib.seq2seq.dynamic_decode(\r\n                        decoder, maximum_iterations=1)\r\nsess.run(outputs_decoding)\r\n```\r\n### Error Message\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1139, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1117, in _run_fn\r\n    self._extend_graph()\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1166, in _extend_graph\r\n    self._session, graph_def.SerializeToString(), status)\r\n  File \"C:\\Program Files\\Python35\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Multiple OpKernel registrations match NodeDef 'decoder/GatherTree = GatherTree[T=DT_INT32](decoder/TensorArrayStack_1/TensorArrayGatherV3, decoder/TensorArrayStack_2/TensorArrayGatherV3, decoder/while/Exit_11)': 'op: \"GatherTree\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }' and 'op: \"GatherTree\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }'\r\n         [[Node: decoder/GatherTree = GatherTree[T=DT_INT32](decoder/TensorArrayStack_1/TensorArrayGatherV3, decoder/TensorArrayStack_2/TensorArrayGatherV3, decoder/while/Exit_11)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"a.py\", line 16, in <module>\r\n    sess.run(outputs_decoding)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Multiple OpKernel registrations match NodeDef 'decoder/GatherTree = GatherTree[T=DT_INT32](decoder/TensorArrayStack_1/TensorArrayGatherV3, decoder/TensorArrayStack_2/TensorArrayGatherV3, decoder/while/Exit_11)': 'op: \"GatherTree\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }' and 'op: \"GatherTree\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }'\r\n         [[Node: decoder/GatherTree = GatherTree[T=DT_INT32](decoder/TensorArrayStack_1/TensorArrayGatherV3, decoder/TensorArrayStack_2/TensorArrayGatherV3, decoder/while/Exit_11)]]\r\n\r\nCaused by op 'decoder/GatherTree', defined at:\r\n  File \"a.py\", line 15, in <module>\r\n    decoder, maximum_iterations=1)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\decoder.py\", line 296, in dynamic_decode\r\n    final_outputs, final_state, final_sequence_lengths)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\beam_search_decoder.py\", line 280, in finalize\r\n    sequence_length=sequence_lengths)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\ops\\gen_beam_search_ops.py\", line 43, in gather_tree\r\n    sequence_length=sequence_length, name=name)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Multiple OpKernel registrations match NodeDef 'decoder/GatherTree = GatherTree[T=DT_INT32](decoder/TensorArrayStack_1/TensorArrayGatherV3, decoder/TensorArrayStack_2/TensorArrayGatherV3, decoder/while/Exit_11)': 'op: \"GatherTree\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }' and 'op: \"GatherTree\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }'\r\n         [[Node: decoder/GatherTree = GatherTree[T=DT_INT32](decoder/TensorArrayStack_1/TensorArrayGatherV3, decoder/TensorArrayStack_2/TensorArrayGatherV3, decoder/while/Exit_11)]]\r\n```", "comments": ["CC: @ebrevdo \r\n\r\nCould you check that the binary is [loaded](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_ops.py#L24) only once?", "Which line triggers the exception?", "The last line (sess.run).", "It could be a bug in the cmake files... Perhaps we're building the op\nkernel library twice. Derek may know more.\n\nOn Jul 4, 2017 3:04 PM, \"madhurcodes\" <notifications@github.com> wrote:\n\n> The last line (sess.run).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11277#issuecomment-312960587>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim01-72jyNvwVu4CjWXag5tF2pKijks5sKrblgaJpZM4ONpFT>\n> .\n>\n", "@madhurcodes could you check that you're not loading the library twice?\r\n\r\n/CC: @guschmue @mrry ", "No I'm doing nothing special. Just installed tensorflow-gpu from pip and executing that python file throws thr shown error.", "It is dynamically loaded as dll here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_python.cmake#L879\r\nbut also statically linked here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_core_kernels.cmake#L66\r\n\r\nThe fix is to add it to the exclusion list (like gru for example)\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_core_kernels.cmake#L135\r\n\r\nI can send a PR tomorrow (want to make sure the unit tests work for this since this might have never worked on windows)", "Thanks Guenther!  We do have unit tests for this op in tf.contrib on the\npython side... were they disabled in the windows build?\n\nOn Tue, Jul 4, 2017 at 8:08 PM, Guenther Schmuelling <\nnotifications@github.com> wrote:\n\n> It is dynamically loaded as dll here:\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/contrib/cmake/tf_python.cmake#L879\n> but also statically linked here:\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/contrib/cmake/tf_core_kernels.cmake#L66\n>\n> The fix is to add it to the exclusion list (like gru for example)\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/contrib/cmake/tf_core_kernels.cmake#L135\n>\n> I can send a PR tomorrow (want to make sure the unit tests work for this\n> since this might have never worked on windows)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11277#issuecomment-312985983>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwdpOWoGhhOJq8b21AYda2SbSoaPks5sKv4VgaJpZM4ONpFT>\n> .\n>\n", "Thanks @guschmue ! Good guess, @ebrevdo !", "Ok, fixed and waiting for the tests to pass. I have enabled all tests in contrib/seq2seq. \r\nIn cmake the unit tests under contrib are enabled selective - I can spend some time going over this to make sure we have the max enabled.", "Can this be closed now?", "Sure, though it would be nice if someone could point me to where the latest windows builds are stored. I checked the jenkins page but couldn't find out how to get the binaries it builds. (Don't want to install everything to build it myself).", "@martinwicke @mrry Any idea where windows builds are stored (see question above).", "See [here](http://ci.tensorflow.org/job/tensorflow-master-win-cmake-py/) under \"Last Successful Artifacts\".", "Thanks derek!", "@madhurcodes @mrry  I installed tf 1.2 from pip in windows and tried to use BeamSearchDecoder and getting the exact same error .  Isn't the pip installs the latest build for 1.2 version ? ", "1.3 is the latest version. 1.2 probably doesn't contain the fix. ", "I think this was the change:\r\nhttps://github.com/tensorflow/tensorflow/pull/11298/files\r\nand it is not in 1.2:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/cmake/tf_core_kernels.cmake#L113\r\n\r\n1.3 has it."]}, {"number": 11276, "title": "Fix memory leak when using `tf.layers`", "body": "Uses `weakref` so that PER_GRAPH_LAYER_NAME_UIDS doesn't prevent Graphs from being garbage collected.\r\n\r\nFixes #11273", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11275, "title": "tfcompile with tf.nn.dynamic_rnn crashes", "body": "Trying to build a C++ binary with tfcompile crashes with `INVALID ARGUMENTS: Mising Exit successor to rnn/while/Switch` if the graph contains `tf.nn.dynamic_rnn`, but it works with `tf.nn.static_rnn`. Why is this?", "comments": ["@tatatodd any idea?", "@carlthome What version of tensorflow are you using?  Since XLA and tfcompile support is currently experimental, I'll suggest you build from sources at HEAD.\r\n\r\nAlso, can you provide a minimal test case that demonstrates the problem?  Thanks!", "I'm having the same issue with tfcompile and tf.nn.dynamic_rnn.\r\n\r\nOS:\r\nUbuntu 16.04\r\nTF version:\r\n1.2.1 built from source without GPU support and no XLA JIT.\r\n\r\nTest case:\r\n```python\r\nxin = tf.placeholder(tf.float32, [1, 10, 10], name='input')\r\ncell = tf.contrib.rnn.LSTMCell(512)\r\nout, _ = tf.nn.dynamic_rnn(cell, xin, dtype=tf.float32)\r\nout = tf.identity(out, name='output')\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nsaver = tf.train.Saver()\r\nsaver.save(sess, 'outgraph')\r\n```\r\n\r\noutgraph.config.pbtxt:\r\n```\r\nfeed {\r\n  id {\r\n    node_name: \"input\"\r\n  }\r\n  shape {\r\n    dim { size: 1 }\r\n    dim { size: 10 }\r\n    dim { size: 10 }\r\n  }\r\n}\r\n\r\nfetch {\r\n  id {\r\n    node_name: \"output\"\r\n  }\r\n}\r\n```\r\n\r\nBUILD file:\r\n```python\r\nload(\"//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\n\r\ntf_library(\r\n  name = \"test_dyn_rnn\",\r\n  cpp_class = \"TestDynRNN\",\r\n  graph = \"outgraph.pb\",\r\n  config = \"outgraph.config.pbtxt\",\r\n  freeze_checkpoint = \"outgraph.ckpt\"\r\n)\r\n```\r\n\r\nCommand:\r\n`bazel build //tensorflow/compiler/aot/tests/test_dyn_rnn:test_dyn_rnn`\r\n\r\nThe build command results in `INVALID ARGUMENTS: Missing Exit successor to encoder/rnn/while/Switch`.\r\nSwapping to `tf.nn.static_rnn` works however.\r\n\r\n[Here's](https://gist.github.com/patrikohlsson/955a059d1f89c5339dd6d28d24c7f914) a complete example.", "I've been able to consistently reproduce the error `INVALID ARGUMENTS: Missing Exit successor to rnn/while/Switch` with master, thanks to @patrikohlsson's example. Perhaps @ebrevdo has some insight into what makes `tf.nn.dynamic_rnn` finicky with tfcompile?", "With the XLA JIT fix posted by kayzhu in #11122 I can now reproduce the `Missing Exit successor` with XLA JIT enabled as well.", "The `tf.nn.dynamic_rnn` library uses TensorFlow control-flow, which isn't well supported by XLA.  There's a little bit of special support in the JIT codepath, but the AOT (i.e. tfcompile) codepath doesn't support this at all right now.\r\n\r\nErrors about `Enter`, `Exit`, `Switch` and `Merge` nodes are all symptoms of of this problem.\r\n\r\nThere is no workaround at the moment.  There are some in-process discussions and prototyping about how to handle this.  Our exact approach hasn't been fleshed out yet.\r\n\r\n@hawkinsp may have some more details.", "Cool. It would be nice if XLA AOT supported loops eventually. Is there a list in the documentation about what ops are not well supported at the moment? Particularly, the `tf.nn` stuff I'd expect to work as it's a core package.\r\n\r\nFor my particular use case with a recurrent neural network (RNN), it's fine to unroll it manually before tfcompile and pad/truncate test data as needed, but I'd imagine there are applications where that fix would not be straightforward.", "@carlthome Yeah, one way or another we'll have support for loops in XLA AOT; we just need to work out the technical details.\r\n\r\nWe don't have documentation about what TensorFlow ops we support via XLA AOT or JIT at the moment, but the implementation of all of the TF->XLA translations is here:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/tf2xla/kernels\r\n\r\nAnd yes, thanks for mentioning unrolling.  Indeed for some models, a workaround is to statically unroll your loops before feeding into XLA.  An example for LSTMs is here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tests/lstm.py\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tests/lstm_test.py", "Keep in mind that tf.nn.dynamic_rnn relies on tf.while_loop; there are now\nsome XLA primitives you can use (as of the last couple of days) in github\nmaster; see, i.e.,\n\nhttps://github.com/tensorflow/tensorflow/commit/3e9cd2e13da566e279396d232004d3c4ffad336e\n\nhttps://github.com/tensorflow/tensorflow/commit/eb1fe50da445d3880b588215f6fadcc7f48dd3ff\n\nYou'll want to download a tensorflow nightly if you want to try it.\n\nKeep in mind this code will perform a *full unroll* internally on the XLA\nside, and nested while_loops (while_loop inside a dynamic_rnn) are not\nreally supported.\n\nYou want to be really careful with this new, relatively untested, xla\nbridge.  See the new xla rnn unit tests for examples.\n\nOn Thu, Jul 13, 2017 at 10:42 AM, Todd Wang <notifications@github.com>\nwrote:\n\n> @carlthome <https://github.com/carlthome> Yeah, one way or another we'll\n> have support for loops in XLA AOT; we just need to work out the technical\n> details.\n>\n> We don't have documentation about what TensorFlow ops we support via XLA\n> AOT or JIT at the moment, but the implementation of all of the TF->XLA\n> translations is here:\n> https://github.com/tensorflow/tensorflow/tree/master/\n> tensorflow/compiler/tf2xla/kernels\n>\n> And yes, thanks for mentioning unrolling. Indeed for some models, a\n> workaround is to statically unroll your loops before feeding into XLA. An\n> example for LSTMs is here:\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/compiler/tests/lstm.py\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/compiler/tests/lstm_test.py\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11275#issuecomment-315150279>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxN9jaKDnZKGMyRBiV6cEDQCLllHks5sNlcDgaJpZM4ONgEv>\n> .\n>\n", "@tatatodd I'm assigning to you to take out of triage queue. Feel free to re-assign ", "ETA on tf.nn.dynamic_rnn support in XLA AOT?", "@carlthome For inference (not training), it might actually work right now. No-one has been working directly on AOT support for tf.nn.dynamic_rnn(), but as it happens I think all the pieces are there. Give it a go and let me know what happens!", "Don't we need the new max iterations attribute on while_loop?\n\nOn Sep 19, 2017 8:56 AM, \"Peter Hawkins\" <notifications@github.com> wrote:\n\n> @carlthome <https://github.com/carlthome> For inference (not training),\n> it might actually work right now. No-one has been working directly on AOT\n> support for tf.nn.dynamic_rnn(), but as it happens I think all the pieces\n> are there. Give it a go and let me know what happens!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11275#issuecomment-330585427>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimx_mR7NDx2bST4JCr89lQLeGXpkQks5sj-QVgaJpZM4ONgEv>\n> .\n>\n", "@ebrevdo I suspect not, actually. We need an iteration bound only for training (for Stacks as it happens, not for TensorArrays.)", "Well, new error at least so that's good:\r\n\r\n`INVALID ARGUMENTS: Input 0 of node encoder/rnn/while/add/y was passed int32 from encoder/rnn/while/Switch:1 incompatible with expected INVALID.`", "@carlthome Yes, we're making progress :-) Would you be able to give me a small self-contained reproduction to look at? Thanks!", "Yeah! :man_dancing: \r\n\r\nAnyway from latest master with all defaults (e.g.\r\n`git clone git@github.com:tensorflow/tensorflow.git && cd tensorflow && PYTHON_BIN_PATH=$(which python) USE_DEFAULT_PYTHON_LIB_PATH=1 CC_OPT_FLAGS='-march=native' TF_ENABLE_XLA=0 TF_NEED_MPI=0 TF_NEED_JEMALLOC=1 TF_NEED_GCP=0 TF_NEED_HDFS=0 TF_NEED_VERBS=0 TF_NEED_OPENCL=0 TF_NEED_CUDA=0 TF_NEED_GDR=0 ./configure`)\r\n\r\nAnd this is basically just @patrikohlsson's example:\r\n## create_graph.py:\r\n```py\r\nimport tensorflow as tf\r\n\r\nx = tf.placeholder(tf.float32, [1, 10, 10], name='input')\r\nc = tf.nn.rnn_cell.LSTMCell(32)\r\ny, _ = tf.nn.dynamic_rnn(c, x, dtype=tf.float32)\r\ny = tf.identity(y, name='output')\r\n\r\nwith tf.Session() as session:\r\n  session.run(tf.global_variables_initializer())\r\n  saver = tf.train.Saver(write_version=tf.train.SaverDef.V1)\r\n  saver.save(session, 'graph.ckpt')\r\n  with open('graph.pb', 'wb') as f:\r\n    f.write(session.graph.as_graph_def().SerializeToString())\r\n```\r\n## graph.config.pbtxt:\r\n```\r\nfeed {\r\n  id { node_name: \"input\" }\r\n  shape {\r\n    dim { size: 1 }\r\n    dim { size: 10 }\r\n    dim { size: 10 }\r\n  }\r\n}\r\nfetch {\r\n  id { node_name: \"output\" }\r\n}\r\n```\r\n## BUILD:\r\n```\r\nload(\"//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\n\r\ntf_library(\r\n  name = \"dynamic_rnn_graph\",\r\n  cpp_class = \"Graph\",\r\n  graph = \"graph.pb\",\r\n  config = \"graph.config.pbtxt\",\r\n  freeze_checkpoint = \"graph.ckpt\"\r\n)\r\n```\r\n\r\nAssuming TensorFlow is installed, run `create_graph.py` and put its output files into the repo root together with `BUILD` and `graph.config.pbtxt`, and then run `bazel build :dynamic_rnn_graph`.", "Looking at the codebase, r1.4 includes this https://github.com/tensorflow/tensorflow/commit/0e71ecaf9512cd8a69af01ac85e5e1632171c651 which is mostly what we already had as a workaround on https://github.com/mozilla/tensorflow/ repo.", "I tried again just now with master and still failing with `INVALID ARGUMENTS: Input 0 of node encoder/rnn/while/add/y was passed int32 from encoder/rnn/while/Switch:1 incompatible with expected INVALID.`", "Ping @ebrevdo, could we expect `tf.nn.dynamic_rnn` to work with XLA eventually?", "Yes; though I don't promise it'll happen by end of December.\n\nOn Tue, Dec 19, 2017 at 11:04 AM, Carl Thom\u00e9 <notifications@github.com>\nwrote:\n\n> Ping @ebrevdo <https://github.com/ebrevdo>, could we expect\n> tf.nn.dynamic_rnn to work with XLA eventually?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11275#issuecomment-352855390>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimy7sJCmmS4s50fmMM6RRPxqOIwR_ks5tCAjMgaJpZM4ONgEv>\n> .\n>\n", "@ebrevdo do you expect to see any speed gains?", "As a follow-up question, is it within XLA's goals to work with seq2seq models at some point? Will it be possible to use XLA on graphs that have variable length inputs and outputs? Right now, it seems that `tfcompile` does not like fetches with dimension `-1`.", "It is a goal; but because XLA will JIT a graph and perform memory layout\n*before* any execution, you will have to provide a \"maximum max_time\" for\nthe memory layout (even if the loop will not calculate up to this maximum\nsize).  When working with dynamic_rnn, for example, this will mean you will\nhave to pad your batched sequence inputs to some specific max_time size(s),\nlike 10, 50, 100, ...; to ensure XLA doesn't try to JIT compile for every\npossible max(sequence_length) across your minibatches.  The tensorflow NMT\ntutorial shows how to bucket batches by sequence lengths, but does not\ncurrently pad the input max_times to the bucket boundaries.\n\n\nOn Wed, Dec 20, 2017 at 8:49 AM, leod <notifications@github.com> wrote:\n\n> As a follow-up question, is it within XLA's goals to work with seq2seq\n> models at some point? Will it be possible to use XLA on graphs that have\n> variable length inputs and outputs? Right now, it seems that tfcompile\n> does not like fetches with dimension -1.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11275#issuecomment-353117651>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim2kBtt6Pd9UOPkYyfUgpZtuBgwDeks5tCTqkgaJpZM4ONgEv>\n> .\n>\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@ebrevdo What happens for a `tf.nn.bidirectional_dynamic_rnn` with no `sequence_length` argument? I am currently removing it because `tfcompile` chokes on `Assert`. This used to work quite well, but since r1.4, compiling with the recurrent layers, even very small ones, fails in a way that looks like infite loop consuming as much as CPU and memory as possible. Last time I tried master, it was exposing the same behavior.\r\n\r\nMy guess is that this is because of the lack of `sequence_length`, and thus it goes weird.\r\n\r\nHowever, keeping the `sequence_length` fails this way:\r\n```\r\n2018-01-05 18:29:44.901953: F external/org_tensorflow/tensorflow/compiler/aot/tfcompile_main.cc:140] Non-OK-status: status status: Not found: Op type not registered 'Assert' in binary running on hostname. Make sure the Op and Kernel are registered in the binary running in this process.\r\n```", "+todd; who I think can say more about tfcompile's interactions with\ntf.Assert.\n\nOn Fri, Jan 5, 2018 at 9:31 AM, lissyx <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> What happens for a\n> tf.nn.bidirectional_dynamic_rnn with no sequence_length argument? I am\n> currently removing it because tfcompile chokes on Assert. This used to\n> work quite well, but since r1.4, compiling with the recurrent layers, even\n> very small ones, fails in a way that looks like infite loop consuming as\n> much as CPU and memory as possible. Last time I tried master, it was\n> exposing the same behavior.\n>\n> My guess is that this is because of the lack of sequence_length, and thus\n> it goes weird.\n>\n> However, keeping the sequence_length fails this way:\n>\n> 2018-01-05 18:29:44.901953: F external/org_tensorflow/tensorflow/compiler/aot/tfcompile_main.cc:140] Non-OK-status: status status: Not found: Op type not registered 'Assert' in binary running on hostname. Make sure the Op and Kernel are registered in the binary running in this process.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11275#issuecomment-355614857>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim94v9vVSrpLY4dtR0F2-sx8nFxbXks5tHlxxgaJpZM4ONgEv>\n> .\n>\n", "Last I heard, @prb12 was working on adding a no-op tf2xla kernel for tf.Assert, he would know more.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ebrevdo So, it looks like with current master, it's not an issue anymore ? At least I can again use `tfcompile`", "Yes; we've fixed this up recently.  I'll close this bug so if you run into\nany issues please open a new one.\n\nOn Mon, Feb 12, 2018 at 3:58 AM, lissyx <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> So, it looks like with current\n> master, it's not an issue anymore ? At least I can again use tfcompile\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11275#issuecomment-364902429>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim2W-19LdOYnoaUi9hC3FuEXqH6OZks5tUCd7gaJpZM4ONgEv>\n> .\n>\n", "Nice! Looking forward to testing this again. \ud83c\udf89 ", "@ebrevdo Is it supposed to be fixed on r1.5 as well ?", "I think not, only nightlies and the upcoming 1.6\n\nOn Tue, Feb 13, 2018, 1:51 AM lissyx <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Is it supposed to be fixed on r1.5\n> as well ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11275#issuecomment-365210803>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimz5KMABVuKMiYsvPXCSa4zAgM8efks5tUVstgaJpZM4ONgEv>\n> .\n>\n"]}, {"number": 11274, "title": "R1.2", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@ShirazMalkani Thanks for the PR. We will merge 1.2.1 back to master when ready. \r\n\r\ncc @av8ramit ", "This has already been merged back into master:\r\nhttps://github.com/tensorflow/tensorflow/commit/e9f357b16b04ff17599b89a90498d64bc10c3c91"]}, {"number": 11273, "title": "Memory leak when using `tf.layers`", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.5\r\n- **Exact command to reproduce**:\r\n\r\n``` python\r\nimport os\r\n\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport psutil\r\n\r\n\r\ndef memory():\r\n    pid = os.getpid()\r\n    py = psutil.Process(pid)\r\n    memory_use = py.memory_info()[0] / 2. ** 30\r\n    return memory_use\r\n\r\n\r\nmemory_usage = []\r\nfor i in range(1000):\r\n    memory_usage.append(memory())\r\n    print(\"iter\", i, memory_usage[-1])\r\n\r\n    with tf.Graph().as_default():\r\n        x = tf.constant(np.ones((100, 1000), dtype=np.float32))\r\n\r\n        # memory leak\r\n        x = tf.layers.dense(x, units=1000)\r\n\r\n        # no memory leak\r\n        # with tf.variable_scope(\"layer\", reuse=False):\r\n        #     x = tf.matmul(x, tf.get_variable(\r\n        #         \"w\", shape=(1000, 1000), dtype=tf.float32,\r\n        #         initializer=tf.ones_initializer()))\r\n\r\nplt.figure()\r\nplt.plot(memory_usage)\r\nplt.xlabel(\"iterations\")\r\nplt.ylabel(\"memory usage\")\r\nplt.show()\r\n```\r\n\r\n### Describe the problem\r\nThere is some kind of memory leak when repeatedly building graphs containing `tf.layers` elements.  The example above shows the memory usage comparing what I think should be roughly equivalent implementations, one using `tf.layers.dense` and the other using manually created kernels/matmul ops.  When using `tf.layers.dense` the memory usage continually increases, whereas the manual approach shows memory being periodically cleaned up by garbage collection.  So my guess would be that there is some internal reference to the `tf.layers` elements that is preventing them from being garbage collected.\r\n\r\nnot using `tf.layers.dense`:\r\n![non_layer](https://user-images.githubusercontent.com/1952220/27835565-78f5628c-60a9-11e7-9134-971b5caba784.png)\r\n\r\n\r\nusing `tf.layers.dense`:\r\n![with_layer](https://user-images.githubusercontent.com/1952220/27835536-570a2bda-60a9-11e7-8936-1631f67be3b0.png)\r\n\r\n\r\n\r\n", "comments": ["I believe this is the cause, as it keeps a global mapping to all the Graphs that are created (preventing them from being garbage collected)\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/base.py#L697"]}, {"number": 11272, "title": "error in tensorflow/tensorflow/examples/tutorials/word2vec/word2vec_basic.py", "body": "### System information\r\n== cat /etc/issue ===============================================\r\nLinux ai 2.6.32-642.11.1.el6.x86_64 #1 SMP Fri Nov 18 19:25:05 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\nLSB_VERSION=base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.9.3\r\nCopyright \u00a9 2015 Free Software Foundation, Inc.\r\n\r\n== uname -a =====================================================\r\nLinux ai 2.6.32-642.11.1.el6.x86_64 #1 SMP Fri Nov 18 19:25:05 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.0)\r\nprotobuf (3.3.0)\r\ntensorflow (1.1.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.1\r\ntf.GIT_VERSION = v1.2.0-5-g435cdfc\r\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/lib64:/usr/local/lib:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./newfile.sh: line 85: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n### Describe the problem\r\nI change the data set and encounter error which is \"TypeError: sequence index must be integer, not 'slice'\" in line 117. \r\n\r\n### Source code / logs\r\nThe code what i have changed is as below\r\n![image](https://user-images.githubusercontent.com/524093/27833171-297c04ee-6104-11e7-88c6-90e963fc7f9c.png)\r\n\r\n### What I Do\r\nChange buffer[:] = data[:span] To buffer.extend(data[:span]) in line 117 and work well. Please check the code.  \r\nThank you for your attention.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11271, "title": "tensorflow.python.framework.errors.NotFoundError: No algorithm worked!", "body": "I install the TensorFlow 0.10.0 on Ubuntu 14.0.4 with cuda 7.5 and cudnn 5.1.The GPU is GTX980 \r\nwhen I run a softmax example on minst dataset the TensorFlow can work but when i build CNN with TensorFlow it will cause error.\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\r\nExtracting MNIST_data/train-images-idx3-ubyte.gz\r\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\r\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\nStart to train the convolutional neural network......\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \r\nname: GeForce GTX 980\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2155\r\npciBusID 0000:02:00.0\r\nTotal memory: 3.94GiB\r\nFree memory: 3.82GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x27e67f0\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \r\nname: GeForce GTX 980\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2155\r\npciBusID 0000:03:00.0\r\nTotal memory: 3.94GiB\r\nFree memory: 3.87GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:02:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 980, pci bus id: 0000:03:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1115] failed to get elapsed time between events: CUDA_ERROR_NOT_READY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1115] failed to get elapsed time between events: CUDA_ERROR_NOT_READY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1115] failed to get elapsed time between events: CUDA_ERROR_NOT_READY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1115] failed to get elapsed time between events: CUDA_ERROR_NOT_READY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1115] failed to get elapsed time between events: CUDA_ERROR_NOT_READY\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 69, in <module>\r\n    test_cnn()  \r\n  File \"test.py\", line 59, in test_cnn\r\n    sess.run(fetches=Step_train, feed_dict={x:batch[0], y:batch[1], dropout_prob:0.5})  \r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors.NotFoundError: No algorithm worked!\r\n\t [[Node: Conv2D_1 = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](MaxPool, weight_1/read)]]\r\nCaused by op u'Conv2D_1', defined at:\r\n  File \"test.py\", line 69, in <module>\r\n    test_cnn()\r\n  File \"test.py\", line 35, in test_cnn\r\n    conv2_out = tf.nn.relu(conv_2d(pool1_out, w_conv2)+b_conv2)\r\n  File \"test.py\", line 12, in conv_2d\r\n    return tf.nn.conv2d(input=x, filter=w, strides=[1, 1, 1, 1], padding=\"SAME\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 394, in conv2d\r\n    data_format=data_format, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nE tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:643] Deallocating stream with pending work\r\n^CWrite failed: Broken pipe\r\n```\r\nif you can help me solve the problem or give some advice, it will be great", "comments": ["@mjanusz any idea? It looks like we're trying to get timings before some streaming operation is done?", "@zheng-xq any idea in this one?", "Do you have some idea to solve this problem.Thank you! @drpngx @andydavis1 ", "Hi @1234clam . Just to make sure, is this the example you are using? https://www.tensorflow.org/get_started/mnist/beginners Thanks!", "Yes I have run this example and  run the minst_mlp.py in keras example,it can work every well but when i use the cnn it  caused such a mistake. Do you have some advice to solve this problem? The result minst_softmax.py example just like this:\r\n```\r\n$ python test.py\r\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\n2017-07-06 11:45:29.078223: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-06 11:45:29.078267: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-06 11:45:29.078277: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-06 11:45:29.078286: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-06 11:45:29.078294: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-06 11:45:29.499407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: GeForce GTX 980\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2155\r\npciBusID 0000:02:00.0\r\nTotal memory: 3.94GiB\r\nFree memory: 3.78GiB\r\n2017-07-06 11:45:29.710753: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x35d7a50 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2017-07-06 11:45:29.711664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 1 with properties: \r\nname: GeForce GTX 980\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2155\r\npciBusID 0000:03:00.0\r\nTotal memory: 3.94GiB\r\nFree memory: 3.87GiB\r\n2017-07-06 11:45:29.712385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 1 \r\n2017-07-06 11:45:29.712412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y Y \r\n2017-07-06 11:45:29.712421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y Y \r\n2017-07-06 11:45:29.712439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:02:00.0)\r\n2017-07-06 11:45:29.712453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 980, pci bus id: 0000:03:00.0)\r\n0.9165\r\n```\r\n@yzhwang ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@1234clam Have you tried with TensorFlow 1.4? Does the problem still exist with the latest version?\r\nNote that the warning means that your thread for running TensorFlow might already have another running CUDA program before you launch TensorFlow, according to this warning:\r\n2017-07-06 11:45:29.710753: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x35d7a50 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\nOne way to make sure nothing is using your GPU is to run `nvidia-smi` and kill any process showed up that are using your GPU before you run any TensorFlow program.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Close due to inactivity. Feel free to reopen if the issue still exists."]}]