[{"number": 9488, "title": "return type of `tf.reduce_any`", "body": "I find that if the input tensor has type `bool`, the output of `tf.reduce_any` has type 'uint8'. My question is, why not `bool`? In the [document](https://www.tensorflow.org/api_docs/python/tf/reduce_any), it says it is equivalent to `np.any`, but the latter return `bool`.\r\n\r\nSame applied to  `tf.reduce_all`.", "comments": ["Well, the examples provided suggest, that the output type should be `bool`.\r\nCould you please tell, how you tested for type that said was `uint8`?", "I find that it has type `bool`.  Can you check again?", "@Androbin @girving \r\nMy bad. I actually use it through a wrapper in `keras`, and I thought it simply call `reduce_any` but it turns out that it changes the type of the result.\r\n\r\nThanks for replying.", "Cc @fchollet: this does seem a bit surprising.", "@girving It is resolved in `keras master`. I was using `keras-1.2`, a version before `2.0`. ", "For a while Keras did not have a bool type and used int instead, for compatibility with Theano. Now Theano has a bool type so we do too."]}, {"number": 9487, "title": "terminate called after throwing an instance of 'std::bad_alloc', not out of memory", "body": "Hi, I'm using Keras with tensorflow back-end to train a LSTM network, I was doing a grid search over the learning_rate and dropout factor with the fixed batch size of 64, It ran perfectly but in the middle of it was interrupted by signal 6: SIGABRT with the following error: \r\n\r\n    terminate called after throwing an instance of 'std::bad_alloc'  what():  std::bad_alloc\r\n\r\nIt should not be a memory allocation problem because it was running earlier for batch size of 64 which is not too much in my case\r\n\r\nyou can find my system information(tf_env.txt) from the following link:  \r\nhttps://www.dropbox.com/s/wcv8y88fh659zck/tf_env.txt?dl=0\r\n\r\n\r\n\r\n\r\n", "comments": ["Your memory usage may be on the edge. That is exactly the error given when running out of memory. Try running it again and monitor using ps or top the memory usage.", "I tried my training with batch-size of 128 (instead of 64) but it ran without any error, I'm wondering how it could be out of memory error while it can be run on a bigger batch size", "Because it could be memory fragmentation causing your memory usage to go up over run. Why don't you rerun your problem and graph the memory usage and see if that is the case?", "thanks for the reply, here is my memory usage (free memory available in time) \r\n\r\n![image](https://cloud.githubusercontent.com/assets/7490180/25541440/ab076890-2c1c-11e7-932d-bbe868453f35.png)\r\n\r\nI noticed those plateaus are when it reaches the last epoch number in the current training and moves to another training with different sets of parameters ( as i said earlier I'm doing a parameter search ), I'm wondering why it cannot free the memory after it reaches the end of the training and then moves to the next training (with different parameter sets). ", "Are you clearing your tensorflow graph between these trainings?  e.g.\r\n`tf.reset_default_graph()`", "I used `clear_session()` in keras which is supposed to do the same thing. it fails to release the RAM that it was using. ", "@fchollet, do you have any ideas?", "`clear_session()` should definitely work here.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@sedghi is this still a problem for you?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@tatatodd I was able to resolve it by \r\n````\r\nfrom keras import backend as K\r\nK.clear_session()\r\n````", "@sedghi did you call the K.clear_session() at the end of training or at the start of it? ", "@ambodi at the end.\r\nI was doing 5-fold cross-validation", "Hi I'm using pi camera interfaced in raspberry pi3 b+ model. \r\n\r\nKindly guide to resolve the error\r\n\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\n", "I got same issue as @KoushikRaghav . I have a Tensorflow 2.0 keras model that use tf.cast, tf.exp and a division by 2. I noticed these operations idependently fail to run the TfLite file using TFLiteInterpreter.\r\n@aselle  Do you have some idea about why these operations are failing in tf-nightly-gpu-2.0-preview?", "I'm getting this error (which I assume is from libtensorflow, the C++ binary) when using Tensorflow.js in a Node.js environment:\r\n\r\n```\r\n2020-06-16 15:46:14.057597: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 8998027264 exceeds 10% of system memory.\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\nAborted (core dumped)\r\n```\r\n\r\nI can confirm that it is absolutely _not_ using anywhere close to 10% of my system memory. In `htop` it caps out at well under 1GiB - I have 24GiB to give it, but it keeps crashing as above."]}, {"number": 9486, "title": "maybe a big bug of save/restore on python3", "body": "tensorflow version:\r\nv1.1.0-rc0-61-g1ec6ed5 1.1.0\r\n\r\npython version:\r\n3.6.1\r\n\r\nNo GPU.\r\n\r\nI use \r\n`saver.save(sess, './model/')`\r\nand\r\n`saver.restore(sess, './model/')`\r\nbut \r\n`saver.restore` doesn't work.The predict result changes at every time.And at Python2.7 the predict result stay the same.\r\nAt first,I think it was the problem of windows.http://stackoverflow.com/questions/43630048/tensorflow-save-restore-a-model-in-windows", "comments": ["This is not enough information to debug the problem.  Please wait for answers on that informative stack overflow question."]}, {"number": 9485, "title": "BernoulliWithSigmoidProbs is obsolete", "body": "I think `tf.contrib.distributions.BernoulliWithSigmoidProbs` can be removed, because `Bernoulli` itself has a `logits` parameter that does the exact same thing, afaik. If you agree, I can make a pull-request if necessary.", "comments": ["@ebrevdo, could you comment on this.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Yes; this looks safe to delete!", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Added PR #16846 for it."]}, {"number": 9484, "title": "ImportError (in import tensorflw-gpu as tf) : Tensorflow windows10, CUDA8.0/cuDNN5.1 and python 3.5.2", "body": "Hi, i am facing an issue if importError on a setup !\r\n1. Windows10, Microsoft visual studio (2017)\r\n2. CUDA 8.0/cuDNN5.1\r\n3. Python 3.5.2\r\n4. PATH for CUDA, cuDNN5.1 Libraries/bin/include set properly !!\r\n5. Setup is created with Docker & tensorflow-gpu environment\r\n\r\nWhile i am trying to verify my installation with \"import tensorflow-gpu as tf\", encountering with  ImportError: python library specific errors. pl do suggest some remedies !!\r\n\r\nHere is complete log of error:  \r\n$ python\r\nPython 3.5.2 |Continuum Analytics, Inc.| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow-gpu as tf\r\n  File \"<stdin>\", line 1\r\n    import tensorflow-gpu as tf\r\n                     ^\r\nSyntaxError: invalid syntax\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Rajeev\\Anaconda3\\envs\\tensorflow-gpu\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@mrry Can you take a look?  Not sure if this is enough information.", "Change you CUDA path C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin\\cudnn64_6.dll filename to cudnn64_5.dll", "@gwind55 OMG, it worked!\r\nI've been trying to fix this issue for the past week and that was it!\r\n\r\nNote: my setup is win10 using vanilla python (no anaconda) and tensorflow 1.1 (no RC).\r\n", "@girving this seems like a bug since that's what the cudnn installer sets up. Do you have a plan to fix it internally?", "@mrry I don't understand this well enough to know if we can fix it on our end.  Thoughts?", "@girving not much. Perhaps @gwind55 can help us understand how he manage to figure this out.\r\nEdit: it seems to be related to  #5968.", "I'm pretty sure that the build hard-codes `64_5` as the CuDNN version here (although I can't see how we use it, because AFAICT the `stream_executor` doesn't use `DSOLoader` anymore):\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/f44d8632a1f0c31ad2061ae9c25ec21e70112179/tensorflow/contrib/cmake/CMakeLists.txt#L210\r\n\r\nAlso, the release builds link against `${CUDNN_HOME}/lib/x64/cudnn.lib` on our build cluster (which I believe is an import library for a corresponding CuDNN DLL; I don't think the build machines have CuDNN 6):\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/f44d8632a1f0c31ad2061ae9c25ec21e70112179/tensorflow/contrib/cmake/CMakeLists.txt#L202\r\n\r\nPerhaps one of these implies a hard dependency on a file called `cudnn64_5.dll`? (@guschmue, does this sound familiar?) Is there any way to make a binary that supports both versions, or is it enough to focus on the latest one only and force everyone to upgrade?", "Yeah, I know about this one. On windows there is no symbolic link like:\r\n/usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\n/usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\r\nso you need to know the full dll name.\r\nIn the master we link against cudnn.lib and cudnn.lib will know the correct dll name but might have the same problem that if you linked with the cudnn.lib for 5 it wants to have the 5 dll and 5 might not work for you. \r\nIn 1.1 I think we are still using dynamic load.\r\nI could take a look and see if there is a clean way out of it.\r\n", "@mrry thanks. Earlier CuDNN 5.1 files were extracted (include, bin, include) in CUDA desinated path. However, somehow they were not placed properly in designate paths. \r\n\r\nPost this discussion thread i recheck lib/files (thoroughly), found they were missing. Now extracted at different location and copied in designated CUDA paths. Problem is resolved. \r\n\r\nThanks all friends for extending their support !!\r\n\r\nRegards, Rajeev Sharma  ", "@guschmue @mrry @nova77 @gwind55 @girving \r\n\r\nHere is final logs (environment - Docker, GPU environment Geforce 1060, and tensorflow with CUDA/cuDNN):\r\n\r\n$ python\r\nPython 3.5.2 |Continuum Analytics, Inc.| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n2017-04-28 09:40:40.028503: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-28 09:40:40.028651: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-28 09:40:40.028777: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-28 09:40:40.028874: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-28 09:40:40.028989: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-28 09:40:40.029113: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-28 09:40:40.029239: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-28 09:40:40.029397: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-28 09:40:41.239629: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:887] Found device 0 with properties:\r\nname: GeForce GTX 1060\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705\r\npciBusID 0000:01:00.0\r\nTotal memory: 6.00GiB\r\nFree memory: 4.99GiB\r\n2017-04-28 09:40:41.239828: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:908] DMA: 0\r\n2017-04-28 09:40:41.241327: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:918] 0:   Y\r\n2017-04-28 09:40:41.242344: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0\r\n2017-04-28 09:40:41.468151: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\direct_session.cc:257] Device mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0\r\n\r\n>>>\r\n\r\n", "@guschmue @mrry @nova77 @gwind55 @girving - Friends, i am willing to explore a few image processing code example (C or python) for Self-Driving Vehicle traffic environment (Lights, objects, potholes, Lane marking or traffic contexts etc....for one of my project). \r\n\r\nCan anyone help me by suggesting sampled repositories to try out before go ahead with my own example/coding?\r\n\r\nThanks  ", "For the missing cudnn?.dll issue in general (maybe we should open a new issue for this) ...\r\nThe important thing first:\r\nIf people run into this missing \u2018cudnn64_5.dll\u2019 if they installed cudnn6, it is better to ask them to download cudnn5.1 instead of \u2018mv cudnn64_6.dll  cudnn64_5.dll\u2019 until we have builds with cudnn6 because there might be some compatibility issues.\r\n\r\nSome findings:\r\nAfter building with cudnn6 (master) bunch of unit tests are failing. As far I can tell cudnn related. I think \r\nthis might be also the case on linux since there is no windows specific code here.\r\nNeed to take a closer look.\r\n tensorflow/python/kernel_tests/atrous_conv2d_test.py\r\n tensorflow/python/kernel_tests/atrous_convolution_test.py\r\n tensorflow/python/kernel_tests/conv2d_transpose_test.py\r\n tensorflow/python/kernel_tests/conv_ops_3d_test.py\r\n tensorflow/python/kernel_tests/conv_ops_test.py\r\n tensorflow/python/kernel_tests/depthwise_conv_op_test.py\r\n tensorflow/python/kernel_tests/lrn_op_test.py\r\n tensorflow/python/kernel_tests/pooling_ops_3d_test.py\r\n tensorflow/python/kernel_tests/pooling_ops_test.py\r\n tensorflow/python/kernel_tests/pool_test.py\r\n\r\nLoading cudnn?.dll on master\r\nSince the master  links now against cudnn.lib, cudnn.lib has the cudnn?.dll burned in that goes with the lib.\r\nThe effect of this is that if you had tensorflow-gpu installed and in the future update to a version that is\r\nbuild with cudnn6 you get this error message\r\n  ImportError: No module named '_pywrap_tensorflow_internal' \r\n  Failed to load the native TensorFlow runtime.\r\nwhich will not guide users to install cudnn6 on their system. We need to add figure some way to get a proper error  message to the user. This is maybe part of https://github.com/tensorflow/tensorflow/issues/9170\r\n\r\nLoading cudnn?.dll on 1.1\r\nSince 1.1 still uses the dynamic load of the cudnn?.dll and the name of the dll to load comes from CMakefiles.txt. If we want switch to cudnn6 we could either update the dll in CMakefile.txt or we could build the dll name on the fly by using the CUDNN_MAJOR from cudnn.h.\r\nLater might be better but since the master is no longer using the dynamic load might not be worth the change.\r\n\r\nIn summary I think we need to:\r\n- Fix the unit tests for cudnn6\r\n- do something for https://github.com/tensorflow/tensorflow/issues/9170 to tell users why what the real \r\n  error was when  if we fail to load tensorflow because the wrong cudnn version was installed.\r\n- If we want to switch 1.1 to cudnn6 a make a change from in CMakefile.txt to reflect \r\n  the new cudnn?.dll name\r\n", "Thanks @gwind55 It took me the whole day searching for solution", "Many thanks! @gwind55 I had the same problem, renamed the file and it worked perfectly!\r\nI spent an insane amount of hours looking for a solution. I'm glad it finally worked out.", "@gwind55 this solution works, thx", "I had the same problem, and renaming the file works for me too.  Thanks @gwind55 ", "I have cudnn 5.1 and cuda-8.0. cudnn is copied in cuda's directory. I've also appended it's path to the system PATHs as:\r\n\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin\r\n\r\nAlso msvcp140.dll is included both in systemWow64 and system32 and I use Python 3.5.2.\r\n\r\nWhat is wrong? Have you faced the same problem?", "My programs are working fine now. Only one change I am taking care of\nadditionally is - (a) low learning rate\n\nOn Mon, May 29, 2017 at 10:46 AM, Abolfazl Meyarian <\nnotifications@github.com> wrote:\n\n> I have cudnn 5.1 and cuda-8.0. cudnn is copied in cuda's directory. I've\n> also appended it's path to the system PATHs as:\n>\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin\n>\n> Also msvcp140.dll is included both in systemWow64 and system32 and I use\n> Python 3.5.2.\n>\n> What is wrong? Have you faced the same problem?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9484#issuecomment-304574105>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AOK79WsXgcVHPCWfMXyUbdePlThtk7AEks5r-lSsgaJpZM4NKELT>\n> .\n>\n\n\n\n-- \nRegards\nRajeev\n", "@RajeevSharma2015 \r\n???", "For users who find this issue when searching for the error message, I've created a script for diagnosing common TensorFlow on Windows installation issues. Please download the script from https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c and run `python tensorflow_self_check.py`. It will print suggestions for how to fix your installation. Let me know if you are still facing problems after running the script.", "@mrry  I find the problem is on tensorflow-gpu 1.1.0  .Today, I use your script to check my DLL. It still no problem but can't import ,and I have NVCUDA.dll (I use python35,tensorflow 1.1.0,install succeed).  Then, I download and install tensorflow 1.0.1,and I can import tensorflow-gpu(1.0.1) successfully.\r\n\r\nNow, I just want to konw why the version 1.1.0 have problem\uff0cand how to solve it.\r\n\r\n\r\n\r\nHere is the message about the problem.(tensorflow 1.1.0)\r\n```\r\nPython 3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 35, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 30, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"E:\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"E:\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 35, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 30, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"E:\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"E:\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n```", "@gwind55 I had the same problem, renamed the file and it worked perfectly!\r\nI spent so many hours looking for a solution. I'm glad I finally found one. I'm not a Python developer but can't the logs be improved so that you see which DLL failed to load? ", "sorry I have to correct. I had the wrong CUDnn version installed (6.0), when installing v 5.1 no renaming was necessary. ", "`>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/home/tensor/anaconda3/envs/tgpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/tensor/anaconda3/envs/tgpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/tensor/anaconda3/envs/tgpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/tensor/anaconda3/envs/tgpu/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/tensor/anaconda3/envs/tgpu/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libnvidia-fatbinaryloader.so.384.69: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/tensor/anaconda3/envs/tgpu/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/tensor/anaconda3/envs/tgpu/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/tensor/anaconda3/envs/tgpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/tensor/anaconda3/envs/tgpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/tensor/anaconda3/envs/tgpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/tensor/anaconda3/envs/tgpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/tensor/anaconda3/envs/tgpu/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/tensor/anaconda3/envs/tgpu/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libnvidia-fatbinaryloader.so.384.69: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> exit()  \r\ni am getting this error , i installed tensorflow on a new conda environment, can't seem to work", "@Nick7hill Try running the script I [mentioned above](https://github.com/tensorflow/tensorflow/issues/9484#issuecomment-305242609), and if you still have problems after following its suggestions, feel free to open a new issue to describe them!", "@xuxianju Same here. Both 1.0 and 1.1 require cudnn5.1, so if 1.0 works, I'd expect 1.1 to load and work as well. But it doesn't. Were you able to fix this? I had to downgrade and stay with 1.0, which is not nice, since we have the 1.4 release now. And all the releases 1.1, 1.2, 1.3, 1.4 fail to import with these meaningless messages like in your log. Have no idea how to debug it. The script says \"failed to import\", but everything else is fine, \"all required dlls appear to be present\".", "For tensorflow 1.6 and higher you will get the same error message if your CPU does not support AVX.", "This seems like an old issue. I will close it. Please try the installation instructions of the latest version of TensorFlow. If the issue still persist, create a new issue."]}, {"number": 9483, "title": " minor change to `tf.contrib.framework.load_variable` doc", "body": "fix issue #9172 by referring to tensor values in a way consistent with the doc of `Variable.eval`\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variables.py#L459\r\n", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9482, "title": "Fix PATH error with non-system GCC (#7543).", "body": "I have been getting errors from `gcc` not finding `as`, due to my user-set PATH not being respected. See issue #7543.\r\nFor example, the following setting on Ubuntu 17.04 failed:\r\n\r\n    export PATH=$HOME/compilers/gcc540/install/bin:$PATH\r\n    export LD_LIBRARY_PATH=$HOME/compilers/gcc540/install/lib64:$LD_LIBRARY_PATH\r\n    ./configure\r\n        [...]\r\n    bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nA fix seems to be rather simple. By just adding `:$PATH` to the respective command, TensorFlow now builds correctly. See diff.\r\nHowever, I did not try to understand the full Python script, so I can't judge potential side effects.", "comments": ["Can one of the admins verify this patch?", "@caisq do you know who might know more about whether this is the right thing to do?  I no longer understand our complicated build environments :)", "@vrv This particular file has a lot of contribution from @keveman and @zheng-xq.\r\n\r\nBut this change is simple enough that it makes sense to me: it just ensures that whatever is already in `PATH` stays in `PATH` when `cmd` is invoked, which is necessary, e.g., if your `gcc` is not the system default. This is good practice in general.", "@tensorflow-jenkins test this please"]}, {"number": 9481, "title": "Error on Windows when installed in a virtualenv that has a non-ASCII character in the path", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64\r\n- **TensorFlow installed from (source or binary)**: binary from PyPI\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Exact command to reproduce**:\r\n\r\n      from tensorflow.contrib.rnn.python.ops.gru_ops import *\r\n\r\n### Describe the problem\r\nI encountered this when importing keras but it's reproducible with the command above. My virtualenv is in a folder that has an accented character in its name (see the log below). If I install TensorFlow globally, it works.\r\n\r\n### Source code / logs\r\n```\r\nD:\\Marci\\Programoz\u00e1s\\algorimp\\test>venv\\scripts\\python -c \"from tensorflow.contrib.rnn.python.ops.gru_ops import *\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"D:\\Marci\\Programoz\u00e1s\\algorimp\\test\\venv\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\", line 26, in <module>\r\n    from tensorflow.contrib import crf\r\n  File \"D:\\Marci\\Programoz\u00e1s\\algorimp\\test\\venv\\lib\\site-packages\\tensorflow\\contrib\\crf\\__init__.py\", line 32, in <module>\r\n    from tensorflow.contrib.crf.python.ops.crf import _lengths_to_masks\r\n  File \"D:\\Marci\\Programoz\u00e1s\\algorimp\\test\\venv\\lib\\site-packages\\tensorflow\\contrib\\crf\\python\\ops\\crf.py\", line 44, in <module>\r\n    from tensorflow.contrib.rnn.python.ops import core_rnn_cell\r\n  File \"D:\\Marci\\Programoz\u00e1s\\algorimp\\test\\venv\\lib\\site-packages\\tensorflow\\contrib\\rnn\\__init__.py\", line 80, in <module>\r\n    from tensorflow.contrib.rnn.python.ops.gru_ops import *\r\n  File \"D:\\Marci\\Programoz\u00e1s\\algorimp\\test\\venv\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\gru_ops.py\", line 32, in <module>\r\n    resource_loader.get_path_to_datafile(\"_gru_ops.so\"))\r\n  File \"D:\\Marci\\Programoz\u00e1s\\algorimp\\test\\venv\\lib\\site-packages\\tensorflow\\contrib\\util\\loader.py\", line 55, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"D:\\Marci\\Programoz\u00e1s\\algorimp\\test\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py\", line 64, in load_op_library\r\n    None, None, error_msg, error_code)\r\ntensorflow.python.framework.errors_impl.NotFoundError: D:\\Marci\\Programoz\u00e1s\\algorimp\\test\\venv\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\_gru_ops.dll not found\r\n```\r\n", "comments": ["@mrry, it looks like we pass to TF_LoadLibrary C api which takes char*, so maybe if we get it utf-8 in python and then at the very last moment in windows/env.cc  convert it to a `LPWSTR` with `MultiByteToWideChar` and use `LoadLibraryExW` instead of `LoadLibraryEx` it might not be terrible to get this working. ", "Sounds like a fine idea, but I'm not going to have time to work on it. Opening this up as \"Contributions Welcome\".", "I have got a similar issue like marczellm. \r\n\r\n- Operating system: Windows 10\r\n- Tensorflow version: tensorflow-gpu 1.1.0 (pip installation, in python 3.5.3 environment)\r\n- IDE: PyCharm\r\n\r\nAlthough I got tensorflow installed, the following command throws an error:\r\n\r\n`from tensorflow.examples.tutorials.mnist import mnist`\r\n\r\nError:\r\n\r\n```\r\n\"C:\\Users\\Rapha\u00ebl Baur\\AppData\\Local\\Programs\\Python\\Python35\\python.exe\" \"C:/Users/Rapha\u00ebl Baur/PycharmProjects/untitled8/y.py\"\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Rapha\u00ebl Baur/PycharmProjects/untitled8/y.py\", line 1, in <module>\r\n    from tensorflow.examples.tutorials.mnist import mnist\r\n  File \"C:\\Users\\Rapha\u00ebl Baur\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\__init__.py\", line 21, in <module>\r\n    from tensorflow.examples.tutorials.mnist import input_data\r\n  File \"C:\\Users\\Rapha\u00ebl Baur\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py\", line 29, in <module>\r\n    from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\r\n  File \"C:\\Users\\Rapha\u00ebl Baur\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\", line 26, in <module>\r\n    from tensorflow.contrib import crf\r\n  File \"C:\\Users\\Rapha\u00ebl Baur\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\crf\\__init__.py\", line 32, in <module>\r\n    from tensorflow.contrib.crf.python.ops.crf import _lengths_to_masks\r\n  File \"C:\\Users\\Rapha\u00ebl Baur\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\crf\\python\\ops\\crf.py\", line 44, in <module>\r\n    from tensorflow.contrib.rnn.python.ops import core_rnn_cell\r\n  File \"C:\\Users\\Rapha\u00ebl Baur\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\rnn\\__init__.py\", line 80, in <module>\r\n    from tensorflow.contrib.rnn.python.ops.gru_ops import *\r\n  File \"C:\\Users\\Rapha\u00ebl Baur\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\gru_ops.py\", line 32, in <module>\r\n    resource_loader.get_path_to_datafile(\"_gru_ops.so\"))\r\n  File \"C:\\Users\\Rapha\u00ebl Baur\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\util\\loader.py\", line 55, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"C:\\Users\\Rapha\u00ebl Baur\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py\", line 64, in load_op_library\r\n    None, None, error_msg, error_code)\r\ntensorflow.python.framework.errors_impl.NotFoundError: C:\\Users\\Rapha\u00ebl Baur\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\_gru_ops.dll not found\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nSo somehow \"_gru_ops.dll\" is not found, altough it is located in the indicated directory (output from my terminal):\r\n\r\n```\r\nC:\\Users\\Rapha\u00ebl Baur\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops>ls\r\n__init__.py  _gru_ops.dll   core_rnn.py       core_rnn_cell_impl.py  gru_ops.py   rnn.py\r\n__pycache__  _lstm_ops.dll  core_rnn_cell.py  fused_rnn_cell.py      lstm_ops.py  rnn_cell.py\r\n```\r\n\r\nIt could be because I'm an new to programming and tensorflow specifically, but this issue puzzles me, especially since tensorflow seems to actually work when running this code:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nhello = tf.constant('Hello, GitHub!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))\r\n```\r\nOutput:\r\n\r\n```\r\n\"C:\\Users\\Rapha\u00ebl Baur\\AppData\\Local\\Programs\\Python\\Python35\\python.exe\" \"C:/Users/Rapha\u00ebl Baur/PycharmProjects/untitled8/test.py\"\r\n2017-06-07 13:21:29.922172: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-07 13:21:29.922508: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-07 13:21:29.922818: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-07 13:21:29.923126: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-07 13:21:29.923433: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-07 13:21:29.923730: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-07 13:21:29.924007: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-07 13:21:29.924284: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-07 13:21:30.346729: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:887] Found device 0 with properties: \r\nname: GeForce GTX 970M\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.038\r\npciBusID 0000:01:00.0\r\nTotal memory: 6.00GiB\r\nFree memory: 5.02GiB\r\n2017-06-07 13:21:30.347086: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:908] DMA: 0 \r\n2017-06-07 13:21:30.347259: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:918] 0:   Y \r\n2017-06-07 13:21:30.347459: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0)\r\nb'Hello, GitHub!'\r\n\r\nProcess finished with exit code 0\r\n```\r\n\r\nAny form of help concerning that issue would be extremely appreciated (I am really beginning to despair).\r\n\r\n\r\n\r\n\r\n", "The only solution is to install both Python and Tensorflow in a directory that does not have \u00e4 or other non-ASCII characters in the path.", "It would be great to fix this problem, but I'll first need to clarify what API guarantees we make about character set. \r\n\r\n@girving: Does the fact that we use of `'utf-8'` as the default encoding in `compat.as_bytes()` imply that we should treat all user-provided strings as UTF-8 encoded? Do you know if we do anything special on Linux?\r\n\r\n@guschmue: Have you run into this problem with the Windows port? My reading of the API is that we can switch all calls to `CreateFileA()` (etc.) to `CreateFileW()` by using `MultiByteToWideChar()` to translate incoming `std::string` values into the necessary `LPCWSTR` argument. However, I tend to assume that handling Unicode properly is \"more complicated than you'd think\", so it would be great to get a second opinion!", "Yes, we treat all user provided `bytes` objects as UTF-8 encoded, and `compat.as_bytes()` will produce UTF-8 encoded `bytes` data regardless of the encoding of the input string.", "MultiByteToWideChar and CreateFileW (and related calls) sounds about right. Should be straight forward for doing the system call. Guess the complication comes with calls like FindFirstFile ... we'd need to convert the results as well but all of this should be contained in windows_file_system.cc", "Installing it in a different directory worked! Thank you! @marczellm ", "Path locale encoding problem, anybody still working on this issue?", "I'll work on this", "talked to @hectorlxm ... he has the fix and is adding some unit tests before sending pr.", "I think this issue is fixed through PR #11562.", "I don't have ANY non-ascii symbols in the path but still have this error. ", "@Jatik  Did find a solution for that? I just bumped into the same issue!", "And here is the error message with the path in it. I also verified that the file exists there.\r\n\r\nNotFoundError: D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\\\_gru_ops.dll not found", "The original problem in this issue was fixed in #11562. @MohammedPUD It sounds like you are facing a different problem. Please create a new issue and answer all of the questions in the template."]}, {"number": 9480, "title": "Tensorboard 0.0.0.0:6006 cannot work On Win7 but localhost:6006 works", "body": "I use tensorboard on Win7.\r\nEveryting works fine and the cmd shows \"Starting TensorBoard b'47' at http://0.0.0.0:6006\r\n\"\r\nBut when I  input \"http://0.0.0.0:6006\" into chrome, nothing happens.\r\nI google for it but find limited answer.\r\nThen I input \"http://localhost:6006\" into chrome, the page comes out.\r\n", "comments": ["Can you give the full output?", "Now I have same problem.. how to resolve this problem? ", "me too. I use 127.0.0.1:6006 to load logs. It works. \r\nI use tensorboard --logdir=logs --debug. It prints nothing.\r\nDoes it have no authority?\r\n![1](https://cloud.githubusercontent.com/assets/23494598/25580600/e4b9e3ca-2eb4-11e7-9273-6f7a464bbd1c.jpg)\r\n![2](https://cloud.githubusercontent.com/assets/23494598/25580601/e546978e-2eb4-11e7-8ebf-f6b80e2a1937.jpg)\r\n![3](https://cloud.githubusercontent.com/assets/23494598/25580605/e7b2ab48-2eb4-11e7-9818-7c8791dc674b.jpg)\r\n", "Mark! The same question, and i run tensorboard on the cloud and it has to show on my desktop because of the lack of GUI of Cloud!", "@dandelionmane Any ideas?", "I have the exact same problem! Does anyone has a solution for this problem?", "Migrating to tensorflow/tensorboard."]}, {"number": 9479, "title": "No audio for detected object on retrained model", "body": "By following [retraining tutorial](https://www.tensorflow.org/tutorials/image_retraining) and @petewarden's instructions from https://github.com/tensorflow/tensorflow/issues/2883, I am able to run the retrained model without errors. It detects any object from the model I created, but does not pronounce from speaker. It was working for demo model.\r\n", "comments": ["Detected object will not be pronunciated if the iPhone is on silent mode."]}, {"number": 9478, "title": "...", "body": "Sorry, I did not mean to create this. \r\n", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9477, "title": "Add 3D operations for layers: conv3d, avg_pool3d and max_pool3d", "body": "1.Fix a TODO(jbms):\r\n\r\nchange 'rate' parameter to 'dilation_rate' for consistency with underlying op.\r\n\r\n2.Add 3D operations for layers:\r\n\r\nconv3d\r\navg_pool3d\r\nmax_pool3d\r\n\r\n3.Replace a deprecated function in layers_test.py.\r\n\r\n4.Add unit test for avg_pool3d and max_pool3d.", "comments": ["Can one of the admins verify this patch?", "It's fine to break things in contrib. The other point stands though -- ideally we'd shrink the amount of code in contrib/layers. I suppose you require the contrib versions for something like arg_scope?", "@fchollet Many researchers who write programs using tensorflow slim need the conv3d and conv3d_transpose in tf.contrib.layers, such as [this issue](https://github.com/tensorflow/tensorflow/issues/9347).", "@martinwicke: it is theoretically fine to break things in contrib --but is this PR worth breaking every Slim model that uses dilation? Neither Slim nor filter dilation are particularly mainstream, but it's still a number of breakages to deal with --I count ~20 in g3.\r\n\r\n> Many researchers who write programs using tensorflow slim need the conv3d and conv3d_transpose in tf.contrib.layers\r\n\r\nFor some definition of \"many\" --only 200k or so people have touched TF, far fewer use 3d convolutions (500-1000?), fewer use transposed 3D convolutions (100-200?), and far fewer use Slim. If we total ~5-10 people that's already a stretch.", "@fchollet Maybe it's some users who want to use a 3D convolutions of Slim, especially in the medical field, since in medical field we need to process a series of CT slices which are organized in a 3D volume. Of course, this is from my own using experience. But I don't think this is a niche needs. So please rethink it.", "@fchollet @martinwicke  Thank you for your comments! However, the comment seems not fair.\r\n\r\n> For some definition of \"many\" --only 200k or so people have touched TF, far fewer use 3d convolutions (500-1000?), fewer use transposed 3D convolutions (100-200?), and far fewer use Slim. If we total ~5-10 people that's already a stretch.\r\n\r\nIn fact, conv3d/conv3d_transpose has become really popular these days: please check out the latest CVPR/NIPS/ICML papers that use 3d convolution:\r\n[1] Unsupervised Learning of 3D Structure from Images In NIPS 16 from Deepmind\r\n[2] Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling In NIPS 2016 from Facebook & MIT\r\n[3] Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision In NIPS 2016 from UM, Adobe & Google Brain\r\n[4] Shape Completion using 3D-Encoder-Predictor CNNs and Shape Synthesis In CVPR 2017 from Stanford\r\n[5] Learning a Predictable and Generative Vector Representation for Objects In ECCV 2016 from CMU & Google Research\r\nand so on.\r\n\r\nWe really appreciate if the conv3d/conv3d_transpose can be added to slim, which may benefit people working on 3D generation.\r\nAs far as I know, Torch has provided the conv3d/conv3d_transpose and that is one reason some researchers tend to avoid using TF for 3D generation.\r\n\r\n@Kongsea \r\nIs it possible to make any modification on your side? As far as I can see, it will cause TF developers to do some extra work (~20 breakages according to @fchollet ) which might not be the high priority for them. Thank you very much!", "I'm fine with this change. I think the only remaining question is whether we want to break existing models due to the `rate` to `dilation_rate` name change. @sguada, do you have a preference on this? We're definitely allowed to do it, but that doesn't necessarily mean we should.", "OK, now for simplicity I closed this PR and opened a [new PR](https://github.com/tensorflow/tensorflow/pull/9773) to add the 3D operations only and leave the 'rate' parameter as original."]}, {"number": 9476, "title": "iOS: No OpKernel was registered to support Op 'Less' with these attrs.  ", "body": "hi , all! I have tried to load the model inside iOS that I generated from python.\r\nand right now, I have the following problem:\r\n```\r\nError adding graph to session:\r\nNo OpKernel was registered to support Op 'Less' with these attrs.  \r\nRegistered devices: [CPU],     Registered kernels: device='CPU';\r\n T in [DT_FLOAT]  [[Node: rnn/while/Less = Less[T=DT_INT32](rnn/while/Merge, rnn/while/Less/Enter)]]\r\n```\r\n\r\nHere is the python script generating the model:\r\n```\r\ndef add_dynamic_rnn_layer(inputs, out_size, batch_size, Xt_size, time_step, num_layer=1, keep_prob=0.5):\r\n    # Reshaping to (batch_size, time_step, Xt_size)\r\n    inputs = tf.reshape(inputs, [-1, time_step, Xt_size])\r\n    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(out_size, state_is_tuple=True,forget_bias=1.0),\r\n                                                input_keep_prob=keep_prob)\r\n                             for _ in range(num_layer)])\r\n    cell =  tf.contrib.rnn.DropoutWrapper(cell,  input_keep_prob=keep_prob)\r\n    sequence_length = np.zeros([batch_size], dtype=int)\r\n    sequence_length += time_step\r\n    init_state = cell.zero_state(batch_size, tf.float32)\r\n    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=init_state, dtype=tf.float32, time_major=False,sequence_length=sequence_length)\r\n    return tf.transpose(rnn_outputs, [1, 0, 2])[-1]\r\n\r\n\r\nwith tf.Session() as sess:\r\n  # ...... other model code....\r\n   add_dynamic_rnn_layer()\r\n   output_graph_def = convert_variables_to_constants(sess, sess.graph_def,\r\n                                                          output_node_names=['predictions', 'prediction_labels'])\r\n   with tf.gfile.FastGFile('inference'+str(time.time())+'.pb', mode='wb') as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n```\r\nHere is the objective-C++ code loading the model and creating the session:\r\n```\r\n{\r\n    NSString *path = [[NSBundle mainBundle] pathForResource:pbname ofType:@\"pb\"];\r\n    if (!path) return false;\r\n    auto status = ReadBinaryProto(tensorflow::Env::Default(), path.fileSystemRepresentation, &graph);\r\n    if (!status.ok()) {\r\n        NSLog(@\"Error reading graph: %s\", status.error_message().c_str());\r\n        return NO;\r\n    }\r\n    \r\n    // This prints out the names of the nodes in the graph.\r\n    auto nodeCount = graph.node_size();\r\n    NSLog(@\"Node count: %d\", nodeCount);\r\n    for (auto i = 0; i < nodeCount; ++i) {\r\n        auto node = graph.node(i);\r\n        NSLog(@\"Node %d: %s '%s'\", i, node.op().c_str(), node.name().c_str());\r\n    }\r\n    \r\n    tensorflow::SessionOptions options;\r\n    auto status = tensorflow::NewSession(options, &session);\r\n    if (!status.ok()) {\r\n        NSLog(@\"Error creating session: %s\", status.error_message().c_str());\r\n        return NO;\r\n    }\r\n    \r\n    status = session->Create(graph);\r\n    if (!status.ok()) {\r\n        NSLog(@\"Error adding graph to session: %s\", status.error_message().c_str());\r\n    }  \r\n}\r\n```\r\nthe environment as follows:\r\n - python 3.5 / xcode 8.3.2\r\n - the iOS based on mac os x 10.12  / the tensorflow version: r1.1\r\n - the model generated based on Ubuntu / tensorflow is r1.1 gpu version\r\n\r\nI have done things as follow:\r\n- build_all_ios.sh    \r\n  - success,don't have any warning and error.\r\n- set the xcode build settings \r\n  - header search path\r\n  ```\r\n  /Users/jw/Desktop/tensorflow  non-recursive\r\n  /Users/jw/Desktop/tensorflow/tensorflow/contrib/makefile/downloads non-recursive\r\n  /Users/jw/Desktop/tensorflow/tensorflow/contrib/makefile/downloads/protobuf/src non-recursive\r\n  /Users/jw/Desktop/tensorflow/tensorflow/contrib/makefile/downloads/eigen non-recursive\r\n  /Users/jw/Desktop/tensorflow/tensorflow/contrib/makefile/gen/proto non-recursive\r\n  ```\r\n  -  other linker flags:        \r\n  ```\r\n  /Users/jw/Desktop/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a\r\n  /Users/jw/Desktop/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf-lite.a\r\n  /Users/jw/Desktop/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a\r\n  -force_load\r\n  ```\r\n - check the tf_op_files.txt\r\n   - have the line ` tensorflow/core/kernels/cwise_op_less.cc`\r\n - using the tensorflow version v1.0 \r\n   - have the same error\r\n\r\nI would be gratefull if anyone has an idea on why iOS seems to not be able to find the less Op ?\r\nOr the solution to the question : No OpKernel was registered to support Op 'Less' with these attrs.  \r\n", "comments": ["`Less` is only registered for `float32` on iOS, I believe.  @petewarden Do we have some documentation for this that I can point people to?  Ideally, the error message would say what is going on, but I realize that's hard.", "I did talk about this area a bit in my dev summit talk:\r\nhttps://www.youtube.com/watch?v=0r9w3V923rk\r\nI didn't cover the specific area of restricted types though, and in general the documentation here is lacking. There is a comment here that may be helpful:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_ops_common.h#L474\r\nThe summary is that you should be able to define `ANDROID_TYPES=__ANDROID_TYPES_FULL__` when you build TensorFlow, and you'll get all types but the binary size will be larger.", " @petewarden , thanks for your comments. \r\n\r\n>  define ANDROID_TYPES=__ANDROID_TYPES_FULL__ when you build TensorFlow \r\n\r\nyou mean edit the build_all_ios.sh as the follow?\r\n```\r\n# Build the iOS TensorFlow libraries.\r\ntensorflow/contrib/makefile/compile_ios_tensorflow.sh \"-O3\" --define ANDROID_TYPES=__ANDROID_TYPES_FULL__ \r\n```\r\n\r\nI am not sure whether I am correct  to add the define in the correct place, look forward to your reply. thanks again.", "The `--define` is Bazel syntax, for the makefile you'll need to use gcc's `-D`. Here's an example that I haven't tried:\r\n\r\n```bash\r\ntensorflow/contrib/makefile/compile_ios_tensorflow.sh \"-O3 -DANDROID_TYPES=__ANDROID_TYPES_FULL__ \"\r\n```\r\n\r\nNote that you have to include the `-D` in quotes alongside the `-O3` for it to work.", "@petewarden , thanks for your comment again. I tried your example. rebuild ios script. but not worked. Tired... Have any idea?\r\n> tensorflow/contrib/makefile/compile_ios_tensorflow.sh \"-O3 -DANDROID_TYPES=__ANDROID_TYPES_FULL__ \"\r\n", "@hansonboy so did you find a solution?", "@s1ddok no\uff0c I have the problem when I use RNN, but no problem when using CNN. So, I choose to use CNN.", "@s1ddok @petewarden @hansonboy I found a solution for this issue. Try this:\r\nin the (your tensorflow root)/tensorflow/contrib/makefile/Makefile \r\nremove the line \"-D__ANDROID_TYPES_SLIM__ \" under \"# Settings for iOS.\" for all \"$(IOS_ARCH)\"\r\nthen recompile the libtensorflow-core.a according to the modified Makefile.\r\n\r\nThe reason why this happened is because:\r\nin tensorflow/core/kernels/cwise_ops_common.h\r\n#if defined(__ANDROID_TYPES_SLIM__)\r\n// Note that __ANDROID_TYPES_SLIM__ is also checked in the cwise_ops*.cc files.\r\n// Normally Android TensorFlow is built with a reduced number of types (float).\r\n// Override on the command-line \"--define ANDROID_TYPES=__ANDROID_TYPES_FULL__\"\r\n// to generate a library with full type support with a consequent increase in\r\n// code size.\r\n#define REGISTER2(OP, D, N, F, T0, T1) REGISTER(OP, D, N, F, T0)\r\n#define REGISTER3(OP, D, N, F, T0, T1, T2) REGISTER(OP, D, N, F, T0)\r\n#define REGISTER4(OP, D, N, F, T0, T1, T2, T3) REGISTER(OP, D, N, F, T0)\r\n#define REGISTER5(OP, D, N, F, T0, T1, T2, T3, T4) REGISTER(OP, D, N, F, T0)\r\n#define REGISTER6(OP, D, N, F, T0, T1, T2, T3, T4, T5) REGISTER(OP, D, N, F, T0)\r\n#define REGISTER7(OP, D, N, F, T0, T1, T2, T3, T4, T5, T6) \\\r\n  REGISTER(OP, D, N, F, T0)\r\n#define REGISTER8(OP, D, N, F, T0, T1, T2, T3, T4, T5, T6, T7) \\\r\n  REGISTER(OP, D, N, F, T0)\r\n#define REGISTER9(OP, D, N, F, T0, T1, T2, T3, T4, T5, T6, T7, T8) \\\r\n  REGISTER(OP, D, N, F, T0)\r\n#else  // !defined(__ANDROID_TYPES_SLIM__)\r\n\r\n(ps: I also wrote a iOS_SSDMobilenet example base on the python ssd_mobilenet and ios_camera_example, and here is the github link:  @https://github.com/JieHe96/ios_SSDMobilenet_tensorflow_example)", "Is there a better solution than adding all the int ops to the binary? Is it possible to instead swap the int less op with a corresponding float less op + a cast to float?", "I got the same exception on Android. Just when I load conv2d net. Is there any solution for Android ?", "@Neilcc: don't know if it's still important for you but you can change line 51 in the Makefile \r\nANDROID_TYPES := -D__ANDROID_TYPES_SLIM__    to \r\nANDROID_TYPES := -D__ANDROID_TYPES_FULL__\r\nand compile again, then it should work", "> @Neilcc: don't know if it's still important for you but you can change line 51 in the Makefile\r\n> ANDROID_TYPES := -D__ANDROID_TYPES_SLIM__ to\r\n> ANDROID_TYPES := -D__ANDROID_TYPES_FULL__\r\n> and compile again, then it should work\r\n\r\nThe solution proposed by @NPetsky is correct. The build complete successfully and with my Xcode 10.1 can run without errors."]}, {"number": 9475, "title": "AttributeError: module 'tensorflow.contrib.cudnn_rnn' has no attribute 'CudnnLSTM'", "body": "I am experiencing this when I call tf.contrib.cudnn_rnn.CudnnLSTM:\r\n`AttributeError: module 'tensorflow.contrib.cudnn_rnn' has no attribute 'CudnnLSTM'`\r\n\r\nThis happens with 1.1.0-rc2, 1.0.1 and probably 1.0.0. Previously, I was using version 0.12.head, and everything is fine. I suspect this is because the model was not exposed in [tensorflow/contrib/cudnn_rnn/\\_\\_init__.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/__init__.py) in an earlier version.\r\n\r\nEven though this is fixed in this [commit]( https://github.com/tensorflow/tensorflow/commit/986d337e7da04de627218c12e20be1e3abbf6097), it's somehow not included in 1.1.0-rc2. In 1.1.0-rc2, when I look at the \\_\\_init__.py, it's still like this:\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnGRU\r\nfrom tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnLSTM\r\nfrom tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnRNNRelu\r\nfrom tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnRNNTanh\r\nfrom tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import RNNParamsSaveable\r\n\r\nfrom tensorflow.python.util.all_util import remove_undocumented\r\nremove_undocumented(__name__)\r\n```\r\n\r\nPlease fix this in the official release of 1.1 version.\r\n\r\nFor the reference, my 1.1.0-rc2 is installed from this:\r\nhttps://pypi.python.org/packages/fd/1a/b6e78223c8e05a8bdee8f9bb20d4926f81db50e583632a1cde6e5b5ec2f0/tensorflow-1.1.0-cp35-cp35m-manylinux1_x86_64.whl#md5=fc5ed08795ef5afa60b48ae916def79c", "comments": ["@zhangyaobit, could you take a look? I'm not sure if there is any intentional API changes here.", "1.1 has been released: https://github.com/tensorflow/tensorflow/releases/tag/v1.1.0\r\n\r\nLooks like the fix is not in. ", "okay, but this still needs to be fixed.", "Yes, thanks, boche! It turned out release 1.1 was cut just 3 days before the fix. \r\n\r\nThe fix will be part of the next release, which will be in the next two months or so and rc0 in about one month.\r\n\r\nIn the meantime, you may consider using the nightly build here: https://github.com/tensorflow/tensorflow", "I am experiencing this when I call tf.contrib.cudnn_rnn.CudnnLSTM with Python3.5.2 and tensorflow1.1.0:\r\n`AttributeError:module 'tensorflow.contrib.cudnn_rnn' has no attribute 'CudnnLSTM'` \r\nwhile the above code works fine with Python3.6 and tensorflow1.2.1.\r\nI checked tensorflow/contrib/cudnn_rnn/__init__.py and found there are some differences.\r\nIn `__init__.py` under tensorflow/contrib/cudnn with python3.6, there reads\r\n\r\n    from tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnLSTM\r\n    from tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnRNNRelu\r\n    from tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnRNNTanh\r\n    from tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import RNNParamsSaveable\r\n    from tensorflow.python.util.all_util import remove_undocumented\r\n    _allowed_symbols = [\r\n    \"CudnnGRU\",\r\n    \"CudnnLSTM\",\r\n    \"CudnnRNNRelu\",\r\n    \"CudnnRNNTanh\",\r\n    \"RNNParamsSaveable\",\r\n    ]\r\n\r\n    remove_undocumented(__name__)\r\nwhile in `__init__.py` under tensorflow/contrib/cudnn with python3.5.2, statement \r\n ` _allowed_symbols = [\r\n    \"CudnnGRU\",\r\n    \"CudnnLSTM\",\r\n    \"CudnnRNNRelu\",\r\n    \"CudnnRNNTanh\",\r\n    \"RNNParamsSaveable\",\r\n    ]`\r\nis lost.\r\n    "]}, {"number": 9474, "title": "Tensorboard text summary ops not on r1.1", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.4\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: `v1.1.0-rc0-61-g1ec6ed5 1.1.0`\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: `tmp = tf.summary.text('Hello')`\r\n\r\n### Describe the problem\r\nTensorboard in r1.1 has a text summary tab and it is mentioned in RELEASE.md but it looks like the command to add a text summary is not available. It's on the [master](https://github.com/tensorflow/tensorflow/blob/6a1825e2369d2537e15dc585705c53c4b763f3f6/tensorflow/python/summary/summary.py#L65) branch but not on [r1.1](https://github.com/tensorflow/tensorflow/blob/1ec6ed51182adf8f1b03a3188c16cd8a45ca6c85/tensorflow/python/summary/summary.py#L44-L65)\r\n\r\n### Source code / logs\r\nn/a\r\n", "comments": ["Sorry for the formalism, I'm sure it's just a li'l mistake :) ", "@dandelionmane It does look like `r1.1` has 55dee478b92486b8f3e420950e30998716fadae9 but not 6ef3aaf51a40d0d8a45c8191da13ebf48f1c1671.", "Fixed in `r1.2`"]}, {"number": 9473, "title": "ModuleNotFoundError: No module named 'tensorflow'", "body": "### Problem\r\n\r\nI have problem running tensorflow, I am quite new in using tensorflow and Python, hence pardon me if this is stupid question to ask. The problem is when I tried running tensorflow in windows command prompt, it worked just fine but then it give me ModuleNotFoundError when I tried running it through .py file. Is there something wrong? Please help me, thank you\r\n------------------------\r\n\r\n### System information\r\n- Tensorflow version 1.1 (installed using instruction given in [Here](https://www.tensorflow.org/install/install_windows) in conda environment named tensorflow-gpu\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: pre-built binary\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n\r\n\r\n### Source code / logs\r\n```\r\n\r\n(tensorflow-gpu) C:\\Users\\USER\\Anaconda3\\envs>python\r\nPython 3.5.3 |Continuum Analytics, Inc.| (default, Feb 22 2017, 21:28:42) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n2017-04-27 12:01:30.461948: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-27 12:01:30.462096: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-27 12:01:30.462199: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-27 12:01:30.462298: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-27 12:01:30.462375: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-27 12:01:30.462448: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-27 12:01:31.657721: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:887] Found device 0 with properties:\r\nname: GeForce GT 740M\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 1.0325\r\npciBusID 0000:07:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.67GiB\r\n2017-04-27 12:01:31.657920: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:908] DMA: 0\r\n2017-04-27 12:01:31.659743: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:918] 0:   Y\r\n2017-04-27 12:01:31.660988: I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 740M, pci bus id: 0000:07:00.0)\r\n>>> print(sess.run(hello))\r\nb'Hello, TensorFlow!'\r\n>>> ^Z\r\n\r\n\r\n(tensorflow-gpu) C:\\Users\\USER\\Anaconda3\\envs>hello_tf.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\USER\\Anaconda3\\envs\\hello_tf.py\", line 1, in <module>\r\n    import tensorflow as tf\r\nModuleNotFoundError: No module named 'tensorflow'\r\n```\r\n\r\nHere is my hello_tf.py:\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))\r\n```\r\n", "comments": ["maybe the default .py is associated w/ a different version of python. Just do \r\n```\r\nC:\\Users\\USER\\Anaconda3\\envs>python hello_tf.py\r\n```\r\nand see if that works.", "That solved the problem. Thanks\r\n"]}, {"number": 9472, "title": "a", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Keyboard shortcuts. Typing in wrong tab. Ignore. Thanks!"]}, {"number": 9471, "title": "Time cost for each training step increases with the training procedure", "body": "When I try to run a SRGAN network by 32 images with 96 * 96 size, each training step the time cost increases. At the beginning, each step cost 35 seconds, but when 160 steps later, the time cost increases to more than 200 seconds. By checking the time log, I can see it do increase with the training step. If I save the model and restart training process, the time cost reduces to about 35 seconds and start increasing again.", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "== cat /etc/issue ===============================================\r\nLinux inverse 4.4.0-75-generic #96-Ubuntu SMP Thu Apr 20 09:56:33 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux inverse 4.4.0-75-generic #96-Ubuntu SMP Thu Apr 20 09:56:33 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.1rc1)\r\nprotobuf (3.2.0)\r\ntensorflow-gpu (1.0.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7845\r\npciBusID 0000:01:00.0\r\nTotal memory: 5.92GiB\r\nFree memory: 1.19GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0)\r\ntf.VERSION = 1.0.1\r\ntf.GIT_VERSION = v1.0.0-65-g4763edf-dirty\r\ntf.COMPILER_VERSION = v1.0.0-65-g4763edf-dirty\r\nSanity check: array([1], dtype=int32)\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda-8.0/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Apr 27 21:44:49 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 106...  Off  | 0000:01:00.0      On |                  N/A |\r\n| 15%   46C    P2    29W / 150W |   4781MiB /  6064MiB |      3%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1432    G   /usr/lib/xorg/Xorg                             209MiB |\r\n|    0      2030    G   compiz                                          62MiB |\r\n|    0      3119    C   python                                        4427MiB |\r\n|    0      4590    G   /home/inverse/.steam/ubuntu12_32/steam          28MiB |\r\n|    0      6412    G   ...el-token=66A5E1EB1D60EC20A68FDE459A10598F    48MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n\r\nstep: 1160\r\nstep cost: 44s\r\nstep: 1161\r\nstep cost: 32s\r\nstep: 1162\r\nstep cost: 34s\r\nstep: 1163\r\nstep cost: 33s\r\nstep: 1164\r\nstep cost: 33s\r\nstep: 1165\r\nstep cost: 34s\r\nstep: 1166\r\nstep cost: 34s\r\nstep: 1167\r\nstep cost: 35s\r\nstep: 1168\r\nstep cost: 36s\r\nstep: 1169\r\nstep cost: 38s\r\nstep: 1170\r\nstep cost: 39s\r\nstep: 1171\r\nstep cost: 40s\r\nstep: 1172\r\nstep cost: 42s\r\nstep: 1173\r\nstep cost: 43s\r\nstep: 1174\r\nstep cost: 44s\r\nstep: 1175\r\nstep cost: 51s\r\nstep: 1176\r\nstep cost: 50s\r\nstep: 1177\r\nstep cost: 51s\r\nstep: 1178\r\n\r\n\r\nYou can see the time cost increases with step, after restart the training task it reduces to 35s again\r\n\r\nMy training code like this\r\n```\r\n while queue.size().eval() > batch_size:\r\n        start_time = time.time()\r\n        step = global_step.eval()\r\n        print('step: %d' % step)\r\n        x_batch = [get_image(image.eval()) for i in xrange(batch_size)]\r\n        summary_str, d_loss, g_loss = sess.run([summary, d_train_op, g_train_op], feed_dict={x: x_batch})\r\n        summary_writer.add_summary(summary_str, step)\r\n\r\n        if global_step.eval() % 20 == 0:\r\n                checkpoint_file = os.path.join(model_dir, 'model.latest')\r\n                saver.save(sess, checkpoint_file)\r\n\r\n        if global_step.eval() % 100 == 0:\r\n                summary_writer.add_run_metadata(run_metadata, 'step%03d' % step)\r\n\r\n        sess.run(update_global_step)\r\n        print(\"step cost: %ds\" % (time.time() - start_time))\r\nsummary_writer.close()\r\n```\r\n\r\n\r\n", "Could you monitor memory usage through the steps? \r\nAre you using jemalloc (which is installed by default)?\r\nIf you remove all the summary code does it still slow down? \r\nCan you produce a minimal reproducible test case?", "How can I monitor the memory usage? My OS uses the default malloc algorithm of Ubuntu 16.04 LTS, which I think is jemalloc. I will try remove the summary code later. My code only contains the definition of network and loss function, and read dir to get image samples, and use them to train", "@aselle \r\nI found the time increase in the read jpeg images, when I start train session the time cost for reading 1 image from HDD is less than 1 second, but it increases with steps,after several steps it reaches more than 3 seconds and have fluctuation which can be more than 10 seconds for some images.\r\nMy read image function is as below\r\n```\r\ndef get_image(image_path):\r\n    file = tf.read_file(image_path)\r\n    image = tf.image.decode_image(file, channels=channels)\r\n    sess = tf.get_default_session()\r\n    file, image = sess.run([file, image])\r\n    return image\r\n```\r\n\r\nI log the time in the start of this function and get time cost before return. For each training step I dequeue the batch_size of image's path from random shuffle queue and read them to build a image batch, this function will be called batch_size times\r\n\r\nThe time cost for Adam Optimizer is stable", "To monitor memory look at\r\nhttps://docs.python.org/2/library/resource.html\r\ngetursage\r\n\r\n", "@aselle \r\nThx for your guide, the memory usage do increases. The log in my MBP with CPU-version tensorflow is as below\r\n\r\n```\r\nstep: 323\r\nread image cost: 40s\r\nstep cost: 65s\r\nMemory usage: 5172464\r\nstep: 324\r\nread image cost: 40s\r\nstep cost: 67s\r\nMemory usage: 5190316\r\nstep: 325\r\nread image cost: 40s\r\nstep cost: 65s\r\nMemory usage: 5224076\r\nstep: 326\r\nread image cost: 42s\r\nstep cost: 67s\r\nMemory usage: 5243708\r\nstep: 327\r\nread image cost: 47s\r\nstep cost: 76s\r\nMemory usage: 5285364\r\n```\r\n\r\nAfter several steps (from check point) the memory usage start increases, I don't know why, you can see my [code](https://github.com/ruiann/SRGAN/blob/master/SRGAN/train.py)", "Hi @ruiann, are you able to construct a minimal reproducible example of this problem? As @aselle asked, if you remove all of the summary code, does the problem still occur?", "@ali01 \r\nThis is the minimal reproduce I think, no useless code, use a SRGAN network. I don't know how can I minimize it without break the network.\r\n\r\nIt occurs regardless of the summary. I wonder if it's because the random shuffle queue but have no time to deal with it recent days.", "Hey @zheng-xq, could you take a look? Thanks!", "Ping!\r\nAre there any updates here?", "@gunan sorry I'm busy with other works. This behavior can be reproduced in my own pc, I guess the reason is the memory usage of random shuffle queue?", "Trying to reproduce this on my local machine with CUDA 8, CUDNN v5.1.10, and tensorflow v1.1.0.\r\nCurrently has run 28 steps:\r\nMemory usage: 4043\r\nstep: 0\r\nread image cost: 37s\r\nstep cost: 73s\r\nMemory usage: 4811\r\nstep: 1\r\nread image cost: 38s\r\nstep cost: 39s\r\nMemory usage: 4811\r\nstep: 2\r\nread image cost: 38s\r\nstep cost: 38s\r\nMemory usage: 4811\r\nstep: 3\r\nread image cost: 40s\r\nstep cost: 40s\r\nMemory usage: 4811\r\nstep: 4\r\nread image cost: 40s\r\nstep cost: 41s\r\nMemory usage: 4811\r\nstep: 5\r\nread image cost: 41s\r\nstep cost: 41s\r\nMemory usage: 4811\r\nstep: 6\r\nread image cost: 39s\r\nstep cost: 40s\r\nMemory usage: 4811\r\nstep: 7\r\nread image cost: 44s\r\nstep cost: 44s\r\nMemory usage: 4811\r\nstep: 8\r\nread image cost: 45s\r\nstep cost: 46s\r\nMemory usage: 4811\r\nstep: 9\r\nread image cost: 46s\r\nstep cost: 46s\r\nMemory usage: 4811\r\nstep: 10\r\nread image cost: 47s\r\nstep cost: 47s\r\nMemory usage: 4811\r\nstep: 11\r\nread image cost: 49s\r\nstep cost: 50s\r\nMemory usage: 4811\r\nstep: 12\r\nread image cost: 49s\r\nstep cost: 49s\r\nMemory usage: 4811\r\nstep: 13\r\nread image cost: 51s\r\nstep cost: 52s\r\nMemory usage: 4811\r\nstep: 14\r\nread image cost: 54s\r\nstep cost: 55s\r\nMemory usage: 4811\r\nstep: 15\r\nread image cost: 54s\r\nstep cost: 54s\r\nMemory usage: 4811\r\nstep: 16\r\nread image cost: 55s\r\nstep cost: 56s\r\nMemory usage: 4811\r\nstep: 17\r\nread image cost: 58s\r\nstep cost: 58s\r\nMemory usage: 4811\r\nstep: 18\r\nread image cost: 58s\r\nstep cost: 58s\r\nMemory usage: 4811\r\nstep: 19\r\nread image cost: 59s\r\nstep cost: 60s\r\nMemory usage: 4811\r\nstep: 20\r\nread image cost: 62s\r\nstep cost: 107s\r\nMemory usage: 5884\r\nstep: 21\r\nread image cost: 63s\r\nstep cost: 64s\r\nMemory usage: 5884\r\nstep: 22\r\nread image cost: 65s\r\nstep cost: 65s\r\nMemory usage: 5884\r\nstep: 23\r\nread image cost: 65s\r\nstep cost: 66s\r\nMemory usage: 5884\r\nstep: 24\r\nread image cost: 66s\r\nstep cost: 67s\r\nMemory usage: 5884\r\nstep: 25\r\nread image cost: 70s\r\nstep cost: 71s\r\nMemory usage: 5884\r\nstep: 26\r\nread image cost: 73s\r\nstep cost: 73s\r\nMemory usage: 5884\r\nstep: 27\r\nread image cost: 72s\r\nstep cost: 73s\r\nMemory usage: 5884\r\n\r\nSo the per-step training time does seem to be increasing. I will let the program run a bit longer to verify this.", "So at step 40, the memory usage and run time per training step is like this:\r\nstep: 40\r\nread image cost: 96s\r\nstep cost: 151s\r\nMemory usage: 7049\r\n\r\nBy restarting the training both memory usage and run time drop:\r\nstep: 40\r\nread image cost: 37s\r\nstep cost: 72s\r\nMemory usage: 4832\r\n\r\nI will investigate on this.", " I found the same problem, and I fixed it by puting `tf.image.decode_image` into building graph step instead of running step.", "@heloowird\r\nSo it seems that the decode call will cost more and more time with the times of call increases?\r\n\r\n@yzhwang ", "yes, you can simply test time cost of `x_batch = [get_image(image.eval()) for i in xrange(batch_size)]`", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ruiann Could you try converting image files to TFRecord first (example here: https://github.com/tensorflow/models/blob/master/research/inception/inception/data/build_image_data.py), then using our new Dataset abstraction? I believe that will solve the problem too.\r\nhttps://www.tensorflow.org/programmers_guide/datasets\r\n\r\nI haven't spent too much time on figuring out why tf.image.decode_image() takes longer and uses more memory as training steps go up. First could you please verify that the problem can be solved by either: 1) moving the batch building to graph building, or 2) trying to use Dataset?", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@yzhwang \r\nthx for your reply. I'll make some try, but I think it's better to solve the tf.image.decode_image issue.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@drpngx Do you know who would be a better one to handle this issue on tf.image.decode_image()?", "@ruiann could you run the heap profiler to see the call graph of the allocations?", "@drpngx \r\nI'm sorry but I'm not familiar with memory debug\ud83d\ude02", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi guys! I have the same problem. I have noticed that the training time is not increasing when I disable all summary operations.", "Nagging Assignee @yzhwang: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "same problem here when training tensorflow object detection api's faster_rcnn_inception", "Reading the comments above, the problem seemed to be with this file: https://github.com/ruiann/SRGAN/blob/master/SRGAN/train.py. I don't know how to run that file, but it adds ops to the graph every iteration, by [calling get_image](https://github.com/ruiann/SRGAN/blob/cfeae6823b41bfcafe6610c423e921b4837ab1de/SRGAN/train.py#L59). Adding ops every iteration will increase memory usage, since ops take memory. I haven't tested, but perhaps it can also make each iteration take longer, since there are more ops in the graph which perhaps are iterated over.\r\n\r\n@ruiann, I'm closing this issue for now. If you still have the problem after removing all op creations before the loop, feel free to reopen and/or comment. If anyone else has this problem, please file a new issue with a self-contained example.", "@wuchichung do you found a solution to your problem? I'm facing the same issue."]}, {"number": 9470, "title": "Cmake build error on windows 10 (fatal error C1001)", "body": "\r\n\r\n### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**: Windows 10\r\n- **TensorFlow installed from**:source\r\n- **TensorFlow version**:master\r\n- **Bazel version**:NA\r\n- **CUDA/cuDNN version**:8.0/5.1\r\n- **GPU model and memory**:GTX 1080\r\n- **Exact command to reproduce**:MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj\r\n\r\n\r\n### Describe the problem\r\nI was following cmake build instruction\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake\r\nto build python package with GPU support using visual studio 2015 Update 3. It took a few hours until I hit fatal error C1001.\r\n\r\n### logs\r\n```\r\nc:\\work\\libraries_local\\tensorflow\\tensorflow\\core\\kernels\\reduction_ops_gpu.cu.cc(64): fatal error C1001: An internal error has occurred in the compiler. [C:\\work\\libraries_local\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n  \r\n  (compiler file 'f:\\dd\\vctools\\compiler\\utc\\src\\p2\\main.c', line 255)\r\n  \r\n   To work around this problem, try simplifying or changing the program near the locations listed above.\r\n```", "comments": ["Ah, an internal compiler error in the main Eigen statement.  Not sure what can be done here other than using a newer version of Visual Studio.\r\n\r\n@mrry: We don't seem to have documentation on which Visual Studio versions work.  I realize none of them are supported, but is there a place I can refer to about unsupported versions?", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md talks about known-good configurations, but I'm pretty sure we build with VS 2015 Update 3 regularly. \r\n\r\nPerhaps the introduction of DT_COMPLEX support in https://github.com/tensorflow/tensorflow/commit/88a6cdeb5ee79765462932a611d4d16dd715007c has broken the GPU build on Windows? ", "@guileryu01 Can you disable the complex instantiation and see if it goes through?", "Thank you. Sure I will give it a try. How do I disable complex instantiation? Is that one of the options when generating solution and project files with cmake?\r\n\r\nBelow is the command I was using\r\n\r\n```\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\r\n-DSWIG_EXECUTABLE=C:/work/tools/swigwin-3.0.12/swig.exe ^\r\n-DPYTHON_EXECUTABLE=C:/Anaconda3/envs/python35/python.exe ^\r\n-DPYTHON_LIBRARIES=C:/Anaconda3/envs/python35/libs/python35.lib ^\r\n-Dtensorflow_ENABLE_GPU=ON ^\r\n-DCUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\cudnn5\" ^\r\n-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX\r\n```", "@guileryu01 Uncomment these lines in `eduction_ops_gpu.cu.cc`:\r\n\r\n    DEFINE_FOR_TYPE_AND_R(complex64, Eigen::internal::SumReducer<complex64>);\r\n    DEFINE_FOR_TYPE_AND_R(complex128, Eigen::internal::SumReducer<complex128>);\r\n    DEFINE_FOR_TYPE_AND_R(complex64, Eigen::internal::MeanReducer<complex64>);\r\n    DEFINE_FOR_TYPE_AND_R(complex128, Eigen::internal::MeanReducer<complex128>);\r\n    DEFINE_FOR_TYPE_AND_R(complex64, Eigen::internal::ProdReducer<complex64>);\r\n    DEFINE_FOR_TYPE_AND_R(complex128, Eigen::internal::ProdReducer<complex128>);\r\n", "I didn't try commenting out those lines, however it turns out that after removing\r\n```\r\n-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX\r\n```\r\nin the configuration, I managed to compile the project. Many thanks!", "@girving \r\nI observed the same thing as @guileryu01 and have a similar setup.\r\n```\r\nHave I written custom code: No\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from:source\r\nTensorFlow version:master\r\nBazel version:NA - (Using Cmake and Visual Studio 2015)\r\nCUDA/cuDNN version:8.0/5.1\r\nGPU model and memory:GTX 1060 6Gb\r\n```\r\nExact command to reproduce:\r\n\r\n\r\n```\r\nc:>cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\r\n-DSWIG_EXECUTABLE=C:/swigwin-3.0.12/swig.exe ^\r\n-DPYTHON_EXECUTABLE=C:/Users/tknight/AppData/Local/Programs/Python/Python35/python.exe ^\r\n-DPYTHON_LIBRARIES=C:/Users/tknight/AppData/Local/Programs/Python/Python35/libs/python35.lib ^\r\n-Dtensorflow_ENABLE_GPU=ON ^\r\n-DCUDNN_HOME=\"C:\\cuDNN\" ^\r\n-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX\r\n\r\nc:>MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj\r\n```\r\nresults in:\r\n```\r\n\\tensorflow\\core\\kernels\\reduction_ops_gpu.cu.cc(64): fatal error C1001: An internal error h as occurred\r\n```\r\nafter about 3 hours of building.\r\n\r\nIf I remove the line regarding arch:AVX in the cmake command\r\n```\r\n-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX \r\n```\r\nThe python wheel builds ok.\r\n\r\nI want to help getting the builds with CPU optimizations to work for Windows -\r\nI wasn't exactly clear what you were asking to be done.  I tried commenting out the six lines you referenced, and this resulted in 31 linking errors instead. - I have clipped them from the console output and put them in [build_errors.txt](https://github.com/tensorflow/tensorflow/files/966527/build_errors.txt).\r\n\r\nLet me know if/how to continue..", "I ran a build with \r\n```\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\r\n-DSWIG_EXECUTABLE=C:/swigwin-3.0.12/swig.exe ^\r\n-DPYTHON_EXECUTABLE=C:/Users/tknight/AppData/Local/Programs/Python/Python35/python.exe ^\r\n-DPYTHON_LIBRARIES=C:/Users/tknight/AppData/Local/Programs/Python/Python35/libs/python35.lib ^\r\n-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX\r\n\r\nMSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj /fl1 /fl2 /fl3 /flp2:logfile=JustErrors.log;errorsonly /flp3:logfile=JustWarnings.log;warningsonly\r\n```\r\nand it built ok (no errors), so the problem appears to be limited to the combination of tensorflow with GPU and arch:AVX.\r\n", "@teeekay Judging from those linker errors, there are a few more registrations to comment out:\r\n\r\nIn https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_mean.cc#L41-L42:\r\n\r\n```c++\r\n//TF_CALL_complex64(REGISTER_GPU_KERNELS);\r\n//TF_CALL_complex128(REGISTER_GPU_KERNELS);\r\n```\r\n\r\nIn https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_prod.cc#L42-L43:\r\n\r\n```c++\r\n//TF_CALL_complex64(REGISTER_GPU_KERNELS);\r\n//TF_CALL_complex128(REGISTER_GPU_KERNELS);\r\n```\r\n\r\nIn https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_sum.cc#L41-L42: \r\n\r\n```c++\r\n//TF_CALL_complex64(REGISTER_GPU_KERNELS);\r\n//TF_CALL_complex128(REGISTER_GPU_KERNELS);\r\n```\r\n", "@mrry , After commenting those lines out it built ok,\r\n\r\n```\r\n\"C:\\tools\\msys64\\home\\tknight\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (1) ->\r\n(PostBuildEvent target) ->\r\n  EXEC : warning : no files found matching '*.so' under directory '*' [C:\\tools\\msys64\\home\\tknight\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj]\r\n\r\n    4632 Warning(s)\r\n    0 Error(s)\r\n\r\nTime Elapsed 03:50:15.38\r\n```\r\n\r\n```\r\n Directory of C:\\tools\\msys64\\home\\tknight\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\dist\r\n\r\n05/01/2017  03:51 PM    <DIR>          .\r\n05/01/2017  03:51 PM    <DIR>          ..\r\n05/01/2017  03:51 PM        51,751,621 tensorflow_gpu-1.1.0rc2-cp35-cp35m-win_amd64.whl\r\n               1 File(s)     51,751,621 bytes\r\n```\r\ninstalled wheel and it is functional.   next steps...", "Looks resolved. Please re-open if not..."]}, {"number": 9469, "title": "ImportError: No module named '_pywrap_tensorflow_internal'", "body": "Hi all,\r\n\r\nUsing windows 7 and have made tensorflow work in the cpu version. But keep getting this error in the gpu version. \r\n\r\nInstalled it using pip install tensorflow-gpu. Have cuda 8.0 and cudnn 5.1. Have the dll file and cudnn files copied over. GPU model is K610M.\r\n\r\nThanks in advance for the assistance.\r\n\r\nCapture of the screen:\r\n(C:\\Users\\KiraYamato\\Anaconda3) C:\\Users\\KiraYamato>activate tensorflow-gpu\r\n\r\n(tensorflow-gpu) C:\\Users\\KiraYamato>python\r\nPython 3.5.3 |Continuum Analytics, Inc.| (default, Feb 22 2017, 21:28:42) [MSC v\r\n.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tens\r\norflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\importlib\\__init__\r\n.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tens\r\norflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tens\r\norflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tens\r\norflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\importlib\\__init__\r\n.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tens\r\norflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tens\r\norflow\\python\\__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tens\r\norflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tens\r\norflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\importlib\\__init__\r\n.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tens\r\norflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tens\r\norflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tens\r\norflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\KiraYamato\\Anaconda3\\envs\\tensorflow-gpu\\lib\\importlib\\__init__\r\n.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_probl\r\nems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> ", "comments": ["Hi @quatre008,\r\nAs the [stack overflow link](http://stackoverflow.com/questions/35953210/error-running-basic-tensorflow-example) suggests, have you tried to invoke tensorflow by staying out of the source code directory?", "Hi @quatre008, sorry you're facing issues, could you please check if your CUDA and cuDNN DLLs are properly set on your `%PATH%`?\r\n", "Hi,\r\n\r\n@karandesai-96 I tried many different directories and still the same. Tensorflow worked but not tensorflow gpu.\r\n\r\n@Carmezim How do I know if it is properly set? I only found the following:\r\nPATH: C:\\Users\\KiraYamato\\Anaconda3;C:\\Users\\KiraYamato\\Anaconda3\\Scripts;C:\\Users\\KiraYamato\\Anaconda3\\Library\\bin\r\nCUDA_PATH : C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\r\nCUDA_PATH_V8_0: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0", "@quatre008 what happens sometimes is that cuDNN DLLs are installed in a different directory of CUDA so you need to add it also. Make sure you installed cuDNN correctly and if its DLLs are not in those directories already in your `%PATH%` you need to add them.", "@Carmezim I have copied the files into the same CUDA directory as mentioned. So I am guessing it should be working fine. I have re-downloaded and overwritten to make sure this was the case too.", "@quatre008 If you went over CUDA and cuDNN installation again and made sure you have everything properly set, completely uninstall TensorFlow and install it (only GPU) again. If the problem persists then it's not CUDA/cuDNN related.", "Ok I finally managed to resolve the issue.\r\n\r\n2 issues at work here. One being my Visual Studio was not installed properly, and the second one (outside of this topic but still interesting) is that my laptop has dual graphics boot-up.\r\n\r\nSo for the first issue, I managed to resolve it by deleting most folders and restarting the installation, starting from visual studio (the free version of community VS2017 did not work previously). \r\n\r\nThen as for the second issue, I only need to remove intel graphics from the bootup menu and solely use the K610M that I am having and everything is fine now, except for a list of warning messages that seems to be common in most cases.\r\n\r\nThanks for the help again. Much appreciated.", "Glad you got it resolved, and thank you for reporting information to help others in the future. Closing for now.", "Hi @quatre008 ... I am having the same error as u had. Which visual studio version did u use ?", "Hi, I am having this issue on Linux. I am trying to install tensorflow with pypy.\r\nMy ./configure has everything default except the python interpreter `/usr/local/bin/pypy`.\r\nWhen importing I get:\r\n```\r\nPython 2.7.13 (c925e73810367cd960a32592dd7f728f436c125c, Jun 08 2017, 09:20:57)\r\n[PyPy 5.8.0 with GCC 6.3.0 20170516] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/nn/pypy2-v5.8.0-src/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/root/nn/pypy2-v5.8.0-src/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/root/nn/pypy2-v5.8.0-src/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/root/nn/pypy2-v5.8.0-src/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/root/nn/pypy2-v5.8.0-src/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/root/nn/pypy2-v5.8.0-src/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nImportError: No module named _pywrap_tensorflow_internal\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>> \r\n```", "@mateon1 \r\nhttps://stackoverflow.com/questions/35953210/error-running-basic-tensorflow-example", "Try to use cuDNN v5.1 for CUDA 8.0, this solves the issue for me.", "If you are using tensorflow 1.3 then the current documentation, you want to use cuDNN v6.0 https://github.com/tensorflow/tensorflow/issues/7705", "* tensorflow 1.2 + cudnn 5.1: works fine\r\n* tensorflow 1.2 + cudnn 6.0: `ImportError`\r\n* tensorflow 1.3 + cudnn 5.1: works fine\r\n* tensorflow 1.3 + cudnn 6.0: works fine", "@chriswbarrett  Thanks so much!\r\n```\r\nTensorFlow 1.3 + cudNN 6.0 with Cuda 8 \r\n``` \r\n\r\nWorking!", "It really does work!\r\n\r\n> TensorFlow 1.3 + CuDNN 6.0 with Cuda 8", "TensorFlow 1.3 + CuDNN 6.0\u4e0eCuda 8", "TensorFlow 1.3 + CuDNN 6.0 with Cuda 8\r\n\r\nIt worked with CuDNN 6.0. I had \"import error\" with CuDNN 5.1.", "tensorflow 1.3 + cudnn 6.0: works fine\r\ntensorflow 1.3 + cudnn 7.0.2: import error", "@lintseju TensorFlow as of now only supports cuDNN v6.0", "@aselle should we lock the thread? The solution mostly comes down to reading the\r\ninstallation docs and is given multiple times in the thread already.", "Look like the TensorFlow has stated using cuDNN 6.x in the past couple of days (from 5.1). \r\n(cuDNN=NVIDIA CUDA Deep Neural Network library )\r\nhttps://www.tensorflow.org/install/install_windows#CommonInstallationProblems\r\nThe problem on my computer got solved after I upgrading cuDNN 6.x (TF will not work with cuDNN  7 yet)\r\nhttps://developer.nvidia.com/cudnn\r\n\r\nMy configuration is \r\nWindows 10 x64, with GPU GTX 1080 ti\r\nAnaconda 5.0.0 (with Python 3.6)\r\nCUDA Toolkit 8.0\r\ncuDNN v6.0", "On Win7, W7, Windows 7 try to install   Microsoft Visual C++ 2015 Redistributable Update 3 \r\n    https://www.microsoft.com/en-us/download/details.aspx?id=53587\r\nfirst. It helped for me.\r\nsee: https://github.com/tensorflow/tensorflow/issues/5949\r\n", "I have experienced the same issue after replacing hardware on my computer. I tried @mrry script to check the install was correct, and it was (no error reported but `import tensorflow` would fail). It appears that either my graphics drivers got corrupted or required to be upgraded (I tested by loading up Factorio and the texture would render all wrong).\r\n\r\nAfter installing the 388.00 drivers (http://us.download.nvidia.com/Windows/388.00/388.00-desktop-win8-win7-64bit-international-whql.exe), tensorflow would import again.", "- Had the same problem. Multiple issues could cause this: \r\nCPU Version, GPU Compute Compatibility, Cuda version, cuDNN version, tf version etc... I've tried to compile it all proper in the post here: https://github.com/rohit-patel/Install_Instructions-Win10-Deeplearning-Keras-Tensorflow", "I had the same problem with tensorflow 1.6, i downgrade it to 1.5 and everything is OK now.\r\n\r\npip uninstall tensorflow\r\nAnd then\r\npip install tensorflow==1.5", "@sddai , I have the same problem, my environment is win7+cuda9.1+cudnn7, I downgrade tensorflow-gpu to 1.5, I still have the same issue", "@yslTech Cuda 9.1 does not work with tf yet. I've outlined the instructions and troubleshooting in the post above.", "Please also check if your CPU supports AVX if you have installed tensorflow 1.6. I got this error since my CPU does not support AVX, so I had to downgrade to tensorflow 1.5.", "@MarioDogan @rohit-patel I uninstall cuda to 9.0 and I checked my CPU with tool CPU-Z , and found it doesn't support AVX, I downgrade tensorflow-gpu to 1.5 and finally I can import it.  Thank you so mach! ", "@sddai solution is working !", "MarioDogan's answer is the one if the PC you're running on is pre-2012, or if you are like \"wtf cuda library? I didn't install GPU version.\"", "[non cuda, cpu only version]\r\nthis is all about your video chip/card and CPU compatibility with TF VERSION! Current version of Tensor Flow is 1.8 and it crashes on both my machines.\r\n\r\nI have a win7 Gateway notebook using an I5 CPU and intel video chip. I have a desktop win7 unit with AMD Phenom II and AMD Radeon HD5450 video card. The I5 notebook requires V1.6 of TensorFlow and the desktop AMD unit takes V1.5. I don't have a GPU available for running ANN's so I don't know how that would affect it.\r\n\r\nHere's the command line for PIP install: pip install --upgrade tensorflow==1.5.0 (or whatever other version you need). This will uninstall your current version and then install the indicated version.\r\n\r\nBuild or copy a small TensorFlow script and run it. Open a second command window and install a different version. I'd recommend starting at 1.5 and working your way up.", "> On Win7, W7, Windows 7 try to install Microsoft Visual C++ 2015 Redistributable Update 3\r\n> https://www.microsoft.com/en-us/download/details.aspx?id=53587\r\n> first. It helped for me.\r\n> see: #5949\r\n\r\nplz confirm your tensorflow version and python version and system 32 bit or 64 bit\r\n", " i am also facing this issue .please suggest some solution", "I gave up on TensorFlow, but now I have Win10 machines, Python 3.7, and a new mother board and CPU so I'll give it a try later. I'll start with V1.5 and work my way up as suggested earlier. Also I have VS2019. I need to see if I've got the older C++ Redist. installed. That seems to be a major issue for TF."]}, {"number": 9468, "title": "Branch 154338128", "body": "", "comments": ["This should fix the ongoing breakage in Mac builds related to grappler. @martinwicke "]}, {"number": 9467, "title": "Link Error for the deprecated/__init__.py", "body": "It seems that the link \"look '[here](https://www.tensorflow.org/code/tensorflow/contrib/deprecated/__init__.py)'\" can not point to the right page in the [doc of histogram_summary](https://www.tensorflow.org/api_docs/python/tf/contrib/deprecated/histogram_summary), should we just change it to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/deprecated/__init__.py?\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@wolffg, can we add this to the fix it... I don't know if it is covered by another more general fix approach.\r\n", "@aselle I would like to work on the issue. How do I start?", "@aselle @wolffg Just wonder how does https://www.tensorflow.org resolve the url, seems that it can resolve https://www.tensorflow.org/code/tensorflow/contrib/deprecated/summaries_test.py to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/deprecated/summaries_test.py successfully while failed for \\_\\_init\\_\\_.py. One possible reason is that the \"_\" is some kind of special character for the URL resolver. It's better for us to check this first, otherwise we can just simply change https://www.tensorflow.org/code/tensorflow/contrib/deprecated/__init__.py to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/deprecated/__init__.py", "Close this issue since a pr has been merged to fix this."]}, {"number": 9466, "title": "Improved documentation for TensorArray", "body": "The current documentation for `TensorArray` could use an expanded introduction explaining its utility and main motivation, examples of actual usage, and further explanation of some of its methods. The docs [here](https://www.tensorflow.org/api_docs/python/tf/TensorArray) do not have a single usage example. The overall description of the section is just:\r\n\r\n\"This class is meant to be used with dynamic iteration primitives such as `while_loop` and `map_fn`. It supports gradient back-propagation via special \"flow\" control flow dependencies.\"\r\n\r\nHow is meant to be used with dynamic iteration primitives? What does that entail and why do we need a special construct for dynamic iteration? And what is \"flow\"? It's very cryptic as it stands. Furthermore, some methods are very poorly described, like `close`. It's unclear if it needs to be invoked at the end, or what? [Comments on stack overflow](http://stackoverflow.com/questions/41113004/what-is-the-effect-of-calling-tensorarray-close) only add to the confusion. With the increasing importance of dynamic graphs, a basic construct like `TensorArray` should really be well documented, like `Variable`.\r\n", "comments": ["Added to the doc fixit.  Assigning to @ebrevdo for now.", "it does need improved documentation. no promises when that documentation will materialize...", "regarding examples, see the unit tests which have a variety of usage examples.", "Ok I will look forward to the full docs and will peruse the unit tests for now. In the meantime, would it be possible to at least explain what `close` is doing and if there's any benefit to calling it in terms of freeing memory or some such?", "which unit tests are you referring to?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The unit tests are [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/tensor_array_ops_test.py).", "The world is still confused about best practices for calling .close().  Does anybody know?", "Sorry; in TF2 does one of two things:\r\n\r\n* In eager mode, it deletes the underlying values in the tensorarray (essentially releases memory)\r\n* In tf.function building mode, it does nothing.  The underlying data is released when there are no more ops referencing it.\r\n\r\nThis is kinda inconsistent.  In eager mode, reusing the TensorArray after close will lead to errors.  In tf.function mode, it'll work just fine (and in that case close has no effect).  @alextp if we care enough about this inconsistency.", "Ok gotcha that is very helpful"]}, {"number": 9465, "title": "Misleading error message when running restore op with path to an .index file", "body": "Using V2 checkpoint files, calling `saver.restore()` with path to the `.index` returns the following error: \r\n\r\n    Error loading checkpoint from /home/peci1/tradr-git/tradr-ws/src/tradr-simulation/safe_exploration/scripts/ddpg/models/my-model.index: Not found: Tensor name \"ActorFullyConnected1/b\" not found in checkpoint files /home/peci1/tradr-git/tradr-ws/src/tradr-simulation/safe_exploration/scripts/ddpg/models/my-model.index\r\n\r\nI think it'd be nicer to get an error saying \"File ... is not a checkpoint. Please, remove the .index extension\". This is roughly what you get if you pass any other file.", "comments": ["@concretevitamin, could you consider making this message more friendly, or could you direct @peci1 on how to prepare a contribution to that effect--provided @peci1  is willing.", "Marking this contributions welcome, if anyone wants to dig in and pick this up..."]}, {"number": 9464, "title": "Fix https://github.com/tensorflow/tensorflow/issues/8207", "body": "@martinwicke ", "comments": ["Can one of the admins verify this patch?", "Sorry, I'm looking at this a little bit out of context.  Some of the places seem to require backticks, some should possibly have single quotes, but I think there are still a bunch where double quotes are proper.  Why did *all* of them have to be changed?", "@dr4b  My original motivation for changing them was to make the doc parseable by Protobuf. Unescaped double quotes would mess up the parser. The parser also has the problem that you can't handle quoted newlines. I have a pull request I am working on for that.\r\n\r\nCurrently the above hasn't been a problem because the docs aren't exposed in the protobuf, but removing a single line can expose them. This can prove beneficial for certain applications.", "@MarkDaoust let me know what the next steps you suggest might be!", "Can one of the admins verify this patch?", "@aidan-plenert-macdonald please rebase to resolve conflicts.", "I'll close this, as it appears inactive. Please create a new PR if you would like to pick this back up."]}, {"number": 9463, "title": "Drupal8 chatbot integrate with TensorFlow", "body": "Looking for developer who able to integrate tensorflow for answer to users in messenger if it was not recognized by prepared static answers. \r\nhttps://www.drupal.org/project/chatbot/releases/8.x-1.x-dev\r\nHigh rate of operating for this job. ", "comments": ["Looking for developer who can use tensorflow ", "This is not a  bug or feature request."]}, {"number": 9462, "title": "numpy prod overflow during creating tensor", "body": "I am trying to allocate super large tensor using tensorflow, but failed.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/tensor_util.py#L417\r\nAbove code uses numpy.prod to calculate shape size, and for numpy everything is typed, say if the shape is [500000000, 5], then numpy.prod returns -1794967296, it's very easy to reproduce it.\r\nSo how about use int64 instead? int64 should be large enough for any tensor.\r\nChanging shape_size = np.prod(shape) to shape_size = np.prod(shape, dtype=np.int64) should fix it. Also about 100 lines of code using np.prod, could we change them all to int64?", "comments": ["@fesun Are you on a 32 bit machine?  I get\r\n\r\n    >>> np.prod([500000000, 5])\r\n    2500000000\r\n    >>> np.prod([500000000, 5]).dtype\r\n    dtype('int64')\r\n\r\nExplicitly setting `dtype=np.int64` does seem like the right solution, but I'd like to understand why it's breaking for you first.", "Maybe it's a Windows issue? https://docs.scipy.org/doc/numpy/user/basics.types.html#id3 says that the default integer type is the same as a C `long`, which is 32 bits in MSVC.", "@girving I am on 64 bit windows machine, I think @mrry is right, numpy default int type inside windows is 32 bit.", "@fesun Would you be up for a pull request?  Adding `dtype=np.int64` sounds good.", "@girving sure, thanks."]}, {"number": 9461, "title": "Branch 154257269", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 9460, "title": "a dead link to the TensorBoard: Graph Visualization tutorial in the TensorBoard page", "body": "Hi all,\r\n\r\nthere is a dead link to the TensorBoard: Graph Visualization tutorial from the TensorBoard Graphs page\r\n[\r\n<img width=\"668\" alt=\"screen shot 2017-04-26 at 16 17 40\" src=\"https://cloud.githubusercontent.com/assets/1344378/25439486/d78f29e2-2a9c-11e7-87e1-f73e0dbf848c.png\">\r\n](url)\r\nnow:\r\n\r\nhttps://www.tensorflow.org/versions/master/how_tos/graph_viz/\r\n\r\nshould be:\r\n\r\nhttps://www.tensorflow.org/get_started/graph_viz\r\n\r\nOS - MacOS,\r\nTensorFlow from pip or conda\r\nVersion: v1.0.0-rc2-15-g47bba63-dirty 1.0.0\r\n\r\nBest,\r\nPaddy", "comments": ["it seems I'm using an old version, everything is fine with the link in the repo:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/52dcb2590bb9274262656c958c105cb5e5cc1300/tensorflow/tensorboard/components/tf_dashboard_common/tf-no-data-warning.html \r\n\r\nsorry :(", "No worries!"]}, {"number": 9459, "title": "Add tensorflow/core/kernels/function_ops.cc to Makefile", "body": "This should fix the breakage in Makefile-based builds after 858e0afcc45c39b6428bf82ab1444323e925cfd8.\r\n\r\nFixes #9453.", "comments": []}]