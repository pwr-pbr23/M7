[{"number": 39926, "title": "ICPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA", "body": "```\r\n2020-05-28 11:16:19.791171: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-05-28 11:16:19.819834: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3599930000 Hz\r\n2020-05-28 11:16:19.820503: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x467ba80 executing computations on platform Host. Devices:\r\n2020-05-28 11:16:19.820524: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\ntf.Tensor(736.5775, shape=(), dtype=float32)\r\n```\r\n\r\n_Originally posted by @SarahLynnePu in https://github.com/tensorflow/tensorflow/issues/24814#issuecomment-635035003_", "comments": ["Is there anyone can support me to solve this issue please?", "Please also fill in issue template.\r\n\r\nNote that nothing in the message there (edited it for you to have proper markdown formatting) is an error. These are just information notices.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39926\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39926\">No</a>\n"]}, {"number": 39925, "title": "Missing pre-processing for Mobilenet V2 model training", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://colab.research.google.com/github/tensorflow/examples/blob/master/community/en/flowers_tf_lite.ipynb\r\n\r\nhttps://github.com/tensorflow/examples/blob/master/lite/codelabs/flower_classification/android/finish/app/src/main/java/org/tensorflow/lite/examples/classification/tflite/Classifier.java#L292\r\n\r\n## Description of issue (what needs changing):\r\nIn the above codelabs tutorial, we see image has been rescaled to [0-1] by dividing by 255. Since pre-trained weights (imagenet) are trained by feed [-1 1] normalized image. Ideally tutorial should add that step to correctly leverage transfer learning. \r\n\r\n### Clear description\r\nSo what is happening is we create a tflite model trained with [0-1] based preprocessing. On android client we are doing [-1 1] based preprocessing before feeding to tflite model.\r\nCan someone please clarify?\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@yashk2810 Could you please help with this?", "@xunkai55 Can you clarify why the TF Lite image classification seems to normalize image input to [-1, 1] instead of [0, 1]?\r\nhttps://github.com/tensorflow/examples/blob/master/lite/codelabs/flower_classification/android/finish/app/src/main/java/org/tensorflow/lite/examples/classification/tflite/Classifier.java#L292\r\n\r\n@getsanjeevdubey Btw have you tried out TF Lite Model Maker? It's an easier way to do transfer learning for TF Lite.\r\nhttps://www.tensorflow.org/lite/tutorials/model_maker_image_classification", "@khanhlvg Thanks for the response. I will look into TF Lite Model Maker.\r\nJust to make my point clear, the issue is with: not having **same** preprocessing for both pipelines.\r\nHad the original model was trained on [-1, 1] it would have been correct.", "You're right. The image classification model in the codelab is trained with Model Maker, which emits models required input normalization to [0, 1]. We have fixed the Android app accordingly. Thanks for reporting the issue.\r\n\r\nhttps://github.com/tensorflow/examples/commit/bb26df46b498eb1a67630f29619705eaa043528f"]}, {"number": 39924, "title": "DynamicPaddedBatchDatasetOp for tf.data", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tf-nightly\r\n- Are you willing to contribute it (Yes/No): Yes. If this feature is agreed, I'm happy to implement it.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n`tf.data` provides a `padded_batch` transformation, but it only supports a fixed batch size. In some use cases (e.g. model inference), `dynamic_padded_batch` will bring some extra performance benefits. \r\n\r\nFor example, for the named entity recognition model (e.g. BiLSTM-based, BERT-based), the input sentences/tokens will be required to be padded and then batched before feed into the model inference. If we use the fixed batch size, the short sentences and the long sentences may be combined together in a batch, which will add many unnecessary padding elements to the short sentences. For this kind of cases, `dynamic_padded_batch` transformation will be more efficient. It enables users to input a customized `dynamic_batch_function` to control how to split the batches. In this case, the sentences with similar lengths may be put together as a batch; If the next sentence is too long, it will be placed into the next batch; With this, the number of padding elements in each batch can be as small as possible. \r\n\r\nIn summary,  `DynamicPaddedBatchDatasetOp` will provide users with more flexibility to control how to split the batch.\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt will not change the current api; another new operation `DynamicPaddedBatchDatasetOp` will be added; \r\n\r\n**Who will benefit with this feature?**\r\n\r\nThe users who use `tf.data` to build the data pipeline.\r\n\r\n**Any Other info.**\r\n\r\ncc: @jsimsa @aaudiber \r\n", "comments": ["I think this use case could be addressed using the existing `tf.data.experimental.group_by_window` transformation. For instance, for the input pipeline for the official Transformer model uses this [transformation](https://github.com/tensorflow/models/blob/master/official/nlp/transformer/data_pipeline.py#L186-L190) to group sentences based on length and creates padded batches for each bucket separately.", "Thanks very much @jsimsa! Yeah, `group_by_window` can be used here. The example in the official Transformer model is exactly what I want do. Thanks again!", "Closed this issue as it has been resolved. "]}, {"number": 39923, "title": "NNAPI Reference does not output the same result as CPU for style transfer app on android for tensorflow lite", "body": "I have altered this app significantly such that \"StyleTransferModelExecutor\" could configure the interpreter with more fine granularity:\r\nFor example: \r\n`StyleExecutor(appContext, quant = true, device = Device.NNAPI, NNAcc = \"nnapi-reference\")`\r\n\r\nI am more than willing to share all that code, however the main point is:\r\nrunning inference with: \r\nstyle_predict_quantized_256.tflite\"\r\nstyle_transfer_quantized_384.tflite\r\n\r\nOn CPU is different than NNAPI reference.\r\n\r\nThis is how I set the options for nnapi reference:\r\n```\r\nval tfliteOptions = Interpreter.Options()\r\nvar opts = NnApiDelegate.Options()\r\nopts.setAcceleratorName(\"nnapi-reference\")\r\ntfliteOptions.addDelegate(NnApiDelegate(opts))\r\nInterpreter(loadModelFile(context, modelName), tfliteOptions)\r\n```\r\n\r\nI have an instrumented test that essentially runs noise through the models, and compares the results, and the results are significantly different. \r\n\r\nI believe this is a bigger issue than just transfer style, and really relates to quantized models on nnapi not agreeing with cpu. I just use this app as a basis for re reproducibility\r\n\r\nFeeding in noise:\r\n![content](https://user-images.githubusercontent.com/16094320/83071496-10d5ca00-a022-11ea-8a42-98d64db06e93.png)\r\n![style](https://user-images.githubusercontent.com/16094320/83071511-13d0ba80-a022-11ea-81da-79234966b65a.png)\r\n\r\nCPU and GPU inference agree:\r\n![cpuInference](https://user-images.githubusercontent.com/16094320/83071501-1206f700-a022-11ea-87fd-46bda36dceee.png)\r\n![gpuInference](https://user-images.githubusercontent.com/16094320/83071504-129f8d80-a022-11ea-97ee-a0c37960ea1d.png)\r\n\r\nNNAPI Reference differs:\r\n![nnInference](https://user-images.githubusercontent.com/16094320/83071508-13382400-a022-11ea-9a1f-b26f9d849aac.png)\r\n", "comments": ["(Further information)\r\n\r\nThe models are taken from the tflite example project \"style transfer\" and looking at this particular gradle file, you can see the links where the models are stored:\r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/style_transfer/android/app/download_model.gradle\r\n\r\nor:\r\nhttps://www.tensorflow.org/lite/models/style_transfer/overview\r\nat the bottom of this page, the models are there (though I used the gradle links)\r\n\r\nAnd I will include them in this message as well \r\n[quantized_models.zip](https://github.com/tensorflow/tensorflow/files/4696938/quantized_models.zip)\r\n\r\n", "Can confirm that the output is different for NNAPI vs CPU using the inference tool:\r\n\r\n```\r\n$ blaze build -c opt   --config=android_arm64   //third_party/tensorflow/lite/tools/evaluation/tasks/inference_diff:run_eval && adb push blaze-bin/third_party/tensorflow/lite/tools/evaluation/tasks/inference_diff/run_eval /data/local/tmp\r\n...\r\n$ adb shell /data/local/tmp/run_eval \\\r\n  --model_file=/data/local/tmp/style_predict_quantized_256.tflite \\\r\n  --delegate=nnapi\r\n...\r\nOutputDiff[0]: avg_error=2.25797, std_dev=0.0116284\r\n```\r\n\r\nThis shows a raw error of 2.2 averaged across all elements of the output tensor. For comparison, `delegate=hexagon` gives an error of ~0.008. I tried delegating only half the graph by modifying the delegate, and the error is still ~0.99.\r\n\r\nAdding Miao from NNAPI.\r\n\r\n", "The issue has been found in NNAPI delegate and the fix has been submitted internally.\r\n\r\nYou should be able to sync later today and verify the fix.\r\n\r\nThanks for reporting the issue!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39923\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39923\">No</a>\n"]}, {"number": 39922, "title": "MultiWorker Mirrored strategy fails to start with TF_CONFIG", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. A few imports, no fundamental changes. \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.9 \r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nScript is getting stuck at: \r\n2020-05-27 12:45:54.403718: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:12345\r\n\r\nNo further further output beyond this point\r\n\r\n**Describe the expected behavior**\r\nThe training should proceed as in tutorial.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nimport json\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"localhost:12345\", \"localhost:23456\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 0}\r\n})\r\n\r\n\r\ndef mnist_dataset(batch_size):\r\n  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\r\n  # The `x` arrays are in uint8 and have values in the range [0, 255].\r\n  # We need to convert them to float32 with values in the range [0, 1]\r\n  x_train = x_train / np.float32(255)\r\n  y_train = y_train.astype(np.int64)\r\n  train_dataset = tf.data.Dataset.from_tensor_slices(\r\n      (x_train, y_train)).shuffle(60000).repeat().batch(batch_size)\r\n  return train_dataset\r\n\r\ndef build_and_compile_cnn_model():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.Input(shape=(28, 28)),\r\n      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu'),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(128, activation='relu'),\r\n      tf.keras.layers.Dense(10)\r\n  ])\r\n  model.compile(\r\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n      metrics=['accuracy'])\r\n  return model\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\n# Worker related variables\r\nnum_workers = 4\r\nper_worker_batch_size = 64\r\nglobal_batch_size = per_worker_batch_size * num_workers\r\nmulti_worker_dataset = mnist_dataset(global_batch_size)\r\n\r\nwith strategy.scope():\r\n  # Model building/compiling need to be within `strategy.scope()`.\r\n  multi_worker_model = build_and_compile_cnn_model()\r\n\r\nmulti_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)\r\n```\r\n\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nComplete output of the script on execution:\r\n2020-05-27 20:36:51.353890: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-05-27 20:36:51.353922: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-05-27 20:36:51.353941: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dell3): /proc/driver/nvidia/version does not exist\r\n2020-05-27 20:36:51.380090: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3392065000 Hz\r\n2020-05-27 20:36:51.381000: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa764000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-27 20:36:51.381033: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-05-27 20:36:51.396045: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345, 1 -> localhost:23456}\r\n2020-05-27 20:36:51.396385: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:12345\r\n\r\nThe code is tested on a fresh virtual environment with tensorflow. The same error is reproduced with the tensorflow/tensorflow docker container. It was also reproduced with Python 3.8.3. \r\n\r\nThe script proceeds with execution if the code that sets the TF_CONFIG environment variable is removed. However, execution on multiple machines is not possible without setting TF_CONFIG.\r\n", "comments": ["@ssagkriotis,\r\nAs per [this document](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#multi-worker_configuration), TF_CONFIG with the above values is likely to fail.\r\n\r\nPlease take a look at [this guide](https://www.tensorflow.org/guide/distributed_training#setting_up_tf_config_environment_variable) to set up TF_CONFIG and let us know if it helps. Thanks!", "> \r\n> \r\n> @ssagkriotis,\r\n> As per [this document](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#multi-worker_configuration), TF_CONFIG with the above values is likely to fail.\r\n> \r\n> Please take a look at [this guide](https://www.tensorflow.org/guide/distributed_training#setting_up_tf_config_environment_variable) to set up TF_CONFIG and let us know if it helps. Thanks!\r\n\r\nThanks for replying. The issue ended up being that I had to run another instance of the whole script with the index number set to 1. "]}, {"number": 39921, "title": "Cannot save keras model in tf format", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): tensorflow-gpu==2.0.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nKeras model throws an error when I'm trying to save it in .tf format\r\n**Describe the expected behavior**\r\nModel saved without error\r\n**Standalone code to reproduce the issue**\r\n```\r\nfrom tensorflow.keras.layers import Input\r\nfrom tensorflow.keras.layers import Conv2DTranspose\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Concatenate\r\nimport tensorflow.keras.backend as K\r\nimport tensorflow.keras as keras\r\nclass MyConcat(keras.layers.Layer):\r\n    def __init__(self):\r\n        super(MyConcat, self).__init__()\r\n    def call(self, inputs):\r\n        x, emb = inputs\r\n        return Concatenate(axis=1)([x, emb])\r\n    \r\n    def compute_output_shape(self, input_shape):\r\n        shape = (None, input_shape[0][1] + input_shape[1][1],\r\n                input_shape[0][2], input_shape[0][3])\r\n        return shape\r\n\r\n```\r\n```\r\ninputs = Input(shape=(5,1,14))\r\nemb = Input(shape=(512,6,18))\r\n\r\nupconv1 = Conv2DTranspose(filters=16, \r\n                          kernel_size=(6,5), \r\n                          strides=(6,1),\r\n                          data_format=\"channels_first\")(inputs)\r\n\r\nx = MyConcat()([upconv1, emb])\r\n\r\ndecoder = Model(inputs=[inputs, emb], outputs=x)      \r\ndecoder.save(\"decoder.tf\", save_format=\"tf\")\r\n```\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-10-1bd6aa1b259b> in <module>\r\n     10 \r\n     11 decoder = Model(inputs=[inputs, emb], outputs=x)\r\n---> 12 decoder.save(\"decoder.tf\", save_format=\"tf\")\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    973     \"\"\"\r\n    974     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n--> 975                       signatures, options)\r\n    976 \r\n    977   def save_weights(self, filepath, overwrite=True, save_format=None):\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    113   else:\r\n    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n--> 115                           signatures, options)\r\n    116 \r\n    117 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)\r\n     72   # default learning phase placeholder.\r\n     73   with K.learning_phase_scope(0):\r\n---> 74     save_lib.save(model, filepath, signatures, options)\r\n     75 \r\n     76   if not include_optimizer:\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    868   if signatures is None:\r\n    869     signatures = signature_serialization.find_function_to_export(\r\n--> 870         checkpoint_graph_view)\r\n    871 \r\n    872   signatures = signature_serialization.canonicalize_signatures(signatures)\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_serialization.py in find_function_to_export(saveable_view)\r\n     62   # If the user did not specify signatures, check the root object for a function\r\n     63   # that can be made into a signature.\r\n---> 64   functions = saveable_view.list_functions(saveable_view.root)\r\n     65   signature = functions.get(DEFAULT_SIGNATURE_ATTR, None)\r\n     66   if signature is not None:\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py in list_functions(self, obj)\r\n    139     if obj_functions is None:\r\n    140       obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access\r\n--> 141           self._serialization_cache)\r\n    142       self._functions[obj] = obj_functions\r\n    143     return obj_functions\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in _list_functions_for_serialization(self, serialization_cache)\r\n   2420   def _list_functions_for_serialization(self, serialization_cache):\r\n   2421     return (self._trackable_saved_model_saver\r\n-> 2422             .list_functions_for_serialization(serialization_cache))\r\n   2423 \r\n   2424 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/base_serialization.py in list_functions_for_serialization(self, serialization_cache)\r\n     89         `ConcreteFunction`.\r\n     90     \"\"\"\r\n---> 91     fns = self.functions_to_serialize(serialization_cache)\r\n     92 \r\n     93     # The parent AutoTrackable class saves all user-defined tf.functions, and\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py in functions_to_serialize(self, serialization_cache)\r\n     77   def functions_to_serialize(self, serialization_cache):\r\n     78     return (self._get_serialized_attributes(\r\n---> 79         serialization_cache).functions_to_serialize)\r\n     80 \r\n     81   def _get_serialized_attributes(self, serialization_cache):\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes(self, serialization_cache)\r\n     92 \r\n     93     object_dict, function_dict = self._get_serialized_attributes_internal(\r\n---> 94         serialization_cache)\r\n     95 \r\n     96     serialized_attr.set_and_validate_objects(object_dict)\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/model_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)\r\n     51     objects, functions = (\r\n     52         super(ModelSavedModelSaver, self)._get_serialized_attributes_internal(\r\n---> 53             serialization_cache))\r\n     54     functions['_default_save_signature'] = default_signature\r\n     55     return objects, functions\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)\r\n    101     \"\"\"Returns dictionary of serialized attributes.\"\"\"\r\n    102     objects = save_impl.wrap_layer_objects(self.obj, serialization_cache)\r\n--> 103     functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n    104     # Attribute validator requires that the default save signature is added to\r\n    105     # function dict, even if the value is None.\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in wrap_layer_functions(layer, serialization_cache)\r\n    164   call_fn_with_losses = call_collection.add_function(\r\n    165       _wrap_call_and_conditional_losses(layer),\r\n--> 166       '{}_layer_call_and_return_conditional_losses'.format(layer.name))\r\n    167   call_fn = call_collection.add_function(\r\n    168       _extract_outputs_from_fn(layer, call_fn_with_losses),\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in add_function(self, call_fn, name)\r\n    492       # Manually add traces for layers that have keyword arguments and have\r\n    493       # a fully defined input signature.\r\n--> 494       self.add_trace(*self._input_signature)\r\n    495     return fn\r\n    496 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in add_trace(self, *args, **kwargs)\r\n    411             fn.get_concrete_function(*args, **kwargs)\r\n    412 \r\n--> 413         trace_with_training(True)\r\n    414         trace_with_training(False)\r\n    415       else:\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in trace_with_training(value, fn)\r\n    409           utils.set_training_arg(value, self._training_arg_index, args, kwargs)\r\n    410           with K.learning_phase_scope(value):\r\n--> 411             fn.get_concrete_function(*args, **kwargs)\r\n    412 \r\n    413         trace_with_training(True)\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in get_concrete_function(self, *args, **kwargs)\r\n    536     if not self.call_collection.tracing:\r\n    537       self.call_collection.add_trace(*args, **kwargs)\r\n--> 538     return super(LayerCall, self).get_concrete_function(*args, **kwargs)\r\n    539 \r\n    540 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)\r\n    774       if self._stateful_fn is None:\r\n    775         initializer_map = object_identity.ObjectIdentityDictionary()\r\n--> 776         self._initialize(args, kwargs, add_initializers_to=initializer_map)\r\n    777         self._initialize_uninitialized_variables(initializer_map)\r\n    778 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    406     self._concrete_stateful_fn = (\r\n    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 408             *args, **kwds))\r\n    409 \r\n    410     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1846     if self.input_signature:\r\n   1847       args, kwargs = None, None\r\n-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1849     return graph_function\r\n   1850 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in wrapper(*args, **kwargs)\r\n    513         layer, inputs=inputs, build_graph=False, training=training,\r\n    514         saving=True):\r\n--> 515       ret = method(*args, **kwargs)\r\n    516     _restore_layer_losses(original_losses)\r\n    517     return ret\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in wrap_with_training_arg(*args, **kwargs)\r\n    109         training,\r\n    110         lambda: replace_training_and_call(True),\r\n--> 111         lambda: replace_training_and_call(False))\r\n    112 \r\n    113   # Create arg spec for decorated function. If 'training' is not defined in the\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py in smart_cond(pred, true_fn, false_fn, name)\r\n     57         pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n     58   return smart_module.smart_cond(\r\n---> 59       pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n     60 \r\n     61 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)\r\n     52   if pred_value is not None:\r\n     53     if pred_value:\r\n---> 54       return true_fn()\r\n     55     else:\r\n     56       return false_fn()\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in <lambda>()\r\n    108     return tf_utils.smart_cond(\r\n    109         training,\r\n--> 110         lambda: replace_training_and_call(True),\r\n    111         lambda: replace_training_and_call(False))\r\n    112 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in replace_training_and_call(training)\r\n    104     def replace_training_and_call(training):\r\n    105       set_training_arg(training, training_arg_index, args, kwargs)\r\n--> 106       return wrapped_call(*args, **kwargs)\r\n    107 \r\n    108     return tf_utils.smart_cond(\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in call_and_return_conditional_losses(inputs, *args, **kwargs)\r\n    555   layer_call = _get_layer_call_method(layer)\r\n    556   def call_and_return_conditional_losses(inputs, *args, **kwargs):\r\n--> 557     return layer_call(inputs, *args, **kwargs), layer.get_losses_for(inputs)\r\n    558   return _create_call_fn_decorator(layer, call_and_return_conditional_losses)\r\n    559 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in call(self, inputs, training, mask)\r\n    706     return self._run_internal_graph(\r\n    707         inputs, training=training, mask=mask,\r\n--> 708         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n    709 \r\n    710   def compute_output_shape(self, input_shape):\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask, convert_kwargs_to_constants)\r\n    858 \r\n    859           # Compute outputs.\r\n--> 860           output_tensors = layer(computed_tensors, **kwargs)\r\n    861 \r\n    862           # Update tensor_dict.\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    845                     outputs = base_layer_utils.mark_as_return(outputs, acd)\r\n    846                 else:\r\n--> 847                   outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    848 \r\n    849             except errors.OperatorNotAllowedInGraphError as e:\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in return_outputs_and_add_losses(*args, **kwargs)\r\n     55     inputs = args[inputs_arg_index]\r\n     56     args = args[inputs_arg_index + 1:]\r\n---> 57     outputs, losses = fn(inputs, *args, **kwargs)\r\n     58     layer.add_loss(losses, inputs)\r\n     59     return outputs\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in __call__(self, *args, **kwargs)\r\n    530   def __call__(self, *args, **kwargs):\r\n    531     if not self.call_collection.tracing:\r\n--> 532       self.call_collection.add_trace(*args, **kwargs)\r\n    533     return super(LayerCall, self).__call__(*args, **kwargs)\r\n    534 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in add_trace(self, *args, **kwargs)\r\n    414         trace_with_training(False)\r\n    415       else:\r\n--> 416         fn.get_concrete_function(*args, **kwargs)\r\n    417     self.tracing = False\r\n    418 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in get_concrete_function(self, *args, **kwargs)\r\n    536     if not self.call_collection.tracing:\r\n    537       self.call_collection.add_trace(*args, **kwargs)\r\n--> 538     return super(LayerCall, self).get_concrete_function(*args, **kwargs)\r\n    539 \r\n    540 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)\r\n    774       if self._stateful_fn is None:\r\n    775         initializer_map = object_identity.ObjectIdentityDictionary()\r\n--> 776         self._initialize(args, kwargs, add_initializers_to=initializer_map)\r\n    777         self._initialize_uninitialized_variables(initializer_map)\r\n    778 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    406     self._concrete_stateful_fn = (\r\n    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 408             *args, **kwds))\r\n    409 \r\n    410     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1846     if self.input_signature:\r\n   1847       args, kwargs = None, None\r\n-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1849     return graph_function\r\n   1850 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in wrapper(*args, **kwargs)\r\n    513         layer, inputs=inputs, build_graph=False, training=training,\r\n    514         saving=True):\r\n--> 515       ret = method(*args, **kwargs)\r\n    516     _restore_layer_losses(original_losses)\r\n    517     return ret\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in call_and_return_conditional_losses(inputs, *args, **kwargs)\r\n    555   layer_call = _get_layer_call_method(layer)\r\n    556   def call_and_return_conditional_losses(inputs, *args, **kwargs):\r\n--> 557     return layer_call(inputs, *args, **kwargs), layer.get_losses_for(inputs)\r\n    558   return _create_call_fn_decorator(layer, call_and_return_conditional_losses)\r\n    559 \r\n\r\n<ipython-input-5-d4972c5cbebe> in call(self, inputs)\r\n      4     def call(self, inputs):\r\n      5         x, emb = inputs\r\n----> 6         return Concatenate(axis=1)([x, emb])\r\n      7 \r\n      8     def compute_output_shape(self, input_shape):\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    815           # Build layer if applicable (if the `build` method has been\r\n    816           # overridden).\r\n--> 817           self._maybe_build(inputs)\r\n    818           cast_inputs = self._maybe_cast_inputs(inputs)\r\n    819 \r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in _maybe_build(self, inputs)\r\n   2139         # operations.\r\n   2140         with tf_utils.maybe_init_scope(self):\r\n-> 2141           self.build(input_shapes)\r\n   2142       # We must set self.built since user defined build functions are not\r\n   2143       # constrained to set self.built.\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py in wrapper(instance, input_shape)\r\n    304     if input_shape is not None:\r\n    305       input_shape = convert_shapes(input_shape, to_tuples=True)\r\n--> 306     output_shape = fn(instance, input_shape)\r\n    307     # Return shapes from `fn` as TensorShapes.\r\n    308     if output_shape is not None:\r\n\r\n~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/merge.py in build(self, input_shape)\r\n    389                        'inputs with matching shapes '\r\n    390                        'except for the concat axis. '\r\n--> 391                        'Got inputs shapes: %s' % (input_shape))\r\n    392 \r\n    393   def _merge_function(self, inputs):\r\n\r\nValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 16, None, None), (None, 512, 6, 18)]", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/772b59094fe6d546ef2b36f394575df3/untitled200.ipynb) Thanks!", "@malyasova \r\nI ran your code in [nightly](https://colab.sandbox.google.com/gist/Saduf2019/50a684ea2d9918014bc4014331a55960/untitled199.ipynb) and do not face the value error as faced in 2.0.\r\n", "For anyone like me who doesn't want to upgrade cuda and tensorflow to the latest version, there's also a quick fix: add a Reshape layer after the Conv2DTranspose. Then keras can determine the shape.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39921\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39921\">No</a>\n"]}, {"number": 39920, "title": "    TfLiteGpuDelegate Init: Tensor \"model/tf_op_layer_Reshape/Reshape\" has bad input dims size: 5.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Android**\r\n- TensorFlow version (use command below): **org.tensorflow:tensorflow-lite:0.0.0-nightly / org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly**\r\n- GPU model and memory: **Hawaii Nova3 built in GPU** \r\n\r\nI am Trying to the run this example https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android with Yolov4 converted model (.weight -> .tflite). It worked fine on the CPU, however when enabling the **GPU** and the following error happens:\r\n\r\n     **Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Following operations are not supported by GPU delegate:\r\n    EXP: Operation is not supported.\r\n    SPLIT_V: Operation is not supported.\r\n    88 operations will run on the GPU, and the remaining 477 operations will run on the CPU.\r\n    TfLiteGpuDelegate Init: Tensor \"model/tf_op_layer_Reshape/Reshape\" has bad input dims size: 5.\r\n    TfLiteGpuDelegate Prepare: delegate is not initialized\r\n    Node number 565 (TfLiteGpuDelegateV2) failed to prepare.\r\n    \r\n    Restored original execution plan after delegat\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:347)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:82)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:237)\r\n        at org.tensorflow.lite.examples.detection.tflite.YoloV4Classifier.create(YoloV4Classifier.java:115)\r\n        \t... 17 more**\r\n\r\nI think this is the model part causeing this error\r\n![image](https://user-images.githubusercontent.com/4397109/83063715-3fc35e80-a069-11ea-9965-d948804a98ea.png)\r\n\r\nAny idea how i can overcome this issue? \r\n", "comments": ["The problem is resolved by using tensorflow/tensorflow-gpu **v2.1** instead of **nightly** ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39920\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39920\">No</a>\n"]}, {"number": 39919, "title": "test", "body": "", "comments": []}, {"number": 39918, "title": "Tflite.tensorflow  Cannot convert a Tensor of dtype resource to a NumPy array. after multi mirrored strategy training", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or binary): Tnsorflow 2.2-gpu docker\r\n- TensorFlow version (or github SHA if from source): tensorflow/tensorflow:2.2.0-gpu\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nhttps://github.com/uidchet/tflite-issue\r\n\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n  ```\r\n  File \"tflite.py\", line 22, in convert_model\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 459, in convert\r\n    self._funcs[0], lower_control_flow=False))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py\", line 706, in convert_variables_to_constants_v2_as_graph\r\n    func, lower_control_flow, aggressive_inlining)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py\", line 457, in _convert_variables_to_constants_v2_impl\r\n    tensor_data = _get_tensor_data(func)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py\", line 217, in _get_tensor_data\r\n    data = val_tensor.numpy()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 961, in numpy\r\n    maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 929, in _numpy\r\n    six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://github.com/uidchet/tflite-issue/blob/master/sav_model.zip\r\n```\r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/3cc83d1f8f00b85b6d707ab67e41e038/39918.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/eca8e32076002fb5c907e6cededf0de9/39918-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@uidchet Looks like there is a mismatch in dtype. Is it possible for you to create a simple standalone code to reproduce the issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Seeing a same error, i doubt it's about type mismatch because when I load the SavedModel into TFServing, it worked well. So it should be something specific to TFLiteConverter", "@jvishnuvardhan i think @uidchet has already provided a SaveModel for you, most likely you just need to run the converter over it.", "Assigning to @karimnosseir since this is related with tflite converter.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Any work around for this?", "well reloading the saved model and the saving it again works...", "@uidchet Is this still an issue? I ran the gist @amahendrakar  without any issue. May be this was resolved in recent `tf-nightly`. Please verify the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/7d0dd3e7a2b5f5a4bd5d89d03f75fc46/39918-tf-nightly.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39917, "title": "test", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39917) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 39916, "title": "segment only person", "body": "resized_im, seg_map = MODEL.run(orignal_im)\r\n\r\nseg_map has multiple categories,Including person.\r\nI want get only person.What should i do\uff1f", "comments": []}, {"number": 39915, "title": "[INTEL MKL] Fixing matmul for DNNL0.x", "body": "Recently we submittted a PR to remove all cblas dependencies in matmul op. However, internally it was decided to keep it around for DNNL0.x builds till TF2.3 release fork. So enabling it back  for just DNNL0.x build.", "comments": []}, {"number": 39914, "title": "[ROCm] Fix for ROCm CSB breakage - 200527", "body": "The following commit introduces a new unit-test which fails on ROCm.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/dbef0933ebe4d3d85be73e88cfe5f83cac0ae1d6\r\n\r\nI think that this unit-test is for checking the reduced memory usage of the gradient checkpointing method.\r\n\r\nThe sub-test `test_does_not_raise_oom_exception` fails on ROCm, because on the ROCm platform the scratch space required for doing backward convolution pushes the total memory allocation just beyond the 1GB limit imposed by the testcase.\r\n\r\nThis fix moves up the threshold by 128MB (from 1024 MB to 1152 MB). This still presevers the intent of the unit-test, i.e. the `test_raises_oom_exception` continues to raise the exception, while also allowing the `test_does_not_raise_oom_exception` sub-test to pass on the ROCm platform.\r\n\r\n-----------------------------------\r\n\r\n/cc @cheshire @chsigg @nvining-work @parallelo ", "comments": []}, {"number": 39913, "title": "[TF 2.2.0/TPU]: tf.data.Dataset segmentation fault with \"the Encode() method is not implemented for DatasetVariantWrapper objects\" after calling TPUCusterResolver() ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (use command below):\r\n- Python version: v2.2.0-0-g2b96f3662b 2.2.0\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n- TPU on Colab\r\n\r\n**Describe the current behavior**\r\nCode is crasing wgen using TPU (working fine with CPU and GPU). Exact same crashes on GCP AI Training and Colab (using TPU). After I use in my code: `tf.distribute.cluster_resolver.TPUClusterResolver()` any action of my tf.data.Dataset (from TFRecord file) like `valid_dataset.take(5)` will cash with the following error message:\r\n\r\n```\r\nINFO:tensorflow:Finished initializing TPU system.\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\n here start the issues!\r\n2020-05-27 16:27:47.913455: I tensorflow/core/common_runtime/eager/execute.cc:966] Executing op TakeDataset on task /job:worker/replica:0/task:0/device:CPU:0\r\n2020-05-27 16:27:47.913542: E tensorflow/core/framework/dataset.cc:88] The Encode() method is not implemented for DatasetVariantWrapper objects.\r\nFatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x00007f67f1a67780 (most recent call first):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 6007 in take_dataset\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3640 in __init__\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1267 in take\r\n  File \"test.py\", line 70 in main\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299 in run\r\n  File \"test.py\", line 78 in <module>\r\n```\r\n\r\nBefore using `TPUCusterResolver()` I can access my data without any issue like `valid_dataset.take(5)`\r\n\r\nSome information:\r\n- On GCP it is always crashing\r\n- On Colab, if I copy the full code in one cell, it working most of the time \r\n- On Colab, running the `!python test.py` from a cell is always cashing\r\n\r\n**Describe the expected behavior**\r\nShould run without crashing as before calling `TPUCusterResolver()` and the output should like that:\r\n```\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\n here start the issues!\r\nExecuting op TakeDataset on task /job:worker/replica:0/task:0/device:CPU:0\r\n take(5) not ok 1 <TakeDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>\r\nExecuting op TakeDataset on task /job:worker/replica:0/task:0/device:CPU:0\r\n take(5) not ok 2 <TakeDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n- Go on Colab\r\n- Choose select TPU in the runtime\r\n- check that TF 2.2.0 is installed (is the default right now)\r\n- normally no other packages are needed and the data are on a public GCP bucket\r\n- In Colab create a python file called test.py and copy the full code below\r\n- In a Colab cell run `!python test.py` --> this will seg fault\r\n- In another Colab cell, copy the code below. It should run without any error.\r\n\r\n\r\n```python\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\r\nos.environ['TF_CPP_MIN_VLOG_LEVEL'] = '0'\r\nimport tensorflow as tf\r\ntf.get_logger().propagate = False\r\ntf.debugging.set_log_device_placement(True)\r\ntf.autograph.set_verbosity(5, alsologtostdout=False)\r\nimport model.tf_bert_classification.model as tf_bert\r\nfrom absl import flags\r\nfrom absl import app\r\nimport sys\r\n\r\nuse_tpu=True\r\nprint(\"Tensorflow version\", tf.__version__)\r\n\r\n#FLAGS = flags.FLAGS\r\n#flags.DEFINE_string('f', '', 'kernel') # just for jupyter notebook and avoid : \"UnrecognizedFlagError: Unknown command line flag 'f'\"\r\n\r\ndef main(argv):\r\n#def main():\r\n  def parse_tfrecord_glue_files(record):\r\n      print(\" ---> parse_tfrecord_glue_files\")\r\n      features_spec = {\r\n          'input_ids': tf.io.FixedLenFeature([], tf.string, default_value=''),\r\n          'attention_mask': tf.io.FixedLenFeature([], tf.string, default_value=''),\r\n          'token_type_ids': tf.io.FixedLenFeature([], tf.string, default_value=''),\r\n          'label': tf.io.FixedLenFeature([], tf.int64, default_value=0)\r\n      }\r\n      example = tf.io.parse_single_example(record, features_spec)\r\n      f0 = tf.ensure_shape(tf.io.parse_tensor(example['input_ids'], out_type=tf.int32), (None,))\r\n      f1 = tf.ensure_shape(tf.io.parse_tensor(example['attention_mask'], out_type=tf.int32), (None,))\r\n      f2 = tf.ensure_shape(tf.io.parse_tensor(example['token_type_ids'], out_type=tf.int32), (None,))\r\n      return {'input_ids': f0, 'attention_mask': f1, 'token_type_ids': f2}, example['label']\r\n\r\n  def build_dataset(input_tfrecords, batch_size, shuffle_buffer=2048):\r\n      print(\" ---> build_dataset\")\r\n      file_pattern = input_tfrecords+'/*.tfrecord'\r\n      dataset = tf.data.Dataset.list_files(file_pattern,\r\n                                          shuffle=True,\r\n                                          seed=None\r\n                                          )\r\n      dataset = dataset.interleave(tf.data.TFRecordDataset,\r\n                                  cycle_length=tf.data.experimental.AUTOTUNE,\r\n                                  num_parallel_calls=tf.data.experimental.AUTOTUNE,\r\n                                  deterministic=False)\r\n      dataset = dataset.shuffle(shuffle_buffer)\r\n      dataset = dataset.map(parse_tfrecord_glue_files, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n      dataset = dataset.batch(batch_size)\r\n      dataset = dataset.cache()\r\n      dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n      return dataset\r\n\r\n  valid_dataset = build_dataset('gs://public-test-data-gs/valid', 64, 2048)\r\n  valid_dataset = valid_dataset.repeat(2)\r\n\r\n  print(\" take(5)ok\", valid_dataset.take(5))\r\n\r\n  if use_tpu:\r\n    print('setting up TPU: cluster resolver')\r\n    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n    print('setting up TPU: \\n {}'.format(tpu_cluster_resolver))\r\n    print('running on TPU: \\n {}'.format(tpu_cluster_resolver.cluster_spec().as_dict()['worker'])) \r\n    tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)\r\n    tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\r\n    strategy = tf.distribute.experimental.TPUStrategy(tpu_cluster_resolver)\r\n  else:\r\n     strategy = tf.distribute.MirroredStrategy()\r\n  \r\n  print(\" here start the issues!\")\r\n  print(\" take(5) not ok 1\", valid_dataset.take(5))\r\n\r\n  with strategy.scope():\r\n    print(\" take(5) not ok 2\", valid_dataset.take(5))\r\n\r\n#main()\r\n#main(sys.argv)\r\nif __name__ == '__main__':\r\n  app.run(main)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\n", "comments": ["As per [TPU documentation](https://www.tensorflow.org/guide/tpu#tpu_initialization), the TPU initialization needs to happen before any TensorFlow operations are executed.\r\n\r\nSo you will need to do the following:\r\n\r\n```\r\n...\r\n  if use_tpu:\r\n    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n    tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)\r\n    tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\r\n\r\n  valid_dataset = build_dataset('gs://public-test-data-gs/valid', 64, 2048)\r\n  valid_dataset = valid_dataset.repeat(2)\r\n\r\n  if use_tpu:\r\n    strategy = tf.distribute.experimental.TPUStrategy(tpu_cluster_resolver)\r\n  else:\r\n     strategy = tf.distribute.MirroredStrategy()\r\n...\r\n```\r\n", "@jsimsa you are right, this solve this issue. I missed this in the documentation. It was clear that the model need to be created within the strategy scope but not what you mentioned. Thanks a lot. Closing", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39913\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39913\">No</a>\n", "I also missed it. :) Maybe it could be more explicit in the docs that the TPU cluster must be initialized before the dataset is created?", "@rxsang FYI"]}, {"number": 39912, "title": "//tensorflow/python/compiler/xla:jit_test fails on s390x and need to add support for llvm", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.2.0\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source):2.0.0\r\n- GCC/Compiler version (if compiling from source):gcc (Ubuntu 9.3.0-10ubuntu2) 9.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI'm building Tensorflow v2.2.0 on s390x (ibm z architecture). When running the test case //tensorflow/python/compiler/xla:jit_test, I got the following error message:\r\n\r\nRunning tests under Python 3.8.2: /usr/bin/python\r\n[ RUN      ] CompilationEnabledInGradientTest.testCompilationGradientScopeNames_function\r\n2020-05-20 20:23:54.560838: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1555500000 Hz\r\n2020-05-20 20:23:54.561101: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1fce270 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-20 20:23:54.561105: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n/home/sidong/.cache/bazel/_bazel_sidong/338a466d2403fbfe3413e7ca6003e4cf/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/compiler/xla/jit_test.runfiles/org_tensorflow/tensorflow/python/framework/indexed_slices.py:349: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\r\n  if not isinstance(values, collections.Sequence):\r\n[       OK ] CompilationEnabledInGradientTest.testCompilationGradientScopeNames_function\r\n[ RUN      ] CompilationEnabledInGradientTest.testCompilationGradientScopeNames_v1_graph\r\n[       OK ] CompilationEnabledInGradientTest.testCompilationGradientScopeNames_v1_graph\r\n[ RUN      ] CompilationEnabledInGradientTest.testCompilationInGradient_function\r\n'z14' is not a recognized processor for this target (ignoring processor)\r\n'z14' is not a recognized processor for this target (ignoring processor)\r\n'z14' is not a recognized processor for this target (ignoring processor)\r\n'z14' is not a recognized processor for this target (ignoring processor)\r\n2020-05-20 20:23:54.964299: F tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:252] Check failed: module->getDataLayout().isLittleEndian() == tensorflow::port::kLittleEndian (1 vs. 0)\r\nFatal Python error: Aborted\r\n\r\nAlso, similar error messages were also observed in other xla related test cases. Please check below for the code to reproduce the error.\r\n\r\n**Describe the expected behavior**\r\nmodule->getDataLayout().isLittleEndian() should return 0.\r\ntest case should pass.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.ops import disable_eager_execution\r\n\r\ndisable_eager_execution()\r\nsess = tf.compat.v1.Session()\r\nwith sess.as_default():\r\n  jit_scope = tf.python.compiler.xla.jit.experimental_jit_scope\r\n  x = tf.constant(3)\r\n  print(x.eval())\r\n  with jit_scope():\r\n    y = tf.constant(5)\r\n  print(x.eval())\r\n  print(y.eval())\r\n```\r\nThe first two evaluation will return 3 and the third evaluation will fail and throws the error on s390x.\r\n\r\n**Other info / logs** \r\n\r\nI dug into this issue and notice that the bug may be caused by llvm configuration. I checked the file `third_party/llvm/llvm.autogenerated.BUILD` and noticed that \"SystemZ\", a target that is supported by llvm, is not listed as a target in this BUILD file. I think this could cause llvm not supporting s390x architecture correctly. Since this is an auto-generated file, I wonder how should I modify it and add support for s390x properly?\r\n", "comments": ["Hi, I have been trying to fix the support issue by modifying the BUILD file directly. The build succeeds and the test case //tensorflow/python/compiler/xla:jit_test passes after applying the patch below:\r\n```\r\ndiff --git a/tensorflow/compiler/aot/BUILD b/tensorflow/compiler/aot/BUILD\r\nindex dfbea9c49e..181fc5f3e5 100644\r\n--- a/tensorflow/compiler/aot/BUILD\r\n+++ b/tensorflow/compiler/aot/BUILD\r\n@@ -64,6 +64,7 @@ cc_library(\r\n         \"@llvm-project//llvm:powerpc_code_gen\",  # fixdeps: keep\r\n         \"@llvm-project//llvm:target\",\r\n         \"@llvm-project//llvm:x86_code_gen\",  # fixdeps: keep\r\n+\t\"@llvm-project//llvm:system_z_code_gen\",  # fixdeps: keep\r\n     ] + if_llvm_aarch64_available([\r\n         \"//third_party/llvm/llvm-project/llvm:aarch64_target\",  # fixdeps: keep\r\n     ]),\r\n@@ -105,6 +106,7 @@ cc_library(\r\n         \"@llvm-project//llvm:powerpc_code_gen\",  # fixdeps: keep\r\n         \"@llvm-project//llvm:target\",\r\n         \"@llvm-project//llvm:x86_code_gen\",  # fixdeps: keep\r\n+\t\"@llvm-project//llvm:system_z_code_gen\",  # fixdeps: keep\r\n     ] + if_llvm_aarch64_available([\r\n         \"//third_party/llvm/llvm-project/llvm:aarch64_target\",  # fixdeps: keep\r\n     ]),\r\ndiff --git a/tensorflow/compiler/aot/compile.cc b/tensorflow/compiler/aot/compile.cc\r\nindex de58c7f8a8..5c2ae10abf 100644\r\n--- a/tensorflow/compiler/aot/compile.cc\r\n+++ b/tensorflow/compiler/aot/compile.cc\r\n@@ -160,6 +160,10 @@ static void InitializeTargets() {\r\n   LLVMInitializePowerPCTargetInfo();\r\n   LLVMInitializePowerPCTargetMC();\r\n   LLVMInitializePowerPCAsmPrinter();\r\n+  LLVMInitializeSystemZTarget();\r\n+  LLVMInitializeSystemZTargetInfo();\r\n+  LLVMInitializeSystemZTargetMC();\r\n+  LLVMInitializeSystemZAsmPrinter();\r\n   LLVMInitializeX86Target();\r\n   LLVMInitializeX86TargetInfo();\r\n   LLVMInitializeX86TargetMC();\r\ndiff --git a/tensorflow/compiler/xla/service/cpu/BUILD b/tensorflow/compiler/xla/service/cpu/BUILD\r\nindex 7e1b8a1e7e..39ce2aae58 100644\r\n--- a/tensorflow/compiler/xla/service/cpu/BUILD\r\n+++ b/tensorflow/compiler/xla/service/cpu/BUILD\r\n@@ -185,6 +185,9 @@ cc_library(\r\n         \"//tensorflow:linux_ppc64le\": [\r\n             \"@llvm-project//llvm:powerpc_code_gen\",  # fixdeps: keep\r\n         ],\r\n+\t\"//tensorflow:linux_s390x\": [\r\n+            \"@llvm-project//llvm:system_z_code_gen\",  # fixdeps: keep\r\n+        ],\r\n         \"//conditions:default\": [\r\n         ],\r\n     }),\r\ndiff --git a/third_party/llvm/llvm.autogenerated.BUILD b/third_party/llvm/llvm.autogenerated.BUILD\r\nindex a89838ebac..ee4097a9fd 100644\r\n--- a/third_party/llvm/llvm.autogenerated.BUILD\r\n+++ b/third_party/llvm/llvm.autogenerated.BUILD\r\n@@ -33,6 +33,7 @@ llvm_targets = [\r\n     \"NVPTX\",\r\n     \"PowerPC\",\r\n     \"X86\",\r\n+    \"SystemZ\",\r\n ]\r\n \r\n llvm_target_asm_parsers = llvm_targets\r\n@@ -538,6 +539,22 @@ llvm_target_list = [\r\n             (\"-gen-disassembler\", \"lib/Target/PowerPC/PPCGenDisassemblerTables.inc\"),\r\n         ],\r\n     },\r\n+    {\r\n+        \"name\": \"SystemZ\",\r\n+        \"lower_name\": \"system_z\",\r\n+        \"short_name\": \"SystemZ\",\r\n+        \"tbl_outs\": [\r\n+            (\"-gen-asm-writer\", \"lib/Target/SystemZ/SystemZGenAsmWriter.inc\"),\r\n+            (\"-gen-asm-matcher\", \"lib/Target/SystemZ/SystemZGenAsmMatcher.inc\"),\r\n+            (\"-gen-emitter\", \"lib/Target/SystemZ/SystemZGenMCCodeEmitter.inc\"),\r\n+            (\"-gen-register-info\", \"lib/Target/SystemZ/SystemZGenRegisterInfo.inc\"),\r\n+            (\"-gen-instr-info\", \"lib/Target/SystemZ/SystemZGenInstrInfo.inc\"),\r\n+            (\"-gen-dag-isel\", \"lib/Target/SystemZ/SystemZGenDAGISel.inc\"),\r\n+            (\"-gen-callingconv\", \"lib/Target/SystemZ/SystemZGenCallingConv.inc\"),\r\n+            (\"-gen-subtarget\", \"lib/Target/SystemZ/SystemZGenSubtargetInfo.inc\"),\r\n+            (\"-gen-disassembler\", \"lib/Target/SystemZ/SystemZGenDisassemblerTables.inc\"),\r\n+        ],\r\n+    },\r\n     {\r\n         \"name\": \"X86\",\r\n         \"lower_name\": \"x86\",\r\n@@ -655,6 +672,7 @@ cc_library(\r\n         \":nvptx_code_gen\",\r\n         \":powerpc_code_gen\",\r\n         \":x86_code_gen\",\r\n+        \":system_z_code_gen\",\r\n     ],\r\n )\r\n \r\n@@ -3859,6 +3877,7 @@ cc_library(\r\n         \":mc\",\r\n         \":support\",\r\n         \":system_z_info\",\r\n+        \":system_z_target_gen\",\r\n     ],\r\n )\r\n \r\n@@ -3892,6 +3911,7 @@ cc_library(\r\n         \"lib/Target/SystemZ/TargetInfo/*.c\",\r\n         \"lib/Target/SystemZ/TargetInfo/*.cpp\",\r\n         \"lib/Target/SystemZ/TargetInfo/*.inc\",\r\n+        \"lib/Target/SystemZ/MCTargetDesc/*.h\",\r\n     ]),\r\n     hdrs = glob([\r\n         \"include/llvm/Target/SystemZ/TargetInfo/*.h\",\r\n@@ -3903,6 +3923,7 @@ cc_library(\r\n     deps = [\r\n         \":config\",\r\n         \":support\",\r\n+        \":system_z_target_gen\",\r\n     ],\r\n )\r\n \r\ndiff --git a/third_party/llvm/llvm.bzl b/third_party/llvm/llvm.bzl\r\nindex f48cf84b0e..b8ecce553f 100644\r\n--- a/third_party/llvm/llvm.bzl\r\n+++ b/third_party/llvm/llvm.bzl\r\n@@ -333,6 +333,14 @@ llvm_all_cmake_vars = select({\r\n             posix_cmake_vars,\r\n         ),\r\n     ),\r\n+    \"@org_tensorflow//tensorflow:linux_s390x\": cmake_var_string(\r\n+        _dict_add(\r\n+            cmake_vars,\r\n+            llvm_target_cmake_vars(\"SystemZ\", \"s390x-unknown-linux_gnu\"),\r\n+            posix_cmake_vars,\r\n+\t    linux_cmake_vars,\r\n+        ),\r\n+    ),\r\n     \"//conditions:default\": cmake_var_string(\r\n         _dict_add(\r\n             cmake_vars,\r\n```\r\nI have tested this patch on Ubuntu 20.04 with s390x architecture, and I wonder will this be a legit way to add llvm support for a new architecture? Thanks.\r\n\r\nSidong", "It has been fixed with the PR.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39912\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39912\">No</a>\n"]}, {"number": 39911, "title": "in resolution of [Wsign-compare] warning id 1", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39911) for more info**.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39911) for more info**.\n\n<!-- ok -->"]}, {"number": 39910, "title": "TFLite: add EXTRA_CFLAGS variable", "body": "Since commit SHA1: d28cf21aa51d12ce9c526f7baf5137bc2e2b7f7d\r\nCross compilation of TFLite is failing.\r\nAdd an EXTRA_CFLAGS varibale to allow cross compilation environment to\r\ndefine extra CFLAGS when needed.\r\n\r\nSigned-off-by: Vincent ABRIOU <vincent.abriou@st.com>", "comments": ["@vinceab  Can you please fix build failures ? Thanks!", "new patch available that solve the build failure.", "What was the build error by the way? I can't seem to see your previous version.", "It was not really a build issue but a check that was failing. One line was over 80 characters in setup.py.", "Okay. Sounds good."]}, {"number": 39909, "title": "Max and min for dynamic tensors should be recorded during calibration: Failed for tensor Shape", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed using pip, it's actually tf-nightly-gpu\r\n- TensorFlow version: tf-nightly-gpu==2.3.0.dev20200527\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nimport numpy\r\nimport tensorflow as tf\r\n\r\n\r\ndef data_reader(size=900):\r\n    return numpy.ones((size, 32, 86, 1), numpy.uint8) * 255\r\n\r\n\r\ndata = data_reader()\r\ndata = tf.cast(data, tf.float32)\r\nrepr_ds = tf.data.Dataset.from_tensor_slices((data)).batch(1)\r\n\r\n\r\ndef representative_data_gen():\r\n    for input_value in repr_ds.take(100):\r\n        yield [input_value]\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"./exported\")\r\nconverter.experimental_new_converter = True\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\ntflite_model = converter.convert()\r\n\r\nopen(\"exported.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n```\r\n**The output from the converter invocation**\r\n```\r\nTraceback (most recent call last):\r\n  File \"...\", line 29, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 920, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 752, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 502, in convert\r\n    constants.FLOAT, False)\r\n  File venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 349, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n  File \"venv/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 93, in calibrate_and_quantize\r\n    np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\nRuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor Shape\r\nEmpty min/max for tensor Shape\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://drive.google.com/file/d/1dSDVWjM2fJbT71r-EiUqkwygtYr0a5aE/view?usp=sharing\r\n```\r\n\r\n**Failure details**\r\nI am trying to convert this model by performing full integer quantization described [here](https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations), but it fails as described above. If I attempt to convert it without a representative dataset it works ok, but the input/output tensors remain float32 and I need int8. Maybe I am missing something, but I'd like to know if this is solvable. Thanks in advance.\r\n", "comments": ["@vladdders \r\nPlease refer to this issue with similar error#39718 and let us know if it helps. ", "@Saduf2019 \r\nI have tried with tensorflow 2.2.0, but unfortunately I get the same error :/", "I think it's due to some latest commit. Since I also encounter this error with the nightly version, but the same code works with stable version.", "It doesn't work for me. Did you try my code by any chance and got it to work?", "@vladdders\r\nCan you please try it on version as suggested by xchani and share a colab gist in case you face any issues.", "@Saduf2019 I have tried what @xchani suggested and sadly the same issue occurs. I have installed the stable version `2.2.0` as mentioned previously. Please find the gist [here](https://colab.research.google.com/drive/1yIx_J3NfIlDpaJw9XMgXpIT4XY69Y6nJ?usp=sharing).", "I am able to replicate this issue, please find [gist here](https://colab.sandbox.google.com/gist/Saduf2019/ff1cf5b8fbd883b1677cf41088166bc0/untitled211.ipynb). Thanks!", "@vladdders Can you please try the solution mentioned [here](https://github.com/tensorflow/tensorflow/issues/39718#issuecomment-638386789) and let us know whether it worked for you. Thanks!", "hey @jvishnuvardhan thanks for your answer. I have attempted the conversion as suggested on that answer but unfortunately I am still getting the same error. I updated the [gist](https://colab.research.google.com/drive/1yIx_J3NfIlDpaJw9XMgXpIT4XY69Y6nJ?usp=sharing#scrollTo=lgWmPjJjy9e-) so you can see it as well.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "It seems that there was a `tf.tile` operation in my graph that cannot be quantized and probably was caught by this min/max exception. The correct exception seems to have been fixed in `tf-nightly-gpu==2.3.0.dev20200615`. Thanks for your support though!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39909\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39909\">No</a>\n", "I have the same issue...\r\nI saw here that installing tf-nightly-gpu==2.3.0.dev2020061 can solve the issue,but I'm unable to install a specific version of tf-nightly-gpu using pip,it always installs the latest version.\r\nHow can tf-nightly-gpu==2.3.0.dev2020061 be installed?", "@sandeep22-v Can you please open a new issue with a simple standalone code to reproduce the issue? Thanks!", "> @sandeep22-v Can you please open a new issue with a simple standalone code to reproduce the issue? Thanks!\r\n\r\nSure sir..will do it now"]}, {"number": 39908, "title": "Connecting to invalid output 163 of source node GRU_1/while which has 163 outputs..", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): VERSION: 2.2.0; GIT_VERSION: v2.2.0-rc4-8-g2b96f3662b\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5\r\n- GPU model and memory: GTX 1080 8Gb; 16 Gb RAM\r\n\r\n**Describe the current behavior**\r\nMy code works great on 2.1.1 but not works at 2.2.0. (**Error log \u21161 below**)\r\nEmpirically found that the problem appears if a dropout or recurrent_dropout is used in GRU layers.\r\nTried to change GRU to LSTM also, - same problem.\r\nI tried to use tf.compat.v1.experimental.output_all_intermediates() True and False - has no effect.\r\nAt 2.2.0 it works ONLY if I remove dropout and reccurent_dropout options from GRU layers AND disable eager_execution with tf.compat.v1.disable_eager_execution() command.\r\nBut if I remove dropouts and eager is enabled - I have another error (**Error log \u21162 below**)\r\n\r\n**Standalone code to reproduce the issue**  \r\nTest case with this problem:\r\nhttps://colab.research.google.com/drive/1HUayaLsHNZ30JaBlxvLyQz7Evf1FnsD5?usp=sharing\r\n\r\n**Other info / logs** \r\n**Error log \u21161:**\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n14 frames\r\nInvalidArgumentError: Node 'training/SGD/gradients/gradients/GRU_1/while_grad/GRU_1/while_grad': Connecting to invalid output 163 of source node GRU_1/while which has 163 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nInvalidArgumentError: Node 'training/SGD/gradients/gradients/GRU_1/while_grad/GRU_1/while_grad': Connecting to invalid output 163 of source node GRU_1/while which has 163 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).\r\n\r\n```\r\n\r\n**Error log \u21162:**\r\n\r\n```\r\ntf.keras.utils.plot_model(model)\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-14-8c47125ededc> in <module>()\r\n----> 1 tf.keras.utils.plot_model(model)\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/vis_utils.py in model_to_dot(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\r\n    141 \r\n    142     # Append a wrapped layer's label to node's label, if it exists.\r\n--> 143     layer_name = layer.name\r\n    144     class_name = layer.__class__.__name__\r\n    145 \r\n\r\nAttributeError: 'dict' object has no attribute 'name'\r\n```\r\n\r\n```\r\nmodel.fit(parsed_alldata_dataset, steps_per_epoch=1000, epochs=100)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-15-1d2f84f55c4b> in <module>()\r\n----> 1 model.fit(parsed_alldata_dataset, steps_per_epoch=1000, epochs=100)\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nAttributeError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:543 train_step  **\r\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:391 update_state\r\n        self._build(y_pred, y_true)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:322 _build\r\n        self._metrics, y_true, y_pred)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1118 map_structure_up_to\r\n        **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1214 map_structure_with_tuple_paths_up_to\r\n        *flat_value_lists)]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1213 <listcomp>\r\n        results = [func(*args, **kwargs) for args in zip(flat_path_list,\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1116 <lambda>\r\n        lambda _, *values: func(*values),  # Discards the path arg.\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:421 _get_metric_objects\r\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:421 <listcomp>\r\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:442 _get_metric_object\r\n        y_t_rank = len(y_t.shape.as_list())\r\n\r\n    AttributeError: 'NoneType' object has no attribute 'shape'\r\n\r\n```\r\n", "comments": ["I tried in colab with TF 2.1.0 and i am not seeing any issue.However i am able to reproduce the issue with TF 2.2.0, nightly version (2.3.0-dev20200527).Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/477ee58cb1a6f299e9f5c6e838f80b09/untitled931.ipynb).Thanks!", "> I tried in colab with TF 2.1.0 and i am not seeing any issue.However i am able to reproduce the issue with TF 2.2.0, nightly version (2.3.0-dev20200527).Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/477ee58cb1a6f299e9f5c6e838f80b09/untitled931.ipynb).Thanks!\r\n\r\nI was talking about this. the bottom line is that in the new \"stable\" version 2.2.0, the code does not work. This is either a bug or some kind of change that I do not understand\r\n\r\nI want to use new version 2.2.0 in all my work's codes, but this one don't want to work. So i reported this as a bug", "Thanks for reporting the issue.\r\n\r\nI think there are several issues we need to address here:\r\n\r\n1. The model is built with 2 inputs and 4 outputs. However, the training data only have 2 inputs and 2 outputs. This is causing the issue for Error log \u21162. After I removed 2 extra output when building the model, the model.fit() still failed with. I need to check with runtime team and see what's the root cause there.\r\n\r\n```\r\n2020-05-31 22:05:45.953509: W tensorflow/core/framework/op_kernel.cc:1760] OP_REQUIRES failed at variable_ops.cc:100 : Already exists: Resource __per_step_0/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/functional_1/BGRU_0/forward_GRU_0/while_grad/body/_347/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/gradients/AddN_7/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\nTraceback (most recent call last):\r\n  File \"/Users/scottzhu/Library/Preferences/PyCharmCE2018.1/scratches/scratch_15.py\", line 73, in <module>\r\n    model.fit(parsed_alldata_dataset, steps_per_epoch=1000, epochs=100)\r\n  File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1090, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 766, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 826, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2812, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1838, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n  File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1915, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 549, in call\r\n    ctx=ctx)\r\n  File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError:  Resource __per_step_0/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/functional_1/BGRU_0/forward_GRU_0/while_grad/body/_347/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/gradients/AddN_8/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n\t [[{{node gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/functional_1/BGRU_0/forward_GRU_0/while_grad/body/_347/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/gradients/AddN_8/tmp_var}}]] [Op:__inference_train_function_7797]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n\r\n2. I can also confirm that Error log \u21161 when tf.compat.v1.disable_eager_execution() is added with dropout and recurrent_dropout. It is probably related to runtime and how gradient is generated, which I need to confirm with runtime team again.", "Btw, I think the issue is probably related to https://github.com/tensorflow/tensorflow/issues/38906, which we have same finding for tensorflow.python.framework.errors_impl.AlreadyExistsError.\r\n\r\nBtw, disable_eager_execution() will probably cause some side effects in our code base, since it will fallback to some legacy behavior, which might not be recommended for current user. Do you really need eager mode turned off? or you are just trying that to see if it can walk around the issue?", "> Thanks for reporting the issue.\r\n> \r\n> I think there are several issues we need to address here:\r\n> \r\n> 1. The model is built with 2 inputs and 4 outputs. However, the training data only have 2 inputs and 2 outputs. This is causing the issue for Error log \u21162. After I removed 2 extra output when building the model, the model.fit() still failed with. I need to check with runtime team and see what's the root cause there.\r\n> \r\n> ```\r\n> 2020-05-31 22:05:45.953509: W tensorflow/core/framework/op_kernel.cc:1760] OP_REQUIRES failed at variable_ops.cc:100 : Already exists: Resource __per_step_0/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/functional_1/BGRU_0/forward_GRU_0/while_grad/body/_347/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/gradients/AddN_7/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n> Traceback (most recent call last):\r\n>   File \"/Users/scottzhu/Library/Preferences/PyCharmCE2018.1/scratches/scratch_15.py\", line 73, in <module>\r\n>     model.fit(parsed_alldata_dataset, steps_per_epoch=1000, epochs=100)\r\n>   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n>     return method(self, *args, **kwargs)\r\n>   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1090, in fit\r\n>     tmp_logs = train_function(iterator)\r\n>   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 766, in __call__\r\n>     result = self._call(*args, **kwds)\r\n>   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 826, in _call\r\n>     return self._stateless_fn(*args, **kwds)\r\n>   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2812, in __call__\r\n>     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n>   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1838, in _filtered_call\r\n>     cancellation_manager=cancellation_manager)\r\n>   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1915, in _call_flat\r\n>     ctx, args, cancellation_manager=cancellation_manager))\r\n>   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 549, in call\r\n>     ctx=ctx)\r\n>   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n>     inputs, attrs, num_outputs)\r\n> tensorflow.python.framework.errors_impl.AlreadyExistsError:  Resource __per_step_0/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/functional_1/BGRU_0/forward_GRU_0/while_grad/body/_347/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/gradients/AddN_8/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n> \t [[{{node gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/functional_1/BGRU_0/forward_GRU_0/while_grad/body/_347/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/gradients/AddN_8/tmp_var}}]] [Op:__inference_train_function_7797]\r\n> \r\n> Function call stack:\r\n> train_function\r\n> ```\r\n> \r\n> 1. I can also confirm that Error log \u21161 when tf.compat.v1.disable_eager_execution() is added with dropout and recurrent_dropout. It is probably related to runtime and how gradient is generated, which I need to confirm with runtime team again.\r\n\r\n1. It's OK for model to have more outputs than labels. In my case it's just two extra outputs for two previous classification outputs with simple argmax. So when I will use this trained model in production, argmax values will be counted as model output for me. Very useful.\r\nAnd when I start this training at TF 2.1.1 it has only one gentle warning:\r\n\r\n```\r\nWARNING:tensorflow:Output y_before_argmaxed missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to y_before_argmaxed.\r\nWARNING:tensorflow:Output y_after_argmaxed missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to y_after_argmaxed.\r\n```", "> Btw, I think the issue is probably related to #38906, which we have same finding for tensorflow.python.framework.errors_impl.AlreadyExistsError.\r\n> \r\n> Btw, disable_eager_execution() will probably cause some side effects in our code base, since it will fallback to some legacy behavior, which might not be recommended for current user. Do you really need eager mode turned off? or you are just trying that to see if it can walk around the issue?\r\n\r\nNo, I don't need eager mode off, I just have tried different setups for my code to work, but unsuccessful", "> > Thanks for reporting the issue.\r\n> > I think there are several issues we need to address here:\r\n> > \r\n> > 1. The model is built with 2 inputs and 4 outputs. However, the training data only have 2 inputs and 2 outputs. This is causing the issue for Error log \u21162. After I removed 2 extra output when building the model, the model.fit() still failed with. I need to check with runtime team and see what's the root cause there.\r\n> > \r\n> > ```\r\n> > 2020-05-31 22:05:45.953509: W tensorflow/core/framework/op_kernel.cc:1760] OP_REQUIRES failed at variable_ops.cc:100 : Already exists: Resource __per_step_0/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/functional_1/BGRU_0/forward_GRU_0/while_grad/body/_347/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/gradients/AddN_7/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n> > Traceback (most recent call last):\r\n> >   File \"/Users/scottzhu/Library/Preferences/PyCharmCE2018.1/scratches/scratch_15.py\", line 73, in <module>\r\n> >     model.fit(parsed_alldata_dataset, steps_per_epoch=1000, epochs=100)\r\n> >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n> >     return method(self, *args, **kwargs)\r\n> >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1090, in fit\r\n> >     tmp_logs = train_function(iterator)\r\n> >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 766, in __call__\r\n> >     result = self._call(*args, **kwds)\r\n> >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 826, in _call\r\n> >     return self._stateless_fn(*args, **kwds)\r\n> >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2812, in __call__\r\n> >     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n> >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1838, in _filtered_call\r\n> >     cancellation_manager=cancellation_manager)\r\n> >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1915, in _call_flat\r\n> >     ctx, args, cancellation_manager=cancellation_manager))\r\n> >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 549, in call\r\n> >     ctx=ctx)\r\n> >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n> >     inputs, attrs, num_outputs)\r\n> > tensorflow.python.framework.errors_impl.AlreadyExistsError:  Resource __per_step_0/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/functional_1/BGRU_0/forward_GRU_0/while_grad/body/_347/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/gradients/AddN_8/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n> > \t [[{{node gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/functional_1/BGRU_0/forward_GRU_0/while_grad/body/_347/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/gradients/AddN_8/tmp_var}}]] [Op:__inference_train_function_7797]\r\n> > \r\n> > Function call stack:\r\n> > train_function\r\n> > ```\r\n> > \r\n> > \r\n> > \r\n> > 1. I can also confirm that Error log \u21161 when tf.compat.v1.disable_eager_execution() is added with dropout and recurrent_dropout. It is probably related to runtime and how gradient is generated, which I need to confirm with runtime team again.\r\n> \r\n> 1. It's OK for model to have more outputs than labels. In my case it's just two extra outputs for two previous classification outputs with simple argmax. So when I will use this trained model in production, argmax values will be counted as model output for me. Very useful.\r\n>    And when I start this training at TF 2.1.1 it has only one gentle warning:\r\n> \r\n> ```\r\n> WARNING:tensorflow:Output y_before_argmaxed missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to y_before_argmaxed.\r\n> WARNING:tensorflow:Output y_after_argmaxed missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to y_after_argmaxed.\r\n> ```\r\n\r\nOk. I guess this might be regression since we refactor the training logic a bit between 2.1 and 2.2. Currently the code is expecting each output of the model should have a matching label.\r\n\r\n@omalleyt12 ", "> > > Thanks for reporting the issue.\r\n> > > I think there are several issues we need to address here:\r\n> > > \r\n> > > 1. The model is built with 2 inputs and 4 outputs. However, the training data only have 2 inputs and 2 outputs. This is causing the issue for Error log \u21162. After I removed 2 extra output when building the model, the model.fit() still failed with. I need to check with runtime team and see what's the root cause there.\r\n> > > \r\n> > > ```\r\n> > > 2020-05-31 22:05:45.953509: W tensorflow/core/framework/op_kernel.cc:1760] OP_REQUIRES failed at variable_ops.cc:100 : Already exists: Resource __per_step_0/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/functional_1/BGRU_0/forward_GRU_0/while_grad/body/_347/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/gradients/AddN_7/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n> > > Traceback (most recent call last):\r\n> > >   File \"/Users/scottzhu/Library/Preferences/PyCharmCE2018.1/scratches/scratch_15.py\", line 73, in <module>\r\n> > >     model.fit(parsed_alldata_dataset, steps_per_epoch=1000, epochs=100)\r\n> > >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n> > >     return method(self, *args, **kwargs)\r\n> > >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1090, in fit\r\n> > >     tmp_logs = train_function(iterator)\r\n> > >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 766, in __call__\r\n> > >     result = self._call(*args, **kwds)\r\n> > >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 826, in _call\r\n> > >     return self._stateless_fn(*args, **kwds)\r\n> > >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2812, in __call__\r\n> > >     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n> > >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1838, in _filtered_call\r\n> > >     cancellation_manager=cancellation_manager)\r\n> > >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1915, in _call_flat\r\n> > >     ctx, args, cancellation_manager=cancellation_manager))\r\n> > >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 549, in call\r\n> > >     ctx=ctx)\r\n> > >   File \"/Users/scottzhu/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n> > >     inputs, attrs, num_outputs)\r\n> > > tensorflow.python.framework.errors_impl.AlreadyExistsError:  Resource __per_step_0/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/functional_1/BGRU_0/forward_GRU_0/while_grad/body/_347/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/gradients/AddN_8/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n> > > \t [[{{node gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/functional_1/BGRU_0/forward_GRU_0/while_grad/body/_347/gradient_tape/functional_1/BGRU_0/forward_GRU_0/while/gradients/AddN_8/tmp_var}}]] [Op:__inference_train_function_7797]\r\n> > > \r\n> > > Function call stack:\r\n> > > train_function\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > \r\n> > > 1. I can also confirm that Error log \u21161 when tf.compat.v1.disable_eager_execution() is added with dropout and recurrent_dropout. It is probably related to runtime and how gradient is generated, which I need to confirm with runtime team again.\r\n> > \r\n> > \r\n> > \r\n> > 1. It's OK for model to have more outputs than labels. In my case it's just two extra outputs for two previous classification outputs with simple argmax. So when I will use this trained model in production, argmax values will be counted as model output for me. Very useful.\r\n> >    And when I start this training at TF 2.1.1 it has only one gentle warning:\r\n> > \r\n> > ```\r\n> > WARNING:tensorflow:Output y_before_argmaxed missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to y_before_argmaxed.\r\n> > WARNING:tensorflow:Output y_after_argmaxed missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to y_after_argmaxed.\r\n> > ```\r\n> \r\n> Ok. I guess this might be regression since we refactor the training logic a bit between 2.1 and 2.2. Currently the code is expecting each output of the model should have a matching label.\r\n> \r\n> @omalleyt12\r\n\r\nNow I removed this outputs, and eager mode is enabled, but:\r\n1) tf.keras.utils.plot_model - not working\r\n2) model.fit - not working.\r\n\r\nCode on colab:\r\nhttps://colab.research.google.com/drive/1HUayaLsHNZ30JaBlxvLyQz7Evf1FnsD5?usp=sharing", "The model.fit is failing the same way as https://github.com/tensorflow/tensorflow/issues/38906, and we will send a fix very soon.\r\n\r\n", "Btw https://github.com/tensorflow/tensorflow/commit/80a93674eafc224a45cbe96c65e993e9735634a3 should fix the issue for training. Let me verify it when we have a new nightly PIP.", "I tested you colab with latest nightly, and it is working now. Closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39908\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39908\">No</a>\n", "> I tested you colab with latest nightly, and it is working now. Closing this issue.\r\n\r\n@qlzh727\u00a0I still report an error when Using tf-nightly (2.4.0-dev20200722)", "> > I tested you colab with latest nightly, and it is working now. Closing this issue.\r\n> \r\n> @qlzh727\u00a0I still report an error when Using tf-nightly (2.4.0-dev20200722)\r\n\r\nDo u have the colab to repro the issue?", "> > > I tested you colab with latest nightly, and it is working now. Closing this issue.\r\n> > \r\n> > \r\n> > @qlzh727\u00a0I still report an error when Using tf-nightly (2.4.0-dev20200722)\r\n> \r\n> Do u have the colab to repro the issue?\r\n\r\n@qlzh727   here  https://colab.research.google.com/drive/1XuQWSLa41BFcHlAD2S25cKa6OdxNEuAf?usp=sharing", "It seems that your code disable the eager execution, and I think the code will work if the eager is enabled. Is there any reason that you disable the eager execution?\r\n", "> It seems that your code disable the eager execution, and I think the code will work if the eager is enabled. Is there any reason that you disable the eager execution?\r\n\r\n@qlzh727 It's a long story.I want to use attention model to extract attention score.But I can't find any TF2 API to use.So I consider train model in TF2 mode and save model in TF1 mode.I am searching for a long time on net. But no use.I repro the issue on colab. If you have good suggestions, please give me some advice. Thank you. https://colab.research.google.com/drive/1hq4WWM481pcKH8JoO43gFciZrFeKe-3_?usp=sharing\r\n\r\nedit:I've found a solution to  save the model in TF2 mode .But still cause an error when disable the eager execution.", "Why disable the eager execution cause an error ? Can the issue be fixed ? \r\n edit:I've found a solution to save the model in TF2 mode."]}, {"number": 39907, "title": "TFlite with hexagon delegate returns wrong result. ", "body": "**System information**\r\n- I have written custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Snapdragon 855 running Android 9\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: python 3.8\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI'm using TFlite on a mobile device to execute a quantized NN network. Without use of delegates, \r\nI get the expected output image. When using the hexagon delegate, I get a white square image instead of the expected output. The dimensions and types remain the same. \r\n\r\n**Describe the expected behavior**\r\n\r\nI expect to have the same output image compared to running TFlite on the same device without delegates. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n\r\n    {\r\n       \r\n        std::unique_ptr<tflite::FlatBufferModel> model =\r\n                 tflite::FlatBufferModel::BuildFromFile(filename);\r\n        TFLITE_MINIMAL_CHECK(model != nullptr);\r\n\r\n    // Build the interpreter\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    InterpreterBuilder builder(*model, resolver);\r\n    std::unique_ptr<Interpreter> interpreter;\r\n    builder(&interpreter);\r\n    TFLITE_MINIMAL_CHECK(interpreter != nullptr);\r\n\r\n    // Allocate tensor buffers.\r\n    TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n    printf(\"=== Pre-invoke Interpreter State ===\\n\");\r\n    tflite::PrintInterpreterState(interpreter.get());\r\n\r\n    // input tensor type is float32\r\n    int input = interpreter->inputs()[0];\r\n    TfLiteIntArray* tensor_dims = interpreter->tensor(input)->dims;\r\n    int wanted_height = tensor_dims->data[1];\r\n    int wanted_width = tensor_dims->data[2];\r\n    int wanted_channels = tensor_dims->data[3];\r\n\r\n    int numberOfPixels = wanted_width*wanted_height*wanted_channels;\r\n    if (numberOfPixels != 224*224*3) // expected input size\r\n    {\r\n        std::cout << \"input tensor is of wrong size\";\r\n        exit(-1);\r\n    }\r\n    float * img_ptr = (float *)processedInput->getHostPtr();\r\n    for (int k = 0; k < numberOfPixels; ++k)\r\n    {\r\n        interpreter->typed_tensor<float>(input)[k] = img_ptr[k];\r\n    }\r\n\r\n\r\n    /// setup hexagon delegate now\r\n    const char * library_directory_path = \"/data/local/\";\r\n    TfLiteHexagonInitWithPath(library_directory_path);  // Needed once at startup.\r\n    TfLiteHexagonDelegateOptions params = {0};\r\n\r\n    auto* delegate_ptr = TfLiteHexagonDelegateCreate(&params);\r\n    Interpreter::TfLiteDelegatePtr delegate(delegate_ptr,\r\n                                            [](TfLiteDelegate* delegate) {\r\n                                                TfLiteHexagonDelegateDelete(delegate);\r\n                                            });\r\n    interpreter->ModifyGraphWithDelegate(delegate.get());\r\n\r\n\r\n\r\n    // Run inference\r\n    TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);\r\n    printf(\"\\n\\n=== Post-invoke Interpreter State ===\\n\");\r\n    tflite::PrintInterpreterState(interpreter.get());\r\n\r\n    int output = interpreter->outputs()[0];\r\n    TfLiteIntArray* output_dims = interpreter->tensor(output)->dims;\r\n\r\n    int output_w = output_dims->data[1]; // as expected, this 224 by 224 pixels\r\n    int output_h = output_dims->data[2];\r\n    numberOfPixels = output_h * output_w;\r\n\r\n    // output tensor type is float32\r\n\r\n    float * output_buffer = new float[numberOfPixels];\r\n    for (int k=0; k < numberOfPixels;++k)\r\n        output_buffer[k] = interpreter->typed_tensor<float>(output)[k];\r\n\r\n    std::ofstream file;\r\n    file.open(\"/data/local/rgbImage_out.bin\", std::ios::binary|std::ofstream::out);\r\n    if (file.is_open())\r\n    {\r\n        file.write((char *)output_buffer, 224*224*sizeof(float)); // assumed the out buffer is a vector\r\n        file.close();\r\n\r\n    }else{\r\n        std::cout << \"couldn't open file\" << std::endl;\r\n    }\r\n\r\n    delete []  output_buffer;\r\n\r\n    // After usage of delegate.\r\n    TfLiteHexagonTearDown();  // Needed once at end of app/DSP usage.\r\n\r\n     }\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nSee attached two log files which contain interpreter state prints before and after calling invoke()\r\nfor both cases - with and without a hexagon delegate\r\n[no_delegate_log.txt](https://github.com/tensorflow/tensorflow/files/4689474/no_delegate_log.txt)\r\n[delegate_log.txt](https://github.com/tensorflow/tensorflow/files/4689475/delegate_log.txt)\r\n\r\n\r\n", "comments": ["Hi,\r\n\r\nIt will be hard to check what is wrong without a model. \r\nCan you share the model ? (you can share it privately, no need to attach it here).\r\n\r\nThanks", "How do I share the model privately?", "@yakovdan either email me directly (email same as username here) or send the question on tflite@tensorflow.org and i will reply and we can follow privately.\r\n\r\nThanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39907\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39907\">No</a>\n"]}, {"number": 39906, "title": "Model cannot be saved because the input shapes have not been set.", "body": "Hello, I'm following [save_and_serialize subclass model](https://www.tensorflow.org/guide/keras/save_and_serialize#whole-model_saving_loading) . When I run the following code,\r\n```python\r\nclass CustomModel(tf.keras.Model):\r\n    def __init__(self, hidden_units):\r\n        super(CustomModel, self).__init__()\r\n        self.dense_layers = [\r\n            tf.keras.layers.Dense(u) for u in hidden_units]\r\n    def call(self, inputs):\r\n        x = inputs\r\n        for layer in self.dense_layers:\r\n            x = layer(x)\r\n        return x\r\n\r\nmodel = CustomModel([16, 16, 10])\r\n# Build the model by calling it\r\ninput_arr = tf.random.uniform((1, 5))\r\noutputs = model(input_arr)\r\nmodel.save('my_custom_model')\r\n```\r\nI get the this error, `ValueError: Model <__main__.CustomModel object at 0x7f96797a2c10> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call model._set_inputs(inputs).`\r\n\r\nBut when I change code as following, \r\n```python\r\nmodel = CustomModel([16, 16, 10])\r\ninput_arr = tf.random.uniform((1, 5))\r\noutputs = model(input_arr)\r\nmodel._set_inputs(input_arr) # add this line\r\nmodel.save('my_custom_model')\r\n```\r\nIt runs without any error. What could be the possible cause. Do I need to add `model._set_inputs(input_arr) ` explicitly ?\r\n\r\nI'm using, \r\n`Ubuntu 20.04 LTS`\r\n`conda environment`\r\n`TensorFlow 2.1.1 `\r\n\r\nThanks\r\n", "comments": ["@menon92 \r\nI ran the code shared by you and do not face any errors, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/7ea93b577b7c605b3656251f739bc09b/untitled200.ipynb)", "@Saduf2019 I try the same code in `pip` version of tensorflow and it works without any issue. \r\nI think there is an issue in `conda` version of tensorflow", "@menon92 Are you sure there is an issue with conda version of tensorflow . I am not running into any error. Thanks!", "@gowthamkpr  I got the following output using `conda list`\r\n```bash\r\ntensorboard                   2.1.0                     py3_0  \r\ntensorflow                      2.1.0           mkl_py37h80a91df_0  \r\ntensorflow-base             2.1.0           mkl_py37h6d63fb7_0  \r\ntensorflow-estimator      2.1.0              pyhd54b08b_0  \r\n```\r\nAnd I can regenerate error using bellow code \r\n```bash\r\nPython 3.7.6 (default, Jan  8 2020, 19:59:22) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.12.0 -- An enhanced Interactive Python. Type '?' for help.\r\nIn [1]: import tensorflow as tf                                                                                   \r\n\r\nIn [2]: tf.__version__                                                                                            \r\nOut[2]: '2.1.0'\r\n\r\nIn [3]: class CustomModel(tf.keras.Model): \r\n   ...:     def __init__(self, hidden_units): \r\n   ...:         super(CustomModel, self).__init__() \r\n   ...:         self.dense_layers = [ \r\n   ...:             tf.keras.layers.Dense(u) for u in hidden_units] \r\n   ...:     def call(self, inputs): \r\n   ...:         x = inputs \r\n   ...:         for layer in self.dense_layers: \r\n   ...:             x = layer(x) \r\n   ...:         return x \r\n   ...:  \r\n   ...: model = CustomModel([16, 16, 10]) \r\n   ...: # Build the model by calling it \r\n   ...: input_arr = tf.random.uniform((1, 5)) \r\n   ...: outputs=model(input_arr) \r\n   ...: model.save('my_custom_model')                                                                             \r\n2020-06-04 19:38:19.299584: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n2020-06-04 19:38:19.590094: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2294900000 Hz\r\n2020-06-04 19:38:19.601769: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5579ab0470d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-04 19:38:19.601891: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-04 19:38:19.647392: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nWARNING:tensorflow:Skipping full serialization of Keras model <__main__.CustomModel object at 0x7f911b73cc10>, because its inputs are not defined.\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-ae79cf94cb11> in <module>\r\n     14 input_arr = tf.random.uniform((1, 5))\r\n     15 outputs=model(input_arr)\r\n---> 16 model.save('my_custom_model')\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n   1006     \"\"\"\r\n   1007     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n-> 1008                     signatures, options)\r\n   1009 \r\n   1010   def save_weights(self, filepath, overwrite=True, save_format=None):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    113   else:\r\n    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n--> 115                           signatures, options)\r\n    116 \r\n    117 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)\r\n     63 \r\n     64   if save_impl.should_skip_serialization(model):\r\n---> 65     saving_utils.raise_model_input_error(model)\r\n     66 \r\n     67   if not include_optimizer:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saving_utils.py in raise_model_input_error(model)\r\n    111       'set. Usually, input shapes are automatically determined from calling'\r\n    112       ' .fit() or .predict(). To manually set the shapes, call '\r\n--> 113       'model._set_inputs(inputs).'.format(model))\r\n    114 \r\n    115 \r\n\r\nValueError: Model <__main__.CustomModel object at 0x7f911b73cc10> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call model._set_inputs(inputs)\r\n```", "@menon92 Yes this bug exists in 2.1.0 but has been fixed in tensorflow 2.2.0. \r\n![image](https://user-images.githubusercontent.com/47574994/84019847-b6e4d680-a936-11ea-9a8e-e7946635f967.png)\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39906\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39906\">No</a>\n", "Bug seems to be back in 2.3", "I am facing the same problem in 2.3.", "My bad, the problem is not with 2.3, it is when trying to save a subclassed model. ", "I am also facing the same problem in 2.3. And I used \"model.compute_output_shape(input_shape=(None, 1080,1920,3))\",can also solve this problem when saving  a subclassed model. But why does it work?", "I am facing the same issue in 2.4.0, with jupyter lab python 3.8. \r\nFor testing i downloaded the notebook from https://www.tensorflow.org/tutorials/generative/cvae\r\nBoth, tf.keras.models.save_model(model, path) and model.save(path) return the same ValueError\r\nIs there any workarround?", "I ran into the same issue as @lmielke. If I used a different VAE example, I could save the model. However, using the example from https://www.tensorflow.org/tutorials/generative/cvae just does not work. The code there is really nice. \r\n\r\nIt would be very nice if an updated version of TensorFlow can solve the issue.", "@lmielke @shixm-cloud Did you find a workaround to save the CVAE model? ", " @kennysong, I try this\r\n```python\r\n(base) menon@menon:~$ ipython\r\nPython 3.8.8 (default, Apr 13 2021, 19:58:26) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.22.0 -- An enhanced Interactive Python. Type '?' for help\r\n\r\nIn [18]: import tensorflow as tf\r\n\r\nIn [19]: tf.__version__\r\nOut[19]: '2.4.1'\r\n\r\nIn [20]: class CustomModel(tf.keras.Model):\r\n    ...:     def __init__(self, hidden_units):\r\n    ...:         super(CustomModel, self).__init__()\r\n    ...:         self.dense_layers = [\r\n    ...:             tf.keras.layers.Dense(u) for u in hidden_units]\r\n    ...:     def call(self, inputs):\r\n    ...:         x = inputs\r\n    ...:         for layer in self.dense_layers:\r\n    ...:             x = layer(x)\r\n    ...:         return x\r\n    ...: \r\n\r\nIn [21]: model = CustomModel([16, 16, 10])\r\n    ...: # Build the model by calling it\r\n    ...: input_arr = tf.random.uniform((1, 5))\r\n    ...: outputs = model(input_arr)\r\n    ...: model.save('my_custom_model')\r\nINFO:tensorflow:Assets written to: my_custom_model/assets\r\n\r\nIn [22]: ls my_custom_model\r\nassets/  saved_model.pb  variables/\r\n\r\nIn [23]:\r\n```\r\nAnd it runs without  any issue", "@menon92 Thanks! That works for basic examples, but not for [the CVAE model in the TF documentation](https://www.tensorflow.org/tutorials/generative/cvae).\r\n\r\nI figured out how to save the CVAE by replacing this line:\r\n```python\r\neps = tf.random.normal(shape=mean.shape)\r\n```\r\nwith:\r\n```python\r\neps = tf.random.normal(shape=tf.shape(mean))\r\n```\r\n\r\nAnd adding the `@tf.function` decorator to all of CVAE's methods so they are reconstructed when loading from disk.\r\n\r\n(And calling `.predict()` before saving as the earlier comments note.)", "Hi all!\r\nThanks @boogalooYison , your solution worked with TensorFlow 2.6.0 \ud83d\udc4d  . About the reason why it works, I can only say that the input shape is set in the call to `model.compute_output_shape(input_shape)`, as it is required to compute the actual output shape.\r\n\r\nIn my opinion, this is a cleaner and more elegant solution than having to call the network with dummy input data.\r\n\r\nCheers!", "> Are you satisfied with the resolution of your issue? [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39906) [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39906)\r\n\r\nno", "Hi all, couldn`t save model after training this model, any help please\r\n\r\n`\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Layer\r\nfrom tensorflow.keras import Model\r\n\r\nfrom ssim2dmodule import similarityattention2d\r\nfrom attention_module import SSCblockaddall, SSCblocksam1dlast, SSCblocksamlast, SSCblockcamlast, SSCblockparallel, SSCblockRandom\r\n\r\n\r\n\r\n#act_fn3 = tf.keras.activations.tanh() #tf.ReLU()\r\n\r\nclass encode_block(tf.keras.layers.Layer):\r\n    def __init__(self, filter_num, BatchNorm2d=True):\r\n        super(encode_block, self).__init__()\r\n        \r\n        self.init = tf.keras.initializers.RandomNormal(stddev=0.02)\r\n        self.act_fn =tf.keras.layers.LeakyReLU(0.2)\r\n       \r\n\r\n        self.conv = tf.keras.layers.Conv2D(filter_num, kernel_size=4, \r\n                                    strides=2, padding='same', kernel_initializer=self.init)\r\n        \r\n        self.BatchNorm2d =None\r\n        if BatchNorm2d:\r\n            self.BatchNorm2d = tf.keras.layers.BatchNormalization()\r\n        \r\n        \r\n    def call(self,x):\r\n        x = self.conv(x)\r\n        \r\n        if self.BatchNorm2d != None:\r\n            x = self.BatchNorm2d(x, training=True)\r\n            \r\n        x = self.act_fn(x)\r\n        \r\n        return x\r\n       \r\nclass decode_block(tf.keras.layers.Layer):\r\n    def __init__(self, filter_num, dropout2d=True):\r\n        super(decode_block, self).__init__()\r\n        \r\n        self.init = tf.keras.initializers.RandomNormal(stddev=0.02)\r\n        self.act_fn =tf.keras.layers.ReLU(0.2)\r\n        self.concat = tf.keras.layers.Concatenate()\r\n        self.BatchNorm2d = tf.keras.layers.BatchNormalization()\r\n       \r\n\r\n        self.conv = tf.keras.layers.Conv2DTranspose(filter_num, kernel_size=4, \r\n                                    strides=2, padding='same', kernel_initializer=self.init)\r\n        \r\n        self.dropout2d = None\r\n        if dropout2d:\r\n            self.dropout2d = tf.keras.layers.Dropout(0.5)\r\n        \r\n        \r\n    def call(self,x,concat_in):\r\n        x = self.concat([x, concat_in])\r\n        x = self.conv(x)\r\n        x = self.BatchNorm2d(x, training=True)\r\n        \r\n        if self.dropout2d != None:\r\n            x = self.dropout2d(x, training=True)\r\n        \r\n        x = self.act_fn(x)\r\n        \r\n        return x\r\n    \r\n\r\nclass bottleneck(tf.keras.layers.Layer):\r\n    def __init__(self, filter_num):\r\n        super(bottleneck, self).__init__()\r\n        \r\n        self.init = tf.keras.initializers.RandomNormal(stddev=0.02)\r\n        self.act_fn =tf.keras.layers.ReLU(0.2)\r\n       \r\n        self.conv = tf.keras.layers.Conv2D(filter_num, kernel_size=4, \r\n                                    strides=2, padding='same', kernel_initializer=self.init)\r\n        \r\n        self.dconv = tf.keras.layers.Conv2DTranspose(filter_num, kernel_size=4, \r\n                                    strides=2, padding='same', kernel_initializer=self.init)\r\n\r\n        \r\n        \r\n    def call(self,x):\r\n        x = self.conv(x)\r\n            \r\n        x = self.act_fn(x)\r\n        \r\n        x = self.dconv(x)\r\n        \r\n        x = self.act_fn(x)\r\n        \r\n        return x\r\n    \r\n\r\nclass final_layer(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(final_layer, self).__init__()\r\n        \r\n        self.init = tf.keras.initializers.RandomNormal(stddev=0.02)\r\n        self.concat = tf.keras.layers.Concatenate()\r\n        self.conv = tf.keras.layers.Conv2DTranspose(3, kernel_size=4, \r\n                                    strides=2, padding='same', kernel_initializer=self.init)\r\n        \r\n        \r\n    def call(self,x, concat_in):\r\n        x = self.concat([x, concat_in])\r\n        x = self.conv(x)\r\n        \r\n        x = tf.keras.activations.tanh(x)\r\n        \r\n        return x\r\n        \r\n\r\n\r\nclass MixedFusion_Block0(tf.keras.layers.Layer):\r\n    \r\n    def __init__(self, inputs1, inputs2, filter_num):\r\n        super(MixedFusion_Block0, self).__init__()\r\n\r\n        self.input1 = inputs1\r\n        self.input2 = inputs2\r\n        self.filter_num  = filter_num\r\n\r\n        self.ssim2d = similarityattention2d(inputs1, inputs2)\r\n        self.encode = encode_block(filter_num, BatchNorm2d=False)\r\n\r\n    def call(self, x1, x2):\r\n        \r\n        # multi-style fusion\r\n        ssim2d_out = self.ssim2d(x1,x2)\r\n        encode_out =  self.encode(ssim2d_out)\r\n        print(encode_out.shape)\r\n        if not encode_out.get_shape()[1:] == (self.filter_num*2, self.filter_num*2, self.filter_num):\r\n            encode_out = tf.keras.layers.Reshape((self.filter_num*2, self.filter_num*2, self.filter_num))(encode_out) #(tf.squeeze(encode_out, axis=[0]))\r\n        \r\n        return encode_out\r\n\r\nclass MixedFusion_Block1(tf.keras.layers.Layer):\r\n    \r\n    def __init__(self, x, filter_num): #SSCblockaddall, SSCblocksam1dlast, SSCblocksamlast, SSCblockcamlast, SSCblockparallel, SSCblockRandom\r\n        super(MixedFusion_Block1, self).__init__()\r\n\r\n        self.filter_num  = filter_num\r\n        self.block_name = SSCblockaddall()\r\n        self.encode = encode_block(filter_num)\r\n        \r\n\r\n    def call(self, x1, x2, x3):\r\n        \r\n        y1 = self.block_name(x1) \r\n        y2 = self.block_name(x2)\r\n        y = tf.concat([y1, y2, x3], 3)\r\n        \r\n        \r\n        encode_out = self.encode(y)\r\n        print(encode_out.shape)\r\n\r\n        return encode_out\r\n\r\n\r\n\r\nclass generator(Model):\r\n\r\n    def __init__(self, input_nc, layer_out, filter_num):\r\n        super(generator,self).__init__()\r\n        \r\n        self.filters = filter_num\r\n        self.in_dim = input_nc\r\n        self.layer_out = layer_out\r\n\r\n            # ~~~ Encoding Paths ~~~~~~ #\r\n        # Encoder (Modality 1)\r\n        \r\n        #######################################################################\r\n        # Encoder **Modality 1\r\n        #######################################################################\r\n\r\n        self.encode_1_0 = encode_block(self.filters*1, BatchNorm2d=False)\r\n\r\n        self.encode_2_0 = encode_block(self.filters*2)\r\n\r\n        self.encode_3_0 = encode_block(self.filters*4)\r\n\r\n        self.encode_4_0 = encode_block(self.filters*8)\r\n\r\n        self.encode_5_0 = encode_block(self.filters*8)\r\n\r\n        self.encode_6_0 = encode_block(self.filters*8)\r\n\r\n        self.encode_7_0 = encode_block(self.filters*8)\r\n        \r\n        #self.encode_8_0 = encode_block(self.filters*8)\r\n        \r\n        #######################################################################\r\n        # Encoder **Modality 2\r\n        #######################################################################\r\n\r\n        self.encode_1_1 = encode_block(self.filters, BatchNorm2d=False)\r\n\r\n        self.encode_2_1 = encode_block(self.filters*2)\r\n\r\n        self.encode_3_1 = encode_block(self.filters*4)\r\n\r\n        self.encode_4_1 = encode_block(self.filters*8)\r\n\r\n        self.encode_5_1 = encode_block(self.filters*8)\r\n\r\n        self.encode_6_1 = encode_block(self.filters*8)\r\n\r\n        self.encode_7_1 = encode_block(self.filters*8)\r\n        \r\n        #self.encode_8_1 = encode_block(self.filters*8)\r\n         \r\n        #######################################################################\r\n        # fusion block\r\n        #######################################################################\r\n        # --- fusion encoder\r\n        self.fu_encoder_1 = MixedFusion_Block0(self.in_dim, self.in_dim, self.filters)\r\n          \r\n        self.fu_encoder_2 = MixedFusion_Block1(self.layer_out, self.filters*2)\r\n        \r\n        self.fu_encoder_3 = MixedFusion_Block1(self.layer_out*2, self.filters*4)\r\n\r\n        self.fu_encoder_4 = MixedFusion_Block1(self.layer_out*4, self.filters*8)\r\n\r\n        self.fu_encoder_5 = MixedFusion_Block1(self.layer_out*8, self.filters*8)\r\n\r\n        self.fu_encoder_6 = MixedFusion_Block1(self.layer_out*8, self.filters*8)\r\n\r\n        self.fu_encoder_7 = MixedFusion_Block1(self.layer_out*8, self.filters*8)\r\n        \r\n        #self.fu_encoder_8 = MixedFusion_Block1(self.layer_out*8, self.filters*8)\r\n        \r\n        # bottleneck layer\r\n        self.bottleneck = bottleneck(self.filters*8)\r\n     \r\n        # ~~~ Decoding Path ~~~~~~ #\r\n     \r\n        self.decod_1_0 = decode_block(self.filters*8)\r\n        self.decod_2_0 = decode_block(self.filters*8)\r\n        self.decod_3_0 = decode_block(self.filters*8)\r\n        self.decod_4_0 = decode_block(self.filters*4, dropout2d=False)  \r\n        self.decod_5_0 = decode_block(self.filters*2, dropout2d=False)\r\n        self.decod_6_0 = decode_block(self.filters*1, dropout2d=False)\r\n        \r\n        \r\n        self.out        = final_layer()\r\n\r\n    \r\n                \r\n    def call(self,x,y):\r\n\r\n        # ##############################\r\n        image_shape=(256,256,3)\r\n        x = tf.keras.Input(image_shape)\r\n        y = tf.keras.Input(image_shape)\r\n        z = (x*y)\r\n        \r\n        # -----  First Level -------- \r\n        encoder_1_0 = self.encode_1_0(x) #(256, 256, input_size[-1])\r\n        encoder_1_1 = self.encode_1_1(y) \r\n\r\n\r\n        # -----  Second Level --------\r\n        \r\n        encoder_2_0 = self.encode_2_0(encoder_1_0) # (128, 128, 64)\r\n        encoder_2_1 = self.encode_2_1(encoder_1_1)\r\n        \r\n        \r\n        # -----  Third Level --------\r\n                \r\n        encoder_3_0 = self.encode_3_0(encoder_2_0) # (64, 64, 128)\r\n        encoder_3_1 = self.encode_3_1(encoder_2_1)\r\n\r\n         # -----  Fourth Level --------\r\n                \r\n        encoder_4_0 = self.encode_4_0(encoder_3_0) # (32, 32, 256)\r\n        encoder_4_1 = self.encode_4_1(encoder_3_1)\r\n\r\n         # -----  Five Level --------\r\n                \r\n        encoder_5_0 = self.encode_5_0(encoder_4_0) # (16, 16, 512)\r\n        encoder_5_1 = self.encode_5_1(encoder_4_1)\r\n\r\n        # -----  sixth Level --------\r\n                \r\n        encoder_6_0 = self.encode_6_0(encoder_5_0) # (8, 8, 512)\r\n        encoder_6_1 = self.encode_6_1(encoder_5_1)\r\n\r\n        # -----  seventh Level --------\r\n                \r\n        encoder_7_0 = self.encode_7_0(encoder_6_0) # (4, 4, 512)\r\n        encoder_7_1 = self.encode_7_1(encoder_6_1)\r\n        \r\n         \r\n        # ----------------------------------------\r\n        # fusion block -- f_block\r\n        f_block_1   = self.encode_1_0(z) #self.fu_encoder_1(x,y) \r\n        \r\n        f_block_2  = self.fu_encoder_2(encoder_1_0, encoder_1_1, f_block_1)\r\n        \r\n        f_block_3   = self.fu_encoder_3(encoder_2_0, encoder_2_1, f_block_2) \r\n\r\n        f_block_4  = self.fu_encoder_4(encoder_3_0, encoder_3_1, f_block_3)\r\n\r\n        f_block_5  = self.fu_encoder_5(encoder_4_0, encoder_4_1, f_block_4)\r\n\r\n        f_block_6  = self.fu_encoder_6(encoder_5_0, encoder_5_1, f_block_5)\r\n\r\n        f_block_7  = self.fu_encoder_7(encoder_6_0, encoder_6_1, f_block_6)\r\n\r\n        #f_block_8  = self.fu_encoder_8(encoder_7_0, encoder_7_1, f_block_7)\r\n        \r\n        #f_block_9  = self.fu_encoder_9(encoder_8_0, encoder_8_1, f_block_8)\r\n        \r\n\r\n\r\n        ####################################################################### \r\n        # ~~~~~~ Bottleneck\r\n        btlnck = self.bottleneck(f_block_7)     # (1 x 1 x 512) and # (2 x 2 x 512)\r\n        print(btlnck.shape)\r\n\r\n        #######################################################################                                                                                                \r\n        # ~~~~~~ Decoding \r\n        decoder_1_0 =  self.decod_1_0 (btlnck,f_block_7)  # (4, 4, 512)\r\n        decoder_2_0 = self.decod_2_0(decoder_1_0,f_block_6)  # (8, 8, 512)\r\n        decoder_3_0 = self.decod_3_0(decoder_2_0,f_block_5)  # (16, 16, 512)\r\n        decoder_4_0 = self.decod_4_0(decoder_3_0,f_block_4)  # (32, 32, 256)\r\n        decoder_5_0 = self.decod_5_0(decoder_4_0,f_block_3)  # (64, 64, 128)\r\n        decoder_6_0 = self.decod_6_0(decoder_5_0,f_block_2)   # (128, 128, 64)\r\n        \r\n        \r\n        decod_out     = self.out(decoder_6_0, f_block_1) # (256, 256, output_channels)\r\n        \r\n       \r\n        # get three channels\r\n        \r\n        \r\n                        \r\n        return decod_out\r\n\r\nclass adversary(Model):\r\n\r\n    def __init__(self,filter_num):\r\n        super(adversary,self).__init__()\r\n        \r\n        self.filters = filter_num\r\n        \r\n        self.init = tf.keras.initializers.RandomNormal(stddev=0.02)\r\n        self.last = tf.keras.layers.Conv2D(1, (4,4), padding='same', kernel_initializer = self.init, \r\n                                            activation=tf.keras.activations.sigmoid)\r\n\r\n        self.conv_1 = encode_block(self.filters*1)\r\n\r\n        self.conv_2 = encode_block(self.filters*2)\r\n\r\n        self.conv_3 = encode_block(self.filters*4)\r\n\r\n        self.conv_4 = encode_block(self.filters*8)\r\n\r\n        self.conv_5 = encode_block(self.filters*8)\r\n\r\n        self.conv_6 = encode_block(self.filters*8)\r\n\r\n\r\n    def call(self,x,y):\r\n\r\n        x = tf.keras.Input(x.shape[1:])\r\n        y = tf.keras.Input(y.shape[1:])\r\n\r\n        adv_1 = tf.keras.layers.concatenate([x, y]) # (256, 256, real_channels+fake_channels)\r\n\r\n        adv_2 = self.conv_1(adv_1)                  # (128, 128,  64)\r\n        \r\n        adv_3 = self.conv_2(adv_2)                  # (64,  64, 128)\r\n        \r\n        adv_4 = self.conv_3(adv_3)                  # (32,  32, 256)\r\n        \r\n        adv_5 = self.conv_4(adv_4)                  # (16, 16, 512)\r\n        \r\n        adv_6 = self.conv_5(adv_5)                  #(8, 8, 512)\r\n        \r\n        adv_7 = self.conv_6(adv_6)                  #(4, 4, 512)\r\n        \r\n        adv_8 = self.last(adv_7)                    #(4, 4, 1)\r\n        \r\n        return adv_8\r\n`", "Hi all,\r\n\r\nI recently came across a similar problem where I had 2 models. One text encoder, and one classification head. I needed to implement some custom logic inside the `train_step`. The subclassed model looked as follows:\r\n\r\n```\r\nclass TestModel(keras.Model):\r\n    \r\n    def __init__(self, first, second, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.first = first\r\n        self.second = second\r\n        \r\n    def call(self, inputs):\r\n        return self.second(self.first(inputs))\r\n        \r\n    def train_step(self, data):\r\n        # Here some custom logic requiring both first and second model\r\n        pass\r\n```\r\n\r\n\r\nWhen calling `model.save()` on the instance, it cannot save the model because of the `ValueError` described above. One can only save the model if you have made a forward pass on the model. I was using `KerasTuner` for hyperparameter tuning and noticed that it does not return a correctly built model. So one solution was to pass some data through the model to make it save. That worked but I didn't like it. Then I came across the following solution:\r\n\r\n\r\n```\r\nclass TestModel(keras.Model):\r\n    \r\n    def __init__(self, first, second, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.first = first\r\n        self.second = second\r\n        \r\n        self._set_inputs(inputs=self.first.inputs, outputs=self(self.first.inputs))\r\n        \r\n    def call(self, inputs):\r\n        return self.second(self.first(inputs))\r\n\r\n    def train_step(self, data):\r\n        # Here some custom logic requiring both first and second model\r\n        pass\r\n```\r\n\r\nThis basically allows you to lazily specify the `inputs` and `outputs` as normal. So far it works properly. I hope this might help others out as well.\r\n"]}, {"number": 39905, "title": "tensorflow.dll build with GPU support fails on Windows with \"( was unexpected at this time\"", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 Enterprise 1809\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.5.6 (via Anaconda)\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): MS VisualStudio 2019 14.25.28610\r\n- CUDA/cuDNN version: 10.1/7.6.5.32\r\n- GPU model and memory: NVIDIA TITAN Xp 12GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nBuilding a TF2 for GPU on Windows craches with a (likely cmd-related) error:\r\n```( was unexpected at this time.```\r\n\r\nThis happens when building //tensorflow:tensorflow.dll, but in particular for any *_gpu target (i.e. can build a smaller target from the very beginning, will lead to the same error - alternative \"short\" command provided below as well)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nStep 0: Setting up & activating a clean virtual environment as per instructions at https://www.tensorflow.org/install/source_windows, installing & upgrading MSYS2, placing [Bazel 2.0.0](https://github.com/bazelbuild/bazel/releases/download/2.0.0/bazel-2.0.0-windows-x86_64.exe) into the appropriate folder.\r\n\r\nStep 1: Executing the following in cmd:\r\n\r\n```\r\ncd C:\\source\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout v2.2.0\r\n\r\n:: setting up paths\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin;%PATH%\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\include;%PATH%\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\extras\\CUPTI\\lib64;%PATH%\r\n:: (CUDA_PATH points to the same v10.1 folder)\r\n:: (cuDNN files already copied into the respective cuda folder)\r\nSET PATH=C:\\Program Files (x86)\\Bazel;%PATH%\r\nSET PATH=C:\\msys64\\usr\\bin;%PATH%\r\n\r\n:: setting up Bazel-related environment variables\r\nset BAZEL_VS=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\r\nset BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\r\nset BAZEL_VC_FULL_VERSION=14.25.28610\r\n:: (not setting BAZEL_SH since added msys64 into path, but setting it also does not help)\r\n```\r\n\r\nConfiguration script output:\r\n```\r\n(TF2Build) C:\\source\\tensorflow>configure.cmd\r\nYou have bazel 2.0.0 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\mikhail.startsev\\Venvs\\TF2Build\\Scripts\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\mikhail.startsev\\Venvs\\TF2Build\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\mikhail.startsev\\Venvs\\TF2Build\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\nFound cuDNN 7 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n```\r\n\r\nWhat I want to build in the end:\r\n```\r\nbazel build --define=no_tensorflow_py_deps=true -c opt //tensorflow/tensorflow.dll\r\n```\r\n\r\nWhat give the same errors but potentially faster (any other *_gpu target will do as well):\r\n```\r\nbazel build --define=no_tensorflow_py_deps=true -c opt tensorflow/core/kernels/debug_ops_gpu\r\n```\r\n\r\nBoth result in the following lines at the end of the output (for //tensorflow/tensorflow.dll, the sub-target varies with each run, but always *_gpu):\r\n\r\n```ERROR: C:/source/tensorflow/tensorflow/core/kernels/BUILD:2046:1: C++ compilation of rule '//tensorflow/core/kernels:gather_functor_gpu' failed (Exit 255)\r\n( was unexpected at this time.\r\nTarget //tensorflow/core/kernels:debug_ops_gpu failed to build\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nFor a more explicit output I ran the build with these extra options: `-s --verbose_failures --jobs=1`. The terminal output of the last subcommand is below:\r\n\r\n``` \r\nINFO: Analyzed target //tensorflow/core/kernels:debug_ops_gpu (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nSUBCOMMAND: # //tensorflow/core/kernels:debug_ops_gpu [action 'Compiling tensorflow/core/kernels/debug_ops_gpu.cu.cc', configuration: 0da6e0533905602ae3b4151f84e97ac897aad4120d80febd63f279c70317ba99]\r\ncd C:/users/mikhail.startsev/_bazel_mikhail.startsev/3watukzj/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.25.28610\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.25.28610\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.25.28610\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.25.28610\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.25.28610\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\Tools\\;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/mikhail.startsev/Venvs/TF2Build/Scripts/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/mikhail.startsev/Venvs/TF2Build/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\MIKHAI~1.STA\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0\r\n    SET TF_ENABLE_XLA=1\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=C:\\Users\\MIKHAI~1.STA\\AppData\\Local\\Temp\r\n  C:/Users/mikhail.startsev/Venvs/TF2Build/Scripts/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/mkl_dnn /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn /Iexternal/curl /Ibazel-out/x64_windows-opt/bin/external/curl /Iexternal/boringssl /Ibazel-out/x64_windows-opt/bin/external/boringssl /Iexternal/jsoncpp_git /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git /Iexternal/aws /Ibazel-out/x64_windows-opt/bin/external/aws /Iexternal/aws-c-common /Ibazel-out/x64_windows-opt/bin/external/aws-c-common /Iexternal/aws-c-event-stream /Ibazel-out/x64_windows-opt/bin/external/aws-c-event-stream /Iexternal/aws-checksums /Ibazel-out/x64_windows-opt/bin/external/aws-checksums /Iexternal/com_github_grpc_grpc /Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc /Iexternal/upb /Ibazel-out/x64_windows-opt/bin/external/upb /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cublas/include /Iexternal/mkl_dnn/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/include /Iexternal/mkl_dnn/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src /Iexternal/mkl_dnn/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/common /Iexternal/mkl_dnn/src/cpu /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu /Iexternal/mkl_dnn/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/gemm /Iexternal/mkl_dnn/src/cpu/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/xbyak /Iexternal/curl/include /Ibazel-out/x64_windows-opt/bin/external/curl/include /Iexternal/boringssl/src/include /Ibazel-out/x64_windows-opt/bin/external/boringssl/src/include /Iexternal/jsoncpp_git/include /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git/include /Iexternal/aws/aws-cpp-sdk-core/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-core/include /Iexternal/aws/aws-cpp-sdk-s3/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-s3/include /Iexternal/aws/aws-cpp-sdk-transfer/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-transfer/include /Iexternal/aws-c-common/include /Ibazel-out/x64_windows-opt/bin/external/aws-c-common/include /Iexternal/aws-c-event-stream/include /Ibazel-out/x64_windows-opt/bin/external/aws-c-event-stream/include /Iexternal/aws-checksums/include /Ibazel-out/x64_windows-opt/bin/external/aws-checksums/include /Iexternal/com_github_grpc_grpc/include /Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc/include /Iexternal/com_github_grpc_grpc/src/core/ext/upb-generated /Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc/src/core/ext/upb-generated /Iexternal/com_github_grpc_grpc/third_party/address_sorting/include /Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc/third_party/address_sorting/include /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DTF_USE_SNAPPY /DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL /DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL /DCURL_STATICLIB /DPLATFORM_WINDOWS /DENABLE_CURL_CLIENT /DOPENSSL_IS_BORINGSSL /DGRPC_ARES=0 /showIncludes /MD /O2 /DNDEBUG /w /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /std:c++14 -x cuda -DGOOGLE_CUDA=1 --no-cuda-include-ptx=all --cuda-include-ptx=sm_35 --cuda-gpu-arch=sm_35 --cuda-include-ptx=sm_70 --cuda-gpu-arch=sm_70 -DGOOGLE_CUDA=1 -DTENSORFLOW_USE_NVCC=1 -DTENSORFLOW_USE_XLA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/debug_ops_gpu/debug_ops_gpu.cu.o /c tensorflow/core/kernels/debug_ops_gpu.cu.cc\r\nERROR: C:/source/tensorflow/tensorflow/core/kernels/BUILD:1013:1: C++ compilation of rule '//tensorflow/core/kernels:debug_ops_gpu' failed (Exit 255)\r\n**( was unexpected at this time.**\r\nTarget //tensorflow/core/kernels:debug_ops_gpu failed to build\r\nINFO: Elapsed time: 1110.066s, Critical Path: 114.51s\r\nINFO: 2748 processes: 2748 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nIt is not the error of the msvc_wrapper_for_nvcc.py, since running it separately (with all the commands listed here) did not result in an error.\r\n\r\nIt seems that this type of errors (\"XXX was unexpected at this time\") are cmd-specific, but the generic solution of `setlocal enableextensions enabledelayedexpansion` did not help or anyhow change anything.\r\n\r\n**Other attempts with the same results**\r\n* Also tried [building from the MSYS shell](https://www.tensorflow.org/install/source_windows#build_using_the_msys_shell)\r\n* Tried using cygwin sh as BAZEL_SH\r\n* TF2.1.0 (as well as TF2.1.1) and the master branch from the TF repo, with respective Bazel versions - 0.29.1 and 3.0.0\r\n\r\nWhat _did_ work was building the tensorflow.dll without GPU support requested in configure.cmd, but that is not what I wanted to have in the end...\r\n\r\n\r\nOverall, it seems that some batch command/script generated/used during building is malformatted, maybe because some environment variable is not set (and is expanded into and empty string) or something else, but I don't have a way to know which one, or which command exactly fails. The `-s` option to `bazel build` does not output the precise command that results into the error I am experiencing (and setting --logging to the highest value does not help either).", "comments": ["@Artem-B @chsigg any ideas?", "> running it separately (with all the commands listed here) did not result in an error.\r\n\r\nI think the message is from bazel when the 'compiler' (msvc_wrapper_for_nvcc.py) returns non-zero.\r\nWould you mind checking the result code of running the command explicitly? And maybe add some manual logging to the wrapper to figure out what's going on?\r\nSorry I don't have any better advice. I just had a similar issue last week, and couldn't get the --copt=--cuda_log to work either and reverted to printf debugging. :-(", "No idea what's going on, but it should be possible to narrow the issues down a bit.\r\n* add printouts in the `msvc_wrapper_for_nvcc.py` to check whether we actually get to launch nvcc.\r\n* if we don't get to the wrapper, the issues is somewhere in bazel\r\n* if the wrapper launches, but we fail to launch nvcc, then there's some sort of issue with python.\r\n* if we do launch nvcc, modify the wrapper to pass `-v` and see whether the failure happens there and if so, at which point.\r\n", "@chsigg @Artem-B Thank you for the suggestions for debugging! Following your recommendations:\r\n* It does get to the msvc_wrapper_for_nvcc.py call, and this finishes successfully (%errorlevel% is 0)\r\n* Because of this, adding -v to the wrapper (as well as calling it with --cuda_log flag) does not shed any extra light on the fail reason...\r\n\r\nTo me, the symptoms look similar to something like this: https://stackoverflow.com/a/10511644/1675506 , but, as I mentioned, `setlocal enableextensions enabledelayedexpansion` does not help in this case... Is there maybe a way to log all cmd command executed by bazel?.. I did not find one, at least. Or a list of _all_ environment variables that are required for this build (for GPU-related targets specifically) would be nice, but I don't see it documented either..\r\n\r\n\r\n**Update**: In the vein of trying to figure out the possible missing environment variable, I tried mirroring all of the possibly relevant `set X=Y` commands from the [CI build log](https://source.cloud.google.com/results/invocations/1266ca24-60da-4870-8fc1-ad33c77fe5d7/targets/tensorflow%2Fgithub%2Fwindows%2Fgpu_py36_full%2Fcontinuous/log): \r\n\r\nI **set** BAZEL_SH, TEMP, TMP, TMPDIR, PY_EXE, PIP_EXE, TF_CUDA_VERSION, TF_CUDNN_VERSION, TF_CUDA_COMPUTE_CAPABILITIES, CUDA_TOOLKIT_PATH, CUDNN_INSTALL_PATH, BAZEL_VERSION, WIN_OUT, WIN_OUT_TARGET, and RC.\r\n\r\nI **did not set** only GCS_BUCKET_NAME, GOOGLE_CLOUD_CREDENTIAL, BAZEL_WRAPPER_PATH, PYTHON_DIRECTORY, BUILD_PATH, GEN_SCRIPT, and GEN_BUILD.\r\n\r\n-> same error still...", "You can use the `-s` flag of bazel to log all commands invoked by bazel.", "@gunan I used it in the initial post already, hoping to catch the actually failing command, but it is not listed (see the output log I posted), unfortunately - the command sequence after which the crash happens completes successfully.\r\n\r\n**Another update:** I set all the variables that were used in the CI build log but that I did not set before (i.e. GCS_BUCKET_NAME, GOOGLE_CLOUD_CREDENTIAL, BAZEL_WRAPPER_PATH, PYTHON_DIRECTORY, BUILD_PATH, GEN_SCRIPT, and GEN_BUILD) to the strings with the same content to at least maybe get a different error, but to no avail.", "I'm fresh out of ideas. Debugging windows issues is such a pain. I can't wait until WSL2 supports GPUs so we could just use linux build. @gunan -- do we have any contacts at MSFT working on/with TF? There's a good chance that someone out there does have the know-how of dealing with these kinds of issues.", "I build tag v2.2.0 with command \r\n`bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/lib_package:libtensorflow`\r\nMy problem then is that .lib file build is missing some symbols and can't link it.\r\nIssue: https://github.com/tensorflow/tensorflow/issues/39407\r\nMake sure you use bazel 2.0.0", "Make sure you install build tools to C:\\msys64, since they don't digest space in 'Program Files'.", "@jgasperlin I tried installing VS Build Tools separately (was using my existing VS installation before; installed into a msys64 subfolder as you suggested) - get exactly the same error as before. Don't think our issues are related, to be honest...", "Ok we misunderstood as build tool i meant `Msys` that resides on `C:\\msys64` by default  path (Just checked)\r\nBAZEL_VS where you set your build tool (Mine is 2019 ) can have spaces and equals to \r\n`C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC`\r\nI remembered i had similar problem and if not Msys path i think it was python then mine is on\r\n`where python` => `C:\\Users\\Targma\\AppData\\Local\\Programs\\Python\\Python38\\python.exe`\r\nwhich is probably same for you but then i switched Bazel 3.2 for 2.0 which build status is confirmed on \r\nhttps://www.tensorflow.org/install/source_windows\r\nif you scroll to the bottom.\r\n>Version | Python version | Compiler | Build tools | cuDNN | CUDA\r\ntensorflow_gpu-2.2.0 | 3.5-3.8 | MSVC 2019 | Bazel 2.0.0 | 7.4 | 10.1\r\n\r\nI just resolved also issues with symbols which you should check out https://github.com/tensorflow/tensorflow/issues/39407\r\n\r\nHope that helps, you should also run Bazel with verbose logging `-s` (https://stackoverflow.com/questions/32823625/bazel-build-verbose-compiler-commands-logging) so you know where the problem is\r\n\r\n\r\n", "@jgasperlin Ah, okay, I see. Either way, my msys64 path has no spaces in it as it is, yes. And the python path is without spaces as well... I am using the recommended bazel version from the same list you are referring to. The only difference is that cuDNN 7.4 should not support cuda 10.1, so I am using a later cuDNN version... I also used the `-s` flag, yes, but it does not print the command that _actually_ results in the error I get...\r\n\r\nLooking at the issue you linked, I tried compiling a different target (//tensorflow/tools/lib_package:libtensorflow) instead of the one I used, with the same error coming out in the end... (i.e. \"( was unexpected at this time.\")\r\n\r\nI don't get as far as the linker errors you were experiencing, but thanks for the heads-up, in case it gets to it!\r\n\r\nSince you succeeded in building tensorflow.dll with CUDA support under Windows, could you please post the exact command sequence you use for this (including setting the PATH and other necessary variables - from a \"clean\" terminal, without anything extra pre-set) - I am afraid that I have some environment variable missing, but can't figure out which one...\r\n\r\nThanks for responding so far :) It is encouraging that you managed to make it work!", "make sure before you rebuild anything with Bazel do \r\n`bazel clean --expunge` this will clean up everything\r\ni used lates cuDNN 7.6.something\r\n~~do you have set cuda path ? when you configure with \r\n`python configure.py`\r\ndoes it pick Cuda v10.1 and cuDNN 7.6 ?~~\r\nMy problem was resolved with using correct bazel to build (2.0.0)\r\n\r\nTry to move bazel:\r\n`SET PATH=C:\\Program Files (x86)\\Bazel` to\r\n`SET PATH=C:\\bazel`\r\ni think some scripts don't escape `(` which is your error i assume\r\n>**( was unexpected at this time.**", "@jgasperlin  \r\nI have been [asking something like this **since 6 months ago**](https://github.com/tensorflow/tensorflow/issues/35405). No progress for anything related to tensorflow2.*\r\n\r\nhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.0.0.zip\r\n\r\n\r\nAny chance if you can provide a link to your gpu version of libtensorflow?  So we can continue to test while Google finally makes up the mind to support Windows developers", "We need to include this =>  label:\"stat:awaiting response\" ", "@av8ramit @bmzhao is working to fix libtensorflow releases.\r\nAs mentioned in other issues, owners of our C++ APIs have left, and now we are ramping up on the releases, moving them to 2.0, and cleaning up stale build scripts all at the same time. This is taking some time, thus the delay. Moreover, we have lost multiple people with windows expertise, so that has been quite difficult for us to ramp up as well.", "@gunan  => @av8ramit @bmzhao  were never assigned BUT U to these issues last few months.", "@av8ramit  support LINUX last one month, not WINDOWS!!\r\n", "@bmzhao has no records of contributing to WINDOWS! last 3 months", "We need to include this => label:\"stat:awaiting response\"", "We need to invite more discussion on the future of WINDOW support for tensorflow.  6 months with no progress but challenges. **Google needs to hire those WHO CLIAM HERE are able to address these challenges IMMEDIATELY.** ", "The latest commit to address this problem is to get more Windows developers to spend MORE TIME trying [new Bazel  (from 2.0 to 3.0)](https://github.com/tensorflow/tensorflow/commit/b4b83222d470afbf0b83d12b0824c0f056235655)\r\n", "Next step is to do DATA SCIENCE, how many TOTAL MONTHS, all windows developers HERE spent on (Attempting) building tensorlfow GPU DLL!", "@jgasperlin  Google should hire you to avoid the WINDOWS developers spending TOTAL many months just to build Windows dll GPU", "> @av8ramit @bmzhao is working to fix libtensorflow releases.\r\n> As mentioned in other issues, owners of our C++ APIs have left, and now we are ramping up on the releases, moving them to 2.0, and cleaning up stale build scripts all at the same time. This is taking some time, thus the delay. Moreover, we have lost multiple people with windows expertise, so that has been quite difficult for us to ramp up as well.\r\n\r\n@gunan  NEXT step is to request/invite discussions at Linkedin on this issue as this is urgent and NO progress has been made last 6 months\r\n", "I have blocked @Dave-Stein for now because of the spamming behavior. Please get in touch Dave if you wish to be unblocked and abide within our community rules for civil and constructive discussion.", "Ooookay, finally an update! @gunan @Artem-B @av8ramit @bmzhao, in case you are interested, the issue that I was experiencing before was connected with the fact that bazel **clears the environment** at some point or other during its build ahead of some operations. Turned out that my system had a special env variable set up, which led to the error I saw being produced by a script that is executed when any new cmd is created on my system. \r\n\r\nTherefore, so far: (1) this is not a general error that will affect everyone, but (2) I think it should be made clear somewhere in the documentation that certain commands during the build will be executed in a cleaned environment, so one cannot rely on the original env variables being available throughout the build, even if they are not used or set by the configuration script or bazel directly. **This can cause all sorts of issues and artefacts on different systems, depending on the set-up**, would be at least good to know this upfront, if not possibly have a workaround for this (to keep certain or all variables intact). **UPD:** see message below, option --action_env.\r\n\r\n~~I still have some issues, the latest being \"Failed to run \\c\\Users\\mikhail.startsev\\AppData\\Local\\Temp/tmpxft_00004654_00000000-1.bat (The system cannot find the file specified.\", but at least it's a start :)~~", "Okay, closing the issue, it works now - having initially had a suspicion that it has to do with env variables, just not knowing which ones, I messed with the environement a lot, this seems to have cause some temporary issue. Now building with my \"normal\" env + setting bazel-specific variables works fine!\r\n\r\nThanks everyone for your responses. Just so that this does not fall through the cracks, repeating again: I still think this behaviour should be explicitly documented.\r\n\r\nThe solution for preserving some env variable that I got recommended on the bazel mailing list is [--action_env](https://docs.bazel.build/versions/master/command-line-reference.html#flag--action_env).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39905\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39905\">No</a>\n"]}, {"number": 39904, "title": "XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time", "body": "Hi,\r\n\r\nI am compiling a custom RNN with XLA (colab link to reproduce with tf-nightly https://colab.research.google.com/drive/1ehh6kklHmXSmcDs6mCdM-6y59v01dXKF?usp=sharing#scrollTo=MpeMlEwGje0n).\r\n\r\nWhat I get is:\r\n    XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time\r\n\r\n It could be as well that I am doing something completely wrong, though.\r\n    \r\n    ", "comments": ["Was able to reproduce the issue. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/2b38a0398bdb8cc66c2713689a44389c/39904.ipynb). Thanks!", "I tried to run the code in colab with TF v2.5 & didn't face any issue reported,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/36c6f4413dd89ea06e9403a5f9cedd5c/untitled285.ipynb) ..Thanks!", "Closing this issue as it is fixed in latest version of TensorFlow. Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39904\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39904\">No</a>\n"]}, {"number": 39903, "title": "Can't import tensorflow ( tf-nightly-gpu)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window10\r\n- TensorFlow installed from (source or binary): Anaconda\r\n- TensorFlow version: tf-nightly-gpu    2.3.0.dev20200526\r\n- Python version: Python 3.7.4\r\n- Installed using virtualenv? pip? conda?: Yes\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory:  NVIDIA GeForce RTX 2070 with Max-Q design (8GB)\r\n\r\n\r\n\r\n**Describe the problem**\r\nI updated my driver to the latest version, installed Visual Studio 2019 and CUDA/cuDNN version 10.1. I installed tensorflow through Anaconda prompt: pip install tf-nightly-gpu\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nfrom tensorflow.keras.applications.efficientnet import EfficientNetB0\r\nfrom tensorflow.keras.preprocessing import image\r\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     63   try:\r\n---> 64     from tensorflow.python._pywrap_tensorflow_internal import *\r\n     65   # This try catch logic is because there is no bazel equivalent for py_extension.\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-4eb6aa7ec913> in <module>\r\n----> 1 from tensorflow.keras.applications.efficientnet import EfficientNetB0\r\n      2 from tensorflow.keras.preprocessing import image\r\n      3 from tensorflow.keras.applications.efficientnet import preprocess_input\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     48 import numpy as np\r\n     49 \r\n---> 50 from tensorflow.python import pywrap_tensorflow\r\n     51 \r\n     52 # Protocol buffers\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     81 for some common reasons and solutions.  Include the entire stack trace\r\n     82 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 83   raise ImportError(msg)\r\n     84 \r\n     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\G7\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n---------------------------------------------------------------------------\r\n\r\nThe installation works for my colleagues but it doesn't work for my notebook and we could not solve the problem. \r\n\r\nThank you in advance!\r\nVoravich", "comments": ["@voravich-ch \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements).\r\nMake sure to download the latest [microsoft visual c++ redistributable](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads) from here.\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "> @voravich-ch\r\n> \r\n> What is make/model of your cpu?\r\n> I suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements).\r\n> Make sure to download the latest [microsoft visual c++ redistributable](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads) from here.\r\n> .Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n> \r\n> Please, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\n> Thanks!\r\n\r\nMy CPU is Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz 2.59 GHz\r\nBoth my CPU and Python is on 64bits.", "I installed microsoft visual c++ redistributable  and it works now \r\nThank you Saduf2019 !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39903\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39903\">No</a>\n"]}, {"number": 39902, "title": "[TFLite] 16x8 reference kernel for PAD operator", "body": "In this PR I extend the operator PAD for 16x8 quantization mode.\r\nThis operator is present in Mobilenet V1/V2 from keras.applications.", "comments": ["cc/ @renjie-liu.", "Hi @renjie-liu Thanks for the review. I addressed your comments - please take a look. ", "@yyoon @renjie-liu this PR has been failing internally , can you please help with this ?", "Hi @renjie-liu Could you please re-approve this PR ? Thanks!", "@wwwind Can you please resolve conflicts? Thanks!", "@renjie-liu  @yyoon can you please help with internal failures ?", "Hmm. AFAICT, the error seems unrelated to this PR. Let me try re-running the internal CI builds.", "@rthadur All checks have passed now.", "Hi @renjie-liu Corrected. Please take a look. Thanks!"]}, {"number": 39901, "title": "Fix make `overriding recipe` warnings", "body": "This PR fixes `overriding recipe` warnings for ruy and person_model_grayscale:\r\n```\r\ntensorflow/lite/micro/tools/make/Makefile:283: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/ruy'\r\ntensorflow/lite/micro/tools/make/Makefile:283: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/ruy'\r\ntensorflow/lite/micro/tools/make/Makefile:283: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale'\r\ntensorflow/lite/micro/tools/make/Makefile:283: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale'\r\n```\r\nIt also fixes `overriding recipe` warnings for target specific Makefiles.", "comments": ["@kamilrakoczy Can you please check @advaitjain's comments and resolve conflicts?. Thanks!", "@kamilrakoczy, Any update on this PR? Please. Thanks!", "@gbaned @advaitjain I resolved conflict and responded to comments.", "@kamilrakoczy  Can you please check @advaitjain's comments and keep us posted ? Thanks!", "@gbaned @advaitjain I added requested comment about limiting maxdepth of find."]}, {"number": 39900, "title": "Keras: Regularizer not saved for Lambda layers", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 16.04.6 LTS\r\n- TensorFlow installed from: binary, using pip\r\n- TensorFlow version: v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.6.6\r\n- CUDA version: 10.2\r\n\r\n**Describe the current behavior**\r\n\r\nUsing the keras API, I can add regularizers to all layers. However, if I add a regularizer to a Lambda layer, after saving and reloading, the regularizer is not existent anymore...\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect that all regularizers exist after saving and reloading.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nCode example:\r\n```python\r\nfrom tensorflow.keras.layers import Input, Dense, Lambda\r\nfrom tensorflow.keras import regularizers\r\nfrom tensorflow.keras.models import Model, load_model\r\n\r\n\r\nlayer_input = Input(shape=(None, 10), name='input')\r\nlayer_dense = Dense(12, activity_regularizer=regularizers.l2(), name='dense')(layer_input)\r\nlayer_lambda = Lambda(lambda batch: batch, activity_regularizer=regularizers.l2(), name='lambda')(layer_dense)\r\n\r\nmodel = Model(inputs=layer_input, outputs=layer_lambda)\r\nmodel.compile(loss='mean_squared_error')\r\n\r\nprint('* Original model *')\r\nprint('regularizer losses', model.losses)\r\nprint('regularizer dense', model.get_layer('dense').activity_regularizer)\r\nprint('regularizer lambda', model.get_layer('lambda').activity_regularizer)\r\nprint()\r\n\r\nmodel.save('test.h5')\r\nmodel_reloaded = load_model('test.h5')\r\n\r\nprint('* Reloaded model *')\r\nprint('regularizer losses', model_reloaded.losses)\r\nprint('regularizer dense', model_reloaded.get_layer('dense').activity_regularizer)\r\nprint('regularizer lambda', model_reloaded.get_layer('lambda').activity_regularizer)\r\n```\r\n\r\nOutput\r\n```\r\n* Original model *\r\nregularizer losses [<tf.Tensor 'dense/ActivityRegularizer/truediv:0' shape=() dtype=float32>, <tf.Tensor 'lambda/ActivityRegularizer/truediv:0' shape=() dtype=float32>]\r\nregularizer dense <tensorflow.python.keras.regularizers.L1L2 object at 0x7f2b61e3c320>\r\nregularizer lambda <tensorflow.python.keras.regularizers.L1L2 object at 0x7f2aeae19048>\r\n\r\n* Reloaded model *\r\nregularizer losses [<tf.Tensor 'dense_1/ActivityRegularizer/truediv:0' shape=() dtype=float32>]\r\nregularizer dense <tensorflow.python.keras.regularizers.L1L2 object at 0x7f2aeaf85470>\r\nregularizer lambda None\r\n```", "comments": ["I tried in colab with TF version 2.1, 2.2, nightly versions and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/c6d52af5beef1cdd7a64c4203d1b788f/untitled930.ipynb).Thanks!", "@fzalkow have you tried saving to a TF saved model format, remove the .h5 extension for save and load and the regularizer is available on loading. ", "If I remove the .h5 extension, then the regularizer is available in the reloaded model. Indeed, it seems to be an issue with the .h5 save format.", "Was able to reproduce the issue in TF 2.5 and Nightly versions as wel. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/efef6af0a051e882b9e4984bd6d0a4c8/untitled80.ipynb).Thanks!", "I was able to replicate issue in `tf2.6` and `tf-nightly(2.8.0-dev20211012)`. Please find the gist [here](https://colab.research.google.com/gist/chunduriv/f03b6f5ea848f256f9f4b12a9c7e3c7d/39900.ipynb).Thanks!\r\n\r\n", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39900\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39900\">No</a>\n"]}, {"number": 39899, "title": "Batch, shuffle, and unbatch", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, TF 1.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CPU\r\n- GPU model and memory: CPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n100 elements, that are batched with a batch size of 5, shuffled, and then unbatched. The returned elements are not shuffled.\r\n\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n16\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n30\r\n31\r\n32\r\n33\r\n34\r\n35\r\n36\r\n37\r\n38\r\n39\r\n40\r\n41\r\n42\r\n43\r\n44\r\n45\r\n46\r\n47\r\n48\r\n49\r\n50\r\n51\r\n52\r\n53\r\n54\r\n55\r\n56\r\n57\r\n58\r\n59\r\n60\r\n61\r\n62\r\n63\r\n64\r\n65\r\n66\r\n67\r\n68\r\n69\r\n70\r\n71\r\n72\r\n73\r\n74\r\n75\r\n76\r\n77\r\n78\r\n79\r\n80\r\n81\r\n82\r\n83\r\n84\r\n85\r\n86\r\n87\r\n88\r\n89\r\n90\r\n91\r\n92\r\n93\r\n94\r\n95\r\n96\r\n97\r\n98\r\n99\r\n**Describe the expected behavior**\r\n100 elements, that are batched with a batch size of 5, shuffled, and then unbatched. The returned elements should be shuffled, but each 5 (batch size) consecutive elements should be unshuffled.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n`\r\nimport tensorflow as tf\r\n\r\ndataset = tf.data.TFRecordDataset.range(100)\r\ndef dataset_fn(ds):\r\n    ds_ = ds.batch(5).shuffle(buffer_size=100).unbatch()\r\n    return ds_\r\n\r\ndataset = dataset.apply(dataset_fn)\r\n\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n\r\n#Actually run in a session\r\nwith tf.Session() as sess:\r\n    for i in range(100):\r\n        print(sess.run(next_element))\r\n`\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["https://colab.research.google.com/drive/1nyymFSV6-tAX9WuZ_LmPaO570Q1dWJsn#scrollTo=mw68rBbhs1Gs\r\n\r\nSolved issue by the link presented above, thank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39899\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39899\">No</a>\n"]}, {"number": 39898, "title": "add TF_LOCKS_EXCLUDED to MutableDenseHashTable::MemoryUsed", "body": "TF_LOCKS_EXCLUDED request", "comments": []}, {"number": 39897, "title": "MutableDenseHashTable should check empty keys before Rebucket when Insert", "body": "MutableDenseHashTable Insert operation should check empty keys before Rebucket, otherwise too much empty keys may cause the table too large", "comments": ["@allenlavoie hi, could you help review this patch ? we got huge memory used since unchecked keys, thanks", "@rohan100jain Can you please review this PR ? Thanks!", "@rohan100jain Can you please review this PR ? Thanks!"]}]