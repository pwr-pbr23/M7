[{"number": 32066, "title": "Difference in graphdef binary between training and serving for tf.nn.embedding_lookup", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nNAME=\"Linux Mint\"\r\nVERSION=\"18 (Sarah)\"\r\nID=linuxmint\r\nID_LIKE=ubuntu\r\nPRETTY_NAME=\"Linux Mint 18\"\r\nVERSION_ID=\"18\"\r\nHOME_URL=\"http://www.linuxmint.com/\"\r\nSUPPORT_URL=\"http://forums.linuxmint.com/\"\r\nBUG_REPORT_URL=\"http://bugs.launchpad.net/linuxmint/\"\r\nUBUNTU_CODENAME=xenial\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): ('v1.14.0-rc1-22-gaf24dc91b5', '1.14.0')\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: V10.0.130/ v7.5.0\r\n- GPU model and memory: GeForce GTX 1060 (x2) each with 6070MiB.\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen trying to use tf.nn.embedding_lookup during a typical training workflow, everything seems to work fine. However when using the same code with the same tensorflow version/ tensorflow api version, etc. while trying to serve the model an error is thrown. The error seems to point to a mismatch in the way embedding_lookup was defined in tensorflow serving. One is expecting a batch_dims argument for gather_v2 while the other believes this is an invalid argument.\r\n\r\n**Describe the expected behavior**\r\nIt's expected that, assuming the pasted code is correct that the predict() function should result in same output as the train() output. However, running predict() throws the error above. I have tested this on both tensorflow==1.14.0 and tensorflow-gpu==1.14.0 both with tensorflow-serving-api==1.14.0\r\n**Code to reproduce the issue**\r\n\r\nRunning this works as expected. Prints out the result of sess.run and exports the 'model'\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport os\r\nimport subprocess as sp\r\nimport grpc\r\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\r\nfrom tensorflow_serving.apis import predict_pb2\r\n\r\nnp.random.seed(0)\r\n\r\n\r\ndef model(indices):\r\n  embeddings = tf.constant(np.random.rand(4, 5))\r\n  embedding_vectors = tf.nn.embedding_lookup(\r\n    embeddings,\r\n    indices\r\n  )\r\n  weights = tf.constant(np.random.rand(5, 2))\r\n  preds = tf.nn.softmax(tf.matmul(embedding_vectors, weights))\r\n  return preds\r\n\r\n\r\ndef train():\r\n  graph = tf.Graph()\r\n  with graph.as_default():\r\n\r\n    preds = model(tf.constant([0, 2, 1], dtype=tf.int32))\r\n\r\n  with tf.Session(graph=graph) as sess:\r\n    print sess.run(preds)\r\n\r\n\r\ndef export():\r\n  graph = tf.Graph()\r\n  with graph.as_default():\r\n\r\n    ph = tf.placeholder(shape=[None], dtype=tf.int32)\r\n    preds = model(ph)\r\n\r\n  with tf.Session(graph=graph) as sess:\r\n    inputs = {'input': tf.saved_model.utils.build_tensor_info(ph)}\r\n    outputs = {'output': tf.saved_model.utils.build_tensor_info(preds)}\r\n    signature = (\r\n        tf.saved_model.signature_def_utils.build_signature_def(\r\n            inputs=inputs,\r\n            outputs=outputs,\r\n            method_name=\"tensorflow/serving/classify\"\r\n        )\r\n    )\r\n\r\n    export_file_path = os.path.join('temp_export', '1')\r\n    builder = tf.saved_model.builder.SavedModelBuilder(export_file_path)\r\n    builder.add_meta_graph_and_variables(\r\n        sess,\r\n        [tf.saved_model.tag_constants.SERVING],\r\n        signature_def_map={tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature}\r\n    )\r\n\r\n    builder.save()\r\n\r\nif __name__ == \"__main__\":\r\n  train()\r\n  export()\r\n```\r\n\r\nThis is the complete output:\r\n```\r\nWARNING:tensorflow:From temp.py:29: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2019-08-28 21:27:01.534749: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100070000 Hz\r\n2019-08-28 21:27:01.539551: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3d285e0 executing computations on platform Host. Devices:\r\n2019-08-28 21:27:01.539602: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-28 21:27:01.550924: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n[[0.30251842 0.69748158]\r\n [0.2578716  0.7421284 ]\r\n [0.22793843 0.77206157]]\r\nWARNING:tensorflow:From temp.py:37: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nWARNING:tensorflow:From temp.py:41: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\r\nWARNING:tensorflow:From temp.py:44: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\r\n\r\nWARNING:tensorflow:From temp.py:52: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\r\n\r\nWARNING:tensorflow:From temp.py:55: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\r\n\r\nWARNING:tensorflow:From temp.py:56: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\r\n```\r\n\r\nThe model is then served using docker via the command:\r\n```\r\ndocker run -p 8505:8500 --mount type=bind,source=$(pwd)/temp_export,target=/models/temp_model -e MODEL_NAME=temp_model -t tensorflow/serving\r\n```\r\nThis is the output of running the docker command:\r\n```\r\n2019-08-29 01:27:32.994250: I tensorflow_serving/model_servers/server.cc:82] Building single TensorFlow model file config:  model_name: temp_model model_base_path: /models/temp_model\r\n2019-08-29 01:27:32.994514: I tensorflow_serving/model_servers/server_core.cc:461] Adding/updating models.\r\n2019-08-29 01:27:32.994538: I tensorflow_serving/model_servers/server_core.cc:558]  (Re-)adding model: temp_model\r\n2019-08-29 01:27:33.094892: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: temp_model version: 1}\r\n2019-08-29 01:27:33.094934: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: temp_model version: 1}\r\n2019-08-29 01:27:33.094951: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: temp_model version: 1}\r\n2019-08-29 01:27:33.094976: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /models/temp_model/1\r\n2019-08-29 01:27:33.094991: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /models/temp_model/1\r\n2019-08-29 01:27:33.095225: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2019-08-29 01:27:33.117391: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:182] Restoring SavedModel bundle.\r\n2019-08-29 01:27:33.117456: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:192] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: /models/temp_model/1/variables/variables.index\r\n2019-08-29 01:27:33.117481: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:285] SavedModel load for tags { serve }; Status: success. Took 22482 microseconds.\r\n2019-08-29 01:27:33.117517: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:101] No warmup data file found at /models/temp_model/1/assets.extra/tf_serving_warmup_requests\r\n2019-08-29 01:27:33.117615: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: temp_model version: 1}\r\n2019-08-29 01:27:33.130660: I tensorflow_serving/model_servers/server.cc:313] Running gRPC ModelServer at 0.0.0.0:8500 ...\r\n[warn] getaddrinfo: address family for nodename not supported\r\n2019-08-29 01:27:33.135024: I tensorflow_serving/model_servers/server.cc:333] Exporting HTTP/REST API at:localhost:8501 ...\r\n[evhttp_server.cc : 237] RAW: Entering the event loop ...\r\n```\r\n\r\nHowever when this is run:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport os\r\nimport subprocess as sp\r\nimport grpc\r\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\r\nfrom tensorflow_serving.apis import predict_pb2\r\n\r\ndef predict():\r\n  channel = grpc.insecure_channel('localhost:8505')\r\n  stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\r\n\r\n  request = predict_pb2.PredictRequest()\r\n  request.model_spec.name = 'temp_model'\r\n  request.model_spec.signature_name = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\r\n\r\n  request.inputs['input'].CopyFrom(\r\n    tf.make_tensor_proto([0, 2, 1])\r\n  )\r\n\r\n  response = stub.Predict(request)\r\n\r\n  print response.outputs\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  predict()\r\n```\r\n\r\nThis error is thrown:\r\n```\r\nWARNING:tensorflow:From temp.py:68: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\r\n\r\nWARNING:tensorflow:From temp.py:71: The name tf.make_tensor_proto is deprecated. Please use tf.compat.v1.make_tensor_proto instead.\r\n\r\n2019-08-29 01:28:35.360196: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:624] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'batch_dims' not in Op<name=GatherV2; signature=params:Tparams, indices:Tindices, axis:Taxis -> output:Tparams; attr=Tparams:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Taxis:type,allowed=[DT_INT32, DT_INT64]>; NodeDef: {{node embedding_lookup}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[{{node embedding_lookup}}]]\r\nTraceback (most recent call last):\r\n  File \"temp.py\", line 82, in <module>\r\n    predict()\r\n  File \"temp.py\", line 74, in predict\r\n    response = stub.Predict(request)\r\n  File \"/home/crsilkworth/temp/venv/local/lib/python2.7/site-packages/grpc/_channel.py\", line 565, in __call__\r\n    return _end_unary_response_blocking(state, call, False, None)\r\n  File \"/home/crsilkworth/temp/venv/local/lib/python2.7/site-packages/grpc/_channel.py\", line 467, in _end_unary_response_blocking\r\n    raise _Rendezvous(state, None, None, deadline)\r\ngrpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with:\r\n\tstatus = StatusCode.INVALID_ARGUMENT\r\n\tdetails = \"NodeDef mentions attr 'batch_dims' not in Op<name=GatherV2; signature=params:Tparams, indices:Tindices, axis:Taxis -> output:Tparams; attr=Tparams:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Taxis:type,allowed=[DT_INT32, DT_INT64]>; NodeDef: {{node embedding_lookup}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[{{node embedding_lookup}}]]\"\r\n\tdebug_error_string = \"{\"created\":\"@1567042115.360651073\",\"description\":\"Error received from peer ipv4:127.0.0.1:8505\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1052,\"grpc_message\":\"NodeDef mentions attr 'batch_dims' not in Op<name=GatherV2; signature=params:Tparams, indices:Tindices, axis:Taxis -> output:Tparams; attr=Tparams:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Taxis:type,allowed=[DT_INT32, DT_INT64]>; NodeDef: {{node embedding_lookup}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\\n\\t [[{{node embedding_lookup}}]]\",\"grpc_status\":3}\"\r\n```\r\n\r\n\r\n**Other info / logs**\r\nresult of pip freeze -l:\r\n```\r\nabsl-py==0.8.0\r\nastor==0.8.0\r\nbackports.weakref==1.0.post1\r\nenum34==1.1.6\r\nfuncsigs==1.0.2\r\nfutures==3.3.0\r\ngast==0.2.2\r\ngoogle-pasta==0.1.7\r\ngrpcio==1.23.0\r\nh5py==2.9.0\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nMarkdown==3.1.1\r\nmock==3.0.5\r\nnumpy==1.16.5\r\nprotobuf==3.9.1\r\nsix==1.12.0\r\ntensorboard==1.14.0\r\ntensorflow==1.14.0\r\ntensorflow-estimator==1.14.0\r\ntensorflow-serving-api==1.14.0\r\ntermcolor==1.1.0\r\nWerkzeug==0.15.5\r\nwrapt==1.11.2\r\n```", "comments": ["@CRSilkworth, \r\nThis is more related to Tf serving Repo. Please post this issue in [Tf serving repo](https://github.com/tensorflow/serving/issues). Thanks!", "ok, posted in tensorflow serving.", "@CRSilkworth, Thanks, Will close the issue here. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32066\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32066\">No</a>\n"]}, {"number": 32065, "title": "Add SaveOptions object with option to whitelist op namespaces. ", "body": "All saved_model saving functions now have an `options` argument where the user may pass in a SaveOptions object.\r\n\r\nPiperOrigin-RevId: 266021878", "comments": []}, {"number": 32064, "title": "Installers for C-API do not include copyright/license information for Tensorflow", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows, Linux and OSX\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n\r\n- TensorFlow installed from (source or binary):\r\nBinary, using the instructions from C-API page at:\r\n(https://www.tensorflow.org/install/lang_c)\r\n\r\n- TensorFlow version: \r\n1.14.0\r\n\r\n- Python version:\r\nN/A\r\n\r\n- Installed using virtualenv? pip? conda?:\r\nInstalled by following instructions at C-API page (https://www.tensorflow.org/install/lang_c)\r\n, basically extracting from tarball or zip file.\r\n\r\n- Bazel version (if compiling from source):\r\nN/A\r\n\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n\r\n- CUDA/cuDNN version:\r\nN/A \r\n\r\n- GPU model and memory:\r\nN/A\r\n\r\n\r\n**Describe the problem**\r\n\r\nThe problem I see is that there is no COPYRIGHT or LICENSE information for the TENSORFLOW library itself. There is a LICENSE file under \\include\\tensorflow\\c\\LICENSE, but that gives all of the LICENSE information for the 3rd party libraries that tensorflow uses, but there is nothing for tensorflow itself.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nWindows:\r\nUnzip the zip file\r\n\r\nLINUX/Darwin\r\nDetar the compressed tarballs\r\n\r\n\r\n**Any other info / logs**\r\nNone", "comments": ["@RochaStratovan, Are you looking for [this](https://github.com/tensorflow/tensorflow/blob/master/LICENSE). ", "Hello @gadagashwini ,\r\n\r\nYes I believe that would be correct. However, I'm not a TF dev. The installers typically have the licensing information included with the installation. This is actually the first one I've encountered that does not.\r\n\r\nThank you.", "@RochaStratovan, You can get the Tensorflow license info by using the command `pip show tensorflow` . Please take a look at screenshot below. Thanks!\r\n![Screenshot from 2019-09-04 15-42-55](https://user-images.githubusercontent.com/48476109/64246722-f6efd680-cf2a-11e9-9511-f701afdacaa9.png)\r\n", "Thank you for the workaround @gadagashwini . This will work for folks that have PIP, python, etc. installed. We are using the C-API, Binary distribution specifically because we don't want/need, all of that underlying stuff that we don't actually need. We just use the Binaries.\r\n\r\nWe are trying to ensure that we follow appropriate licensing requests/guidelines. However, the distribution for the C-API binaries does not have any licensing information for Tensorflow itself included in it. So technically those distros could be viewed as unlicensed.\r\n\r\nI'm pointing out an enhancement that should be made to those binary distributions so that the Tensorflow products and licenses are protected.\r\n\r\nThank you for your consideration.", "@gunan can you triage this?", "Thank you very much for reporting this.\r\nYou are correct, the license file was missing in some of the distributables.\r\nThe fixes are merged into master. https://github.com/tensorflow/tensorflow/pull/32852 will fix this in 2.0 branch. https://github.com/tensorflow/tensorflow/pull/32853 will fix this in 1.15 branch. We will push new binaries with the fixes within 1 week.\r\n", "Closing this issue since associated PR has been merged. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32064\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32064\">No</a>\n"]}, {"number": 32063, "title": "MKL DNN: Fixing deprecation test for MKL DNN when OpenMP threads are set", "body": "This unit test just tests if proper deprecation warning is raised using logging.warning call. Generally when tf.keral.backend.get_session() is called, one deprecation message is shown (to using the v2 version, that is, tf.compat.v1.keras.backend.get_session). But, if OMP_NUM_THREADS is set through environment, the tf.keras.backend.get_session() raises two separate logging.warning messages. These are: 1) usual deprecation warning as above, and 2) warning \"how to set effectively set OMP_NUM_THREADS for Keras API. Thus, the warning call count becomes 2. The test is edited to incorporate this scenario when OMP_NUM_THREADS is set.", "comments": ["Hi @annarev if possible, please review this PR. It is a small one, but need it to fix a unit text for Intel MKL DNN."]}, {"number": 32062, "title": "Fix typo in enabling auto-clustering via python", "body": "", "comments": []}, {"number": 32061, "title": "Cherrypick: add an `enter_master_device` flag in tf.config.experimental_connect_to_cluster API.", "body": "", "comments": []}, {"number": 32060, "title": "ValueError: Dimensions must be equal, but are 19 and 39 for 'metrics/accuracy/Equal' (op: 'Equal') with input shapes: [?,19], [?,39].", "body": "**System information**\r\n- OS Platform and Distribution : Win10\r\n- TensorFlow installed from (source or binary): pip3 install tensorflow==2.0.0-rc0 \r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nI have try to make a chatbot based on a transformer model.\r\nBut when I try to train the model, i have an error :\r\n`ValueError: Dimensions must be equal, but are 19 and 39 for 'metrics/accuracy/Equal' (op: 'Equal') with input shapes: [?,19], [?,39].`\r\n\r\nThe entire error code: \r\n\r\n> PS C:\\Users\\Neicureuil\\Workspace\\Python\\ChatBot Prototype> py .\\train.py\r\nEpoch 1/10\r\n      1/Unknown - 3s 3s/stepTraceback (most recent call last):\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1610, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 19 and 39 for 'metrics/accuracy/Equal' (op: 'Equal') with input shapes: [?,19], [?,39].\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \".\\train.py\", line 38, in <module>\r\n    chatbot_model.fit(dataset_var, epochs=EPOCHS)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 734, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 427, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 370, in _initialize\r\n    *args, **kwds))\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1847, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2147, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2038, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 320, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 73, in distributed_function\r\n    per_replica_function, args=(model, x, y, sample_weights))\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\", line 760, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\", line 1787, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\", line 2132, in _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 264, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\", line 315, in train_on_batch\r\n    model, outs, targets, sample_weights=sample_weights, masks=masks)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\", line 74, in _eager_metrics_fn\r\n    skip_target_masks=model._prepare_skip_target_masks())\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2027, in _handle_metrics\r\n    target, output, output_mask))\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1978, in _handle_per_output_metrics\r\n    metric_fn, y_true, y_pred, weights=weights, mask=mask)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\", line 1066, in call_metric_function\r\n    return metric_fn(y_true, y_pred, sample_weight=weights)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 194, in __call__\r\n    replica_local_fn, *args, **kwargs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\distribute\\distributed_training_utils.py\", line 1135, in call_replica_local_fn\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 177, in replica_local_fn\r\n    update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\metrics_utils.py\", line 75, in decorated\r\n    update_op = update_state_fn(*args, **kwargs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 582, in update_state\r\n    matches = self._fn(y_true, y_pred, **self._fn_kwargs)\r\n  File \"C:\\Users\\Neicureuil\\Workspace\\Python\\ChatBot Prototype\\model.py\", line 195, in accuracy\r\n    accuracy_ = tf.metrics.SparseCategoricalAccuracy()(y_true, y_pred)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 194, in __call__\r\n    replica_local_fn, *args, **kwargs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\distribute\\distributed_training_utils.py\", line 1135, in call_replica_local_fn\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 177, in replica_local_fn\r\n    update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\metrics_utils.py\", line 75, in decorated\r\n    update_op = update_state_fn(*args, **kwargs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 582, in update_state\r\n    matches = self._fn(y_true, y_pred, **self._fn_kwargs)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 2787, in sparse_categorical_accuracy\r\n    return math_ops.cast(math_ops.equal(y_true, y_pred), K.floatx())\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\", line 3628, in equal\r\n    \"Equal\", x=x, y=y, name=name)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 793, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 548, in create_op\r\n    compute_device)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3429, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1773, in __init__\r\n    control_input_ops)\r\n  File \"C:\\Users\\Neicureuil\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1613, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Dimensions must be equal, but are 19 and 39 for 'metrics/accuracy/Equal' (op: 'Equal') with input shapes: [?,19], [?,39].\r\n\r\n\r\n**Code to reproduce the issue**\r\ndataset.py: https://pastebin.com/pTVqZaiG\r\nmodel.py: https://pastebin.com/pgnWRhEt\r\ntrain.py: https://pastebin.com/FwJK9yDb\r\n", "comments": ["@neicureuil \r\nI am not able to see the files in the links you have provided.Thanks!", "> \r\n> \r\n> @neicureuil\r\n> I am not able to see the files in the links you have provided.Thanks!\r\n\r\nFixed", "I am getting the below error while trying to reproduce the issue.\r\n\r\n`FileNotFoundError: [Errno 2] No such file or directory: './dataset.txt'`.\r\n\r\nCan you please provide the `dataset.txt` file (or dummy data, if your data is confidential) so we can reproduce the issue and try for the resolution. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32060\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32060\">No</a>\n"]}, {"number": 32059, "title": "[r1.15:Cherrypick] Fix CTC compilation on MacOS: make kLogZero a templated constexpr function.", "body": "PiperOrigin-RevId: 264729008", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32059) for more info**.\n\n<!-- need_sender_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32059) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 32058, "title": "user can't add_loss w/ input dependence in custom layers in eager mode? (or docs unclear)", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThanks for making tensorflow. \r\n\r\nI want to make custom layers which handle their own losses to write less code \r\n(ex, layerwise reconstruction + kl divergence losses in stacked autoencoders)\r\nThe [docs](https://github.com/tensorflow/tensorflow/blob/a2e398f299e62559118f3e59bd8ef11925cdc449/tensorflow/python/keras/engine/base_layer.py#L1083) indicate this isn't possible in eager mode, which sucks because Eager is default in 2.0... \r\n\r\nI like tf.function / graph mode but frankly it's a different dialect of TF which forces users to waste time translating code into \"graph dialect\" ... so i want to use Eager, even if performance is worse, it's better than debugging tf.function. \r\n\r\nwithout modular custom losses, users must write complicated/annoying training loops to apply correct loss function to correct combination of inputs and outputs from a model (and models return arrays so ordering gets complicated... can we name model outputs?)\r\n\r\nwould it be possible to permit TF users to define module-specific input-dependent losses for custom layers in Eager mode? \r\n\r\n**Will this change the current api? How?**\r\nusers could add custom input-dependent losses to custom layers. \r\nthus, users can dramatically simplify training loops\r\n\r\n**Who will benefit with this feature?**\r\n2.0 keras users with custom layers that use custom losses\r\n\r\n**Any Other info.**\r\nI want to make these \"coder\" bricks into Layers which handle reconstruction + KL divergences in a modular way\r\n```\r\ndef get_sensor_and_actuator(agent, in_spec):\r\n    if in_spec.rank is 3:\r\n        sensor = use_image_sensor(agent)\r\n        actuator = use_image_actuator(agent)\r\n    elif in_spec.rank is 2 and in_spec.shape[1] is None:\r\n        sensor = use_ragged_sensor(agent, in_spec)\r\n        actuator = use_ragged_actuator(agent, in_spec)\r\n    else:\r\n        sensor = use_resizer(agent.code_spec.shape)\r\n        actuator = use_resizer(in_spec.shape)\r\n    return sensor, actuator\r\n\r\n\r\ndef use_coder(agent, in_spec):\r\n    log('use_coder', in_spec, color=\"blue\")\r\n    normalizer = norm = use_norm()\r\n\r\n    if in_spec.rank is 3:\r\n        h, w = get_hw(in_spec.shape)\r\n        hw = [h, w]\r\n\r\n        def resize_then_norm(x):\r\n            x = tf.image.resize(x, hw)\r\n            return norm(x)\r\n        normalizer = resize_then_norm\r\n\r\n    coordinator = L.Lambda(concat_coords)\r\n    sensor, actuator = get_sensor_and_actuator(agent, in_spec)\r\n\r\n    def call(x):\r\n        normie = normalizer(x)\r\n        normie_w_coords = coordinator(normie)\r\n        code = sensor(normie_w_coords)\r\n        reconstruction = actuator(code)\r\n        return normie, code, reconstruction\r\n    return call\r\n```\r\nalas, since i can't use input-dependent modular losses, I need to\r\n1. design my model to return a bunch of extra outputs\r\n2. keep track of the order of those outputs. which one is a code? which one is a reconstruction?\r\n3. unpack the outputs of the model to organize codes, reconstructions, normalized inputs, and \"actual\" desired outputs\r\n4. organize pairs of these normalized inputs and reconstructions\r\n5. loop over the pairs and apply a loss function\r\n6. append error terms to a list of losses.\r\n\r\nIs there some reason we cannot add a loss function to the layers themselves and avoid all this BS?\r\n\r\nthis would REALLY simplify modular agents... please advise!", "comments": ["this would be sweet because then you could do more with less code:\r\n```\r\nclass Coder(Layer):\r\n    def __init__(self, in_spec, out_spec):\r\n          super(Coder, self).__init__()\r\n          self.norm = InstanceNormalization()\r\n          self.sensor = Dense(out_spec.size)\r\n          self.actuator = Dense(in_spec.size)\r\n\r\n     def call(self, x):\r\n            normie = self.norm(x)\r\n            code = self.sensor(normie)\r\n            reconstruction = self.actuator(code)\r\n            self.add_loss(MSLE(normie, reconstruction))\r\n            return code\r\n```", "the difference between this:\r\n```\r\ndef build_task_model(G, agent):\r\n    outs = [get_out(G, agent, id, task_model=True)\r\n            for id in list(G.predecessors(\"sink\"))]\r\n    in_nodes = [G.node[id] for id in list(G.successors('source'))]\r\n    reconstructions = [n['reconstruction'] for n in in_nodes]\r\n    normies = [n['normie'] for n in in_nodes]\r\n    outputs = normies + reconstructions + outs\r\n    inputs = [n['input'] for n in in_nodes]\r\n    n = ['normie'] * len(normies)\r\n    r = ['reconstruction'] * len(reconstructions)\r\n    o = ['action'] * len(outs)\r\n    roles = n + r + o\r\n    return K.Model(inputs, outputs), roles\r\n\r\n    ...inside training loop:\r\n                normies, reconstructions, out = self.unpack(\r\n                    self.task.roles, outputs)\r\n                    reconstruction_errors = self.compute_errors(\r\n                        normies, reconstructions)\r\n                    [losses.append(e) for e in reconstruction_errors]\r\n\r\n    @staticmethod\r\n    def unpack(roles, outputs):\r\n        unpacked = AttrDict()\r\n        for n, (role, output) in enumerate(zip(roles, outputs)):\r\n            if role not in unpacked.keys():\r\n                unpacked[role] = [output]\r\n            else:\r\n                unpacked[role].append(output)\r\n        return (unpacked.normie,\r\n                unpacked.reconstruction,\r\n                unpacked.action[0])\r\n\r\n    def compute_errors(self, y_true_list, y_pred_list):\r\n        errors = []\r\n        for n, (y_true, y_pred) in enumerate(zip(y_true_list, y_pred_list)):\r\n            errors.append(self.regresser_loss(y_true, y_pred))\r\n        return errors\r\n```\r\nand this\r\n```\r\ndef build_inner_model(G, agent):\r\n    outs = [get_out(G, agent, id) for id in list(G.predecessors(\"sink\"))]\r\n    inputs = [G.node[id]['input'] for id in list(G.successors('source'))]\r\n    return K.Model(inputs, outs)\r\n\r\nlosses = model.losses + [self.loss_fn(y_true, y_pred]\r\n```\r\n`\u00af\\_(\u30c4)_/\u00af`", "bump @ymodak @oanush @fchollet @dynamicwebpaige is this a docs issue or a feature request?\r\n\r\nTLDR: to simplify code, I want to make a \"coder\" layer which computes the reconstruction loss in the call method; but i'm using Eager, and the docs indicate this is impossible ... \r\n\r\nhere's what I want to do:\r\n```\r\nclass Coder(Layer):\r\n    def __init__(self, in_spec, out_spec):\r\n          super(Coder, self).__init__()\r\n          self.norm = InstanceNormalization()\r\n          self.sensor = Dense(out_spec.size)\r\n          self.actuator = Dense(in_spec.size)\r\n\r\n     def call(self, x):\r\n            normie = self.norm(x)\r\n            code = self.sensor(normie)\r\n            reconstruction = self.actuator(code)\r\n            **self.add_loss(MSLE(normie, reconstruction))**  # <--- does this work?\r\n            return code\r\n```\r\nnote: i'm using the functional method to build a keras model. The goal of the coder is to learn a latent representation of the inputs. It's much more complicated to put this loss in the training loop\r\n\r\nref:\r\nhttps://www.tensorflow.org/beta/guide/keras/custom_layers_and_models#layers_recursively_collect_losses_created_during_the_forward_pass\r\n\r\n`inputs: Ignored when executing eagerly.` \r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Layer#add_loss", "```\r\nimport tensorflow as tf\r\n\r\nfrom nature import Resizer, ConcatCoords2D, AllAttention, Norm\r\nfrom tools import concat_coords\r\n\r\nK = tf.keras\r\nL = K.layers\r\n\r\nL1 = 1e-3\r\nL2 = 1e-3\r\n\r\nclass Predictor(L.Layer):\r\n    \"\"\"generates inputs and returns a surprise value\"\"\"\r\n\r\n    def __init__(self, agent, in_spec):\r\n        super(Predictor, self).__init__()\r\n        self.batch_size = agent.batch_size\r\n        self.out_spec = agent.code_spec\r\n        self.in_spec = in_spec\r\n\r\n    def build(self, shape):\r\n        self.regularize = L.ActivityRegularization(l1=L1, l2=L2)\r\n        self.resizer = Resizer(self.out_spec.shape)\r\n        self.coordinator = ConcatCoords2D()\r\n        self.attention = AllAttention(self.batch_size)\r\n        self.subtract = L.Subtract()\r\n        self.noise_norm = Norm()\r\n        self.x_norm = Norm()\r\n        self.built = True\r\n\r\n    @tf.function\r\n    def call(self, x):\r\n        noise = tf.random.normal(tf.shape(x))\r\n        noise = self.coordinator(noise)\r\n        noise = self.noise_norm(noise)\r\n        p = self.attention(noise)\r\n        x = self.coordinator(x)\r\n        p = tf.reshape(p, tf.shape(x))\r\n        x = self.x_norm(x)\r\n        surprise = self.subtract([x, p])\r\n        **self.add_loss(surprise)  \r\n        # self.add_loss(K.losses.MSLE(x, p))\r\n        # surprise = self.regularize(surprise) \r\n        # both fail here, in both eager and tf.function** \r\n        return self.resizer(surprise)\r\n\r\n    def compute_output_shape(self, shape):\r\n        return self.out_spec\r\n```\r\ncc @omalleyt12 @ymodak @oanush @fchollet \r\ntrying this custom layer for predictive coding, but it gives errors like:\r\n\r\n> TypeError: An op outside of the function building code is being passed\r\n> a \"Graph\" tensor. It is possible to have Graph tensors\r\n> leak out of the function building context by including a\r\n> tf.init_scope in your function building code.\r\n> For example, the following function will fail:\r\n>   @tf.function\r\n>   def has_init_scope():\r\n>     my_constant = tf.constant(1.)\r\n>     with tf.init_scope():\r\n>       added = my_constant * 2\r\n> The graph tensor has name: dense_1/ActivityRegularizer/add_1:0\r\n^^^ if training loop is eager\r\n\r\nwith @tf.function on training loop:\r\n\r\n>     InaccessibleTensorError: The tensor 'Tensor(\"resizer/dense/ActivityRegularizer/Cast:0\", shape=(), dtype=float32)' \r\n>     cannot be accessed here: it is defined in another function or code block. Use return values, \r\n>     explicit Python locals or TensorFlow collections to access it. \r\n>     Defined in: FuncGraph(name=call, id=139631001707744);\r\n>     accessed from: FuncGraph(name=reduce_reduce_body, id=139631001171336).\r\n\r\nhere is the training loop:\r\n```\r\n    @tf.function\r\n    def run_data_session(self, data, model, loss_fn):\r\n        for step, (image, label) in data.enumerate():\r\n            with tf.GradientTape() as tape:\r\n                loss = loss_fn(label, model(image))\r\n            gradients = tape.gradient(\r\n                **[loss] + model.losses**, model.trainable_variables)\r\n            self.optimizer.apply_gradients(\r\n                zip(gradients, model.trainable_variables))\r\n            tf.print(step, loss)\r\n```\r\n `model.losses` triggers these errors, if we don't attempt to get those losses, then it works ", "hey, it works if tf.function is on the training loop and not on the predictor call", "I'm hitting similar things but using tfp as well so not sure if there is a good pattern to avoid this.", "If I'm not mistaken, this also means that this tutorial does not run in eager mode: https://www.tensorflow.org/tutorials/generative/deepdream\r\n(Hits InaccessibleTensorErrors as well)", "Having the same issue, it seems related to #32477, any news on this?", "@bionicles Thanks for the issue!\r\n\r\nYes, this is supported, the issue is indeed the `tf.function` wrapping the Layer.call. You can remove this, as `Model.fit` runs inside a `tf.function` anyway, and for custom training loops it's more performant to wrap your whole training step or training loop in a `tf.function`\r\n\r\nWe're in the process of updating documentation, this is one \"gotcha\" we should document better\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32058\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32058\">No</a>\n", "Trying to briefly summarize this issue, which I also ran into:\r\n- Layer.add_loss(...) does not work in eager mode\r\n- it works in graph mode, but _not_ if Layer.call is decorated with tf.function\r\n\r\nShould we consider this a satisfying state of things? Having code that runs in graph mode but fails in eager mode is problematic for a number of reasons. E.g., it makes debugging and testing complicated  (you cannot even call Layer.call in a unit test since it fails in eager mode and cannot be decorated with tf.function)."]}, {"number": 32057, "title": "[r2.0:Cherrypick] `is_keras_tensor` in the Keras backend API.", "body": "", "comments": []}, {"number": 32056, "title": "Trying to install Tensorflow on a locked-down terminal server with no internet access", "body": "We have a terminal server that is locked down and would not have any internet connection.  We have researchers that go to this system and some of them have expressed interest in using TensorFlow when doing their analysis.\r\n\r\nI have only seen a few areas where people have tried this.  So I would need to get definitive answeres to the following:\r\n1.  Can this be installed in this fashion?\r\n2.  If so, what is the best way to go about this?\r\n3.  If not, is there just a particular URL/website that the product needs to go to all the time that could be whitelisted withing our system?", "comments": ["@afhlamanna, Please provide the Platform details in which your trying to install? or Do you have pip or conda installed on your terminal? ", "Platform is a Windows 10 Terminal Server.  64-bit.  I have no idea if pip or conda is installed.  Please explain what pip is and what conda is.", "@afhlamanna, Pip and Conda are installer package through which we can install Tensorflow. Please follow the steps mentioned in the [Tensorflow website](https://www.tensorflow.org/install/pip) to install. Thanks! ", "We cannot install Pip and Conda onto the terminal server.  Is there any other way(s) to get Tensorflow downloaded as an executable, and then brought over to our terminal server and installed from there?", "Get a computer that is as much similar to the locked down one as possible. Then, follow the following steps:\r\n\r\n```console\r\nvirtualenv .\r\nsource bin/activate\r\npython -m pip install --upgrade pip\r\npython -m pip install tensorflow tensorflow_estimator\r\n```\r\n\r\nThen, copy all the files in this directory to the terminal server.\r\n\r\nThis assumes the terminal server has python installed. Without that, there's no way you can use TensorFlow anyway", "Also, this question is best answered on Stack Overflow, as it is totally not related to development related bugs/issues/features.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32056\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32056\">No</a>\n", "Those instructions, where are you having someone do those?\r\nFrom a command prompt?"]}, {"number": 32055, "title": "TFLite build for rpi armv6 broken", "body": "Cross compiling as per https://www.tensorflow.org/lite/guide/build_rpi#cross-compile_for_raspberry_pi\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Debain):\r\n- TensorFlow installed from (source or binary): github\r\n- TensorFlow version: v1.13.2\r\n- Installed using virtualenv? pip? conda?: docker tensorflow/tensorflow:nightly-devel\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n\r\n```\r\n$ git clone https://github.com/tensorflow/tensorflow\r\n$ cd tensorflow/\r\n$ git checkout v1.13.2\r\n$ sed -i 's/armv7l/armv6/g' ./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n$ ./tensorflow/lite/tools/make/download_dependencies.sh\r\n$ ./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n+ set -e                                                                                                                                                                                                          \r\n+++ dirname ./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n++ cd ./tensorflow/lite/tools/make\r\n++ pwd                                                                                                                                                                                                            \r\n+ SCRIPT_DIR=/root/tensorflow/tensorflow/lite/tools/make                                                                                                                                                          \r\n+ cd /root/tensorflow/tensorflow/lite/tools/make/../../../..                                                                                                                                                      \r\n+ CC_PREFIX=arm-linux-gnueabihf-                                                                                                                                                                                  \r\n+ make -j 3 -f tensorflow/lite/tools/make/Makefile TARGET=rpi TARGET_ARCH=armv6\r\n/bin/sh: 1: [[: not found\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG  --std=c++11 -march=armv6 -mfpu=vfp -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/root/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/root/tensorflow/te\r\nnsorflow/lite/tools/make/../../../../../../ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/root/tensorflow/tensorflow/lite/tools/make/do\r\nwnloads/absl -I/root/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/root/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/root/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/s\r\nrc -I/root/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/allocation.cc -o /root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow\r\n/lite/allocation.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG  --std=c++11 -march=armv6 -mfpu=vfp -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/root/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/root/tensorflow/te\r\nnsorflow/lite/tools/make/../../../../../../ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/root/tensorflow/tensorflow/lite/tools/make/do\r\nwnloads/absl -I/root/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/root/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/root/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/s\r\nrc -I/root/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/arena_planner.cc -o /root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorf\r\nlow/lite/arena_planner.o\r\narm-linux-gnueabihf-gcc -O3 -DNDEBUG  -march=armv6 -mfpu=vfp -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/root/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/root/tensorflow/tensorflow/lit\r\ne/tools/make/../../../../../../ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/root/tensorflow/tensorflow/lite/tools/make/downloads/absl\r\n -I/root/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/root/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/root/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/root/t\r\nensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/c/c_api_internal.c -o /root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/c\r\n/c_api_internal.o\r\nIn file included from tensorflow/lite/c/c_api_internal.c:16:0:\r\n./tensorflow/lite/c/c_api_internal.h:60:34: warning: 'struct TfLiteContext' declared inside parameter list will not be visible outside of this definition or declaration\r\n   TfLiteStatus (*Refresh)(struct TfLiteContext* context);\r\n                                  ^~~~~~~~~~~~~\r\nIn file included from /usr/arm-linux-gnueabihf/include/stdio.h:859:0,\r\n                 from tensorflow/lite/c/c_api_internal.c:18:\r\n/usr/arm-linux-gnueabihf/include/bits/stdio.h: In function 'getchar':\r\n/usr/arm-linux-gnueabihf/include/bits/stdio.h:45:1: sorry, unimplemented: Thumb-1 hard-float VFP ABI\r\n {\r\n ^\r\ntensorflow/lite/tools/make/Makefile:179: recipe for target '/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/c/c_api_internal.o' failed\r\nmake: *** [/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/c/c_api_internal.o] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\nIn file included from /usr/arm-linux-gnueabihf/include/stdio.h:859:0,\r\n                 from /usr/arm-linux-gnueabihf/include/c++/7/cstdio:42,\r\n                 from ./tensorflow/lite/allocation.h:20,\r\n                 from tensorflow/lite/allocation.cc:16:\r\n/usr/arm-linux-gnueabihf/include/bits/stdio.h: In function 'int getchar()':\r\n/usr/arm-linux-gnueabihf/include/bits/stdio.h:44:14: sorry, unimplemented: Thumb-1 hard-float VFP ABI\r\n getchar (void)\r\n              ^\r\nIn file included from /usr/arm-linux-gnueabihf/include/c++/7/bits/stl_algobase.h:62:0,\r\n                 from /usr/arm-linux-gnueabihf/include/c++/7/memory:62,\r\n                 from ./tensorflow/lite/arena_planner.h:18,\r\n                 from tensorflow/lite/arena_planner.cc:15:\r\n/usr/arm-linux-gnueabihf/include/c++/7/ext/type_traits.h: In function 'bool __gnu_cxx::__is_null_pointer(std::nullptr_t)':                                                                                        \r\n/usr/arm-linux-gnueabihf/include/c++/7/ext/type_traits.h:162:35: sorry, unimplemented: Thumb-1 hard-float VFP ABI                                                                                                 \r\n   __is_null_pointer(std::nullptr_t)\r\n                                   ^\r\ntensorflow/lite/tools/make/Makefile:175: recipe for target '/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/allocation.o' failed                                                    \r\nmake: *** [/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/allocation.o] Error 1                                                                                                    \r\ntensorflow/lite/tools/make/Makefile:175: recipe for target '/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/arena_planner.o' failed                                                 \r\nmake: *** [/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv6/obj/tensorflow/lite/arena_planner.o] Error 1        \r\n```\r\n", "comments": ["Note the same error happens on HEAD.", "I have the same issue when cross comipling for raspberry pi zero on version, however if I change the armv6 compiler  options in \r\n\r\ntensorflow/lite/tools/make/targets/rpi_makefile.inc\r\n\r\nto the following\r\n\r\nifeq ($(TARGET_ARCH), armv6)\r\n    CXXFLAGS += \\\r\n      -marm \\\r\n      -mfpu=vfp \\\r\n      -mlong-calls \\\r\n      -mthumb-interwork \\\r\n      -mfloat-abi=hard\r\n\r\n    CCFLAGS += \\\r\n      -marm \\\r\n      -mfpu=vfp \\\r\n      -mlong-calls \\\r\n      -mthumb-interwork \\\r\n      -mfloat-abi=hard\r\n\r\n    LDFLAGS := \\\r\n      -Wl,--no-export-dynamic \\\r\n      -Wl,--exclude-libs,ALL \\\r\n      -Wl,--gc-sections \\\r\n      -Wl,--as-needed \\\r\n      -mfpu=vfp \\\r\n      -mlong-calls \\\r\n      -mfloat-abi=hard \\\r\n      -mthumb-interwork \r\n  endif\r\n\r\nI can compile without errors (and just a couple of warnings).\r\n\r\nGiven that it seems to be difficult (or just very slow) to compile bazel on a raspberry pi zero due to the lack of memory, I think that for a PI Zero, cross compiling is the only way forward, but it would be nice to have a way to generate a python wheel as well. The main ci_build process does this for the full fat tensorflow package, (and i can run one of those on the pi zero), but I'd like to just use tensorflow-lite for my inferencing.  \r\n\r\n", "In  attempting to reproduce my results I note that I also needed  to edit the following file which appears to hard code the TARGET_ARCH parameter\r\n\r\n./tensorflow/lite/tools/make/build_rpi_lib.sh \r\n\r\nTARGET_ARCH=armv6\r\n\r\nI now have the following as the final line of that file (whcih hard codes it for armv6 instead of armv7\r\n\r\nCC_PREFIX=arm-linux-gnueabihf- make -j 3 -f tensorflow/lite/tools/make/Makefile TARGET=rpi TARGET_ARCH=armv6\r\n\r\n\r\n``\r\n\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "You can also use CMake to build ARMv6 images.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake_arm#build_for_raspberry_pi_zero_armv6\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32055\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32055\">No</a>\n"]}, {"number": 32054, "title": "TFLite v1.14.0 download_dependencies.sh is broken", "body": "- OS Platform and Distribution (e.g., Linux Debian):\r\n- TensorFlow installed from (source or binary): github\r\n- TensorFlow version: v1.14.0\r\n- Installed using virtualenv? pip? conda?: docker tensorflow/tensorflow:nightly-devel\r\n\r\n```\r\n$ git clone https://github.com/tensorflow/tensorflow\r\nCloning into 'tensorflow'...\r\nremote: Enumerating objects: 38, done.\r\nremote: Counting objects: 100% (38/38), done.\r\nremote: Compressing objects: 100% (37/37), done.\r\nremote: Total 668742 (delta 8), reused 29 (delta 1), pack-reused 668704\r\nReceiving objects: 100% (668742/668742), 378.17 MiB | 28.86 MiB/s, done.\r\nResolving deltas: 100% (542405/542405), done.\r\nChecking out files: 100% (20047/20047), done.\r\n$ cd tensorflow/ \r\n$ git checkout v1.14.0\r\nChecking out files: 100% (8943/8943), done.\r\nNote: checking out 'v1.14.0'.\r\n\r\nYou are in 'detached HEAD' state. You can look around, make experimental\r\nchanges and commit them, and you can discard any commits you make in this\r\nstate without impacting any branches by performing another checkout.\r\n\r\nIf you want to create a new branch to retain commits you create, you may\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n\r\n  git checkout -b <new-branch-name>\r\n\r\nHEAD is now at 87989f6959 Add Sergii Khomenko to contributor list\r\n$ ./tensorflow/lite/tools/make/download_dependencies.sh\r\ndownloading http://mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/a0d250e79c79.tar.gz\r\n\r\ngzip: stdin: not in gzip format\r\ntar: Child returned status 1\r\ntar: Error is not recoverable: exiting now\r\n```", "comments": ["@petewarden, @dansitu, could you please take a lok", "Thanks for the report Since TensorFlow Lite for Microcontrollers is experimental and continually evolving, you should use the latest code at HEAD rather than checking out a particular branch.\r\n\r\nIn the latest code, download_dependencies.sh is deprecated (with a warning message), since all required dependencies are downloaded by the makefile. Let me know if there's some documentation that instructed you to use download_dependencies.sh, so we can update it.", "Oh. Yes. That would be https://www.tensorflow.org/lite/guide/build_rpi", "Sadly building from HEAD is also broken (at least for rpi armv6).\r\nSee https://github.com/tensorflow/tensorflow/issues/32055#issuecomment-525857017", "Also default (armv7l) rpi build is broken at HEAD: https://github.com/tensorflow/tensorflow/issues/32055#issuecomment-525862626", "Apologies, I misread the paths and thought this was in reference to TensorFlow Lite for Microcontrollers (as opposed to TF Lite generally). In that case, I take back my previous comment!\r\n\r\n@petewarden, it looks like this mirror URL is no longer working:\r\n\r\nhttp://mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/a0d250e79c79.tar.gz\r\n", "edit download_dependencies.sh, then change 'http.*bitbucket.org' to 'https.*bitbucket.org', It worked;", "URLs are all updated. Let me close this.\r\nPlease, feel free to reopen if the problem still exists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32054\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32054\">No</a>\n", "FYI, updated URLs are available since TFLite v1.15."]}, {"number": 32053, "title": "Tensorflow, Windows Server 2019 and WSL", "body": "**System information**\r\n- Windows Server 2019 Essentials and Ubuntu 18.04 LTS in WSL\r\n- TensorFlow installed with pip3 install\r\n- TensorFlow version: All versions above 1.6.0 throw an error\r\n- Python version: 3.6.8\r\n- Installed using pip\r\n- Only CPU\r\n\r\nI am trying to use this GitHub repository: https://github.com/begeekmyfriend/tacotron. And all versions above 1.6.0 don't work. I get an error: core dumped. If I install older versions I get different errors like this: `AttributeError: module 'tensorflow.python.ops.rnn_cell_impl' has no attribute 'assert_like_rnncell'` -> On version 1.5.0\r\nGoogle search didn't helped.\r\n\r\nI ran this command:  `sudo python3 train.py`\r\n\r\nLog after I run command above:\r\n```\r\nuser@SERVER:/mnt/d/tts/tacotron-master$ sudo python3 train.py\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nCheckpoint path: ./logs-tacotron/model.ckpt\r\nLoading training data from: ./training/train.txt\r\nUsing model: tacotron\r\nHyperparameters:\r\n  adam_beta1: 0.9\r\n  adam_beta2: 0.999\r\n  attention_depth: 128\r\n  batch_size: 32\r\n  cleaners: english_cleaners\r\n  decay_learning_rate: True\r\n  decoder_depth: 1024\r\n  embed_depth: 512\r\n  encoder_depth: 256\r\n  fmax: 7600\r\n  fmin: 125\r\n  frame_length_ms: 50\r\n  frame_shift_ms: 12.5\r\n  griffin_lim_iters: 60\r\n  initial_learning_rate: 0.001\r\n  max_abs_value: 4\r\n  max_frame_num: 1000\r\n  max_iters: 300\r\n  min_level_db: -100\r\n  num_freq: 1025\r\n  num_mels: 160\r\n  outputs_per_step: 5\r\n  postnet_depth: 512\r\n  power: 1.2\r\n  preemphasis: 0.97\r\n  prenet_depths: [256, 256]\r\n  ref_level_db: 20\r\n  reg_weight: 1e-06\r\n  sample_rate: 24000\r\n  use_cmudict: False\r\nLoaded metadata for 32 examples (0.09 hours)\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 157, in <module>\r\n    main()\r\n  File \"train.py\", line 153, in main\r\n    train(log_dir, args)\r\n  File \"train.py\", line 66, in train\r\n    model.initialize(feeder.inputs, feeder.input_lengths, feeder.mel_targets, feeder.linear_targets, feeder.stop_token_targets, global_step)\r\n  File \"/mnt/d/tts/tacotron-master/models/tacotron.py\", line 77, in initialize\r\n    CustomDecoder(decoder_cell, helper, decoder_init_state),\r\n  File \"/mnt/d/tts/tacotron-master/models/custom_decoder.py\", line 46, in __init__\r\n    rnn_cell_impl.assert_like_rnncell(type(cell), cell)\r\nAttributeError: module 'tensorflow.python.ops.rnn_cell_impl' has no attribute 'assert_like_rnncell'\r\n```\r\n", "comments": ["@nikigre,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Hi!\r\nThe code for train process is here: https://github.com/begeekmyfriend/tacotron/blob/master/train.py\r\nI am just wondering is there a problem with WSL and Windows Server 2019? Because similar code worked okay on my laptop running Windows 10 in WSL.", "@nikigre, I tried only importing the `from tensorflow.python.ops import rnn_cell_impl` with Tensorflow 1.14.0. I could import without any issue. Just to verify did you check with Tf latest version. Thanks ", "Hi!\r\nYes, I checked. I checked with versions from 1.2.0 to 1.9.0 and the newest. And above 1.6.0 I get an error about core dumped.", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32053\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32053\">No</a>\n"]}, {"number": 32052, "title": "Linear RAM memory increase with Dataset and Estimator with epoch loops", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below):  tensorflow-gpu==2.0.0-rc0\r\n- Python version: 3.7.1\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: Cuda 10.1 / cuDNN 7.6\r\n- GPU model and memory: GeForce GTX 1060 Mobile 6GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen using Dataset with Estimator, the memory foot print of RAM keeps raising when estimator's train and evaluate APIs are called in loop.\r\n\r\n**Describe the expected behavior**\r\nRAM usage should not increase with epochs.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nPlease find the source code @ https://gist.github.com/Mageswaran1989/facc3fc2a003807d029a914c721629db\r\n\r\n[Update] My latest test case @ https://github.com/dhiraa/tf_issue_32052\r\n\r\nStackOverflow Rereference :  https://stackoverflow.com/questions/55211315/memory-leak-with-tf-data\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[Updated the graph]\r\n![tf_memory_test](https://user-images.githubusercontent.com/3304549/63917881-30bb6b80-ca59-11e9-8598-ebdca798dff5.png)\r\n", "comments": ["@Mageswaran1989 thank you for the detailed instructions to reproduce the issue.\r\n\r\n1) I believe your graph is mislabeled and the measurements are in MB not GBs.\r\n\r\n2) The increase that happens between 1st epoch and 25th is sub linear -- if you draw a line from 1st epoch memory usage to 25th epoch memory usage, the graph curve will be above that line.\r\n\r\n3) My hypothesis is that the marginal memory increase is due to per epoch tracing of functions (the lifetime of the traced objects is global).", "@goldiegadde Thanks for the quick response.\r\n\r\nThe [Gist script](https://gist.github.com/Mageswaran1989/facc3fc2a003807d029a914c721629db) I posted here replicates my setup with dummy data and dummy model.\r\n\r\n[Update] My latest test case @ https://github.com/dhiraa/tf_issue_32052\r\n\r\nWhen actual image data and heat map used with Resnet50 model architecture, the memory increase slowly for each epoch and eats up the whole RAM.\r\n\r\nYou can find the graph from my setup, for 10 epochs. If I train with 100 epochs it gets killed by OS in the middle.\r\n\r\n![east_10_iterations](https://user-images.githubusercontent.com/3304549/63912577-f21eb480-ca4a-11e9-847e-2538e80e1e3e.png)\r\n\r\nI even tried calling explicit `gc.collect()`, but no use.\r\n\r\nIn my observation, I doubt on the objects (tuples, list, dict etc.,) created as part of the `dataset map` operation, somehow it remains in the memory without getting released (I may be wrong too).\r\n\r\nI used `objgraph` package and collected following logs, to see how the object counts are between epochs.\r\n[tf_dataset_objgraph.log](https://github.com/tensorflow/tensorflow/files/3553933/tf_dataset_objgraph.log)\r\n\r\nAnd I see tuple,list,dict are getting increased for each epoch.\r\n\r\nInfo regarding my actual experimentation:\r\n- We had a working EAST model based TF1.x, now I am porting that to TF 2.0. (still in progress)\r\n- **Dataset** : Text Image + Text File with bounding region details ---> (Text Image, Heat map with 5 channels, 1 channel Score image) ---> Write --->  TFRecords ---> Read --> Map ---> Batch ---> Model [Code](https://github.com/dhiraa/text-information-extraction/blob/master/dataset/icdar/icdar_data.py#L219)\r\n- **Model** : Resnet based model. [Code](https://github.com/dhiraa/text-information-extraction/blob/master/models/east/east_model.py)\r\n\r\nIf time permits I will put up a small repo with actual data and model.  [Update] My latest test case @ https://github.com/dhiraa/tf_issue_32052\r\n\r\n[Update] : Object counts scrapped from tf_dataset_objgraph.log and made as CSV\r\n[objgraph_tf_dataset_analysis.txt](https://github.com/tensorflow/tensorflow/files/3554213/objgraph_tf_dataset_analysis.txt)\r\n", "similar issue @ https://github.com/tensorflow/tensorflow/issues/24047", "Not sure if it helps, but I had a similar issue. I solved it with installing and running tcmalloc allocator:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/2942\r\n", "@goldiegadde  Please find the attached zip file, which has the README to reproduce the issue.\r\n\r\n[tf_dataset_memory_test.zip](https://github.com/tensorflow/tensorflow/files/3555207/tf_dataset_memory_test.zip)\r\n\r\nWith simple model , as you pointed out there is not much significant difference with each epoch.\r\n\r\nHowever with convolutional networks, complex loss calculation and optimization routines, the memory footprint is very large.\r\n\r\nNote : The EAST model in the zip file is still under porting and testing phase from TF 1.x version. It may also have something to do with this memory increase.", "Some findings with [mem_profiler](https://pypi.org/project/memory-profiler/). After each dataset function call the memory increases.\r\n\r\n```python\r\n# epoch 1\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n   398    302.4 MiB    302.4 MiB   @profile\r\n   399                             def train(estimator, max_steps=None):\r\n   400    302.5 MiB      0.1 MiB       train_spec = _get_train_spec(max_steps=max_steps)\r\n   401    302.5 MiB      0.0 MiB       estimator.train(\r\n   402    302.5 MiB      0.0 MiB           input_fn=train_spec.input_fn,\r\n   403    302.5 MiB      0.0 MiB           hooks=train_spec.hooks,\r\n   404   5257.2 MiB   4954.7 MiB           max_steps=train_spec.max_steps)\r\n\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n   407   5257.0 MiB   5257.0 MiB   @profile\r\n   408                             def evaluate(estimator, steps=None, checkpoint_path=None):\r\n   409   5257.0 MiB      0.0 MiB       eval_spec = _get_eval_spec(steps=steps)\r\n   410   5257.0 MiB      0.0 MiB       estimator.evaluate(\r\n   411   5257.0 MiB      0.0 MiB           input_fn=eval_spec.input_fn,\r\n   412   5257.0 MiB      0.0 MiB           steps=eval_spec.steps,\r\n   413   5257.0 MiB      0.0 MiB           hooks=eval_spec.hooks,\r\n   414   7506.0 MiB   2249.0 MiB           checkpoint_path=checkpoint_path)\r\n\r\n#epoch 2\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n   398   7506.2 MiB   7506.2 MiB   @profile\r\n   399                             def train(estimator, max_steps=None):\r\n   400   7506.2 MiB      0.0 MiB       train_spec = _get_train_spec(max_steps=max_steps)\r\n   401   7506.2 MiB      0.0 MiB       estimator.train(\r\n   402   7506.2 MiB      0.0 MiB           input_fn=train_spec.input_fn,\r\n   403   7506.2 MiB      0.0 MiB           hooks=train_spec.hooks,\r\n   404   9878.6 MiB   2372.4 MiB           max_steps=train_spec.max_steps)\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n   407   9878.6 MiB   9878.6 MiB   @profile\r\n   408                             def evaluate(estimator, steps=None, checkpoint_path=None):\r\n   409   9878.6 MiB      0.0 MiB       eval_spec = _get_eval_spec(steps=steps)\r\n   410   9878.6 MiB      0.0 MiB       estimator.evaluate(\r\n   411   9878.6 MiB      0.0 MiB           input_fn=eval_spec.input_fn,\r\n   412   9878.6 MiB      0.0 MiB           steps=eval_spec.steps,\r\n   413   9878.6 MiB      0.0 MiB           hooks=eval_spec.hooks,\r\n   414  12068.0 MiB   2189.4 MiB           checkpoint_path=checkpoint_path)\r\n\r\n```\r\n\r\n\r\n**dataset function** [full code](https://github.com/tensorflow/tensorflow/files/3555207/tf_dataset_memory_test.zip)\r\n\r\n```python\r\n\r\n@profile\r\ndef _get_dataset(data_path):\r\n    \"\"\"\r\n    Reads TFRecords, decode and batches them\r\n    :return: dataset\r\n    \"\"\"\r\n    _num_cores = 4\r\n    _batch_size = BATCH_SIZE\r\n\r\n    path = os.path.join(data_path, \"*.tfrecords\")\r\n    path = path.replace(\"//\", \"/\")\r\n    files = tf.data.Dataset.list_files(path)\r\n    # files = glob.glob(pathname=path)\r\n\r\n    # TF dataset APIs\r\n    dataset = files.interleave(\r\n        tf.data.TFRecordDataset,\r\n        cycle_length=_num_cores,\r\n        num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n\r\n    # dataset = tf.data.TFRecordDataset(files, num_parallel_reads=_num_cores)\r\n    # dataset = dataset.shuffle(_batch_size*10, 42)\r\n    # Map the generator output as features as a dict and label\r\n\r\n    if EAST_IMAGE_TEST:\r\n      dataset = dataset.map(map_func=east_features_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    else:\r\n      dataset = dataset.map(map_func=numpy_array_decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n\r\n    dataset = dataset.batch(batch_size=_batch_size, drop_remainder=False)\r\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    batch_feats, batch_label = iterator.get_next()\r\n    return batch_feats, batch_label\r\n\r\n@profile\r\ndef _get_train_spec(max_steps=None):\r\n    # Estimators expect an input_fn to take no arguments.\r\n    # To work around this restriction, we use lambda to capture the arguments and provide the expected interface.\r\n    return tf.estimator.TrainSpec(\r\n        input_fn=lambda: _get_dataset(data_path=TRAIN_DATA),\r\n        max_steps=max_steps,\r\n        hooks=None)\r\n\r\n\r\n@profile\r\ndef _get_eval_spec(steps):\r\n    return tf.estimator.EvalSpec(\r\n        input_fn=lambda: _get_dataset(data_path=VAL_DATA),\r\n        steps=steps,\r\n        hooks=None)\r\n\r\n```", "@Mageswaran1989  I met the same issue, have you solve it  ?", "@burness nope. We faced similar issue while using generator with TF dataset, few months back, we thought we were doing something wrong in our generator code. So we adopted different strategy. \r\n\r\nCan you share your setup details ? which may give us more clues", "@jsimsa Please find the current version of the code I use to replicate this memory issue @ https://github.com/dhiraa/tf_issue_32052\r\n\r\nobjgraph, memory profiler and tracemalloc logs @ https://github.com/dhiraa/tf_issue_32052/blob/master/log.txt", "@ymodak the memory increase happens inside of `estimator.train` and `estimator.evaluate` and is not necessarily coming from tf.data. As per user's previous instructions, using a similar input pipeline with a trivial model does not reproduce the memory increase. So someone on the high-level APIs / Estimator team should pinpoint where is the memory increase coming from before and if it is indeed coming from tf.data, I would be happy to take a further look.", "@Mageswaran1989 Thanks for your reply,   I found that if I use \"prefetch\" after the parallel_interleave\uff0cthe code which use estimator to train model will increase RAM memory, but If I just read dataset from hdfs, it's ok.\r\n", "No luck when I disabled `prefetch` on my code", "I get this warning \r\n\r\n```python\r\n Entity <bound method CodeMap.items of {<code object east_features_decode at 0x7f0b09e84e40, file \"/opt/tf_issue_32052/dummy_datasets.py\", line 139>: {}}> appears to be a generator function. It will not be converted by AutoGraph.\r\n\r\n```", "When I simply iterate through the dataset I could see the memory increase.\r\n\r\n[EAST Dataset : east_itr_log.txt](https://github.com/dhiraa/tf_issue_32052/blob/master/east_itr_log.txt) \r\n[Numpy Dataset: numpy_itr_log.txt](https://github.com/dhiraa/tf_issue_32052/blob/master/numpy_itr_log.txt)\r\n[Code](https://github.com/dhiraa/tf_issue_32052/blob/master/dummy_datasets.py#L208)\r\n\r\nPlease check my [repo](https://github.com/dhiraa/tf_issue_32052) for test command", "I dont know whether this is a valid test case, however when I create a [3-D random array](https://github.com/dhiraa/tf_issue_32052/blob/master/dummy_datasets.py#L144) as part of `dataset map` operation, the memory increases.\r\n\r\nAlso this memory consumption is only once, even if I call iterator twice the memory remains same.\r\n\r\n```python\r\ndef numpy_array_decode(serialized_example,\r\n                       NUM_FEATURES=250): #TODO make it as arg\r\n    # define a parser\r\n    features = tf.io.parse_single_example(\r\n        serialized_example,\r\n        # Defaults are not specified since both keys are required.\r\n        features={\r\n            'data': tf.io.FixedLenFeature([1 * NUM_FEATURES], tf.float32),\r\n            'label': tf.io.FixedLenFeature([1], tf.float32),\r\n        })\r\n\r\n    data = tf.reshape(\r\n        tf.cast(features['data'], tf.float32), shape=[1, NUM_FEATURES])\r\n\r\n    label = tf.reshape(\r\n        tf.cast(features['label'], tf.float32), shape=[1])\r\n\r\n     return {\"data\": data, \"dummy\": np.random.rand(512, 512, 5)}, label\r\n    #return {\"data\": data}, label\r\n\r\n```", "I'm too facing a similar issue\r\n\r\nMy code is something like\r\n        `test = [None]*50`\r\n        `for i in range(50):`\r\n        `    test[i] = fooWorker( numpyData, constantTensors, i)`\r\n\r\nwhere fooWorker is my customOp decorated with @tf.function. It returns some small lists and I just don't get why memory grows and is not collected in garbage. numpyData is substantial in size, all the other sizes are minimal.\r\n\r\nIt seems similar to case in tf1 where we kept creating graphs instead of simulating in sess.run", "@Mageswaran1989 If you define the dataset just once, then re-iterate over it during each epoch, it will reduce the memory consumption. Otherwise each time you create a new dataset, the dataset will be added to the global default graph, so it won't be cleaned up when it leaves scope.\r\n\r\n```python\r\ndataset = files.interleave(...)\r\nfor i in range(num_epochs):\r\n  for elem in dataset:\r\n    # process elem\r\n```\r\n\r\nCreating the dataset once and then iterating over it once per epoch is the idiomatic way of doing things in TF2.", "@aaudiber Thanks for clearing my assumption. As I thought the data is going to the global graph.  \r\n\r\nI tried your suggestion before raising this issue, when I came across this [StackOverflow question](https://github.com/tensorflow/tensorflow/issues/32052) . The problem arises when I use dataset with estimator.\r\n\r\nI get following error, when I try to move the dataset creation outside of `input_fn` definition.\r\n\r\n```python\r\nThe graph (<tensorflow.python.framework.ops.Graph object at 0x7f2837489898>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7f283748b278>) the dataset: Tensor(\"PrefetchDataset:0\", shape=(), dtype=variant) was created in. If you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function. Please ensure that all datasets in the pipeline are created in the same graph as the iterator. NOTE: This warning will become an error in future versions of TensorFlow.\r\n```\r\n\r\n If I keep it part of the `input_fn` and whenever [`tf.estimator.TrainSpec`](https://www.tensorflow.org/api_docs/python/tf/estimator/TrainSpec) is getting called by the Estimator there is a memory increase.", "The problem is that you are calling estimator `train` method multiple times (once for each epoch). This will create multiple copies of the input pipeline graph (and since the input pipeline graph inlines any constants such as input to `from_tensors` and `from_tensor_slices`, you will see a memory increase.\r\n\r\nThe recommended way for iterating through multiple epochs of your data using the Estimator API is to use the `tf.data.Dataset.repeat` transformation in your input pipeline.", "@jsimsa I just double confirmed in my setup ([dataset](https://github.com/dhiraa/tf_issue_32052/blob/master/dummy_datasets.py), [estimator](https://github.com/dhiraa/tf_issue_32052/blob/master/tf_memory_test.py)), even if I use `repeat` along with step calculation, the RAM memory that gets allocated for my data size is out of proportion.\r\n\r\n **Numpy Datasize : ~ 52MB + ~21MB**\r\n\r\n```python\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n   149    261.8 MiB    261.8 MiB   @profile\r\n   150                             def train_n_evaluate(estimator, TRAIN_DATA, VAL_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, max_steps=None, NUM_EPOCHS=None):\r\n   151    261.8 MiB      0.0 MiB       train_spec = _get_train_spec(TRAIN_DATA=TRAIN_DATA,\r\n   152    261.8 MiB      0.0 MiB                                    max_steps=max_steps,\r\n   153    261.8 MiB      0.0 MiB                                    BATCH_SIZE=BATCH_SIZE,\r\n   154    261.8 MiB      0.0 MiB                                    IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST,\r\n   155    439.8 MiB    178.0 MiB                                    NUM_EPOCHS=NUM_EPOCHS)\r\n   156    439.8 MiB      0.0 MiB       eval_spec = _get_eval_spec(VAL_DATA=VAL_DATA,\r\n   157    439.8 MiB      0.0 MiB                                  steps=200,\r\n   158    439.8 MiB      0.0 MiB                                  BATCH_SIZE=BATCH_SIZE,\r\n   159    439.8 MiB      0.0 MiB                                  IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)\r\n   160                             \r\n   161  11630.0 MiB  11190.2 MiB       tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\n\r\n**Image Datasize : ~140 MB + ~28MB**\r\n\r\n```python\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n   149    261.8 MiB    261.8 MiB   @profile\r\n   150                             def train_n_evaluate(estimator, TRAIN_DATA, VAL_DATA, BATCH_SIZE, IS_EAST_IMAGE_TEST, max_steps=None, NUM_EPOCHS=None):\r\n   151    261.8 MiB      0.0 MiB       train_spec = _get_train_spec(TRAIN_DATA=TRAIN_DATA,\r\n   152    261.8 MiB      0.0 MiB                                    max_steps=max_steps,\r\n   153    261.8 MiB      0.0 MiB                                    BATCH_SIZE=BATCH_SIZE,\r\n   154    261.8 MiB      0.0 MiB                                    IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST,\r\n   155    439.8 MiB    178.0 MiB                                    NUM_EPOCHS=NUM_EPOCHS)\r\n   156    439.8 MiB      0.0 MiB       eval_spec = _get_eval_spec(VAL_DATA=VAL_DATA,\r\n   157    439.8 MiB      0.0 MiB                                  steps=200,\r\n   158    439.8 MiB      0.0 MiB                                  BATCH_SIZE=BATCH_SIZE,\r\n   159    439.8 MiB      0.0 MiB                                  IS_EAST_IMAGE_TEST=IS_EAST_IMAGE_TEST)\r\n   160                             \r\n   161  11630.0 MiB  11190.2 MiB       tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\n```\r\n\r\nReference Log: \r\n[5epochs_with_repeat.log](https://github.com/tensorflow/tensorflow/files/3585336/5epochs_with_repeat.log)\r\n", "@Mageswaran1989 can you simplify your repro example as much as possible, ideally in a single file that demonstrates the issue? That would be super helpful for finding the root cause.", "@aaudiber Please find the updated file @ https://github.com/dhiraa/tf_issue_32052. Hope it helps!\r\n\r\nObservations \r\n- Memory remains constant when loop is not used, however the memory it takes is disproportional \r\n- Previously when I used loop, I used to run evaluation after each epoch and store the evaluation logs, in current `train_and_evaluate`  setup I had to run the entire `num_epochs` before I can `evaluate` my model. I may be missing something here or may be some API can help me out with this.\r\n\r\n\r\n**[Log](https://github.com/dhiraa/tf_issue_32052/blob/new_master/log.txt)**\r\n```python\r\n\r\n\r\nFilename: test_memory_leak.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n   367    300.7 MiB    300.7 MiB   @profile\r\n   368                             def train_n_evaluate(estimator,\r\n   369                                                  train_data_path,\r\n   370                                                  val_data_path,\r\n   371                                                  batch_size,\r\n   372                                                  num_features,\r\n   373                                                  num_epochs=None,\r\n   374                                                  max_train_steps=None,\r\n   375                                                  max_val_steps=None):\r\n   376    300.7 MiB      0.0 MiB       train_spec = _get_train_spec(train_data_path=train_data_path,\r\n   377    300.7 MiB      0.0 MiB                                    batch_size=batch_size,\r\n   378    300.7 MiB      0.0 MiB                                    num_features=num_features,\r\n   379    300.7 MiB      0.0 MiB                                    num_epochs=num_epochs,\r\n   380    420.7 MiB    119.9 MiB                                    max_steps=max_train_steps)\r\n   381    420.7 MiB      0.0 MiB       eval_spec = _get_eval_spec(val_data_path=val_data_path,\r\n   382    420.7 MiB      0.0 MiB                                  batch_size=batch_size,\r\n   383    420.7 MiB      0.0 MiB                                  num_features=num_features,\r\n   384    420.7 MiB      0.0 MiB                                  num_epochs=num_epochs,\r\n   385    420.7 MiB      0.0 MiB                                  max_steps=max_val_steps)\r\n   386                             \r\n   387  10256.1 MiB   9835.4 MiB       tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\n\r\nFilename: test_memory_leak.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n   407    242.0 MiB    242.0 MiB   @profile\r\n   408                             def main(args):\r\n   409    242.0 MiB      0.1 MiB       memory_usage_psutil(\"1. Before generating data\")\r\n   410                             \r\n   411                                 #  1. Generate regression data\r\n   412    242.0 MiB      0.0 MiB       generate_numpy_tf_records(out_dir=args[\"train_path\"],\r\n   413    242.0 MiB      0.0 MiB                                 num_tfrecord_files=args[\"num_tfrecord_train_files\"],\r\n   414    242.0 MiB      0.0 MiB                                 num_samples_per_file=args[\"num_samples_per_file\"],\r\n   415    262.2 MiB     20.2 MiB                                 num_features=args[\"num_features\"])\r\n   416    262.2 MiB      0.0 MiB       generate_numpy_tf_records(out_dir=args[\"val_path\"],\r\n   417    262.2 MiB      0.0 MiB                                 num_tfrecord_files=2,\r\n   418    262.2 MiB      0.0 MiB                                 num_samples_per_file=args[\"num_samples_per_file\"],\r\n   419    300.5 MiB     38.3 MiB                                 num_features=args[\"num_features\"])\r\n   420                             \r\n   421    300.5 MiB      0.0 MiB       total_steps_per_file = args[\"num_samples_per_file\"] / args[\"batch_size\"]\r\n   422                             \r\n   423    300.5 MiB      0.0 MiB       memory_usage_psutil(\"2. Before defining model\")\r\n   424                             \r\n   425                                 # 2. Define the model\r\n   426    300.5 MiB      0.0 MiB       model = NNet()\r\n   427                             \r\n   428    300.5 MiB      0.0 MiB       memory_usage_psutil(\"3. Before defining estimator\")\r\n   429                             \r\n   430                                 # 3. Define engine to train i.e Estimator\r\n   431    300.5 MiB      0.0 MiB       estimator = tf.estimator.Estimator(model_fn=model,\r\n   432    300.5 MiB      0.0 MiB                                          config=_init_tf_config(total_steps_per_file=total_steps_per_file,\r\n   433    300.7 MiB      0.2 MiB                                                                 model_dir=args[\"model_dir\"]),\r\n   434    300.7 MiB      0.0 MiB                                          params=None)\r\n   435                             \r\n   436    300.7 MiB      0.0 MiB       memory_usage_psutil(\"4. Before training\")\r\n   437                             \r\n   438                                 # 4. Train and evaluate the model with generated regression data\r\n   439    300.7 MiB      0.0 MiB       train_n_evaluate(estimator=estimator,\r\n   440    300.7 MiB      0.0 MiB                        train_data_path=args[\"train_path\"],\r\n   441    300.7 MiB      0.0 MiB                        val_data_path=args[\"val_path\"],\r\n   442    300.7 MiB      0.0 MiB                        batch_size=args[\"batch_size\"],\r\n   443    300.7 MiB      0.0 MiB                        num_features=args[\"num_features\"],\r\n   444    300.7 MiB      0.0 MiB                        num_epochs=args[\"num_epochs\"],\r\n   445    300.7 MiB      0.0 MiB                        max_train_steps=None,\r\n   446  10256.1 MiB   9955.3 MiB                        max_val_steps=None)\r\n   447                             \r\n   448  10256.1 MiB      0.0 MiB       memory_usage_psutil(\"5. Before exporitng the model\")\r\n   449                             \r\n   450  10266.6 MiB     10.5 MiB       export_model(estimator=estimator, model_export_path=args[\"model_export_path\"], num_features=args[\"num_features\"])\r\n\r\nINFO:absl:>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\r\nWARNING:absl:Final memory usage:  : Memory used is 10266.5546875\r\nINFO:absl:<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\r\n```\r\n", "Thanks @Mageswaran1989. It looks like this follows the expected behavior that memory increases linearly only when the loop is outside of Dataset/Estimator. You should be able to achieve everything you need without having an external loop. \r\n\r\nTo perform evaluation after every epoch, remove the `.repeat()` from your dataset input function. See [this](https://stackoverflow.com/a/49655277/401884) stackoverflow answer.\r\n", "As per the stackoverflow answer, I tried the dataset to repeat once (as well as removing dataset repeat) and used calculated `max_steps`  in the train spec.\r\n\r\nExpectations/My assumption:\r\n- When train dataset reaches its end, the training pauses and evaluation kicks in.\r\n- After evaluation is done, the training starts again \r\n\r\nObserved:\r\n- Only once the evaluation is done irrespective of dataset settings. Estimator evaluation starts only after the train `max_steps` are reached\r\n \r\n\r\n[From the user who raised that question:](https://stackoverflow.com/questions/49619995/how-to-control-when-to-compute-evaluation-vs-training-using-the-estimator-api-of)\r\n\r\n`EDIT: after running experiments I realize that max_steps controls the number of steps of the whole training procedure, not just the amount of steps before computing the metrics on the test set. Reading tf.estimator.Estimator.train, I see it has a steps argument, which works incrementally and is bounded by max_steps; however, tf.estimator.TrainSpec does not have the steps argument, which means I cannot control the number of steps to take before computing metrics on the validation set.`", "I had this problem just using ``model.fit_generator()`` on 2.0.0-rc1, so I back to the 2.0.0-beta1", "Can anyone help me with the right way of setting up the TFEstimator and Dataset APIs in Tensorflow 2.0 so that I can evaluate after each epoch, like we did in Tensorflow 1.1x ?\r\n\r\n- If I use loop the memory increases linearly with each `estimator.train()` and `estimator.eval()` call (which was discussed earlier in this thread)\r\n- If I use `tf.estimator.train_and_evaluate()` API, the evaluation is done at the end of all epochs/max_steps ", "@karmel could you please comment on the recommended use of Estimator APIs in TF 2.0?", "same issue,\r\nsame code will cause gpu memory leak on tf-gpu 2.0.0-rc-0 and 2.0.0-rc-1, but work fine on 2.0.0beta1", "I'm not sure if it's related to this same issue, but just iterating through a regular `TFRecordDataset` and using `model.predict_on_batch` results in a gpu memory leak that eventually crashes with an OOM exception for me. Training using `model.fit()` is not problematic for me, however.", "Not sure if this is the same issue, but I'm experiencing linear memory leaks on loops as well [example below]. I tried including a recommended fix from other threads (k.clear_session()), but that only slowed the memory leak.\r\n\r\nLet me know if I should open a separate ticket. Hope this helps/a fix can be found.\r\n\r\nOS: Ubuntu 18.04.3\r\nGPU: GeForce GTX 1080/PCIe/SSE2\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\nimport psutil\r\nimport matplotlib.pyplot as plt\r\nfrom tensorflow.keras import backend as k\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(layers.Dense(4, input_shape = (50,)))\r\nmodel.compile(loss='mse')\r\n\r\n\r\nx = []\r\ny = []\r\nfor xs in range(10000):\r\n    \r\n    values = np.random.uniform(size = (1,50))\r\n    prediction = model.predict(values)\r\n    \r\n    k.clear_session()\r\n    \r\n    mem = psutil.virtual_memory()\r\n    mem = mem[2]\r\n    \r\n    x.append(xs)\r\n    y.append(mem)\r\nplt.plot(x, y)\r\n```\r\n\r\nEdit: [Wanted to add an update]\r\nJust tried this same code on a separate computer with a clean install of Tensorflow 2.0 RC.\r\nIn this case I did not use a GPU and the OS was Windows 10.\r\n\r\nLet me know if you need anything from me.", "any updates please?", "same issue, TF 2.0\r\nI run Estimator with distributed mode on 6 machines ( PS x 1 / evaluator x 1 / worker x 4) ,\r\nmy code is super simple\uff1a\r\n```\r\nmodel = tf.estimator.DNNLinearCombinedClassifier()\r\ntrain_spec = tf.estimator.TrainSpec(...) # input_fn: epoches = infinite\r\neval_spec  = tf.estimator.EvalSpec(...) # input_fn: epoches = 1 \r\ntf.estimator.train_and_evaluate(model, train_spec, eval_spec)\r\n```\r\nAs `save_checkpoints_steps = 500` ,  so it will generate a checkpoint every 500 steps, and the evaluator starts to evaluate when there is a new checkpoint. I found that each evaluation will result in an increase of about 500MB memory used on the evaluator node and other nodes (PS & Worker) did not change significantly. \r\n\r\nPS: **Even if I use very small datasets when evaluating, memory leaks still exist. (~500MB\u2191/run)**\r\n", "using keras api\r\ncalling model.evaluate() in \"on_batch_end()\" callback will lead memory leaks", "Sorry for the delay. There is a lot going on in this thread, and many of these issues are unrelated.\r\n\r\n@Mageswaran1989 -- Estimator is not designed to interleave calls to train and evaluate (or predict), and we would recommend the train_and_evaluate API. If you want to be able to interleave and inspect between epochs, I recommend looking at the [Keras](https://www.tensorflow.org/guide/keras) API instead. This will be more 2.0 native, and more conducive to fine-grained control of the training loop.\r\n\r\n@ialdencoots , @Callahman , @dlutkaka -- these are unrelated problems. If you are still having issues, please open new Issues with code samples and the information requested in the bug template.\r\n\r\n@FelixHo -- this may or may not be a related issue, but it's hard to tell without more information. Please file a new issue with a minimal code sample + the information requested in the bug template.\r\n\r\nThanks, all.", "I was having a similar issue with both TF 2.0 and nightly using the Keras API to fit the model while doing some predictions between epochs with a callback. TF was leaking memory quite rapidly (a rate of around 8GB/hr), making long training runs impossible. I tried using [tcmalloc](http://goog-perftools.sourceforge.net/doc/tcmalloc.html) as suggested in [this guide](https://dantkz.github.io/How-To-Debug-A-Memory-Leak-In-TensorFlow/) and it solved the problem completely. If you're using conda, the fix is as simple as installing the `gperftools` package from `conda-forge` and setting the `LD_PRELOAD` flag like so:\r\n```\r\nLD_PRELOAD=$CONDA_PREFIX/lib/libtcmalloc.so python your_training_script.py\r\n```", "Any more updates? I had similar issue on [Google Cloud AI Platform](https://cloud.google.com/ai-platform), and I cannot use different `tcmalloc` on the cloud. Memory leakage got worse especially when I concatenated multiple dataset. ", "I too had a memory leak, where my RAM usage increased linearly. I could resolve it by removing the num_parallel_calls parameter of the map function when using it on my dataset.", "Like @DanielFirmbach said, I also did remove `num_parallel_cells` of map function too. A memory leak was alleviated pretty much, but the problem still exists somewhere. Of course, training speed got worse than before about 20%. Here is graph of worker memories.\r\n\r\n\r\n- When I use `num_parallel_cells` of map function on dataset.\r\n![image](https://user-images.githubusercontent.com/1433166/74820096-c1a63c00-5344-11ea-8762-0f4ba4971188.png)\r\n\r\n\r\n- After removing parameter.\r\n![image](https://user-images.githubusercontent.com/1433166/74820103-c4a12c80-5344-11ea-9ec9-89420b26dc91.png)\r\n", "Same here in TF 2.1.0. Any updates?", "I am facing an issue probably due to memory leak. \r\nI am using a Dataset generator which upload images in batches. If I only train images without validation, everything works fine. But as soon as I introduce validation dataset generator, i start facing an issue of memory. \r\n\r\n1. Only training data: no memory issue\r\n2. training data and validation data: memory issue. \r\n\r\nI have one dataset generator which is called twice (one by training dataset and one by validation)\r\n\r\nany solution to resolve this issue.\r\n", "In my case, `input_signature` argument in `tf.function` decorated functions dramatically reduced memory usage.\r\n\r\nFor example, if you have a function like this:\r\n```\r\ndef train_step(self, image_stack):\r\n    # Forward pass with gradient tape and loss calc\r\n    with tf.GradientTape() as tape:\r\n      image_reconstructed = self.model(image_stack)\r\n      loss = self.calc_loss(image_stack, image_reconstructed )\r\n\r\n    # Obtain gradients of trainable vars w.r.t. loss and perform update\r\n    gradients = tape.gradient(loss, self.model.trainable_weights)\r\n    self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\r\n\r\n    return (image_reconstructed, loss)\r\n\r\n```\r\n\r\nYou can specify the `input_signature` like this:\r\n```\r\nself.train_step = tf.function(self.train_step,\r\n    input_signature=[\r\n        tf.TensorSpec(shape=[None, None, None, 3], dtype=tf.float32),\r\n      ]\r\n    )\r\n```\r\nwhere the input image is of the shape `[batch, height, width, channels]`, and of the type `tf.float32`.\r\n\r\nAlthough this avoids expensive retracing and significantly reduces memory usage, I still have an increasing memory usage.", "This is observable, when we try to design our code to train the model based on epochs fashion in a user loop. ", "I figured it out. I have no RAM OOM now. In my case, the problem was `Dataset.cache()`. If there are random transformations (e.g. images augmentations) in the dataset pipeline, they should be after `Dataset.cache()`. Otherwise, it looks like `Dataset.cache()` caches all possible random transformations.", "> Like @DanielFirmbach said, I also did remove `num_parallel_cells` of map function too. A memory leak was alleviated pretty much, but the problem still exists somewhere. Of course, training speed got worse than before about 20%. Here is graph of worker memories.\r\n> \r\n> * When I use `num_parallel_cells` of map function on dataset.\r\n>   ![image](https://user-images.githubusercontent.com/1433166/74820096-c1a63c00-5344-11ea-8762-0f4ba4971188.png)\r\n> * After removing parameter.\r\n>   ![image](https://user-images.githubusercontent.com/1433166/74820103-c4a12c80-5344-11ea-9ec9-89420b26dc91.png)\r\n\r\nI'm facing a similar issue with TFv2.3 as well although, I'm using `keras.Model.fit` for training on a TPU. I'd assume that the same is happening due to use of `ds.map(map_fn, AUTOTUNE)` with AUTOTUNE?\r\n\r\nPS: my map_fn makes use of a lot of Python dicts and lists which are serialized together with the help of two for loops executed with `tf.autograph`.\r\n\r\nhttps://github.com/tanzhenyu/image_augmentation/blob/master/image_augmentation/image/augmentation.py#L402-L432\r\nhttps://github.com/tanzhenyu/image_augmentation/blob/master/image_augmentation/ai_platform/train_efficientnet.py#L251-L252\r\n\r\nTypically, I'd see that there is a linear increase in the memory consumption of the VM (**the VM which helps run model train** on TPU, not the CPU side of the TPU host; CPU utilisation of TPU node seems okay and cyclic) per 5-10 epochs. I do hope that the same is due to some form of an increase in the graph size of the dataset pipeline due to the clogging of Python objects/functions repeatedly. Training would bottleneck out after like 100 epochs.\r\n\r\nAlso, it's noteworthy to mention that I'm using a callback to perform an extra validation which might leak some memory?\r\n> I was having a similar issue with both TF 2.0 and nightly using the Keras API to fit the model while doing some predictions between epochs with a callback. TF was leaking memory quite rapidly (a rate of around 8GB/hr), making long training runs impossible. I tried using [tcmalloc](http://goog-perftools.sourceforge.net/doc/tcmalloc.html) as suggested in [this guide](https://dantkz.github.io/How-To-Debug-A-Memory-Leak-In-TensorFlow/) and it solved the problem completely. If you're using conda, the fix is as simple as installing the `gperftools` package from `conda-forge` and setting the `LD_PRELOAD` flag like so:\r\n> \r\n> ```\r\n> LD_PRELOAD=$CONDA_PREFIX/lib/libtcmalloc.so python your_training_script.py\r\n> ```\r\n\r\n\r\n\r\n/cc: @tanzhenyu \r\n\r\n", "Maybe a workaround similar to the one [here](https://github.com/google/automl/issues/688) would be useful to other people. \r\n\r\nThe idea is to run the content of the epoch loop in a child process and thus memory is being released after every epoch iteration.", "@NikZak I am not sure if we could use the same TPU device across different processes. ", "@swghosh Valid concern. However, I am not sure whether it will be an issue. There is no parallelization here. One process is made per one epoch and epochs are run sequentially. Every time an epoch ends the process also ends. There are always only two simultaneously running processes - the parent process and the child process created inside the epoch loop. Does this help to address your concern?", "@NikZak I guess it should work with forking a child process every time for the evals. Just that it'd cost us a bit of overhead in connecting to the TPU device (in the child process) each time we want to perform validation in between training. \r\n\r\nAlso, what do you think is `model` object pickle-able (should be Python serializable, but not sure) to be transferred to the child process each time?", "@Mageswaran1989 did you find a solution to this? \r\n", "Nope. I haven't returned back to TF after this memory issue. I was using TF from version 1.0 and the TF 2.x Keras changes made me to loose the track of things sadly. ", "I experience the same issue on 2.4.0.", "> I experience the same issue on 2.4.0.\r\n\r\nSame", "This was happening to me and I was able to fix by not running dataset.cache() before training", "@Mageswaran1989 Could you try as per the above comment with latest stable version of TF 2.5 and let us know if the issue still pesists? Thanks!", "> same issue, TF 2.0\r\n> I run Estimator with distributed mode on 6 machines ( PS x 1 / evaluator x 1 / worker x 4) ,\r\n> my code is super simple\uff1a\r\n> \r\n> ```\r\n> model = tf.estimator.DNNLinearCombinedClassifier()\r\n> train_spec = tf.estimator.TrainSpec(...) # input_fn: epoches = infinite\r\n> eval_spec  = tf.estimator.EvalSpec(...) # input_fn: epoches = 1 \r\n> tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\r\n> ```\r\n> \r\n> As `save_checkpoints_steps = 500` , so it will generate a checkpoint every 500 steps, and the evaluator starts to evaluate when there is a new checkpoint. I found that each evaluation will result in an increase of about 500MB memory used on the evaluator node and other nodes (PS & Worker) did not change significantly.\r\n> \r\n> PS: **Even if I use very small datasets when evaluating, memory leaks still exist. (~500MB\u2191/run)**\r\n\r\n@FelixHo Have you figured out why this happened? We have the same issue. We also used train_and_evaluate with ParameterServerStrategy. In RunConfig we set eval_distribute=None (we used to set eval_distribute to ParameterServerStrategy in tf 1.15 and it worked well, but it won't work in tf 2.5.0), not sure whether it's related.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32052\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32052\">No</a>\n", "I have the same problem in TF 2.8.0", "I am having the same issue when running the trainer.py from the keypose repo from Google. [https://github.com/google-research/google-research/blob/master/keypose/trainer.py] I am running TF 2.8.0 and in conda"]}, {"number": 32051, "title": "Update docstring for tf.linalg.matvec", "body": "In tensorflow, `matvec` is exposed as `tf.linalg.matvec`\r\nbut in the docstring `tf.matvec` is used instead.\r\n\r\nThis fix fixes the docstring `tf.matvec` => `tf.linalg.matvec`\r\n\r\nThis fix fixes #31923.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 32050, "title": "RMSprop can\u2018t convergence.", "body": "When I use a Adam optimizer, it can convergence. But when I use the RMSprop, it can't convergence. Maybe there is a bug in the keras.optimizers.RMSprop.\r\n", "comments": ["Addition: I am optimizing a bid-lstm model.", "Addition: I am using the tensorflow-gpu==2.0.0rc0", "Can you please provide a code snippet that repros the issue ?", "> Can you please provide a code snippet that repros the issue ?\r\n\r\nhttps://github.com/jiawei6636/Bioinfor_DanQ  this is my project, when i change the optimizer in the **main_DanQ.py** to RMSProp, my result is far from the original paper shows(The paper use RMSProp optimizer), but when i use the Adam optimizer,  the result is alright. So i think there is a bug in the tf.keras.optimizers.RMSProp.  And the issue #30263 is still trouble me in the tensorflow2.0rc0. ", "> Can you please provide a code snippet that repros the issue ?\r\n\r\nI write a custom training loop in the **trainer.py**.", "HI Jiawei Li, \r\ncan you please provide a link to minimum repro code that can reproduce the issue you described above ?", "> HI Jiawei Li,\r\n> can you please provide a link to minimum repro code that can reproduce the issue you described above ?\r\n\r\nOK, i will do it as soon as possible.", "@jiawei6636 As mentioned Can you please provide a minimum reproducible code as mentioned. ", "Closing this issue as this has been inactive for more than 2 weeks. Please add additional comments and we can open the issue again. Thanks!"]}, {"number": 32049, "title": "Creating a boolean constant prints a deprecation warning", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.0.0rc0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n\r\nCreating a boolean constant prints a deprecation warning:\r\n\r\n> W0828 15:45:36.142576 139852094695168 deprecation.py:323] From /lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py:253: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.identity instead.\r\n\r\n**Describe the expected behavior**\r\n\r\nNo deprecation warning.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.zeros([10], dtype=tf.bool)\r\n```", "comments": ["Please find the Gist of Colab for [TF 2.0rc0](https://colab.sandbox.google.com/gist/oanush/14d18b393e1bd59d77307a28ec2b0c19/32049.ipynb).Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32049\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32049\">No</a>\n", "I am still getting this warning on the current version of Tensorflow (I just installed it yesterday). When will this be fixed for mainstream users?", "Although far from satisfactory, I managed to avoid the warning through\r\n`tf.cast(tf.zeros((10, )), tf.bool)`"]}, {"number": 32048, "title": "[lite] only armv8.so carsh", "body": "[lite] resloved, unity problem", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nCan you please elaborate about the issue & the context.Will it be possible to provide related code. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32048\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32048\">No</a>\n"]}, {"number": 32047, "title": "Replace tab with space in the PTX code.", "body": "@cheshire This PR merely changes tabs with spaces per request.", "comments": ["@yongfeng-nv  Could you please resolve the conflicts? Thanks!", "Can this PR be closed? IIRC there was a commit with the same effect on top of the original PR?"]}, {"number": 32046, "title": "Fix deprecation warning when calling boolean_mask", "body": "Running the examples from the [`tf.boolean_mask`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/boolean_mask) documentation prints the following deprecation warning:\r\n\r\n> W0828 15:35:30.538258 140631514593024 deprecation.py:323] From .../lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:1486: where (from tensorflow.python.ops.array_os) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use tf.where in 2.0, which has the same broadcast rule as np.where\r\n\r\nThis PR fixes that.", "comments": []}, {"number": 32045, "title": "static int64 GetDirectConvCost - Integer overflow", "body": "/tensorflow/core/kernels/deep_conv2d.cc: ln 74,\r\nstatic int64 GetDirectConvCost(int filter_rows, int filter_cols, int in_depth,\r\nint out_depth, int out_rows, int out_cols) {\r\nreturn filter_rows * filter_cols * in_depth * out_depth * out_rows * out_cols;\r\n}\r\n\r\nCan lead to integer overflow and weird results\r\nI think, it should be smth like that\r\nreturn (int64)filter_rows * (int64)filter_cols * (int64)in_depth * (int64)out_depth * (int64)out_rows * (int64)out_cols;", "comments": ["@iur-kvasniuk That makes sense. Are you willing to create a PR for that?", "Ok, I will create a PR tomorrow morning. Thanks for reply!\n\n\u0421\u0440, 28 \u0430\u0432\u0433. 2019 \u0433. \u0432 18:24, Yong Tang <notifications@github.com>:\n\n> @iur-kvasniuk <https://github.com/iur-kvasniuk> That makes sense. Are you\n> willing to create a PR for that?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32045?email_source=notifications&email_token=ANAZPT44S3JEJKU4WRLX3RTQG2KC5A5CNFSM4IRG4QXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5LPHSA#issuecomment-525792200>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ANAZPT6BCNSI4ZFINLAPKNTQG2KC5ANCNFSM4IRG4QXA>\n> .\n>\n", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n\r\n\r\n\r\n\r\n", "- OS Platform and Distribution: Linux Ubuntu 16.04 LTS\r\n- TensorFlow built from source (tf_nightly-1.14.0-cp35-cp35m-linux_x86_64)\r\n- For Conv2D parameters\r\nin_depth = 4, input_cols = 1920, filter_cols = 3, input_rows = 1080, filter_rows = 3, out_depth = 32\r\noutput of GetDirectConvCost function: -1906180096, but should be 2388787200\r\n- For Conv2D parameters\r\nin_depth = 32, input_cols = 1920, filter_cols = 3, input_rows = 1080, filter_rows = 3, out_depth = 32\r\noutput of CanUseDeepConv2D function - false (deep_direct_ratio: 7.14965), but should be true (deep_direct_ratio: 0.722222)", "The function clearly overflows, but the fix suggested is not enough.", "created new PR with additional checks: #32120", "~~The caller should check, this is defined behavior. It's very probable that the caller already has all checks in place, if not, that's where they should go~~\r\n\r\nEdit, I was wrong, __signed__ integer overflow is UB.", "There are more issues in that file. I'll handle those changes this week", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32045\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32045\">No</a>\n"]}, {"number": 32044, "title": "Update protobuf.patch for Bazel's incompatible change bazelbuild/bazel#7362", "body": "This PR updates Tensorflow to change protobuf's cc_library //third_party/protobuf:protobuf to include `alwayslink = 1`. \r\n\r\nThis is needed because tensorflow_framework.so currently defines the symbols provided by //third_party/protobuf:protobuf. Incompatible change bazelbuild/bazel#7362 is going to be flipped in Bazel 1.0. and is going to disable `--legacy_whole_archive` that was enabled by default and that caused every transitive cc_library linked into a transitive shared library to be linked in a whole-archive block. Putting `alwayslink = 1` recreates the behavior from before the flag flip.\r\n\r\nThe principled solution is to talk to the protobuf team and ask them to maintain a cc_library with alwayslink = 1 target that projects such as TF can depend on. This PR is a fast-fix for Bazel 1.0. And maybe we'll discover that those symbols shouldn't be defined by the protobuf_tensorflow.so at all and instead //third_party/protobuf:protobuf should be a dependency of executables using those symbols.", "comments": ["CC @gunan @r4nt @scentini", "Abandoning this PR, uploading internally."]}, {"number": 32043, "title": "[TF 2.0.0rc0] Cannot connect to TPU device", "body": "Created VM and v3-8 TPU with ctpu up command and updated TF version to TF2.0.0rc0 via pip3. \r\nWhen i try to connect to tpu device returns error:\r\n```\r\nInvalidArgumentError: Unable to find a context_id matching the specified one (5613663074031560004). Perhaps the worker was restarted, or the context was GC'd?\r\nAdditional GRPC error information:\r\n{\"created\":\"@1566994715.938381293\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Unable to find a context_id matching the specified one (5613663074031560004). Perhaps the worker was restarted, or the context was GC'd?\",\"grpc_status\":3}\r\n2019-08-28 12:18:36.196440: E tensorflow/core/distributed_runtime/rpc/eager/grpc_eager_client.cc:72] Remote EagerContext with id 5613663074031560004 does not seem to exist.\r\n```\r\nI also tried the same in Colab, with rc0 version and i have the same error. The code i used is the one given in documentation:\r\n\r\n```\r\ntpu='test'\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu)\r\ntf.config.experimental_connect_to_host(resolver.master())\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\ntpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n```\r\n", "comments": ["Same situation here. I am trying to train a NN in a TPU in eager mode. Following the advice given in #30129 by @capilano, I switched to the nightly build: 2.0.0-dev20190731, and I found this error.", "@jmgc TPUs do not support eager execution. Try training without enabling it. For more info look at the question 11 [here](https://cloud.google.com/tpu/docs/faq).\r\n@nsantavas maybe you are also using eager execution?", "@rishabhsahrawat no", "@nsantavas You have to pass the TPU_address in the first line,in colab you can leave it blank, but the TPU would default to using 1.14 in colab,even if you change the tensorflow version to 2.0 in the colab VM.\r\n\r\nIf you are using TF2.0 , tf.function will automatically handle graphs, so the input pipeline can be executed in eager mode.  \r\nEdit:  Here is an example that uses TF 2.0, which uses eager by default\r\nhttps://github.com/tensorflow/tpu/blob/master/models/experimental/resnet50_keras/resnet50_tf2.py\r\n\r\n", "I am currently facing the same problem.\r\nI also use v3-8 TPU and tf2.0 rc0.\r\n\r\n@capilano \r\n\r\nWhich version of tensorflow can run [that link code](https://github.com/tensorflow/tpu/blob/master/models/experimental/resnet50_keras/resnet50_tf2.py)?\r\nI think this is not tf2.0 code because `import tensorflow.compat.v2 as tf` and ` tf.enable_v2_behavior()` are called in the source code.\r\nhttps://github.com/tensorflow/tpu/blob/aada81f730478e66ed736aea5db33fc9895341a4/models/experimental/resnet50_keras/resnet50_tf2.py#L34\r\nhttps://github.com/tensorflow/tpu/blob/aada81f730478e66ed736aea5db33fc9895341a4/models/experimental/resnet50_keras/resnet50_tf2.py#L170\r\n\r\n@goldiegadde \r\nI want to know the status of TPU support on tensorflow 2.0rc0.\r\nI have no other information than release notes and tensorflow guide.\r\n\r\nhttps://github.com/tensorflow/tensorflow/releases\r\nhttps://www.tensorflow.org/beta/guide/distribute_strategy\r\n\r\nIn the current situation I don't know if this is a bug to report or just not supported.\r\n\r\n\r\n", "@pshiko I am not really sure, I think you can call that in 2.0 and 1.0. I have seen people post TF2.0 and TPU issues(training issues) , so there is support, maybe there is a bug. Or possibly you could try TF 2.0-nightly.\r\nPersonally, I do not have access to  a TPU that has tensorflow 2.0, so all my code is on 1.14 and I have eager disabled. Apparently later versions of 1.0 also support eager training similar to 2.0 if you call those lines.", "Hello! I don't think there is an official TPU release for 2.0 (i.e. you cannot specify --version=2.0 when creating a TPU with ctpu or gcloud) so it is currently unsupported.", "In general, we anticipate that TF 2.1 (coming later this year) will have much better support for TPUs than the initial 2.0 release.", "Yes, but if you update VM host via pip to TF-nightly-2.0.0rc0 ?", "No, the TPU itself must have the same version as the VM host, or the configuration is not supported.", "@frankchn \r\nThank you for your kind reply !!\r\nI understood the current status of TPU support. \r\n\r\nAccording to https://www.tensorflow.org/beta/guide/distribute_strategy , `TPUStrategy` with `Keras API` support is planned in 2.0RC.\r\nBut the situation seems to have change.\r\n\r\nI will consider using TF1.x for a while.\r\nThank you!!\r\n\r\n", "@pshiko Yeah, we have been working on that, but only a small subset of customers have access to the internal TF2.0 image for the TPU itself, so it is not broadly available yet.\r\n\r\nOtherwise, running a TF2.0 VM host with a TF1.14 TPU is not compatible because the 1.14 TPU (having been released months prior to any 2.0 release) would lack code that supports the new features in 2.0.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32043\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32043\">No</a>\n"]}, {"number": 32042, "title": "Distributed Tensorflow eval values are coming as 0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI'm running asynchronous distributed training in Tensorflow using Parameter server strategy. Multi worker on multiple CPUs with evaluator as a separate node.\r\n\r\n**Describe the expected behavior**\r\nTraining and evaluation should happen fine.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nSample tf_config of Parameter server: Index and type varies for chief, worker and evaluator on other TF_CONFIGs.\r\nTF_CONFIG={\r\n\"task\": {\r\n    \"type\": \"ps\",\r\n    \"index\": 0\r\n},\r\n\"cluster\": {\r\n    \"chief\": [\"machine2:2222\"],\r\n    \"worker\": [\"machine3:2223\",\"machine4:2224\"],\r\n    \"evaluator\": [\"machine5:2225\"],\r\n    \"ps\": [\"machine1:2218\"]\r\n}\r\n}\r\n\r\n**model_main.py**\r\npredict_input_fn = train_and_eval_dict['predict_input_fn']\r\ntrain_steps = train_and_eval_dict['train_steps']\r\n\r\nif FLAGS.checkpoint_dir:\r\n    if FLAGS.eval_training_data:\r\n        name = 'training_data'\r\n        input_fn = eval_on_train_input_fn\r\n    else:\r\n        name = 'validation_data'\r\n\r\n    # The first eval input will be evaluated.\r\n    input_fn = eval_input_fns[0]\r\n\r\n    if FLAGS.run_once:\r\n        estimator.evaluate(input_fn,\r\n            num_eval_steps=None,\r\n            checkpoint_path=tf.train.latest_checkpoint(\r\n                FLAGS.checkpoint_dir))\r\n    else:\r\n        model_lib.continuous_eval(estimator, FLAGS.checkpoint_dir, input_fn, train_steps, name)\r\nelse:\r\n    train_spec, eval_specs = model_lib.create_train_and_eval_specs(\r\n        train_input_fn,\r\n        eval_input_fns,\r\n        eval_on_train_input_fn,\r\n        predict_input_fn,\r\n        train_steps,\r\n        eval_on_train_data=False)\r\n\r\n  # Currently only a single Eval Spec is allowed.\r\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.app.run()\r\n\r\n**Other info / logs**\r\nWarnings:\r\n\r\n    W0828 00:03:55.229441 140490069309248 estimator.py:1924] Estimator's model_fn (.model_fn at 0x7fc5da9b5268>) includes params argument, but params are not passed to Estimator.\r\n\r\n    ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node Preprocessor/ResizeToRange/strided_slice_3. Error: Pack node (Preprocessor/ResizeToRange/stack_2) axis attribute is out of bounds: 0\r\n\r\n**But the training runs fine and evaluation happens. But my evaluation results are 0 all the time.**\r\n\r\ncreating index... \r\nindex created! \r\ncreating index... \r\nindex created! \r\nRunning per image evaluation... \r\nEvaluate annotation type bbox \r\nDONE (t=1.66s). \r\nAccumulating evaluation results... \r\nDONE (t=0.52s).\r\n\r\n    Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.000\r\n\r\n    Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.000\r\n\r\n    Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.000\r\n\r\n    Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\r\n\r\n    Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\r\n\r\n    Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\r\n\r\n    Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.000\r\n\r\n    Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.000\r\n\r\n    Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.000\r\n\r\n    Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\r\n\r\n    Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\r\n\r\n    Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\r\n\r\n**Any help would be much appreciated. Thanks in advance.**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Looks like the code is incomplete.Can you please provide full code snippet to reproduce it on our environment.Thanks!", "Attaching model_main.py file as txt\r\n\r\n[model_main.py.txt](https://github.com/tensorflow/tensorflow/files/3555396/model_main.py.txt)\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32042\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32042\">No</a>\n"]}, {"number": 32041, "title": "micro: riscv32_mcu build failed with undefined references", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 298534b745db43b2ad18256ed781bd2f142e5bc7\r\n- Python version: 2.7.15+/3.6.8\r\n- Installed using virtualenv? pip? conda?: pip3\r\n\r\n**Describe the problem**\r\nTF Lite for micro, riscv32 build fails with undefined references.\r\n\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=riscv32_mcu TARGET_ARCH=riscv32 hello_world_bin\r\n\r\n/home/ehirdoy/src/tensorflow/tensorflow/lite/experimental/micro/riscv32_mcu/debug_log.cc:18: undefined reference to `__wrap_puts'\r\nexit.c:(.text.exit+0x2e): undefined reference to `__wrap__exit'\r\nsbrkr.c:(.text._sbrk_r+0x12): undefined reference to `__wrap__sbrk'\r\nwriter.c:(.text._write_r+0x16): undefined reference to `__wrap__write'\r\ncloser.c:(.text._close_r+0x12): undefined reference to `__wrap__close'\r\nlseekr.c:(.text._lseek_r+0x16): undefined reference to `__wrap__lseek'\r\nreadr.c:(.text._read_r+0x16): undefined reference to `__wrap__read'\r\nfstatr.c:(.text._fstat_r+0x14): undefined reference to `__wrap__fstat'\r\nisattyr.c:(.text._isatty_r+0x12): undefined reference to `__wrap__isatty'\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nexport PATH=\"$PATH:./tensorflow/lite/experimental/micro/tools/make/downloads/riscv_toolchain/bin\"\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile clean\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=riscv32_mcu TARGET_ARCH=riscv32 hello_world_bin\r\n\r\n\r\n**Any other info / logs**\r\n[make.log](https://github.com/tensorflow/tensorflow/files/3550454/make.log)\r\n", "comments": ["@dansitu, please take a look.", "I am running into a similar error when trying to compile any of the examples for riscv using the same commands as listed above. \r\nSo far i have tested 3 different riscv-toolchains:  \r\n- /tensorflow/lite/experimental/micro/tools/make/downloads/riscv_toolchain/bin:\r\n  undefined reference to `__wrap__exit`, `__wrap_puts`\r\n\r\n- selfbuild toolchain (June 2019): \r\n   undefined reference to `__ashldi3`, `__clzsi2`\r\n\r\n- sifive toolchain (https://static.dev.sifive.com/dev-tools/riscv64-unknown-elf-gcc-20181030-x86_64-linux-ubuntu14.tar.gz):\r\n  undefined reference to `__wrap__exit`, `__wrap_puts`\r\n", "I write patches which fix this bug, and tested on my machine.\r\nThis pull request to fix this issue is  https://github.com/tensorflow/tensorflow/pull/33972\r\nHi @ehirdoy @Zielasko \r\nCould you have a try of these patches pls? If you find it helpful, pls have a support click.\r\n\r\nBest,\r\n", "Thank you for writing a fix @zhoupeng .\r\nI tried to write a workaround in an older version of this repository that let me compile the hello world example and a few examples i have written myself, but they didn't behave correctly. \r\nUsing your version lets me compile and successfully run the hello world example on a risc-v simulator with the expected output. \r\n(Using `make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=riscv32_mcu hello_world` to build the executable) \r\n\r\nUnfortunately I still can't build the micro_speech, magic_wand or person_detection examples for \r\nriscv32_mcu. \r\nFor the magic_wand example it cant find `__wrap_malloc` and `__wrap_free` .\r\n\r\nperson_detection fails with the error \r\n``` \r\nperson_detection section `.bss' will not fit in region `ram' \r\n[...]\r\nsection .stack VMA [0000000080003800,0000000080003fff] overlaps section .bss VMA [00000000800002b8,0000000080012d87]\r\n[...]\r\nregion `ram' overflowed by 0 bytes\r\n```\r\n\r\nI have yet to test my own models. \r\n\r\nAlso, trying to build a linux version of the hello world example with the __dso fix gives me this error: \r\n`multiple definition of `__dso_handle'`\r\n\r\n\r\n \r\n\r\n\r\n", "Hi @Zielasko , Thanks for your response. Sorry for late reply, I am not convenient to handle mail in time, because on a travel of RISC-V tech forum.\r\n1)For the `.bss' will not fit in region `ram error:\r\nSet the `MEMORY.ram` option with a larger value in the link script can fix.\r\nThe `ram` option is in the link script `tensorflow.git/tensorflow/lite/experimental/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/flash.lds`.\r\nIn my case, I only try to set `ram (wxa!ri) : ORIGIN = 0x80000000, LENGTH = 128K` (Maybe `64K` or `32K` is enough for some examples, but I didn't try)\r\n\r\n2)__dso_handle missing problem. Thanks for response, very helpful.\r\nThis problem may only exists on non-Linux target (riscv32_mcu,Windows ...), I will submit a new version patch adding target OS check.\r\nBefore the new patch, If you try to build a linux version, you can try to comment `__dso_handle`  by replacing `void* __dso_handle;`\r\nwith `// void* __dso_handle;`, in `tensorflow.git/tensorflow/lite/experimental/micro/arduino/abi.cc`,\r\n\r\nHi @nkreeger @gbaned @rockyrhodes, sorry for  late, thanks @nkreeger for your detail review I will submit a new version patch, catching up on this Fridays after my RISC-V tech forum journey.", "Closing since the associated PR has been merged. Feel free to reopen if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32041\">No</a>\n"]}, {"number": 32040, "title": "Update tensorflow for upcoming incompatible change: https://github.com/bazelbuild/bazel/issues/7362", "body": "Update tensorflow for upcoming incompatible change: https://github.com/bazelbuild/bazel/issues/7362\r\n\r\nThis PR updates `cc_library`s to use `alwayslink=1` when needed. Currently, library archives and library object groups are wrapped in `--whole_archive`, `--no_whole_archive`, thus the whole library is always linked. This will change with Bazel 1.0, and libraries which export symbols must be tagged as `alwayslink=1`.", "comments": ["cc/ @gunan @hlopko ", "This was submitted internaly as c3619ce"]}, {"number": 32039, "title": "[2.0] Wrong result in tf.keras.losses.BinaryCrossentropy?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.0 beta\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1 / 7.6.2\r\n- GPU model and memory: rtx 2080 ti (11GB)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTrying to translate a model from PyTorch to TF, i've encountered a strange behaviour which I don't understand. The value of the bceloss seems to be different from the value obtained in PyTorch, which instead coincides with the \"correct\" one.\r\n\r\n**Describe the expected behavior**\r\nThe expected behaviour is to have the same value.\r\n\r\n**Code to reproduce the issue**\r\nout = [0.8800, 0.9271, 0.8596, 0.8748]\r\ntarget = [1, 1, 1, 1]\r\n\r\nexpected value: -ln(0.88) - ln(0.9271) - ln(0.8596) - ln(0.8748) = 0.4886/4 = 0.1221\r\n\r\nTF: \r\ntf.keras.losses.BinaryCrossentropy()(out, target)\r\n<tf.Tensor: id=1452660, shape=(), dtype=float32, numpy=1.7575724>\r\n\r\nPyTorch:\r\nnn.BCELoss()(out, target)\r\ntensor(0.1221, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\r\n\r\nThe PyTorch one is the expected value, I don't understand what the TF value is.\r\n\r\n**Other info / logs**\r\nSolved temporarily by (re)defining the BCELoss simply copying the source from https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/backend.py\r\n\r\ndef real_bce_loss(output, target):\r\n    epsilon = 1e-7\r\n    output = tf.clip_by_value(output, epsilon, 1. - epsilon)\r\n    bce = target * tf.math.log(output + epsilon)\r\n    bce += (1 - target) * tf.math.log(1 - output + epsilon)\r\n    return -tf.reduce_mean(bce)", "comments": ["I think you should switch the logits and targets.\r\n`tf.keras.losses.BinaryCrossentropy()( [1, 1, 1, 1], [0.8800, 0.9271, 0.8596, 0.8748])` gives you `0.12214369` which is correct.", "Easy as that :) Thanks"]}, {"number": 32038, "title": "Failed to build TFLite OpenCL delegate", "body": "**System information**\r\n- OS Platform and Distribution: [Official dockerfile for Android CI](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/Dockerfile.android)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: latest\r\n- Python version:3.5\r\n- Bazel version (if compiling from source):0.26.1\r\n- GCC/Compiler version (if compiling from source):5.4.0\r\n- CUDA/cuDNN version:not used\r\n\r\n**Describe the problem**\r\n\r\nI couldn't build TFLite OpenCL delegate.\r\nWould like to tell me how to build the added feature?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\nbazel build -c opt --config=opt --config android_arm64 --cxxopt=--std=c++14 --color=yes //tensorflow/lite/delegates/gpu/cl:gpu_api_delegate\r\n```\r\n\r\n**Any other info / logs**\r\n\r\n[Here](https://dev.azure.com/mlops/tflite/_build/results?buildId=332&view=logs&j=6040bc1a-72ff-56a7-5fe9-eeb7e9979f36&t=5d8150e6-3798-5a08-0300-038b15469693&l=55) is my complete CI logs.\r\n\r\n```\r\n[__w/1/s/tensorflow/tensorflow/lite/delegates/gpu/cl/BUILD:141:1: C++ compilation of rule '//tensorflow/lite/delegates/gpu/cl:gl_interop' failed (Exit 1): clang failed: error executing command \r\n  (cd /__w/1/b/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ANDROID_BUILD_TOOLS_VERSION=28.0.0 \\\r\n    ANDROID_NDK_API_LEVEL=18 \\\r\n    ANDROID_NDK_HOME=/android/ndk \\\r\n    ANDROID_SDK_API_LEVEL=23 \\\r\n    ANDROID_SDK_HOME=/android/sdk \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/android/sdk/tools:/android/sdk/platform-tools:/android/ndk \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64 -target aarch64-none-linux-android -ffunction-sections -funwind-tables -fstack-protector-strong -fpic -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -isystemexternal/androidndk/ndk/sysroot/usr/include/aarch64-linux-android '-D__ANDROID_API__=18' -O2 -g -DNDEBUG -MD -MF bazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/gpu/cl/_objs/gl_interop/gl_interop.pic.d '-frandom-seed=bazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/gpu/cl/_objs/gl_interop/gl_interop.pic.o' -fPIC -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEGL_EGLEXT_PROTOTYPES -iquote . -iquote bazel-out/arm64-v8a-opt/bin -iquote external/com_google_absl -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_absl -iquote external/opencl_headers -iquote bazel-out/arm64-v8a-opt/bin/external/opencl_headers -iquote external/FP16 -iquote bazel-out/arm64-v8a-opt/bin/external/FP16 -iquote external/flatbuffers -iquote bazel-out/arm64-v8a-opt/bin/external/flatbuffers -iquote external/farmhash_archive -iquote bazel-out/arm64-v8a-opt/bin/external/farmhash_archive -Ibazel-out/arm64-v8a-opt/bin/external/FP16/_virtual_includes/FP16 -isystem external/opencl_headers -isystem bazel-out/arm64-v8a-opt/bin/external/opencl_headers -isystem external/FP16/include -isystem bazel-out/arm64-v8a-opt/bin/external/FP16/include -isystem tensorflow/lite/delegates/gpu/cl -isystem bazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/gpu/cl -isystem external/flatbuffers/include -isystem bazel-out/arm64-v8a-opt/bin/external/flatbuffers/include -isystem external/farmhash_archive/src -isystem bazel-out/arm64-v8a-opt/bin/external/farmhash_archive/src -w '-std=c++14' '--std=c++14' '--sysroot=external/androidndk/ndk/platforms/android-28/arch-arm64' -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/lite/delegates/gpu/cl/gl_interop.cc -o bazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/gpu/cl/_objs/gl_interop/gl_interop.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ntensorflow/lite/delegates/gpu/cl/gl_interop.cc:30:30: error: unknown type name 'EGLSync'\r\nusing PFNEGLCREATESYNCPROC = EGLSync(EGLAPIENTRYP)(\r\n                             ^\r\ntensorflow/lite/delegates/gpu/cl/gl_interop.cc:33:1: error: unknown type name 'PFNEGLCREATESYNCPROC'; did you mean 'PFNEGLCREATESYNCKHRPROC'?\r\nPFNEGLCREATESYNCPROC g_eglCreateSync = nullptr;\r\n^~~~~~~~~~~~~~~~~~~~\r\nPFNEGLCREATESYNCKHRPROC\r\nexternal/androidndk/ndk/sysroot/usr/include/EGL/eglext.h:152:34: note: 'PFNEGLCREATESYNCKHRPROC' declared here\r\ntypedef EGLSyncKHR (EGLAPIENTRYP PFNEGLCREATESYNCKHRPROC) (EGLDisplay dpy, EGLenum type, const EGLint *attrib_list);\r\n                                 ^\r\ntensorflow/lite/delegates/gpu/cl/gl_interop.cc:42:3: error: unknown type name 'EGLSync'\r\n  EGLSync egl_sync;\r\n  ^\r\ntensorflow/lite/delegates/gpu/cl/gl_interop.cc:43:35: error: use of undeclared identifier 'EGL_CL_EVENT_HANDLE'\r\n  const EGLAttrib attributes[] = {EGL_CL_EVENT_HANDLE,\r\n                                  ^\r\ntensorflow/lite/delegates/gpu/cl/gl_interop.cc:46:39: error: use of undeclared identifier 'EGL_SYNC_CL_EVENT'\r\n                                      EGL_SYNC_CL_EVENT, attributes));\r\n                                      ^\r\ntensorflow/lite/delegates/gpu/cl/gl_interop.cc:47:19: error: use of undeclared identifier 'EGL_NO_SYNC'\r\n  if (egl_sync == EGL_NO_SYNC) {\r\n                  ^\r\ntensorflow/lite/delegates/gpu/cl/gl_interop.cc:58:40: error: unknown type name 'PFNEGLCREATESYNCPROC'; did you mean 'PFNEGLCREATESYNCKHRPROC'?\r\n    g_eglCreateSync = reinterpret_cast<PFNEGLCREATESYNCPROC>(\r\n                                       ^~~~~~~~~~~~~~~~~~~~\r\n                                       PFNEGLCREATESYNCKHRPROC\r\nexternal/androidndk/ndk/sysroot/usr/include/EGL/eglext.h:152:34: note: 'PFNEGLCREATESYNCKHRPROC' declared here\r\ntypedef EGLSyncKHR (EGLAPIENTRYP PFNEGLCREATESYNCKHRPROC) (EGLDisplay dpy, EGLenum type, const EGLint *attrib_list);\r\n                                 ^\r\n7 errors generated.\r\nTarget //tensorflow/lite/delegates/gpu/cl:gpu_api_delegate failed to build\r\n```", "comments": ["1. you need Android NDK r20\r\n2. you may want to check my hacks to use it in label_image and benchmark_model [here](https://github.com/tensorflow/tensorflow/compare/master...freedomtan:opencl_hacks)", "@freedomtan I appreciate your quick reply. I will try Android NDK r20.", "@freedmtan\r\nI succeeded to build cl:gpu_api_delegate. Thanks a lot!\r\nBTW I guess cl:gpu_api_delegate should be inserted into benchmark_performance_options as follows.\r\n\r\n```\r\ncc_library(\r\n    name = \"benchmark_performance_options\",\r\n    srcs = [\r\n        \"benchmark_performance_options.cc\",\r\n    ],\r\n    hdrs = [\"benchmark_performance_options.h\"],\r\n    copts = common_copts,\r\n    deps = [\r\n        \":benchmark_model_lib\",\r\n        \":benchmark_params\",\r\n        \":benchmark_utils\",\r\n        \":logging\",\r\n        \"//tensorflow/core:stats_calculator_portable\",\r\n        \"//tensorflow/lite/c:c_api_internal\",\r\n        \"//tensorflow/lite/profiling:time\",\r\n        \"//tensorflow/lite/tools:command_line_flags\",\r\n    ] + select({\r\n        \"//tensorflow:android\": [\r\n            \"//tensorflow/lite/delegates/gpu:gl_delegate\",\r\n            \"//tensorflow/lite/delegates/gpu/cl:gpu_api_delegate\"\r\n        ],\r\n        \"//conditions:default\": [],\r\n    }),\r\n)\r\n```", "nope, I added it to `benchmark_tflite_model_lib` because I added related code to `benchmark_tflite_model.cc`, see my [hack](https://github.com/tensorflow/tensorflow/commit/680746b36abee2f9b61c98e5d2c361e1fd3bacf3)", "@freedomtan Thanks.\r\nTo avoid the build error of non-android platform, I will modify the BUILD file as follows.\r\n\r\n```\r\ncc_library(\r\n    name = \"benchmark_tflite_model_lib\",\r\n    srcs = [\r\n        \"benchmark_tflite_model.cc\",\r\n        \"logging.h\",\r\n    ],\r\n    hdrs = [\"benchmark_tflite_model.h\"],\r\n    copts = common_copts,\r\n    deps = [\r\n        \":benchmark_model_lib\",\r\n        \":benchmark_utils\",\r\n        \":logging\",\r\n        \"//tensorflow/lite:framework\",\r\n        \"//tensorflow/lite:string_util\",\r\n        \"//tensorflow/lite/kernels:builtin_ops\",\r\n        \"//tensorflow/lite/profiling:profile_summarizer\",\r\n        \"//tensorflow/lite/profiling:profiler\",\r\n        \"//tensorflow/lite/tools/evaluation:utils\",\r\n        \"@gemmlowp\",\r\n    ] + select({\r\n        \"//tensorflow:android\": [\r\n            \"//tensorflow/lite/delegates/gpu/cl:gpu_api_delegate\"\r\n        ],\r\n        \"//conditions:default\": [],\r\n    }),\r\n)\r\n``` ", "@stakemura looks good. What I did was a hack to test it on Android devices. Will clean it up when OpenCL backend is formally announced. \r\n\r\n@impjdi: do you have target date for announcement? TensorFlow World?", "I confirmed TFLite benchmark tool supported OpenCL delegate by 592d786f3077ee97fbe4bdc67b521c02100a1bf9 and 7bb0ace8572de53c0409f120c7faf5fa55a777b7.\r\nThanks.\r\n\r\nHowever [CI script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/Dockerfile.android) is still using old Android NDK r17c. Would you like to update this Dockerfile?", "Oops.  Apologies.  Somehow, this issue completely dropped off my radar.\r\n\r\nWe were holding on to an announcement, waiting for the delegate to get ready.  I think we're pretty close (the code is actually in), and we might do another blog post, before TF World :)", "I updated label_image to be GPU Delegate V2 compatible https://github.com/tensorflow/tensorflow/pull/33362", "It looks like Dockerfile was updated.\r\nhttps://github.com/tensorflow/tensorflow/commit/a1309f4e3c4780c8b24ba6b0fbddad278d3071e3#diff-5f85f919491d70a8ec63d18a0e93c898\r\nLet's close this."]}, {"number": 32037, "title": "when i follow the guide to create a mbed folder,i get this error,this is the first time i use TFlite ,can any one help me please", "body": "when i follow the [guide] to  demonstrate the absolute basics of using TensorFlow Lite for Microcontrollers.i create a folder for mbed,but i get this error:\r\nmake:*** no rule to make target \"tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/tensorflow/lite/experimental/micro/tools/make/downloads/CMSIS_ext/arm_cmplx_mag_squared_q10p6.c\",needed by \"generate_hello_world_mbed_project\"\r\n(https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/hello_world)\r\n- OS Platform and Distribution (Linux Ubuntu 18.04):\r\n", "comments": ["@dansitu, could you please take a look?", "@aselle @dansitu  could you please help me? i have no idea to fix this problem", "Thanks for the bug report! Can you please provide the full list commands you used?", "> Thanks for the bug report! Can you please provide the full list commands you used?\r\nthanks for your reply! this is the commands i  @dansitu \r\nTensorFlow version (or github SHA if from source):master\r\nGCC/Compiler version (if compiling from source): 7.4.0\r\n```\r\ngit clone --depth 1 https://github.com/tensorflow/tensorflow.git\r\nmake -f tensorflow / lite / experimental / micro / tools / make / Makefile test_hello_world_test\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project\r\n```\r\nwhen i want create the folder for mbed, its report an error", "I was able to build the project successfully with the commands you gave. I followed these steps:\r\n\r\n1. Clone the TensorFlow repo\r\n`git clone --depth 1 https://github.com/tensorflow/tensorflow.git`\r\n\r\n2. Change into the directory\r\n`cd tensorflow`\r\n\r\n3. Run the make command\r\n`make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project`\r\n\r\nSome output is then printed. The last line of it is `Finished patching kissfft`.\r\n\r\nAfter this, you should see the expected mbed folder here:\r\n\r\n`tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/`\r\n\r\nTry deleting your `tensorflow` repository directory and following all of these steps again.\r\n\r\nWarmly,\r\nDan", "alright,im done it according to your method,i got the error as the follows.\r\nchould you please help me check out what steps i do  wrong?\r\ni deleting my `tensorflow` repository and clone again\r\n```\r\nwang@Lenovo:~$ git clone --depth 1 https://github.com/tensorflow/tensorflow.git\r\ncloning'tensorflow'...\r\nremote: Enumerating objects: 21025, done.\r\nremote: Counting objects: 100% (21025/21025), done.\r\nremote: Compressing objects: 100% (15225/15225), done.\r\nremote: Total 21025 (delta 7564), reused 9688 (delta 5180), pack-reused 0\r\nreceiving object: 100% (21025/21025), 47.72 MiB | 1.60 MiB/s, completed.\r\nprocessing delta : 100% (7564/7564), completed.\r\nchecking out files: 100% (20085/20085),completed.\r\n```\r\nwhen i do the first step,that works ok.\r\n```\r\nwang@Lenovo:~$ cd tensorflow\r\nwang@Lenovo:~/tensorflow$  make -f tensorflow/lite/experimental/micro/tools/make/Makefile test_hello_world_test\r\n/bin/sh: 1: [[: not found\r\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh \"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853d75de2af87622ad293ba\" tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp \r\n.......\r\n.......\r\nTesting LoadModelAndPerformInference\r\n1/1 tests passed\r\n~~~ALL TESTS PASSED~~~\r\n```\r\nand i run the make command to create the mbed folder,i receive the error again\r\n```\r\nwang@Lenovo:~/tensorflow$ make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project\r\n/bin/sh: 1: [[: not found\r\nmake: *** no rule to make target\u201ctensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/tensorflow/lite/experimental/micro/tools/make/downloads/CMSIS_ext/arm_cmplx_mag_squared_q10p6.c\u201d\uff0cneeded by\u201cgenerate_hello_world_mbed_project\u201d \u3002stop\u3002\r\n```\r\ndo you have the other method to complete the same work? thanks!", "Thanks for trying this out. What version of Make are you using? I'm using\n4.2.1. If your version is lower than this, I'd suggest installing 4.2.1 and\ntrying again.\n\nOn Thu, Sep 5, 2019 at 6:05 AM Yooong-W <notifications@github.com> wrote:\n\n> alright,im done it according to your method,i got the error as follws.\n> chould you please help me check out what steps i do wrong?\n>\n> **wang@Lenovo:~$** git clone --depth 1 https://github.com/tensorflow/tensorflow.git\n>\n> cloning'tensorflow'...\n>\n> remote: Enumerating objects: 21025, done.\n>\n> remote: Counting objects: 100% (21025/21025), done.\n>\n> remote: Compressing objects: 100% (15225/15225), done.\n>\n> remote: Total 21025 (delta 7564), reused 9688 (delta 5180), pack-reused 0\n>\n> receiving object: 100% (21025/21025), 47.72 MiB | 1.60 MiB/s, completed.\n>\n> processing delta : 100% (7564/7564), completed.\n>\n> checking out files: 100% (20085/20085),completed.\n>\n> **wang@Lenovo:~$** cd tensorflow\n>\n> **wang@Lenovo:~/tensorflow$**  make -f tensorflow/lite/experimental/micro/tools/make/Makefile test_hello_world_test\n>\n> /bin/sh: 1: [[: not found\n>\n> tensorflow/lite/experimental/micro/tools/make/download_and_extract.sh \"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853d75de2af87622ad293ba\" tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp\n>\n> .......\n>\n> .......\n>\n> Testing LoadModelAndPerformInference\n>\n> 1/1 tests passed\n>\n> ~~~ALL TESTS PASSED~~~\n>\n>\n> **wang@Lenovo:~/tensorflow$** make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project\n>\n> /bin/sh: 1: [[: not found\n>\n> make: *** no rule to make target\u201ctensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/tensorflow/lite/experimental/micro/tools/make/downloads/CMSIS_ext/arm_cmplx_mag_squared_q10p6.c\u201d\uff0cneeded by\u201cgenerate_hello_world_mbed_project\u201d \u3002stop\u3002\n>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32037?email_source=notifications&email_token=AAC4SYYTD6VRDSPVSOWXFJ3QID72FA5CNFSM4IQ5W622YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD56725Y#issuecomment-528350583>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAC4SY5SSZWHLMPKCKHSIULQID72FANCNFSM4IQ5W62Q>\n> .\n>\n\n\n-- \n\nDaniel Situnayake | Developer Advocate, TensorFlow Lite\n<https://www.tensorflow.org/lite/> | dansitu@google.com | tensorflow.org\n", "my version of make is 4.1, i installing 4.2.1 and trying again. but i get a same error...\r\nthats really makes me very upset.\r\n> Thanks for trying this out. What version of Make are you using? I'm using 4.2.1. If your version is lower than this, I'd suggest installing 4.2.1 and trying again.\r\n> [\u2026](#)\r\n> On Thu, Sep 5, 2019 at 6:05 AM Yooong-W ***@***.***> wrote: alright,im done it according to your method,i got the error as follws. chould you please help me check out what steps i do wrong? **wang@Lenovo:~$** git clone --depth 1 https://github.com/tensorflow/tensorflow.git cloning'tensorflow'... remote: Enumerating objects: 21025, done. remote: Counting objects: 100% (21025/21025), done. remote: Compressing objects: 100% (15225/15225), done. remote: Total 21025 (delta 7564), reused 9688 (delta 5180), pack-reused 0 receiving object: 100% (21025/21025), 47.72 MiB | 1.60 MiB/s, completed. processing delta : 100% (7564/7564), completed. checking out files: 100% (20085/20085),completed. **wang@Lenovo:~$** cd tensorflow **wang@Lenovo:~/tensorflow$** make -f tensorflow/lite/experimental/micro/tools/make/Makefile test_hello_world_test /bin/sh: 1: [[: not found tensorflow/lite/experimental/micro/tools/make/download_and_extract.sh \"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853d75de2af87622ad293ba\" tensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp ....... ....... Testing LoadModelAndPerformInference 1/1 tests passed ~~~ALL TESTS PASSED~~~ **wang@Lenovo:~/tensorflow$** make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_hello_world_mbed_project /bin/sh: 1: [[: not found make: *** no rule to make target\u201ctensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/tensorflow/lite/experimental/micro/tools/make/downloads/CMSIS_ext/arm_cmplx_mag_squared_q10p6.c\u201d\uff0cneeded by\u201cgenerate_hello_world_mbed_project\u201d \u3002stop\u3002 \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#32037?email_source=notifications&email_token=AAC4SYYTD6VRDSPVSOWXFJ3QID72FA5CNFSM4IQ5W622YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD56725Y#issuecomment-528350583>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAC4SY5SSZWHLMPKCKHSIULQID72FANCNFSM4IQ5W62Q> .\r\n> -- Daniel Situnayake | Developer Advocate, TensorFlow Lite <https://www.tensorflow.org/lite/> | dansitu@google.com | tensorflow.org\r\n\r\n", "I'm sorry you're having problems with this. What is your operating system and version, and which shell are you using?", "thanks for your reply \uff0cim using hardware for the first time to run the program. take the liberty to ask, working with TensorFlow Lite for Microcontrollers can only use this method?\r\nmy operating system : Linux Ubuntu 18.04\r\nshell : terminal\r\n", "Although the creation failed,but I can find the mbed folder in the directory. it look likes some file in this folder are missing, When I compile with mbed, I will get an error, suggesting that some file is missing...\r\nwhere should i get the required source files in this structure? \r\ni got some mbed file that have created from this link, can i move the source files into the mbed file that i create with the command?\r\n[https://drive.google.com/open?id=1OtgVkytQBrEYIpJPsE8F6GUKHPBS3Xeb](url)\r\nLooking forward your reply!\r\n\r\n> I'm sorry you're having problems with this. What is your operating system and version, and which shell are you using?\r\n\r\n", "Hi,\r\n\r\nI can confirm I'm seeing the same issue too when building the code for disco_f746ng, and I can't figure why.\r\nIn my case, I'm building the micro-speech demo. I see the same error, i.e. a message concerning **/bin/sh: 1: [[: not found** and the **..q10p6.c** file issue too, see here: \r\n\r\n```\r\nuser@linux1:~/development/tensorflow$ make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_micro_speech_mbed_project\r\n/bin/sh: 1: [[: not found\r\nmake: *** No rule to make target 'tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/tensorflow/lite/experimental/micro/tools/make/downloads/CMSIS_ext/arm_cmplx_mag_squared_q10p6.c', needed by 'generate_micro_speech_mbed_project'. Stop.\r\n```\r\n\r\nI too had make version 4.1, so I downloaded 4.2.1 from gnu, and built and installed it (and confirmed with make -v that it was indeed 4.2.1). I'm running Ubuntu too, but version 16.04.6 LTS in my case.\r\n\r\nAny suggestions what it could be?\r\nMany thanks!\r\n", "I can confirm the same issue on Ubuntu 18.04.3 LTS too.\r\nHere were the steps:\r\n1. Install Ubuntu 18.04.3 LTS in a virtual machine (using ESXi).\r\n2. as root user, **apt-get update**, **apt-get upgrade**, and then **reboot**\r\n3. as root user:\r\n**apt install python2.7 python-pip git mercurial\r\npip install mbed-cli\r\napt install curl**\r\n4. as non-root user:\r\n**git clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile test_micro_speech_test**\r\n(I see the ALL TESTS PASSED message)\r\n**make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_micro_speech_mbed_project**\r\nI see this output:\r\n```\r\nuser@u18-nuc:~/development/tensorflow$ make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_micro_speech_mbed_project\r\n/bin/sh: 1: [[: not found\r\nmake: *** No rule to make target 'tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/tensorflow/lite/experimental/micro/tools/make/downloads/CMSIS_ext/arm_cmplx_mag_squared_q10p6.c', needed by 'generate_micro_speech_mbed_project'. Stop.\r\n```\r\n\r\nSince this latest Ubuntu comes with make version 4.1, I then tried [Gnu Make version 4.2 installing as discussed here](https://askubuntu.com/questions/1079470/finding-ppas-getting-a-modern-gnu-make-on-18-04?rq=1)  )\r\nand I confirmed with make -v that the version was 4.2.\r\nThen I typed:\r\n**make -f tensorflow/lite/experimental/micro/tools/make/Makefile clean**\r\nand then repeated the earlier make commands. But I get the same error.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "hi\r\nhave you successfully transplanted any demo into the board(stm32f746ng)?\r\ni tried a lot of method but failed, do you have an operating system in your borad?\r\n\r\n> I can confirm the same issue on Ubuntu 18.04.3 LTS too.\r\n> Here were the steps:\r\n> \r\n> 1. Install Ubuntu 18.04.3 LTS in a virtual machine (using ESXi).\r\n> 2. as root user, **apt-get update**, **apt-get upgrade**, and then **reboot**\r\n> 3. as root user:\r\n>    **apt install python2.7 python-pip git mercurial\r\n>    pip install mbed-cli\r\n>    apt install curl**\r\n> 4. as non-root user:\r\n>    **git clone https://github.com/tensorflow/tensorflow.git\r\n>    cd tensorflow\r\n>    make -f tensorflow/lite/experimental/micro/tools/make/Makefile test_micro_speech_test**\r\n>    (I see the ALL TESTS PASSED message)\r\n>    **make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_micro_speech_mbed_project**\r\n>    I see this output:\r\n> \r\n> ```\r\n> user@u18-nuc:~/development/tensorflow$ make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_micro_speech_mbed_project\r\n> /bin/sh: 1: [[: not found\r\n> make: *** No rule to make target 'tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/tensorflow/lite/experimental/micro/tools/make/downloads/CMSIS_ext/arm_cmplx_mag_squared_q10p6.c', needed by 'generate_micro_speech_mbed_project'. Stop.\r\n> ```\r\n> \r\n> Since this latest Ubuntu comes with make version 4.1, I then tried [Gnu Make version 4.2 installing as discussed here](https://askubuntu.com/questions/1079470/finding-ppas-getting-a-modern-gnu-make-on-18-04?rq=1) )\r\n> and I confirmed with make -v that the version was 4.2.\r\n> Then I typed:\r\n> **make -f tensorflow/lite/experimental/micro/tools/make/Makefile clean**\r\n> and then repeated the earlier make commands. But I get the same error.", "Hi Yooong,\r\n\r\nNot yet, this was going to be my first mbed CLI program for the ST chip. I have used mbed CLI with other processors, and it was straightforward, just drag it to the drive, and mbed will program it (same as with the cloud version of mbed). Once this micro-speech demo compiles, I intend to modify it actually, since I wish to use it on the NUCLEO-F746ZG board, and not the discovery board (the processor is identical, just different package, and less pins).\r\n", "maybe you should try get the mbed folder from officially generated\r\n[https://drive.google.com/open?id=1OtgVkytQBrEYIpJPsE8F6GUKHPBS3Xeb](url)\r\nso you can skip that step, and use mbed-cli to compile a `.bin` . this can only solve the problem temporarily", "Hi Yooong,\r\n\r\nThank you for this, it worked to generate the .bin file for now. (For DISCO_F746NG, I will now try to see if it builds with NUCLEO_F746ZG flag instead, and see what I need to edit to do so.\r\nI hope the error we mentioned can get resolved soon.\r\nThanks again for your help.\r\n", "I've put together a fix, but I'd love you to try it out before I publish it. Please try replacing line 25 of `tensorflow/lite/experimental/micro/tools/make/Makefile` with the following:\r\n\r\n```sh\r\nHOST_ARCH := $(shell if $(shell uname -m | grep -Eq 'i[345678]86'); then echo x86_32; else echo $(shell uname -m); fi)\r\n```\r\n\r\nThis removes some bash-only keywords from the Makefile so it is more likely to work on non-bash shells.", "Hi,\r\n\r\nI tried that line 25 mod, but I saw this:\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile clean\r\n**/bin/sh: 1: Syntax error: \";\" unexpected**\r\n\r\nSo, I edited line 25 to just state for now:\r\n**HOST_ARCH := x86_64** \r\n\r\nand then I don't get any error, and I was able to execute this successfully:\r\n**make -f tensorflow/lite/experimental/micro/tools/make/Makefile test_micro_speech_test**\r\n\r\nHowever the next line of the instructions is to type:\r\n**make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_micro_speech_mbed_project**\r\n\r\nbut when I do that, I see this:\r\n**make: *** No rule to make target 'tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/tensorflow/lite/experimental/micro/tools/make/downloads/CMSIS_ext/arm_cmplx_mag_squared_q10p6.c', needed by 'generate_micro_speech_mbed_project'. Stop.**\r\n\r\nThere is no folder called **tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/tensorflow/lite/experimental/micro/tools/make/downloads/CMSIS_ext**  \r\nI only see folders up to **tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/tensorflow/lite/experimental/micro**\r\nso I'm guessing there is a step missing to download additional content.\r\n", "Incidentally the user shell that executes these commands is a bash shell, according to /etc/passwd, it is /bin/bash. It was the default user account that gets created when installing ubuntu.", "Thanks, @shabaz123; looks like we have two problems here.\r\n\r\nI've hopefully fixed that replacement line; give this one a try:\r\n\r\n```sh\r\nHOST_ARCH := $(shell if uname -m | grep -Eq 'i[345678]86'; then echo x86_32; else echo $(shell uname -m); fi)\r\n```\r\n\r\nI've now been able to reproduce your second problem, so I'll let you know when we have a fix.", "I have a fix! Replace the contents of `tensorflow/lite/experimental/micro/tools/make/targets/mbed_makefile.inc` with the following:\r\n\r\n```make\r\n# Settings for mbed platforms.\r\nifeq ($(TARGET), mbed)\r\n  TARGET_ARCH := cortex-m4\r\n  $(eval $(call add_third_party_download,$(CMSIS_URL),$(CMSIS_MD5),cmsis,))\r\n  $(eval $(call add_third_party_download,$(CUST_CMSIS_URL),$(CUST_CMSIS_MD5),CMSIS_ext,))\r\nendif\r\n```\r\n\r\nLet me know if this works and I'll submit a proper fix. Before testing, run the following to clean the build:\r\n\r\n```bash\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile clean\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile clean_downloads\r\n```\r\n\r\n", "Hi dansitu,\r\n\r\nThank you!!\r\nI tried both of the fixes you suggested (the **HOST_ARCH** fix, and the **mbed_makefile.inc** fix) and together it is much better : ) Now it proceeds to correctly compile.\r\nThere were a few other minor changes that were needed to fully compile. Basically, the file\r\n**tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/ mbed-os/cmsis/TARGET_CORTEX_M/arm_math.h**\r\nneeds three modifications.\r\n### Modification 1 at line 1924:\r\nline 1924, change this:\r\n  void arm_mult_q15(\r\n  q15_t * pSrcA,\r\n  q15_t * pSrcB,\r\n  q15_t * pDst,\r\n  uint32_t blockSize);\r\n\r\nto:\r\n  void arm_mult_q15(\r\n  **const** q15_t * pSrcA,\r\n  **const** q15_t * pSrcB,\r\n  q15_t * pDst,\r\n  uint32_t blockSize);\r\n\r\n### Modification 2 at line 6509:\r\nchange from\r\n\r\n  void arm_max_q7(\r\n  q7_t * pSrc,\r\n  uint32_t blockSize,\r\n  q7_t * pResult,\r\n  uint32_t * pIndex);\r\n\r\nto:\r\n\r\n*/\r\n  void arm_max_q7(\r\n  **const** q7_t * pSrc,\r\n  uint32_t blockSize,\r\n  q7_t * pResult,\r\n  uint32_t * pIndex);\r\n\r\n### Modification 3 at line 6182:\r\nchange from:\r\n  void arm_mean_q15(\r\n  q15_t * pSrc,\r\n  uint32_t blockSize,\r\n  q15_t * pResult);\r\n\r\nto:\r\n  void arm_mean_q15(\r\n  **const** q15_t * pSrc,\r\n  uint32_t blockSize,\r\n  q15_t * pResult);\r\n\r\nAfter this, the mbed.bin file compiles successfully (I have not run it on the board though, since I do not own a Discovery board), to about 173436 bytes Flash, and 119464 bytes SRAM with the GCC ARM compiler I was using.\r\n\r\nAlso, the instructions on the page https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech\r\nneed slight editing, at the **Deploy to STM32F746** section, it needs changing from \r\nThis will result in the creation of a new folder:\r\n**tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed**\r\n\r\nto:\r\n\r\nThis will result in the creation of a new folder:\r\n**tensorflow/lite/experimental/micro/tools/make/gen/mbed_cortex-m4/prj/**micro_speech**/mbed**\r\n\r\nThat is very minor of course.\r\n\r\nThank you so much for your support! Also thank you Yooong for helping too.", "@dansitu ; thanks for your suggestions, I tried all of that and its works very well.\r\nwhen i use mbed to compile,i got a new error,thanks @shabaz123 , his Modification successfully solved this problem.\r\n```\r\n[ERROR] ./tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c:53:6: error: conflicting types for 'arm_mult_q15'\r\n void arm_mult_q15(\r\n      ^~~~~~~~~~~~\r\nIn file included from ./tensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c:29:0:\r\n./mbed-os/cmsis/TARGET_CORTEX_M/arm_math.h:1924:8: note: previous declaration of 'arm_mult_q15' was here\r\n   void arm_mult_q15(\r\n        ^~~~~~~~~~~~\r\n\r\n```\r\nwhen i build the second example micro_speech , i find i have to fix the HOST_ARCH and use this command first.\r\n`make -f tensorflow/lite/experimental/micro/tools/make/Makefile test_micro_speech_test`\r\nand then fix mbed_makefile.inc, next use the command\uff1a\r\n`make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f746ng\" generate_micro_speech_mbed_project`\r\nif i fix both of them at first ,and then use the command, i will get the same error.\r\n\r\n", "Thanks so much @shabaz123 and @Yooong-W, we'll work on a proper fix!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32037\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32037\">No</a>\n", "Thanks so much, @shabaz123 and @dansitu!!! :heart_eyes: :+1: "]}]