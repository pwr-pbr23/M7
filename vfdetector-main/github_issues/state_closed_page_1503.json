[{"number": 7844, "title": "word2vec at tensorflow 1.0", "body": "hello everyone\r\nI have a error \u201c\r\nTraceback (most recent call last):\r\n  File \"D:/software install/Pycharm/pycharm workplace/Word2vecTest/word2vec.py\", line 45, in <module>\r\n    word2vec = tf.load_op_library(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'word2vec_ops.so'))\r\n  File \"C:\\Users\\yuquanle\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py\", line 63, in load_op_library\r\n    raise errors_impl._make_specific_exception(None, None, error_msg, error_code)\r\ntensorflow.python.framework.errors_impl.NotFoundError: D:\\software install\\Pycharm\\pycharm workplace\\Word2vecTest\\word2vec_ops.so not found\u201d\r\n\r\nBut that file(word2vec_ops.so) I have used ubuntu to generate the document.", "comments": ["Please ensure you are running the most recent version, and include all the information requested in the issues template.", "Hi,\r\n\r\nI can follow up on what this issue is. \r\n\r\nWhen running tensorflow_models/tutorials/embedding/word2vec.py in Windows 10, it fails. Because we do not have the word2vec_ops.so file. \r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttp://stackoverflow.com/questions/42106032/tensorflow-word2vec-error\r\n\r\n### Environment info\r\nOperating System: Windows 10\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nRun tensorflow_models/tutorials/embedding/word2vec.py on windows. \r\n\r\nHope this clarifies further. Perhaps we could have some kind of source files to make or gcc through cygwin?\r\n\r\n/J\r\n", "I tried to fix this issue, by updating my Tensorflow and then running:\r\n\r\n`gcc -std=c++11 -shared word2vec_ops.cc word2vec_kernels.cc -o word2vec_ops.so -fPIC -I C:/Python35/lib/site-packages/tensorflow/include -O2 -D_GLIBCXX_USE_CXX11_ABI=0\r\n`\r\n\r\nwhich outputs:\r\n\r\n```\r\nword2vec_ops.cc:1:0: warning: -fPIC ignored for target (all code is position independent)\r\n /* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n ^\r\nword2vec_ops.cc:13:42: fatal error: tensorflow/core/framework/op.h: No such file or directory\r\n #include \"tensorflow/core/framework/op.h\"\r\n                                          ^\r\ncompilation terminated.\r\nword2vec_kernels.cc:1:0: warning: -fPIC ignored for target (all code is position independent)\r\n /* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n ^\r\nword2vec_kernels.cc:13:42: fatal error: tensorflow/core/framework/op.h: No such file or directory\r\n #include \"tensorflow/core/framework/op.h\"\r\n                                          ^\r\ncompilation terminated.\r\n```\r\nNaturally I traveled to tensorflow/core/framework and there was no op.h \r\n\r\nI will continue to explore a solution. Or am I missing something obvious?\r\n\r\n/J", "@poxvoculi\r\n@icyJoseph\r\nthe environment is tensorflow 1.0\uff0cbut in windows10 is fail\uff0cthe file word2vec_ops.so i use ubuntu generate and copy to windows10 . \r\nIn the windows10 \uff0cthe error is\uff1a\r\n\r\n File \"C:\\Users\\yuquanle\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py\", line 63, in load_op_library\r\n    raise errors_impl._make_specific_exception(None, None, error_msg, error_code)\r\ntensorflow.python.framework.errors_impl.NotFoundError: D:\\software install\\Pycharm\\pycharm workplace\\Word2vecTest\\word2vec_ops.so not found\r\n\r\nbut the word2vec_ops.so already exists.\r\n\r\nbut in ubuntu \uff0cit can run successfully. I want to know the code is only support in linux\uff1f\r\n", "@yuquanle \r\nHi,\r\nThe problem is that .so extensions do not work on windows. So what you'd need to do is build Tensorflow on Windows as it is shown here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake\r\n\r\nI will execute that step list in the upcoming days and report back to you. In the mean time, enjoy TF :)!", "@keveman @mmry I'm guessing that dynamically loaded user ops on Windows10 may be difficult?"]}, {"number": 7843, "title": "./configure should be made script friendly", "body": "Building tensorflow artefacts today is made difficult by the fact that the ./configure script requires interactive inputs. For default automated builds, one solution is: `echo \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\" | ./configure` (or `yes ''` and catching the error) but this is far from satisfactory, and tweaking the parameter is even more difficult.\r\n\r\nSee https://github.com/Homebrew/homebrew-core/pull/10273#discussion_r102895387 for instance in homebrew, but other packager will presumably hit the same issue.", "comments": ["You can set the environment variables appropriately as I did in the following bash script:\r\nhttps://gist.github.com/PatWie/0c915d5be59a518f934392219ca65c3d\r\n", "@poxvoculi, looking through [configure.py](https://github.com/tensorflow/tensorflow/blob/c527d67d21f4b1d778ca92d3c3478c381aba5fd6/configure.py), these are the only variables I found:\r\n```\r\nCUDA_TOOLKIT_PATH\r\nLD_LIBRARY_PATH\r\nMPI_HOME\r\nPYTHON_LIB_PATH\r\nPYTHONPATH\r\nTF_CUDA_CLANG\r\nTF_DOWNLOAD_CLANG\r\nTF_NEED_COMPUTECPP\r\nTF_NEED_CUDA\r\nTF_NEED_MPI\r\nTF_NEED_OPENCL_SYCL\r\nUSE_DEFAULT_PYTHON_LIB_PATH\r\n```\r\n\r\nThe gist you linked to uses these:\r\n```\r\nCC_OPT_FLAGS\r\nCUDA_TOOLKIT_PATH\r\nCUDNN_INSTALL_PATH\r\nGCC_HOST_COMPILER_PATH\r\nPYTHON_ARG\r\nPYTHON_BIN_PATH\r\nPYTHON_LIB_PATH\r\nPYTHONPATH\r\nTF_CUDA_CLANG\r\nTF_CUDA_COMPUTE_CAPABILITIES\r\nTF_CUDA_VERSION\r\nTF_CUDNN_VERSION\r\nTF_DOWNLOAD_MKL\r\nTF_ENABLE_XLA\r\nTF_NEED_CUDA\r\nTF_NEED_GCP\r\nTF_NEED_HDFS\r\nTF_NEED_JEMALLOC\r\nTF_NEED_MKL\r\nTF_NEED_MPI\r\nTF_NEED_OPENCL\r\nTF_NEED_VERBS\r\n```\r\n\r\nHere's the diff:\r\n![image](https://user-images.githubusercontent.com/5461398/44063593-672aaa4e-9f1e-11e8-8f24-1417ebaa1ddc.png)\r\n\r\n\r\nThere are some you include that are extra, and some that you don't include. Is there a complete list of options for configuring the build somewhere?\r\n\r\nEdit: And documentation for each of those options - just trying to optimize an automatic build.", "You need to set these\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/configure.py#L1462-L1483\r\n\r\nAnd additionally\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/configure.py#L872\r\nhttps://github.com/tensorflow/tensorflow/blob/master/configure.py#L883\r\n...\r\n\r\nThere even some new variables like [`TF_NEED_KAFKA`](\r\nhttps://github.com/tensorflow/tensorflow/blob/master/configure.py#L1466) which are not in the gist.\r\n\r\nJust grep for `environ_cp.get` in `configure.py`. I will update the gist.\r\n\r\n\r\n\r\n"]}, {"number": 7842, "title": "Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)", "body": "I got the following error when I run `python cifar10_train.py`.\r\n\r\n```\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:390] Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5105 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n```\r\n\r\nOperating System: Windows 10\r\nCUDA: `Cuda compilation tools, release 8.0, V8.0.44`\r\ncuDNN: 5.1\r\ntensorflow: 1.0.0\r\n\r\nThe output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`\r\n\r\n```\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally\r\n1.0.0\r\n```\r\n\r\nI have upgrade cudnn from 5.0 to 5.1. But it didn't work.\r\n", "comments": ["How did you install cuDNN 5.1? Is it possible that cuDNN 5.0 is still in your `%PATH%`?", "@mrry Yes, that is the problem. I reset the cuDNN PATH. Problem solved! Thank you!", "It was a lucky guess :). Glad to hear it!", "When I use the keras==1.2.0, I have got the same problem. Fortunately,  the issue was solved after I upgrade the tensorflow from 1.2.0 to 1.3.0 .Maybe you can have a try"]}, {"number": 7841, "title": "TensorFlow's introductory examples errors", "body": "I am following along from this point: https://www.tensorflow.org/get_started/get_started#basic_usage\r\nIt is a linear regression example.\r\n\r\nI have approached this execution from two angles:\r\n\r\nAngle 1: Running python and copying one line at a time.\r\nWhen run this line:  estimator = `tf.contrib.learn.LinearRegressor(feature_columns=features)`\r\n\r\nI get this issue: `WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\elavi\\AppData\\Local\\Temp\\tmprirdvmnn\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_tf_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 1\r\n}\r\n, '_task_type': None, '_save_checkpoints_steps': None, '_master': '', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000021D2049C390>, '_task_id': 0, '_save_checkpoints_secs': 600, '_save_summary_steps': 100, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_tf_random_seed': None, '_environment': 'local', '_num_ps_replicas': 0}`\r\n\r\n\r\n\r\nAngle 2: It may be more helpful to see how I run it as a script as a whole. The output is:\r\n`WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\elavi\\AppData\\Local\\Temp\\tmp41huenz5\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From C:\\Users\\elavi\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:521: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\n2017-02-24 00:02:36.386704: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-24 00:02:36.388071: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-24 00:02:36.389235: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-24 00:02:36.389290: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-24 00:02:36.390010: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-24 00:02:36.390894: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-24 00:02:36.391659: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-24 00:02:36.392248: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From C:\\Users\\elavi\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:521: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\n`\r\n\r\nI am interested in what is going on but I really need things simplified. A list of steps to fix this would be great. Also, if there are any better ways I could pose my problems in the future, please let me know.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nMany. The issue is that I am very new. My ability to understand solutions is very small. I am not well versed with the required terminology. So I have seen this problem posed and closed but I just can't follow along.\r\n\r\n### Environment info\r\nOperating System: Windows 10\r\n\r\nInstalled version of CUDA and cuDNN: I have the latest version but I probably shouldn't because I do not have a dedicated graphics card. (I am doing initial development on Microsoft Surface Pro 4) \r\n\r\nTF Version\r\n1.0.0-rc2", "comments": ["All I see in the output you posted are warnings, not errors.  Those warnings point out issues you ought to be aware of, but should not prevent a correct evaluation.  If you ignore those warnings, does the example not work?", "Same problem for me", "same problem here ", "@wenquanzhao / @ElliotVilhelm can you see if poxvoculli's reply addresses your problem (ie, the solution is to ignore these messages)", "@poxvoculi @yaroslavvb \r\n No it doesn't. If it did, the expected output would at least include\r\n`{'loss': 5.9819476e-11, 'global_step': 1000}` According to the example by TF.", "Make sure a print is working for you at all ...\r\nyou can surround the final print in that code with\r\n\r\n```\r\nprint(\"STARTING\")\r\nprint(estimator.evaluate(input_fn=input_fn, steps=10))\r\nprint(\"ENDING\")\r\n```\r\nSee if you can see either of those added prints working.", "@aselle \r\nThe result is\r\n```\r\nSTARTING\r\nWARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\elavi\\AppData\\Local\\Temp\\tmp_ukogsp9\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From C:\\Users\\elavi\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:521: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\n2017-02-27 11:35:27.716156: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-27 11:35:27.716322: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-27 11:35:27.719845: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-27 11:35:27.720688: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-27 11:35:27.721719: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-27 11:35:27.722310: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-27 11:35:27.722955: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-27 11:35:27.723499: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From C:\\Users\\elavi\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:521: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\nWARNING:tensorflow:From C:\\Users\\elavi\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:521: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\n{'loss': 1.209928e-08, 'global_step': 1000}\r\nENDING\r\n```\r\n\r\nSo something is certainly working. I have questions though:\r\n1. Every time I run the same code, the loss changes. Why?\r\n2. Why did you pick steps = 10?\r\n3. Least importantly, am I going to have to perpetually live with these warnings?\r\n\r\n", "Your last three lines here\r\n```\r\nlist of tags to a scalar summary op is no longer supported. WARNING:tensorflow:Skipping\r\nsummary for global_step, must be a float or np.float32. {'loss': 1.209928e-08, 'global_step': 1000} ENDING\r\n```\r\nhave \r\n```\r\n{'loss': 1.209928e-08, 'global_step': 1000} \r\n```\r\nSo it did work. the loss became effectively zero (1e-8 is about zero for the purposes of this discussion). The loss changes because the examples are ordered randomly and the weights are initialized randomly. That means that the exact numeric values are not deterministic (this is inherent in stochastic gradient descent learning approaches).\r\n\r\nTensorFlow is verbose on startup so it is hard to see the result, but it is there.\r\n", "@aselle I see, thank you! I still don't like the warnings but I am glad this all works! \r\n\r\nFor future TF Beginners, the example code on the TF website is missing this line where it prints the estimator results at the end:\r\n`print(estimator.evaluate(input_fn=input_fn)`\r\n\r\nThanks to everybody on this thread, I am closing it now.\r\n", "I really don't like the perspective, that warnings are ok. Especially when they are triggered by official example code from the website.\r\nWhere is the sense of quality and soundness?", "It might be helpful to take a look at an overview of TensorFlow logging.\n<https://www.tensorflow.org/get_started/monitors>  There are multiple\nlevels of logging available, and their labels are somewhat arbitrary.\nERROR is supposed to be used for things which are really errors, i.e. a\nvalue has been encountered somewhere which is outside of the legal range,\nand correct computation cannot continue, but maybe we can unwind up the\nstack and retry something.  FATAL is even more severe, it means the program\nmust terminate now.   There are multiple gentler logging levels for\ninformational or debugging messages.  WARNING is intermediate, for\nsomething that isn't definitely an error, but you ought to know about.\n (WARNING also has the sometimes useful property that it flushes to the log\nfile immediately, so it should be there if a hard program failure occurs\nsoon afterward.)  According to the documentation I cited, the default\nlogging level is WARN, which means that logging messages of lesser severity\nwill likely not display to novices, and if one of the TF developers wants\nto make sure you see a message, it needs to be a WARNING.  So, some of\nthese WARNINGs are just messages that someone hopes will help users\nself-diagnose a non-optimal situation without needing to post issues here.\n\n\n\nOn Wed, Mar 29, 2017 at 11:25 PM, Stefano <notifications@github.com> wrote:\n\n> I really don't like the perspective, that warnings are ok. Especially when\n> they are triggered by official example code from the website.\n> Where is the sense of quality and soundness?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7841#issuecomment-290314030>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AO818Vzm8XNJa05FlVZ7F4fEzS3cbT4qks5rq0rPgaJpZM4MK7aI>\n> .\n>\n", "I managed to suppress all the warnings with this.  \r\n\r\n```\r\n# https://github.com/tensorflow/tensorflow/issues/7778\r\n# TF_CPP_MIN_LOG_LEVEL is a TensorFlow environment variable responsible for the logs, \r\n# to silence INFO logs set it to 1, to filter out WARNING 2 and to additionally silence ERROR logs (not recommended) set it to 3\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# supress 'monitor' warnings...\r\n# https://www.tensorflow.org/get_started/monitors\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\n```\r\n", "You have to set the `model_dir`, eg. if you want to store the outputs in `./output `folder:\r\n\r\n`estimator = tf.contrib.learn.LinearRegressor(feature_columns=features, model_dir='./output')`\r\n\r\nSame thing if you follow the next example (Custom model):\r\n\r\n`estimator = tf.contrib.learn.Estimator(model_fn=model,model_dir='./output')`\r\n\r\nThen you can use TensorBoard to show the result:\r\n`>tensorboard --logdir=output`\r\n\r\n<img width=\"730\" alt=\"tb\" src=\"https://cloud.githubusercontent.com/assets/2390257/26750453/63e5c074-4855-11e7-9cee-349b73f2d9f9.PNG\">\r\n\r\n\r\n\r\n", "@tonytangau Unlike elvai001's case, mine DOES end up in an error if I don't explicitly add model_dir into the parameter like you've mentioned :/", "Tutorial code should always work and give exactly the same result. \r\nNot mention \"warning\" is something i should not ignore and should not be in tutorial code. \r\nIf someone think in the opposite way, sorry but it make bad image of project, some people are not capable to make human communication even if they are good at same others things.\r\nFor me it is a message: \"I will have more problems in future\" so before to invest money and time in it, i will try the other solution. Sorry but it is always hard at start and when creator do not care about clean, working code, it gets much harder.", "same here"]}, {"number": 7840, "title": "fused batch norm delay_updates confusion", "body": "In https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/layers/python/layers/layers.py around line 347:\r\n\r\nFrom its definition, `delay_updates` should delay updates to moving mean and moving average if we are training. \r\n\r\nThe way it is defined now, when `is_training` evaluates to Trure, `smart_cond` will pick the `_delay_updates` function which actually adds the update ops to the graph. Is that intended ?", "comments": ["Yes, it is intended. Note that the ops are added to updates_collections=ops.GraphKeys.UPDATE_OPS, and will not be updated immediately, but later (why it is  called \"delayed\") before the loss computation [here](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/training/python/training/training.py#L414).\r\n\r\nOn the other hand, _force_updates() will immediately update the moving mean and var, noting the added [control dependencies](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/layers/python/layers/layers.py#L333).", "Thanks for the clarification. What are the benefits of delaying the update ?", "You are welcome! Looks like it is a more efficient option in a distributed setting:\r\n\r\n\" One can set updates_collections=None to force the updates in place, but that\r\n  can have speed penalty, specially in distributed settings.\"\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/layers/python/layers/layers.py#L399\r\n\r\nBTW, I'm closing this issue as it has been resolved."]}, {"number": 7839, "title": " double free or corruption (!prev) and tensorboard show nothing", "body": "\r\n\r\n\r\nI'm new to use tensorflow.\r\nAnd I want to  learn how to use tensorboard. \r\nSo I run the demo code mnist_with_summaries.py, \r\nBut in the end it shows  \r\n \r\n*** Error in `/home/wxy/anaconda2/envs/tensorflow/bin/python': double free or corruption (!prev): 0x0000000002166320 ***\r\n\r\nAnd I input tensorboard --logdir=/tmp/mnist/logs and open  http://0.0.0.0:6006/ it shows nothing.\r\nso I follow the guide to input \r\n\r\n/tmp/mnist/logs | grep tfevents\r\n\r\nthen\r\n\r\n/tmp/mnist/logs/test/events.out.tfevents.1487915616.wxy\r\n/tmp/mnist/logs/train/events.out.tfevents.1487915615.wxy\r\n\r\ninput\r\n\r\ntensorboard --inspect --logdir=/tmp/mnist/logs\r\n\r\nthen \r\n\r\nProcessing event files... (this can take a few minutes)\r\n======================================================================\r\n\r\nFound event files in:\r\n/tmp/mnist/logs/test\r\n/tmp/mnist/logs/train\r\n\r\nThese tags are in /tmp/mnist/logs/test:\r\naudio -\r\nhistograms\r\n   layer1/Wx_plus_b/pre_activations\r\n   layer1/activations\r\n   layer1/biases/summaries/histogram\r\n   layer1/weights/summaries/histogram\r\n   layer2/Wx_plus_b/pre_activations\r\n   layer2/activations\r\n   layer2/biases/summaries/histogram\r\n   layer2/weights/summaries/histogram\r\nimages\r\n   input_reshape/input/image/0\r\n   input_reshape/input/image/1\r\n   input_reshape/input/image/2\r\n   input_reshape/input/image/3\r\n   input_reshape/input/image/4\r\n   input_reshape/input/image/5\r\n   input_reshape/input/image/6\r\n   input_reshape/input/image/7\r\n   input_reshape/input/image/8\r\n   input_reshape/input/image/9\r\nscalars\r\n   accuracy_1\r\n   cross_entropy_1\r\n   dropout/dropout_keep_probability\r\n   layer1/biases/summaries/max\r\n   layer1/biases/summaries/mean\r\n   layer1/biases/summaries/min\r\n   layer1/biases/summaries/stddev_1\r\n   layer1/weights/summaries/max\r\n   layer1/weights/summaries/mean\r\n   layer1/weights/summaries/min\r\n   layer1/weights/summaries/stddev_1\r\n   layer2/biases/summaries/max\r\n   layer2/biases/summaries/mean\r\n   layer2/biases/summaries/min\r\n   layer2/biases/summaries/stddev_1\r\n   layer2/weights/summaries/max\r\n   layer2/weights/summaries/mean\r\n   layer2/weights/summaries/min\r\n   layer2/weights/summaries/stddev_1\r\n======================================================================\r\n\r\nEvent statistics for /tmp/mnist/logs/test:\r\naudio -\r\ngraph -\r\nhistograms\r\n   first_step           0\r\n   last_step            990\r\n   max_step             990\r\n   min_step             0\r\n   num_steps            100\r\n   outoforder_steps     []\r\nimages\r\n   first_step           0\r\n   last_step            990\r\n   max_step             990\r\n   min_step             0\r\n   num_steps            100\r\n   outoforder_steps     []\r\nscalars\r\n   first_step           0\r\n   last_step            990\r\n   max_step             990\r\n   min_step             0\r\n   num_steps            100\r\n   outoforder_steps     []\r\nsessionlog:checkpoint -\r\nsessionlog:start -\r\nsessionlog:stop -\r\n======================================================================\r\n\r\nThese tags are in /tmp/mnist/logs/train:\r\naudio -\r\nhistograms\r\n   layer1/Wx_plus_b/pre_activations\r\n   layer1/activations\r\n   layer1/biases/summaries/histogram\r\n   layer1/weights/summaries/histogram\r\n   layer2/Wx_plus_b/pre_activations\r\n   layer2/activations\r\n   layer2/biases/summaries/histogram\r\n   layer2/weights/summaries/histogram\r\nimages\r\n   input_reshape/input/image/0\r\n   input_reshape/input/image/1\r\n   input_reshape/input/image/2\r\n   input_reshape/input/image/3\r\n   input_reshape/input/image/4\r\n   input_reshape/input/image/5\r\n   input_reshape/input/image/6\r\n   input_reshape/input/image/7\r\n   input_reshape/input/image/8\r\n   input_reshape/input/image/9\r\nscalars\r\n   accuracy_1\r\n   cross_entropy_1\r\n   dropout/dropout_keep_probability\r\n   layer1/biases/summaries/max\r\n   layer1/biases/summaries/mean\r\n   layer1/biases/summaries/min\r\n   layer1/biases/summaries/stddev_1\r\n   layer1/weights/summaries/max\r\n   layer1/weights/summaries/mean\r\n   layer1/weights/summaries/min\r\n   layer1/weights/summaries/stddev_1\r\n   layer2/biases/summaries/max\r\n   layer2/biases/summaries/mean\r\n   layer2/biases/summaries/min\r\n   layer2/biases/summaries/stddev_1\r\n   layer2/weights/summaries/max\r\n   layer2/weights/summaries/mean\r\n   layer2/weights/summaries/min\r\n   layer2/weights/summaries/stddev_1\r\n======================================================================\r\n\r\nEvent statistics for /tmp/mnist/logs/train:\r\naudio -\r\ngraph\r\n   first_step           0\r\n   last_step            0\r\n   max_step             0\r\n   min_step             0\r\n   num_steps            1\r\n   outoforder_steps     []\r\nhistograms\r\n   first_step           1\r\n   last_step            999\r\n   max_step             999\r\n   min_step             1\r\n   num_steps            900\r\n   outoforder_steps     []\r\nimages\r\n   first_step           1\r\n   last_step            999\r\n   max_step             999\r\n\r\nCan you tell me what's the problem? \r\nHow could I deal with it?\r\nSo I want some people to help me.\r\nThank you so much! ", "comments": ["This forum is for bug reports.  For use issues, please post to stackoverflow.   Since TensorFlow recently relased version 1.0 please make sure you're running the latest version.  If you suspect a bug, please report all the information requested in the issues template.", "@wxy920801 can you see if suggestions here help? https://github.com/tensorflow/tensorflow/issues/6968\r\ncc @jhseu ", "@yaroslavvb \r\nThank you!!\r\nI can't believe that solves my problem!", "@wxy920801 meaning you got tensorboard running by setting LD_PRELOAD ?", "Confirm `LD_PRELOAD=\"/usr/lib/libtcmalloc_minimal.so.4\" tensorboard --logdir ...` fixed it for me, ubuntu 14.04 with 1.0.0 container"]}, {"number": 7838, "title": "InvalidArgumentError Assign requires shapes of both tensors to match. lhs shape= [5] rhs shape= [1001]", "body": "Caused by op u'save/Assign', defined at:\r\n  File \"train_image_classifier.py\", line 585, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"train_image_classifier.py\", line 575, in main\r\n    init_fn=_get_init_fn(),\r\n  File \"train_image_classifier.py\", line 370, in _get_init_fn\r\n    ignore_missing_vars=FLAGS.ignore_missing_vars)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 579, in assign_from_checkpoint_fn\r\n    saver = tf_saver.Saver(var_list, reshape=reshape_variables)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 986, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1015, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 620, in build\r\n    restore_sequentially, reshape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 369, in _AddRestoreOps\r\n    assign_ops.append(saveable.restore(tensors, shapes))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 212, in restore\r\n    self.op.get_shape().is_fully_defined())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 45, in assign\r\n    use_locking=use_locking, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [5] rhs shape= [1001]\r\n         [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@InceptionV4/AuxLogits/Aux_logits/biases\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/\r\ncpu:0\"](InceptionV4/AuxLogits/Aux_logits/biases, save/restore_slice)]]\r\n\r\n\r\nThis error was caused when i pre-train fine-tuning inception-v4. I'm opening this issue.Can you help me?\r\n Thank you so much !", "comments": ["Make sure you're using the current version of tensorflow and any example programs.  If you suspect a bug, please provide the information requested in the issue template.  ", "Thanks for your response. I tried on version r0.11 and r0.12 . Both raised this error. I suspect checkpoint. Similarly, this error was also raised when i tried to pre-train with inception-v3, vgg-19 model. I see the instruction on model slim page about this error, but i don't know the reason why the differential between the value of lhs shape in my error and in the instruction.\r\nhttps://github.com/tensorflow/models/tree/master/slim#the-resnet-and-vgg-models-have-1000-classes-but-the-imagenet-dataset-has-1001  ", "The current version of TensorFlow is 1.0.  I don't know whether those checkpoints are compatible with older versions.  Have you tried the examples exactly as written?  ", "I have tried on version 1.0. But i faced another error:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_image_classifier.py\", line 585, in <module>\r\n    tf.app.run()\r\n  File \"/home/cloud/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"train_image_classifier.py\", line 448, in main\r\n    image = image_preprocessing_fn(image, train_image_size, train_image_size)\r\n  File \"/home/cloud/hoa/workspace/models/slim/preprocessing/preprocessing_factory.py\", line 70, in preprocessing_fn\r\n    image, output_height, output_width, is_training=is_training, **kwargs)\r\n  File \"/home/cloud/hoa/workspace/models/slim/preprocessing/inception_preprocessing.py\", line 302, in preprocess_image\r\n    return preprocess_for_train(image, height, width, bbox, fast_mode)\r\n  File \"/home/cloud/hoa/workspace/models/slim/preprocessing/inception_preprocessing.py\", line 195, in preprocess_for_train\r\n    tf.image_summary('image_with_bounding_boxes', image_with_box)\r\nAttributeError: module 'tensorflow' has no attribute 'image_summary'", "Please see this page which gives advice and tools for upgrading your model code to the new TF1.0 API: https://www.tensorflow.org/install/migration\r\n\r\nNote: I don't believe that all code in the tensorflow/models github repo has been migrated to TF1.0 yet!  If you encounter problems there, please contact the authors of the models, rather than filing a tensorflow bug.\r\n\r\nSince the error above is unrelated to the original issue, I'm going to close this thread.  Please open a separate issue if the issue persists after migrating to 1.0. ", "I set the `num_classes` in the inception_v3 model to 1001 and now it can restore the variables from the appropriate checkpoint. There seems to be a bug or the checkpoints are not updated yet."]}, {"number": 7837, "title": "Hands-on TensorBoard example issue on TF 1.0.0 Win GPU", "body": "### Environment info\r\nOperating System: Windows 10.0.14393, latest patches\r\n\r\nInstalled version of CUDA and cuDNN: \r\nnvcc --version says 8.0.44; CUDA is 8.0, cuDNN is 5.1\r\n05.09.2016  15:51            87.834 cuda.lib\r\n05.09.2016  15:51           681.064 cudadevrt.lib\r\n05.09.2016  15:51            64.550 cudart.lib\r\n05.09.2016  15:51         2.318.456 cudart_static.lib\r\n27.07.2016  09:35            37.452 cudnn.lib\r\n\r\nAnaconda 4.2.0\r\npip install --ignore-installed --upgrade tensorflow\r\ntensorflow.__version__ 1.0.0\r\n\r\nI ran the \r\n[example from Dandelion Mane's Hands-on TensorBoard talk at TF Dev Summit 2017](https://gist.github.com/dandelionmane/4f02ab8f1451e276fea1f165a20336f1#file-mnist-py)\r\n\r\n### What other attempted solutions have you tried?\r\nIt works with TF 1.0.0 on Ubuntu 16.04, both CPU and GPU versions.\r\n\r\n### Logs or other output that would be helpful\r\nOn TF Windows GPU (versions as noted above):\r\n```\r\n(tensorflow) C:\\dl\\tf\\mnist_tutorial>python mnist_tutorial.py\r\n```\r\n```\r\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\r\nExtracting log/data\\train-images-idx3-ubyte.gz\r\n...\r\nExtracting log/data\\t10k-labels-idx1-ubyte.gz\r\nStarting run for lr_1E-04,conv=2,fc=2\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\n...\r\n```\r\n\r\nThanks\r\nG.", "comments": ["This should be working... is there any problem apart from the logged errors about unknown ops (which are fixed at HEAD, but you can safely ignore)?", "Closing due to lack of activity.  Please reopen if necessary."]}, {"number": 7836, "title": "Breaking changes to API", "body": "`tf.nn.sampled_softmax_loss` and `tf.nn.nce_loss` have both changed their API such that you need to switch the `inputs, labels` to `labels, inputs` parameters. You might want to put a note here (under breaking changes): https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\r\n\r\nIncase you want to check here is the API of the two versions of tf:\r\nhttps://www.tensorflow.org/versions/r0.12/api_docs/python/\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/nce_loss\r\n\r\nAlthough I do find it odd that tf would simply change the order of these inputs.", "comments": ["@aselle Was this API change intentional?\r\nWe might have missed this in our list for 1.0.", "I think this was intentional and done by @martinwicke ", "This was intentional and done by @alextp. If it's not in the 1.0 release notes that was an oversight, these functions should have been included in the list of functions for which we switched arg order (such as softmax_cross_entropy_with_logits). We can add it to the 1.0 release note section of the release notes.", "Release.dm updated. Closing this issue."]}, {"number": 7835, "title": "TypeError: zeros_initializer() takes at least 1 argument", "body": "Tensorflow 0.12.1 (4d924e7)\r\nI cloned the up-to-dated tensorflow-models today. \r\n\r\nI got the following error msg:\r\nTraceback (most recent call last):\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/flowers_train.py\", line 41, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/flowers_train.py\", line 37, in main\r\n    inception_train.train(dataset)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/inception_train.py\", line 241, in train\r\n    scope, reuse_variables)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/inception_train.py\", line 109, in _tower_loss\r\n    scope=scope)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/inception_model.py\", line 87, in inference\r\n    scope=scope)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/slim/inception_model.py\", line 87, in inception_v3\r\n    scope='conv0')\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/slim/scopes.py\", line 155, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/slim/ops.py\", line 234, in conv2d\r\n    outputs = batch_norm(conv, **batch_norm_params)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/slim/scopes.py\", line 155, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/slim/ops.py\", line 88, in batch_norm\r\n    initializer=tf.zeros_initializer(),\r\nTypeError: zeros_initializer() takes at least 1 argument (0 given)\r\n\r\nI read through previous issues on zeors_initializer() & ones_initializer(). And I replaced zeros_initializer() w/ constant_initializer(0.0), but got the new error:\r\nTraceback (most recent call last):\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/flowers_train.py\", line 41, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/flowers_train.py\", line 37, in main\r\n    inception_train.train(dataset)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/inception_train.py\", line 241, in train\r\n    scope, reuse_variables)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/inception_train.py\", line 109, in _tower_loss\r\n    scope=scope)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/inception_model.py\", line 87, in inference\r\n    scope=scope)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/slim/inception_model.py\", line 125, in inception_v3\r\n    net = tf.concat([branch1x1, branch5x5, branch3x3dbl, branch_pool], 3)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1075, in concat\r\n    dtype=dtypes.int32).get_shape(\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 367, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 302, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.\r\n\r\nAnybody can help? Thanks a lot!", "comments": ["Sounds like a snag with TF 1.0 transition. Can you run with \"python -m pdb script.py\", and use \"up/down/print\" to get the relevant values (ie, it was expecting int32, what was the actual value it got, and why?) this may need reading source-code around those lines", "Thanks a lot for your info. \r\n\r\nI run into this issue when i perform the following command. (This command comes from Nvidia's official tutorial on using Tensorflow. http://www.nvidia.com/object/gpu-accelerated-applications-tensorflow-running-jobs.html.)\r\n\r\nbazel-bin/inception/flowers_train --train_dir=$FLOWERS_DIR/train --data_dir=$FLOWERS_DIR/data --pretrained_model_checkpoint_path=$INCEPTION_DIR/inception-v3/model.ckpt-157585 --fine_tune=True --initial_learning_rate=0.001 -input_queue_memory_factor=1 --max_steps=500 --num_gpus 1 --batch_size=64\r\n\r\nTherefore, I failed to use \"python -m pdb script.py\" to debug. \r\n\r\nAs its error msg indicated, \"TypeError: Expected int32, got list containing Tensors of type '_Message' instead.\" \r\n\r\nDoes this msg provide any info useful? Look forward to your kindly support.:-)", "BTW, you are using TF 12.1, while a lot of models have been updated to TF 1.0, can you try to use TF 1.0 instead?", "@yaroslavvb Thanks a lot for you advices. \r\n\r\nI tried to install tensorflow-gpu V1.0.0 using 2 approaches. \r\n1. build from source code followed the instructions on https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/g3doc/get_started/os_setup.md;\r\n2. sudo pip install --upgrade tensorflow-gpu\r\nBoth of these approaches give rise to the same error I mentioned earlier when trying Nvidia's official instructions on running inception V3 jobs (http://www.nvidia.com/object/gpu-accelerated-applications-tensorflow-running-jobs.html). \r\n\r\nomnisky@omnisky:~/data/tensorflow-models/inception$ bazel-bin/inception/flowers_train --train_dir=$FLOWERS_DIR/train --data_dir=$FLOWERS_DIR/data --pretrained_model_checkpoint_path=$INCEPTION_DIR/inception-v3/model.ckpt-157585 --fine_tune=True --initial_learning_rate=0.001 -input_queue_memory_factor=1 --max_steps=500 --num_gpus 1 --batch_size=64\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nTraceback (most recent call last):\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/flowers_train.py\", line 41, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/flowers_train.py\", line 37, in main\r\n    inception_train.train(dataset)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/inception_train.py\", line 217, in train\r\n    num_preprocess_threads=num_preprocess_threads)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/image_processing.py\", line 136, in distorted_inputs\r\n    num_readers=FLAGS.num_readers)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/image_processing.py\", line 490, in batch_inputs\r\n    example_serialized)\r\n  File \"/home/omnisky/data/tensorflow-models/inception/bazel-bin/inception/flowers_train.runfiles/inception/inception/image_processing.py\", line 397, in parse_example_proto\r\n    bbox = tf.concat(0, [ymin, xmin, ymax, xmax])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1047, in concat\r\n    dtype=dtypes.int32).get_shape(\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 651, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 716, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 367, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 302, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.", "I found it was caused by a switch of parameter positions in tf.concat. But another error popped up \"AttributeError: 'module' object has no attribute 'image_summary'\". I will probably give up inception and go on with TF-slim instead. Thank you anyway. @yaroslavvb ", "@gitmcdull ps, conversion script is at https://www.tensorflow.org/install/migration . Not sure what the status of 1.0 compatibility for tensorflow/models, I've had to upgrade resnet [myself](https://github.com/tensorflow/models/pull/959), it might help to file bugs on tensorflow/models with respective maintainers if you see any that aren't 1.0 ready"]}, {"number": 7834, "title": "Fixed typo in _DNNEstimator doc and minor reformat", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 7833, "title": "Add a non-debug avx2 fma build", "body": "", "comments": ["FYI, I may be a little slow in reviewing PRs in the next couple of days.", "I have some concerns about this cl.\r\n1-ci_parameterized_build script is already trying to do too much. It is getting more and more confusing with each addition.\r\n2- Im not sure if all our machines have avx2 (Some old machines on GCE, and we cannot really select CPUs.) I just checked, and we have a lot of builds running on Ivy bridge machines.\r\n\r\nSo I think, we should \r\n1-Get dedicated machines to run these builds.\r\n2-Also, we can have a dedicated script under `ci_build/linux/optimized`?", "1. Yeah, I agree. There's a lot of code that can be deleted in ci_parameterized_build right now (e.g., TF_BUILD_IS_OPT, TF_BUILD_MAVX now that TF_BUILD_OPTIONS works with pip.sh). I'd favor migrating this script to Python (all machines we use have Python preinstalled) so it's more maintainable, and then this list can just be a Python dictionary. I think cleaning up the script should be separated from this change, though.\r\n2. Yeah, we should get AVX2/FMA-supported machines, but it's not an immediate priority because:\r\n- You can build and just choose not to run tests on Jenkins. That's what I'm doing right now.\r\n- Some tests fail with AVX2/FMA anyway because people are hardcoding epsilons in tests that are too small. We need an ongoing AVX2/FMA test somewhere.\r\n- Testing manually doesn't take much time."]}, {"number": 7832, "title": "Android Improvements", "body": "This branch adds minor improvements to TensorFlow on Android.\r\n\r\n1. e02edef lowers API support to level 14 \u2014 which is confirmed to work, rather than level 21 as indicated right now. This setting is commented out by default, but could be a useful guideline to most users. It might be more interesting to update the CI server to naturally produce nightly builds at that level, but it doesn\u2019t seem like I can send a PR for that (unless I missed it).\r\n\r\n2. 5971f69 introduces the use of [GC sections](https://gcc.gnu.org/onlinedocs/gnat_ugn/Compilation-options.html) when compiling libtensorflow_inference.so, which has the effect of reducing final binary size.\r\n\r\n    At the time of this PR, the following improvements were observed:\r\n    \r\n    |  arch | master | this branch | delta | % |\r\n    |  ------ | ------ | ------ | ------ | ------ |\r\n    |  armeabi-v7a | 10197000 bytes | 9353164 bytes | -0.8 MB | -8% |\r\n    |  arm64-v8a | 16568952 bytes | 14693056 bytes | -1.8 MB | -11% |\r\n    |  x86 | 16631748 bytes | 14751612 bytes | -1.8 MB | -11% |\r\n    |  x86_64 | 16509880 bytes | 14715632 bytes | -1.7 MB | -11% |\r\n\r\n    Final observed impact on an app is around 1MB on a unified release APK.", "comments": ["Can one of the admins verify this patch?", "@andrewharp PTAL", "Just wondering, is there anything else I need to do here? It seems like the Sanity Checks haven\u2019t been completed/reported yet. cc @andrewharp ", "@tensorflow-jenkins Test this, please.", "Jenkins, test this please", "Sweet, thanks for the help @andrewharp @dandelionmane @jhseu. And thanks @petewarden for giving me the motivation to get into TensorFlow in the first place. Y\u2019all are doing an amazing job with this project & this community, I hope you know how much you\u2019re killing it! \ud83c\udfc6"]}, {"number": 7831, "title": "Updated strided_slice doc for #7815", "body": "Tried fixing the documentation. Someone should verify it though, cause I'm not sure I got it right", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 7830, "title": "Error while trying out mnist_softmax", "body": "Hello everyone,\r\n\r\nI am using python3.4.2 and tensorflow-1.0.0\r\n\r\nI am getting started with tensorflow and so was trying out MNIST For ML Beginners (https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist_softmax.py)\r\n\r\nI am getting the following output:\r\n\r\n$ python3 test2.py --data_dir=./MNIST_data\r\nExtracting ./MNIST_data/train-images-idx3-ubyte.gz\r\nExtracting ./MNIST_data/train-labels-idx1-ubyte.gz\r\nExtracting ./MNIST_data/t10k-images-idx3-ubyte.gz\r\nExtracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n0.9187\r\n*** Error in `python3': free(): invalid next size (normal): 0x00000000025985f0 ***\r\n[1]    25610 abort      python3 test2.py --data_dir=./MNIST_data\r\n\r\nIf someone could help me fix the Error in 'python3': free() problem.\r\n\r\nThank You", "comments": ["@vivek-roy Are you sure the version of TensorFlow that you've installed matches your python version?  The following documentation might help:\r\nhttps://www.tensorflow.org/install/install_linux#the_url_of_the_tensorflow_python_package\r\n\r\nAlso - please provide details about what platform you are using  (operating system, architecture).  Thanks!", "Closing due to lack of activity.  Please reopen if necessary.", "@tatatodd @prb12 I'm getting this error as well, though not on MNIST; I'm training a simple logistic regression model using tensorflow and keras.  Also, it doesn't seem to happen every time, and it's not always the same error message.  Sometimes it's \r\n\r\n```\r\n*** Error in `/home/ubuntu/babilonia-training/.tox/py36/bin/python': free(): invalid pointer: 0x0000000000f63930 ***\r\n```\r\n\r\nAll my code runs fine, passing the tests I wrote, it's just at the end that the process crashes with an error message.  It never seems to happen when I run on MacOS; just when my tests run on CircleCI.\r\n\r\nTo answer your previous question, I installed tensorflow using pip.  The wheel is `tensorflow-1.0.1-cp36-cp36m-manylinux1_x86_64.whl` and the test is running on python 3.6.  Here's the output from nose:\r\n\r\n```\r\npy36 create: /home/ubuntu/babilonia-training/.tox/py36\r\npy36 installdeps: setuptools>=17.1\r\npy36 inst: /home/ubuntu/babilonia-training/.tox/dist/babilonia_training-0.1.0.zip\r\npy36 installed: annoy==1.8.3,appdirs==1.4.3,babilonia-training==0.1.0,click==6.7,cloudpickle==0.2.2,cymem==1.31.2,cytoolz==0.8.2,decorator==4.0.11,flexmock==0.10.2,future==0.16.0,h5py==2.6.0,hyperopt==0.1,Keras==2.0.0,murmurhash==0.26.4,networkx==1.11,nose==1.3.7,numpy==1.12.0,packaging==16.8,pathlib==1.0.1,plac==0.9.6,preshed==0.46.4,protobuf==3.2.0,pymongo==3.4.0,pyparsing==2.2.0,PyYAML==3.12,scikit-learn==0.18.1,scipy==0.19.0,semver==2.7.6,six==1.10.0,spacy==1.6.0,sputnik==0.9.3,tensorflow==1.0.1,thinc==6.2.0,toolz==0.8.2,tqdm==4.11.2,ujson==1.35\r\npy36 runtests: PYTHONHASHSEED='2028354074'\r\npy36 runtests: commands[0] | python setup.py nosetests --with-coverage --cover-package=babilonia_training --cover-erase --cover-html --logging-level=WARN\r\nrunning nosetests\r\nrunning egg_info\r\nwriting babilonia_training.egg-info/PKG-INFO\r\nwriting dependency_links to babilonia_training.egg-info/dependency_links.txt\r\nwriting entry points to babilonia_training.egg-info/entry_points.txt\r\nwriting requirements to babilonia_training.egg-info/requires.txt\r\nwriting top-level names to babilonia_training.egg-info/top_level.txt\r\nreading manifest file 'babilonia_training.egg-info/SOURCES.txt'\r\nreading manifest template 'MANIFEST.in'\r\nwarning: no files found matching 'AUTHORS.rst'\r\nwarning: no files found matching 'CONTRIBUTING.rst'\r\nwarning: no files found matching 'HISTORY.rst'\r\nwarning: no files found matching 'LICENSE'\r\nwarning: no files found matching 'README.rst'\r\nwarning: no previously-included files matching '__pycache__' found under directory '*'\r\nwarning: no previously-included files matching '*.py[co]' found under directory '*'\r\nwarning: no files found matching '*.rst' under directory 'docs'\r\nwarning: no files found matching 'conf.py' under directory 'docs'\r\nwarning: no files found matching 'Makefile' under directory 'docs'\r\nwarning: no files found matching 'make.bat' under directory 'docs'\r\nwarning: no files found matching '*.jpg' under directory 'docs'\r\nwarning: no files found matching '*.png' under directory 'docs'\r\nwarning: no files found matching '*.gif' under directory 'docs'\r\nwriting manifest file 'babilonia_training.egg-info/SOURCES.txt'\r\nSearching for coverage>=3.7.1\r\nReading https://pypi.python.org/simple/coverage/\r\nDownloading https://pypi.python.org/packages/6e/33/01cb50da2d0582c077299651038371dba988248058e03c7a7c4be0c84c40/coverage-4.3.4.tar.gz#md5=89759813309185efcf4af8b9f7762630\r\nBest match: coverage 4.3.4\r\nProcessing coverage-4.3.4.tar.gz\r\nWriting /tmp/easy_install-1dstgqnl/coverage-4.3.4/setup.cfg\r\nRunning coverage-4.3.4/setup.py -q bdist_egg --dist-dir /tmp/easy_install-1dstgqnl/coverage-4.3.4/egg-dist-tmp-l3rhxkzv\r\nwarning: no previously-included files found matching 'ci/appveyor.token'\r\nno previously-included directories found matching 'doc/_build'\r\nno previously-included directories found matching 'tests/eggsrc/build'\r\nno previously-included directories found matching 'tests/eggsrc/dist'\r\nno previously-included directories found matching 'tests/eggsrc/*.egg-info'\r\nwarning: no previously-included files matching '*.py[co]' found anywhere in distribution\r\ncreating /home/ubuntu/babilonia-training/.eggs/coverage-4.3.4-py3.6-linux-x86_64.egg\r\nExtracting coverage-4.3.4-py3.6-linux-x86_64.egg to /home/ubuntu/babilonia-training/.eggs\r\n\r\nInstalled /home/ubuntu/babilonia-training/.eggs/coverage-4.3.4-py3.6-linux-x86_64.egg\r\nUsing TensorFlow backend.\r\n  0%|          | 0/1 [00:00<?, ?it/s]100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:08<00:00,  8.51s/it]\r\n...\r\nName                                             Stmts   Miss  Cover\r\n--------------------------------------------------------------------\r\nbabilonia_training.py                                6      0   100%\r\nbabilonia_training/cli.py                           38      2    95%\r\nbabilonia_training/config.py                         4      0   100%\r\nbabilonia_training/dataset.py                       15      0   100%\r\nbabilonia_training/dirs.py                           5      0   100%\r\nbabilonia_training/eval.py                           3      0   100%\r\nbabilonia_training/eval/commands.py                 28      0   100%\r\nbabilonia_training/eval/grad_similarity_map.py       6      0   100%\r\nbabilonia_training/eval/predict.py                  27      0   100%\r\nbabilonia_training/eval/top.py                      15      0   100%\r\nbabilonia_training/eval/visualize.py                63      0   100%\r\nbabilonia_training/fit.py                           62      0   100%\r\nbabilonia_training/model.py                         17      1    94%\r\nbabilonia_training/model_info.py                    11      0   100%\r\nbabilonia_training/params.py                        24      0   100%\r\nbabilonia_training/search_hyper_parameters.py       38      1    97%\r\nbabilonia_training/seed_cluster.py                  13      0   100%\r\nbabilonia_training/train.py                          2      0   100%\r\nbabilonia_training/train/commands.py                27      0   100%\r\nbabilonia_training/train/final.py                   11      0   100%\r\nbabilonia_training/train/kfold.py                   46      0   100%\r\nbabilonia_training/util.py                           0      0   100%\r\nbabilonia_training/util/func.py                      6      0   100%\r\nbabilonia_training/util/hyperopt.py                  8      0   100%\r\nbabilonia_training/util/io.py                        7      0   100%\r\nbabilonia_training/util/keras.py                    32      0   100%\r\nbabilonia_training/util/math.py                      5      0   100%\r\nbabilonia_training/util/spacy.py                    19      0   100%\r\nbabilonia_training/vocab.py                         29      0   100%\r\n--------------------------------------------------------------------\r\nTOTAL                                              567      4    99%\r\n----------------------------------------------------------------------\r\nRan 4 tests in 237.987s\r\n\r\nOK\r\n*** Error in `/home/ubuntu/babilonia-training/.tox/py36/bin/python': free(): invalid next size (normal): 0x0000000002762240 ***\r\nERROR: InvocationError: '/home/ubuntu/babilonia-training/.tox/py36/bin/python setup.py nosetests --with-coverage --cover-package=babilonia_training --cover-erase --cover-html --logging-level=WARN'\r\n```"]}, {"number": 7829, "title": "Branch 148383964", "body": "", "comments": []}, {"number": 7828, "title": "Adding missing close parenthesis in code listing.", "body": "Fixing code syntax error that is already corrected in 1.0 and master branches.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "The test failures are due to some buildifier changes between r0.12 and now and are unrelated to the doc-only changes in this PR. Merging PR now."]}, {"number": 7827, "title": "Adding missing close parenthesis in code listing.", "body": "Fixing code syntax error that is already corrected in 1.0 and master branches.", "comments": ["@sandersk, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener to be a potential reviewer.", "Can one of the admins verify this patch?", "Closing, duplicate PR.", "@maciekcc, did you close because it's a duplicate of #7828? I made 2 separate PRs: one to merge this fix into the r0.11 branch (this one) and one to merge this fix into the r0.12 branch (#7828), so that the tutorial doc will be correct in both API versions on tf.org.", "Oh, sorry, didn't notice :/", "Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "@tensorflow-jenkins test this please", "The test failures are due to some buildifier changes between r0.11 and now and are unrelated to the doc-only changes in this PR. Merging PR now."]}, {"number": 7826, "title": "bazel error", "body": "Hello everyone,\r\n![bazelerror](https://cloud.githubusercontent.com/assets/20130992/23277161/ec375632-f9da-11e6-8117-d834540c3d4e.png)\r\n\r\nAs I am having an error \"AttributeError: 'module' object has no attribute 'GRUCell'\" when tried to run transate.py i removed tensorflow from my system and started installing it again from sources. I installed bazel and configured the installation. After \"Build the pip package\", I am getting the following error.\r\n\r\nSystem specifications:\r\nI installed Ubuntu 16.04 on vmware with 2 GB RAM and 100 GB disk space \r\n\r\nCan anyone please help me in resolving these errors\r\n\r\nThanks in advance ", "comments": ["Hello @ymakkapa ,\r\nAs suggested by your console, did you try executing Sudo with -H option?\r\n I had a similar issue and I used virtual environment because I did not have root access. Please try these options and let me know what happened.", "Hello,\r\nWhich command should I execute with -H option?", "Hello,\r\nI did as u said and still got these following errors\r\n![baze](https://cloud.githubusercontent.com/assets/20130992/23278861/edcc001e-f9e0-11e6-9804-9cae3494ed47.PNG)\r\n", "After configuring, you need to make sure that you have this command executed ccorrectly:\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\nThen, you need to make sure you enter the pip install command with sudo -H option.\r\nGiven your RAM, I suggest you limit the system to use only less memory with the following command instead:\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --local_resources 1024,.5,1.0`", "Yes, I did the same way. the command \"bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\" is executed previously. please see the picture attached\r\n![b2](https://cloud.githubusercontent.com/assets/20130992/23279448/52b18a2e-f9e3-11e6-9403-7e248c193f11.PNG)\r\n\r\n", "are you still having trouble?", "Yes @poxvoculi ", "your screen image does not indicate a problem.", "This is my problem actually\r\n![baze](https://cloud.githubusercontent.com/assets/20130992/23317098/076a3e9a-fa9b-11e6-9c46-4c7a8c735624.PNG)\r\n", "You're trying to install a file that doesn't exist.", "I don't see a problem here. If you have evidence of an issue with tensorflow, please open a new issue."]}, {"number": 7825, "title": "tf version not working on older linux version", "body": "Hi, may I ask if `tf` is built on `ubuntu`? The many linux wheel from pip or even anaconda causes errors on systems such as `CentOS 6.6`. Particularly it is looking for a `GLIBC-2.16.so` but can't find it. On my system there exists only `GLIBC-2.14.so`. Does anyone have any solution for this?\r\n", "comments": ["For older versions of linux (incl. CentOS 6.x) you have to build from source. ", "@martinwicke thanks for the answer. But for some of us operating on a cluster with limited permissions that's not an option. Because the cluster might be missing some libraries required to build from source and it's impossible to install them if you don't have permissions.", "I am sorry. We have no way to build for all platforms out there. If the binaries we publish don't work on your platform, you have to build from source. Our build process has dependencies too, if you have no way to install those dependencies, you're out of luck. \r\n\r\nOften, the simplest solution is docker, or using an installation of your target system inside a docker container running on a modern machine to build.", "@martinwicke thanks a lot, appreciated. I've tried many things but nothing has worked so far. I'm hopping the guys from anaconda keep up they word and resolve this issue with the next release of anaconda. They said the next release would be able to build packages from source. Let's hope so."]}, {"number": 7824, "title": "Primary install page returning 404 https://www.tensorflow.org/get_started/os_setup ", "body": "https://www.tensorflow.org/get_started/os_setup \r\n\r\n404. That's an error.\r\n\r\nThe requested URL was not found on this server.\r\nThat's all we know.", "comments": ["Hey, I'm assuming you got to that page through a Google search, or a link from elsewhere, not through navigation on the Tensorflow site itself, right?\r\n\r\nWe've been redoing a lot of the site structure for tensorflow.org and haven't propagated all of it out to github quite yet, but the entire installing section of getting started -- covered in the os_setup doc -- is now here: \r\n\r\nhttps://www.tensorflow.org/install/\r\n\r\nI'll put in a redirect for that particular file though, since we didn't catch it before, so it should go to the proper place instead of a 404 for you.", "Correct, the page was a google search result. Thank you.\n\nOn Fri, Feb 24, 2017 at 8:11 PM, Deanna Rubin <notifications@github.com>\nwrote:\n\n> Hey, I'm assuming you got to that page through a Google search, or a link\n> from elsewhere, not through navigation on the Tensorflow site itself, right?\n>\n> We've been redoing a lot of the site structure for tensorflow.org and\n> haven't propagated all of it out to github quite yet, but the entire\n> installing section of getting started -- covered in the os_setup doc -- is\n> now here:\n>\n> https://www.tensorflow.org/install/\n>\n> I'll put in a redirect for that particular file though, since we didn't\n> catch it before, so it should go to the proper place instead of a 404 for\n> you.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7824#issuecomment-282446692>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABXVTazlt0FIx-xMN1cqfJzsvwvtf0qyks5rf3_JgaJpZM4MKV0F>\n> .\n>\n"]}, {"number": 7823, "title": "Gradient Boosting Decision Trees at TensorFlow", "body": "At TensorFlow Dev Summit 2017 Mr. Ashish Agarwal gave a presentation for ML Toolkit and said that Gradient Boosting Decision Trees [are coming soon](https://youtu.be/Tuv5QYKU-MM?t=271).  I would like to ask two questions:\r\n\r\n * Is there an approximate timeline for when this feature might be released? \r\n * Is there any information if we will be able to leverage tensorflow's local (GPU computing) and distributed capabilities (for example as [presented by Mr. Jonathan Hseu](https://www.youtube.com/watch?v=yALzr4A2AzY)) for this particular algorithm?\r\n", "comments": ["We are hoping to open source these in Q2. Initial support will be on CPU and distributed.", "Hi\r\n\r\nHas this been released?", "Hello Guys,\r\nIs there any update on this issue? ", "I can't find anything in the 1.2 release notes for GBM's. Only improvements in deployment for Random Forest Implementation.", "Waiting for this too.  Also wondering if the boosted tree and random tree models will have regression not just classification?", "+1 on this - we are interested in regression trees for tensorflow", "+1\r\n", "Seems to be a lot of activity:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/boosted_trees", "The [tf.GradientBoostedDecisionTree](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/boosted_trees/estimator_batch/estimator.py) estimators inherit the \"old\" tf.contrib.learn.Estimator. Will these be updated to inherit the \"stable\" tf.estimator.Estimator instead?", "@nicholas-leonard Yes. That's the plan. \r\n", "Hello, \r\nWill TF Boosted Trees will be supported on GPU soon ? \r\n\r\nThx", "+1", "Do TF Boosted Trees support distributed training?", "Yes, distributed training is supported. \r\nThere are no plans to support GPU training at the moment but contributions are always welcomed :) ", "+1 on contributions welcome. I added the label.", "@tanzhenyu The issue has been closed. Maybe reopen since contribution are welcome?"]}, {"number": 7821, "title": "Cannot upgrade to CUDNN 5.1 (How to compile using CUDNN 5 and CUDA 7.5)", "body": "I am using a University Cluster with Tesla K80 with CUDA 7.5 and CUDnn 5.0.5. I don't have root access and cannot upgrade either CUDA or CUDnn. I have tried compiling TF 1.0 from sources using bazel by configuring CUDnn to 5.0.5. I am still experiencing this error while running deep mnist.\r\n\r\n2017-02-23 12:29:43.883764: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 5110 (compatibility version 5100) but source was compiled with 5005 (compatibility version 5000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n2017-02-23 12:29:43.883914: F tensorflow/core/kernels/conv_ops.cc:654] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\nAborted\r\n\r\nHowever, the mnist with simple linear model worked. Please let me know a way around this and also if you need anything else.\r\n\r\nThanks", "comments": ["Hello,\r\n\r\nI have copied the CUDNN 5.1 binaries to a virtual environment in my local space for CUDA 7.5. I have tried installing Tensorflow version 0.10.0 using wheel given by the tensorflow official website and it worked.\r\nI will try and install Tensorflow 1.0 from source again and will post any updates here.\r\n\r\nThanks", "Hi, [TF 1.0 with CUDA 7.5 and CUDNN 5.1 on Tesla K80 with compute capability 3.7] successfully installed from sources without root access but using virtualenv.  Upgrading to CUDNN 5.1 fixed my problem temporarily. but building the tf source when built with CUDNN 5.0 failed.\r\n\r\nI am closing this issue for now as upgrading to CUDNN 5.1 was a breeze even without root access. Anyone can request the wheel for TF 1.0 with CUDA 7.5 and CUDNN 5.1, compute capability 3.7 on a Tesla K80 from me directly."]}, {"number": 7820, "title": "Improved support for AVX512.", "body": "Added missing definition for the Packet16q16i.\r\nFixed a couple of bugs in the implementation of max reductions for\r\navx512", "comments": ["```\r\n/workspace/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:291:10: error: no match for 'operator=' (operand types are 'Eigen::QInt16' and 'const Eigen::internal::Packet16q16i')\r\n {  (*to) = from; }\r\n```", "Jenkins, test this please.", "Ready to go, right?", "It's ready. "]}, {"number": 7819, "title": "XLA build error on macOS 10.12.4 Beta", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNothing seems to be related\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nmacOS 10.12.4 Beta\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nNone\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`): 4ac9c09d5ca57a03b8daa5fb9e295947b1619854\r\n2. The output of `bazel version`: \r\n```\r\nBuild label: 0.4.4-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Feb 2 01:05:15 2017 (1485997515)\r\nBuild timestamp: 1485997515\r\nBuild timestamp as int: 1485997515\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nEnable XLA at configuration and build with `bazel build --config=opt`\r\n\r\n### What other attempted solutions have you tried?\r\nBuilding without XLA is working smoothly.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```bash\r\nERROR: /Users/inflation/workspace/tensorflow/tensorflow/compiler/xla/service/BUILD:108:1: C++ compilation of rule '//tensorflow/compiler/xla/service:versioned_computation_handle' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 93 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from tensorflow/compiler/xla/service/versioned_computation_handle.cc:16:\r\n./tensorflow/compiler/xla/service/versioned_computation_handle.h:33:19: error: unknown type name 'int64'; did you mean 'tensorflow::int64'?\r\n  using Version = int64;\r\n                  ^~~~~\r\n                  tensorflow::int64\r\n./tensorflow/core/platform/default/integral_types.h:27:19: note: 'tensorflow::int64' declared here\r\ntypedef long long int64;\r\n                  ^\r\nIn file included from tensorflow/compiler/xla/service/versioned_computation_handle.cc:16:\r\n./tensorflow/compiler/xla/service/versioned_computation_handle.h:38:3: error: unknown type name 'string'; did you mean 'std::string'?\r\n  string ToString() const;\r\n  ^~~~~~\r\n  std::string\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/iosfwd:194:65: note: 'std::string' declared here\r\ntypedef basic_string<char, char_traits<char>, allocator<char> > string;\r\n                                                                ^\r\ntensorflow/compiler/xla/service/versioned_computation_handle.cc:22:1: error: unknown type name 'string'; did you mean 'std::string'?\r\nstring VersionedComputationHandle::ToString() const {\r\n^~~~~~\r\nstd::string\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/iosfwd:194:65: note: 'std::string' declared here\r\ntypedef basic_string<char, char_traits<char>, allocator<char> > string;\r\n```\r\n", "comments": ["I believe this was @meheffernan's bug.  Should be fixed in next uplift.\r\n\r\nSorry about the breakage.", "I believe the relevant uplift has occurred."]}, {"number": 7818, "title": "sparse_placeholder after SparseTensor eval method changed", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it ", "CLAs look good, thanks!\n\n<!-- ok -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "I signed it", "Can one of the admins verify this patch?", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 7817, "title": "Reading data with queue is even slower than using feed_dict", "body": "It seems reading data with feed_dict is inefficient in tensorflow (I found it 3-4 times slower than a theano-based implementation with a totally same network structure), I turn to a queue-based method as official recommendation. However, my experiment result give a even worse performance. Is there anything wrong ?\r\n\r\n### Related issue\r\n[#3377 Moving data from CPU to GPU is slow](https://github.com/tensorflow/tensorflow/issues/3377)\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\nCPU: Intel i4790-k\r\nGPU: Nvidia GTX 1070 (8 GB)\r\nMemory: 16 GB\r\nTensorflow version: 1.0\r\nCUDA version: 8.0\r\ncuDNN version: 5.0\r\n\r\n### Implementation Example\r\n1) Using feed_dict\r\n```python\r\nx, y = np.array(...)\r\nin_x, in_y = tf.placeholder(...)\r\n\"\"\" f() is some computation in the network, including embedding_lookup, \r\nbidirectional_dynamic_rnn, dense \"\"\"\r\ntrain_op = f(in_x, in_y) \r\nsess = tf.Session()\r\nfor _ in range(num_epochs): # num_epochs is 100 here\r\n    sess.run(train_op, {in_x: x, in_y: y})\r\n```\r\n2) Using queue\r\n```python\r\nx, y = np.array(...)\r\nx, y = tf.convert_to_tensor(...)\r\nin_x, in_y = tf.train.slice_input_producer([x, y], num_epochs=100)\r\nin_x, in_y = tf.train.batch([in_x, in_y], batch_size=32)\r\ntrain_op = f(in_x, in_y)\r\nsess = tf.Session()\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(sess, coord)\r\ntry:\r\n    while not coord.should_stop():\r\n        sess.run(train_op)\r\n    except Exception as e:\r\n        coord.request_stop(e)\r\n    finally:\r\n        coord.request_stop()\r\ncoord.join(threads)\r\n```", "comments": ["I troubleshooted similar issue [here](http://stackoverflow.com/questions/39840323/benchmark-of-howto-reading-data/39842628#39842628) and it was caused by Python thread scheduler choosing a bad strategy. Essentially Python would schedule computation thread that issues a single enqueue call, then this thread would block and have to be pre-empted. Python doesn't support parallel execution of Python code and pre-emption is slow so this part is a performance hit. Eventually it pre-empts dequeue (main) thread to schedule enqueue thread, which does a single enqueue call before Python decides to give execution back to main thread. This back-and-forth dance introduced 10x slowdown in the pipeline.\r\n\r\nOne solution is to make sure that you never have queue starvation, for instance, by making enqueue part faster (ie by using enqueue many as here https://github.com/tensorflow/tensorflow/issues/3009 ), making dequeue part slower (ie, add more computation to `train_op` until it becomes slow enough to be bottleneck), and by letting queue runners add some things to the queue (make queue larger and add `time.sleep(1)` right after `start_queue_runners`) ", "@yaroslavvb Thx for your advice.\r\nHowever, I fail to fix the problem. I try replace *tf.train.batch* with *tf.train.shuffle_batch* and set *min_after_dequeue=10000*  to avoid queue starvation (I assume this parameter help fix it, right ?), but my program as slow as before.\r\nWhat's more, I follow the [link](https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225754659) that you mentioned to generate timeline. It did take a long time to generate a timeline.json file (600 MB+, with correct .json file format), but failed to be visualized using *chrome://tracing* and give a blank output. Do you have any idea of what's going wrong ?", "What I meant was -- make sure your `dequeue` part of the graph is slower than your `enqueue` part. Pre-loading the queue is nota solution on its own because if you dequeue faster than you enqueue, eventually you'll consume all the elements on the queue and be back to queue starvation (check your queue size, there's queue fullness statistic in TensorBoard and also q.size() operation). The reality is that queues are bad when your computation is extremely cheap because of the way Python multi-threading works, feed_dict may actually be better for those cases.", "@yaroslavvb Thank you for reminding me of checking in TensorBoard.\r\nI do some test and it seems that *enqueue* part is not a bottleneck of the program, it's full (and capacity is large enough) all the time. There is no problem of starvation.\r\nAnd I find that *dequeue* operation really cause problems, according to the compute time statistics in TensorBoard, *dequeue* cost 6x compute time than my model, which result in low GPU-Util and even worse performance than using feed_dict.", "You could narrow it down by making your program single threaded - first\npreload the queue and then consume it in the same thread. If it's still\nslow, then could narrow it down to a case that spends most time in dequeue\nand is single threaded, at which point CPU profile can tell where the time\nis spent\n\nOn Feb 27, 2017 2:43 AM, \"akaitsuki-ii\" <notifications@github.com> wrote:\n\n> @yaroslavvb <https://github.com/yaroslavvb> Thank you for reminding me of\n> checking in TensorBoard.\n> I do some test and it seems that *enqueue* part is not a bottleneck of\n> the program, it's full (and capacity is large enough) all the time. There\n> is no problem of starvation.\n> And I find that *dequeue* operation really cause problems, according to\n> the compute time statistics in TensorBoard, *dequeue* cost 6x compute\n> time than my model, which result in low GPU-Util and even worse performance\n> than using feed_dict.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7817#issuecomment-282686023>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHL5LVWVVBOMnyKTCJPSkiGVC56Ioks5rgqi3gaJpZM4MJ7ZR>\n> .\n>\n", "BTW, here's an example of making reader pipeline single-threaded -- https://github.com/yaroslavvb/stuff/blob/master/ericyue-slowreader/benchmark-batch.py\r\n\r\nEspecially see the line with `sess.run(a_queue_qr.enqueue_ops)`. Restructuring the problem in this way can help rule out some Python-threads related strangeness going on ", "@yaroslavvb Sorry for check it so late.\r\nI did preload the queue full enough (using single thread `sess.run(enqueue_ops)`) this time before start running my model and consume the queue, there is no more enqueue operation after dequeue is start.\r\nBut it seems to be very slow for dequeue in this situation.\r\n\r\nThe following is the graph and node metadata showed in TensorBoard. You can see that compute time of dequeue operation is 24.2ms, which is much longer than that of my model, 1.36ms.\r\n![graph](https://cloud.githubusercontent.com/assets/8386553/23578611/322056d4-0116-11e7-9f48-621e7b6a55d0.png)\r\n![dequeue-status](https://cloud.githubusercontent.com/assets/8386553/23578630/a4d7c284-0116-11e7-81f4-7d4aec8b49cd.png)\r\n![model-status](https://cloud.githubusercontent.com/assets/8386553/23578632/aecdc41e-0116-11e7-8047-501c6e3dd442.png)\r\n\r\nHowever, if I replace my model with much light operation like `tf.square`, dequeue will be much faster with same tensors output.\r\n![graph-light](https://cloud.githubusercontent.com/assets/8386553/23578663/45ab84e8-0117-11e7-9d6f-7c6316857d2f.png)\r\n![dequeue-status-light](https://cloud.githubusercontent.com/assets/8386553/23578664/494bfe02-0117-11e7-94e0-a43dccceb613.png)\r\n\r\nI can not figure out what the problem is...?", "Can you share self contained reproducible example?\n\nOn Mar 4, 2017 4:24 AM, \"akaitsuki-ii\" <notifications@github.com> wrote:\n\n> @yaroslavvb <https://github.com/yaroslavvb> Sorry for check it so late.\n> I did preload the queue full enough (using single thread\n> sess.run(enqueue_ops)) this time before start running my model and\n> consume the queue, there is no more enqueue operation after dequeue is\n> start.\n> But it seems to be very slow for dequeue in this situation.\n>\n> The following is the graph and node metadata showed in TensorBoard. You\n> can see that compute time of dequeue operation is 24.2ms, which is much\n> longer than that of my model, 1.36ms.\n> [image: graph]\n> <https://cloud.githubusercontent.com/assets/8386553/23578611/322056d4-0116-11e7-9f48-621e7b6a55d0.png>\n> [image: dequeue-status]\n> <https://cloud.githubusercontent.com/assets/8386553/23578630/a4d7c284-0116-11e7-81f4-7d4aec8b49cd.png>\n> [image: model-status]\n> <https://cloud.githubusercontent.com/assets/8386553/23578632/aecdc41e-0116-11e7-8047-501c6e3dd442.png>\n>\n> However, if I replace my model with much light operation like tf.square,\n> dequeue will be much faster with same tensors output.\n> [image: graph-light]\n> <https://cloud.githubusercontent.com/assets/8386553/23578663/45ab84e8-0117-11e7-9d6f-7c6316857d2f.png>\n> [image: dequeue-status-light]\n> <https://cloud.githubusercontent.com/assets/8386553/23578664/494bfe02-0117-11e7-94e0-a43dccceb613.png>\n>\n> I can not figure out what the problem is...?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7817#issuecomment-284148196>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYoxVCRAVhRooixrFahBsW1ABrHjmQ40ks5riVgYgaJpZM4MJ7ZR>\n> .\n>\n", "@yaroslavvb \r\nMy implementation rely on 3rd library tensorlayer and it takes me sometime to rewrite it using tensorflow only.\r\nYou can check and download the example .py file with [this](https://www.dropbox.com/sh/ijc7lccquf1g4l7/AABX9pf5LIqkvPxplbhFRVafa?dl=0) Dropbox link.\r\nPlease check different performance of *dequeue* operation between `sess.run(train_op)` and `sess.run(light_op)`.", "@akaitsuki-ii so I tried making a timeline for your stuff (https://github.com/yaroslavvb/stuff/commit/5d2feffd62fa50d71dd2dfd314ee405c21d58a25), and I couldn't because your execution graph is too large. There are 942,316 ops being executed in the first session.run call. The dump of stepstats is over a GB and timeline generation chokes. So I think that's the real problem, and the \"enqueue\" op being slow is a false positive since tensorboard couldn't parse all the ops", "@yaroslavvb\r\nErrr...In fact I did try to make timeline before and fail because of the same reason.\r\nThere may be a mistake that do you really mean that *dequeue* op being slow is false positive? As *enqueue* op is not contained in `sess.run` call.\r\nBut if *dequeue* op is not really the problem, why I get worse performance changing to queue-based implementation than feeding with feed_dict at each `sess.run` call ?\r\nIs it just better using feed_dict instead of queue-based method in this situation ?", "There are about 1 million ops being executed (`grep node_stats stepstats-1.json | wc`), and it takes 4.72 second on my 2014 MacBook, which seems fairly fast for 1 million ops. Such large computation graphs are outside of typical range of cases TensorFlow is tuned for, so there's significant overhead involved.\r\n\r\nAs to why I said it is a false positive, if you open `stepstats-1.json` generated by the commit I referenced, you will see the following\r\n\r\n```\r\n    node_stats {\r\n      node_name: \"random_shuffle_queue_DequeueMany/n\"\r\n      all_start_micros: 1489378683948609\r\n      op_start_rel_micros: 1\r\n      op_end_rel_micros: 1\r\n      all_end_rel_micros: 52\r\n      memory {\r\n        allocator_name: \"cpu\"\r\n```\r\n\r\nSo the Dequeue op takes 51 microseconds.\r\n\r\n\r\nAs to why `feed_dict` is faster, it's an interesting question, and would require some digging to getto. One theory is that `dequeue` op requires a kernel launch, so when you already have a million ops being launched, perhaps the thread scheduling system is overloaded,  any new kernel launches are slow. Meanwhile, `feed_dict` approach does not add an additional op for reading data from queue, so no kernel launch.", "I'm not sure I follow the explanation for feed_dict, @yaroslavvb , since feed_dict translates to adding a Recv op in the graph for each thing being fed, and those still have to run.\r\n\r\nDequeue on the other hand is always suspicious since it can be sometimes fast and sometimes slow (if the queue is often full but sometimes empty). It might make sense to aggregate the costs across many invocations before saying it's not the issue.\r\n\r\n", "@alextp Good point about feed_dict, I can [see](https://github.com/yaroslavvb/stuff/blob/master/akaitsuki-slow/feed_dict.pbtxt) an extra _Recv node in that graph.\r\n\r\nBTW that script removes that queue variability -- there's no queue runners, things are first loaded onto the queue, and then consumed from the queue in the same thread.", "Just want to add something here, I implemented a **multiprocess-based** data feeding pipeline for multi-task learning. avg. GPU utilization >90% and quad-core CPU utilization >95%. In case anyone interested: https://hanxiao.github.io/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing/", "I don't know if it is still relevant, but reading through the discussion I do not find the reason or the solution to the problem. I have a very simple network (no operations almost) and I use:\r\ntf.train.shuffle_batch(decoded, capacity=batch_size * 50,\r\n                                 batch_size=batch_size,\r\n                                 num_threads=6,\r\n                                 min_after_dequeue=batch_size * 10,\r\n                                 allow_smaller_final_batch=True)\r\n\r\nAs my performance drops as I increase the batch_size almost linearly, i.e., increasing batch_size 4 times, it takes 4 times as much. "]}, {"number": 7816, "title": "tensorflow debugger assertion fails", "body": "When using tensorflow debugger, the program crashes with the following message : \r\n\r\n```\r\nF tensorflow/core/framework/tensor_util.cc:37] Check failed: DT_STRING == other.dtype() (7 vs. 20)\r\nAborted (core dumped)\r\n\r\n```\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNothing\r\n\r\n### Environment info\r\nOperating System: \r\n\r\nLinux ip-172-31-46-19 4.4.0-57-generic #78-Ubuntu SMP Fri Dec 9 23:50:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nInstalled version of CUDA and cuDNN: 8.0 / 5.1\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\n\r\n-rw-r--r-- 1 root root   558720 sept. 14 23:02 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 sept. 14 23:05 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 sept. 14 23:05 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   415432 sept. 14 23:02 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 sept. 14 23:02 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 d\u00e9c.   7 11:05 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       17 d\u00e9c.   7 11:05 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxr-xr-x 1 root root 79337624 d\u00e9c.   7 11:05 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 d\u00e9c.   7 11:05 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. pip package you installed:\r\n\r\n```\r\n$  pip show tensorflow-gpu\r\nName: tensorflow-gpu\r\nVersion: 1.0.0\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages\r\nRequires: wheel, protobuf, numpy, six\r\n```\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n```\r\n\r\npython -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.0\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nCurrently cannot find what operation makes the debugger crash\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nNothing\r\n\r\n### Logs or other output that would be helpful\r\n```\r\n\r\n(gdb) bt\r\n#0  0x00007ffff6a1e428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n#1  0x00007ffff6a2002a in __GI_abort () at abort.c:89\r\n#2  0x00007fffdf6ce0a4 in tensorflow::internal::LogMessageFatal::~LogMessageFatal() () from /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#3  0x00007fffdf61747f in tensorflow::tensor::DeepCopy(tensorflow::Tensor const&) () from /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#4  0x00007fffdda07082 in tensorflow::CopyOp::Compute(tensorflow::OpKernelContext*) () from /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#5  0x00007fffdf362183 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()\r\n   from /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#6  0x00007fffdf351e20 in std::_Function_handler<void (), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> (tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_invoke(std::_Any_data const&) ()\r\n   from /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#7  0x00007fffdf6a6960 in std::_Function_handler<void (), Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, tensorflow::thread::EigenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#8  0x00007fffdf6a5c10 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /home/ubuntu/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#9  0x00007fffdc573c80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#10 0x00007ffff76d16ba in start_thread (arg=0x7fff83fff700) at pthread_create.c:333\r\n#11 0x00007ffff6aef82d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\n```", "comments": ["This is a known issue due to TF Debugger (tfdbg)'s incompatibility with DT_RESOURCE, which is used in, among other things, queue ops. This bug has been fixed in master/HEAD. So please either try our nightly builds: https://github.com/tensorflow/tensorflow/#installation or wait for the next release. Sorry for the inconvenience and thanks for reporting the issue.", "The  TensorFlow version I use is TF1.1, when I'm trying to use tfdbg in the code of  https://github.com/david-gpu/srez   .\r\nI met the aborted problem ,and the error is : tensor_util.cc:37] Check failed: DT_STRING == other.dtype() (7 vs. 20)", "@chencjiajy thanks for reporting this issue.\r\n\r\nBut the failure looks weird to me. The fix (https://github.com/tensorflow/tensorflow/commit/bb0e05772fd839e211267f216bd60ae2df9167b4), should be in 1.1. If possible, can you upgrade to TF1.2 and see if the same problem persists?", "@caisq  sorry to reply until today because I didn't notice the message timely. \r\n The problem disappeared when I upgrade to TF1.2.\r\n  ", "@chencjiajy OK. Thanks for letting us know!"]}, {"number": 7815, "title": "tf r1.0 strided_slice documentation", "body": "In the [docs](https://www.tensorflow.org/api_docs/python/tf/strided_slice), the following sentences seem incomplete/incoherent:\r\n\r\n- If the ith bit of ellipsis_mask `[is-set-to-something]`, as many unspecified dimensions as needed will be inserted between other dimensions. Only one non-zero bit is allowed in ellipsis_mask.\r\n\r\n- If the ith bit of new_axis_mask is one, then `a?` begin, end, and stride are ignored and a new length 1 dimension is added at this point in the output tensor", "comments": ["Great, could you make a PR with your change?", "I did, and it's already been [merged](https://github.com/tensorflow/tensorflow/pull/7831)\r\nSo I guess this can be closed", "sounds good to me!  thanks!"]}, {"number": 7814, "title": "tensorflow 1.0 error: ValueError: Only call `sigmoid_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)", "body": "### Environment info\r\nOperating System:\r\nUbuntu 14.0.4\r\ntensorflow 1.0\r\ncuda 8.0 + cudnn 5.1\r\n\r\n### Project\r\nDCGAN: the code is downloaded here\r\nhttps://github.com/carpedm20/DCGAN-tensorflow\r\n\r\n### Logs or other output that would be helpful\r\n![image](https://cloud.githubusercontent.com/assets/25976861/23252160/bd6c2570-f9ea-11e6-92fd-523a2acc8461.png)\r\n### Codes that may have problems\r\n![image](https://cloud.githubusercontent.com/assets/25976861/23252201/e972a39c-f9ea-11e6-8938-a0d4dcc29c9f.png)\r\n\r\nI`m new in tf and I don`t know how to solve the problem.\r\nI`ll appreciate any suggestions. Thank you!\r\n", "comments": ["What happens if you replace this expression:\r\n\r\n```python\r\ntf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_))\r\n```\r\n\r\n...with the following:\r\n\r\n```python\r\ntf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_, labels=tf.ones_like(self.D_))\r\n```", "Thanks a lot! It perfectly solved my problem! @mrry "]}]