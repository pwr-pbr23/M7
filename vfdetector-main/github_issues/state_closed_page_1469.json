[{"number": 8875, "title": "Update version.", "body": "Updating the r1.1.0-rc0 references to r1.1.0-rc1.", "comments": ["Jenkins, test this please.", "This will keep failing until #8881 is merged.\r\nPlease hold on until that PR goes through."]}, {"number": 8874, "title": "[CMake] Experimental Jemalloc support.", "body": "My benchmarks of TF with Jemalloc on Windows had mixed results so far. However, some specific scenarios may benefit from Jemalloc support, so adding it to CMake (OFF by default).", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 8873, "title": "Arbitrary dimension support for tf.tile and binary operators", "body": "`tf.transpose` uses template specialization for dimensions <= 5, then falls back to a slightly slower generic implementation which works for any dimension:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/504b91de8fa9a9cc4a4e17e59ed753ab677a1410/tensorflow/core/kernels/transpose_functor_cpu.cc#L23\r\n\r\nIt would be nice to use the same mechanism for `tf.tile` and the various binary operators.", "comments": ["If anyone wants to take this on, I'd recommend trying `tf.tile` first: it's closer to `tf.transpose`.  Happy to answer questions.", "Hi @girving ,\r\nI'd like to work on this issue. I've written sketch code for it:\r\nhttps://github.com/yanchen036/tensorflow/blob/arbitrary_dim_for_tile/tensorflow/core/kernels/tile_ops_impl.h#L55\r\nhttps://github.com/yanchen036/tensorflow/blob/arbitrary_dim_for_tile/tensorflow/core/kernels/tile_ops.cc#L187\r\n\r\nAre these the same as what you think? If I'm wrong pls correct me!", "@yanchen036 Those links don't make it easy to see diffs.  Am I right that you just added a stub for TileSimple but didn't write it?  If so, the existence of TileSimple is on the right path.  I'm not sure if you made other changes.", "@girving I did 2 things. \r\nFirst, I wrote a TileSimple stub(now I know that's not you need) and a TileUsingEigen function\r\n```c++\r\nvoid TileUsingEigen(const Device& d, const Tensor& in, Tensor* out,\r\n                    const gtl::ArraySlice<int32> broadcast_array)\r\n```\r\nwhich implements the old Tile::operator(). See https://github.com/yanchen036/tensorflow/blob/arbitrary_dim_for_tile/tensorflow/core/kernels/tile_ops_impl.h#L32\r\n\r\nSecond, I changed the Tile::operator() function from\r\n```c++\r\ntemplate <typename Device, typename T, int NDIM>\r\nvoid operator()(const Device& d, typename TTypes<T, NDIM>::Tensor out,\r\n                typename TTypes<T, NDIM>::ConstTensor in,\r\n                const Eigen::array<int32, NDIM>& broadcast_array) const\r\n```\r\nTo\r\n```c++\r\ntemplate <typename Device, typename T>\r\nvoid operator()(const Device& d, const Tensor& in, Tensor* out,\r\n                const gtl::ArraySlice<int32> broadcast_array) const {\r\n    switch (in.dims()) {\r\n        case 0:\r\n            internal::TileUsingEigen<Device, T, 0>(d, in, out, broadcast_array);\r\n            break;\r\n        case 1:\r\n            internal::TileUsingEigen<Device, T, 1>(d, in, out, broadcast_array);\r\n            break;\r\n...\r\n    }\r\n}\r\n```\r\nThis uses template specialization as you mentioned.", "The `TileSimple` stub *is* the right idea; didn't mean to imply that wasn't.  So that looks good, except that I assume you don't mean to templatize your new `operator()` over `NDIM`?  I'm happy to comment further, but I'd much prefer to be looking at a pull request with diffs.\r\n\r\nI'm not sure how many fully specialized versions you should have, by the way.  I think there is a fairly common case which uses 8 dimensions.  It's worth profiling your new `TileSimple` once you have it to see what the slowdown is; it may or may not be significant.\r\n\r\nThank you for taking this on!  One more annoying blemish gone. :)", "Hi @girving I've finished the Tile Op. However, I'm confused with the TileGrad Op. What dose TileGrad do? I haven't found any docs about TileGrad Op in the website. ", "@yanchen036 No need to touch `TileGrad`.  It's a legacy op that someone wrote without noticing that it's just `tf.reduce_sum`.  Should be unused unless you have a `GraphDef` generated by a very old version of TensorFlow.", "@girving pls review this https://github.com/tensorflow/tensorflow/pull/10793", "Closing as it is resolved"]}, {"number": 8872, "title": "Tensorboard showing problems under keras callbacks", "body": "\r\nMy code runs well without any errors showing. However, when I use the command \r\ntensorboard --logdir = \"my path\", it shows nothing. I want someone to help me! Thanks.\r\n\r\ntensorflow 1.0.1 keras 2.0  and python 3.4\r\n\r\nAnd I found the debug shows:\r\n\r\n```\r\nINFO:tensorflow:TensorBoard is in debug mode.\r\nINFO:tensorflow:Starting TensorBoard in directory /home/lk/lk\r\nINFO:tensorflow:TensorBoard path_to_run is: {'/home/lk/lk/=': None}\r\nINFO:tensorflow:Event Multiplexer initializing.\r\nINFO:tensorflow:Event Multiplexer done initializing\r\nINFO:tensorflow:TensorBoard reload process beginning\r\nINFO:tensorflow:Starting AddRunsFromDirectory: /home/lk/lk/=\r\nINFO:tensorflow:Done with AddRunsFromDirectory: /home/lk/lk/=\r\nINFO:tensorflow:TensorBoard reload process: Reload the whole Multiplexer\r\n_INFO:tensorflow:Beginning EventMultiplexer.Reload()\r\nINFO:tensorflow:Finished with EventMultiplexer.Reload()_\r\nINFO:tensorflow:TensorBoard done reloading. Load took 0.002 secs\r\nINFO:tensorflow:TensorBoard is tag: b'41'\r\nStarting TensorBoard b'41' on port 6006\r\n(You can navigate to http://127.0.1.1:6006)\r\n\r\n```\r\nof which between INFO:tensorflow:Beginning EventMultiplexer.Reload() and INFO:tensorflow:Finished with EventMultiplexer.Reload(). There is no debuging information. ", "comments": ["Please read the tutorial https://www.tensorflow.org/get_started/summaries_and_tensorboard.\r\n This should answer your question. Otherwise, ask a more specific question on StackOverflow (including your code)\r\n"]}, {"number": 8871, "title": "text_classification.py doesn't works ", "body": " ....\r\n\r\n  File \"C:\\Users\\danie\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\ops\\sparse_feature_cross_op.py\", line 21, in <module>\r\n    from tensorflow.contrib.framework import deprecated_arg_values\r\n    ImportError: cannot import name 'deprecated_arg_values'\r\n\r\nI'm using Spider.\r\n\r\n\r\n\r\n  ", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 8870, "title": "Bugs when dequeue a 3-D input tensor from FIFOQueue", "body": "```python\r\nx = tf.random_normal([time_length, batch_size, feature_size], mean=0, stddev=1)\r\n## prepare data\r\nq = tf.FIFOQueue(capacity=4, dtypes=tf.float32) \r\nenqueue_op = q.enqueue(x)\r\nnum_threads = 1 \r\nqr = tf.train.QueueRunner(q, [enqueue_op] * num_threads)\r\ntf.train.add_queue_runner(qr)\r\ninputs = q.dequeue() \r\n```\r\nAs above code shows, when I use FIFOQueue and Cordinator to do input pipeline, the input is a 3-D tensor variable, but when I dequeue it from the FIFOQueue, errors comes:\r\n```python\r\nValueError: as_list() is not defined on an unknown TensorShape.\r\n```\r\n\r\nThus, I go to the source code of FIFOQueue, I found that FIFOQueue.enqueue method doesn't use a shape function. So, I add a line of code as following, it works well.\r\n```python\r\nx = tf.random_normal([time_length, batch_size, feature_size], mean=0, stddev=1)\r\n\r\n## prepare data\r\nq = tf.FIFOQueue(capacity=4, dtypes=tf.float32) \r\nenqueue_op = q.enqueue(x)\r\nnum_threads = 1 \r\nqr = tf.train.QueueRunner(q, [enqueue_op] * num_threads)\r\ntf.train.add_queue_runner(qr)\r\ninputs = q.dequeue() \r\ninputs.set_shape(x.get_shape())\r\n```\r\n\r\nBut if I change my input from 3-D tensor into 2-D tensor, the code runs very well without set_shape function manually. So I want to know why my 3-D input tensor can't work well but 2-D tensor input work well? Can the source code of FIFOQueue or QueueBase support shape function? ", "comments": ["@mrry, this seems like it could need fixing. Please take a look. Thanks!", "Can you please share the entire stack trace for that `ValueError`? I can't see any calls to `TensorShape.as_list()` in the queue implementation.", "@mrry Ok, my code is as follows:\r\n```python\r\nq = tf.FIFOQueue(capacity=4, dtypes=tf.float32) \r\nenqueue_op = q.enqueue(x)\r\nnum_threads = 1 \r\nqr = tf.train.QueueRunner(q, [enqueue_op] * num_threads)\r\ntf.train.add_queue_runner(qr)\r\ninputs = q.dequeue() \r\n#inputs.set_shape(x.get_shape())\r\ny = tf.reduce_mean(tf.reduce_sum(inputs, axis=0), axis=1, keep_dims=True)\r\nlabels = tf.cast(tf.greater(y, 0), tf.int32)\r\n\r\n## build model\r\nsequence_length = tf.Variable([time_length]*batch_size, dtype=tf.int32)\r\ncell_fw = LSTMCell(num_units=hidden_size)\r\ncell_bw = LSTMCell(num_units=hidden_size)\r\noutputs, state = tf.nn.bidirectional_dynamic_rnn(\r\n      cell_fw=cell_fw,\r\n      cell_bw=cell_bw,\r\n      inputs=inputs, \r\n      sequence_length=sequence_length,\r\n      dtype=tf.float32,\r\n      time_major=True)\r\n```\r\n\r\nerror is\r\n```python\r\nTraceback (most recent call last):\r\n  File \"test1.py\", line 32, in <module>\r\n    time_major=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 350, in bidirectional_dynamic_rnn\r\n    time_major=time_major, scope=fw_scope)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 546, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 615, in _dynamic_rnn_loop\r\n    const_time_steps, const_batch_size = inputs_got_shape[0].as_list()[:2]\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 761, in as_list\r\n    raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\nValueError: as_list() is not defined on an unknown TensorShape.\r\n```\r\n and I think the error is caused by BLSTM, maybe the set_shape bug should be fixed.", "This doesn't sound like a bug (in the queues, at least). The actual error is in the `tf.nn.bidirectional_dynamic_rnn()` code, which does not seem to support inputs with unknown shape; the solution is to provide more static shape information.\r\n\r\nWhen you create a `tf.FIFOQueue` with no `shapes` argument, you are allowing the queue to store tensors of *any* shape. Therefore, since when you create the `q.dequeue()` op we don't know exactly which tensors will be dequeued, we have to give it an unknown shape.\r\n\r\nBy calling `inputs.set_shape(x.get_shape())` you are asserting that all elements have a particular shape. You could have done the same thing by creating the queue as:\r\n\r\n```python\r\nq = tf.FIFOQueue(capacity=4, dtypes=tf.float32, shapes=x.get_shape())\r\n```", "@mrry Thank you a lot, I understand now.", "Glad to help clear things up!"]}, {"number": 8869, "title": "[cherry-pick] tfdbg: fix a bug in graph validation related to tf.while_loops", "body": "CL/147488620 fixed a bug where the debugger would hang at Enter and NextIteration nodes under certain conditions. But it introduced another bug where the debug dumps from Enter and NextIteration may get generated later than downstream nodes in the tf.while_loop body, causing \"causility violation\" during debug_data.DebugDumpDir's validation process under certain conditions (e.g., backpropagation on a dynamic_rnn). This CL fixes that by excluding Enter and NextIteration nodes from the validation process.\r\n\r\nFixes: #8337\r\nChange: 151787432", "comments": ["@tensorflow-jenkins test this please", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "Jenkins, test this please."]}, {"number": 8868, "title": "Branch 151810398", "body": "", "comments": []}, {"number": 8867, "title": "Introduce a new tool/function to visualize a pb model as requested by #8854", "body": "Issue #8854 asked for a tool to easily visualize .pb models as graphs. I pointed out it was possible to do with Tensorboard so i've created a function to allow someone to pass a .pb model and a tensorboard log directory to then view the imported .pb graph.\r\nI guess this is a temporary measure until a decision is made on creating something a bit more friendly.\r\n\r\nAlso, For some reason I think that this PR is including some previously merged commits and I don't know how to get rid of them on github! I thought i'd sync'd everything but i'm a bit rubbish at this. I guess those can be safely ignored. Thanks", "comments": ["Can one of the admins verify this patch?", "@drpngx changes have been made. I'm unsure as to why my previous commits from a separate PR have ended up in here. Sorry for the delay in getting this out.", "The change looks good to me. And, please make the comment into a docstring per @drpngx's request.", "Sorry what so you mean by docstring? Can you link an example?", "@dandelionmane @drpngx I made the changes for the user, PTAL", "Jenkins, test this please."]}, {"number": 8866, "title": "How to print and write \"Predictions\" into a file?", "body": "```python\r\n    predictions = tf.argmax(logits, 1) ######this predictions!!!!!!!!!!!!!!\r\n    labels = tf.squeeze(labels)\r\n\r\n    # Define the metrics:\r\n    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\r\n        'Accuracy': slim.metrics.streaming_accuracy(predictions, labels),\r\n        'Predictions': slim.metrics.streaming_precision(predictions, labels),\r\n        'Recall@5': slim.metrics.streaming_recall_at_k(\r\n            logits, labels, 5)\r\n    })\r\n\r\n    # Print the summaries to screen.\r\n    for name, value in names_to_values.iteritems():\r\n      summary_name = 'eval/%s' % name\r\n      op = tf.summary.scalar(summary_name, value, collections=[])\r\n      op = tf.Print(op, [value], summary_name)\r\n      tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\r\n```\r\nI try lots of ways, but I fail to print or write \"predictions\".\r\nI want to print  the test results(\"predictions\")  of a model and write it into aaa.txt, and how?\r\n\r\nTHANKS.", "comments": ["Please read the getting started guide, it explains the difference between graph nodes and evaluating graph nodes. If you still have issues, please write a stackoverflow question instead of an issue (this is not a bug)."]}, {"number": 8865, "title": "[FeatureRequest] Decode gzipped files in tf.FixedLengthRecordReader", "body": "My dataset is stored in binary format that is easily compressible (from 100 MB down to ~0.5 MB), but I currently cannot make use of this in TF, unless I write a TFRecords file, which is way more cumbersome and leads to a lot of duplication -- I need to work with those files in other programs as well, so the binary format is the easiest solution for me. It would be nice if  tf.FixedLengthRecordReader would support on-the-fly decompression.", "comments": ["We would happy to accept a batch that allows this behavior, but we are unlikely to work on it ourselves, since we either use TFRecords or other data formats internally. It shouldn't be super difficult to do yourself. @saxenasaurabh can probably give you some direction on how to use the existing gzip classes, if you'd be willing to help out.", "This shouldn't be too hard imho. You should be able to swap out the `input_buffer_` in [FixedLengthRecordReader](https://github.com/tensorflow/tensorflow/blob/c664520a1a7642cd3c9368cd1bd036892d8966b2/tensorflow/core/kernels/fixed_length_record_reader_op.cc#L28) with the [ZlibInputStream](https://github.com/tensorflow/tensorflow/blob/bad7c50b9dc9789ad7dd0a62daca40b7269841ed/tensorflow/core/lib/io/zlib_inputstream.h#L38). You may need to subclass the [InputBuffer](https://github.com/tensorflow/tensorflow/blob/bad7c50b9dc9789ad7dd0a62daca40b7269841ed/tensorflow/core/lib/io/inputbuffer.h#L32) from [InputStreamInterface](https://github.com/tensorflow/tensorflow/blob/bad7c50b9dc9789ad7dd0a62daca40b7269841ed/tensorflow/core/lib/io/inputstream_interface.h#L27) though.\r\nYou can specify the `compression_type` in a manner similar to the [TFRecordReader](https://github.com/tensorflow/tensorflow/blob/b07791f6e9b306937eb58f7bb6c3300cd26583af/tensorflow/core/kernels/tf_record_reader_op.cc) and add it to the op registration similar to [here](https://github.com/tensorflow/tensorflow/blob/b07791f6e9b306937eb58f7bb6c3300cd26583af/tensorflow/core/ops/io_ops.cc#L481).", "@untom @aselle @saxenasaurabh I created a PR #8901 to add gzip and zlib compression support for FixedLengthRecordReader. Please take a look."]}, {"number": 8864, "title": "Branch 151792071", "body": "", "comments": []}, {"number": 8863, "title": " minor edit to sequence_loss doc. fix issue-8755", "body": "this PR fixes issue #8755 by\r\n- editting the doc of `tf.contrib.seq2seq.sequence_loss` to elaborate on the effect of arguments `average_across_timesteps` and `average_across_batch.` \r\n- a few minor stylistic changes to improve notational consistency.\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "Jenkins, test this please."]}, {"number": 8862, "title": "MSVC 2015 port.", "body": "My version of MSVC 2015 has some issues with some newer c++ constructs.\r\nThere are 3 type of fixes:\r\n1. Remove optional 'typename' specifications outside template definitions.  They are optional on newer compilers, but msvc does not thinks so.\r\n2. Remove brackets from initializer lists, `perm({0,1})` becomes `perm{0,1}`. I am not sure why this is different on msvc.\r\n3. Explicitly convert from `Eigen::TensorMap` to `gtl::ArraySlice` via .data() and .size().  Again, I'm not sure why this is required for msvc, but this solves the remaining compiler issues.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@hughsando TensorFlow currently builds with Visual Studio 2015 Update 3 using CMake... Are you using earlier version of Visual Studio 2015 to build?", "Yes, I was running Update 1, so that was the problem.\r\nI will close this one - I'm not sure what the best practice use of `typename` is, but there seems to be only a small number of optional uses, and can you can always compiler with Update1 of you want to find them."]}, {"number": 8861, "title": "How can i export model into HDFS", "body": "The issue have beed submitted in [#381](https://github.com/tensorflow/serving/issues/381), thanks all kind of you", "comments": ["Closing duplicate.", "@aselle i think tensorflow community is more active than tensorflow serving. can I reopen this issue to let this issue into more and more people\u2018s Vision? thanks a lot"]}, {"number": 8860, "title": "Server terminated abruptly when build from source", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nInitially I encountered the problem described as in https://github.com/tensorflow/tensorflow/issues/8731\r\n\r\nAfter modified configure file as in https://github.com/tensorflow/tensorflow/commit/fba05c300bf6840e76787680ed7fd1239cdb9ad0. This error happened\r\n\r\n### Environment info\r\nOperating System: CentOS Linux release 7.3.1611 (Core)\r\n\r\nInstalled version of CUDA and cuDNN: CUDA 7.5 and cuDNN 5.0\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): \r\n$ ls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root   322936 Dec  6 11:05 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Dec  6 11:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root       19 Dec  6 11:05 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root   383336 Dec  6 11:05 /usr/local/cuda/lib64/libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root   720192 Dec  6 11:05 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 asb  ttic       13 Apr 22  2016 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 asb  ttic       17 Apr 22  2016 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.0.5\r\n-rwxr-xr-x 1 asb  ttic 59909104 Apr 22  2016 /usr/local/cuda/lib64/libcudnn.so.5.0.5\r\n-rw-r--r-- 1 asb  ttic 58775484 Apr 22  2016 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n$ git rev-parse HEAD\r\n83be34c47c7ed81c62bd61a908fde017e301b578\r\n\r\n2. The output of `bazel version`\r\n$ bazel version\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n./configure\r\nPlease specify the location of python. [Default is /home-nfs/mingdachen/anaconda2/envs/py35gpu/bin/python]:\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to use jemalloc as the malloc implementation? [Y/n]\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]\r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /home-nfs/mingdachen/anaconda2/envs/py35gpu/lib/python3.5/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home-nfs/mingdachen/anaconda2/envs/py35gpu/lib/python3.5/site-packages]\r\n\r\nUsing python library path: /home-nfs/mingdachen/anaconda2/envs/py35gpu/lib/python3.5/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\r\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]:\r\nINFO: $TEST_TMPDIR defined: output root default is '/scratch/mingda/'.\r\nExtracting Bazel installation...\r\n.........\r\nINFO: All external dependencies fetched successfully.\r\nConfiguration finished\r\n\r\n$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package &> ../log\r\n\r\n### What other attempted solutions have you tried?\r\nI have tried different branches, including r1.0, r1.1 and master\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/884405/log.txt)\r\n\r\nThe log file of /scratch/mingda/_bazel_mingdachen/b48312abf86569e91b44fe1197c1c461/server/jvm.out is empty", "comments": ["If you rerun the bazel command, will it continue on some more and eventually finish? Maybe @jart  or @gunan  have some insight?\r\n", "Looks like bazel crashed?\r\n@damienmg any ideas?\r\n", "Definitely a Bazel issue, no idea on why", "@aselle Thanks for your reply!\r\nI didn't rerun from the bazel command, but I reran from the ./configure, and in each time, it would stop at different places. Hope this information would be helpful.\r\nAs a workarond, I just installed the CUDA 8.0, and used the pip to install the tensorflow with gpu support.", "is your root disk maybe nearly full? Otherwise, it's pretty difficult to say. Maybe a bazel bug? ", "@mingdachen after failures, please only rerun the bazel command.\r\nYou only need to run configure once.\r\nThen we can see if bazel can incrementally complete the build on the 2nd run", "This was happening to me too on the v1.0.1 tag with bazel 0.4.5. I've put the bazel cache on a separate partition so it has sufficient space.\r\n\r\nRerunning the bazel command will quickly fail again, repeatedly, and also ask you to rerun ./configure even though I specified GPU support the first time:\r\n`ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\r\n`\r\nRerunning ./configure and repeating everything doesn't help. Based on kernel logs, it doesn't seem like we hit an out-of-memory error.\r\n\r\nHowever, I was able to get it to build by adding `--local_resources 2048,.5,1.0` to the build command anyway, as suggested by the tip in https://www.tensorflow.org/install/install_sources. It takes a lot longer, but doesn't max out the CPU and eventually completes without issues.\r\n", "@kahkeng: I believe you should look at the syslog to see if a OOM killer is in action, which would explain.\r\n\r\nBazel tries to use all the available memory and CPU to schedule as many action as possible but the resource estimation is currently broken which might explain why --local_resources works.", "Got the same error but it worked fine when I just reran it."]}, {"number": 8859, "title": "code auto completion error", "body": "i'm using pycharm. And can't get code completion when using tf.contrib.\r\nbut when I write tensorflow.contrib import learn. it has a right behavior.\r\n\r\nI guess it happened because of the lasy loading of tf.contrib.\r\n\r\ncan anyone help me?", "comments": ["Can you please post your import code. This doesn't sound like a Tensorflow bug.\r\nAFAIK, you can't create an import statement using an _as_ alias such as tf. You have to do use the full module name because python is looking for the tensorflow module to import from. tf is just a shorthand that you've defined. ", "@jubjamie I'm sure i write the right code and import tensorflow not tf. \r\nIt has a right result when runing. but can'g get auto code completion when coding", "Ok well this doesn't sound like a tensorflow bug. May I suggest that you try posting on stack overflow including the pycharm tag?", "Code completion is a feature offered by your IDE.\r\nPlease either browse the forums of your IDE, or use stackooverflow as recommended by @jubjamie ", "@jubjamie thanks for your patient. I will notify you when get the reason ", "@sunnysuhappy i think you were previously trying `tf.contrib.layers` alike. If that is the case, then you can add a statement `import tensorflow.contrib as contrib`, after that, the auto completion of PyCharm can work, at least this works for me, you can have a try! ^_^\r\n\r\n@jubjamie But i notice that contrib is only an attibute of tf while others are not (take nn for example, nn tf.nn is a module). So, can you make contrib act like nn ?", "@youkaichao thanks for your answer after such a long duration", "@sunnysuhappy The latest pycharm 2018 still have this questions . Have you solved these questions(pycharm cannot auto complete tf.contrib )   why does that happen ? ", "push"]}, {"number": 8858, "title": "Machine restarts when running TensorFlow with GPU", "body": "A simple Python program which runs a few TensorFlow computations consequently crashes when running on GPU.\r\n\r\nCode:\r\n\r\n```\r\nfrom __future__ import print_function\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import timeline\r\n\r\ndef train_model(run_number):\r\n    image_size = 28\r\n    num_labels = 10\r\n    batch_size = 16\r\n    layer1_neuron_count = 16384\r\n\r\n    graph = tf.Graph()\r\n    \r\n    with graph.as_default():\r\n        tf_valid_dataset = tf.constant(valid_dataset)\r\n\r\n        # Variables.\r\n        weights0 = tf.Variable(\r\n            tf.truncated_normal([image_size * image_size, layer1_neuron_count]))\r\n        biases0 = tf.Variable(tf.zeros([layer1_neuron_count]))\r\n\r\n        weights1 = tf.Variable(\r\n            tf.truncated_normal([layer1_neuron_count, num_labels]))\r\n        biases1 = tf.Variable(tf.zeros([num_labels]))\r\n\r\n        valid_layer0 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)\r\n        valid_prediction = tf.matmul(valid_layer0, weights1) + biases1\r\n    \r\n    with tf.Session(graph=graph) as session:\r\n        tf.global_variables_initializer().run()\r\n\r\n        print('Validation')\r\n        \r\n        session.run(valid_prediction)\r\n            \r\n        print('Validation done')\r\n\r\nvalid_dataset = np.random.uniform(-1, 1, (10000, 784)).astype(dtype=np.float32)\r\nvalid_labels = np.random.uniform(0, 1, (10000, 10)).astype(dtype=np.float32)\r\n\r\nfor i in range(10):\r\n    print(\"Run #{}\".format(i))\r\n    train_model(i)\r\n```\r\n\r\nIt should run the same computation 10 times, recreating a graph and a session every time. \r\nWorks fine when I run it on CPU. When running on GPU, it fails on running computation for 2nd, 3rd or 4th session.\r\n\r\nConsole output:\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nRun #0\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 980 Ti\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:01:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 5.83GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\r\nValidation\r\nValidation done\r\nRun #1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\r\nValidation\r\nValidation done\r\nRun #2\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\r\nValidation\r\nValidation done\r\nRun #3\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\r\nValidation\r\n```\r\n\r\nThen the machine just restarts.\r\nThere are no relevant messages in syslog before the restart.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttp://stackoverflow.com/questions/39122984/system-auto-reboot-when-tensorflow-model-is-too-large\r\nhttp://stackoverflow.com/questions/41237115/computer-restarts-with-large-mini-batches-in-tensorflow\r\n\r\nWhen running other TensorFlow programs, I noticed that sometimes such crashes happen when I use large tensors. Issues above seem to be related, at least symptoms are similar.\r\n\r\n\r\n### Environment info\r\nGPU: GeForce GTX 980 Ti\r\nOperating System: Ubuntu 16.04.2 LTS\r\n\r\nInstalled version of CUDA and cuDNN: CUDA 8.0.61, cuDNN 7.5\r\nOutput of `ls -l /usr/local/cuda/lib64/libcud*`:\r\n\r\n`-rw-r--r-- 1 root root   556000 Mar 30 18:05 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Mar 30 18:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Mar 30 18:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rwxr-xr-x 1 root root   415432 Mar 30 18:05 /usr/local/cuda/lib64/libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root   775162 Mar 30 18:05 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 Mar 30 19:42 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       18 Mar 30 19:42 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10\r\n-rwxr-xr-x 1 root root 84163560 Mar 30 19:42 /usr/local/cuda/lib64/libcudnn.so.5.1.10\r\n-rw-r--r-- 1 root root 70364814 Mar 30 19:42 /usr/local/cuda/lib64/libcudnn_static.a`\r\n\r\nTensorFlow:\r\n1. \"pip install tensorflow-gpu\". Version 1.0.1\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`:\r\n\r\n`I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.1`\r\n\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nTried to reinstall Ubuntu/CUDA/cuDNN/TensorFlow, didn't help.\r\n\r\n", "comments": ["@surmenok Are you able to monitor nvidia-smi as you run the code. I think you can run `nvidia-smi -l 1` and it should auto-update every second.\r\nI'm struggling to understand your code but if you are running your code example above 10 times (or similar) perhaps you are running out of memory? Now, perhaps this is a Tensorflow memory bug or maybe it's poor memory coding on your part. I'm not too well versed in that area!\r\n\r\nIf nvidia-smi is showing your graphics card powering through the memory or over-heating or similar then we can go from there. Can I ask what GPU options you have set up for your session calls please?", "Formatting of my code in the original post was not good. I updated it, hopefully, it is more clear now. \r\nBasically, the function train_model creates a tf.Graph and a session, runs a simple computation: get a variable, multiply a constant tf_valid_dataset with a variable weights0, then add biases0 variable, then run tf.nn.relu , then multiply with weights1 and add biases1. \r\nConstant tf_valid_dataset is initialized randomly.\r\nThen this train_model function is executed sequentially 10 times. It runs well first 2-3 times and then the machine crashes.\r\nI doubt that there are any memory related issues in this code. I think TensorFlow is designed to throw nice OOM exceptions if there is not enough memory.\r\n\r\nI tried to add a 3 second delay between function executions to be able to monitor nvidia-smi output better. \r\nOutput of nvidia-smi when program just started:\r\n\r\n```\r\nFri Mar 31 13:03:17 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 980 Ti  Off  | 0000:01:00.0     Off |                  N/A |\r\n| 26%   37C    P2    65W / 250W |   5827MiB /  6076MiB |      8%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1790    C   python                                        5825MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n\r\nOutput of nvidia-smi in the middle (when running the function 2nd or 3rd time):\r\n\r\n```\r\nFri Mar 31 13:03:21 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 980 Ti  Off  | 0000:01:00.0     Off |                  N/A |\r\n| 26%   39C    P2    77W / 250W |   5838MiB /  6076MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1790    C   python                                        5836MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n\r\nThe last output of nvidia-smi, one second or less before machine restarts:\r\n\r\n```\r\nFri Mar 31 13:03:29 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 980 Ti  Off  | 0000:01:00.0     Off |                  N/A |\r\n| 26%   40C    P2    77W / 250W |   5838MiB /  6076MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1790    C   python                                        5836MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nDoesn't seem to overheat. Memory consumtpion is tricky to measure. I think TensorFlow grabs almost all GPU memory from the beginning and manages memory itself, so for nvidia-smi it always looks like memory consumption is near maximum.\r\n\r\n> Can I ask what GPU options you have set up for your session calls please?\r\n\r\nWhere can I see these options?\r\nI don't pass any special options to the Session constructor in Python code. This is the code for session initialization:\r\n\r\n`with tf.Session(graph=graph) as session:`\r\n\r\nAre there any other options I should look at?", "I simplified the code a bit more. This is the minimal failing code I could get:\r\n\r\n```\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\n\r\ndef train_model(run_number):\r\n    graph = tf.Graph()\r\n    \r\n    with graph.as_default():\r\n        tf_dataset = tf.Variable(tf.truncated_normal([input_dimension, 1000]))\r\n        weights0 = tf.Variable(tf.truncated_normal([1000, 10000]))\r\n        result = tf.matmul(tf_dataset, weights0)\r\n    \r\n    with tf.Session(graph=graph) as session:\r\n        tf.global_variables_initializer().run()\r\n\r\n        print('Before computation')\r\n        session.run(result)\r\n\r\ninput_dimension = 10000\r\n\r\nfor i in range(100):\r\n    print(\"Run #{}\".format(i))\r\n    train_model(i)\r\n```\r\n\r\nThe function creates 2 variables using tf.truncated_normal for initialization, then multiplies these variables. The function is called 100 times sequentially.\r\nSeems like the bug is related to memory, because size of TensorFlow variables impacts behavior of the program. Behavior with different values of input_dimension variable (it changes size of one of TensorFlow variables used in computation):\r\n\r\n```\r\n1000 - works fine\r\n2000 - reboots the machine on 61st execution\r\n3000 - reboots the machine on 12th execution\r\n4000 - reboots the machine on 21st execution\r\n5000 - reboots the machine on 4th execution\r\n10000 - reboots the machine on 3rd execution\r\n25000 - reboots the machine on 2nd execution\r\n29000 - reboots the machine on 9th execution\r\n29500 - reboots the machine on 19th execution\r\n29900 - reboots the machine on 10th execution\r\n30000 - reboots the machine on 30th execution\r\n40000 - works fine\r\n50000 - works fine\r\n100000 - works fine\r\n500000 - OOM on 1st execution:\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 18.63GiB.  See logs for memory state.\r\nW tensorflow/core/framework/op_kernel.cc:993] Resource exhausted: OOM when allocating tensor with shape[500000,10000]\r\n```\r\n\r\nIt's not deterministic. For some time it was working fine with values up to 4260, but then it started failing for values as low as 2000.\r\nInteresting that it works well with higher values like 40000.\r\n\r\nIf I change the program to run the train_model function only once and run the program (executing 'python test.py' in a terminal window) multiple times, it also crashes the machine after a few runs.\r\n\r\n\r\n", "This looks like it is an issue in your workstation.\r\nYou could check dmesg and syslog to see if the OS logged anything informative right before crashing.\r\n", "There are no messages in dmesg or syslog logged right before the system crashes.", "I can't reproduce this example. I'd try upgrading or downgrading your nvidia kernel driver.", "Also looking at the stackoverflow responses, only possibility I can see here is a power supply issue on your machine.\r\nWhen running TF, you will run your GPU and your CPU to the max, so they will draw as much power as they can get. With smaller power supplies, we also observed machines freezing, locking up. Other than that, I have no idea what can be causing this.\r\n\r\n", "I don't think that it is a power supply issue. nvidia-smi shows that power consumption is very low. At the same time I can run training of large neural networks which bring GPU utilization to over 90% and it is running for many days without issues. Based on how differently TensorFlow programs on my machine react to tensors of different sizes, I think the problem is related to memory.", "I tried a suggestion from StackOverflow to limit GPU power using \"sudo nvidia-smi -pl 150\", it didn't help.", "When you have strange hardware errors like this, it's best to swap in a new identical GPU or use your existing GPU in a different PC.", "Thanks, Andrew. Currently, I don't have another GPU or PC to try. Will do when I have a chance.", "Closing due to inactivity.\r\nPlease reopen when you have more information.", "A system with two P6000 24GB GPU's just hangs soon after some Tensorflow calculations. Ubuntu 16.04.2 LTS. No messages in dmesg and syslog. The workstation has >1000 W PSU. nVidia drivers and CUDA toolkit are the latest as-of June 2nd, 2017. I will have to check the versions later.", "Do you have any example code? I dont mind trying to run it (although I only have one GPU). If this is happening with all Tensorflow code then it would seem to be your workstation. Odd that nothing shows up in dmesg and syslog.", "I have further tested the system. The power draw doesn't seem to be too large as reported by nvidia-smi tool, but the temperatures stabilized at 80 degrees Celcius while the fans remained at 30-40% at most.\r\n\r\nI was also able to reproduce the freeze issue with Matlab code that ran large matrix calculations on both GPU's. It could be that nVidia's linux drivers might have a problem.", "_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Ovefrlow._\r\n\r\nYes this does sound like you don't have a Tensorflow issue but instead have problems with your drivers perhaps. May i suggest re-installing your drivers/updating them. Also, have a search online for the exact driver you are using as sometimes nVidia drivers have issues that the community tend to report. They might suggest you have a bad driver a give you a version to roll back to.\r\n\r\nNonetheless, it seems like this isn't a Tensorflow problem so this will probably get closed. If you ask on Stack Overflow you'll probably have more luck there! Good luck getting it fixed!", "Now I feel confident that the issue is related to power supply.\r\nI changed the power supply from \"Corsair CX750 Builder Series ATX 80 PLUS\" to \"Cooler Master V1000\" and don't get the system crash anymore. \r\nI think the problem is not in max output of the power supply (750W should be more than enough for one GPU), but perhaps with other qualities of power supply.", "Power supplies are fiddly things indeed. If you think your power supply is bust I would speak to Corsair. They're usually pretty good with sorting it out if it's in warranty. Glad this is solved. Thanks", "Our issue turned out to be an issue with power supply as well. Sadly, the workstations are provided by Lenovo (P910), so there are no other compatible PSUs on the market. Our GPUs were Quardo P6000. We are now trying to solve the issue by experimenting with limiting the GPU power consumption in Linux by command: nvidia-smi --power-limit=150", "this is a power supply issue, I've been experiencing these problems with my workstation (which had 650W 80+) with i7 CPU & Titan X Pascal GPU (1 ssd, 16gig ram, no hdd, no extra hardware), but I had to connect another PSU to the case and feed 8 Pin power input from 2nd supply. no problems so far, but voltage fluctuations still continuing. Its better to run these machines with at least 800W+ PSU to be safe. 12V rail dancing between 11.1V (which is highly dangerous) to 12.1V.", "I must say that Lenovo's most powerful 1200 W PSU for P910 is not enough for two P6000's, as the PSU +12V rails are not powerful enough. \r\n\r\nJust wanted to write this here in case anyone else comes up with this issue.", "Fortron FSP650-80EGN (650 W) with GTX 980 Ti also reboots in my case. Even limiting power usage to 150 W doesn't help. A strange thing is that one model fails at around 90 W with limit to 150 W, whereas different model runs fine at 219 W.\r\n\r\n````\r\n# OK:\r\n# https://github.com/rossumai/keras-multi-gpu/blob/6ee0efe5b2e5d7ac8deec1af86ce03f27418780c/keras_tf_multigpu/examples/benchmark_inception3_resnet50.py\r\n$ python keras_tf_multigpu/examples/benchmark_inception3_resnet50.py --b 32 -a inception3\r\n# higher batch results in OOM, not reboot\r\n```\r\n\r\n```\r\n# reboot:\r\n# https://github.com/bzamecnik/ml/blob/42bd25ea4c4b34012f375ea03bbaa402ee064c46/chord-recognition/lstm_chord_classification_training.py\r\n# with batch_size=64\r\n$ python lstm_chord_classification_training.py\r\n```\r\n\r\nWhen I tried to decrease sequence length of the LSTM from 100 to 10 and increase batch size from 32 to 512 the machine didn't restart and GPU drew 200 W.", "Hi I meet the same problem when I run vgg16 net on faster cnn. THe system just suddenly reboot when I run the code.\r\nI'm using TF1.4 cuda8.0 Geforce GTX TITAN X in ubuntu14.04.\r\nAny solutions\uff1fThanks a lot\uff01\uff01\uff01 @surmenok ", "My solution was to replace a power supply with a better one. My machine works well with \"Cooler Master V1000\".", "Just a report: my 980 Ti works fine on 100% load from mining (at 78 deg C\nwith water cooling). So I'm not sure how it can be affected by a power\nsource.\n\nDne 18. 12. 2017 2:24 napsal u\u017eivatel \"Pavel Surmenok\" <\nnotifications@github.com>:\n\nMy solution was to replace a power supply with a better one. My machine\nworks well with \"Cooler Master V1000\".\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/8858#issuecomment-352278676>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AAbOrOYG4LpG8YBWt8DZAkS_LOoBsR_Bks5tBWp-gaJpZM4MvKsu>\n.\n", "I'm not sure either. I tried to run gpu_burn to utilize GPU 100% and it worked fine. But TensorFlow was able to bring it down. The problem was solved by replacing the power supply.", "@surmenok \r\nI tired to run gpu_burn to utilize GPU 100% and it worked fine too. But when I run your code with `input_dimension=10000` or more will make my system reboot suddenly. I'm using 600W power supply. I may have to replace the power supply for a try.\r\n", "I fixed this problem temporarily by limiting the power available to my Titan X to 150W (instead of 250W):\r\n\r\n```\r\nsudo nvidia-smi --persistence-mode=1\r\nsudo nvidia-smi --power-limit=150\r\n```\r\n\r\nBefore, my Ubuntu 14.04 LTS machine would shutdown or reboot without any errors in the syslog when starting the training on a VGG16 + SSD network.", "I replaced the PSU to a more powerfull one to correct a similar problem.", "I've encountered the same reboot / hard-crashes on a single Nvidia GTX 1080ti with an Ubuntu 17.10 host anytime I run some examples of TF models (e.g. nearest neighbor, linear regression etc.), and I can't come to a conclusion as to why the crashes continue.\r\n\r\nI've tried various nvidia drivers, against different kernel versions, all with CUDA 9 + cuDNN 7 + TF 1.5.0:\r\n- Linux 4.14.x -> Nvidia 384.111, 387.34, 390.25, 390.12\r\n- Linux 4.13.x -> Nvidia 384.98\r\n\r\nI'm also using a 1200W, 80+ Platinum Certified PSU which is more than plenty, so I doubt its a PSU issue. I tried @chrschorn's [suggestion](https://github.com/tensorflow/tensorflow/issues/8858#issuecomment-352744466) to limit the power to 150W instead of the default 250W, but it still crashes with TF jobs.\r\n\r\nI don't currently have another machine or GPU to test against, so hoping anyone can provide some clarity or possible next steps. Thanks!", "Hello everyone.\r\n\r\nI want to report similar problem. Script from @surmenok does not restart my machine. What does is Keras VGGNet training running on TensorFlow backend (see below) on Arch Linux with NVIDIA 390.25 drivers.\r\n\r\nWhat's more interesting, it crashes only when using batch size of 64. Also, it reports few out of memory errors, continues to run, then my machine reboots after ~1.5 epochs.\r\n\r\n~~~\r\n$ python vggnet_keras.py 64\r\nUsing TensorFlow backend.\r\n2018-02-08 15:04:21.206315: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-02-08 15:04:21.345612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-08 15:04:21.346057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.83GiB\r\n2018-02-08 15:04:21.346073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTrain on 1224 samples, validate on 136 samples\r\nEpoch 1/500\r\n2018-02-08 15:04:28.236834: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2018-02-08 15:04:29.400414: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.45GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n1224/1224 [==============================] - 25s 20ms/step - loss: 3.3798 - acc: 0.1389 - val_loss: 14.8144 - val_acc: 0.0809\r\nEpoch 2/500\r\n 832/1224 [===================>..........] - ETA: 5s - loss: 3.0877 - acc: 0.1719\r\npacket_write_wait: Connection to ... port 22: Broken pipe\r\n~~~\r\n\r\nReducing batch size to 32, makes memory errors disappear, and everything works well, no restart.\r\nAlso I tried it on Windows (10) and it works well - no restarts, noticeably slower than on Linux though.\r\n\r\nI also tried VGGNet using TFLearn but it fails to run with batch size of 64, claiming out of memory.\r\n\r\nI believe this can be a indeed PSU problem, but also very likely some corner-case of NVIDIA GPU that draws some \"out of specs\" current when running specific TensorFlow setup, causing restarts on machines using weaker but within-spec PSUs.\r\n\r\nI am using gaming PC (Lenovo Y710 Cube) that came with GTX 1070 and 450W PSU, and it was assembled by Lenovo - not me, so I believe the PSU should be good enough for this GPU. NOTE: This box is also sold with GTX 1080 and same 450W PSU. Moreover, I had no such a problem with this machine.\r\n\r\nNow I wonder if we should report this to NVIDIA?\r\n\r\nI was able to narrow down the example to following script:\r\n~~~py\r\n# VGGNet learning with NVIDIA 1070 restarts my Linux machine\r\n# Using Miniconda Python 3.6 + keras-gpu\r\n# Ported from https://github.com/the-deep-learners/TensorFlow-LiveLessons/blob/master/notebooks/vggnet_in_keras.ipynb\r\n\r\nimport numpy as np\r\nnp.random.seed(42)\r\n\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.callbacks import TensorBoard  # for part 3.5 on TensorBoard\r\n\r\n# import tflearn.datasets.oxflower17 as oxflower17\r\nX, Y = np.random.random((1360, 224, 224, 3)), np.random.random((1360, 17)) # oxflower17.load_data(one_hot=True)\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(Conv2D(64, 3, activation='relu', input_shape=(224, 224, 3)))\r\nmodel.add(Conv2D(64, 3, activation='relu'))\r\nmodel.add(MaxPooling2D(2, 2))\r\nmodel.add(BatchNormalization())\r\n\r\nmodel.add(Conv2D(128, 3, activation='relu'))\r\nmodel.add(Conv2D(128, 3, activation='relu'))\r\nmodel.add(MaxPooling2D(2, 2))\r\nmodel.add(BatchNormalization())\r\n\r\nmodel.add(Conv2D(256, 3, activation='relu'))\r\nmodel.add(Conv2D(256, 3, activation='relu'))\r\nmodel.add(Conv2D(256, 3, activation='relu'))\r\nmodel.add(MaxPooling2D(2, 2))\r\nmodel.add(BatchNormalization())\r\n\r\nmodel.add(Conv2D(512, 3, activation='relu'))\r\nmodel.add(Conv2D(512, 3, activation='relu'))\r\nmodel.add(Conv2D(512, 3, activation='relu'))\r\nmodel.add(MaxPooling2D(2, 2))\r\nmodel.add(BatchNormalization())\r\n\r\nmodel.add(Conv2D(512, 3, activation='relu'))\r\nmodel.add(Conv2D(512, 3, activation='relu'))\r\nmodel.add(Conv2D(512, 3, activation='relu'))\r\nmodel.add(MaxPooling2D(2, 2))\r\nmodel.add(BatchNormalization())\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(4096, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(4096, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(17, activation='softmax'))\r\n\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer='adam', metrics=['accuracy'])\r\n\r\nimport sys\r\nbatch_size = int(sys.argv[1]) if len(sys.argv) > 1 else 64\r\nmodel.fit(X, Y,\r\n          batch_size=batch_size,\r\n          epochs=500, shuffle=True,\r\n          verbose=1, validation_split=0.1)\r\n~~~\r\n\r\nI will try to find out if I can produce some more minimal example with TensorFlow rather than Keras and let you know.", "I also have this issue running a Nvidia 1070Ti and driver version 390.25. It happens when the system is under load \u2013\u00a0both CPU and GPU. It seems to not be tensorflow, though, since I can recreate it using gpu-burn and stress. Maybe a driver problem?", "It is indeed a corner case issue, as NVidia GPU's can be made to draw an average current at, for example, 150 W via the nvidia-smi --power-limit=150 command. However, the cards still silently spike up to much higher than that during heavy operations, and eventually the overcurrent protections in uncompatible power supplies trip.", "I tried the suggestion from @chrschorn, it works. \r\nWas running the code from here : https://github.com/lppier/Convolutional_Neural_Networks/blob/master/high_accuracy_cnn_minst.py\r\nwhich is pretty basic tensorflow stuff. \r\n \r\n```\r\nsudo nvidia-smi --persistence-mode=1\r\nsudo nvidia-smi --power-limit=150\r\n```\r\nRunning Ubuntu 16.04LTS and NVIDIA GTX1080. \r\nIt feels like a driver problem - NVIDIA are you reading? \r\n\r\nEdit: The above works for certain cases. Now when I'm running more intensive cases like using inception_v3, I still get the restarting sometimes. :(", "I am seeing a similar problem running tensorflow on AWS p2.8xlarge instances.  It is inconsistent in that some instances seem to have this problem and others do not.  In fact, I am only seeing the problem with spot instances and not regular (on-demand) instances.", "I'm not seeing restarts in Tensorflow for the same code, Windows 10. Just in Ubuntu, so it doesn't make sense that it a hardware issue. \r\n\r\nI filed a bug report following the following steps:\r\n\r\n1. Go to developer.nvidia.com\r\n2. If you are not already a registered developer, register to become a registered developer (\"Join\" link in upper right hand corner.)\r\n3. Wait until your registration is approved. Typically less than 48 hours.\r\n4. Once registration is approved, log in using your credentials. (login link in upper right corner)\r\n5. In the upper right hand corner, use the drop-down by your name to click on \"my account\"\r\n6. On the left hand side click on \"My Bugs\"\r\n7. On the right hand side click on \"Submit a new bug\"\r\n\r\nIf you can spare some time, do submit a bug report as well, so that we can get NVIDIA to look at this. ", "I have encountered the same question! After the power consumption of GPU is limited to 70%, my computer could finally work.\r\nthe configuration is:\r\nwindows7; \r\nCPU: i7-6700;\r\nGPU: asus 1080Ti;\r\ntf: 1.8.0;\r\npython: 3.5.2\r\nand the Power Supply Unit is 700W in total.", "CPU: i7-2700k \r\nGPU: brand new Asus STRIX 1080TI\r\nPSU: OCZ Fatal1ty 750W\r\n\r\nCan't run one epoch of code posted by @nanoant and would cause a reboot.\r\n\r\nI've monitored power using nvidia-smi and doesn't go over 64W. \r\n\r\nI've ordered a new 1200W PSU but my feeling is that it's a combination of GPU instructions and memory access that's causing the reset.\r\n\r\nUPDATE 1: Installed the 1200W PSU - no more crash ... so it's related to W usage ....\r\n\r\nUPDATE 2: Sadly my computer still reboots while training - I'm training Mask R-CNN - Keras 2.2.3 + Tensorflow 1.10.0.  PSU - FPS AURUM PT 1200W - so Platinum rated PSU", "Ubuntu 16.04 (KDE Neon), Tensorflow 1.9.0, CUDA 9 + CuDNN 7.0\r\nCPU: Xeon W-2125\r\nGPU: Asus GTX 1080 Turbo with version 384 drivers (from distribution)\r\nPSU: Fujitsu proprietary 800W\r\n\r\nHaving similar trouble when training VAE for longer than ~30mins, after which computer reboots without any errors in logs. Temperatures were at ok range (max 80C for GPU). Running Hashcat on GPU + stressing CPU with primesieve did not crash the computer over night.\r\n\r\nAfter checking this thread I decided to record the power usage with nvidia-smi under different loads, if that would shed more light into this.\r\n\r\nRecorded with ` nvidia-smi --loop-ms=20 --format=csv,noheader,nounits --query-gpu=power.draw > out.txt`.\r\n\r\n![Imgur](https://i.imgur.com/scRQ5Bt.png)\r\n\r\n![Imgur](https://i.imgur.com/Cv1m9wE.png)\r\n\r\n![Imgur](https://i.imgur.com/eV8AgEq.png)\r\n\r\nDefault max of this GTX 1080 is 180W.  Draw drops with Hashcat because of throttling, but the draw is more or less constant over time. @nanoant 's code is a horrible mess for PSU to handle, peaking at 230W. My VAE training code is not that terrible, but could still cause issues.\r\n\r\nGeneral \"wattage\" of PSU does not guarantee its capability to handle spiky draws like these. Drawing too much from one line may cause drops in voltage and such, and especially cheap PSUs die under these conditions. A 8-pin power connector (150W) and power from PCI-E port (75W) supply supported 225W, so the 230W peak draws more power from either connector than specified.\r\n\r\nNow that was an interesting little side-project to dig in, now to fix the situation... I guess the best bet is to reduce maximum allowed draw.\r\n\r\nEdit: Changing from batch-size 64 to 32 reduced average power draw by 20W and training has lasted over a hour now (previously crashed well before 1h).\r\nEdit2: Reducing batch size worked for me, and now run has been going for 48h without issues. \r\n", "Thank you @Miffyli ! I have re-done your analysis for a project of mine which was causing the machine to reboot and I also notice power draws beyond the GPU specification, namely up to 290 watts with a 1080Ti, which is rated at 250 W.\r\n\r\n![image](https://user-images.githubusercontent.com/2991890/45425682-6295bf80-b69a-11e8-8aed-060c4bf9fd31.png)\r\n<sup>I should've labelled the axes: `x` is the sample, `y` is the power draw. The line at 30000 is the eval run after the first epoch</sup>\r\n\r\nHowever, I have a PSU of 1000 W, and only 1 GPU out of 2 was in use, so I don't think I reached the limit of the PSU. I have previously done trainings on both GPUs at the same time (different architectures on each) and didn't have this problem.\r\nCould there be a different issue, maybe something with the driver ?\r\n\r\n", "@cipri-tom \r\n\r\nI should have added that these components are designed to handle occasional and short spikes above their designed draw, so with good components these kind of draws shouldn't create issues.  The PSU can be specified to provide X watts, but with cheap PSUs the voltage drops as you try to draw more power, which may cause computer to crash/shut down.\r\n\r\nYou could try reducing your overall draw by e.g. using smaller batches or forcing it via nvidia-smi with:\r\n```\r\nnvidia-smi -pm 1 \r\nnvidia-smi -pl 200\r\n```\r\nFor me the former worked and reduced average draw by 20W. I do not if setting power-limit via nvidia-smi is going to help with the spikes.\r\n\r\n", "Our experience with the Lenovo P910 workstation with the top of the line Lenovo PSU is that even when limiting the power using the nvidia-smi command to very lowest possible value, Tensorflow will make the PSU go to shutdown state due to the power spikes. The system may last a minute, an hour, a day or anything in between, so it is not predictable at all. ", "I ran into this while running code that's almost identical to what @nanoant posted, and I have a very similar setup (Lenovo Y700 with GTX 1070 on Ubuntu 18.04) -- creepy :) The machine always restarts during the first epoch.\r\n\r\nThis works for me:\r\n`sudo nvidia-smi -pm 1 && sudo nvidia-smi -pl 120`\r\n\r\nI watch the power draw via\r\n`watch -n 0.5 nvidia-smi -q -d power`\r\n\r\nValues above 120 don't work. Reducing the batch size to 32 works for me, too, but is slightly slower than 64 with power limit (15s vs. 14s per epoch).\r\n\r\nIn order to avoid running into this issue, I believe it's best to set the power limit at boot. On Ubuntu, I'm doing this with a one-line script in /etc/cron.d:\r\n`@reboot         root    nvidia-smi -pm 1 && nvidia-smi -pl 120`", "> I've encountered the same reboot / hard-crashes on a single Nvidia GTX 1080ti with an Ubuntu 17.10 host anytime I run some examples of TF models (e.g. nearest neighbor, linear regression etc.), and I can't come to a conclusion as to why the crashes continue.\r\n> \r\n> I've tried various nvidia drivers, against different kernel versions, all with CUDA 9 + cuDNN 7 + TF 1.5.0:\r\n> \r\n> * Linux 4.14.x -> Nvidia 384.111, 387.34, 390.25, 390.12\r\n> * Linux 4.13.x -> Nvidia 384.98\r\n> \r\n> I'm also using a 1200W, 80+ Platinum Certified PSU which is more than plenty, so I doubt its a PSU issue. I tried @chrschorn's [suggestion](https://github.com/tensorflow/tensorflow/issues/8858#issuecomment-352744466) to limit the power to 150W instead of the default 250W, but it still crashes with TF jobs.\r\n> \r\n> I don't currently have another machine or GPU to test against, so hoping anyone can provide some clarity or possible next steps. Thanks!\r\n\r\nI would suggest installing a (downloadable pre-compiled or compile it yourself) TF based on different versions of cuDNN (not necessary to be the latest version). It may be issues related to glitches happening at certain combinations of software/hardware. ", "I had completely the same syndrome, tensorflow occasionally caused system reboot. Such reboots were greeted by Asus Power-Surge warning. Certain network configurations gave higher chance of reboot (e.g. placing a 1x1x1 conv-kernel at the deepest layer instead of 3x3x3 conv-kernel, I have no idea why). The situation became more and more frequent, to a point that I couldn't do any meaningful training.\r\n\r\nThe problem started when I upgraded from i5-4570 to the wattage-beast i7-4790k. At the beginning, it rebooted very rarely, like once a week. After 3 weeks, it was rebooting every hour. Limiting the power usage to 150W and underclocking the GPU slightly lowered the chance of reboot, but not eliminating it. Problem finally solved after upgrading my PSU to EVGA G1+ 1000W. \r\n\r\nI have 4x120mm fans, 3xHDD, 1xSSD, 1xDVDRW, EVGA 1080ti without OC, Asus Z97-p, i7-4790k with stock cooler (also tried with Coolermaster Hyper 212 evo), with 5 USB flashdrive always plugged-in, Coolermaster 750W PSU (now 1000W PSU). A few online calculators showing my configuration had reached over 90% usage of the total of 3.3V+5V+12V rails of the 750W PSU.", "My verdict is that the software/firmware wattage limit of GPU isn't a laser-accurate power limit of the hardware. It may be a limit of, say the 5-second-averaged power draw of the GPU. Some hardware ops may momentarily have a burst of current draw, lasting only perhaps a few hundred ms. Limiting GPU power draw won't remove but rather reduce the usage of such ops. How hardware ops triggered according to program codes is related to the driver, cuDNN and TF compilation. Also, whether or not the system survives the voltage fluctuation incurred from the burst of current draw depends on luck. \r\n", "I am having similar issue, CPU is i9-7940x, one gtx 1080 ti, motherboard is ASUS WS X299 sage, PSU: Corsair AX 1600i, OS: ubuntu 18.04, CUDA 9.2, CUDNN: 7.1/7.2/7.4. The power is more than sufficient, however, whenever I run a certain tensorflow code(still trying to figure out the problematic part), the computer will reboot, no warning, the screen freeze for a while then reboot. And I cannot find any record related to the reboot. It is just like a regular computer booting. \r\nThe same code runs fine on my 5-year old computer with gtx 1080 ti, evga 650w psu, i5-3570, asus sabertooth z77 and ubuntu 16.04. ", "> I am having similar issue, CPU is i9-7940x, one gtx 1080 ti, motherboard is ASUS WS X299 sage, PSU: Corsair AX 1600i, OS: ubuntu 18.04, CUDA 9.2, CUDNN: 7.1/7.2/7.4. The power is more than sufficient, however, whenever I run a certain tensorflow code(still trying to figure out the problematic part), the computer will reboot, no warning, the screen freeze for a while then reboot. And I cannot find any record related to the reboot. It is just like a regular computer booting.\r\n> The same code runs fine on my 5-year old computer with gtx 1080 ti, evga 650w psu, i5-3570, asus sabertooth z77 and ubuntu 16.04.\r\n\r\nI would suggest running stress test using some benchmark software to maximize power consumption of both CPU and GPU. It may not be the issue of PSU but the aging of mobo power supply circuit. You may look for popped capacitor on mobo or the graphic card. Swapping components between the i9 and i5 computers also help to troubleshoot the issue.", "> > I am having similar issue, CPU is i9-7940x, one gtx 1080 ti, motherboard is ASUS WS X299 sage, PSU: Corsair AX 1600i, OS: ubuntu 18.04, CUDA 9.2, CUDNN: 7.1/7.2/7.4. The power is more than sufficient, however, whenever I run a certain tensorflow code(still trying to figure out the problematic part), the computer will reboot, no warning, the screen freeze for a while then reboot. And I cannot find any record related to the reboot. It is just like a regular computer booting.\r\n> > The same code runs fine on my 5-year old computer with gtx 1080 ti, evga 650w psu, i5-3570, asus sabertooth z77 and ubuntu 16.04.\r\n> \r\n> I would suggest running stress test using some benchmark software to maximize power consumption of both CPU and GPU. It may not be the issue of PSU but the aging of mobo power supply circuit. You may look for popped capacitor on mobo or the graphic card. Swapping components between the i9 and i5 computers also help to troubleshoot the issue.\r\n\r\nThank you for replying. I have the tested the same GPU on the old computer, it works great, it runs the same code without crash.\r\nThe CPU itself also works ok, I used that to compile tensorflow, the CPU usage is over 90%, no problem found so far. I also tested the computer with three 1600w PSU, symptoms are the same.  It seems that the computer will reboot when the program involves both CPU and GPU and the power consumption is very low.\r\nI highly suspect that it is the motherboard's problem, it is a new motherboard(asus x299 sage), 18.04 is the only version of Ubuntu I can successfully install. ", "Your rebooting issue recalls my memory of a super bad luck computer building. All components of that computer were new, but it always reboots after 1 or 2 hours of use, guaranteed an unsuccessful windows installation. It was ok when I blew the entire machine under an AC with the side panel off. It passed all kind of memtest and cputest that required no OS to operate. Turn out it was the faulty new RAM from Asus. The memtest would fail at the 3rd complete iterations after about an hour. The components were functional but broke down under certain circumstances. \r\n\r\nI guess your situation is similar. PCIe 16x alone provides 75W of power. This current is regulated by mobo circuitry and not connected directly to the PSU 12v rail. That part of circuitry stressed out during GPU full loading, even if the PSU has plenty of reserved juice. Try plugging another wattage beast in your Asus mobo. You may eventually pinpoint the problem, and in the best case, require a replacement from the vendor.", "The problem was fixed by downgrading the bios (asus X299 sage) from 0601 to 0502. \r\n\r\nBoth the initial and lastest(0601) bios of asus x299 sage are problematic. With 0601, a script of several \"tf.load_op_library('custom_cuda_module.so')\" may reboot the computer, depends on the PSU and whether 'watch -d -n 0.2 nvidia-smi' is used, computer may reboot at the script's first run, third run or fifth run ... ; the keras example provided by @nanoant will also cause computer rebooting after a few epochs. \r\n\r\nAfter downgrading to 0502, all previous problematic codes seem to work fine. However, I am still a little worried that some unknown motherboard bugs may get triggered by other codes in the future. \r\n", "Just wanted to confirm the above comment: this exact issue (random reboots) happened to me on an ASUS x299 Sage with Bios 0601. There was a new Bios available (0701) which fixed it, system has been running stable for 24 hours now with four GPUs on full load.\r\n", "If you experience sudden reboots, the most probable cause is that your PSU is underrated. A machine rebooting or shutting off suddenly is actually a good sign, because it suggests that the PSU in cleanly shutting off to prevent damage to the hardware. This happens with high-end PSUs that have overcurrent (OCP) / overload (OLP) protection (these two terms refer to the same thing). By contrast, lower-end PSUs may cause hardware crashes such as system freezes due to voltage instability.\r\n\r\nI have seen a dual Titan X/Xp + Xeon E5 v4 145W setup work fine with a 850W PSU when doing simple TensorFlow compute involving both GPUs, but that almost always failed for heavy models that tend to maximize the compute and memory use. This was solved by upgrading to a 1300W PSU. In short, you will be safe with a 800W PSU for a single 250W GPU, and a 1200W PSU for two 250W  GPUs. Some people recommend ~1000W PSUs for dual-GPU setups, but in my experience this will not be sufficient.\r\n\r\nAs for PSU recommendations, I have seen the Seasonic Primes series work flawlessly, specifically the Titanium 850W for a single-GPU setup, and the Platinum 1300W for a dual-GPU setup. There are many other PSU manufacturers out there that do extremely well in these wattage categories.\r\n\r\nI though that it would be useful to add some explanations as to why sudden reboots might be happening:\r\n\r\nHigh-end GPUs that are rated for 250W routinely exceed 320W of power draw at stock frequencies and default wattage limit, for what appears to be periods up to hundreds of milliseconds. Looking at NVIDIA\u2019s official recommendations for PSU wattage, 600W is the minimum requirement for a single Titan X/Xp/1080ti GPU, and 850W for two of these GPUs. Wattage recommendation for the newer RTX 2080ti GPU was bumped to 650W, not sure about the Titan RTX. Based on my own experience, these recommendations will definitely lead to sudden reboots with high-end CPUs that are rated for 145W+ (and probably peaking over 200W). The overall system power draw will easily exceed the recommended figures, seemingly randomly during compute and gaming. Good PSUs are accurate in detecting transient overloads, and will shut off at the first opportunity.\r\n\r\nHigh transient currents were mentioned as a possible cause for sudden reboots, and it was also suggested that this might be a corner case of GPU utilization. It is true that some people have reported issues with PSUs shutting off wrongly in response to high transient currents that were supposedly still under the wattage limit of the PSU. In my opinion this was never clearly demonstrated, and for the vast majority of cases, sudden reboots are caused by underrated PSUs as explained above.\r\n\r\nIt was also mentioned that reboots might be caused by a software issue, as someone here reported that their hardware experienced sudden reboots on Linux but worked well on Windows. Linux and Windows drivers have different implementations, which may change how GPUs draw current. In addition, TensorFlow on Linux appears to run faster than on Windows, suggesting higher current draw on Linux. This is consistent with the observation that sudden reboots occur more frequently on Linux.\r\n\r\nAnother possible cause is the motherboard. As mentioned earlier in this thread, GPUs will get 75W from the PCIe slot, an additional 150W from the 8-pin connector and the last 75W from the 6-pin connector. For high-end GPUs that are rated for 250W+, this means getting the full (or nearly) 75W from the PCIe slot. Any weakness on the motherboard power circuitry could result in all sorts of crashes, even if the PSU is fine. If you think that your PSU is fine, try a different PCIe slot and a different motherboard BIOS version. If your RAM and CPU are doing fine in other tasks, it is unlikely that the power circuitry of the motherboard is affected.\r\n\r\nIf you experience random reboots that do not seem to correlate with system load or heat, I would suggest looking at the RAM. Bad RAM may work flawlessly for days in memtest, and then crash a system during idle just 5 minutes after booting. This kind of unpredictable behavior makes it very difficult to understand what is faulty in a system. If you can afford for more expensive server parts, get a Xeon and ECC RAM. This would definitely help if you experience hardware-related issues.\r\n\r\nIn conclusion, always opt for a premium PSU with plenty of extra watts. The overall power requirements and stress on the PSU will be high during GPU compute. Official wattage recommendations are way too low and will likely cause issues during GPU compute. Last, beware of cheap PSUs. Melted connectors were reported for dual-GPU gaming systems. It is clearly a better option to have a PSU cleanly shut off when a system draws too much current, than find melted wires and connectors.", "I had similar issues, random reboots under Tensorflow GPU load with Gigabyte X299 UD4 motherboard shipped with BIOS version F3. Official support for my CPU (Intel Core i7-9800X CPU @ 3.80GHz) came under BIOS version F6j. Updated the BIOS and completely stable. Echoing others before me, make sure your motherboard BIOS explicitly supports the CPU even if all appears to be well.\r\n\r\n", "I've been struggling with this issue as well. For me a BIOS update fixed the problem as well!", "Just coming here to add that I was having a similar issue: was using Keras to make predictions on my GTX 1080 TI with larger images than what the training had used. At certain sizes my computer would auto reboot, no messages, no warnings. I tried monitoring the temperature but it was below the set limit, so that wasn't the issue.\r\n\r\nMy awesome IT guy switched out the standard PSU I had with a better one and the problem has gone away.", "This is pretty much the only place I've found with others having the same issue as me so I'm commenting here in case it helps others. I was having similar issues with both Tensorflow & PyTorch -- computer basically just shutting off/rebooting randomly while training certain networks on GPU. No messages and nothing would show in any logs I checked. `stress` and `gpu-burn` would not trigger a crash. Things I tried that **did not work**:\r\n\r\n- Replacing GPU\r\n- Corsair replaced my RAM under warranty. I ran `memtest` and got some errors so they replaced it.\r\n- Upgraded to a 1200W PSU. I previously had 860W and 1 GPU, now I have 1200W with 2GPU. Would still crash even with 1200W and 1 GPU.\r\n- Updating BIOS\r\n\r\nThe one thing that worked for me: **Turning off the CPU Turbo Mode** in my BIOS settings (ASUS X299 Deluxe Prime motherboard). I did this 2 days ago and I haven't had any crashes since despite running almost constantly. Before this it would crash within 10 minutes of starting training.", "> I'm not seeing restarts in Tensorflow for the same code, Windows 10. Just in Ubuntu, so it doesn't make sense that it a hardware issue.\r\n> \r\n> I filed a bug report following the following steps:\r\n> \r\n> 1. Go to developer.nvidia.com\r\n> 2. If you are not already a registered developer, register to become a registered developer (\"Join\" link in upper right hand corner.)\r\n> 3. Wait until your registration is approved. Typically less than 48 hours.\r\n> 4. Once registration is approved, log in using your credentials. (login link in upper right corner)\r\n> 5. In the upper right hand corner, use the drop-down by your name to click on \"my account\"\r\n> 6. On the left hand side click on \"My Bugs\"\r\n> 7. On the right hand side click on \"Submit a new bug\"\r\n> \r\n> If you can spare some time, do submit a bug report as well, so that we can get NVIDIA to look at this.\r\n\r\nHi @lppier ,Can you please let me know if the above issue got resolved for you as the issue occurs for me in a modern gpu(RTX 3090) with all the latest drivers for me on ubuntu system but Windows 10 perfectly works .\r\ni was able to run perfectly by limiting power to 150 w .\r\nIs this a hardware issue ", "@kuruvilla2087 , sorry to jump in, but the rest of the thread suggests that this is due to a power source that is limited / unreliable. You should limit the GPU or get a platinum rated GPU that has enough power for the GPU and the rest of the system. ", "Hi @cipri\r\n\r\nI have a 1000 w system corsair gold ,wouldn't that be sufficient to handle it", "@kuruvilla2087  check out this thread : https://twitter.com/radekosmulski/status/1386046714981801984", "> This is pretty much the only place I've found with others having the same issue as me so I'm commenting here in case it helps others. I was having similar issues with both Tensorflow & PyTorch -- computer basically just shutting off/rebooting randomly while training certain networks on GPU. No messages and nothing would show in any logs I checked. `stress` and `gpu-burn` would not trigger a crash. Things I tried that **did not work**:\r\n> \r\n>     * Replacing GPU\r\n> \r\n>     * Corsair replaced my RAM under warranty. I ran `memtest` and got some errors so they replaced it.\r\n> \r\n>     * Upgraded to a 1200W PSU. I previously had 860W and 1 GPU, now I have 1200W with 2GPU. Would still crash even with 1200W and 1 GPU.\r\n> \r\n>     * Updating BIOS\r\n> \r\n> \r\n> The one thing that worked for me: **Turning off the CPU Turbo Mode** in my BIOS settings (ASUS X299 Deluxe Prime motherboard). I did this 2 days ago and I haven't had any crashes since despite running almost constantly. Before this it would crash within 10 minutes of starting training.\r\n\r\nI have the same issue with RTX 3080Ti + Intel i9-11900k + Corsair RM1000x.\r\nDisabling the **CPU Turbo** works for me!\r\nSimilar issue: https://github.com/pytorch/pytorch/issues/3022#issuecomment-419093454", "We had a similar issue - the machine would reboot in the middle of running a simple TF script. The machine has 3 GPUs and 160 CPUs. Turning off the CPU turbo by setting `intel_pstate/no_turbo` to `1`, as outlined in the top answer on [this page](https://askubuntu.com/questions/619875/disabling-intel-turbo-boost-in-ubuntu), worked. Just make sure to reboot the machine.\r\nHardware solution might involve replacing the power unit and cables that deal well with power consumption spikes."]}, {"number": 8857, "title": "Utility for repeatedly running tensors on queued input and accumulating the results", "body": "I've written a utility function in TensorFlow that I've found quite helpful for loading Tensors with queue based inputs into memory, e.g., for looking at input data stored in the form of TF-records files or for looking at inference results. I've found it especially useful for interactively exploring the results of saved models from IPython notebooks, i.e., doing inference on small datasets or small portions of big datasets.\r\n\r\nHere's what the API looks like:\r\n```python\r\nfrom typing import Mapping, Optional, Dict\r\n\r\ndef run_repeatedly(\r\n    batched_tensors: Mapping[object, tf.Tensor],\r\n    checkpoint_dir: Optional[str] = None,\r\n    max_num_batches: Optional[int] = None) -> Dict[object, np.ndarray]:\r\n  \"\"\"Repeatedly run tensors until they are exhausted.\r\n\r\n  Args:\r\n    batched_tensors: dict of tensors to evaluate, each of which should have a\r\n      first axis corresponding to a batch of examples.\r\n    checkpoint_dir: optional path to checkpoint to load.\r\n    max_num_batches: optional maximum number of batches to run.\r\n\r\n  Returns:\r\n    A dict of numpy.ndarray objects containing the result of evaluating\r\n    batched_tensors and concatenated across batches along the first axis.\r\n\r\n  Raises:\r\n    ValueError: if checkpoint_dir has no valid checkpoint\r\n  \"\"\"\r\n  # the implementation makes use of tf.contrib.metrics.streaming_concat\r\n  # and a tf.train.Supervisor: it handles all the boilerplate around starting\r\n  # up a session.\r\n```\r\n\r\nAnd a few usage example:\r\n\r\n- Previewing features loaded from files:\r\n```\r\ntensors = tf.contrib.learn.read_batch_features(....)\r\narrays = run_repeatedly(tensors, max_num_batches=10)\r\n```\r\n- For running inference on a full dataset while also accumulating input tensors (useful for debugging):\r\n```\r\ntensors = tf.contrib.learn.read_batch_features(....)\r\npredictions = make_predictions(tensor_inputs, ...)\r\ntensors.update(predictions)\r\narrays = run_repeatedly(tensors, checkpoint_dir=path_to_saved_model)\r\n```\r\n\r\n**Does something like this belong somewhere in core TensorFlow, or maybe one of the contrib libraries?**\r\n\r\nThis is somewhat similar to `Estimator.predict` from `tf.contrib.learn`, but with a few key differences:\r\n- It's more flexible, not expecting inputs in the form of a `tf.learn` model.\r\n- It automatically concatenates across batches. In practice, I find this highly useful, because I can often store the results of a model in memory even though I don't have enough memory to run inference on everything at once.", "comments": ["@mrry, could you evaluate this contribution since it is firmly in your area of expertise. Thanks!", "friendly ping, Derek: is this something that's still useful for users?", "It sounds like a utility that could stand alone as a self-contained project, but I don't think it's something we'd  want to support as part of the core library."]}, {"number": 8856, "title": "Cherrypicks for r1.1-rc1", "body": "Cherrypicks from the list emailed internally. \r\n\r\nPlease verify the configure script behavior is correct. There was a conflict when cherrypicking fba05c300bf6840e76787680ed7fd1239cdb9ad0", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLA check ok in this state. ", "Jenkins, test this please."]}, {"number": 8855, "title": "Branch 151761382", "body": "", "comments": []}, {"number": 8854, "title": "tensorboard: view graph from saved_model.pb file [feature request]", "body": "Plot a graph from just a saved_model.pb file. \r\n\r\nCurrently tensorboard only works given a training folder containing checkpoints and summary events. Understanding the output graph is important, especially if you don't have access to the training output files.", "comments": ["@brandondutra As far as I know, you don't need to create any summaries to load the graph into Tensorboard. If you begin to create a summarywriter and then add the graph to it then you should see the graph appear in Tensorboard. I have some code that does that (as part of another project). I do however agree that being able to import something into Tensorboard would be handy.\r\n\r\nI've quickly created a bit of code to load a graph into Tensorboard. See how that goes. [Also available as a gist here](https://gist.github.com/jubjamie/2eec49ca1e4f58c5310d72918d991ef6)\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.platform import gfile\r\nwith tf.Session() as sess:\r\n    model_filename ='PATH_TO_PB.pb'\r\n    with gfile.FastGFile(model_filename, 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        g_in = tf.import_graph_def(graph_def)\r\nLOGDIR='YOUR_LOG_LOCATION'\r\ntrain_writer = tf.summary.FileWriter(LOGDIR)\r\ntrain_writer.add_graph(sess.graph)\r\n```\r\nYou should then be able to see this in Tensorboard.\r\nSomething like that perhaps in the mean time? I might put a PR in to get some conversation going,", "This is awesome! Thank you for your help. I took your script and ran it, but I ran into an issue:\r\n\r\n`google.protobuf.message.DecodeError: Error parsing message`\r\n\r\nBecause the error is in parsing the message, I looked into tensorflow protos. I modified your script, and got it working on my saved_model.pb file.\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\nfrom tensorflow.python.platform import gfile\r\n\r\nfrom tensorflow.core.protobuf import saved_model_pb2\r\nfrom tensorflow.python.util import compat\r\n\r\nwith tf.Session() as sess:\r\n    model_filename ='saved_model.pb'\r\n    with gfile.FastGFile(model_filename, 'rb') as f:\r\n\r\n      \tdata = compat.as_bytes(f.read())\r\n      \tsm = saved_model_pb2.SavedModel()\r\n      \tsm.ParseFromString(data)\r\n      \t#print(sm)\r\n      \tif 1 != len(sm.meta_graphs):\r\n      \t\tprint('More than one graph found. Not sure which to write')\r\n      \t\tsys.exit(1)\r\n\r\n      \t#graph_def = tf.GraphDef()\r\n        #graph_def.ParseFromString(sm.meta_graphs[0])\r\n        g_in = tf.import_graph_def(sm.meta_graphs[0].graph_def)\r\nLOGDIR='YOUR_LOG_LOCATION'\r\ntrain_writer = tf.summary.FileWriter(LOGDIR)\r\ntrain_writer.add_graph(sess.graph)\r\n```\r\n\r\nAgain, thank you!", "@brandondutra I guess that there was something odd with your .pb then. Worked fine on mine! Glad I could help.", "Closing since this feature is available through existing TensorBoard functionality.", "@aselle Can I just confirm what you mean by existing Tensorboard functionality? Do you mean the code that myself and @brandondutra wrote (and associated functions) or is there another way?", "Yes that's what I mean. @dandelionmane, do you want to add a more direct way to visualize saved graphs?", "Yes. Telling users to write a python script to visualize a saved model graph is a very bad user experience [it's not even documented as a thing that can be done], and seems like something tensorboard should be very good at.", "Actually, tf.summary.FileWriter accepts a GraphDef parameter which marks as deprecated.", "I think the point/ask of this issue is that tensorboard should show the graph from a saved_model.pb file", "When trying to load a frozen pb model to visualise its graph, the snippets above didn't work for me in tensorflow 1.4, Python 3.6 (the resulting folder was empty).\r\n\r\nUsing the Upload - Choose file option in tensorboard's GUI gave `Graph visualization failed: The graph is empty`.\r\n\r\nHowever, using the `import_to_tensorboard` function did the trick:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/tools/import_pb_to_tensorboard.py", "hey @MikulasZelinka how to use this function?\r\n\r\ncan you post an example how to call it with an example model and log dir? \r\nFor me somehow it does not work\r\n\r\n@jubjamie your code works fine, thanks :+1: \r\n\r\nThank you!\r\n  ", "@GustavZ You can visualize the graph of your frozen pb model using this command\r\n\r\npython import_pb_to_tensorboard.py --model_dir=\"path/to/your/model/file.pb\" --log_dir=\"path/to/your/log/dir\"", "I found I had to change  @brandondutra script slightly to add a flush and close at the end.\r\nElse on some models nothing would be written to the LOG_DIR.\r\nAlso found that import_pb_to_tensorboard gave the same error as `google.protobuf.message.DecodeError: Error parsing message` so had to use this in the first place.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\nfrom tensorflow.python.platform import gfile\r\n\r\nfrom tensorflow.core.protobuf import saved_model_pb2\r\nfrom tensorflow.python.util import compat\r\n\r\nwith tf.Session() as sess:\r\n\tmodel_filename =sys.argv[1]\r\n\twith gfile.FastGFile(model_filename, 'rb') as f:\r\n\r\n\t\tdata = compat.as_bytes(f.read())\r\n\t\tsm = saved_model_pb2.SavedModel()\r\n\t\tsm.ParseFromString(data)\r\n\t\t#print(sm)\r\n\t\tif 1 != len(sm.meta_graphs):\r\n\t\t\tprint('More than one graph found. Not sure which to write')\r\n\t\t\tsys.exit(1)\r\n\t\t\t\r\n\t\t#graph_def = tf.GraphDef()\r\n\t\t#graph_def.ParseFromString(sm.meta_graphs[0])\r\n\t\tg_in = tf.import_graph_def(sm.meta_graphs[0].graph_def)\r\n\tLOGDIR=sys.argv[2]\r\n\r\ntrain_writer = tf.summary.FileWriter(LOGDIR)\r\ntrain_writer.add_graph(sess.graph)\r\ntrain_writer.flush()\r\ntrain_writer.close()\r\n```", "@Jon889, I followed you suggestion and I can overcome error \"the google.protobuf.message.DecodeError: Error parsing message\"\r\n\r\nBut when I use tensorboard to view the graph, it is empty graph,\r\n\r\ni made below change, and i can view graph, \r\nI tested with .pb created by \"mnist_saved_model.py\" (https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_saved_model.py) \r\n\r\ncode snippet:\r\n```\r\nimport argparse\r\nimport sys\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.platform import app\r\nfrom tensorflow.python.summary import summary\r\n\r\ndef import_to_tensorboard(model_dir, log_dir):\r\n  \"\"\"View an imported protobuf model (`.pb` file) as a graph in Tensorboard.\r\n\r\n  Args:\r\n    model_dir: The location of the protobuf (`pb`) model to visualize\r\n    log_dir: The location for the Tensorboard log to begin visualization from.\r\n\r\n  Usage:\r\n    Call this function with your model location and desired log directory.\r\n    Launch Tensorboard by pointing it to the log directory.\r\n    View your imported `.pb` model as a graph.\r\n  \"\"\"\r\n\r\n  with tf.Session(graph=tf.Graph()) as sess:\r\n    tf.saved_model.loader.load(\r\n        sess, [tf.saved_model.tag_constants.SERVING], model_dir)\r\n\r\n    pb_visual_writer = summary.FileWriter(log_dir)\r\n    pb_visual_writer.add_graph(sess.graph)\r\n    print(\"Model Imported. Visualize by running: \"\r\n          \"tensorboard --logdir={}\".format(log_dir))\r\n\r\n```\r\n\r\n\r\n", "If you have two .pb files with the same structure and format, is it possible to add them together???\r\n\r\nLet say that you have a trained model then you made a fine-tune for new calsses, then you add the result pb file to the original one so you reduce training time and having one pb file like you made a full training operation", "I don't really think import_pb_to_tensorboard solves the issue here: 1) firstly, model_dir takes a graph.pb, instead of a saved_model.pb; 2) what is it with \"log_idr\"?\r\n\r\nOn a separate note: I think TF folks could've done a much better job handling RFEs from users, instead of just keeping on closing them.", "how tom open .pb files?\r\n", "While adding my frozen.pb file to it, it gives me an error\r\nTraceback (most recent call last):\r\nFile \"convert.py\", line 30, in \r\ng_in = tf.import_graph_def(graph_def)\r\nFile \"/home/ios/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\nreturn func(*args, **kwargs)\r\nFile \"/home/ios/.local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 671, in import_graph_def\r\nnode, 'Input tensor %r %s' % (input_name, te)))\r\nValueError: graph_def is invalid at node u'Genc/Conv/BatchNorm/AssignMovingAvg': Input tensor 'Genc/Conv/BatchNorm/moving_mean:0' Cannot convert a tensor of type float32 to an input of type float32_ref.\r\n\r\nand if run it through another code which is:\r\n\r\nimport tensorflow as tf\r\nimport sys\r\nfrom tensorflow.python.platform import gfile\r\n\r\nfrom tensorflow.core.protobuf import saved_model_pb2\r\nfrom tensorflow.python.util import compat\r\n\r\nwith tf.Session() as sess:\r\nmodel_filename ='saved_model.pb'\r\nwith gfile.FastGFile(model_filename, 'rb') as f:\r\n\r\n  \tdata = compat.as_bytes(f.read())\r\n  \tsm = saved_model_pb2.SavedModel()\r\n  \tsm.ParseFromString(data)\r\n  \t#print(sm)\r\n  \tif 1 != len(sm.meta_graphs):\r\n  \t\tprint('More than one graph found. Not sure which to write')\r\n  \t\tsys.exit(1)\r\n\r\n  \t#graph_def = tf.GraphDef()\r\n    #graph_def.ParseFromString(sm.meta_graphs[0])\r\n    g_in = tf.import_graph_def(sm.meta_graphs[0].graph_def)\r\nLOGDIR='YOUR_LOG_LOCATION'\r\ntrain_writer = tf.summary.FileWriter(LOGDIR)\r\ntrain_writer.add_graph(sess.graph)\r\n\r\nand gave me error:\r\n\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n\r\nany solution for this..??\r\n\r\ni converted to .pb file from checkpoint file using tensorflow frozen_python file.\r\n\r\n", "I also tried to run it through import_pb_to_tensorboard.py file but in that also i got an error of float32 to float32_ref\r\n\r\nI think there is something wrong in .pb file, but i converted it through frozen_graph.py of tensorflow . and passed all the correct output_node_name. so i dont figure what is error. ", "First answers are outdated:\r\n\r\nNow you can use:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/import_pb_to_tensorboard.py", "No you can't:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow_core/python/tools/import_pb_to_tensorboard.py\", line 86, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow_core/python/tools/import_pb_to_tensorboard.py\", line 68, in main\r\n    import_to_tensorboard(FLAGS.model_dir, FLAGS.log_dir)\r\n  File \"/home/user/.local/lib/python3.6/site-packages/tensorflow_core/python/tools/import_pb_to_tensorboard.py\", line 58, in import_to_tensorboard\r\n    graph_def.ParseFromString(f.read())\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n```\r\nThis is in 2.0. The whole SavedModel debacle is an unmitigated disaster.", "any solution to this?", "^ the above PR should fix the problem. For those that are impatient:\r\n```\r\nwget https://raw.githubusercontent.com/ltn100/tensorflow/feature/LN/fix_import_pb_to_tensorboard/tensorflow/python/tools/import_pb_to_tensorboard.py\r\npython import_pb_to_tensorboard.py --model_dir model_dir/1 --log_dir logs/001\r\n```", "\r\n\r\n\r\n\r\n> @brandondutra As far as I know, you don't need to create any summaries to load the graph into Tensorboard. If you begin to create a summarywriter and then add the graph to it then you should see the graph appear in Tensorboard. I have some code that does that (as part of another project). I do however agree that being able to import something into Tensorboard would be handy.\r\n> \r\n> I've quickly created a bit of code to load a graph into Tensorboard. See how that goes. [Also available as a gist here](https://gist.github.com/jubjamie/2eec49ca1e4f58c5310d72918d991ef6)\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> from tensorflow.python.platform import gfile\r\n> with tf.Session() as sess:\r\n>     model_filename ='PATH_TO_PB.pb'\r\n>     with gfile.FastGFile(model_filename, 'rb') as f:\r\n>         graph_def = tf.GraphDef()\r\n>         graph_def.ParseFromString(f.read())\r\n>         g_in = tf.import_graph_def(graph_def)\r\n> LOGDIR='YOUR_LOG_LOCATION'\r\n> train_writer = tf.summary.FileWriter(LOGDIR)\r\n> train_writer.add_graph(sess.graph)\r\n> ```\r\n> \r\n> You should then be able to see this in Tensorboard.\r\n> Something like that perhaps in the mean time? I might put a PR in to get some conversation going,\r\n\r\nDon't forget to flush ;)\r\n\r\n```\r\ntrain_writer.flush()\r\n```", "Here's a handy Colab notebook in case anyone wants to try out [this](https://github.com/tensorflow/tensorflow/pull/33954): https://colab.research.google.com/drive/13LAUUT9tEH2XeoNA_z9A7uE5omc-lzNv", "@sayakpaul thanks for the notebook!\r\n@jubjamie\r\n@Jon889\r\n\r\nany idea why that doesn't work like that - without your function?\r\n```\r\nfrom tensorflow.python.tools.import_pb_to_tensorboard import import_to_tensorboard\r\nimport os\r\n\r\nmodel_dir = './model'\r\nmodel = os.path.join(model_dir, 'saved_model.pb')\r\nprint(model)\r\nimport_to_tensorboard(model_dir=model, log_dir='./logs/')\r\n```\r\n\r\nI see this error:\r\n```\r\n~$ python main.py \r\n2020-03-06 23:14:33.962070: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-03-06 23:14:33.970833: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200075000 Hz\r\n2020-03-06 23:14:33.972744: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563e6e9a9f60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-03-06 23:14:33.972774: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"main4.py\", line 7, in <module>\r\n    import_to_tensorboard(model_dir=model, log_dir='./logs/')\r\n  File \"/home/mike18/venv/lib/python3.6/site-packages/tensorflow_core/python/tools/import_pb_to_tensorboard.py\", line 58, in import_to_tensorboard\r\n    graph_def.ParseFromString(f.read())\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n```\r\n\r\nalso why does this code not work with Tensorflow 2?\r\nhttps://github.com/tobegit3hub/tfmodel/blob/22c1c48a7049a9cac8e533e317f41697fb85ff7a/tfmodel/savedmodel_analyst.py\r\n\r\n\r\nwhen I use your function it works:\r\n```\r\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ================================\r\n\"\"\"Imports a protobuf model as a graph in Tensorboard.\"\"\"\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport sys\r\n\r\nfrom tensorflow.python.client import session\r\nfrom tensorflow.python.framework import importer\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.platform import app\r\nfrom tensorflow.python.summary import summary\r\nfrom tensorflow.python.tools import saved_model_utils\r\n\r\n# Try importing TensorRT ops if available\r\n# TODO(aaroey): ideally we should import everything from contrib, but currently\r\n# tensorrt module would cause build errors when being imported in\r\n# tensorflow/contrib/__init__.py. Fix it.\r\n# pylint: disable=unused-import,g-import-not-at-top,wildcard-import\r\ntry:\r\n  from tensorflow.contrib.tensorrt.ops.gen_trt_engine_op import *\r\nexcept ImportError:\r\n  pass\r\n# pylint: enable=unused-import,g-import-not-at-top,wildcard-import\r\n\r\ndef import_to_tensorboard(model_dir, log_dir, tag_set):\r\n  \"\"\"View an imported protobuf model (`.pb` file) as a graph in Tensorboard.\r\n\r\n  Args:\r\n    model_dir: The location of the protobuf (`pb`) model to visualize\r\n    log_dir: The location for the Tensorboard log to begin visualization from.\r\n    tag_set: Group of tag(s) of the MetaGraphDef to load, in string format,\r\n        separated by ','. For tag-set contains multiple tags, all tags must be\r\n        passed in.\r\n\r\n  Usage:\r\n    Call this function with your model location and desired log directory.\r\n    Launch Tensorboard by pointing it to the log directory.\r\n    View your imported `.pb` model as a graph.\r\n  \"\"\"\r\n  with session.Session(graph=ops.Graph()) as sess:\r\n    input_graph_def = saved_model_utils.get_meta_graph_def(\r\n        model_dir, tag_set).graph_def\r\n    importer.import_graph_def(input_graph_def)\r\n\r\n    pb_visual_writer = summary.FileWriter(log_dir)\r\n    pb_visual_writer.add_graph(sess.graph)\r\n    print(\"Model Imported. Visualize by running: \"\r\n          \"tensorboard --logdir={}\".format(log_dir))\r\n\r\n\r\ndef main(_):\r\n  import_to_tensorboard(FLAGS.model_dir, FLAGS.log_dir, FLAGS.tag_set)\r\n\r\nif __name__ == \"__main__\":\r\n  parser = argparse.ArgumentParser()\r\n  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\r\n  parser.add_argument(\r\n      \"--model_dir\",\r\n      type=str,\r\n      default=\"\",\r\n      required=True,\r\n      help=\"The directory containing the SavedModel to import.\")\r\n  parser.add_argument(\r\n      \"--log_dir\",\r\n      type=str,\r\n      default=\"\",\r\n      required=True,\r\n      help=\"The location for the Tensorboard log to begin visualization from.\")\r\n  parser.add_argument(\r\n      '--tag_set',\r\n      type=str,\r\n      default=\"serve\",\r\n      required=False,\r\n      help='tag-set of graph in SavedModel to load, separated by \\',\\'')\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```\r\n\r\nand than run:\r\n```\r\n$ python import_pb_to_tensorboard.py --model_dir ./model --log_dir logs/\r\n2020-03-06 23:23:07.019262: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-03-06 23:23:07.027290: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200075000 Hz\r\n2020-03-06 23:23:07.029262: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557ede9a02e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-03-06 23:23:07.029284: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nModel Imported. Visualize by running: tensorboard --logdir=logs/\r\n```", "> @sayakpaul thanks for the notebook!\r\n> @jubjamie\r\n> @Jon889\r\n> \r\n> any idea why that doesn't work like that - without your function?\r\n> \r\n> ```\r\n> from tensorflow.python.tools.import_pb_to_tensorboard import import_to_tensorboard\r\n> import os\r\n> \r\n> model_dir = './model'\r\n> model = os.path.join(model_dir, 'saved_model.pb')\r\n> print(model)\r\n> import_to_tensorboard(model_dir=model, log_dir='./logs/')\r\n> ```\r\n> \r\n> I see this error:\r\n> \r\n> ```\r\n> ~$ python main.py \r\n> 2020-03-06 23:14:33.962070: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n> 2020-03-06 23:14:33.970833: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200075000 Hz\r\n> 2020-03-06 23:14:33.972744: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563e6e9a9f60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-03-06 23:14:33.972774: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n> Traceback (most recent call last):\r\n>   File \"main4.py\", line 7, in <module>\r\n>     import_to_tensorboard(model_dir=model, log_dir='./logs/')\r\n>   File \"/home/mike18/venv/lib/python3.6/site-packages/tensorflow_core/python/tools/import_pb_to_tensorboard.py\", line 58, in import_to_tensorboard\r\n>     graph_def.ParseFromString(f.read())\r\n> google.protobuf.message.DecodeError: Error parsing message\r\n> ```\r\n> \r\n> also why does this code not work with Tensorflow 2?\r\n> https://github.com/tobegit3hub/tfmodel/blob/22c1c48a7049a9cac8e533e317f41697fb85ff7a/tfmodel/savedmodel_analyst.py\r\n> \r\n> when I use your function it works:\r\n> \r\n> ```\r\n> # Copyright 2017 The TensorFlow Authors. All Rights Reserved.\r\n> #\r\n> # Licensed under the Apache License, Version 2.0 (the \"License\");\r\n> # you may not use this file except in compliance with the License.\r\n> # You may obtain a copy of the License at\r\n> #\r\n> #     http://www.apache.org/licenses/LICENSE-2.0\r\n> #\r\n> # Unless required by applicable law or agreed to in writing, software\r\n> # distributed under the License is distributed on an \"AS IS\" BASIS,\r\n> # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n> # See the License for the specific language governing permissions and\r\n> # limitations under the License.\r\n> # ================================\r\n> \"\"\"Imports a protobuf model as a graph in Tensorboard.\"\"\"\r\n> \r\n> from __future__ import absolute_import\r\n> from __future__ import division\r\n> from __future__ import print_function\r\n> \r\n> import argparse\r\n> import sys\r\n> \r\n> from tensorflow.python.client import session\r\n> from tensorflow.python.framework import importer\r\n> from tensorflow.python.framework import ops\r\n> from tensorflow.python.platform import app\r\n> from tensorflow.python.summary import summary\r\n> from tensorflow.python.tools import saved_model_utils\r\n> \r\n> # Try importing TensorRT ops if available\r\n> # TODO(aaroey): ideally we should import everything from contrib, but currently\r\n> # tensorrt module would cause build errors when being imported in\r\n> # tensorflow/contrib/__init__.py. Fix it.\r\n> # pylint: disable=unused-import,g-import-not-at-top,wildcard-import\r\n> try:\r\n>   from tensorflow.contrib.tensorrt.ops.gen_trt_engine_op import *\r\n> except ImportError:\r\n>   pass\r\n> # pylint: enable=unused-import,g-import-not-at-top,wildcard-import\r\n> \r\n> def import_to_tensorboard(model_dir, log_dir, tag_set):\r\n>   \"\"\"View an imported protobuf model (`.pb` file) as a graph in Tensorboard.\r\n> \r\n>   Args:\r\n>     model_dir: The location of the protobuf (`pb`) model to visualize\r\n>     log_dir: The location for the Tensorboard log to begin visualization from.\r\n>     tag_set: Group of tag(s) of the MetaGraphDef to load, in string format,\r\n>         separated by ','. For tag-set contains multiple tags, all tags must be\r\n>         passed in.\r\n> \r\n>   Usage:\r\n>     Call this function with your model location and desired log directory.\r\n>     Launch Tensorboard by pointing it to the log directory.\r\n>     View your imported `.pb` model as a graph.\r\n>   \"\"\"\r\n>   with session.Session(graph=ops.Graph()) as sess:\r\n>     input_graph_def = saved_model_utils.get_meta_graph_def(\r\n>         model_dir, tag_set).graph_def\r\n>     importer.import_graph_def(input_graph_def)\r\n> \r\n>     pb_visual_writer = summary.FileWriter(log_dir)\r\n>     pb_visual_writer.add_graph(sess.graph)\r\n>     print(\"Model Imported. Visualize by running: \"\r\n>           \"tensorboard --logdir={}\".format(log_dir))\r\n> \r\n> \r\n> def main(_):\r\n>   import_to_tensorboard(FLAGS.model_dir, FLAGS.log_dir, FLAGS.tag_set)\r\n> \r\n> if __name__ == \"__main__\":\r\n>   parser = argparse.ArgumentParser()\r\n>   parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\r\n>   parser.add_argument(\r\n>       \"--model_dir\",\r\n>       type=str,\r\n>       default=\"\",\r\n>       required=True,\r\n>       help=\"The directory containing the SavedModel to import.\")\r\n>   parser.add_argument(\r\n>       \"--log_dir\",\r\n>       type=str,\r\n>       default=\"\",\r\n>       required=True,\r\n>       help=\"The location for the Tensorboard log to begin visualization from.\")\r\n>   parser.add_argument(\r\n>       '--tag_set',\r\n>       type=str,\r\n>       default=\"serve\",\r\n>       required=False,\r\n>       help='tag-set of graph in SavedModel to load, separated by \\',\\'')\r\n>   FLAGS, unparsed = parser.parse_known_args()\r\n>   app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n> ```\r\n> \r\n> and than run:\r\n> \r\n> ```\r\n> $ python import_pb_to_tensorboard.py --model_dir ./model --log_dir logs/\r\n> 2020-03-06 23:23:07.019262: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n> 2020-03-06 23:23:07.027290: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200075000 Hz\r\n> 2020-03-06 23:23:07.029262: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557ede9a02e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-03-06 23:23:07.029284: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n> Model Imported. Visualize by running: tensorboard --logdir=logs/\r\n> ```\r\nWorks for me.\r\n", "> Here's a handy Colab notebook in case anyone wants to try out [this](https://github.com/tensorflow/tensorflow/pull/33954): https://colab.research.google.com/drive/13LAUUT9tEH2XeoNA_z9A7uE5omc-lzNv\r\n\r\nI tried the coloab notebook but the graph in tensorboard is not showing anything, is it working for you?", "This method gets me the logs needed to visualise my graph in tensorboard, however i can't locate my output nodes.\r\n\r\nDoes anyone know how to find out the output node name from the graph that we have created?", "> This method gets me the logs needed to visualise my graph in tensorboard, however i can't locate my output nodes.\r\n> \r\n> Does anyone know how to find out the output node name from the graph that we have created?\r\n\r\nIt's been a while since I've dealt with this but I think it goes bottom to top so your outputs should be near the top, probably labelled as output, or whatever you've named them as. I would ask this on s/o or look at tb docs rather than here.\r\n\r\n## New Readers/Visitors\r\nPlease note that my original solution was thrown together to temporarily answer the issue to help the user. I then PR'd a helper function into tf where much smarter people have taken it over and seemingly converted for 2.0. I've been fairly dormant on TF now too. Judging by the reactions on my original solution this is clearly a high traffic issue, but please ensure you checkout the latest version of https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/import_pb_to_tensorboard.py\r\n\r\nYou may be best off raising a new issue with that code explicitly (or other tb bits. Tb has become a lot more sophisticated since the early days when I was still working with tf regularly!). *Do reference this issue still so those who land here can link over to your issue and find a better solution*.\r\n\r\nAlso note that this method is most likely superseded by more accessible/superior ways of loading your model into TB. I've not actually ever needed to use my own helper function, especially if you are using the Keras functionality, so I would recommend that you take some time to get your TB integration working in the best, native way possible so you can get this most out of it. I'll keep an eye on this to help where I can. Cheers", "> Plot a graph from just a saved_model.pb file.\r\n> \r\n> Currently tensorboard only works given a training folder containing checkpoints and summary events. Understanding the output graph is important, especially if you don't have access to the training output files.\r\n\r\nbut how can we plot the graph?", "If someone needs a 2022 update of the script above, works on tf 2.6.0.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import session\r\nfrom tensorflow.compat.v1 import GraphDef, Graph, summary\r\nfrom tensorflow.python.platform import gfile\r\n\r\nMODEL_FILENAME = \"MY_MODEL.pb\"  # change me\r\nLOGDIR = \"MY_OUTPUT_LOGDIR/\"\r\n\r\nwith session.Session() as sess:\r\n    with gfile.FastGFile(MODEL_FILENAME, 'rb') as f:\r\n        graph_def = GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        g_in = tf.import_graph_def(graph_def)\r\n\r\nwith Graph().as_default():\r\n    train_writer = summary.FileWriter(LOGDIR)\r\n    train_writer.add_graph(sess.graph)\r\n\r\n```\r\n\r\nand after this simply spawn tensorboard via\r\n\r\n```\r\ntensorboard --logdir MY_OUTPUT_LOGDIR\r\n```\r\n\r\nThe included script in tf https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/import_pb_to_tensorboard.py did not recognize my pb file.", "May I suggest you put a pull request in to out this into the project and\nupdate please? I'm not active on TF any more so won't be updating it. Cheers\n\nOn Mon, 28 Mar 2022, 20:32 Dennis Scheiba, ***@***.***> wrote:\n\n> If someone needs a 2022 update of the script above, works on tf 2.6.0.\n>\n> import tensorflow as tffrom tensorflow.python.client import sessionfrom tensorflow.compat.v1 import GraphDef, Graph, summaryfrom tensorflow.python.platform import gfile\n> MODEL_FILENAME = \"MY_MODEL.pb\"  # change meLOGDIR = \"MY_OUTPUT_LOGDIR/\"\n> with session.Session() as sess:\n>     with gfile.FastGFile(MODEL_FILENAME, 'rb') as f:\n>         graph_def = GraphDef()\n>         graph_def.ParseFromString(f.read())\n>         g_in = tf.import_graph_def(graph_def)\n> with Graph().as_default():\n>     train_writer = summary.FileWriter(LOGDIR)\n>     train_writer.add_graph(sess.graph)\n>\n> and after this simply spawn tensorboard via\n>\n> tensorboard --logdir MY_OUTPUT_LOGDIR\n>\n> The included script in tf\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/import_pb_to_tensorboard.py\n> did not recognize my pb file.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8854#issuecomment-1081055916>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AF62KKBGXIXGY4PY7J24D4TVCICMNANCNFSM4DF4BGBQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n"]}, {"number": 8853, "title": "Warning: TensorFlow library wasn't compiled to use SSE instructions", "body": "Installed nightly build using \r\n>pip3 install tensorflow-1.1.0rc0-cp35-cp35m-win_amd64.whl \r\non Windows 7 Professional x64.  Simple run gives warning that the libs were not compiled to use SSE, SSE2, SSE3, SSE4.1, SSE4.2 and AVX instructions even though available. \r\n\r\nc:\\python\r\nPython 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print( tf.__version__)\r\n1.1.0-rc0\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n2017-03-30 15:20:06.126686: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_\r\ntions, but these are available on your machine and could speed up CPU computations.\r\n<<lines ommitted>\r\n>>> print(sess.run(hello))\r\nb'Hello, TensorFlow!'", "comments": ["Duplicate of #7778 ", "This is not a bug.", "import os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\r\nimport tensorflow as tf\r\n\r\nThis will work fine without warning"]}, {"number": 8852, "title": "Allow SavedModelBuilder to overwrite existing directory", "body": "Currently, SavedModelBuilder throws an exception when called with a directory that already exists. It will be helpful to add a flag `overwrite` when initializing SavedModelBuilder, which allows it to overwrite the contents of the directory when .save() is called. \r\n\r\nWhen training a model every couple of hours on fresh data, it will be easier to overwrite existing models than having to implement housekeeping code around cleaning old models or writing code to move around new model after training.", "comments": ["much more agree with you", "We currently don't have anybody working on this. It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!", "I was looking into this, and I think that is cleaner to have one single export dir per trained graph. You can add a time signature to the export dir '<model-id>-<YYYYHHMM>'  and link it to a  \"last model\" directory. Changing the behaviour of the SaveModelBuilder to account for overwrite cases is not worth it.", "We had some internal discussions about this and we vehemently agree with @Mistobaan. We would very much avoid giving users a very sharp stick with which to poke themselves in the eye. We have lost lots of work to setups like this, let's not make this easier.\r\n\r\nIf you must, you can remove the directory before making the SavedModelBuilder.", "Yeah, already following single-export-dir per trained graph. Thanks for the discussion.", "Would still be nice to have an optional overwrite flag though. (Sorry for warming up this old pizza with my 5 cents, btw)"]}, {"number": 8851, "title": "Changed the path to graph directory", "body": "Previously, it was showing \"Could not create TensorFlow Graph: Not found: ~/graphs/inception/tensorflow_inception_graph.pb\". Now, this change ensures that the graph generation will work for everyone smoothly.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I have signed CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "test failed due to an irrelevant issue. Merging."]}, {"number": 8850, "title": "Error in running text_classification_character_rnn.py", "body": "I am running the exact example given in the repo for text classification using rnn. I am getting the following error. \r\nTypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [int32, float32] that don't all match.\r\nExample: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/text_classification_character_rnn.py\r\n\r\nWould you please let me know how can I fix it. \r\n\r\n```\r\nDetail error. \r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-5-90c7a682f760> in <module>()\r\n     23 \r\n     24 # Train and predict\r\n---> 25 classifier.fit(x_train, y_train, steps=100)\r\n     26 y_predicted = [\r\n     27   p['class'] for p in classifier.predict(\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)\r\n    278             _call_location(), decorator_utils.get_qualified_name(func),\r\n    279             func.__module__, arg_name, date, instructions)\r\n--> 280       return func(*args, **kwargs)\r\n    281     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\r\n    282         func.__doc__, date, instructions)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\r\n    408     _verify_input_args(x, y, input_fn, None, batch_size)\r\n    409     if x is not None:\r\n--> 410       SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)\r\n    411       return self\r\n    412 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, batch_size, steps, max_steps, monitors)\r\n   1351                         steps=steps,\r\n   1352                         max_steps=max_steps,\r\n-> 1353                         monitors=all_monitors)\r\n   1354     return self\r\n   1355 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)\r\n    278             _call_location(), decorator_utils.get_qualified_name(func),\r\n    279             func.__module__, arg_name, date, instructions)\r\n--> 280       return func(*args, **kwargs)\r\n    281     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\r\n    282         func.__doc__, date, instructions)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\r\n    424       hooks.append(basic_session_run_hooks.StopAtStepHook(steps, max_steps))\r\n    425 \r\n--> 426     loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n    427     logging.info('Loss for final step: %s.', loss)\r\n    428     return self\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _train_model(self, input_fn, hooks)\r\n    932       features, labels = input_fn()\r\n    933       self._check_inputs(features, labels)\r\n--> 934       model_fn_ops = self._call_legacy_get_train_ops(features, labels)\r\n    935       ops.add_to_collection(ops.GraphKeys.LOSSES, model_fn_ops.loss)\r\n    936       all_hooks.extend([\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_legacy_get_train_ops(self, features, labels)\r\n   1001 \r\n   1002   def _call_legacy_get_train_ops(self, features, labels):\r\n-> 1003     train_ops = self._get_train_ops(features, labels)\r\n   1004     if isinstance(train_ops, model_fn_lib.ModelFnOps):  # Default signature\r\n   1005       return train_ops\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _get_train_ops(self, features, labels)\r\n   1160       `ModelFnOps` object.\r\n   1161     \"\"\"\r\n-> 1162     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n   1163 \r\n   1164   def _get_eval_ops(self, features, labels, metrics):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc in _call_model_fn(self, features, labels, mode)\r\n   1131     if 'model_dir' in model_fn_args:\r\n   1132       kwargs['model_dir'] = self.model_dir\r\n-> 1133     model_fn_results = self._model_fn(features, labels, **kwargs)\r\n   1134 \r\n   1135     if isinstance(model_fn_results, model_fn_lib.ModelFnOps):\r\n\r\n<ipython-input-4-813de742180a> in char_rnn_model(features, target)\r\n      6 \r\n      7     cell = tf.contrib.rnn.GRUCell(HIDDEN_SIZE)\r\n----> 8     _, encoding = tf.contrib.rnn.static_rnn(cell, byte_list, dtype=tf.float32)\r\n      9 \r\n     10     logits = tf.contrib.layers.fully_connected(encoding, 15, activation_fn=None)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.pyc in static_rnn(cell, inputs, initial_state, dtype, sequence_length, scope)\r\n    195             state_size=cell.state_size)\r\n    196       else:\r\n--> 197         (output, state) = call_cell()\r\n    198 \r\n    199       outputs.append(output)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn.pyc in <lambda>()\r\n    182       if time > 0: varscope.reuse_variables()\r\n    183       # pylint: disable=cell-var-from-loop\r\n--> 184       call_cell = lambda: cell(input_, state)\r\n    185       # pylint: enable=cell-var-from-loop\r\n    186       if sequence_length is not None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc in __call__(self, inputs, state, scope)\r\n     90         r, u = array_ops.split(\r\n     91             value=_linear(\r\n---> 92                 [inputs, state], 2 * self._num_units, True, 1.0, scope=scope),\r\n     93             num_or_size_splits=2,\r\n     94             axis=1)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc in _linear(args, output_size, bias, bias_start, scope)\r\n    749       res = math_ops.matmul(args[0], weights)\r\n    750     else:\r\n--> 751       res = math_ops.matmul(array_ops.concat(args, 1), weights)\r\n    752     if not bias:\r\n    753       return res\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.pyc in concat(values, axis, name)\r\n   1032   return gen_array_ops._concat_v2(values=values,\r\n   1033                                   axis=axis,\r\n-> 1034                                   name=name)\r\n   1035 \r\n   1036 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.pyc in _concat_v2(values, axis, name)\r\n    517   \"\"\"\r\n    518   result = _op_def_lib.apply_op(\"ConcatV2\", values=values, axis=axis,\r\n--> 519                                 name=name)\r\n    520   return result\r\n    521 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\r\n    462                                 (prefix, dtype.name))\r\n    463               else:\r\n--> 464                 raise TypeError(\"%s that don't all match.\" % prefix)\r\n    465             else:\r\n    466               raise TypeError(\"%s that are invalid.\" % prefix)\r\n\r\nTypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [int32, float32] that don't all match.\r\n```\r\n\r\n ", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Thank you for the response. Following are the information you asked for:\r\n\r\nOperating system: GNU/Linux Ubuntu 14.04.5 LTS\r\nArchitecture : x86_64\r\nNVIDIA -Driver Version: 375.39 , GeForce GTX 980\r\nCuda V8.0.61\r\ntensorflow-gpu==1.0.1\r\n\r\nThanks, \r\n\r\n\r\n\r\n", "Can you see if there is a second python back trace that indicates where the op was created (it should have such a label). This is the runtime backtrace which is not useful for finding where in the original source the problem arose. Thanks!", "I can reproduce this. A simple fix but not complete one is here\r\nhttps://github.com/aselle/tensorflow/commit/163fd843b787e43f085383eca91dc8dd68d78809\r\nAfter applying it, it works fine for me (basically just need to make the byte_list  float32 type instead of int32 type).\r\n\r\nThere are several uses of deprecated functions in that example. Not sure if @nealwu is planning to fix this as part of moving examples out of TensorFlow, or if @martinwicke has a better suggestion.\r\n\r\n", "I don't have any immediate plans to move this. It may make sense to do so at some point in the future though.", "Thanks! after fixing, sample code works with warning. ", "@martinwicke, who would be best to fix these examples so they don't use the wrong apis. This is in the main tensorflow repo, not even models, so it would be good it if were canonical.", "@jamieas, this is an old character RNN demo -- would you be up for adapting it to use your RNN classes?", "Hi Martin.  Rui from my team is going to take a look at this issue.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Ought to be fixed. Please reopen if not."]}, {"number": 8849, "title": "Upgraded libxsmm to version 1.8", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "@benoitsteiner Are all the CLAs by the commit authors verified?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 8848, "title": "How to have a \"static\" like variable inside a function", "body": "The StackOverflow website does not work for logging in. Sorry, I have to post my question here.\r\n\r\nI want a simple test. A simple function F(x), inside which mean and variance are calculated from the input x. Meanwhile, I also want to keep two \"static\" like variables (like in C) avg_mean, avg_variance. So that every time F is called avg_mean and avg_variance are updated based on their previous values. \r\n\r\nAlso, I want to have two sets of \"avg_mean, avg_variance\" for different scopes. My test codes are as below, but the avg_mean and avg_variance are only the values calculated from the last call and does not include the influence from the first call. If I remove the two \"reuse_variables()\" lines, the program does not run.\r\n\r\nCould anyone help what should I do? By the way, please help withOUT using tf.contrib libs please, because those are not fully supported in Windows now. Thank you.\r\n\r\nimport tensorflow as tf\r\n\r\ndef getsta(x):\r\n  print('getsta start...')\r\n  params_shape = [x.get_shape()[-1]]\r\n\r\n  decay=0.9\r\n  \r\n  mean = tf.get_variable(\r\n          'mean', [1], tf.float32,\r\n          initializer=tf.constant_initializer(0.0, tf.float32))\r\n  variance = tf.get_variable(\r\n          'howvariance', [1], tf.float32,\r\n          initializer=tf.constant_initializer(1.0, tf.float32))\r\n\r\n  avg_mean = tf.get_variable(\r\n          'avg_mean', [1], tf.float32,\r\n          initializer=tf.constant_initializer(0.0, tf.float32))\r\n  avg_variance = tf.get_variable(\r\n          'avg_variance', [1], tf.float32,\r\n          initializer=tf.constant_initializer(0.0, tf.float32))\r\n\r\n  mean, variance = tf.nn.moments(x, [0], name='moments')\r\n\r\n  avg_mean -= (1.0 - decay) * (avg_mean - mean)\r\n  avg_variance -= (1.0 - decay) * (avg_variance - variance)\r\n\r\n  return x, mean, variance, avg_mean, avg_variance\r\n\r\ndef main(argv=None):\r\n  x1 = tf.constant([1,2,3,4], tf.float32)\r\n  x2 = tf.constant([5,6,7,8], tf.float32)\r\n  x3 = tf.constant([1,3,5,7], tf.float32)\r\n  x4 = tf.constant([4,8,12,16], tf.float32)\r\n\r\n  with tf.variable_scope(\"AAA\") as scopeA:\r\n    y1, mean1, variance1, avg_mean1, avg_variance1 = getsta(x1)\r\n    scopeA.reuse_variables()\r\n    y1, mean1, variance1, avg_mean1, avg_variance1 = getsta(x2)\r\n  with tf.variable_scope(\"BBB\") as scopeB:\r\n    y2, mean2, variance2, avg_mean2, avg_variance2 = getsta(x3)\r\n    scopeB.reuse_variables()\r\n    y2, mean2, variance2, avg_mean2, avg_variance2 = getsta(x4)\r\n\r\n  sess = tf.InteractiveSession()\r\n  sess.run(tf.global_variables_initializer())\r\n  print(sess.run([y1, mean1, variance1, avg_mean1, avg_variance1]))\r\n  print(sess.run([y2, mean2, variance2, avg_mean2, avg_variance2]))\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run(main=main)\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nBut since you told me stackoverflow doesn't work, it seems like  a normal variable would be sufficient for your purposes. There aren't transient activation records like an imperative language where auto allocated and instantiated variables are created and destroyed with function overheads. Please read the documentation to get a feeling for how graph base computation works.", "Thank you aselle. I have posted it on StackOverflow now; I found that StackOverflow may not be able to submit questions in Windows' 10 system."]}, {"number": 8847, "title": "cannot run test_session on warpctc", "body": "python 3.5 ;  gtx1080*2\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'CTCLoss': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: CTCLoss = CTCLoss[_kernel=\"WarpCTC\", ctc_merge_repeated=true, preprocess_collapse_repeated=false, _device=\"/device:GPU:0\"](Const_3, Const, Const_1, CTCLoss/sequence_length)]]\r\n```\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN:  8.0.44 5\r\n\r\n1. A link to the pip package you installed: 9.0.1\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 1.0.1 \r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nhttps://github.com/baidu-research/warp-ctc/blob/master/tensorflow_binding/tests/test_ctc_loss_op.py\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nafter some search, I flip the flag to  `allow_soft_placement=True`, however I get\r\n```\r\nanaconda3/lib/python3.5/site-packages/tensorflow/python/framework/test_util.py\", line 248, in prepare_config\r\n    config.allow_soft_placement = False\r\nAttributeError: 'NoneType' object has no attribute 'allow_soft_placement'\r\n```\r\n\r\nAnd if I replace `self.test_session` to `tf.Session` and remove the `use_gpu` and `force_gpu` option\r\nthe error will disappear, don't know what happend", "comments": ["Since this is a third party kernel and tool, please file this bug on warp-ctc's issue tracker."]}, {"number": 8846, "title": "Crashing when trying distributed implementation", "body": "Hi, I start saying that I'm a new Tensorflow user :-)\r\n\r\nI was trying to test the potential speed-up due to a distributed implementation vs. a sequential one, so I came up with the following:\r\n\r\n1. A script sequentially computing matmul of two matrices with themselves and then adding them up\r\n2. A script that distributes the two computations to different devices and then sum them up in a third device\r\n\r\nThe problem is the following: if I run the code below with matrices' dimensions of 100x100, it works. If I run the same with dimensions 1000x1000, the following error occurs:\r\n\r\n> tensorflow.python.framework.errors_impl.InternalError: {\"created\":\"@1490890419.097000000\",\"description\":\"RST_STREAM\",\"file\":\"C:\\tf_jenkins\\home\\workspace\\release-win\\DEVICE\\cpu\\OS\\windows\\cmake_build\\grpc\\src\\grpc\\src\\core\\ext\\transport\\chttp2\\transport\\frame_rst_stream.c\",\"file_line\":107,\"http2_error\":1}\r\n\r\nI can't figure it out, so any help would be appreciated...\r\n\r\nHere is the distributed code (after 3 local Servers have been created):\r\n\r\n```\r\nimport tensorflow as tf\r\nimport time\r\n\r\ncluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\", \"localhost:2224\"]})\r\n\r\nwith tf.device(\"/job:local/task:1\"):\r\n    print('task 1')\r\n    x1 = tf.Variable(tf.zeros([1000,1000])+10, dtype=tf.float32, name=None)\r\n    _matmul_1 = tf.matmul(x1,x1)\r\n\r\nwith tf.device(\"/job:local/task:0\"):\r\n    print('task 0')\r\n    x0 = tf.Variable(tf.zeros([1000,1000])+10, dtype=tf.float32, name=None)\r\n    _matmul_0 = tf.matmul(x0, x0)\r\n\r\nwith tf.device(\"/job:local/task:2\"):\r\n    print('task 2')\r\n    _matmul_ = tf.add(_matmul_0,_matmul_1)\r\n\r\ntime4 = time.clock()\r\nsess = tf.Session(\"grpc://localhost:2222\")\r\nprint('session')\r\ninit = tf.global_variables_initializer()\r\nsess.run(init)\r\nprint(sess.run(_matmul_))\r\ntime5 = time.clock()\r\nprint(time5-time4)\r\n```", "comments": ["Since this is your custom code, it is much more likely a stackoverflow question. Start by using some of our example distributed code\r\nhttps://www.tensorflow.org/deploy/distributed\r\nto make sure the basics are working as they should be.", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}]