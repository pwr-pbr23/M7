[{"number": 38295, "title": "tensorflow1.13 add an op which use mkldnn cannot build sucessfully", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, in tensorflow/core/user_ops addop\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows10\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.13\r\n- **Python version**:3.6\r\n- **Bazel version (if compiling from source)**:0.20.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:None\r\n- **GPU model and memory**:None\r\n- **Exact command to reproduce**:\r\nI just add an matmul op which use a function in mkldnn,and I can build successfully with `bazel build  -c opt --copt=-msse4.1 --copt=-msse4.2  tensorflow:libtensorflow.so`  \r\nBut, when I build the target with `bazel build  -c opt --copt=-msse4.1 --copt=-msse4.2   //tensorflow/tools/pip_package:build_pip_package` , I got an error\r\n`ERROR: D:/tf_install/tensorflow_addop/tensorflow/python/BUILD:4057:1: in cmd attribute of genrule rule //tensorflow/python:gen__pywrap_tensorflow_internal.pyd: variable '$<' : more than one input file. Since this rule was created by the macro 'tf_py_wrap_cc', the error might have been caused by the macro implementation in D:/tf_install/tensorflow_addop/tensorflow/tensorflow.bzl:1704:15\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '//tensorflow/python:gen__pywrap_tensorflow_internal.pyd' failed; build aborted`\r\n\r\nI just add some codes in `tensorflow/python/user_ops/user_ops.py`\r\n`@tf_export(v1=['user_ops.own_dnnl_mul'])\r\ndef own_dnnl_mul(arg1, arg2):\r\n  \"\"\"Example of overriding the generated code for an Op.\"\"\"\r\nreturn _gen_user_ops.own_dnnl_mul(arg1, arg2)\r\n`", "comments": ["@wangyunxiaa, Could you provide the standalone code to analyze the issue. Thanks", "Thanks for your reply,\r\n\r\nI just add some codes in \"tensorflow/core/user_ops\"\r\nand changed the BUILD file in \"tensorflow/core\"\r\n```\r\ncc_library(\r\n    name = \"user_ops_op_lib\",\r\n    srcs = glob([\"user_ops/**/*.cc\", \"user_ops/**/*.h\", \"user_ops/**/*.hpp\"]),\r\n    visibility = [\"//visibility:public\"],\r\n    deps = [\":framework\", \":dnnl\", \"mkldnn\", \":iomp5md\"],\r\n    copts = [\"-fexceptions\"],\r\n    linkopts = [\"-fopenmp\"],\r\n    alwayslink = 1,\r\n    )\r\n```\r\nWith above changes, I can build libtensorflow.so successfully.\r\n\r\nBut I want to build a whl to do a test,\r\nI changed the file in \"tensorflow/python/user_ops/user_ops.py\"\r\n\r\n```\r\n@tf_export(v1=['user_ops.fsmn_forward'])\r\ndef fsmn_forward(arg1, arg2, arg3, arg4, arg5, arg6, arg7):\r\n  \"\"\"Example of overriding the generated code for an Op.\"\"\"\r\n  return _gen_user_ops.fsmn_forward(arg1, arg2, arg3, arg4, arg5, arg6, arg7)\r\n\r\n@tf_export(v1=['user_ops.own_dnnl_mul'])\r\ndef own_dnnl_mul(arg1, arg2):\r\n  \"\"\"Example of overriding the generated code for an Op.\"\"\"\r\n  return _gen_user_ops.own_dnnl_mul(arg1, arg2)\r\n\r\n```\r\nI am confused that, the problem was  caused by mkldnn, because, when I add one op which is not dependences on mkl-dnn, I can build .so and .whl, when I add `deps = [\":framework\", \":dnnl\", \"mkldnn\", \":iomp5md\"],` I just can build .so but not .whl\r\nthe error is as floowings:\r\n`ERROR: D:/tf_install/tensorflow_addop/tensorflow/python/BUILD:4057:1: in cmd attribute of genrule rule //tensorflow/python:gen__pywrap_tensorflow_internal.pyd: variable '$<' : more than one input file. Since this rule was created by the macro 'tf_py_wrap_cc', the error might have been caused by the macro implementation in D:/tf_install/tensorflow_addop/tensorflow/tensorflow.bzl:1704:15 ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '//tensorflow/python:gen__pywrap_tensorflow_internal.pyd' failed; build aborted\r\n`", "Sorry,\r\nI have solved this problem by the following way.\r\nAt the beginning,I add the cc_import library as followings:\r\n```\r\ncc_import(\r\n  name = \"mylib\",\r\n  hdrs = [\"mylib.h\"],\r\n  # mylib.lib is a import library for mylib.dll which will be passed to linker\r\n  interface_library = \"mylib.lib\",\r\n  # mylib.dll will be available for runtime\r\n  shared_library = \"mylib.dll\",\r\n)\r\n```\r\nI was always got the error metioned above.\r\nHowever, I changed the  cc_import library to:\r\n```\r\ncc_import(\r\n  name = \"mylib\",\r\n  hdrs = [\"mylib.h\"],\r\n  # mylib.lib is an import library for mylib.dll which will be passed to linker\r\n  interface_library = \"mylib.lib\",\r\n  # mylib.dll is provided by system environment, for example it can be found in PATH.\r\n  # This indicates that Bazel is not responsible for making mylib.dll available.\r\n  system_provided = 1,\r\n)\r\n```\r\nThen, I could build the .whl successfully.\r\nBut I donnot know the difference between `Linking a shared library with interface library (Windows)` and  `Linking a shared library with system_provided=True (Windows)` \r\n\r\nBest Regards", "@wangyunxiaa Have the above solution resolved your issue? If yes, could you close it?", "@wangyunxiaa \r\n\r\nCould you feedback?", "@wangyunxiaa \r\n\r\nIf your issue is fixed, could you close this issue?", "@wangyunxiaa \r\nCould you feedback?\r\n\r\nThank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38295\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38295\">No</a>\n", "Closing as resolved"]}, {"number": 38294, "title": "Tensorflow keras.callbacks.ModelCheckPoint with wrong path doesn't save anything instead of raising error", "body": "**System information** \r\n- Have I written custom code: NO\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- Mobile device: N/A\r\n- TensorFlow installed from: `pip install --upgrade tf-nightly`\r\n- TensorFlow version: 2.2.0-dev20200403\r\n- Python version: 3.7.7\r\n- CUDA version: cuda 10.1\r\n- GPU model and memory: NVIDIA 2080 and TITAN X\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint('Output/task1/Model/Task1_ResNet50_ImageNet/Task1_ResNet50-{epoch:02d}-{val_loss:.2f}.h5', verbose=1, save_best_only=True)\r\n```\r\n**The path here in ModelCheckpoint is a wrong path since there is NO sub-folder named \"Task1_ResNet50_ImageNet\" in \"Model\" folder. While after 30 epochs training, there is no error raised, neither model saved.**\r\n\r\n**Describe the expected behavior**\r\nIt was supposed to raise error since there is no such a directory to save modelcheckpoint. While after 3 days training, I got nothing saved! It wastes me lots of time.\r\n\r\n**Standalone code to reproduce the issue** \r\nSince my own code is too complicated, I use the example code from [Keras.io](https://keras.io/getting-started/sequential-model-guide/#multilayer-perceptron-mlp-for-multi-class-softmax-classification) to explain this problem: [stand alone gist](https://colab.research.google.com/gist/MakeCent/5ac59661a91e1d5126b89e6b0026e1c0/38294.ipynb)\r\n\r\n**Other info / logs**\r\nDuring the training, it print save model information like below, which looks like working well:\r\n`Epoch 00008: val_loss improved from 9.19709 to 8.58308, saving model to Output/task1/Model/Task1_ResNet50_ImageNet/Task1_ResNet50_ImageNet-08-8.58.h5`\r\n", "comments": ["I have also faced the same issue.", "@MakeCent \r\nplease provide us with complete standalone code for us to replicate the issue faced.", "> @MakeCent\r\n> please provide us with complete standalone code for us to replicate the issue faced.\r\n\r\n@Saduf2019 I have updated the standalone code based on an example code from [Keras.io ](https://keras.io/getting-started/sequential-model-guide/#multilayer-perceptron-mlp-for-multi-class-softmax-classification)", "@MakeCent\r\nplease share simple stand alone code for us to replicate the issue, if possible please share a colab gist of the code and issue faced.", "@Saduf2019 But the stand alone code I had provided in this issue can already replicate the issue.", "@MakeCent \r\ni have made the required change in gist shared, now the code works fine, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/adf8644fb73b495d48c5bcfee2f9e070/38294.ipynb)\r\n\r\nimport tensorflow.keras instead import keras, please confirm if this resolves your issue.", "@Saduf2019 \r\nIt seems it only appear in tf-nightly. Please check the [code](https://colab.research.google.com/gist/MakeCent/5ac59661a91e1d5126b89e6b0026e1c0/38294.ipynb) here. ", "**I want to save Model as XXX.ckpt. But when I set \"save_weights_only=False\":** \r\n`cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_models/test.ckpt,  save_weights_only=False, verbose=0,mode='auto')`\r\n`model.fit(x_train, y_train, epochs=5,callbacks=[cp_callback])`\r\n\r\n**It raise an error::**\r\n`tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: ckpt_models/test.ckpt\\variables; No such file or directory`\r\n\r\nWhen I set \"save_weights_only=True\",everything Ok, but no XXX.ckpt.meta file saved.\r\n", "please find the gist of the [issue here](https://colab.sandbox.google.com/gist/Saduf2019/f922b54fd9c5d41bcd4c5568216bb83a/untitled131.ipynb)", "@ZimmerZheng  \r\n[Gist](https://colab.research.google.com/gist/MakeCent/6b82f91978ced9b32a07b7401b7e355c/38294.ipynb) I test your code on tensorflow 2.1, but no error raised and every ckpt files saved  correctly. Maybe you need check if it's related to your tf version. By the way, if you want to save whole model in addition to weights, you'd better to use HDM5(.h5) or SaveModel(no extension) instead of ckpt. ", "@Saduf2019 @MakeCent \r\nThank everyone. I have solved problem. Error was caused by save path have both '/' and '\\\\'. Tensorflow attach dir with '\\\\' automatically.\r\nIf I only use \u2018\\\\\u2019 in path, the problem gone.\r\nI guess it raleted to OS or enviroment.\r\nMy enviroment: WIN10_X64, PYTHON 3.6, TF2.1. ", "@MakeCent\r\nplease let us know if you are still facing the issue", "@MakeCent\r\nIs this still an issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38294\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38294\">No</a>\n"]}, {"number": 38293, "title": "Tensorflow 2.0 make_csv_dataset reads files too slowly", "body": "It takes 10 seconds to read a batch of 1024 data from the csv file. After setting the num_parallel_reads parameter to 2, the time becomes 5 seconds. But continue to increase the reading thread, the time will not be reduced. Am I setting something wrong?", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\nMake sure you also include the code snippet to reproduce the issue in our environment. It helps us in localizing the issue faster. If you are unclear what to include see the issue template displayed in the [Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Thanks!\r\n\r\n", "The tensorflow version is 2.0.0(gpu).\r\nThe system information is as follows\uff1a\r\ncat /etc/redhat-release\r\nCentOS Linux release 7.6.1810 (Core)", "@wkzqn \r\n\r\nWill it be possible to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@wkzqn \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 38292, "title": "r1.15 cherry-pick: solve memory leak due to defer host callbacks", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nThis is a cherry-pick pr that merge commit 80851c0ad to r1.15 to solve the memory leak because of the accumulated defered callbacks.\r\n\r\nThe two merge conlicts are in\r\n- `BUILD`, where the `XLA_OPS_DEPS` does not exist in r1.15\r\n- `cudnn_batchnorm_thunk.cc`, where the `stream` is an object instead of a pointer.\r\n\r\n@hawkinsp Thank you for your commit to solve this memory leak problem! Could you please have a look at this cherry-pick?\r\n\r\nThank you for your time on reviewing this PR.", "comments": ["Thank you for the cherry-pick.\r\n\r\nNote that we will only merge this when we do a patch release since we need to change CI infrastructure to build the old branch. See similarly labeled PRs against r1.15. ", "FYI, this cherry-pick relies on c8dc5d6d533 by using `down_cast` and this can be resolved by substitute the `down_cast` to `static_cast`.", "```\r\ntensorflow/compiler/jit/kernels/xla_ops.cc:43:10: fatal error: tensorflow/core/platform/casts.h: No such file or directory\r\n #include \"tensorflow/core/platform/casts.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n```", "Will attempt to cherry-pick this with all the changes needed in #39383", "Closing in favor of #39383"]}, {"number": 38291, "title": "Revert \"[MLIR][XLA] Buffer Assignment\"", "body": "Reverts tensorflow/tensorflow#37212\r\n\r\nThis lacks test coverage, it was merged incorrectly without addressing comments.\r\n\r\n", "comments": ["Can you clarify what is the extra work exactly?\r\n(My main issue will be that it will be harder to assess the coverage of the tests without a revert)", "@joker-eph Waiting for rollback to be approved, submitted, then approved in gerrit, then approved again. Then doing the same for the next PR with added tests. I think it is a waste of time.", "How are you gonna check the test coverage?\r\n\r\n> Then doing the same for the next PR with added tests. \r\n\r\nYou have to do this anyway, there is only the approval of the revert that is extra.", "@joker-eph Do we need to? I think it is pretty clear what tests have to be added.", "@joker-eph Can you please resolve conflicts? Thanks!\r\n", "This revert is no longer needed."]}, {"number": 38290, "title": "Use GemmStridedBatched in batch matmul kernels", "body": "When broadcasting is not required or is trivial, the strided batched gemm API can be used instead of the more general batched version.\r\nThis improves performance especially for small matrices due to lower overheads.\r\n\r\nLet me know if there are any benchmarks I should run related to this.\r\n\r\ncc @nluehr ", "comments": ["Thanks @benbarsdell for the PR! \r\nYou can run the benchmarks here to measure performance improvement:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/batch_matmul_op_test.cc"]}, {"number": 38289, "title": "[ROCm] Adding no_rocm tag to CSB tests currently failing on the ROCm platform", "body": "/cc @whchung  @cheshire @chsigg @nvining-work ", "comments": ["@gbaned gentle ping", "@gbaned gentle ping", "@gbaned gentle ping\r\n\r\nMerging this PR will get the ROCm CSB passing again...let me know if there is anything we can do to expedite this", "@deven-amd: Apologies, the PR was missing one approval internally. It should be landing shortly now."]}, {"number": 38288, "title": "Add uint16, uint32, uint64 support for tf.math.equal", "body": "\r\nThis PR tries to address the issue raised in https://github.com/tensorflow/tensorflow/issues/26069#issuecomment-604608722 where\r\ntf.math.equal does not suport basic data types such as\r\nuint16, uint32, and uint64.\r\n\r\nWhile there might be some restrictions on comparision (e.g. >, <, etc)\r\nfor certain data types due to CPU or GPU, the comparision\r\nof basic data types such as uint16, uint32, uint64 are very much\r\nsimple operation across the board. They are important in many\r\nops as well.\r\n\r\nFor that reason, it makes sense to make sure at least all basic\r\ndata types support `equal`.\r\n\r\nThis PR adds the missing uint16, uint32, uint64 support for tf.math.equal\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@mihaimaruseac The `import/copybara` is still in Pending state, wondering if there is anything else that need to be done?", "I'll manually import"]}, {"number": 38287, "title": "Embedding layer ~10x slower in tf.function", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Standard colab environment\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): tf-nightly pip\r\n- Python version: - Bazel\r\nversion (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from\r\nsource): N/A\r\n- CUDA/cuDNN version: - GPU model and memory: N/A\r\n\r\nThis is a follow-up to https://github.com/tensorflow/tensorflow/issues/29075#issuecomment-608976001, to capture a separate issue identified by @szc11121.\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nimport time\r\nimport tensorflow as tf\r\ntf.__version__\r\n\r\nclass Toymodel(tf.keras.Model):\r\n    def __init__(self, use_embedding):\r\n        super(Toymodel, self).__init__()\r\n        if use_embedding:\r\n          self.emb = tf.keras.layers.Embedding(100000, 512)\r\n        self.use_embedding = use_embedding\r\n        self.fc = tf.keras.layers.Dense(1)\r\n\r\n    def call(self, constant_input):\r\n        if self.use_embedding:\r\n          constant_input_emb = self.emb(constant_input)\r\n        else:\r\n          constant_input_emb = tf.expand_dims(constant_input, -1)\r\n        logit = self.fc(constant_input_emb)\r\n        output = tf.keras.activations.sigmoid(logit)\r\n        return logit, output\r\n\r\ndef run_step(model, optimizer, constant_input, y):\r\n    with tf.GradientTape() as tape:\r\n        logit, output = model(constant_input)\r\n        loss = tf.reduce_mean(\r\n                        tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\r\n                                                                labels=tf.cast(y, dtype=tf.float32)))\r\n    gradient = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradient, model.trainable_weights))\r\n    return loss, output\r\n\r\ndef run_loop(in_graph, use_embedding):\r\n  model = Toymodel(use_embedding)\r\n  opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.0)\r\n\r\n  if in_graph:\r\n    f = tf.function(run_step)\r\n  else:\r\n    f = run_step\r\n\r\n  for _ in range(20):\r\n      start = time.time()\r\n      loss, output  = f(model, opt, tf.random.uniform([100], minval=0, maxval=100000), tf.random.uniform([100, 1]))\r\n      end = time.time()\r\n      print(end - start)\r\n\r\nprint('Eager, no embedding')\r\nrun_loop(False, False)\r\nprint('Graph, no embedding')\r\nrun_loop(True, False)\r\nprint('Eager, embedding')\r\nrun_loop(False, True)\r\nprint('Graph, embedding')\r\nrun_loop(True, True)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe graph versions sound be in faster in both cases (ignoring initial calls). However, the case which includes an embedding layer shows a 10x performance drop compared to the same model running eagerly:\r\n\r\nEager, no embedding\r\n0.010893821716308594\r\n0.0032660961151123047\r\n...\r\nGraph, no embedding\r\n0.34873294830322266\r\n0.0013270378112792969\r\n...\r\nEager, embedding\r\n0.4676947593688965\r\n0.006254434585571289\r\n...\r\nGraph, embedding\r\n0.5206460952758789\r\n0.07123064994812012\r\n...", "comments": ["Was able to reproduce the issue with [TF v2.2.0rc2](https://colab.research.google.com/gist/amahendrakar/6b3d4d3899eff968778f67463ae03da7/38287-tf2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/5e0e28711de26d660cfe72d6da17d9d9/38287.ipynb). Please find the attached gist. Thanks!", "any update? ", "@omalleyt12 @fchollet ", "hi, please update and try to fix this issue. There is some people report this bug recently. See (https://github.com/tensorflow/tensorflow/issues/42475). This bug cause slow training on language, signal model, please fix that. @mdanatg @omalleyt12 @fchollet ", "Looks like this was resolved. Please check the performance of the four cases below. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/ea9832ac0b481990a849790f547ed40f/38287.ipynb). Thanks!\r\n\r\n```\r\n> Eager, no embedding\r\n> 0.13755321502685547\r\n> 0.005505561828613281\r\n> ------\r\n> \r\n> Graph, no embedding\r\n> 0.30103421211242676\r\n> 0.0021209716796875\r\n> ------\r\n> Eager, embedding\r\n> 0.5504169464111328\r\n> 0.006941795349121094\r\n> ---\r\n> Graph, embedding\r\n> 0.5892281532287598\r\n> 0.002460002899169922\r\n> ---\r\n```\r\nPerformance of graph mode is better than eager model as shown above. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Looks like this was resolved. Please check the performance of the four cases below. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/ea9832ac0b481990a849790f547ed40f/38287.ipynb). Thanks!\r\n> \r\n> ```\r\n> > Eager, no embedding\r\n> > 0.13755321502685547\r\n> > 0.005505561828613281\r\n> > ------\r\n> > \r\n> > Graph, no embedding\r\n> > 0.30103421211242676\r\n> > 0.0021209716796875\r\n> > ------\r\n> > Eager, embedding\r\n> > 0.5504169464111328\r\n> > 0.006941795349121094\r\n> > ---\r\n> > Graph, embedding\r\n> > 0.5892281532287598\r\n> > 0.002460002899169922\r\n> > ---\r\n> ```\r\n> \r\n> Performance of graph mode is better than eager model as shown above. Thanks!\r\n\r\nwhen will this fix release? since its tf2.4.1 now and tf2.6 seems still has a long way to go", "@szc11121 The following are the results with `TF2.5rc0`. May be there is a `2.5rc1` and `2.5rc2` then stable `TF2.5` (near future). If you want to use stable version, then in near future `TF2.5` will be released. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/9b2f28fc264efd38bdb1cc323d61e84c/38287.ipynb). Thanks!\r\n```\r\n> Eager, no embedding\r\n> 0.12328195571899414\r\n> 0.0040400028228759766\r\n> 0.0034770965576171875\r\n\r\n> Graph, no embedding\r\n> 0.283444881439209\r\n> 0.0018429756164550781\r\n> 0.0014486312866210938\r\n\r\n> Eager, embedding\r\n> 0.5126852989196777\r\n> 0.00421595573425293\r\n> 0.004243135452270508\r\n\r\n> Graph, embedding\r\n> 0.5634112358093262\r\n> 0.005776643753051758\r\n> 0.0012359619140625\r\n```", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38286, "title": "Update version numbers for TensorFlow 2.2.0-rc3", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 2 -> 2\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.2.0-rc2\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.2.0rc2\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 38285, "title": "Unsupported Full-Integer TensorFlow Lite models in TF 2", "body": "**Describe the issue**\r\n**In TF2, the full-integer quantized models produced by the TFLite Converter can only have *float* input and output type. This is a blocker for users who require *int8* or *uint8* input and/or output type.**\r\n\r\n**UPDATE**: We now support this workflow.\r\n\r\n**End-to-End Tutorial**: https://colab.sandbox.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb\r\n\r\n**Only TFLite Conversion: Convert TF Models to TFLite Full-Integer models**\r\nYou can refer to the code [here](https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only), also given below:\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ndef representative_dataset_gen():\r\n  for _ in range(num_calibration_steps):\r\n    # Get sample input data as a numpy array in a method of your choosing.\r\n    yield [input]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8  # or tf.uint8\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**Only TFLite Inference: Run inference on the TFLite model**\r\n**Note** that the one caveat with integer-only models is this -- you need to manually map (aka quantize) the float inputs to integer inputs during inference. To understand how this can be done -- refer to the equation provided in [TensorFlow Lite 8-bit quantization specification](https://www.tensorflow.org/lite/performance/quantization_spec) document and it's equivalent code in python below:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Input to the TF model are float values in the range [0, 10] and of size (1, 100)\r\nnp.random.seed(0)\r\ntf_input = np.random.uniform(low=0, high=10, size=(1, 100)).astype(np.float32)\r\n\r\n# Output of the TF model.\r\ntf_output = keras_model.predict(input)\r\n\r\n# Output of the TFLite model.\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model) \r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()[0]\r\n# Manually quantize the input from float to integer\r\nscale, zero_point = input_details['quantization']\r\ntflite_integer_input = tf_input / scale + zero_point\r\ntflite_integer_input = tflite_integer_input.astype(input_details['dtype'])\r\ninterpreter.set_tensor(input_details['index'], tflite_integer_input)\r\ninterpreter.invoke()\r\noutput_details = interpreter.get_output_details()[0]\r\ntflite_integer_output = interpreter.get_tensor(output_details['index'])\r\n# Manually dequantize the output from integer to float\r\nscale, zero_point = output_details['quantization']\r\ntflite_output = tflite_integer_output.astype(np.float32)\r\ntflite_output = (tflite_output - zero_point) * scale\r\n \r\n# Verify that the TFLite model's output is approximately (expect some loss in \r\n# accuracy due to quantization) the same as the TF model's output\r\nassert np.allclose(tflite_output, tf_output, atol=1e-04) == True\r\n```\r\n", "comments": ["I am interested in this issue. When fix is available, please let me know. \r\n(I suppose I will get the notify when write message here, right?)\r\n\r\nRuey-An", "@rayeh5 Yes. Whenever we post updates, you would receive emails.", "@MeghnaNatraj: in the meanwhile, could you update https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations and other documentation to reflect that the full-integer path (including inputs and outputs) is only available in TF 1.X and not TF 2.X?", "expecting", "Hello , \r\nAny updates about this issue ? Will v2 converter support input output Flags?", "Yes, it is currently a work in progress. We will update this github issue\nonce it's completed.\n\nOn Thu, Apr 23, 2020 at 8:22 PM mahdichtourou24051994 <\nnotifications@github.com> wrote:\n\n> Hello ,\n> Any updates about this issue ? Will v2 converter support input output\n> Flags?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38285#issuecomment-618780215>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACDCGJ2GOLCDOWIFHCDJ2VDROEAX3ANCNFSM4MCTYA6A>\n> .\n>\n", "Just checking in, im also waiting for v2.", "Also waiting for this issue to be resolved, thanks.", "waiting for this issue to be resolved, thanks.", "waiting too", "I am facing this issue in TF 1.15 and TF 2.", "By the way, can we use converter to create an uint8 inference tflite model rather than an int8 inference model?", "**Note:** *The following discussion is not related to the current issue of supporting full integer tensorflow lite models, including input and output, in TF 2.0*\r\n\r\n@dreamPoet No, this is not possible in TensorFlow 2. We cannot create a uint8 inference tflite model and only support int8 inference model. We've moved away from the uint8 quantization because with int8 we're able to use asymmetrical quantization ranges without paying the same penalty. Refer to [https://www.tensorflow.org/lite/performance/quantization_spec](https://www.tensorflow.org/lite/performance/quantization_spec) for more information\r\n\r\nFor more details:\r\nYoutube: https://www.youtube.com/watch?v=-jBmqY_aFwE\r\nSlides used in the Youtube Video: https://docs.google.com/presentation/d/1zGm5bqGrkAepwJZ5PABiYjrIKq1pDnzafa8ZYeaFhXY/edit?usp=sharing", "@MeghnaNatraj  How about in tf1.x? I can only use toco and lite.converter without representative_dataset attributes to produce uint8 inference tflite model, while for int8 inference model, I can set representative_dataset to help quantize, is that right?", "@dreamPoet Could you create a separate github issue describing your problem and assign it to me? I'd be glad to provide more information on the new issue.", "@MeghnaNatraj Hi, I am working on a benchmark for the Edge TPU and this is blocking me for using it, which is really frustrating. Are you aware of any workaround? Could you provide an estimation of when the converter would be ready? Since this issue wasn't mentioned anywhere I embarked on this project but now I've reached a point where there is nothing else to do but waiting.", "@MeghnaNatraj @stefano555  Excuse me, do your test the speed of the quantified model? I found there was only a slight increase in speed.", "> @MeghnaNatraj @stefano555 Excuse me, do your test the speed of the quantified model? I found there was only a slight increase in speed.\r\n\r\nI would test it, if tf lite allowed me to do that :-) since I cannot get the fully quantized model, I cannot do the benchmark", "**Update**: We now support TensorFlow Lite Full-Integer models in TF 2.0, i.e, with integer (`tf.int8` and `tf.uint8` types) input and output.\r\n*Exception: Support for [quantize-aware trained models](https://www.tensorflow.org/lite/performance/model_optimization) is still in progress* \r\n\r\n**End-to-End Tutorial**: https://colab.sandbox.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb\r\n\r\n**Only TFLite Conversion: Convert TF Models to TFLite Full-Integer models**\r\nYou can refer to the code [here](https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only), also given below:\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ndef representative_dataset_gen():\r\n  for _ in range(num_calibration_steps):\r\n    # Get sample input data as a numpy array in a method of your choosing.\r\n    yield [input]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8  # or tf.uint8\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**Only TFLite Inference: Run inference on the TFLite model**\r\n**Note** that the one caveat with integer-only models is this -- you need to manually map (aka quantize) the float inputs to integer inputs during inference. To understand how this can be done -- refer to the equation provided in [TensorFlow Lite 8-bit quantization specification](https://www.tensorflow.org/lite/performance/quantization_spec) document and it's equivalent code in python below:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Input to the TF model are float values in the range [0, 10] and of size (1, 100)\r\nnp.random.seed(0)\r\ntf_input = np.random.uniform(low=0, high=10, size=(1, 100)).astype(np.float32)\r\n\r\n# Output of the TF model.\r\ntf_output = keras_model.predict(input)\r\n\r\n# Output of the TFLite model.\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model) \r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()[0]\r\n# Manually quantize the input from float to integer\r\nscale, zero_point = input_details['quantization']\r\ntflite_integer_input = tf_input / scale + zero_point\r\ntflite_integer_input = tflite_integer_input.astype(input_details['dtype'])\r\ninterpreter.set_tensor(input_details['index'], tflite_integer_input)\r\ninterpreter.invoke()\r\noutput_details = interpreter.get_output_details()[0]\r\ntflite_integer_output = interpreter.get_tensor(output_details['index'])\r\n# Manually dequantize the output from integer to float\r\nscale, zero_point = output_details['quantization']\r\ntflite_output = tflite_integer_output.astype(np.float32)\r\ntflite_output = (tflite_output - zero_point) * scale\r\n \r\n# Verify that the TFLite model's output is approximately (expect some loss in \r\n# accuracy due to quantization) the same as the TF model's output\r\nassert np.allclose(tflite_output, tf_output, atol=1e-04) == True\r\n```\r\n", "FYI, you can try this TF2 API for full-integer post-training quantization here:\r\nhttps://colab.sandbox.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb", "@scottamain @MeghnaNatraj Hi,\r\nI've trained a resnet50v2 model. TensorFlow nightly 2.3.0-dev20200608. The model works fine and I tried some optimization such as \"simple\" tf lite, tf lite dynamic range, tf lite 16float, and they all work fine. \r\nI have updated today tf to use full-integer post-training quantization with uint8. I converted my model from SavedModel format with:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/path/to/my/saved_models')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ndef representative_dataset_gen():\r\n    for i in range(100):\r\n        yield [x_train[i].astype(np.float32)]\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model = converter.convert()\r\n\r\nwith open('resnet.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\nI then compiled the model for edge tpu. It works, in the sense that edge tpu allows me to run it without errors but results are gibberish. It predicts always the same value. I tried then on cpu with tf lite interpreter. Input/output tensor are correctly uint8, but again it predicts again the same value. On cpu wiht tf lite issue persists moving to int8.\r\n\r\nIs anyone else experiencing the same issue?", "Hey @stefano555 \r\nYou said your input tensors are uint8, and that was my first concern. So I don't know what else it might be. But to be sure, can you verify that your results are accurate when you use float input/output (remove the inference_input_type/inference_output_type lines)?\r\n\r\nIf this model seems to be accurate, you can then refer to the \"Compare the accuracy\" section in [retrain_classification_ptq_tf2.ipynb](https://colab.sandbox.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb) to compare the TF model and TFLite full-integer post-training quantization with uint8 input and output.\r\n\r\nIf nothing works, is it possible to share the end-end process (TF saved model to TFLite model conversion and TFLite model inference code using a [google colab notebook](https://colab.research.google.com/notebooks/intro.ipynb#recent=true). You can attach the shared model so we can debug this on our end.", "Hi @scottamain ,\r\n\r\nplease find here a link to a google folder containing the tf trained model (validation accuracy 64.15% on 2000 test images), the dataset, the google colab file to convert from tf to tf_lite, the converted tf lite model, and the google colab file to test the tf lite model on cpu.\r\nI was able to replicate the issue. On a subset of 100 test images (using all the images would have taken hours since tf lite on cpu is extremely slow) I got 40% but only because it kept predicting the same value and in that subset there were by chance 40 pictures corresponding to that value, but it is only gibberish.\r\n\r\nhttps://drive.google.com/drive/folders/11XruNeJzdIm9DTn7FnuIWYaSalqg2F0B?usp=sharing\r\n\r\nMany thanks for your help!", "Hi @MeghnaNatraj and @scottamain I have tested today DenseNet-169 and I got the identical issue with Full-Integer TensorFlow Lite.", "> **Update**: We now support TensorFlow Lite Full-Integer models in TF 2.0, i.e, with integer (`tf.int8` and `tf.uint8` types) input and output.\r\n> _Exception: Support for [quantize-aware trained models](https://www.tensorflow.org/lite/performance/model_optimization) is still in progress_\r\n> \r\n> **End-to-End Tutorial**: https://colab.sandbox.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb\r\n> \r\n> **Only TFLite Conversion: Convert TF Models to TFLite Full-Integer models**\r\n> You can refer to the code [here](https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only), also given below:\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> def representative_dataset_gen():\r\n>   for _ in range(num_calibration_steps):\r\n>     # Get sample input data as a numpy array in a method of your choosing.\r\n>     yield [input]\r\n> converter.representative_dataset = representative_dataset_gen\r\n> converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n> converter.inference_input_type = tf.int8  # or tf.uint8\r\n> converter.inference_output_type = tf.int8  # or tf.uint8\r\n> tflite_model = converter.convert()\r\n> ```\r\n> \r\n> **Only TFLite Inference: Run inference on the TFLite model**\r\n> **Note** that the one caveat with integer-only models is this -- you need to manually map (aka quantize) the float inputs to integer inputs during inference. To understand how this can be done -- refer to the equation provided in [TensorFlow Lite 8-bit quantization specification](https://www.tensorflow.org/lite/performance/quantization_spec) document and it's equivalent code in python below:\r\n> \r\n> ```\r\n> import numpy as np\r\n> import tensorflow as tf\r\n> \r\n> # Input to the TF model are float values in the range [0, 10] and of size (1, 100)\r\n> np.random.seed(0)\r\n> tf_input = np.random.uniform(low=0, high=10, size=(1, 100)).astype(np.float32)\r\n> \r\n> # Output of the TF model.\r\n> tf_output = keras_model.predict(input)\r\n> \r\n> # Output of the TFLite model.\r\n> interpreter = tf.lite.Interpreter(model_content=tflite_model) \r\n> interpreter.allocate_tensors()\r\n> input_details = interpreter.get_input_details()[0]\r\n> # Manually quantize the input from float to integer\r\n> scale, zero_point = input_details['quantization']\r\n> tflite_integer_input = tf_input / scale + zero_point\r\n> tflite_integer_input = tflite_integer_input.astype(input_details['dtype'])\r\n> interpreter.set_tensor(input_details['index'], tflite_integer_input)\r\n> interpreter.invoke()\r\n> output_details = interpreter.get_output_details()[0]\r\n> tflite_integer_output = interpreter.get_tensor(output_details['index'])\r\n> # Manually dequantize the output from integer to float\r\n> scale, zero_point = output_details['quantization']\r\n> tflite_output = tflite_integer_output.astype(np.float32)\r\n> tflite_output = (tflite_output - zero_point) * scale\r\n>  \r\n> # Verify that the TFLite model's output is approximately (expect some loss in \r\n> # accuracy due to quantization) the same as the TF model's output\r\n> assert np.allclose(tflite_output, tf_output, atol=1e-04) == True\r\n> ```\r\n\r\nIs it possible to manually quantize/dequantize the inputs/outputs using the TFLite for Microcontrollers C++ library? Here you're just showing how to do that using the Python API...", "Quantization aware training can not convert to int8 or unint8 ?\r\nthis is my code ,thanks.\r\nhttps://colab.research.google.com/drive/1qPD3Zd5qU5FpMxj_aCwIpjn7y_eR_Wxt?usp=sharing", "@anferico Unfortunately, it looks like we don't have an example for TFLite Interpreter in C++ which uses the quantize/dequantize scale and zero-point to preprocess the image and process the result. We will be updating the TFLite micro examples, but until then, you can try the following suggestion:\r\n\r\nThe closest example is [micro/examples/person_detection_experimental](https://source.corp.google.com/piper///depot/google3/third_party/tensorflow/lite/micro/examples/person_detection_experimental/person_detection_test.cc?q=128&ss=piper%2FGoogle%2FPiper:google3%2Fthird_party%2Ftensorflow%2Flite%2Fmicro%2Fexamples%2Fperson_detection_experimental%2F) -- specifically refer to [this](https://source.corp.google.com/piper///depot/google3/third_party/tensorflow/lite/micro/examples/person_detection_experimental/person_detection_test.cc;l=87?q=128&ss=piper%2FGoogle%2FPiper:google3%2Fthird_party%2Ftensorflow%2Flite%2Fmicro%2Fexamples%2Fperson_detection_experimental%2F) -- a uint8 image input it converter to int8 by subtracting 128 and feeding it into the model. For this example, we noticed that the zero point is -128 and the scale is 1.0 so we directly just subtract 128 to keep the code simple. \r\n\r\nHowever you should be able to access `input->params->zero_point` and `input->params->scale` and write a C++ equivalent of the python TFLite Interpreter code that we have shared.\r\n\r\nHope this helps!\r\n\r\n@aa12356jm  correct, we are still working on adding support for this feature.\r\n", "Following. Two of my blog posts would need to be changed when the int support for QAT models goes live. ", "@sayakpaul Check this issue: https://github.com/tensorflow/model-optimization/issues/431", "Hi @scottamain and @MeghnaNatraj I just wanted to know if there are any updates and if could reproduce my same issue. ", "@MeghnaNatraj @alanchiao   i train a quant model with dynamic fuction, input and output are float. i test two model (float 3.6M and int8 900k) in the android phone. there are no speed accelarate.", "@hahadashi could you confirm if you are using delegate when using the models from your Android phone? ", "> @hahadashi could you confirm if you are using delegate when using the models from your Android phone?\r\nthxs your reply.\r\n\r\n\r\ni just follow the tensorflow/lite/g3doc/guide/inference.md \r\n(```c++\r\n// Load the model\r\nstd::unique_ptr<tflite::FlatBufferModel> model =\r\n    tflite::FlatBufferModel::BuildFromFile(filename);\r\n\r\n// Build the interpreter\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n\r\n// Resize input tensors, if desired.\r\ninterpreter->AllocateTensors();\r\n\r\nfloat* input = interpreter->typed_input_tensor<float>(0);\r\n// Fill `input`.\r\n\r\ninterpreter->Invoke();\r\n\r\nfloat* output = interpreter->typed_output_tensor<float>(0);\r\n```)\r\n\r\ndevelop a inference code to run the tflite on the phone.  i need do more thing?", "You might want to follow this tutorial once: https://www.tensorflow.org/lite/performance/gpu. Here's the demo code snippet:\r\n\r\n![image](https://user-images.githubusercontent.com/22957388/86256183-a17d6800-bbd5-11ea-9af4-924c5119796e.png)\r\n", "> You might want to follow this tutorial once: https://www.tensorflow.org/lite/performance/gpu. Here's the demo code snippet:\r\n> \r\n> ![image](https://user-images.githubusercontent.com/22957388/86256183-a17d6800-bbd5-11ea-9af4-924c5119796e.png)\r\n\r\nthxs, i have a question. why we need use GPU delegate to accelerate speed by gpu. the arm cpu not support quant inference?\r\ni see the tensorflow.org,  it say quant model can speed up 1.5X ~ 4X", "> https://www.youtube.com/watch?v=-jBmqY_aFwE\r\n\r\n@MeghnaNatraj thanks for the great link to Pete Wardens screencast!\r\n\r\n\r\n", "@stefano555 I am working on debugging your issue right now. Looks like https://drive.google.com/corp/drive/folders/11XruNeJzdIm9DTn7FnuIWYaSalqg2F0B has a ton of jupyter notebooks, and i'm not sure how to debug this. Please create a new github issue for \"TFLite Converter\", add me as the assignee and post detailed instructions on how I can reproduce your issue.", "It looks like the issue has been solved in TF 2.4? Could we get an official confirmation?", "@MeghnaNatraj is this solved in TF 2.4 (nightly)? ", "Hello, the issue is resolved now with TF 2.4 (nightly). Let us know if you face any issue. (This issue will remain open until we also fix all documentation, and resolve any issues that may arise in the next few days)", "Dear @MeghnaNatraj, sorry for answering you only now. The issue has not been resolved. I will create a new GitHub issue and add you as an assignee.", "@MeghnaNatraj ran into: `RuntimeError: Quantization not yet supported for op: 'DEQUANTIZE'.` TensorFlow Version: `2.4.0-dev20200728`. Let me know if you'd want me to create a separate issue. \r\n\r\nHere's the [Colab Gist](https://colab.research.google.com/gist/sayakpaul/8c8a1d7c94beca26d93b67d92a90d3f0/qat-bad-accuracy.ipynb). ", "@MeghnaNatraj I wasn't able to assign you to the new issue, you can find it here https://github.com/tensorflow/tensorflow/issues/41840#issue-667409532\r\n", "sorry, I don't know if the question is appropriate to ask here:\r\nit seems that the int8 quantization is a symmetric quantization one, will TF-Lite (or Micro vesion) has plan to support \"symmetric quantization\"?\r\n\r\nthanks for all of your time", "@sayakpaul Could you create a separate issue and tag me on it? I am looking into it. \r\n@stefano555 I am looking into #41840 right now.\r\n\r\n@rayeh5  could you explain your question in detail? Currently we support int8 quantization by default. \r\n\r\n", "@MeghnaNatraj I don't think community members can assign someone to an issue (correct me if I am wrong). I referred to the following statement of yours and hence I decided to inform you about the issue here. \r\n\r\n> (This issue will remain open until we also fix all documentation, and resolve any issues that may arise in the next few days)\r\n\r\nPlease let me know if you'd still like me to open up a new issue. \r\n\r\n", "@sayakpaul Yes, community members cannot assign an issue to a specific person. You can create a new issue, post the details about your issue (as in this [comment](https://github.com/tensorflow/tensorflow/issues/38285#issuecomment-665444152) and post a link to the issue here. I will continue the discussion on that thread.", "@sayakpaul  I have created a separate issue here: https://github.com/tensorflow/tensorflow/issues/42082", "> @sayakpaul Could you create a separate issue and tag me on it? I am looking into it.\r\n> @stefano555 I am looking into #41840 right now.\r\n> \r\n> @rayeh5 could you explain your question in detail? Currently we support int8 quantization by default.\r\n\r\nAs my understanding of the \"TensorFlow Lite 8-bit quantization specification\", Weights are symmetric, but the Activations are asymmetric. Is it possible to support both as symmetric quantization? \r\n\r\nThanks for your time.\r\n\r\nruey-an\r\n", "> > @sayakpaul Could you create a separate issue and tag me on it? I am looking into it.\r\n> > @stefano555 I am looking into #41840 right now.\r\n> > @rayeh5 could you explain your question in detail? Currently we support int8 quantization by default.\r\n> \r\n> As my understanding of the \"TensorFlow Lite 8-bit quantization specification\", Weights are symmetric, but the Activations are asymmetric. Is it possible to support both as symmetric quantization?\r\n> \r\n> Thanks for your time.\r\n> \r\n> ruey-an\r\n\r\n@suharshs could you respond to this question?", "@MeghnaNatraj  hi, i use post quantization as follows:\r\n```\r\n.........\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n```\r\n only input and output  of the quantization model  have the  datatype uint8,  the convolution of the middle layer is still int8.\r\nhow do I get  the model that  is  all uint8?  thanks\r\n", "@aa12356jm This is currently unsupported, you can refer to this [comment](https://github.com/tensorflow/tensorflow/issues/38285#issuecomment-635533037) for more information.", "Closing this issue as it has been resolved in the latest tensorflow version (`tf-nightly`, version >= 2.4.0-dev20200823). The documentation for TensorFlow Lite has been updated. If you face an issue due to this change, either re-open this issue or create a new one and mention this issue (by adding #38285 in a comment)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38285\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38285\">No</a>\n", "> Quantization aware training can not convert to int8 or unint8 ?\r\n> this is my code ,thanks.\r\n> https://colab.research.google.com/drive/1qPD3Zd5qU5FpMxj_aCwIpjn7y_eR_Wxt?usp=sharing\r\n\r\n@MeghnaNatraj  now QAT can convert to int8 or unint8 ?  thanks", "@aa12356jm We only support int8 quantization but the model input/output can be int8 or uint8. Use the latest TF version \r\n`pip install tensorflow` should install the latest stable version", "@MeghnaNatraj say `model` (it's a `tf.keras.Model` instance) is trained using QAT and now I would like to integer-quantize it, will the following be the right way to achieve that? I truly apologize if this is redundant: \r\n\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model_quant = converter.convert()\r\n```\r\n\r\nNo representative dataset is required since the model is already calibrated to adjust to the activation changes. ", "@sayakpaul yes this is perfectly right. ", "@MeghnaNatraj with the above-mentioned code snippet I get the following error, though - \r\n\r\n```\r\nValueError: representative_dataset is required when specifying TFLITE_BUILTINS_INT8 or INT8 supported types.\r\n```\r\n\r\nHere's the [Colab Notebook](https://colab.research.google.com/gist/sayakpaul/1eafb47439c8045466c40de371c6635c/scratchpad.ipynb). ", "Remove the line \r\n``` converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]```\r\nas for QAT you are assuming that the model can be fully quantized.", "Thank you and that worked. Here's my Colab Gist for anyone else that might run into a similar problem: https://colab.research.google.com/gist/sayakpaul/d209ac353d3bcea06287be5e91628f18/scratchpad.ipynb. ", "> **Update**: We now support TensorFlow Lite Full-Integer models in TF 2.0, i.e, with integer (`tf.int8` and `tf.uint8` types) input and output.\r\n> _Exception: Support for [quantize-aware trained models](https://www.tensorflow.org/lite/performance/model_optimization) is still in progress_\r\n> \r\n> **End-to-End Tutorial**: https://colab.sandbox.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb\r\n> \r\n> **Only TFLite Conversion: Convert TF Models to TFLite Full-Integer models**\r\n> You can refer to the code [here](https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only), also given below:\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> def representative_dataset_gen():\r\n>   for _ in range(num_calibration_steps):\r\n>     # Get sample input data as a numpy array in a method of your choosing.\r\n>     yield [input]\r\n> converter.representative_dataset = representative_dataset_gen\r\n> converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n> converter.inference_input_type = tf.int8  # or tf.uint8\r\n> converter.inference_output_type = tf.int8  # or tf.uint8\r\n> tflite_model = converter.convert()\r\n> ```\r\n> \r\n> **Only TFLite Inference: Run inference on the TFLite model**\r\n> **Note** that the one caveat with integer-only models is this -- you need to manually map (aka quantize) the float inputs to integer inputs during inference. To understand how this can be done -- refer to the equation provided in [TensorFlow Lite 8-bit quantization specification](https://www.tensorflow.org/lite/performance/quantization_spec) document and it's equivalent code in python below:\r\n> \r\n> ```\r\n> import numpy as np\r\n> import tensorflow as tf\r\n> \r\n> # Input to the TF model are float values in the range [0, 10] and of size (1, 100)\r\n> np.random.seed(0)\r\n> tf_input = np.random.uniform(low=0, high=10, size=(1, 100)).astype(np.float32)\r\n> \r\n> # Output of the TF model.\r\n> tf_output = keras_model.predict(input)\r\n> \r\n> # Output of the TFLite model.\r\n> interpreter = tf.lite.Interpreter(model_content=tflite_model) \r\n> interpreter.allocate_tensors()\r\n> input_details = interpreter.get_input_details()[0]\r\n> # Manually quantize the input from float to integer\r\n> scale, zero_point = input_details['quantization']\r\n> tflite_integer_input = tf_input / scale + zero_point\r\n> tflite_integer_input = tflite_integer_input.astype(input_details['dtype'])\r\n> interpreter.set_tensor(input_details['index'], tflite_integer_input)\r\n> interpreter.invoke()\r\n> output_details = interpreter.get_output_details()[0]\r\n> tflite_integer_output = interpreter.get_tensor(output_details['index'])\r\n> # Manually dequantize the output from integer to float\r\n> scale, zero_point = output_details['quantization']\r\n> tflite_output = tflite_integer_output.astype(np.float32)\r\n> tflite_output = (tflite_output - zero_point) * scale\r\n>  \r\n> # Verify that the TFLite model's output is approximately (expect some loss in \r\n> # accuracy due to quantization) the same as the TF model's output\r\n> assert np.allclose(tflite_output, tf_output, atol=1e-04) == True\r\n> ```\r\n\r\nGot the error\r\ntensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(callsite(\"Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___21591\" at \"StatefulPartitionedCall@__inference_signature_wrapper_23250\") at \"StatefulPartitionedCall\")): 'tf.Size' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.Size {device = \"\"}\r\n\r\nremoved this error using \r\n**converter.target_spec.supported_ops=[tf.lite.OpsSet.SELECT_TF_OPS]**\r\n\r\nBut got another error with this : \r\nTraceback (most recent call last):\r\n  File \"abc.py\", line 45, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"lite.py\", line 742, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"lite.py\", line 459, in _calibrate_quantize_model\r\n    return calibrate_quantize.calibrate_and_quantize(\r\n  File \"calibrator.py\", line 97, in calibrate_and_quantize\r\n    self._calibrator.Prepare([list(s.shape) for s in sample])\r\n  File \"calibrator.py\", line 97, in <listcomp>\r\n    self._calibrator.Prepare([list(s.shape) for s in sample])\r\nAttributeError: 'list' object has no attribute 'shape'\r\n\r\n\r\n\r\n\r\n\r\n", "@MeghnaNatraj \r\nThis issue was resolved completely?\r\nEven with **TF 2.4.1**, I still have this error:\r\n`RuntimeError: Quantization not yet supported for op: 'CUSTOM'.`\r\n\r\nWithout QAT,\r\n```\r\ndef representative_dataset_gen():\r\n    for f_name in os.listdir(image_path):\r\n       file_path = os.path.normpath(os.path.join(image_path, f_name))\r\n        img = cv2.imread(file_path)\r\n        img = cv2.resize(img, (320,320))\r\n        img = img / 255.0\r\n        img = np.reshape(img, (1, 320, 320, 3))\r\n        image = img.astype(np.float32)\r\n        yield [image]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(args.model)\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\n\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.target_spec.supported_types = [tf.int8]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n\r\n#converter.allow_custom_ops = True\r\n#converter.experimental_new_converter = False\r\n\r\ntflite_model = converter.convert()\r\n```\r\nI cannot find out what's wrong at all.\r\n", "> @MeghnaNatraj\r\n> This issue was resolved completely?\r\n> Even with **TF 2.4.1**, I still have this error:\r\n> `RuntimeError: Quantization not yet supported for op: 'CUSTOM'.`\r\n> \r\n> Without QAT,\r\n> \r\n> ```\r\n> def representative_dataset_gen():\r\n>     for f_name in os.listdir(image_path):\r\n>        file_path = os.path.normpath(os.path.join(image_path, f_name))\r\n>         img = cv2.imread(file_path)\r\n>         img = cv2.resize(img, (320,320))\r\n>         img = img / 255.0\r\n>         img = np.reshape(img, (1, 320, 320, 3))\r\n>         image = img.astype(np.float32)\r\n>         yield [image]\r\n> \r\n> converter = tf.lite.TFLiteConverter.from_saved_model(args.model)\r\n> \r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> converter.representative_dataset = representative_dataset_gen\r\n> \r\n> converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n> converter.target_spec.supported_types = [tf.int8]\r\n> converter.inference_input_type = tf.int8\r\n> converter.inference_output_type = tf.int8\r\n> \r\n> #converter.allow_custom_ops = True\r\n> #converter.experimental_new_converter = False\r\n> \r\n> tflite_model = converter.convert()\r\n> ```\r\n> \r\n> I cannot find out what's wrong at all.\r\n\r\nAs it says in the error, one of your operations called `'CUSTOM'` is not yet supported. Therefore, it's something with your original neural network (probably Keras).\r\nYou might want to check which operands are supported [here](https://www.tensorflow.org/lite/guide/ops_compatibility).", "@lheim Thanks,\r\n\r\nI got a pb file from saved model, as this [link](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html).\r\n\r\nThe error message says 'CUSTOM' is not supported, but I cannot find where it is.", "@sayakpaul\r\n> Thank you and that worked. Here's my Colab Gist for anyone else that might run into a similar problem: https://colab.research.google.com/gist/sayakpaul/d209ac353d3bcea06287be5e91628f18/scratchpad.ipynb.\r\nThanks for your sharing. I meet the same problem and try your script, it doesn't work now. Is it still work for you ?\r\n\r\n"]}, {"number": 38284, "title": "[TFMicro]  Add versions to logistic and max pool", "body": "", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38284) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38284) for more info**.\n\n<!-- ok -->"]}, {"number": 38283, "title": "Fix issue 36146", "body": "Improved error message to clarify why a custom mapping is not being accepted. Fix for issue #36146.", "comments": []}, {"number": 38282, "title": "[r2.2:CherryPick] Adding a missing dependency to version.cc, fixing an undefined_symbol_error in tensorflow_serving.", "body": "PiperOrigin-RevId: 302783112\nChange-Id: I472ec370963ac2d752ea3ccf23e066da3d090da4", "comments": []}, {"number": 38281, "title": "TFlite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@JamhangC \r\nPlease provide details for us to look into the issue, tensorflow version,simple standalone code.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@JamhangC\r\nPlease update as per above comment", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 38279, "title": "MutableGraphView::SortTopologically error with tf.nn.conv2d in custom RNN cell", "body": "**System information**\r\n\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Ubuntu 18.04 LTS\r\n- TensorFlow installed from: binary (`conda install tensorflow-gpu=2.1`)\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.7\r\n- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6\r\n- GPU model and memory: GeForce GTX 1080, 8 GB\r\n\r\n**Describe the current behavior**\r\n\r\nCalling `predict` on a model containing a custom RNN layer whose cell calls `tf.nn.conv2d` results in the following error being printed to the console:\r\n```\r\n2020-04-06 12:40:50.843591: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] layout failed: Invalid argument: MutableGraphView::SortTopologically error: detected edge(s) creating cycle(s) {'Func/sequential/rnn/while/body/_1/input/_55' -> 'sequential/rnn/while/body/_1/add'}.\r\n```\r\nThe prediction proceeds (and appears to be correct), but has poor performance.\r\n\r\nThis does not occur if the `tf.nn.conv2d` is replaced with other similar operations (`tf.nn.conv1d` for example).\r\n\r\n**Describe the expected behavior**\r\n\r\nThe expected behavior is for no error to be produced.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nThe following code produces the error:\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nclass CustomCell(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        self.state_size = [tf.TensorShape((16, 16, 1))]\r\n        super(CustomCell, self).__init__(**kwargs)\r\n\r\n    def call(self, inputs, states, **kwargs):\r\n        output = states[0] + tf.nn.conv2d(inputs, tf.ones((3, 3, 1, 1)), (1, 1), \"SAME\")\r\n        new_state = output\r\n        return output, new_state\r\n\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.RNN(CustomCell(), batch_input_shape=(1, 1, 16, 16, 1)))\r\n\r\n# Error here\r\nmodel.predict(tf.ones(model.input_shape))\r\n```\r\n\r\nIf the `conv2d` is replaced with `conv1d`, however, no error occurs:\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nclass CustomCell(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        self.state_size = [tf.TensorShape((16, 1))]\r\n        super(CustomCell, self).__init__(**kwargs)\r\n\r\n    def call(self, inputs, states, **kwargs):\r\n        output = states[0] + tf.nn.conv1d(inputs, tf.ones((3, 1, 1)), (1,), \"SAME\")\r\n        new_state = output\r\n        return output, new_state\r\n\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.RNN(CustomCell(), batch_input_shape=(1, 1, 16, 1)))\r\n\r\n# No error\r\nmodel.predict(tf.ones(model.input_shape))\r\n```", "comments": ["@mattdutson,\r\nI was able to run the above code snippets without any error messages with both [TF v2.1](https://colab.research.google.com/gist/amahendrakar/1bfea7bbe1311e108c5864964ef07b91/38279-2-1.ipynb) and [TF v2.2.0rc2](https://colab.research.google.com/gist/amahendrakar/a5a66e6f344ce0fd75943c7d48426305/38279-2-2.ipynb). Please find the attached gist.\r\n\r\nCould you please try running the same code in a virtual environment and let us know if it works? Thanks!", "@amahendrakar Thanks for the quick reply. It looks like IPython is hiding/suppressing the error message. If you upload the example code snippet to a file `test.py` in the Colab folder and then run from the command-line via `!python test.py` you can see the error message.", "I have the same issue", "I noticed today that the error goes away if the RNN layer is constructed with `unroll=True`.", "Unrolling is apparently more memory-intensive, however.", "Was able to reproduce the issue with TF v2.2.0rc2 and TF-nightly. Please find the gist [here](https://colab.research.google.com/gist/amahendrakar/d59dbcfeb6358851fb6f2994e49f7357/38279-2-2.ipynb#scrollTo=sxLOo03U2rnI&line=1&uniqifier=1). Thanks!", " \nPlease let me know when you have a fix, thanks!    On Thursday, April 9, 2020, 03:59:55 PM GMT+8, amahendrakar <notifications@github.com> wrote:  \n \n \n\n\nWas able to reproduce the issue with TF v2.2.0rc2 and TF-nightly. Please find the gist here. Thanks!\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or unsubscribe.\n  ", "Seems to  be a grappler issue.", "This appears to be fixed in the latest version at head. I cannot reproduce the error with the code snippet provided. Feel free to reopen if you see this again.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38279\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38279\">No</a>\n", "Just tested this with `tf-nightly`. The issue does not appear to be resolved. Running the first snippet results in the following error message:\r\n```\r\n2020-06-01 14:34:55.279045: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:575] layout failed: Invalid argument: MutableGraphView::SortTopologically error: detected edge(s) creating cycle(s) {'sequential/rnn/while/next_iteration/_37-0-0-TransposeNCHWToNHWC-LayoutOptimizer' -> 'sequential/rnn/while/merge/_13'}.\r\n```", "@rmlarsen Don't think I have the permissions required to reopen the issue. Could you reopen it?", "I have the same issue, any advice?\r\n", "In my case, it seems that the error occurs when I let tensorflow do its Layout optimization.\r\nAs soon as I added:\r\n`tf.config.optimizer.set_experimental_options({'layout_optimizer': False})`\r\nthe error disappeared. I suspect the optimization procedure to be the origin of the problem. This should be investigated.\r\nHope it helps !"]}, {"number": 38278, "title": "[r2.2:CherryPick] Ensure saved_model_cli and deps are included in pip package.", "body": "Unbreaks tf2.2 rc's saved_model_cli (fixes #38042)\n\nPiperOrigin-RevId: 304724007\nChange-Id: I832653cfc63d0d5338f7ca82e823987337fee429", "comments": []}, {"number": 38277, "title": "[r2.2:CherryPick] Updated model `metrics_names` doc with note on when `metrics_names` will be populated. Also added some examples.", "body": "PiperOrigin-RevId: 303883781\nChange-Id: Ib999d8efb13695ece1bbc61414d05066e82c9cb8", "comments": []}, {"number": 38276, "title": "Cherry-pick curl CVE fix", "body": "PiperOrigin-RevId: 304938718\r\nChange-Id: I408e3b1d9ce1badfb08666ddac6400bae2c97936\r\n\r\nThis cherry-picks CVE fix for curl from #38200", "comments": []}, {"number": 38275, "title": "Cherry-pick curl CVE fix", "body": "PiperOrigin-RevId: 304938718\r\nChange-Id: I408e3b1d9ce1badfb08666ddac6400bae2c97936\r\n\r\nThis cherry-picks CVE fix for curl from #38200", "comments": []}, {"number": 38274, "title": "Cherry-pick curl CVE fix", "body": "PiperOrigin-RevId: 304938718\r\nChange-Id: I408e3b1d9ce1badfb08666ddac6400bae2c97936\r\n\r\nThis cherry-picks CVE fix for curl from #38200", "comments": []}, {"number": 38273, "title": "Cherry-pick curl CVE fix", "body": "PiperOrigin-RevId: 304938718\r\nChange-Id: I408e3b1d9ce1badfb08666ddac6400bae2c97936\r\n\r\nThis cherry-picks CVE fix for curl from #38200", "comments": []}, {"number": 38272, "title": "TPU Bug when using categorical_crossentropy", "body": "ResourceExhaustedError: {{function_node _inferencedistributedfunction153395}} Compilation failure: Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.\r\n\r\nLargest program allocations in vmem:\r\n\r\nXLA label: register allocator spill slots\r\nAllocation type: scoped\r\n\r\nXLA label: %fusion.11963 = f32[4,192,1024]{2,0,1:T(4,128)} fusion(f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, \u2026(+188)), kind=kLoop, calls=%fused_computation.10858\r\nAllocation type: scoped\r\n\r\nXLA label: %fusion.11963 = f32[4,192,1024]{2,0,1:T(4,128)} fusion(f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, \u2026(+188)), kind=kLoop, calls=%fused_computation.10858\r\nAllocation type: scoped\r\n\r\nXLA label: %fusion.11963 = f32[4,192,1024]{2,0,1:T(4,128)} fusion(f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, \u2026(+188)), kind=kLoop, calls=%fused_computation.10858\r\nAllocation type: scoped\r\n\r\nXLA label: %fusion.11963 = f32[4,192,1024]{2,0,1:T(4,128)} fusion(f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, f32[4,1,1024]{2,0,1:T(4,128)}, \u2026(+188)), kind=kLoop, calls=%fused_computation.10858\r\nAllocation type: scoped\r\n\r\nTPU compilation failed\r\n [[{{node tpu_compile_succeeded_assert/_5292462253713114861/_5}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add reporttensorallocationsuponoom to RunOptions for current allocation info.", "comments": ["@HarshitSheoran, Please provide the complete standalone code reproduce the issue and also share the Tensorflow version information. Thanks.", "`def build_model(transformer, max_len=512):\r\n    \"\"\"\r\n    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\r\n    \"\"\"\r\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\r\n    sequence_output = transformer(input_word_ids)[0]\r\n    cls_token = sequence_output[:, 0, :]\r\n    out = Dense(1, activation='sigmoid')(cls_token)\r\n    \r\n    model = Model(inputs=input_word_ids, outputs=out)\r\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['acc'])\r\n    \r\n    return model`", "you get the idea, sry I have some bugs in my browser to github comments, it does not let me put everything in code sometimes.", "Use any transformer from `import transformers`", "@HarshitSheoran, Can you share the modified complete code to analyze the issue. Thanks", "Actually, I did give enough code, also, I linked to kernel to get even more information to code, okay, so you can see it as, copy the whole kernel and just change the loss to \"categorical_crossentropy\", that should reproduce, sorry but I can not give exact code I use to achieve my highest score in a competition.", "@HarshitSheoran,\r\nas requested for, could you please share the tensorflow version", "@HarshitSheoran,\r\nI was able to run the code without any issues with TF v2.2.0rc3, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/2566909a1f654b17ef77e7b53ba1ec65/38272.ipynb). Thanks!", "@HarshitSheoran,\r\nAny updates regarding this issue? Thanks!", "I guess the problem was there for some other reason, thank you!"]}, {"number": 38271, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Source and then through PyCharm\r\n- **TensorFlow version (use command below)**: 2.1.0\r\n- **Python version**: 3.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:  10.1 / 7.6.5.32\r\n- **GPU model and memory**: GeForce 1050\r\n- **Exact command to reproduce**: Keras DQN agent with tensorflow gpu\r\n\r\n\r\n### Describe the problem\r\nI run my agent and this error comes out\r\n\r\n### Source code / logs\r\n```\r\nimport random\r\nimport gym\r\nimport gym_zombiediceAI\r\nimport numpy as np\r\nimport os\r\nfrom collections import deque\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.optimizers import Adam\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n```\r\n.\r\n.\r\n.\r\n```\r\nif __name__ == \"__main__\":\r\n    env = gym.make('zb-v0')\r\n    state_size = env.observation_space.shape[0]\r\n    action_size = env.action_space.n\r\n    agent = DQNAgent(state_size, action_size)\r\n    # agent.load(\"./save/cartpole-dqn.h5\")\r\n    done = False\r\n    batch_size = 32\r\n\r\n    for e in range(EPISODES):\r\n        state = env.reset()\r\n        state = np.reshape(state, [1, state_size])\r\n        for time in range(500):\r\n            # env.render()\r\n            action = agent.act(state)\r\n            next_state, reward, done, _ = env.step(action)\r\n            reward = reward if not done else -10\r\n            next_state = np.reshape(next_state, [1, state_size])\r\n            agent.memorize(state, action, reward, next_state, done)\r\n            state = next_state\r\n            if done:\r\n                print(\"episode: {}/{}, score: {}, e: {:.2}\"\r\n                      .format(e, EPISODES, time, agent.epsilon))\r\n                break\r\n            if len(agent.memory) > batch_size:\r\n                loss = agent.replay(batch_size)\r\n                # Logging training loss every 10 timesteps\r\n                if time % 10 == 0:\r\n                    print(\"episode: {}/{}, time: {}, loss: {:.4f}\"\r\n                        .format(e, EPISODES, time, loss))\r\n\r\n```\r\nError Message:\r\n```\r\nC:\\Users\\Krakas\\AppData\\Local\\Programs\\Python\\Python37\\python.exe C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\pydevconsole.py --mode=client --port=52043\r\nimport sys; print('Python %s on %s' % (sys.version, sys.platform))\r\nsys.path.extend(['C:\\\\Users\\\\Krakas\\\\gym-zombiediceAI', 'C:/Users/Krakas/gym-zombiediceAI'])\r\nPython 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.8.0 -- An enhanced Interactive Python. Type '?' for help.\r\nPyDev console: using IPython 7.8.0\r\nPython 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)] on win32\r\nrunfile('C:/Users/Krakas/gym-zombiediceAI/gym_zombiediceAI/envs/kerasAgent.py', wdir='C:/Users/Krakas/gym-zombiediceAI/gym_zombiediceAI/envs')\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Krakas\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Krakas\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Krakas\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-85efe6c332ce>\", line 1, in <module>\r\n    runfile('C:/Users/Krakas/gym-zombiediceAI/gym_zombiediceAI/envs/kerasAgent.py', wdir='C:/Users/Krakas/gym-zombiediceAI/gym_zombiediceAI/envs')\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\_pydev_bundle\\pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"C:/Users/Krakas/gym-zombiediceAI/gym_zombiediceAI/envs/kerasAgent.py\", line 7, in <module>\r\n    from keras.models import Sequential\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Krakas\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Krakas\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Krakas\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Krakas\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Krakas\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Krakas\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Krakas\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\192.7142.42\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 21, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Krakas\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Krakas\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Krakas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\npip show tensorflow\r\nName: tensorflow\r\nVersion: 2.1.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: c:\\users\\krakas\\appdata\\roaming\\python\\python37\\site-packages\r\nRequires: absl-py, six, tensorflow-estimator, keras-preprocessing, google-pasta, astor, termcolor, gast, grpcio, numpy, scipy, opt-einsum, protobuf, wrapt, wheel, tensorboard, keras-applications\r\nRequired-by: huskarl\r\nNote: you may need to restart the kernel to use updated packages.\r\n```", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@koullisHP \r\nplease refer to [this link](https://github.com/tensorflow/tensorflow/issues/38013#issuecomment-606108871) and let us know if it helps", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38271\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38271\">No</a>\n", "I added gpu paths but can't find C:\\tools\\cuda\r\nI downloaded and extracted cuDNN in D:\\Downloads, should I change my path of cuDNN?\r\n", "@koullisHP \r\nplease follow [this link ](https://www.tensorflow.org/install/gpu#windows_setup)"]}, {"number": 38270, "title": "Train simple audio recognition ", "body": "@petewarden \r\n@tensorflow/micro\r\n\r\n**System information**\r\n\r\n- OS Platform and Distribution -Windows 8\r\n- TensorFlow installed from (source or binary): - \r\n-TensorFlow version - 2.2.0.dev20200210\r\n- Python version: - 3.6 version (if compiling from source):\r\n- CUDA/cuDNN version: - GPU not supported in my system trying to train model in colab\r\n\r\n\r\n###  Describe the current behavior\r\nI am trying to train a simple audio recognition model as described in book \"TinyML\" . I am using colab to train the model. \r\nFirst I got the error while Install Dependencies \"ERROR: Could not find a version that satisfies the requirement tf-nightly-gpu==1.15.0.dev20190729 (from versions: 2.2.0.dev20200210, 2.2.0.dev20200211, ...)\r\nERROR: No matching distribution found for tf-nightly-gpu==1.15.0.dev20190729\"\r\nAs temsorflow version 1.5 is not supported is replaced the version by 2.2.0.dev20200210 , this error got resolved.\r\n\r\nAgain while \"Begin Training\" I got error \"Traceback (most recent call last):\r\n  File \"tensorflow/tensorflow/examples/speech_commands/train.py\", line 81, in <module>\r\n    import input_data\r\n  File \"/content/tensorflow/tensorflow/examples/speech_commands/input_data.py\", line 35, in <module>\r\n    from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\r\nModuleNotFoundError: No module named 'tensorflow.contrib\"\r\n\r\nPlease provide the exact sequence of commands/steps to solve above problem and train the simple audio in colab.\r\n\r\n\r\n\r\n", "comments": ["@rajaniyadav \r\n\r\ntf.contrib no longer exists in post-2.0 world. It has been moved (partially) to TF Addons.\r\n\r\nIf you really want tf.contrib, you can use TF 1.15 for the next 3 years, but after that you will have to switch as there won't be a TF 1.16 or later.\r\n\r\nAlso,you check  whether you have 64-bit version of Python (32-bit will not support).Thanks!", "Hi,\r\nIs there any way that we can migrate this code to tensorflow 2.0 version?", "@rajaniyadav \r\n\r\ncan you please go through the [link ](https://www.tensorflow.org/guide/migrate?hl=en)and see if it helps you.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@rajaniyadav \r\n\r\nAny update on this issue please. Thanks!", "Hi \r\nThank you for responding. The above bug is recently fix by the owner of the model.", "@rajaniyadav \r\n\r\nGlad to know the issue got fixed.Please close this thread, as the issue got resolved.. Thanks!", "Issue resolved so closing it."]}, {"number": 38269, "title": "TF 2.2.0-rc2 tf.Keras API:  model.loss_functions no longer exists", "body": "\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  `Yes`\r\n- OS Platform and Distribution:  `Ubuntu 18.04`\r\n- TensorFlow installed from (binary): `v2.2.0-rc1-34-ge6e5d6df2a 2.2.0-rc2`\r\n- CUDA/cuDNN version: `cuda-10.1`\r\n\r\n**Describe the current behavior**\r\nIn `tf.Keras` API in  TensorFlow 2.2.0, `model.loss_functions` no longer exists.  Instead the loss functions are buried and seem to be only accessible with code like this:\r\n```python\r\nmodel.compiled_loss._get_loss_object(model.compiled_loss._losses).fn\r\n```\r\nBeing able to easily access loss functions associated with a model is important for many applications that are built on TensorFlow.  Moreover, this is an undocumented breaking change.\r\n\r\n**Describe the expected behavior**\r\nThe loss functions associated with a model should be more directly and easily accessible like `model.loss_functions`, which is the way loss functions are accessed in TensorFlow 2.1.0.\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nfrom tensorflow.keras.models import Sequential  \r\nfrom tensorflow.keras.layers import Dense, Activation  \r\nmodel = Sequential()  \r\nmodel.add(Dense(10, input_dim=10, activation='softmax'))  \r\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) \r\n\r\n# works in TF 2.2.0-rc2, but does NOT work in previous versions like 2.1.0\r\nmodel.compiled_loss._get_loss_object(model.compiled_loss._losses).fn\r\n\r\n# does NOT work in TF 2.2.0-rc2, but works in TF 2.1.0\r\nmodel.loss_functions[0].fn\r\n\r\n", "comments": ["This looks related to #38150 ", "@amaiya This was removed as a part of a refactor and was an intended change. Sorry this was not communicated in our release notes, i have updated the notes now. This was an undocumented property that was accidentally public. \r\n\r\nYou can access list of loss functions in the `tf.keras.losses` module.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38269\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38269\">No</a>\n", "The `tf.keras.losses` module does not allow  you to identify the loss function tied to a compiled model.  So, this solution isn't really satisfactory.  The loss function associated with a compiled model should be publicly accessible, which was part of the point of this issue.", "Oh i meant you can use `tf.keras.losses.get` to get the loss function using a string. Since loss value is passed to the compile API it should be easy to compile list of loss functions from what's being passed. You can also use `model.compiled_loss.metrics`.\r\n", "I see - thanks.   \r\n\r\nSo, the idea is to obtain the loss function string names and supply that to `tf.keras.losses.get`.   In situations where you may not have access to the loss function string names originally supplied to model.compile (e.g., libraries that accept a compiled model as input to facilitate training), what is the best way to obtain the loss function string names directly from the model?  If a model is compiled with a loss function using a string name or loss object, it would make sense for it be possible to query the model and retrieve the loss function (or loss function string name) from the model using public API.\r\n\r\nYou suggested `model.compiled_loss.metrics`, but this is an empty list using the example above.  Only  `model.compiled_loss._losses` contains the string names:\r\n```\r\nIn [1]: from tensorflow.keras.models import Sequential   \r\n   ...: from tensorflow.keras.layers import Dense, Activation   \r\n   ...: model = Sequential()   \r\n   ...: model.add(Dense(10, input_dim=10, activation='softmax'))   \r\n   ...: model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])  \r\n\r\nIn [2]: model.compiled_loss.metrics                                                                                                                                                                         \r\nOut[2]: []\r\n\r\nIn [3]: model.compiled_loss._losses                                                                                                                                                                         \r\nOut[3]: 'categorical_crossentropy'\r\n```\r\n\r\n", "`model.compiled_loss.metrics` gets populated after a model is actually trained /evaluated.", "I see that `model.compiled_loss.metrics` is populated after training, but it doesn't return what you claim it returns:\r\n\r\n```\r\nIn [18]: model.compiled_loss.metrics # no loss function string name                                                                                                                                                                         \r\nOut[18]: [<tensorflow.python.keras.metrics.Mean at 0x7fc85af8a6d8>]\r\n```\r\nThere should be an easy and public way to query a model to identify either the loss function or loss function string name (in this case 'categorical_crossentropy') without resorting to underscore atrributes like this::\r\n```\r\nIn [19]: model.compiled_loss._losses                                                                                                                                                                        \r\nOut[19]: [<tensorflow.python.keras.losses.LossFunctionWrapper at 0x7fc85aed4a90>]\r\n\r\nIn [20]: model.compiled_loss._losses[0].fn                                                                                                                                                                  \r\nOut[20]: <function tensorflow.python.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0)>\r\n\r\n```"]}, {"number": 38268, "title": "Pickleable tf.function, attempt #2", "body": "Updated attempt at #37837 \r\n\r\n@mihaimaruseac as requested. Thank you!", "comments": ["Thank you"]}, {"number": 38267, "title": "[XLA][LHLO] Added support for SideEffects interfaces to LHLO operations.", "body": "This PR adds support for the recently added `SideEffects` interfaces in MLIR to all LHLO operations. It also covers some invalidly annotated LHLO operations in terms of side effects.", "comments": ["Is this something we could test? Like writing a pass that look for the side-effect interface and just print each op and the result? That should allow a lit-test.", "@joker-eph I was wondering about the same fact. Maybe I could add a simple test case that does just that. However, I wasn't sure if these tests would really help significantly, since an error in the `.td` file would (more or less) be included in the test case."]}, {"number": 38266, "title": "Add support for attr classes in Dataset", "body": "See issue: https://github.com/tensorflow/tensorflow/issues/38192\r\nI have not run the tests (can't manage to build tf locally...) but I modified a venv locally and it seems to work on the example attached.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38266) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38266) for more info**.\n\n<!-- ok -->", "Thank you @AdrienCorenflos. However, this PR also needs test for the newly supported functionality. Ideally, you would add test cases for parameterized tests in [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/util/structure_test.py) and minimally adding a single unit tests similar to [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/util/structure_test.py#L751-L756).", "> Thank you @AdrienCorenflos. However, this PR also needs test for the newly supported functionality. Ideally, you would add test cases for parameterized tests in [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/util/structure_test.py) and minimally adding a single unit tests similar to [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/util/structure_test.py#L751-L756).\r\n\r\n@jsimsa I think I already have added a test similar to the one you are mentioning. Tell me if that's not what you want.\r\n\r\nAdrien", "@aaudiber see https://github.com/tensorflow/tensorflow/pull/38266/commits/c4bf9b0c2ef80b48ac76c7128a3b34cda87f8d0a", "@AdrienCorenflos Can you please fix build failures ? Thanks!", "Hi\r\nHow do I know which one of the 1000+ failures and errors is due to my change? There are no logs for errors, just error code 1...", "If you click on Target Log tab you should see this: \r\n<img width=\"1532\" alt=\"Screen Shot 2020-04-09 at 9 26 07 AM\" src=\"https://user-images.githubusercontent.com/1072079/78917770-32fc9400-7a44-11ea-8fbb-159c2adc8cea.png\">\r\n", "@AdrienCorenflos you can find the detailed error message under the \"Target Log\" tab. It looks like the cause of the failures is \"ModuleNotFoundError: No module named 'attr'\". This is because the `attr` module may not always be available (TF is still in the process of dropping python2 within Google). You can fix this issue by guarding your attr imports with\r\n\r\n```\r\ntry:\r\n  import attr  # pylint:disable=g-import-not-at-top\r\nexcept ImportError:\r\n  attr = None\r\n```\r\n\r\nand then checking whether attr is None before using it. See function_test.py for an example of doing this: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/function_test.py", "@AdrienCorenflos Can you please check @aaudiber's comments and resolve conflicts?. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n", "Hi, FYI I have not continued on this one because there is an underlying bug with datasets that needs correcting first (basically you have different behaviours when you do tf.data.Dataset.from_tensor_slices([namedtuples]) vs data.map(namedtuples) and which was propagated to my stuff). I will raise a separate issue when I have some time.", "@gbaned see this\r\nhttps://github.com/tensorflow/tensorflow/issues/39403"]}, {"number": 38265, "title": "java.lang.IllegalArgumentException: Cannot assign a device for operation", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n\r\n\r\n- Linux version : 3.10.0-862.el7.x86_64 \r\n- TensorFlow installed from : binary\r\n- TensorFlow version (use command below): tensorflow_gpu==1.14.0\r\n- Python version: - 2.7.5\r\n\r\n- CUDA/cuDNN version: NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nI convert a pre-trained tensorflow model to .pb file with python code as bellow\r\n\r\n`\r\nimport tensorflow as tf\r\nfrom argparse import ArgumentParser\r\n\r\ndef main():\r\n    parser = ArgumentParser()\r\n    parser.add_argument('--checkpoint', type=str,\r\n                        dest='checkpoint',\r\n                        help='dir or .ckpt file to load checkpoint from',\r\n                        metavar='CHECKPOINT', required=True)\r\n    parser.add_argument('--model', type=str,\r\n                        dest='model',\r\n                        help='.meta for your model',\r\n                        metavar='MODEL', required=True)\r\n    parser.add_argument('--out-path', type=str,\r\n                        dest='out_path',\r\n                        help='model output directory',\r\n                        metavar='MODEL_OUT', required=True)\r\n    opts = parser.parse_args()\r\n    tf.reset_default_graph()\r\n    saver = tf.train.import_meta_graph(opts.model)\r\n    #builder = tf.saved_model.builder.SavedModelBuilder(opts.out_path)\r\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\r\n        # Restore variables from disk.\r\n        saver.restore(sess, opts.checkpoint)\r\n        print(\"Model restored.\")\r\n        #builder.add_meta_graph_and_variables(sess,['tfckpt2pb'],strip_default_attrs=False)\r\n        #builder.save()\r\n        constant_graph = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, ['Tower_0/parsing_fc/BiasAdd','Tower_0/parsing_rf_fc/BiasAdd','Tower_0/edge_rf_fc/BiasAdd'])\r\n\r\n        with tf.gfile.FastGFile(opts.out_path, mode='wb') as f:\r\n                f.write(constant_graph.SerializeToString())\r\n                print(\"pb Model saved.\")\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n   ` and I import the .pb model with java code as bellow\r\n `         //import model\r\n              byte[] graphBytes = IOUtils.toByteArray(new FileInputStream(MODEL_PATH));\r\n\r\n              graph.importGraphDef(graphBytes);\r\n\r\n\r\n              //create session\r\n              try(Session session = new Session(graph)){\r\n\r\n                  ConfigProto config = ConfigProto.newBuilder()\r\n                          .setGpuOptions(GPUOptions.newBuilder().setAllowGrowth(true))\r\n                          .build();\r\n                   //get the output\r\n                  Tensor<?> output = session.runner()\r\n                          .setOptions(config.toByteArray())\r\n                          .feed(\"Tower_0/strided_slice\", imageTensor)\r\n                          .fetch(\"Tower_0/parsing_fc/BiasAdd\").run().get(0);\r\n                  System.out.println(output);\r\n              }`\r\n\r\nthen error as bellow\r\n`here are error message 2020-04-06 12:13:50.269556: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA 2020-04-06 12:13:50.281419: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz 2020-04-06 12:13:50.288167: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f968ded1db0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2020-04-06 12:13:50.288206: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version java.lang.IllegalArgumentException: Cannot assign a device for operation Tower_0/strided_slice: {{node Tower_0/strided_slice}} was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0 ]. Make sure the device specification refers to a valid device. The requested device appears to be a GPU, but CUDA is not enabled. [[Tower_0/strided_slice]] at org.tensorflow.Session.run(Native Method) at org.tensorflow.Session.access$100(Session.java:48) at org.tensorflow.Session$Runner.runHelper(Session.java:326) at org.tensorflow.Session$Runner.run(Session.java:276) at `\r\n\r\n**Describe the expected behavior**\r\n  it can correctly run up \r\n\r\n", "comments": ["@bewithme, Can you provide the proper intended stand alone code to reproduce the issue.Thanks!", "@bewithme, Can you update for the above comment. ", "@bewithme,\r\nplease update as per above comment", "> @bewithme, Can you provide the proper intended stand alone code to reproduce the issue.Thanks!\r\n\r\nhere are code and model https://gist.github.com/bewithme/82b588e2b7b86c8e211e02aff5ea573c\r\n\r\nthank you so much", "@bewithme \r\ni ran the code shared, please find the error faced [here](https://colab.sandbox.google.com/gist/Saduf2019/2cac10919b20364aa0c7a89fbabe3283/untitled152.ipynb), please provide with simple stand alone code[indented] such that we could relpicate the issue faced", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38265\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38265\">No</a>\n"]}]