[{"number": 12704, "title": "Memory leak when reusing variables with slim", "body": "### System information:\r\n- Windows 7 x64\r\n- Python 3.5.2 |Anaconda 4.2.0 (64-bit)\r\n- Tensorflow 1.3.0 installed via pip\r\n\r\n\r\n### Problem\r\nI want to use TF-Slim models to classify images in a server. For this, I would like to load the network only once and reuse the variables. I adjusted the tutorial given here: https://github.com/tensorflow/models/blob/master/slim/slim_walkthrough.ipynb\r\n\r\n### Code to reproduce\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import slim\r\n\r\nimport urllib.request as urllib\r\nfrom nets import inception\r\nfrom preprocessing import inception_preprocessing\r\n\r\n\r\nimage_size = inception.inception_v1.default_image_size\r\n\r\ninitialized = False\r\ngraph = tf.Graph()\r\n\r\ninit_fn = None\r\n\r\ndef predict():\r\n    global initialized\r\n    global init_fn\r\n    with graph.as_default():\r\n        url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg'\r\n        image_string = urllib.urlopen(url).read()\r\n        image = tf.image.decode_jpeg(image_string, channels=3)\r\n        processed_image = inception_preprocessing.preprocess_image(image, image_size, image_size, is_training=False)\r\n        processed_images  = tf.expand_dims(processed_image, 0)\r\n        with slim.arg_scope(inception.inception_v1_arg_scope()):\r\n            logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False, reuse=initialized)\r\n        probabilities = tf.nn.softmax(logits)\r\n        if not initialized:\r\n            init_fn = slim.assign_from_checkpoint_fn(\"tmp/checkpoints/inception_v1.ckpt\", slim.get_model_variables(\"InceptionV1\"))\r\n        with tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=8)) as sess:\r\n            init_fn(sess)\r\n            np_probabilities = sess.run(probabilities)\r\n        initialized = True\r\n    return np_probabilities.tolist()\r\n\r\nfor _ in range(20):\r\n    predict()\r\n```\r\n\r\n\r\n### Memory Usage monitored with Process Explorer\r\n![memory](https://user-images.githubusercontent.com/20746434/29866915-33ed6506-8d7a-11e7-8dd4-1fc3e1b2f1d7.png)\r\nThis is the memory usage when calling the predict function 20 times. As you can see, it keeps increasing.\r\n\r\n\r\nAm I doing it wrong, or is it a bug?", "comments": ["@sguada Could you take a look at this potential slim memory leak?", "I think creating ops in loops is the problem.", "As @ppwwyyxx mentioned you are creating a bigger and bigger graph each time.\r\n\r\nTry to create the graph once and use it multiple times, for example\r\n\r\n```\r\ndef build():\r\n        image_placeholder = tf.placeholder([], tf.string)\r\n        image = tf.image.decode_jpeg(image_placeholder, channels=3)\r\n        processed_image = inception_preprocessing.preprocess_image(image, image_size, image_size, is_training=False)\r\n        processed_images  = tf.expand_dims(processed_image, 0)\r\n        with slim.arg_scope(inception.inception_v1_arg_scope()):\r\n            logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False, reuse=initialized)\r\n        probabilities = tf.nn.softmax(logits)\r\n        return image_placeholder, probabilities\r\n\r\ndef predict(url, sess, image_placeholder):\r\n        image_string = urllib.urlopen(url).read()\r\n        np_probabilities = sess.run(probabilities, {image_placeholder: image_string})\r\n        return np_probabilities.tolist()\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n  build()\r\n  init_fn = slim.assign_from_checkpoint_fn(\"tmp/checkpoints/inception_v1.ckpt\", slim.get_model_variables(\"InceptionV1\"))\r\n  with tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=8)) as sess:\r\n    init_fn(sess)\r\n\r\n  url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg'\r\n  for _ in range(20):\r\n      predict(url)\r\n```", "Thanks, this solved it. Your code was not running, but the idea was clear, so I could fix it.\r\n\r\nNow, if I have requests which use different batch sizes, I would have to reinit the network, right? I tried with resetting the graph, but then I again got a leak...", "Just a quick example:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import slim\r\n\r\nfrom nets import inception\r\n\r\nimport numpy as np\r\nimport random\r\nfrom time import sleep\r\n\r\n\r\ninitialized = False\r\nbatchSize = None\r\n\r\ndef init(imgs):\r\n    global initialized\r\n    tf.reset_default_graph()\r\n    graph = tf.Graph()\r\n    graph.as_default()\r\n    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=8))\r\n    initialized = False\r\n    placeholder, probas = build(imgs)\r\n    init_fn = slim.assign_from_checkpoint_fn(\"tmp/checkpoints/inception_v1.ckpt\", slim.get_model_variables(\"InceptionV1\"))\r\n    init_fn(sess)\r\n    initialized = True\r\n    return sess, placeholder, probas\r\n\r\n\r\ndef build(imgs):\r\n    global batchSize\r\n    batchSize = len(imgs)\r\n    image_placeholder = tf.placeholder(tf.float32, shape=imgs.shape)\r\n    tensorImgs = tf.convert_to_tensor(image_placeholder)\r\n    with slim.arg_scope(inception.inception_v1_arg_scope()):\r\n        logits, _ = inception.inception_v1(tensorImgs, num_classes=1001, is_training=False, reuse=initialized)\r\n    probabilities = tf.nn.softmax(logits)\r\n    return image_placeholder, probabilities\r\n\r\n\r\ndef predict(imgs, image_placeholder, probabilities):\r\n        np_probabilities = sess.run(probabilities, {image_placeholder: imgs})\r\n        return np_probabilities\r\n\r\nimgs1 = np.empty((4, 224, 224, 3))\r\nimgs2 = np.empty((2, 224, 224, 3))\r\n\r\nsess, placeholder, probas = init(imgs1)\r\nsleep(30)\r\n\r\nfor _ in range(10):\r\n    print(predict(imgs1, placeholder, probas).argmax())\r\n    sleep(1)\r\n\r\nsleep(30)\r\n\r\nfor _ in range(50):\r\n    if random.choice([0,1]):\r\n        imgs = imgs1\r\n    else:\r\n        imgs = imgs2\r\n    \r\n    if batchSize != len(imgs) or not initialized:\r\n        sess, placeholder, probas = init(imgs)\r\n    print(predict(imgs, placeholder, probas).argmax())\r\n```\r\n\r\nMemory usage:\r\n![mem](https://user-images.githubusercontent.com/20746434/30154843-66d3c018-93bb-11e7-9f47-0720c7057f3e.png)\r\n", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "You are still building parts of the graph when you change the imgs.\r\n\r\nWhat you need to do is use a placeholder that doesn't specify the batch_size dimension or even the HxW dimensions, it would work for any batch_size and image size, as far all the images in the batch has the same HxW.\r\n\r\n\r\n```\r\ndef init(imgs):\r\n    global initialized\r\n    tf.reset_default_graph()\r\n    graph = tf.Graph()\r\n    graph.as_default()\r\n    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=8))\r\n    initialized = False\r\n    image_placeholder = tf.placeholder(tf.float32, shape=[None, None, None, 3])\r\n    probas = build(image_placeholder)\r\n    init_fn = slim.assign_from_checkpoint_fn(\"tmp/checkpoints/inception_v1.ckpt\", slim.get_model_variables(\"InceptionV1\"))\r\n    init_fn(sess)\r\n    initialized = True\r\n    return sess, placeholder, probas\r\n\r\n\r\ndef build(images):\r\n    with slim.arg_scope(inception.inception_v1_arg_scope()):\r\n        logits, _ = inception.inception_v1(images, num_classes=1001, is_training=False, reuse=initialized)\r\n    probabilities = tf.nn.softmax(logits)\r\n    return probabilities\r\n```\r\n"]}, {"number": 12703, "title": "Merge pull request #2 from tensorflow/master", "body": "Update from origin", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "I assume this PR was opened in error."]}, {"number": 12702, "title": "Merge pull request r1.3 from tensorflow/master", "body": "Update from origin to r1.3", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Done"]}, {"number": 12700, "title": "Fix duplicate symbol CreateGPUTracerEv", "body": "Closes #12699", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@petewarden does that sound right?", "Jenkins, test this please.", "@petewarden ping", "Jenkins, test this please.", "Unfortunately you took so long that this seems to be fixed on master already.", "Sorry about that. :( At least it is fixed."]}, {"number": 12699, "title": "libtensorflow-core.a contains duplicate symbol CreateGPUTracerEv", "body": "Running `build_all_ios.sh` produces a `libtensorflow-core.a` that contains a symbol twice. When linking on iOS the error is\r\n\r\n```\r\nduplicate symbol __ZN10tensorflow15CreateGPUTracerEv in:\r\n    <PROJDIR>/.../libtensorflow-core.a(gpu_tracer.o)\r\nld: 1 duplicate symbol for architecture x86_64\r\n```\r\n\r\nI checked with `nm` just to be sure\r\n\r\n```\r\n$ nm libtensorflow-core.a | grep CreateGPUTracerEv\r\n---------------- T __ZN10tensorflow15CreateGPUTracerEv\r\n---------------- T __ZN10tensorflow15CreateGPUTracerEv\r\n```\r\n\r\nBranch `master`\r\nMacOS Sierra 10.12.6 (16G29)\r\nXcode Version 8.3.3 (8E3004b)", "comments": ["Removing line `513` from `Makefile`\r\n\r\n    TF_CC_SRCS += tensorflow/core/platform/default/gpu_tracer.cc\r\n\r\nresolves this issue. ", "@petewarden It seems duplicate symbols are being linked into a static library, when using build_all_ios.sh.", "Can you assign @petewarden ?", "Same problem on android, the object file `gpu_tracer.o` contains the dummy function returning nullptr:\r\n\r\n```\r\nint __fastcall tensorflow::CreateGPUTracer(int result)\r\n{\r\n  *(_DWORD *)result = 0;\r\n  return result;\r\n}\r\n```\r\n\r\nand a \"global\":\r\n\r\n```\r\nint GLOBAL__sub_I__ZN10tensorflow15CreateGPUTracerEv()\r\n{\r\n  std::ios_base::Init::Init((std::ios_base::Init *)&std::__ioinit);\r\n  return _aeabi_atexit(&std::__ioinit, std::ios_base::Init::~Init, &_dso_handle);\r\n}\r\n```\r\n", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 12698, "title": "Issue loading label file on Android example", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nAndroid 8.0\r\n- **TensorFlow installed from (source or binary)**:\r\nAndroid Binary\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\nNA\r\n- **Bazel version (if compiling from source)**:\r\nNA\r\n- **CUDA/cuDNN version**:\r\nNA\r\n- **GPU model and memory**:\r\nNA\r\n- **Exact command to reproduce**:\r\nNA\r\n\r\n\r\nWhenever I try to build and launch the TF Classify demo app provided, the app crashes with the following error:\r\n\r\n`08-29 21:03:01.791 9614-9614/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: main\r\n                                                                   Process: org.tensorflow.demo, PID: 9614\r\n                                                                   java.lang.RuntimeException: Problem reading label file!\r\n                                                                       at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:100)\r\n                                                                       at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:107)\r\n                                                                       at org.tensorflow.demo.CameraActivity$2.onPreviewSizeChosen(CameraActivity.java:324)\r\n                                                                       at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:407)\r\n                                                                       at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:414)\r\n                                                                       at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:64)\r\n                                                                       at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:95)\r\n                                                                       at android.view.TextureView.getHardwareLayer(TextureView.java:390)\r\n                                                                       at android.view.TextureView.draw(TextureView.java:339)\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18069)\r\n                                                                       at android.view.View.draw(View.java:18847)\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18060)\r\n                                                                       at android.view.View.draw(View.java:18847)\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)\r\n                                                                       at android.view.View.draw(View.java:19122)\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18069)\r\n                                                                       at android.view.View.draw(View.java:18847)\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18060)\r\n                                                                       at android.view.View.draw(View.java:18847)\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18060)\r\n                                                                       at android.view.View.draw(View.java:18847)\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)\r\n                                                                       at android.view.View.draw(View.java:19122)\r\n                                                                       at com.android.internal.policy.DecorView.draw(DecorView.java:785)\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18069)\r\n                                                                       at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:643)\r\n                                                                       at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:649)\r\n                                                                       at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:757)\r\n                                                                       at android.view.ViewRootImpl.draw(ViewRootImpl.java:2980)\r\n                                                                       at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2794)\r\n                                                                       at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2347)\r\n                                                                       at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1386)\r\n                                                                       at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6733)\r\n                                                                       at android.view.Choreographer$CallbackRecord.run(Choreographer.java:911)\r\n                                                                       at android.view.Choreographer.doCallbacks(Choreographer.java:723)\r\n                                                                       at android.view.Choreographer.doFrame(Choreographer.java:658)\r\n                                                                       at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:897)\r\n                                                                       at android.os.Handler.handleCallback(Handler.java:789)\r\n                                                                       at android.os.Handler.dispatchMessage(Handler.java:98)\r\n                                                                       at android.os.Looper.loop(Looper.java:164)\r\n                                                                       at android.app.ActivityThread.main(ActivityThread.java:6541)\r\n                                                                       at java.lang.reflect.Method.invoke(Native Method)\r\n                                                                       at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\r\n                                                                       at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)\r\n                                                                    Caused by: java.io.FileNotFoundException: imagenet_comp_graph_label_strings.txt\r\n                                                                       at android.content.res.AssetManager.openAsset(Native Method)\r\n                                                                       at android.content.res.AssetManager.open(AssetManager.java:374)\r\n                                                                       at android.content.res.AssetManager.open(AssetManager.java:348)\r\n                                                                       at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:93)\r\n                                                                       at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:107)\u00a0\r\n                                                                       at org.tensorflow.demo.CameraActivity$2.onPreviewSizeChosen(CameraActivity.java:324)\u00a0\r\n                                                                       at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:407)\u00a0\r\n                                                                       at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:414)\u00a0\r\n                                                                       at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:64)\u00a0\r\n                                                                       at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:95)\u00a0\r\n                                                                       at android.view.TextureView.getHardwareLayer(TextureView.java:390)\u00a0\r\n                                                                       at android.view.TextureView.draw(TextureView.java:339)\u00a0\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18069)\u00a0\r\n                                                                       at android.view.View.draw(View.java:18847)\u00a0\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)\u00a0\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)\u00a0\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18060)\u00a0\r\n                                                                       at android.view.View.draw(View.java:18847)\u00a0\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)\u00a0\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)\u00a0\r\n                                                                       at android.view.View.draw(View.java:19122)\u00a0\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18069)\u00a0\r\n                                                                       at android.view.View.draw(View.java:18847)\u00a0\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)\u00a0\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)\u00a0\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18060)\u00a0\r\n                                                                       at android.view.View.draw(View.java:18847)\u00a0\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)\u00a0\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)\u00a0\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18060)\u00a0\r\n                                                                       at android.view.View.draw(View.java:18847)\u00a0\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:4214)\u00a0\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4000)\u00a0\r\n                                                                       at android.view.View.draw(View.java:19122)\u00a0\r\n                                                                       at com.android.internal.policy.DecorView.draw(DecorView.java:785)\u00a0\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:18069)\u00a0\r\n                                                                       at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:643)\u00a0\r\n                                                                       at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:649)\u00a0\r\n                                                                       at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:757)\u00a0\r\n                                                                       at android.view.ViewRootImpl.draw(ViewRootImpl.java:2980)\u00a0\r\n                                                                       at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2794)\u00a0\r\n                                                                       at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2347)\u00a0\r\n                                                                       at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1386)\u00a0\r\n                                                                       at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6733)\u00a0\r\n                                                                       at android.view.Choreographer$CallbackRecord.run(Choreographer.java:911)\u00a0\r\n                                                                       at android.view.Choreographer.doCallbacks(Choreographer.java:723)\u00a0\r\n                                                                       at android.view.Choreographer.doFrame(Choreographer.java:658)\u00a0\r\n                                                                       at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:897)\u00a0\r\n                                                                       at android.os.Handler.handleCallback(Handler.java:789)\u00a0\r\n                                                                       at android.os.Handler.dispatchMessage(Handler.java:98)\u00a0\r\n                                                                       at android.os.Looper.loop(Looper.java:164)\u00a0\r\n                                                                       at android.app.ActivityThread.main(ActivityThread.java:6541)\u00a0\r\n                                                                       at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n                                                                       at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\u00a0\r\n                                                                       at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)\u00a0\r\n`\r\nThe app has storage permissions enabled and was built on windows from the makefile, not Bazel.", "comments": ["any solutions for this?\r\n\r\n####UPDATE\r\n\r\nSolution Actually is what the exception is, Re-check label file list.", "Add READ_EXTERNAL_STORAGE permission in manifest. \r\n`<uses-permission android:name=\"android.permission.READ_EXTERNAL_STORAGE\"/>`\r\n\r\nLooks like this error is caused by `java.io.FileNotFoundException: imagenet_comp_graph_label_strings.txt`, but the missing text file exists in Android example. Just giving permission to READ the file solved the issue in my case."]}, {"number": 12697, "title": "During handling of the above exception. Another exception occured", "body": "![qq 20170829141338](https://user-images.githubusercontent.com/13164077/29851834-e8c4b112-8d68-11e7-971e-f29455d764d5.png)\r\n![qq 20170829141241](https://user-images.githubusercontent.com/13164077/29851833-e8c00a36-8d68-11e7-913b-82aafd793e75.png)\r\nDoes anyone know what is the cause of this?\r\nMy system environment\uff1aUbuntu_16.04_X86_64; nvidia dirver version:v384.66 nvcc_version:v8.0 cuDNN:6.0 .\r\nI dont know why.\r\nThank you before", "comments": ["The first exception describes itself as the process having run out of memory - this would be your root problem. Often everything else being dealt with after running out of memory is just trying to land a plane that is on fire.\r\n", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@quaeler @jart  ok\uff0cthank you. But I'm running with 8G of RAM maximum use 2G.And then it ends. I processed the file 15M\u3002Whether the TensorFlow itself causes an exception to be thrown", "@sdlmw This user also ran into your same issue: https://github.com/tensorflow/tensorflow/issues/10826\r\n\r\nPerhaps reach out to them for assistance?\r\n\r\nAnd also [this StackOverflow answer.](https://stackoverflow.com/questions/39076388/tensorflow-deep-mnist-resource-exhausted-oom-when-allocating-tensor-with-shape)", "Finally I found out that the graphics card memory is low. thanks @quaeler @jart "]}, {"number": 12696, "title": "Training model on Android ", "body": "I want to training my model on the Android mobile phones by using Tensorflow. How to train the model and save the model on Android mobile phone? I can not find corresponding functions on Java API of TensorFlow.\r\n\r\nIs it possible to find a way to implement such functions on android mobile phone? maybe tensorflow on Android with Python bindings? Or other Deep learning framework\uff1f", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Hi @humanlost Have u figured out how to train a model on Android?", "This was answered in #17328 ", "@humanlost @xumengwei This is something that is possible from Java with the [JavaCPP Presets for TensorFlow](https://github.com/bytedeco/javacpp-presets/tree/master/tensorflow) because the C++ API of TensorFlow supports training, and it works on Android too, as @rajatmonga pointed out."]}, {"number": 12695, "title": "Bundling tensorflow app to desktop", "body": "I have got a tensorflow based app which recognizes the objects state. I use opencv and tensorflow to get this done. The final application has to be a desktop app. How do I bundle all dependencies and export it for desktop?\r\n\r\n**PS:** I have asked the same question in stackoverflow with no response for my question hence Im asking this here hoping for a positive reply", "comments": ["You may give a try on \"libtensorflow\", it has no extra dependency.", "@snnn can you please share the github url for the same", "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/lib_package", "it again requires bazel for installation. How can it be bundled with the bazel dependency? Any working example will help a lot", "I think he is suggesting you build `libtensorflow.so` (which you can build via `bazel build --config=opt //tensorflow:libtensorflow.so`)  and then link that to your desktop application.\r\n\r\nOnce you've built the library, you no longer need involve bazel if you don't want to use it with your desktop application.", "so after building tensorflow:libtensorflow.so, I have to write wrapper around it if I have to interface it with other languages?", "Yes, for example if you were writing an application in Scala, you'd need to write the native interface layer yourself for the wrapper around that. What language are you using?\r\n", "Im using python... i want them to install it without downloading tensorflow with pip...  im creating a standalone desktop app written in python ", "if you can provide a sample code from github, it will be a great help for me.", "Hmm... you don't want to install tensorflow via pip at all? (In other words, not from a public repository and not using a locally built `.whl` archive?)\r\n\r\nI cannot think of a \"clean\" way to deploy a desktop app written in Python in this situation that does not involve a package manager.\r\n\r\nI would believe that there is a way - like building the `//tensorflow/tools/pip_package:build_pip_package` target and then selectively copying all of the generated library and python files from the `bazel-out` directory tree to a development directory structure for your own app - but that would require a knowledge of the python subsystem that i don't have.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@jart if I ask this question in stackoverflow, people are redirecting me to github repo. if i ask this in github, people are redirecting me to stackoverflow. I have asked it [already](https://stackoverflow.com/questions/45931097/bundle-tensorflow-for-desktop) in stackoverflow but no answers so far", "@quaeler let me try that ", "Hi! \r\nI've the same problem right now.\r\nI need to connect my tensorflow ML model to the desktop application written in python.\r\nCould you share your experience with me? :) "]}, {"number": 12694, "title": "queue skips first few elements under multiple enqueue threads", "body": "### Possible problems\r\nI want to use `tf.slice_input_producer` to produce a list of filenames, and then use multiple threads to load data and feed it to a `tf.FIFOQueue`. It seems first few elements have been skipped unexpectedly.\r\n\r\nIt only happens when there exist multiple enqueue threads.\r\n\r\nI have searched the web, and only find one similar question on `stackoverflow.com` with zero answer.\r\nhttps://stackoverflow.com/questions/44725917/tensorflow-train-batch-skip-3-examples\r\n\r\nSorry for creating this issue. Maybe I missed something. However, I really feel deeply puzzled about this and tried a lot to solve this.\r\n\r\n### Minimum reproducible example\r\n```python\r\nimport tensorflow as tf\r\n\r\na, = tf.train.slice_input_producer([tf.range(100)], shuffle=False)\r\nq = tf.FIFOQueue(32, [tf.int32], shapes=[[]])\r\ntf.train.queue_runner.add_queue_runner(\r\n    tf.train.queue_runner.QueueRunner(q, [q.enqueue([a])] * 8)\r\n)\r\nwith tf.Session() as sess:\r\n    tf.train.start_queue_runners(sess=sess)\r\n    for _ in range(100):\r\n        print(sess.run(q.dequeue()))\r\n```\r\n\r\n### Expected output\r\nSome reordering may take place, but roughly, the programs count from 0 to 99.\r\n\r\n### Actual output\r\n```\r\n7\r\n8\r\n9\r\n...\r\n96\r\n97\r\n98\r\n99\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n```\r\n\r\n### System information\r\nMy TensorFlow is installed from `pip install tensorflow-gpu --user` the day before yesterday.\r\n* System: ArchLinux x86_64\r\n* CUDA: V8.0.61\r\n* Python: 3.6.2\r\n* GPU: GTX 860M (2G memory)\r\n* CPU: i7-4710MQ (4 cores 8 threads)\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux archlinux-sun 4.12.8-2-ARCH #1 SMP PREEMPT Fri Aug 18 14:08:02 UTC 2017 x86_64 GNU/Linux\r\nLSB_VERSION=1.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 7.1.1 20170630\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux archlinux-sun 4.12.8-2-ARCH #1 SMP PREEMPT Fri Aug 18 14:08:02 UTC 2017 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.3.2)\r\ntensorflow (1.3.0)\r\ntensorflow-gpu (1.3.0)\r\ntensorflow-tensorboard (0.1.5)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/lib/nvidia:/usr/lib32/nvidia:/usr/lib:/usr/lib32:/usr/lib:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Aug 30 08:42:01 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.59                 Driver Version: 384.59                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 860M    Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   53C    P0    N/A /  N/A |      5MiB /  2002MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      4172    G   /usr/lib/xorg-server/Xorg                        4MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n```", "comments": ["Because queues are multithreaded, you have no guarantees about ordering, at all.  This particular case seems to fit within the \"vagaries of concurrency on your particular system\".  If you want proper ordering, use tf.contrib.data primitives in tf 1.3.  they're much easier to use and provide reproducible sequences."]}, {"number": 12693, "title": "TensorBoard executed stuck", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro 1703\r\n- **TensorFlow installed from (source or binary)**: install binary with GPU version by pip based on Python 3.5.2\r\n- **TensorFlow version (use command below)**:('1.3.0')\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:none\r\n- **CUDA/cuDNN version**:cuda v8.0/cuDNN v6,0\r\n- **GPU model and memory**:GeForce GTX 1080 Ti 11GB\r\n\r\n### Describe the problem\r\nI am trying to open TensorBoard after running mnist_with_summaries.py source code from the TensorBoard tutorials (https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py). However, \r\nAfter I run tensorboard --logdir=/tmp/tensorflow/mnist in cmd, it stuck and did not showing anything else.  I try to use tensorboard --logdir=/tmp/tensorflow/mnist --debug and have the same result. \r\n![capture](https://user-images.githubusercontent.com/16062793/29848367-ae4c35b0-8ce5-11e7-8e9e-0ee44058fda3.PNG)\r\n\r\n", "comments": ["@DedongZhang any update? Did you figure out why the tensorboard stucks? I have the same problem. Windows 7, TF 1.3. \r\nIn my case, cmd does not shown the usual message to let me know that I see the result on Port XXXX but if I go to my localhost, I can see that the scalars are updating anyway.", "@fastlater not yet, I try to use another pc, and the same thing happens. ", "@DedongZhang \r\nDid you tried ctrl+c to interrupt the execution?\r\nIf the execution stop, it means it was running. So, if it was running, it should be displaying the results.\r\nWhile running, open a tab in google chrome and type http://admin-pc:6006/ or localhost:6006.\r\nIf in scalars or graph are not displayed, maybe your graph or logdir is incorrect.\r\nAlso, try to update tensorflow or tensorboard. You never know when your antivirus or windows itself delete a small file.\r\nI am not an expert but when I face problems with tensorflow, I just try all the possibilities.", "@fastlater it is working now. I try to open by type http://admin-pc:6006/ but never try localhost:6006. I guess my cmd some how just not show result message. I could open it by localhost:6006 now.Thank you very much!", "From what I understand, TensorBoard isn't showing logs output, but if you go to the web address, it still shows the GUI.\r\n\r\nWe recently made some changes to the logging code, that has special cases for Windows.\r\n\r\nIs there any chance you could try locally modifying this piece of code right here: https://github.com/tensorflow/tensorboard/blob/master/tensorboard/util.py#L282 where it says `os.name != 'nt'`? We might need to update this, or something related.", "We've now pushed the [0.1.6](https://github.com/tensorflow/tensorboard/releases/tag/0.1.6) release to [PyPi](https://pypi.python.org/pypi/tensorflow-tensorboard/0.1.6) which should fix this issue. Please run:\r\n\r\n```sh\r\npip install --upgrade tensorflow-tensorboard\r\n```", "first pip uninstall tensorflow-tensorboard and then pip install tensorboard.\r\nThis will work"]}, {"number": 12692, "title": "Speech Commands Example URL broken", "body": "Found here:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands#speech-commands-example\r\n\r\nhttp://tensorflow.org/tutorials/audio_recognition is broken", "comments": ["The audio recognition tutorial has only been added recently:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/tutorials/audio_recognition.md\r\n\r\nI think the link will show up once `http://tensorflow.org` has been updated (from rebuilding `docs_src`).", "That makes sense. Thanks @yongtang."]}, {"number": 12691, "title": "Addn mklml kernel", "body": "This branch contains the implementations for Intel MKL AddN kernel.\r\nWhile the number of inputs are two, tf.add_n will call the MKL AddN kernel if compiled with MKL.", "comments": ["Can one of the admins verify this patch?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "Waiting for MacOS test rerun in http://ci.tensorflow.org/job/tensorflow-pull-requests-mac/6169/", "@martinwicke it seems that the rerun MacOS cpu test is all passed this morning\r\n\"Executed 1193 out of 1193 tests: 1193 tests pass.\"\r\n\r\nCould you please let me know if there are any other things that is needed to be done before merge? Thanks.", "@tensorflow-jenkins test this please"]}, {"number": 12690, "title": "Try compile for raspbery pi and got graph.pb.h missing", "body": "tensorflow/core/framework/graph.pb.h: No such file or directory\r\n #include \"tensorflow/core/framework/graph.pb.h", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 12689, "title": "multi-GPU training too slow when L2 regularizer enabled ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 home edition\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.3.0\r\n- **Python version**: \r\n3.5.3\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nCuda 8.0 / cudnn 6.0\r\n- **GPU model and memory**:\r\nNvidia 1080 GTX 8GB x 2\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI try to train a face recognition classifier using inception-resnet-v1 model with 2 GPUs. On single GPU the training proceeds just fine, with a processing capacity around 220 images/sec, but when I train with 2 GPUs I only observe marginal benefit (capacity increases to around 280 images/sec). After some profiling I found out that the poor performance was somehow due to the introduced L2 regularizer. When the regularizer is enabled, the computation of the gradient becomes unexpectedly slow, resulting in the slowdown of the entire training cycle. As a comparison, if the regularizer is disabled, I could obtain a processing capacity around 350 images/sec, which although not perfect, is more or less satisfactory. The boost in the performance in the latter scenario cannot be attributed to the reduced computation complexity from the removal of the regularizer. This is evidenced by a reference experiment with exactly the same parameter except for on a single GPU, see below. \r\n\r\nI cannot figure out an explanation for this. It took me several days to narrow down the problem to, seemingly,  the introduction of L2 regularizer and the computation of gradient.\r\n\r\nIn my program I used standard tf.slim layers together with a downloaded inception-resnet-v1 model script.  The main part of the code is the following:\r\n\r\n```python\r\ndef main(args):\r\n    num_gpus = args.num_gpus\r\n    batch_size_per_gpu = args.batch_size_per_gpu\r\n    batch_size = batch_size_per_gpu * num_gpus\r\n\r\n    num_classes = 10000\r\n    synthetic_images = 0.01 * np.random.randn(batch_size, 160, 160, 3)\r\n    synthetic_labels = np.random.randint(0, 10000, batch_size)\r\n\r\n    with tf.Graph().as_default(), tf.device('/cpu:0'):\r\n        # the synthetic array is converted to a tf.Variable\r\n        images = tf.Variable(tf.convert_to_tensor(synthetic_images, dtype=tf.float32), trainable=False)\r\n        labels = tf.Variable(tf.convert_to_tensor(synthetic_labels, dtype=tf.int32), trainable=False)\r\n\r\n        image_batches = tf.split(images, num_or_size_splits=num_gpus)\r\n        label_batches = tf.split(labels, num_or_size_splits=num_gpus)\r\n\r\n        total_loss = [None] * num_gpus\r\n        grads = [None] * num_gpus\r\n\r\n        opt = tf.train.RMSPropOptimizer(0.01, epsilon=0.01)\r\n\r\n        reuse_variables = False\r\n        for i in range(num_gpus):\r\n            with tf.device(\"/gpu:{0}\".format(i)), tf.name_scope(\"tower{0}\".format(i)) as scope:\r\n                with slim.arg_scope([slim.variable], device='/cpu:0'):\r\n                    print(\"Building graph for tower{0}...\".format(i))\r\n\r\n                    total_loss[i] = tower_loss(images=image_batches[i],\r\n                                               labels=label_batches[i],\r\n                                               num_classes=num_classes,\r\n                                               keep_probability=1,\r\n                                               phase_train=True,\r\n                                               embedding_size=512,\r\n                                               weight_decay=args.weight_decay,\r\n                                               reuse=reuse_variables)\r\n\r\n                    grads[i] = opt.compute_gradients(total_loss[i])\r\n                    reuse_variables = True\r\n\r\n        # then open a session and calculate the tower gradients\r\n        with tf.Session() as sess:\r\n            sess.run(init)\r\n            print(\"Warming up...\")\r\n            for i in range(10):\r\n                _ = sess.run(grads)\r\n\r\n            print(\"Benchmarking...\")\r\n            for i in range(num_iterations):\r\n                _ = sess.run(grads)\r\n```\r\nwhere `tower_loss()` is defined as follows:\r\n\r\n``` python   \r\ndef tower_loss(images, labels, num_classes, keep_probability, phase_train, embedding_size=128, weight_decay=0.0,\r\n               reuse=None):\r\n    batch_norm_params = {\r\n        'decay': 0.995,\r\n        'epsilon': 0.001,\r\n        'fused': True\r\n    }\r\n\r\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\r\n                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\r\n                        weights_regularizer=slim.l2_regularizer(weight_decay),\r\n                        normalizer_fn=slim.batch_norm,\r\n                        normalizer_params=batch_norm_params\r\n                        ):\r\n        embeddings = inception_resnet_v1(images, is_training=phase_train,\r\n                                         dropout_keep_prob=keep_probability,\r\n                                         bottleneck_layer_size=embedding_size,\r\n                                         reuse=reuse)\r\n\r\n    logits = slim.fully_connected(embeddings,\r\n                                  num_classes,\r\n                                  activation_fn=None,\r\n                                  reuse=reuse,\r\n                                  weights_regularizer=slim.l2_regularizer(weight_decay),\r\n                                  scope=\"fc\")\r\n    entropy_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\r\n    if weight_decay == 0:\r\n        print(\"L2 regularizer disabled.\")\r\n        return entropy_loss\r\n    else:\r\n        print(\"Regularization losses added to total loss.\")\r\n        return tf.add_n([entropy_loss] + tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\r\n```\r\n\r\nThe whole script (contains codes to generate synthetic data) can be found [here](https://github.com/TianwenWei/TianwenWei/blob/master/issue.py). The inception-resnet-v1 model script can be found [here](https://github.com/TianwenWei/TianwenWei/blob/master/inception_resnet_modified.py).\r\n\r\nTo reproduce the problem, just run \r\n`python issue.py --num_gpus 2  --weight_decay 0\r\n`\r\nThe output would be something like\r\n```\r\nDuration: 7.1 secs; 358.1 images/sec or 0.71 sec/batch\r\nDuration: 7.5 secs; 343.1 images/sec or 0.75 sec/batch\r\nDuration: 7.2 secs; 354.9 images/sec or 0.72 sec/batch\r\nDuration: 7.1 secs; 358.8 images/sec or 0.71 sec/batch\r\nDuration: 7.2 secs; 354.9 images/sec or 0.72 sec/batch\r\nTotal duration: 36.2sec        Performance: 353.9 images/sec\r\n```\r\nIn contrast, with the presence of L2 regularizer, running\r\n`python issue.py --num_gpus 2  --weight_decay 0.0001\r\n`\r\none would obtain\r\n```\r\nDuration: 9.0 secs; 284.9 images/sec or 0.90 sec/batch\r\nDuration: 9.1 secs; 279.9 images/sec or 0.91 sec/batch\r\nDuration: 8.7 secs; 292.8 images/sec or 0.87 sec/batch\r\nDuration: 9.3 secs; 276.7 images/sec or 0.93 sec/batch\r\nDuration: 8.5 secs; 302.4 images/sec or 0.85 sec/batch\r\nTotal duration: 44.6 sec        Performance: 287.0 images/sec\r\n```\r\n\r\nAs a reference, with single GPU, the difference of processing capacity between with and without regularizer is negligeable, at least in this demo. To check it, run\r\n`python issue.py --num_gpus 1  --weight_decay 0\r\n`\r\ngives\r\n```\r\nDuration: 5.6 secs; 228.1 images/sec or 0.56 sec/batch\r\nDuration: 5.6 secs; 228.7 images/sec or 0.56 sec/batch\r\nDuration: 5.7 secs; 225.8 images/sec or 0.57 sec/batch\r\nDuration: 5.6 secs; 230.3 images/sec or 0.56 sec/batch\r\nDuration: 5.6 secs; 226.8 images/sec or 0.56 sec/batch\r\nTotal duration: 28.1 sec        Performance: 227.9 images/sec\r\n```\r\nand\r\n`python issue.py --num_gpus 1  --weight_decay 0.0001\r\n`\r\ngives\r\n```\r\nDuration: 5.6 secs; 227.5 images/sec or 0.56 sec/batch\r\nDuration: 5.7 secs; 226.3 images/sec or 0.57 sec/batch\r\nDuration: 5.6 secs; 229.4 images/sec or 0.56 sec/batch\r\nDuration: 5.6 secs; 227.5 images/sec or 0.56 sec/batch\r\nDuration: 5.7 secs; 224.3 images/sec or 0.57 sec/batch\r\nTotal duration: 28.2 sec        Performance: 227.0 images/sec\r\n```\r\n\r\n", "comments": ["I think I have found the problem. To make sure that the Variables are stored in CPU, we need to use context manager\r\n`with slim.arg_scope([slim.model_variable, slim.variable], device='/cpu:0'):`\r\ninstead of \r\n`with slim.arg_scope([slim.variable], device='/cpu:0'):`\r\nAs a matter of fact, although `slim.model_variable` eventually calls `slim.variable` and variables in collection `sim.get_model_variables()` is a subset of those in collection `sim.get_variables()`, code\r\n`with slim.arg_scope([slim.variable], device='/cpu:0'):` does not set default device to cpu for variabels created by `sim.get_model_variables`.", "@TianwenWei \r\nThis is seems really strange, since as I'm looking into the slim implementations, it seems the regular weights and bias are created with slim.model_variable, and regularizer are slim.variable\r\nThough I'm 100% sure, I think all variables are created with their closest with device scope, which is individual gpu (and reuseable variables are pinned memory between each gpus and have a sync copy in the cpu?), therefore all operations are in theory performed within gpu. But your line is forcing these variables (thus operations) to happen in cpu.\r\n\r\nIn short, I think without the line\r\n`\r\nwith slim.arg_scope([slim.model_variable, slim.variable], device='/cpu:0'):\r\n`\r\nyour code should be more tf-correect and faster\r\nBut I'm not 100% sure, and really hope someone who knows these graph operations better can confirm on this."]}, {"number": 12688, "title": "Updating docs to show support for Python 3.6 in Windows. (#12687)", "body": "Cherrypicking into r1.3 for docs gen.", "comments": ["Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 12687, "title": "Updating docs to show support for Python 3.6 in Windows.", "body": "", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @girving and @Carmezim to be potential reviewers."]}, {"number": 12686, "title": "Feature Request: C++ gradient for SoftmaxCrossEntropyWithLogits", "body": "Implement the gradient for SoftmaxCrossEntropyWithLogits in c++ so that it is available for TF_AddGradients.\r\n\r\nThis is the python code that I believe would need to be ported:\r\nhttps://github.com/tensorflow/tensorflow/blob/4b2fb49fd7578afe7e289936f347af581b5bdab1/tensorflow/python/ops/nn_grad.py#L409\r\n", "comments": ["@bpiel you're working on this right?\r\n\r\n@suharshs FYI", "@skye Not currently. If I start, I will post here. If someone else starts first, hopefully they'll post here.", "Hi, I'm really interested in the work because it is a little challenge for me. #12391 of @bpiel seems like a perfect example to imitate, right? I could be more confident if more detailed guide are given or someone would like to shepherd it. Thanks.", "@facaiy Happy to review your PR. :) \r\n@kbsriram may also be interested in reviewing/guiding.", "Thanks, @suharshs .\r\n\r\nI have read the code in python side,\r\n```python\r\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\r\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_loss, grad_grad):\r\n```\r\n\r\nIf I understand correctly, the arguments `grad_loss` and `grad_grad` in python side will be packed in `grad_inputs` in C++ side, right?\r\n\r\nNamely, the outline of code in C++ should like:\r\n```c++\r\nStatus SoftmaxCrossEntropyWithLogitsGrad(const Scope& scope, const Operation& op,\r\n                                         const std::vector<Output>& grad_inputs,\r\n                                         std::vector<Output>* grad_outputs) {\r\n  auto grad_loss = grad_inputs[0];\r\n  auto grad_grad = grad_inputs[1];\r\n\r\n  // migrate the implementation in python here.\r\n  // code...........\r\n  // auto grad = .....\r\n\r\n  grad_outputs->push_back(grad);\r\n  grad_outputs->push_back(null);\r\n  return scope.status();\r\n}\r\nREGISTER_GRADIENT_OP(\"SoftmaxCrossEntropyWithLogitsGrad\", SoftmaxCrossEntropyWithLogitsGrad);\r\n```\r\n\r\nPlease correct me if I'm wrong.\r\n", "@facaiy the outline of the code looks fine, one change would be to use\r\n```c++\r\ngrad_outputs->push_back(NoGradient())\r\n```\r\nrather than `null` to indicate gradients should not be propagated.\r\n\r\nOne thing you may run into is that that python has various utilities (e.g. [tensor_util.constant_value()](https://github.com/tensorflow/tensorflow/tree/77b5f6a956c61e6ada9d4a6c61892f9bd2464fdb/tensorflow/python/framework/tensor_util.py#L695)) that don't yet have equivalents available in C++. Should you find yourself writing  a long stack of utilities, one strategy might be to start with gradients that have  fewer dependencies. Some options might be functions like [Erf](https://github.com/tensorflow/tensorflow/blob/77b5f6a956c61e6ada9d4a6c61892f9bd2464fdb/tensorflow/python/ops/math_grad.py#L450) and [LGamma](https://github.com/tensorflow/tensorflow/blob/77b5f6a956c61e6ada9d4a6c61892f9bd2464fdb/tensorflow/python/ops/math_grad.py#L471) in `math_ops`. You can look at c++ implementations like [Tanh](https://github.com/tensorflow/tensorflow/blob/8624ecc9e827a40f9b514ff6b8ed925390ca79cc/tensorflow/cc/gradients/math_grad.cc#L191) to see how to structure the code and tests.", "re: @kbsriram's point about missing Python utilities, for SoftmaxCrossEntropyWithLogitsGrad, you can skip the tensor_util.constant_value() call and leave a TODO to implement it in C++. This is OK in this case since the IsZero() check is an optimization anyway (I think...).", "@kbsriram @skye Thanks for your enlightening and detailed guide. I agree that it's better to start with easier task at first. Hence, I'd like to work on `LGamma` and `Erf` as suggested.\r\n\r\nIf someone else is interested in `SoftmaxCrossEntropyWithLogitsGrad` as well, please let us know, and just go head and take over it. ", "I would like to take over this feature. What level of C++ expertise is needed. I'm a total C++ noob but am seeing this as an opportunity to learn the language as well as make a meaningful contribution. I was just checking at the python code and this line caught my eye\r\n`softmax = nn_ops.softmax(logits)`\r\nI presume the C++ equivalent of nn_ops.softmax(logits) is implemented somewhere - I couldn't find out where. Is this correct ? Can someone please point out where to find the equivalent c++ implementation for nn_ops.softmax. ", "@pbanavara [`tensorflow::ops::Softmax`](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/softmax) ?", "Hi @pbanavara, @facaiy is correct. We have functions for every op in both Python and C++. I would look at the C++ API documentation (https://www.tensorflow.org/api_guides/cc/guide, https://www.tensorflow.org/api_docs/cc/group/core) and existing gradient functions to get a feel for what they look like. Also make sure you can successfully build TensorFlow before you start writing code. Maybe I'm overly familiar with the code, but I don't think gradient functions should be too hard to start with. If SoftmaxCrossEntropyWithLogitsGrad proves to be too complicated you can also try a simpler one. Good luck, and looking forward to a PR!", "@facaiy @skye Thank you for the pointers. I will start working on this and will ping you guys once I have the tests working.", "Also, if you want instructions for adding the c++ gradient functions see here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/gradients/README.md", "@pbanavara This is coming up for me again. How's it going?", "@bpiel Going slow. I was down with a minor cervical spine injury. So couldn't type much. Will keep you posted. Sorry I couldn't update earlier.", "@pbanavara Very sorry to hear about your injury. I hope you're doing better. Updates appreciated. thanks!", "Hey guys - I got around to making some progress. Need some help and apologies for super noob questions. This is what I have done to port the Python code\r\n\r\n```\r\nStatus SoftmaxCrossEntropyWithLogitsGrad(const Scope& scope,\r\n                                          const Operation& op,\r\n                                          const std::vector<Output>&\r\n                                          grad_inputs,\r\n                                          std::vector<Output>* grad_outputs) {\r\n  // Softmax gradient with cross entropy logits function\r\n  // We multiply the backprop for cost with the gradients - op.output[1]\r\n  // There is no gradient for labels\r\n\r\n  auto softmaxGrad = op.output(1);\r\n  auto gradLoss = grad_inputs[0];\r\n  auto gradGrad = grad_inputs[1];\r\n\r\n  auto grad = Mul(scope, gradLoss, softmaxGrad);\r\n\r\n  // TODO Check if the grad is not zero\r\n\r\n  auto logits = op.input(0);\r\n  auto softmax = ops::Softmax(scope, logits);\r\n\r\n  auto prod = ops::MatMul(scope, gradGrad, softmax);\r\n  auto squeezeProd = ops::Squeeze(scope, prod);\r\n  auto fProd = Sub(scope, gradGrad, squeezeProd);\r\n  grad = Add(scope, grad, fProd);\r\n  grad_outputs->push_back(grad);\r\n  return scope.status();\r\n}\r\n```\r\nI get an error at the line\r\n\r\n`grad = Add(scope, grad, fProd);`\r\n\r\ntensorflow/cc/gradients/nn_grad.cc:74:8: error: no viable overloaded '='\r\n\r\nI have totally forgotten about operator overloading. Have I done some fundamental mistake in the code above or do I have to write a operator overloader for =. \r\n\r\nI don't think I can do something like \r\n`grad = grad + fProd;`", "Instead of assigning to `grad` which was used for `Mul`, if you assign a new variable (`auto new_var = Add(...)`) things should work. (but pick a better name than `new_var` :) )\r\n\r\nEdit: Removed incorrect statement to be less confusing :)", "Suharsh's suggestion is correct (create a new variable), but it's not because of Output (you can assign Outputs). Mul and Add are both classes, so when you call them, you're calling their constructors. See https://www.tensorflow.org/api_guides/cc/guide#operation_constructors. So you're creating a Mul variable, then trying to assign an Add to it. You need to create a new Add variable instead.\r\n\r\nAnd if you're wondering, no, Mul and Add (and all the other generated op classes) are not all subclasses of the same base class. So you might be further wondering, how is it legal to copy Mul and Add objects into vector<Output> grad_outputs? We clearly don't have an overloaded Output constructor for every generated op. The answer is that the generated op classes that have a single Output use an implicit Output conversion operator (see http://en.cppreference.com/w/cpp/language/cast_operator).\r\n\r\nAlso, definitely not a super noob question, no worries :)", "Oh i see, thanks for the correction @skye ", "Thank you @suharshs and @Skye. Adding a new variable did the job. Follow up question - Can you guys  please tell me what's the equivalent of eager/Context in C++ and where can I find the related API docs. I tried in vain searching through the documentation tree. ", "There is no such concept in C++, so you can ignore it.", "Cool, ok, I've got the build and test complete. Test fails so am looking into that. If I can't figure it out by tomorrow will ask you guys again. Thank you for all the help.", "Hey guys sorry I couldn't get the test to pass. Help needed. I just copied the test from the Softmax test and made changes. This is what I have\r\n\r\n```\r\nTEST_F(NNGradTest, SoftmaxCrossEntropyWithLogitsGrad) {\r\n  TensorShape shape({5,3});\r\n  auto x = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\r\n  auto l = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\r\n  auto y = SoftmaxCrossEntropyWithLogits(scope_, x, l).backprop;\r\n  RunTest(x, shape, y, shape);\r\n}\r\n```\r\nHere are my questions:\r\nI get a compilation error if I just use \r\n`auto y = SoftmaxCrossEntropyWithLogits(scope_, x, l)`\r\nSo I made an educated guess to use the backprop - is this wrong ?\r\nIs there any such thing as enabling debug flag for tests. In the log I just have a failure message. \r\n\r\nThanks again for all your help.", "What's the failure message?\r\n\r\nSoftmaxCrossEntropyWithLogits has two outputs, loss and backprops: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L2002\r\n\r\nRunTest() calls ComputeGradientError(), so I suspect you somehow need to incorporate both outputs in the gradient test. I'm not sure though. Maybe @suharshs knows more.", "@skye Am getting a segmentation fault. I tried both on Ubuntu and Mac, same result.\r\n\r\n```\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n-----------------------------------------------------------------------------\r\nRunning main() from test_main.cc\r\n[==========] Running 12 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 12 tests from NNGradTest\r\n[ RUN      ] NNGradTest.SoftmaxGrad\r\n2017-11-05 06:53:43.521514: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n[       OK ] NNGradTest.SoftmaxGrad (163 ms)\r\n[ RUN      ] NNGradTest.SoftmaxCrossEntropyWithLogitsGrad\r\nSegmentation fault (core dumped)\r\n```", "Ah. Have you used gdb before? I suggest getting a stacktrace, and you can also poke around and find out more about what's causing the error. To use gdb:\r\n1. Build the test in debug mode by using `bazel build -c dbg tensorflow/cc/gradients_nn_grad_test`\r\n2. Open the test binary in gdb: `gdb bazel-bin/tensorflow/cc/gradients_nn_grad_test`\r\n3. Once you get the gdb prompt, run the test in gdb: `(gdb) r`\r\n4. It should automatically break when it hits the segfault. Once this happens, use `bt` to print the stacktrace. You can also inspect variables, etc. I suggest looking online for a basic tutorial if you haven't used gdb before.\r\n\r\nHope that helps, feel free to post anything you find here and I can try to help you more.", "Apologies for the delayed response, you don't need to use the `.backprop` in your test. You just write the usage of the op as normal. The gradient checker will add gradients and verify that they match the numerical gradient. \r\n\r\nHere is an example of another test in that same file that does this. The only thing you need to make sure of is that the x shape and y shape match what SoftMaxWithCrossEntropyLogits does, which i believe you have correct.\r\n\r\n```\r\nTEST_F(NNGradTest, Conv2DGrad) {\r\n  TensorShape shape({1, 2, 2, 1});\r\n  auto x = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\r\n  Tensor filter = test::AsTensor<float>({0.5f}, {1, 1, 1, 1});\r\n  const std::vector<int> strides{1, 1, 1, 1};\r\n  auto y = Conv2D(scope_, x, filter, strides, \"SAME\");\r\n  RunTest(x, shape, y, shape);\r\n}\r\n```", "You will also need to use the last override of RunTest which can handle multiple outputs and inputs.\r\n\r\n```\r\nvoid RunTest(const OutputList& xs, const std::vector<TensorShape>& x_shapes,\r\n           const OutputList& ys, const std::vector<TensorShape>& y_shapes) \r\n```", "Hey @suharshs and @skye Thanks and sorry for the delayed response. I haven't gone cold on this - still working. Trying to run the correct RunTest variant. Will keep you posted here soon.", "Ok did some amateurish stuff ... \r\n\r\n```\r\n  OutputList inputs = OutputList();\r\n  std::vector<TensorShape> shapes = std::vector<TensorShape>();\r\n  OutputList outputs = OutputList();\r\n  inputs.push_back(x);\r\n  shapes.push_back(shape);\r\n  auto y = SoftmaxCrossEntropyWithLogits(scope_, x, l);\r\n  outputs.push_back(y);\r\n  RunTest(inputs, shapes, outputs, shapes);\r\n\r\n```\r\nI get this error \r\n\r\n`candidate function not viable: no known conversion from 'tensorflow::ops::SoftmaxCrossEntropyWithLogits' to 'const value_type' (aka 'const tensorflow::Output') for 1st argument`\r\n\r\nAt the line where I am adding the outputs to the OutputList vector. Since the RunTest variant requires an OutputList I did it this way. Perhaps there is another way to do this ? \r\nSoftmaxCrossEntropyWithLogits call returns an Output as per the documentation. How it returns multiple outputs is not clear to me. Can you guys please help out with that ?", "It's a little confusing, but SoftmaxCrossEntropyWithLogits has two outputs, 'loss' and 'backprop'. These are public properties of the returned SoftmaxCrossEntropyWithLogits instance (see https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/softmax-cross-entropy-with-logits), so can be accessed in your code via `y.loss` and `y.backprop`. Both of these should be added the `outputs` list I think.", "Thanks @skye That solved the error. I had tried this before but had called loss and backdrop as methods :). I overlooked the fact that they are just attributes.  \r\nNow the output shapes are not matching. I am trying to fix that. The error messages are quite hard to interpret.\r\n```\r\n[ RUN      ] NNGradTest.SoftmaxCrossEntropyWithLogitsGrad\r\ntensorflow/cc/gradients/nn_grad_test.cc:55: Failure\r\n      Expected: ::tensorflow::Status::OK()\r\n      Which is: OK\r\nTo be equal to: ((ComputeGradientError<float, float, float>( scope_, xs, x_shapes, ys, y_shapes, &max_error)))\r\n      Which is: Invalid argument: Dimensions must be equal, but are 3 and 5 for 'MatMul' (op: 'MatMul') with input shapes: [5,3], [5,3].\r\n```\r\nSo clearly one of the matrices need to be of shape [3,5] but which one. So I am just trying all combinations  in the inputs and outputs. ", "Could this be an actual bug in your SoftmaxCrossEntropyWithLogits function? If you can't find the bug, post what you have so far in a PR and I can try to take a look.", "Quite possibly it\u2019s a bug in my code. Am investigating the same and will\r\npost a PR soon\r\n\r\nOn Wed, 15 Nov 2017 at 06:47, Skye Wanderman-Milne <notifications@github.com>\r\nwrote:\r\n\r\n> Could this be an actual bug in your SoftmaxCrossEntropyWithLogits\r\n> function? If you can't find the bug, post what you have so far in a PR and\r\n> I can try to take a look.\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/12686#issuecomment-344454094>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AAkn9lvBb8GIrd3iVYros2-0sEJ93wjJks5s2jvDgaJpZM4PGLqj>\r\n> .\r\n>\r\n", "@skye Have submitted a PR. I will keep debugging the test case fail issue in the interim. Please let me know if you find some obvious mistake.", "@skye - Revisiting this after a long gap. Sorry but I am unable to process the matrix shape mismatch error. Here is my test case\r\n```\r\n\r\nTEST_F(NNGradTest, SoftmaxCrossEntropyWithLogitsGrad) {\r\n  TensorShape shape({5,3});\r\n  TensorShape lossShape({5});\r\n  auto x = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\r\n  auto l = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\r\n  OutputList inputs = OutputList();\r\n  OutputList outputs = OutputList();\r\n  std::vector<TensorShape> inputShapes = std::vector<TensorShape>();\r\n  std::vector<TensorShape> outputShapes = std::vector<TensorShape>();\r\n  inputShapes.push_back(shape);\r\n  outputShapes.push_back(shape);\r\n  outputShapes.push_back(lossShape);\r\n  auto y = SoftmaxCrossEntropyWithLogits(scope_, x, l);\r\n  inputs.push_back(x);\r\n  outputs.push_back(y.backprop);\r\n  outputs.push_back(y.loss);\r\n  RunTest(inputs, inputShapes, outputs, outputShapes);\r\n}\r\n```\r\nThe shapes of backprop and loss seem to be appropriate - {5,3} and {5}. There is no way for me to print the shapes of the loss and backprop as the Output data structure doesn't have those APIs. Can you please advise what's wrong ? Here's the error. \r\n\r\n```\r\nTo be equal to: ((ComputeGradientError<float, float, float>( scope_, xs, x_shapes, ys, y_shapes, &max_error)))\r\n      Which is: Invalid argument: Shape must be rank 2 but is rank 1 for 'MatMul' (op: 'MatMul') with input shapes: [5], [5,3].\r\n```\r\nHere's my code:\r\n\r\n```\r\nStatus SoftmaxCrossEntropyWithLogitsGrad(const Scope& scope,\r\n                                          const Operation& op,\r\n                                          const std::vector<Output>&\r\n                                          grad_inputs,\r\n                                          std::vector<Output>* grad_outputs) {\r\n  // Softmax gradient with cross entropy logits function\r\n  // We multiply the backprop for cost with the gradients - op.output[1]\r\n  // There is no gradient for labels\r\n\r\n  std::cout << \"Step 1\" << \"\\n\";\r\n  auto softmaxGrad = op.output(1);\r\n  auto gradLoss = grad_inputs[0];\r\n  auto gradGrad = grad_inputs[1];\r\n  auto tempGrad = Mul(scope, gradLoss, softmaxGrad);\r\n\r\n  // TODO: Check if this sufficient, need a check for null ?\r\n  if (gradGrad.op().output_type(0) != 0) {\r\n          auto logits = op.input(0);\r\n          auto softmax = ops::Softmax(scope, logits);\r\n          auto prod = ops::MatMul(scope, gradGrad, softmax);\r\n          auto squeezeProd = ops::Squeeze(scope, prod);\r\n          auto fProd = Sub(scope, gradGrad, squeezeProd);\r\n          auto grad = Add(scope, tempGrad, fProd);\r\n          grad_outputs->push_back(grad);\r\n          grad_outputs->push_back(ops::MatMul(scope, gradLoss, ops::LogSoftmax(scope, logits)));\r\n          grad_outputs->push_back(NoGradient());\r\n  }\r\n  return scope.status();\r\n}\r\n```", "Hey @pbanavara, just a heads up that I'm pretty slammed this week so I can't look into this right now :( Sorry to keep you hanging. Maybe someone else cc'd can take a look, otherwise I'll try to make some time next week.", "Hey @skye no worries at all. please comment when you can.", "I have added to the conversation surrounding this issue in the pull request pbanavara made. I am [cross commenting](https://github.com/tensorflow/tensorflow/pull/14727#issuecomment-377968097]) here just so readers can find the results of the discussion."]}, {"number": 12685, "title": "Update rules_closure to 0.4.2", "body": "This change is required for compatibility with Bazel 0.6+", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 12684, "title": "Add TensorFlow User Group Utsunomiya into TensorFlow Communities Around the World.", "body": "We operate TensorFlow User Group Utsunomiya sponsored by GCP Community Support.\r\nPlease add to TensorFlow Communities list!", "comments": ["Can one of the admins verify this patch?", "Thanks a lot \ud83d\udc4d "]}, {"number": 12683, "title": "XLA leads to core dump", "body": "### System information\r\n\r\n[output of tf_env_collect.sh](http://paste.ubuntu.com/25424565/)\r\n\r\n#### Tensorflow\r\n\r\nTensorflow compiled from the source v1.3.0(9e76bf3)\r\n\r\nwith cuda, with xla, without mpi, without mkl\r\n\r\n#### OS\r\nCentOS 7\r\n\r\nout put of `uname -a`:\r\n\r\nLinux zhanghao 3.10.0-514.26.2.el7.x86_64 #1 SMP Tue Jul 4 15:04:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n#### python\r\nPython 2.7.13 |Intel Corporation| (default, Apr 27 2017, 15:33:46)\r\n\r\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux2\r\n\r\n#### Bezel\r\nBuild label: 0.5.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jun 27 13:27:03 2017 (1498570023)\r\nBuild timestamp: 1498570023\r\nBuild timestamp as int: 1498570023\r\n\r\n#### GPU\r\nCUDA 8.0 cuDNN 6.0.21\r\nGPU: GeForce GTX 950M\r\n\r\n### Describe the problem\r\ncore dump when use xla with gpu,\r\n\r\nBTW, if use cpu only, xla won't lead to core dump\r\n\r\n### Source code\r\nThis is code to reproduce the bug\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\nD = 2\r\nA = tf.random_normal(shape=[D, D, 2], dtype=tf.float32,name=\"A\")\r\nB = tf.random_normal(shape=[D, D, 2], dtype=tf.float32, name=\"B\")\r\nE = tf.ones(shape=[D], dtype=tf.float32, name=\"EBA\")\r\nH = tf.reshape(tf.constant([[0.25,0,0,0],[0,-0.25,0.5,0],[0,0.5,-0.25,0],[0,0,0,0.25]],\r\n                           dtype=tf.float32),[2,2,2,2],name=\"Hamiltonian\")\r\nEA = tf.multiply(A,tf.reshape(E,[D,1,1]))\r\nAB = tf.tensordot(EA,B,[[1],[0]],name=\"AB\")\r\nS, U, V = tf.svd(tf.reshape(AB,[2*D,2*D]))\r\nUU = tf.transpose(tf.multiply(tf.reshape(U[:,:D],[D,2,D]),tf.reshape(E,[D,1,1])),[0,2,1],name=\"nA\")\r\ndata = UU / tf.reduce_max(UU)\r\nconfig = tf.ConfigProto()\r\nif len(sys.argv)>1:\r\n    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\nsess = tf.Session(config=config)\r\nsess.run(tf.global_variables_initializer())\r\nprint sess.run(data)\r\n```\r\nsave as MPS.py and run `python MPS.py` and get:\r\n```\r\n2017-08-29 20:51:54.856662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-08-29 20:51:54.857232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\r\nname: GeForce GTX 950M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\r\npciBusID 0000:0a:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.92GiB\r\n2017-08-29 20:51:54.857292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\r\n2017-08-29 20:51:54.857301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\r\n2017-08-29 20:51:54.857312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0)\r\n[[[ 0.64684284 -0.48666194]\r\n  [-0.56722689 -0.19181968]]\r\n\r\n [[ 0.34154066  0.77098727]\r\n  [ 1.         -0.0881796 ]]]\r\n```\r\nand then run `python MPS.py -`, and get:\r\n```\r\n2017-08-29 20:52:18.127327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-08-29 20:52:18.127719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\r\nname: GeForce GTX 950M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\r\npciBusID 0000:0a:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.92GiB\r\n2017-08-29 20:52:18.127780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\r\n2017-08-29 20:52:18.127789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\r\n2017-08-29 20:52:18.127799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0)\r\n2017-08-29 20:52:18.319392: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-08-29 20:52:18.319435: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Executor present with 1 visible devices\r\n2017-08-29 20:52:18.319739: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-08-29 20:52:18.320615: I tensorflow/compiler/xla/service/service.cc:187] XLA service 0x7f59008c0350 executing computations on platform CUDA. Devices:\r\n2017-08-29 20:52:18.320640: I tensorflow/compiler/xla/service/service.cc:195]   StreamExecutor device (0): GeForce GTX 950M, Compute Capability 5.0\r\n2017-08-29 20:52:18.474432: F tensorflow/compiler/xla/util.cc:183] Check failed: p1.size() == p2.size() (3 vs. 0)\r\n[1]    14077 abort (core dumped)  python MPS.py -\r\n```", "comments": ["@learyg Could you take a look at this XLA assertion failure coredump?", "For what it is worth, I think I'm observing the same problem with our models. The bug seems to be present in both tensorflow 1.2.1 and 1.3.0, no matter how TF was compiled - I have tested with a couple of variants. CUDA 8, CuDNN 6 and 1080Ti.\r\n\r\nIf useful I can upload a frozen version of one of these models.", "Same here", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Sorry for the seriously long delay, will try to repro. It's also helpful if you use TF_XLA_FLAGS=\"--xla_dump_computations_to=/tmp/\" as a prefix on your cmdline and post the protobuf binary that appears as a result along with the git revision (the XLA compilation can be replayed directly with https://github.com/tensorflow/tensorflow/blob/e210cb140a60a74d5e9ce3bf9ebedb21b4910f1c/tensorflow/compiler/xla/tools/replay_computation.cc ).", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@jlebar @sanjoy do one of you have time to look into this issue?", "I am working on this issue.", "Cool, thanks @bixia1 !  I think you might need to join the TensorFlow github organization in order to assign this issue to you.\r\n\r\nIn the meantime I'll set \"contributions welcome\" to avoid our bot (tensorflowbutler) from nagging us.", "I am able to reproduce the problem and verify that the latest tensorflow source doesn't have this problem. It looks to me that the following single line change can fix the problem, would you please give this a try? My source file may not be exactly the same as yours, so please manually apply the change to routine ConstantFolding::SimplifyGraph.\r\n\r\n--- .../tensorflow/core/grappler/optimizers/constant_folding.cc\r\n+++ .../tensorflow/core/grappler/optimizers/constant_folding.cc\r\n@@ -1357,7 +1357,7 @@ Status ConstantFolding::SimplifyGraph(GraphDef* output,\r\n     const bool is_any_div = IsAnyDiv(*node);\r\n     // Simplify multiplication by ones or zeros, and addition/subtraction of\r\n     // zeros.\r\n --    if (is_aggressive && use_shape_info &&\r\n ++    if (safe_to_use_shapes &&\r\n         (is_mul || is_matmul || is_add || is_sub || is_any_div) &&\r\n         properties.HasInputProperties(node->name()) &&\r\n         properties.HasOutputProperties(node->name())) {\r\n\r\n", "well, lastest tensorflow doesn't have this problem for this code indeed. but the following code still reproduces the similar core dump.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\nD = 2\r\nA = tf.random_normal(shape=[D, D, 2], dtype=tf.float32,name=\"A\")\r\nB = tf.random_normal(shape=[D, D, 2], dtype=tf.float32, name=\"B\")\r\nif len(sys.argv)%2:\r\n    fun = tf.random_normal\r\nelse:\r\n    fun = tf.ones\r\nE = fun(shape=[D], dtype=tf.float32, name=\"EBA\")\r\nH = tf.reshape(tf.constant([[0.25,0,0,0],[0,-0.25,0.5,0],[0,0.5,-0.25,0],[0,0,0,0.25]],\r\n                           dtype=tf.float32),[2,2,2,2],name=\"Hamiltonian\")\r\nEA = tf.multiply(A,tf.reshape(E,[D,1,1]))\r\nAB = tf.tensordot(EA,B,[[1],[0]],name=\"AB\")\r\nS, U, V = tf.svd(tf.reshape(AB,[2*D,2*D]))\r\nUU = tf.transpose(tf.multiply(tf.reshape(U[:,:D],[D,2,D]),tf.reshape(E,[D,1,1])),[0,2,1],name=\"nA\")\r\ndata = UU / tf.reduce_max(UU)\r\nconfig = tf.ConfigProto()\r\nif len(sys.argv)>2:\r\n    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\nsess = tf.Session(config=config)\r\nsess.run(tf.global_variables_initializer())\r\nprint sess.run(data)\r\n```\r\n\r\nthe only difference is changing tf.ones to tf.random_normal.\r\n\r\nall of `python2 main.py`, `python2 main.py -`, `python2 main.py - - -` run correctly\r\nbut `python2 main.py - -` still core dump\r\n\r\ntf version: (56422034fe)", "Thanks for the bug report, @hzhangxyz.\r\n\r\nCan you please file a separate bug and cc me and bixia@?  I don't expect this is the same issue as before, even though it looks very similar from your end.\r\n\r\nIt would be helpful if you could include the new log output in the new bug.  The first one wasn't a proper core dump but was instead an assertion failure; those log messages are useful to us.\r\n\r\nThanks again."]}, {"number": 12682, "title": "modify lambda capture list, and prohibit copying objects.", "body": "1. capture `work_name, done` by reference to avoid copying object.\r\n2. capture pointer by value, and remove reference.\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Looking closer at the test failures, this pull request breaks the distributed runtime. Since it's not clear that there's a fix other than maintaining the status quo, I'm going to close this PR."]}, {"number": 12681, "title": "Feature request: dilated pooling", "body": "Hi,\r\n\r\nI posted initially in keras-users, but @fchollet suggested that this would need to be implemented first at TensorFlow level\r\n\r\nhttps://groups.google.com/forum/#!topic/keras-users/ZVtI8Ef6508\r\n\r\nSimilarly to dilated convolution, dilated pooling uses a dilated kernel. For example, dilated kernel with pool_size=(3,2), dilation_rate=(2,4) (* = non-zero elements: this is where values are sampled to compute max pooling)\r\n        \r\n        * 0 0 0 *\r\n        0 0 0 0 0\r\n        * 0 0 0 *\r\n        0 0 0 0 0\r\n        * 0 0 0 *\r\n\r\nThis was proposed in \r\n\r\nLi H, Zhao R, Wang X. Highly efficient forward and backward propagation of convolutional neural networks for pixelwise classification. arXiv preprint arXiv:14124526. 2014.\r\n\r\nand it's used by DeepCell\r\n\r\nVan Valen et al. (2016), PLoS Comput Biol, \"Deep Learning Automates the Quantitative Analysis of Individual Cells in Live-Cell Imaging Experiments\"\u200b http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005177\r\n\r\nDeepCell is state of the art for cell segmentation, but it's implemented with Keras 1, and ad-hoc code based on a bit outdated Theano.\r\n\r\nhttps://github.com/CovertLab/DeepCell\r\n\r\nTo the best of my knowledge, there's a lack of testing of whether dilation in pooling improves segmentation. I have written one in Keras, by shifting the kernel and slicing the image, then running regular 2D pooling on each slice, and reassembling the results (code at the end of post, but probably not very interesting). However, this approach is probably quite inefficient. \r\n\r\nThe question would be whether any TF developer would be willing to extend the current tf.contrib.keras.backend.pool2d\r\n\r\n```\r\npool2d(\r\n    x,\r\n    pool_size,\r\n    strides=(1, 1),\r\n    padding='valid',\r\n    data_format=None,\r\n    pool_mode='max'\r\n)\r\n\r\n```\r\n\r\nwith a `dilation_rate` argument, and the functionality at that level. From there, @fchollet is happy to modify the Keras API to include this option.\r\n\r\n\r\nCode for Keras implementation of dilated 2D pooling\r\n\r\n```\r\n    def _pooling_function(self, inputs, pool_size, strides,\r\n                          padding, data_format, dilation_rate):\r\n\r\n        # inputs for tests\r\n        inputs = K.variable(np.reshape(range(1,4*3*5*8+1), (4, 3, 5, 8)))########\r\n        inputs = K.variable(np.reshape(range(1,1*1*5*8+1), (1, 1, 5, 8)))########\r\n        \r\n        if data_format == 'channels_first': # (batch,chan,row,col)\r\n            nbatch = K.get_variable_shape(inputs)[0]\r\n            #nchan = K.get_variable_shape(inputs)[1]\r\n            nrows = K.get_variable_shape(inputs)[2]\r\n            ncols = K.get_variable_shape(inputs)[3]\r\n        elif data_format == 'channels_last': # (batch,row,col,chan)\r\n            nbatch = K.get_variable_shape(inputs)[0]\r\n            #nchan = K.get_variable_shape(inputs)[1]\r\n            nrows = K.get_variable_shape(inputs)[2]\r\n            ncols = K.get_variable_shape(inputs)[3]\r\n        else:\r\n            raise ValueError('Expected data format to be channels_first or channels_last')\r\n\r\n        # number of blocks to split the input into. Each dilation (row or \r\n        # column) goes into a separate block\r\n        nblocks = dilation_rate\r\n        \r\n        # size of each block we are going to split the input images in\r\n        block_sz = (int(np.ceil(nrows / dilation_rate[0])), \r\n                    int(np.ceil(ncols / dilation_rate[1])))\r\n\r\n        # pad inputs so that they can be split into equal blocks\r\n        padded_size = np.multiply(block_sz, nblocks)\r\n        padding_len = ((0, padded_size[0] - nrows), (0, padded_size[1] - ncols))\r\n        inputs = K.spatial_2d_padding(inputs, padding=padding_len, data_format=data_format)\r\n \r\n        # split the inputs into blocks\r\n        split_inputs = []\r\n        for row_offset in range(nblocks[0]):\r\n            for col_offset in range(nblocks[1]):\r\n                if data_format == 'channels_first': # (batch,chan,row,col)\r\n                    split_inputs = split_inputs + [inputs[:, :, row_offset::dilation_rate[0], col_offset::dilation_rate[1]]]\r\n                elif data_format == 'channels_last': # (batch,row,col,chan)\r\n                    split_inputs = split_inputs + [inputs[:, row_offset::dilation_rate[0], col_offset::dilation_rate[1], :]]\r\n        split_inputs = K.concatenate(split_inputs, axis=0)                        \r\n    \r\n        # pool each block without dilation\r\n        split_inputs = K.pool2d(split_inputs, pool_size, strides=(1,1),\r\n                                padding='same', data_format=data_format,\r\n                                pool_mode='max')\r\n\r\n        # reassemble blocks\r\n        output = np.zeros(shape=list(inputs.shape.eval()), dtype=inputs.dtype)\r\n        for idx in range(nbatch*nblocks[0]*nblocks[1]):\r\n            multi_index = np.unravel_index(idx, dims=(nblocks[0], nblocks[1], nbatch))\r\n            row_offset = multi_index[0]\r\n            col_offset = multi_index[1]\r\n            batch = multi_index[2]\r\n            if data_format == 'channels_first': # (batch,chan,row,col)\r\n                output[batch, :, row_offset::dilation_rate[0], col_offset::dilation_rate[1]] = split_inputs[idx, :, :, :].eval()\r\n            elif data_format == 'channels_last': # (batch,row,col,chan)\r\n                output[batch, row_offset::dilation_rate[0], col_offset::dilation_rate[1], :] = split_inputs[idx, :, :, :].eval()\r\n        output = K.variable(output)\r\n        \r\n        # remove padding\r\n        if padding == 'valid':\r\n            \r\n            if data_format == 'channels_first': # (batch,chan,row,col)\r\n                output = output[:, :, (pool_size[0]-1)*dilation_rate[0]:nrows, \r\n                                (pool_size[1]-1)*dilation_rate[1]:ncols]\r\n            elif data_format == 'channels_last': # (batch,row,col,chan)\r\n                output = output[:, (pool_size[0]-1)*dilation_rate[0]:nrows, \r\n                                (pool_size[1]-1)*dilation_rate[1]:ncols, :]\r\n                    \r\n        elif padding == 'same':\r\n        \r\n            if data_format == 'channels_first': # (batch,chan,row,col)\r\n                output = output[:, :, 0:nrows, 0:ncols]\r\n            elif data_format == 'channels_last': # (batch,row,col,chan)\r\n                output = output[:, 0:nrows, 0:ncols, :]\r\n                \r\n        else:\r\n            \r\n            raise NotImplementedError\r\n\r\n        # return tensor\r\n        return output\r\n\r\n```", "comments": ["@fchollet From what I understand, you're already in contact with our friend who submitted this feature request. How do you think this issue should be triaged?", "@jart we should not merge any implementation built on top of atomic TF ops, because its performance would be really bad. We would need to implement the change at the level of the pooling ops in C++/CUDA. I think this *could* be a worthwhile change, if we find someone who is interested in working on it, although there is little data to confirm this. ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Based on what I gather from Fran\u00e7ois' assessment, I'd say we need more feedback from the community on this particular feature request and want to thank you for bringing it to our attention. Until then I'd like to take it off of triage.", "@jart, it seems that dilated pooling is available in API r1.6?\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/pool\r\n\r\n`tf.nn.pool(\r\n    input,\r\n    window_shape,\r\n    pooling_type,\r\n    padding,\r\n    dilation_rate=None,\r\n    strides=None,\r\n    name=None,\r\n    data_format=None\r\n)`\r\n\r\n> `dilation_rate`: Optional. Dilation rate. List of N ints >= 1. Defaults to [1]*N. If any value of `dilation_rate` is > 1, then all values of strides must be 1."]}, {"number": 12680, "title": "Add eager_pip to simple_console_for_windows.zip", "body": "To fix unitest errors in tf-master-win-bzl build\r\n\r\nhttp://ci.tensorflow.org/job/tf-master-win-bzl/1491/console", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Testing this change on Windows: http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/37/console", "Hi @meteorcloudy  Could you please re-try it? Thanks.", "Retrying here: http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/38/console\r\nOnly three tests are broken now:\r\n```\r\n01:31:18 //py_test_dir/tensorflow/python/eager:backprop_test                      FAILED in 29.6s\r\n01:31:18   C:/tmp/_bazel_system/7rgg5n4w/execroot/org_tensorflow/bazel-out/msvc_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/eager/backprop_test/test.log\r\n01:31:18 //py_test_dir/tensorflow/python/kernel_tests:checkpoint_ops_test         FAILED in 18.8s\r\n01:31:18   C:/tmp/_bazel_system/7rgg5n4w/execroot/org_tensorflow/bazel-out/msvc_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/kernel_tests/checkpoint_ops_test/test.log\r\n01:31:18 //py_test_dir/tensorflow/python:saver_test                               FAILED in 42.6s\r\n01:31:18   C:/tmp/_bazel_system/7rgg5n4w/execroot/org_tensorflow/bazel-out/msvc_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/saver_test/test.log\r\n```", "ping @snnn ", "sorry, I'm trying to find out why backprop_test  was failed. Any suggestions? \r\n\r\nIndeed, I have another pull request #12296 ,  which **almost** contains all the changes of this. So, we can abandon this if that was approved. \r\n", "@asimshankar, since you added backprop_test, any idea why it's failing on Windows with:\r\n```\r\n01:30:24 FAIL: //py_test_dir/tensorflow/python/eager:backprop_test (see C:/tmp/_bazel_system/7rgg5n4w/execroot/org_tensorflow/bazel-out/msvc_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/eager/backprop_test/test.log)\r\n01:30:24 INFO: From Testing //py_test_dir/tensorflow/python/eager:backprop_test:\r\n01:30:24 ==================== Test output for //py_test_dir/tensorflow/python/eager:backprop_test:\r\n01:30:24 C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\r\n01:30:24   if d.decorator_argspec is not None), _inspect.getargspec(target))\r\n01:30:24 ...ss.........Ess.\r\n01:30:24 ======================================================================\r\n01:30:24 ERROR: testSecondGrad (__main__.BackpropTest)\r\n01:30:24 ----------------------------------------------------------------------\r\n01:30:24 Traceback (most recent call last):\r\n01:30:24   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\autograd\\core.py\", line 251, in vspace\r\n01:30:24     return vspace_mappings[type(value)](value)\r\n01:30:24 KeyError: <class 'tensorflow.python.eager.tensor_node.TensorNode'>\r\n01:30:24 \r\n01:30:24 During handling of the above exception, another exception occurred:\r\n01:30:24 \r\n01:30:24 Traceback (most recent call last):\r\n01:30:24   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\autograd\\errors.py\", line 48, in wrapped\r\n01:30:24     try: return fun(*args, **kwargs)\r\n01:30:24   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\autograd\\convenience_wrappers.py\", line 23, in gradfun\r\n01:30:24     vjp, ans = make_vjp(fun, argnum)(*args, **kwargs)\r\n01:30:24   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\autograd\\core.py\", line 12, in vjp_maker\r\n01:30:24     start_node, end_node = forward_pass(fun, args, kwargs, argnum)\r\n01:30:24   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\autograd\\core.py\", line 23, in forward_pass\r\n01:30:24     start_node = new_progenitor(args[argnum])\r\n01:30:24   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\autograd\\core.py\", line 145, in new_progenitor\r\n01:30:24     node = new_node(x,       (identity, (x,), {}, []      ), set())\r\n01:30:24   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\autograd\\core.py\", line 245, in new_node\r\n01:30:24     return node_type_mappings[type(value)](value, recipe, progenitors)\r\n01:30:24   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\autograd\\core.py\", line 160, in __init__\r\n01:30:24     self.vspace = vspace(value)\r\n01:30:24   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\autograd\\core.py\", line 251, in vspace\r\n01:30:24     return vspace_mappings[type(value)](value)\r\n01:30:24   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\autograd\\container_types.py\", line 66, in __init__\r\n01:30:24     self.shape = [vspace(x) for x in value]\r\n01:30:24   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\autograd\\container_types.py\", line 66, in <listcomp>\r\n01:30:24     self.shape = [vspace(x) for x in value]\r\n01:30:24   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\autograd\\core.py\", line 253, in vspace\r\n01:30:24     raise TypeError(\"Can't find vspace for type {}\".format(type(value)))\r\n01:30:24 TypeError: Can't find vspace for type <class 'tensorflow.python.eager.tensor_node.TensorNode'>\r\n```", "Hmm...that is most likely because the version of the `autograd` pip package installed on the test machine is old. I'm not familiar enough with how the Jenkins Windows machines get set up, perhaps we can chat off-thread on how to (a) update the version, and (b) ensure that updates to the required version happen automatically\r\n\r\n(FYI @alextp )", "This might be due to an old version of autograd installed on the Jenkins Windows slaves. The testing-infra folks can log in and find out whether that's the case. We use autograd==1.1.12 on Linux and Mac.\r\n\r\ncc @gunan, @av8ramit, @yifeif ", "@tensorflow-jenkins test this please", "Upgrading autograd from 1.1.11 to 1.1.12 on the Windows Jenkins resolved the issue. Merging PR now. @snnn, thanks for the contribution! We can fix the ordering later."]}, {"number": 12679, "title": "object_detection eval", "body": "When evaluating the model? tensorboard values is NaN why?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12678, "title": "Fixed code formatting", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 12677, "title": "Use corret SDK paths from xcrun", "body": "Closes #12650 ", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "I think I merged something similar. Sorry we sat on this for too long."]}, {"number": 12676, "title": "Tensorflow building error", "body": "I am trying to build tensorflow with this statement \"bazel build -c opt //tensorflow/examples/android:tensorflow_demo\" \r\n\r\nthe WORKSPACE is \r\n`android_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 23,    \r\n    #build_tools_version = \"25.0.2\",    \r\n\tbuild_tools_version = \"26.0.1\",\r\n    path = \"C:/Users/ST/AppData/Local/Android/Sdk\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n    path=\"C:/Users/ST/Downloads/Tensorflow_Compile/android-ndk-r12b\",    \r\n    api_level=14)\r\n`\r\n\r\nbut I got an error message:\r\n\"ERROR: C:/msys64/tmp/_bazel_st/_ztysx-6/external/protobuf_archive/BUILD:93:1: C++ compilation of rule '@protobuf_archive//:protobuf_lite' failed (Exit 3).\r\n\r\nThis application has requested the Runtime to terminate it in an unusual way.\r\nPlease contact the application's support team for more information.\r\nCannot create temporary file in C:\\WINDOWS\\: Permission denied\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n____Elapsed time: 5.465s, Critical Path: 0.63s\"\r\n\r\nI am using Windows 10, bazel 0.5.4, python 2.7. Is there anyway to fix this problem?", "comments": ["@petewarden will the android demo build work on Windows?", "According to the [doc for Android examples](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android), Windows build is not supported via Bazel:\r\n> NOTE: Bazel does not currently support building for Android on Windows. Full support for gradle/cmake builds is coming soon, but in the meantime we suggest that Windows users download the prebuilt binaries instead.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "what @damienstanton said."]}, {"number": 12675, "title": "Update __init__.py", "body": "-Add Python formatting for TF website\r\n-Refactor comment\r\n-Import statements are shorter than 80 chars, so pylint should not need to be suppressed here\r\n-Add python formatting to correct website formatting away newlines", "comments": ["@alanyee, thanks for your PR! By analyzing the history of the files in this pull request, we identified @dandelionmane to be a potential reviewer.", "Can one of the admins verify this patch?", "@martinwicke does the commit look good now?", "Jenkins, test this please.\r\n\r\n@martinwicke any thoughts on the new version?", "@drpngx @martinwicke any thoughts on the changes?", "Jenkins, test this please. ", "Hitting the permission denied. @gunan FYI it looks like we are seeing more of [this](https://ci.tensorflow.org/job/tensorflow-pull-requests-makefile/10811/) recently.\r\n\r\nJenkins, test this please.\r\n\r\n"]}, {"number": 12674, "title": "Same code, runs fine in one machine, ValueError in another. ", "body": "I am running a modified version of the LFADS code available at\r\n\r\nhttps://github.com/tensorflow/models/tree/master/lfads\r\n\r\nI run the exact same code in two machines, in one it works fine, in another it throws a ValueError. Here are the two tf_env.txt\r\n\r\n## MY MACHINE (CODE WORKS)\r\n== cat /etc/issue ===============================================\r\nDarwin Daniels-MacBook-Pro-2.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.6\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin Daniels-MacBook-Pro-2.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-gpu (1.1.0)\r\ntensorflow-tensorboard (0.1.2)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.1.0\r\ntf.GIT_VERSION = unknown\r\ntf.COMPILER_VERSION = unknown\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n## THE CLUSTER MACHINE (THROWS THE VALUE ERROR)\r\n== cat /etc/issue ===============================================\r\nLinux holmes 3.10.0-327.el7.x86_64 #1 SMP Thu Oct 29 17:29:29 EDT 2015 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7.3 (Maipo)\"\r\nVERSION_ID=\"7.3\"\r\nREDHAT_BUGZILLA_PRODUCT_VERSION=7.3\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7.3\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 6.1.0\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux holmes 3.10.0-327.el7.x86_64 #1 SMP Thu Oct 29 17:29:29 EDT 2015 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nnumpydoc (0.6.0)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.5)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /cm/shared/apps/slurm/16.05.8/lib64/slurm:/cm/shared/apps/slurm/16.05.8/lib64:/cm/local/apps/gcc/6.1.0/lib:/cm/local/apps/gcc/6.1.0/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./collect.sh: line 105: nvidia-smi: command not found\r\n\r\n\r\n### SOURCE CODE/ LOGS\r\n\r\nHere is the log for the error:\r\n\r\nTraceback (most recent call last):\r\n.....\r\n  File \"./lfads.py\", line 1014, in _optimization\r\n    grads = tf.gradients(self.cost, tvars)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 562, in gradients\r\n    in_grad.set_shape(t_in.get_shape())\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 378, in set_shape\r\n    self._shape = self._shape.merge_with(shape)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 566, in merge_with\r\n    raise ValueError(\"Shapes %s and %s are not compatible\" % (self, other))\r\nValueError: Shapes (50, ?) and (1,) are not compatible\r\n\r\n\r\nI have diffed the files involved, to double check that they are the same. Can someone point me to what can possibly be happening?\r\n", "comments": ["Can you give any more details of the modifications you made to the original?", "Hi, Michael, thanks for the response.\r\n\r\nIn fact, I just tried the unmodified LFADS code and got the same problem (working on my machine, nyet on the cluster). I am using the dataset below that I generated, only modifying the DATA_FILENAME_STEM global in the run_lfads.py file to\r\n\r\nDATA_FILENAME_STEM = 'dataset'\r\n\r\nand the globals indicating paths in my computer.\r\n\r\n[dataset_poissondata001.zip](https://github.com/tensorflow/tensorflow/files/1261988/dataset_poissondata001.zip)\r\n\r\nRunning run_lfads.py, yields a list of the variables to be optimized so you can see that the only variable with dimension 50 in shape[0] is the first one listed.\r\n", "@sussillo \r\nThe question is about https://github.com/tensorflow/models/tree/master/lfads", "It is my understanding that this bug was fixed in commit 44bdf29fcbae0c9d748c9761d1be270a917b3ff6 on July 18th by @allenlavoie. Have you updated the code since then?", "@sussillo Thanks, you are right. I was using a relatively old copy of LFADS I had stored in my local machine. I updated to the new version and it works fine now."]}]