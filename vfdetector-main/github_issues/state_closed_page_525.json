[{"number": 37983, "title": "Calling next with a default value on an exhausted Dataset iterator raises an OutOfRangeError in graph mode", "body": "\r\n**System information** \r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution:  Windows 10\r\n- TensorFlow installed from binary: 2.1.0\r\n\r\n**Describe the current behavior**\r\n`next(iterator, default)` is supposed to give the next element in the iterator or the value given as _default_ if the iterator is at the end.\r\nHowever, when using the above construction in a function with @tf.function, the default value is not returned and an error (tensorflow.python.framework.errors_impl.OutOfRangeError) is produced when trying to call _next_ on an iterator that is at the end.\r\nWhen running this code in eager mode, the default value is returned as expected.\r\n\r\n**Describe the expected behavior**\r\nIn graph mode the default value should be returned when at the end of an iterator.\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.convert_to_tensor([[1], [2], [3]])\r\nds = tf.data.Dataset.from_tensor_slices(x)\r\ndsi = iter(ds)\r\n\r\n\r\n@tf.function # remove this to get the expected behaviour\r\ndef func():\r\n    for _ in range(4):\r\n        tf.print(next(dsi, -1))\r\n\r\n\r\nfunc()\r\n```\r\nOutput (see below for a full stacktrace):\r\n```\r\n[1]\r\n[2]\r\n[3]\r\n2020-03-27 18:56:09.523946: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n```\r\n\r\nExpected output:\r\n```\r\n[1]\r\n[2]\r\n[3]\r\n-1\r\n\r\n```\r\n\r\n**Other info / logs**\r\n**Colab link:** https://colab.research.google.com/drive/1PBxoXiE48aC-bo-aY-Bau1Igt4Aj6OFy\r\n\r\n[stacktrace.txt](https://github.com/tensorflow/tensorflow/files/4395116/stacktrace.txt)\r\n", "comments": ["@hannesdm, I have tried after removing `@tf.function` decorator and got expected output.", "> \r\n> \r\n> @hannesdm, I have tried after removing `@tf.function` decorator and got expected output.\r\n\r\nYes, everything works as it should without `@tf.function`, the bug only occurs in graph mode i.e. **with** `@tf.function`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37983\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37983\">No</a>\n"]}, {"number": 37981, "title": "I get cultural errors when I import the Tensorflow library. Can you help me?", "body": "\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Muhammet\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\Muhammet\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Muhammet\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Muhammet\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Muhammet\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Muhammet/PycharmProjects/Tez/Sentiment_Analysis.py\", line 3, in <module>\r\n    from tensorflow.python.keras.models import Sequential\r\n  File \"C:\\Users\\Muhammet\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Muhammet\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Muhammet\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Muhammet\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\Muhammet\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Muhammet\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Muhammet\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Muhammet\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n### ", "comments": ["@BgMuhendis, This looks like a similiar of [#11571](https://github.com/tensorflow/tensorflow/issues/11571).", "@BgMuhendis, Can you fill the issue template by providing Tensorflow version, python version and other stuff to understand the root cause of this issue. Thanks!", "@BgMuhendis, Provide more information to replicate the issue. Thanks", "@gadagashwini  \r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10 -64-bit\r\n\r\nPython version:\r\n3.7.4\r\n\r\nCUDA/cuDNN version:\r\nCUDA 10.0, cudnn64_7.dll (Windows file description is: NVIDIA CUDA CUDNN Library. Version 10.0)\r\n\r\nGPU model and memory:\r\nGPU NVidia Geforce 1050TI\r\n\r\nTensorFlow version (use command below):\r\n2.1.0 (command does not work, it includes the failed command)\r\n\r\n", "@gadagashwini  \r\nPython 3.6 version is also installed on my computer and tensorflow is installed there\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37981\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37981\">No</a>\n", "Sorry accidentally closed. Have you tried installing the MSVC 2019 redistributable?", "@BgMuhendis \r\nplease update on the above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37981\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37981\">No</a>\n"]}, {"number": 37980, "title": "Kernel crashes with 'Check failed: work_element_count > 0 (0 vs. 0)' after first model has finished training.", "body": "**System information**  \r\n- Have I written custom code (as opposed to using example directory):  No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10\r\n- TensorFlow backend (yes / no):  Yes\r\n- TensorFlow version:  2.1\r\n- Keras version:  2.2.4-tf\r\n- Python version:  3.7.6\r\n- CUDA/cuDNN version:  CUDA 10.1, cuDNN: 7.5.6\r\n- GPU model and memory:  Nvidia GTX1070, 8GB\r\n\r\n**Describe the current behavior**  \r\n\r\nI'm currently trying to train multiple permutations of the same model using the \"Keras Tuner\" with the following setup:\r\n\r\n```\r\nfrom kerastuner.tuners import BayesianOptimization\r\nfrom kerastuner.engine.hyperparameters import HyperParameters\r\n\r\ntuner = BayesianOptimization(build_model,\r\n                             objective='val_loss',\r\n                             max_trials=1000,\r\n                             executions_per_trial=3,\r\n                             directory=LOG_DIR)\r\n\r\ntuner.search(train_data_single,\r\n             verbose=0,\r\n             epochs=EPOCHS,\r\n             steps_per_epoch=EVALUATION_INTERVAL,\r\n             validation_data=val_data_single,\r\n             validation_steps=EVALUATION_INTERVAL // 4)\r\n```\r\n\r\nWhere my dynamic model is defined as:\r\n\r\n```\r\ndef build_model(hp):\r\n    model = tf.keras.models.Sequential()\r\n\r\n    model.add(tf.keras.layers.LSTM(units=hp.Int(f'LSTM_0_Units', min_value=8, max_value=128, step=8),\r\n                                   dropout=hp.Float(f'LSTM_0_Dropout_Rate', min_value=0, max_value=0.5, step=0.1),\r\n                                   batch_input_shape=(BATCH_SIZE, x_train_single.shape[1], x_train_single.shape[2]),\r\n                                   return_sequences = True))\r\n\r\n    for i in range(hp.Int('n_Extra_Layers', 0, 3)):\r\n        model.add(tf.keras.layers.LSTM(units=hp.Int(f'LSTM_{i + 1}_Units', min_value=8, max_value=128, step=8),\r\n                                       dropout=hp.Float(f'LSTM_{i + 1}_Dropout_Rate', min_value=0, max_value=0.5, step=0.1),\r\n                                       return_sequences = True))\r\n    \r\n    model.add(tf.keras.layers.LSTM(units=hp.Int(f'LSTM_Closing_Units', min_value=8, max_value=128, step=8),\r\n                                   dropout=hp.Float(f'LSTM_Closing_Dropout_Rate', min_value=0, max_value=0.5, step=0.1),\r\n                                   return_sequences = False))\r\n    \r\n    if hp.Boolean(\"Extra_Dense\"):\r\n        model.add(tf.keras.layers.Dense(units=hp.Int(f'Extra_Dense_Units', min_value=8, max_value=128, step=8)))\r\n    \r\n    if hp.Boolean(\"Extra_Dropout\"):\r\n        model.add(tf.keras.layers.Dense(units=hp.Float(f'Extra_Dropout_Rate', min_value=0.1, max_value=0.5, step=0.1)))\r\n    \r\n    model.add(tf.keras.layers.Dense(1))\r\n\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(), \r\n                  loss='mae')\r\n    \r\n    return model\r\n```\r\n\r\nI'm faced with the following after the first successfully trained model:\r\n\r\n```\r\n2020-03-27 17:21:28.275596: F .\\tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)\r\n[I 17:21:29.843 LabApp] KernelRestarter: restarting kernel (1/5), keep random ports\r\nkernel 2bac517a-c195-47ca-952b-c25881cf0757 restarted\r\n```\r\n\r\nAnd the Jupyter Kernel crashes. Any ideas? :) \r\n\r\n**Describe the expected behavior**  \r\n\r\nI would expect the session to NOT crash during hyperparameter optimization after the first model has completed since I would ideally want to train hundreds if not thousands of permutations.\r\n\r\n**Other info**  \r\nPrior to this issue I was faced with the following: https://github.com/tensorflow/tensorflow/issues/37932", "comments": ["@nistrup \r\nplease let us know if this issue still persist", "> @nistrup\r\n> please let us know if this issue still persist\r\n\r\nSadly this is still an issue", "@nistrup \r\ni ran the code shared by you, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/aea68b462ac4c4be17f8a88d5cc4376c/your_version.ipynb) please find with comeplete code or a colab gist for us to analyse.", "@nistrup\r\nPlease update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37980\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37980\">No</a>\n", "@nistrup did you found a solution to your problem, because I'm having the same problem?", "I am having the same problem.  Has anyone found a solution to this?"]}, {"number": 37979, "title": "Error when tf2 model is ask to output gradients with prediction", "body": "-------------------------------------\r\nSystem information\r\n\r\nTensorflow==2.1.0 in a conda env\r\n\r\n-------------------------------------\r\nTest Goal \r\nThe goal was to get the gradients as an output of the model to get it during inference\r\n\r\n------------------------------------\r\nTest Code\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.config.experimental_run_functions_eagerly(True)\r\n\r\ndef get_model():\r\n    \r\n    input_sequences = tf.keras.layers.Input(shape=(10,10))\r\n    \r\n    output = tf.keras.layers.Flatten()(input_sequences)\r\n    output = tf.keras.layers.Dense(10*10)(output)\r\n    output = tf.keras.layers.Dense(10)(output)\r\n    output = tf.keras.layers.Dense(1)(output)\r\n    model = tf.keras.models.Model(inputs=[input_sequences], outputs=[output])\r\n    \r\n    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[tf.keras.metrics.mae])\r\n    model.build((10,10))\r\n    return model\r\n\r\ndef get_model_grad():\r\n    \r\n    input_sequences = tf.keras.layers.Input(shape=(10,10))\r\n    \r\n    m = get_model()    \r\n    t = m(input_sequences)\r\n    \r\n    loss = tf.keras.losses.mean_squared_error(t, 1.)\r\n    gradients = tf.keras.layers.Lambda(lambda x : tf.keras.backend.gradients(loss, input_sequences))(t)\r\n    \r\n    model = tf.keras.models.Model(inputs=[input_sequences], outputs=[t, gradients])\r\n    \r\n    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[tf.keras.metrics.mae])\r\n    model.build((10,10))\r\n    print(model.summary())\r\n    return model\r\n\r\nmodel = get_model_grad()\r\n\r\nrandom_input = np.random.rand(10,10)\r\nprint(random_input.shape)\r\nrandom_batch = np.expand_dims(random_input, 0)\r\nprint(random_batch.shape)\r\nprint(model.predict(random_batch))\r\n\r\n-------------------------------------\r\nERROR \r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-13-dc70cf4714b3> in <module>\r\n      5 random_batch = np.expand_dims(random_input, 0)\r\n      6 print(random_batch.shape)\r\n----> 7 print(model.predict(random_batch))\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n   1011         max_queue_size=max_queue_size,\r\n   1012         workers=workers,\r\n-> 1013         use_multiprocessing=use_multiprocessing)\r\n   1014 \r\n   1015   def reset_metrics(self):\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in predict(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    496         model, ModeKeys.PREDICT, x=x, batch_size=batch_size, verbose=verbose,\r\n    497         steps=steps, callbacks=callbacks, max_queue_size=max_queue_size,\r\n--> 498         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\r\n    499 \r\n    500 \r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _model_iteration(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    473               mode=mode,\r\n    474               training_context=training_context,\r\n--> 475               total_epochs=1)\r\n    476           cbks.make_logs(model, epoch_logs, result, mode)\r\n    477 \r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    553     context.ensure_initialized()\r\n    554     if RUN_FUNCTIONS_EAGERLY:\r\n--> 555       return self._python_function(*args, **kwds)\r\n    556 \r\n    557     tracing_count = self._get_tracing_count()\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in distributed_function(input_iterator)\r\n     83     args = _prepare_feed_values(model, input_iterator, mode, strategy)\r\n     84     outputs = strategy.experimental_run_v2(\r\n---> 85         per_replica_function, args=args)\r\n     86     # Out of PerReplica outputs reduce or pick values to return.\r\n     87     all_outputs = dist_utils.unwrap_output_dict(\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)\r\n    761       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\r\n    762                                 convert_by_default=False)\r\n--> 763       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    764 \r\n    765   def reduce(self, reduce_op, value, axis):\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\r\n   1817       kwargs = {}\r\n   1818     with self._container_strategy().scope():\r\n-> 1819       return self._call_for_each_replica(fn, args, kwargs)\r\n   1820 \r\n   1821   def _call_for_each_replica(self, fn, args, kwargs):\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)\r\n   2162         self._container_strategy(),\r\n   2163         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\r\n-> 2164       return fn(*args, **kwargs)\r\n   2165 \r\n   2166   def _reduce_to(self, reduce_op, value, destinations):\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    256   def wrapper(*args, **kwargs):\r\n    257     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\r\n--> 258       return func(*args, **kwargs)\r\n    259 \r\n    260   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in _predict_on_batch(***failed resolving arguments***)\r\n    210       del y, sample_weights\r\n    211       # Note that the x and batch_index is already per-replica value.\r\n--> 212       result = predict_on_batch(model, x)\r\n    213       if batch_index is None:\r\n    214         return result\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in predict_on_batch(model, x, standalone)\r\n    554 \r\n    555   with backend.eager_learning_phase_scope(0):\r\n--> 556     return predict_on_batch_fn(inputs)  # pylint: disable=not-callable\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    820           with base_layer_utils.autocast_context_manager(\r\n    821               self._compute_dtype):\r\n--> 822             outputs = self.call(cast_inputs, *args, **kwargs)\r\n    823           self._handle_activity_regularization(inputs, outputs)\r\n    824           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in call(self, inputs, training, mask)\r\n    715     return self._run_internal_graph(\r\n    716         inputs, training=training, mask=mask,\r\n--> 717         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n    718 \r\n    719   def compute_output_shape(self, input_shape):\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask, convert_kwargs_to_constants)\r\n    889 \r\n    890           # Compute outputs.\r\n--> 891           output_tensors = layer(computed_tensors, **kwargs)\r\n    892 \r\n    893           # Update tensor_dict.\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    820           with base_layer_utils.autocast_context_manager(\r\n    821               self._compute_dtype):\r\n--> 822             outputs = self.call(cast_inputs, *args, **kwargs)\r\n    823           self._handle_activity_regularization(inputs, outputs)\r\n    824           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/core.py in call(self, inputs, mask, training)\r\n    844     with backprop.GradientTape(watch_accessed_variables=True) as tape,\\\r\n    845         variable_scope.variable_creator_scope(_variable_creator):\r\n--> 846       result = self.function(inputs, **kwargs)\r\n    847     self._check_variables(created_variables, tape.watched_variables())\r\n    848     return result\r\n\r\n<ipython-input-10-fe47f043d606> in <lambda>(x)\r\n      7 \r\n      8     loss = tf.keras.losses.mean_squared_error(t, 1.)\r\n----> 9     gradients = tf.keras.layers.Lambda(lambda x : tf.keras.backend.gradients(loss, input_sequences))(t)\r\n     10 \r\n     11     model = tf.keras.models.Model(inputs=[input_sequences], outputs=[t, gradients])\r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py in gradients(loss, variables)\r\n   3782   \"\"\"\r\n   3783   return gradients_module.gradients(\r\n-> 3784       loss, variables, colocate_gradients_with_ops=True)\r\n   3785 \r\n   3786 \r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\r\n    156         ys, xs, grad_ys, name, colocate_gradients_with_ops,\r\n    157         gate_gradients, aggregation_method, stop_gradients,\r\n--> 158         unconnected_gradients)\r\n    159   # pylint: enable=protected-access\r\n    160 \r\n\r\n~/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    489   \"\"\"Implementation of gradients().\"\"\"\r\n    490   if context.executing_eagerly():\r\n--> 491     raise RuntimeError(\"tf.gradients is not supported when eager execution \"\r\n    492                        \"is enabled. Use tf.GradientTape instead.\")\r\n    493   if src_graph is None:\r\n\r\nRuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\r\n", "comments": ["@Axelsalamartin, I have tried after disabling eager execution with `tf.compat.v1.disable_eager_execution()` and got no error. For your reference link of gist is [here](https://gist.github.com/khimraj/b4b2f08fbd2e7d430704fe5ba24adb24).", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@Axelsalamartin \r\n\r\nAny update on this issue please. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37978, "title": " cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version", "body": "I installed Anaconda 2020(64 bit) after uninstalling every installation and files related to python before that and then used these instructions:\r\nconda install tensorflow\r\nconda install tensorflow-gpu\r\nnow I have \r\ncudatoolkit               10.0.130                      0\r\ncudnn                     7.6.5                cuda10.0_0\r\n and NVIDIA Graphics Driver 388.73\r\n\r\nThen I run these instructions in spyder:\r\n\r\ncheckpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1, save_best_only=True)\r\nclass_weights = {0:1, 1:1, 2:1, 3:1}\r\nfittedModel = model.fit(X_train, Y_train, batch_size = 16, epochs = 10, \r\n                        verbose = 1, validation_data=(X_validate, Y_validate),\r\n                        callbacks=[checkpointer], class_weight = class_weights)\r\n \r\nand I have this error:\r\nTrain on 144 samples, validate on 72 samples\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-2-4eb902219300>\", line 3, in <module>\r\n    callbacks=[checkpointer],class_weight = class_weights)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 780, in fit\r\n    steps_name='steps_per_epoch')\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 250, in model_iteration\r\n    model.reset_metrics()\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1084, in reset_metrics\r\n    m.reset_states()\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\", line 199, in reset_states\r\n    K.batch_set_value([(v, 0) for v in self.variables])\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3071, in batch_set_value\r\n    get_session().run(assign_ops, feed_dict=feed_dict)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 459, in get_session\r\n    session = _get_session(op_input_list)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 431, in _get_session\r\n    config=get_default_session_config())\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1570, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 693, in __init__\r\n    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)\r\n\r\nInternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version\r\n\r\n what should I do?\r\nPlease help me. thanks", "comments": ["please help", "@ljafari,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "Any updates regarding this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37977, "title": "Micro -- Crash in ErrorReporter::Report", "body": "**System information** \r\nTensorFlow Lite for Microcontrollers\r\nrunning greedy_memory_planner_test from master\r\nbuilding with a Clang based compiler where va_list has the type void*\r\n\r\n**Describe the current behavior**\r\nTest crashes on \r\n`TF_LITE_REPORT_ERROR(error_reporter, \"%s\", line);`\r\nMost likely because the string arg matches two prototypes in this configuration of Clang, and its picking the wrong one. Ultimately it tries do dereference the literal string leading to a crash.\r\n\r\n**Describe the expected behavior**\r\nNo crash, print some ASCII art.\r\n\r\n**Standalone code to reproduce the issue** \r\nIf you have Docker up and running, then call this from the tensorflow folder:\r\n`rm -rf tensorflow/lite/micro/tools/make/downloads/*`\r\n`make -f tensorflow/lite/micro/tools/make/Makefile clean`\r\n`docker run -it -v$(pwd):/home/builder:z --rm xcoreai/build-tools:latest make -f tensorflow/lite/micro/tools/make/Makefile TARGET=\"xcore\" test_greedy_memory_planner_test`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nPR #37976\r\nFixes the immediate issue in greedy_memory_planner by adding an explicit typecast:\r\n`TF_LITE_REPORT_ERROR(error_reporter, \"%s\", (const char *)line);`", "comments": ["Closing this issue since the associated PR has been merged. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37977\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37977\">No</a>\n"]}, {"number": 37976, "title": "Micro: fix crash in greedy memory planner test", "body": "Test crashes on \r\n`TF_LITE_REPORT_ERROR(error_reporter, \"%s\", line);`\r\nMost likely because the string arg matches two prototypes in this configuration of Clang, and its picking the wrong one. Ultimately it tries do dereference the literal string leading to a crash.\r\n\r\nThis PR \r\nFixes the immediate issue in greedy_memory_planner by adding an explicit typecast:\r\nTF_LITE_REPORT_ERROR(error_reporter, \"%s\", (const char *)line);\r\n\r\nIt's just a workaround on an instance of this corner case, but a true fix would either involve a less portable code base where we don't support compilers who define va_list as void* OR we would need to make more extreme re-factors up at the TF-Lite level.", "comments": ["Fixes issue #37977 also submitted by me.", "Can't fathom how my change broke:\r\n\r\ntest.tensorflow/compiler/mlir/tensorflow/tests/mlir2graphdef/tf_add.mlir.test \r\n\r\nis this a known, failing test?"]}, {"number": 42796, "title": "[zh-cn] Notebooks failing", "body": "### site/zh-cn/tutorials/distribute/multi_worker_with_keras.ipynb\r\n\r\n```\r\nnbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:\r\n------------------\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard = False\r\ntrain_datasets_no_auto_shard = train_datasets.with_options(options)\r\n------------------\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-9-540e26d88b40> in <module>\r\n      1 options = tf.data.Options()\r\n----> 2 options.experimental_distribute.auto_shard = False\r\n      3 train_datasets_no_auto_shard = train_datasets.with_options(options)\r\n\r\n/tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow_core/python/data/util/options.py in __setattr__(self, name, value)\r\n     54     else:\r\n     55       raise AttributeError(\r\n---> 56           \"Cannot set the property %s on %s.\" % (name, type(self).__name__))\r\n     57 \r\n     58 \r\n\r\nAttributeError: Cannot set the property auto_shard on DistributeOptions.\r\n```\r\n\r\n###  site/zh-cn/tutorials/generative/style_transfer.ipynb\r\n\r\n```\r\nnbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:\r\n------------------\r\nfile_name = 'kadinsky-turtle.png'\r\nmpl.image.imsave(file_name, image[0])\r\n\r\ntry:\r\n  from google.colab import files\r\nexcept ImportError:\r\n   pass\r\nelse:\r\n  files.download(file_name)\r\n------------------\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-35-aea7d0ffa719> in <module>\r\n      1 file_name = 'kadinsky-turtle.png'\r\n----> 2 mpl.image.imsave(file_name, image[0])\r\n      3 \r\n      4 try:\r\n      5   from google.colab import files\r\n\r\n~/.local/lib/python3.6/site-packages/matplotlib/image.py in imsave(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\r\n   1548         if origin == \"lower\":\r\n   1549             arr = arr[::-1]\r\n-> 1550         rgba = sm.to_rgba(arr, bytes=True)\r\n   1551         if format == \"png\" and pil_kwargs is None:\r\n   1552             with cbook.open_file_cm(fname, \"wb\") as file:\r\n\r\n~/.local/lib/python3.6/site-packages/matplotlib/cm.py in to_rgba(self, x, alpha, bytes, norm)\r\n    215                         alpha = np.uint8(alpha * 255)\r\n    216                     m, n = x.shape[:2]\r\n--> 217                     xx = np.empty(shape=(m, n, 4), dtype=x.dtype)\r\n    218                     xx[:, :, :3] = x\r\n    219                     xx[:, :, 3] = alpha\r\n\r\nTypeError: data type not understood\r\n```\r\n\r\n### site/zh-cn/tutorials/keras/text_classification_with_hub.ipynb\r\n\r\n```\r\nnbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:\r\n------------------\r\n# \u5c06\u8bad\u7ec3\u96c6\u6309\u7167 6:4 \u7684\u6bd4\u4f8b\u8fdb\u884c\u5207\u5272\uff0c\u4ece\u800c\u6700\u7ec8\u6211\u4eec\u5c06\u5f97\u5230 15,000\r\n# \u4e2a\u8bad\u7ec3\u6837\u672c, 10,000 \u4e2a\u9a8c\u8bc1\u6837\u672c\u4ee5\u53ca 25,000 \u4e2a\u6d4b\u8bd5\u6837\u672c\r\ntrain_validation_split = tfds.Split.TRAIN.subsplit([6, 4])\r\n\r\n(train_data, validation_data), test_data = tfds.load(\r\n    name=\"imdb_reviews\", \r\n    split=(train_validation_split, tfds.Split.TEST),\r\n    as_supervised=True)\r\n------------------\r\n\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-3-fd9cf994df99> in <module>\r\n      6     name=\"imdb_reviews\",\r\n      7     split=(train_validation_split, tfds.Split.TEST),\r\n----> 8     as_supervised=True)\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_datasets/core/api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)\r\n     50     _check_no_positional(fn, args, ismethod, allowed=allowed)\r\n     51     _check_required(fn, kwargs)\r\n---> 52     return fn(*args, **kwargs)\r\n     53 \r\n     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter\r\n\r\n...\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_datasets/core/tfrecords_reader.py in _str_to_relative_instruction(spec)\r\n    354   res = _SUB_SPEC_RE.match(spec)\r\n    355   if not res:\r\n--> 356     raise AssertionError('Unrecognized instruction format: %s' % spec)\r\n    357   unit = '%' if res.group('from_pct') or res.group('to_pct') else 'abs'\r\n    358   return ReadInstruction(\r\n\r\nAssertionError: Unrecognized instruction format: NamedSplit('train')(tfds.percent[0:60])\r\n```\r\n", "comments": ["Thanks Mark \ud83d\ude00\r\n\r\nYou can also use the nb_code_sync tool (https://github.com/tensorflow/docs/blob/master/tools/nb_code_sync.py) to sync your notebooks to the SOT.", "REVIEWERS cc: @kuri-leo @JayYip @yantaozhao @loveunk @Wind2esg @tigerneil @MofiiTech @gaoljhy @Mr-Linus @flopsySong @echosun1996", "These files should be synced using our GitLocalize project: https://gitlocalize.com/tensorflow/docs-l10n\r\n\r\n* https://gitlocalize.com/repo/4592/zh-cn/site/en-snapshot/tutorials/distribute/multi_worker_with_keras.ipynb\r\n* https://gitlocalize.com/repo/4592/zh-cn/site/en-snapshot/tutorials/generative/style_transfer.ipynb\r\n* https://gitlocalize.com/repo/4592/zh-cn/site/en-snapshot/tutorials/keras/text_classification_with_hub.ipynb\r\n\r\nBut this issue is old and I'm working on a better notification/status system. Will close"]}, {"number": 37975, "title": "Binary add op BF16 has lower performance than FP32", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Centos 7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below):  binary pip install tensorflow==2.1.0\r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nRunning the binary op of two tensor, change the tensor type to Bfloat16 increase  the running time of 2x\r\n\r\n**Describe the expected behavior**\r\nBfloat16 has lower memory consumption should be a little bit quick.\r\n\r\n**Standalone code to reproduce the issue** \r\npip install tensorflow==2.1.0\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nif __name__ == \"__main__\":\r\n  input_shape = [1, 224, 224, 1024]\r\n  images = tf.random.uniform(input_shape, 0.0, 255.0, dtype=tf.float32, name='images')\r\n  images2 = tf.random.uniform(input_shape, 0.0, 255.0, dtype=tf.float32, name='images2')\r\n  images_bf16 = tf.random.uniform(input_shape, 0.0, 255.0, dtype=tf.bfloat16, name='images_bf16')\r\n  images_bf16_2 = tf.random.uniform(input_shape, 0.0, 255.0, dtype=tf.bfloat16, name='images_bf16_2')\r\n  mysum = tf.add(images, images2)\r\n  mysum_bf16 = tf.add(images_bf16, images_bf16_2)\r\n  with tf.compat.v1.Session() as sess:\r\n    def run():\r\n      res = sess.run(mysum)\r\n      #print(res)\r\n    def run2():\r\n      res = sess.run(mysum_bf16)\r\n    import timeit\r\n    cost = timeit.timeit(stmt=run, number=200)\r\n    print(\"fp32 cost time: {}\".format(cost))\r\n    cost_bf16 = timeit.timeit(stmt=run2, number=200)\r\n    print(\"bf16 cost time: {}\".format(cost_bf16))\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nCurrently result:\r\nfp32 cost time: 15.485223675030284\r\nbf16 cost time: 30.486599242896773\r\n\r\n", "comments": ["i am able to replicate the issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/682a5774dd443e218ffbe86574dcdcd1/37975.ipynb)\r\n\r\n", "Could replicate the issue with **`Tensorflow Version 2.5`**. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Was able to reproduce the issue in tf v2.7.Please find the gist [here](https://colab.sandbox.google.com/gist/tilakrayal/78b6611062cc3d87c24a382af2902456/untitled160.ipynb).", "Hi @Leslie-Fang ! This issue is getting resolved in the  2.8 version(almost same processing time compared to v1 apis). Attaching [gist](https://colab.sandbox.google.com/gist/mohantym/a78fe100d2ced4df36c31dbf7d2c03dc/untitled160.ipynb#scrollTo=oggjNRN6vJcQ) for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 37974, "title": "Graph Transform Tool remove_nodes is unable to remove Switch nodes but Identity nodes", "body": "I can remove all the Identity nodes from my .pb model with the commands:\r\n\r\n```\r\nbazel build tensorflow/tools/graph_transforms:transform_graph\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=../m.pb --out_graph=../new.pb --inputs='batch_size,phase_train' --outputs='label_batch,embeddings' --transforms='strip_unused_nodes(type=float, shape=\"1,299,299,3\") remove_nodes(op=Identity) fold_old_batch_norms fold_constants(ignore_errors=true)'\r\n```\r\n\r\nHowever, I cannot do the same things if change Identity to Switch. It means the below command does not remove any nodes.\r\n\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=../m.pb --out_graph=../new.pb --inputs='batch_size,phase_train' --outputs='label_batch,embeddings' --transforms='strip_unused_nodes(type=float, shape=\"1,299,299,3\") remove_nodes(op=Switch) fold_old_batch_norms fold_constants(ignore_errors=true)'\r\n```\r\n\r\nThis is how I check the model nodes:\r\n\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=../new.pb\r\n```\r\n\r\n**The result as below:**\r\n\r\nFound 2 possible inputs: (name=phase_train, type=bool(10), shape=<unknown>) (name=batch_size, type=int32(3), shape=<unknown>) \r\nNo variables spotted.\r\nFound 2 possible outputs: (name=label_batch, op=Identity) (name=embeddings, op=Mul) \r\nFound 23512506 (23.51M) const parameters, 0 (0) variable parameters, and 676 control_edges\r\nOp types used: 2019 Switch, 1105 Const, 566 Identity, 449 Merge, 448 Sub, 249 Mul, 224 FusedBatchNormV3, 132 Conv2D, 131 Relu, 23 ConcatV2, 21 BiasAdd, 21 AddV2, 3 Shape, 3 MaxPool, 3 Reshape, 2 Placeholder, 1 Maximum, 1 Pack, 1 MatMul, 1 QueueDequeueUpToV2, 1 RandomUniform, 1 GreaterEqual, 1 FIFOQueueV2, 1 Rsqrt, 1 AvgPool, 1 Square, 1 StridedSlice, 1 Cast, 1 Sum, 1 Add\r\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\r\nbazel run tensorflow/tools/benchmark:benchmark_model -- --graph=../new.pb --show_flops --input_layer=phase_train,batch_size --input_layer_type=bool,int32 --input_layer_shape=: --output_layer=label_batch,embeddings\r\n\r\n\r\n**My question is how can I remove the Switch nodes?**\r\n ", "comments": ["Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37974\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37974\">No</a>\n"]}, {"number": 37973, "title": "Keras loading a saved model - ValueError: Could not find matching function to call loaded from the SavedModel", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information** \r\n- Have written custom code: Yes\r\n- OS Platform and Distribution: Windows 10 \r\n- Mobile device: No \r\n- TensorFlow installed from: conda install tensorflow\r\n- TensorFlow version: 2.1.0 (CPU only)\r\n- Python version: 3\r\n\r\n**Describe the current behavior**\r\nI train a TF Keras model (summary below) and use it for slot classification and it works fine.\r\nI then save it using: `tf.saved_model.save(joint_model, 'BERT2.tf')`\r\nI then load the model: bertmodel = `tf.keras.models.load_model('BERT2.tf', compile=False)`\r\nIt is compiled using the same arguments as saved model.\r\nAnd then try predictions using the same function as before but I getting the following error: ValueError: Could not find matching function to call loaded from the SavedModel\r\n\r\n**Describe the expected behavior**\r\nI run prediction using the same function but the loaded model gives the error.\r\nI tried installing the nightly tensorflow build but was not successful.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (1 total):\r\n    * Tensor(\"inputs:0\", shape=(1, 8), dtype=int32)\r\n  Keyword arguments: {'training': False}\r\n\r\nExpected these arguments to match one of the following 4 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (1 total):\r\n    * {'attention_masks': TensorSpec(shape=(None, 25), dtype=tf.int32, name='inputs/attention_masks'), 'input_ids': TensorSpec(shape=(None, 25), dtype=tf.int32, name='inputs/input_ids')}\r\n  Keyword arguments: {'training': False}\r\n\r\nOption 2:\r\n  Positional arguments (1 total):\r\n    * {'attention_masks': TensorSpec(shape=(None, 25), dtype=tf.int32, name='attention_masks'), 'input_ids': TensorSpec(shape=(None, 25), dtype=tf.int32, name='input_ids')}\r\n  Keyword arguments: {'training': False}\r\n\r\nOption 3:\r\n  Positional arguments (1 total):\r\n    * {'attention_masks': TensorSpec(shape=(None, 25), dtype=tf.int32, name='inputs/attention_masks'), 'input_ids': TensorSpec(shape=(None, 25), dtype=tf.int32, name='inputs/input_ids')}\r\n  Keyword arguments: {'training': True}\r\n\r\nOption 4:\r\n  Positional arguments (1 total):\r\n    * {'attention_masks': TensorSpec(shape=(None, 25), dtype=tf.int32, name='attention_masks'), 'input_ids': TensorSpec(shape=(None, 25), dtype=tf.int32, name='input_ids')}\r\n  Keyword arguments: {'training': True}\r\n\r\n```\r\n```", "comments": ["@hepbc, please can you share full code or link of colab file to reproduce and analyse issue as above given code is not sufficient to reproduce issue. ", "@hepbc, I have not required csv file for data. Although I suggest you to do not use `model.save()` for custom subclass keras model and use `save_weights()` and `load_weights()` instead.", "Hello, @hepbc !\r\nI'm trying to recreate your code in a jupyter notebook on Google Colab - the link to that is [here](https://colab.research.google.com/drive/1oxL0S446ctgkY6UPOKZGt9U5siSYs0G3) .\r\nYour code failed for me because it was importing a CSV file from your local computer's storage, and I don't have that. \r\nCan you provide any dummy data so that it will be easier to understand and help you? Thank you!\r\n\r\nLove to hear back from you soon.", "@khimraj Your solution works, many thanks!\r\n\r\n@Rubix982 Thanks for all the help Saif. I changed the way I was saving and loading the model as suggested by Khimraj. Instead of model.save(), I saved the weights and then loaded the model as per guidelines in Part II-Approach 1 here: https://www.tensorflow.org/guide/keras/save_and_serialize. Seems to be working now.  ", "Fantastic! \r\nI haven't touched those guide before, so I didn't exactly know where the error could come. good to see it's resolved now. ", ">Seems to be working now.\r\n\r\n@hepbc,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "> @hepbc, I have not required csv file for data. Although I suggest you to do not use `model.save()` for custom subclass keras model and use `save_weights()` and `load_weights()` instead.\r\n\r\nI got the same error with model.save(). Hence I used save_weights() and load_weights(). But here again there is another issue. The prediction done where the model is trained and the prediction done where the model is loaded for the same input is different. How to solve this?"]}, {"number": 37972, "title": "Tensorflow 2.x version is used in 1.x Tutorials", "body": "## URL(s) with the issue: https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/keras/save_and_restore_models.ipynb\r\n\r\n## Description of issue (what needs changing): The Tutorials corresponding to 1.x Version in Github has the version 2.x used inside it, thus leaving no Tutorials corresponding to 1.x (at least for Save and Restore)\r\n\r\n### Clear description: Please find the screenshot in [this link](https://screenshot.googleplex.com/gT6SwTNeY3E).\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? : Yes\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? : N/A\r\n\r\n### Returns defined\r\n\r\nAre return values defined? : N/A\r\n\r\n### Usage example\r\n\r\nIs there a usage example? : No, usage example for Save and Restore is not present for Tensorflow 1.x version\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content? : N/A\r\n\r\n### Submit a pull request?: No\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["I'd like to work on this issue. But is this really a doc related issue?? Cause I think the changes involve migrating the code to tf-1.x from 2.x.", "I cannot open the screenshot link. Can you upload the image on imgur instead? t is leading me to a Google Sign Page, and no matter how many times I open my account, it keeps looping back. ", "Should we create a new folder specifically for archives and examples for Tensorflow 2.x as well? Like we have docs/site/en/r1/ for TensorFlow 1.x?\r\n\r\nI'm willing to open up a new issue for it.", "@ManishAradwad I guess yes because that file which is mentioned is in [/docs/site/en/r1](https://github.com/tensorflow/docs/tree/master/site/en/r1) which basically has a README which says:\r\n\r\n```\r\nTensorFlow 1.x\r\nThis archive of the TensorFlow 1.x docs is in maintenance mode only.\r\n```\r\n\r\n", "Hi,  this is somewhat intentional. These run in tensorflow2's compatibility-mode. \r\n\r\nI'll add a note to all of them explaining.", "@MarkDaoust,\r\nThank you for your response. If I interpret your comment correctly, it means that the 1.x tutorials will be made 2.x compatible. Does it mean `Tensorflow Version 1.x` will be deprecated and there will be no Tutorials corresponding to `Tensorflow Version 1.x`, which uses `Sessions` inside the Code?", "@rakeshmothukuru1\r\n\r\nYes we want to move away from TF1. But we're keeping this r1 snapshot as is for the foreseeable future.\r\n\r\nThey run with colab's tf2 installation, but if you use tf1 compatibility mode: \r\n \r\n```\r\nimport tf.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n```\r\n\r\nIt should make (almost) no difference whether you actually have tf1 or tf2 installed.\r\n\r\nThere are some `tf.Session` in these old tutorials. We hav e no plans to _delete_ them, we just discourage anyone from writing code this way."]}, {"number": 37971, "title": "Broken link for tf.debugging.assert_same_float_dtype", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/index.md\r\n\r\n## Description of issue (what needs changing):\r\nThe link for `tf.debugging.assert_same_float_dtype` is dead\r\n\r\n### Clear description\r\n\r\nThe previous link leads to: https://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/tf/debugging/assert_same_float_dtype \r\nThe new link must be:\r\nhttps://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/tf/debugging/assert_same_float_dtype.md\r\n(The .md file extension is missing in the link)\r\n\r\n\r\n### Submit a pull request?\r\n\r\nYes, will be updating the issue soon enough\r\n", "comments": ["Hi, @theadityasam , I just checked https://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/index.md , let's call that [index](https://github.com/tensorflow/docs/blob/r2.0/site/en/api_docs/python/index.md), and none of the links work. It's not only for `assert_same_float_dtype`, they do however work when I append `.md` at the end. I guess this means a complete modification of the contents of the file `/docs/index.md`.\r\n\r\nPlease update your title as well as per the issue.", "Is it okay if I look into this instead? Should be pretty easy for me.", "Hey @Rubix982 \r\nSure, work on the issue. I saw your PR, from what I gather, the index.md is generated by some script and I guess it's not as simple as adding .md to the url's(not really sure about that, need to ask the code owners once). Also, I believe the API docs were moved to the TF website and were deprecated in the previous versions.\r\n", "@theadityasam \r\nAs there is a PR to monitor the issue, please confirm if we may move this to closed status.", "Don't think it's right to close the issue until the PR is merged", "@theadityasam PR successfully merged at https://github.com/tensorflow/docs/pull/1517 . This issue can be closed now.", "Thanks"]}, {"number": 37970, "title": "[XLA][MLIR] Add const forwording pass", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n@sherhut @pifon2a\r\n\r\nWe work on TensorFlow/MLIR to make mlir_gpu enable.", "comments": ["Below is an actual example for const forwarding:\r\n\r\n*** IR Dump Before xla::mlir_gpu::{anonymous}::ConstForwardingPass ***\r\n```\r\nfunc @fusion.95(%arg0: memref<1024x16x64xf32>, %arg1: memref<1024x16x64xf32>) {\r\n  %c0 = constant 0 : index\r\n  %c1 = constant 1 : index\r\n  %cst = constant -0.00855816528 : f32\r\n  %cst_0 = constant 0.0171163306 : f32\r\n  %c1024 = constant 1024 : index\r\n  %c16 = constant 16 : index\r\n  %c64 = constant 64 : index\r\n  %0 = alloc() {temp = true} : memref<1024x16x64xf32>\r\n  %1 = alloc() {temp = true} : memref<1024x16x64xf32>\r\n  %2 = alloc() {temp = true} : memref<f32>\r\n  %3 = alloc() {temp = true} : memref<1024x16x64xf32>\r\n  %4 = alloc() {temp = true} : memref<f32>\r\n  store %cst, %4[] : memref<f32>\r\n  store %cst_0, %2[] : memref<f32>\r\n  loop.for %arg2 = %c0 to %c1024 step %c1 {\r\n    loop.for %arg3 = %c0 to %c16 step %c1 {\r\n      loop.for %arg4 = %c0 to %c64 step %c1 {\r\n        %5 = subview %3[%arg2, %arg3, %arg4] [] [] : memref<1024x16x64xf32> to memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %6 = subview %0[%arg2, %arg3, %arg4] [] [] : memref<1024x16x64xf32> to memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %7 = subview %arg1[%arg2, %arg3, %arg4] [] [] : memref<1024x16x64xf32> to memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %8 = load %5[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %9 = load %4[] : memref<f32>\r\n        store %9, %5[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %10 = subview %arg0[%arg2, %arg3, %arg4] [] [] : memref<1024x16x64xf32> to memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %11 = subview %1[%arg2, %arg3, %arg4] [] [] : memref<1024x16x64xf32> to memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %12 = load %11[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %13 = load %2[] : memref<f32>\r\n        store %13, %11[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %14 = load %10[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %15 = load %6[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %16 = mulf %14, %13 : f32\r\n        store %16, %6[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %17 = load %7[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %18 = addf %9, %16 : f32\r\n        store %18, %7[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n      }\r\n    }\r\n  }\r\n  dealloc %4 : memref<f32>\r\n  dealloc %3 : memref<1024x16x64xf32>\r\n  dealloc %2 : memref<f32>\r\n  dealloc %1 : memref<1024x16x64xf32>\r\n  dealloc %0 : memref<1024x16x64xf32>\r\n  return\r\n}\r\n```\r\n\r\n*** IR Dump After xla::mlir_gpu::{anonymous}::ConstForwardingPass ***\r\n```\r\nfunc @fusion.95(%arg0: memref<1024x16x64xf32>, %arg1: memref<1024x16x64xf32>) {\r\n  %c0 = constant 0 : index\r\n  %c1 = constant 1 : index\r\n  %cst = constant -0.00855816528 : f32\r\n  %cst_0 = constant 0.0171163306 : f32\r\n  %c1024 = constant 1024 : index\r\n  %c16 = constant 16 : index\r\n  %c64 = constant 64 : index\r\n  %0 = alloc() {temp = true} : memref<1024x16x64xf32>\r\n  %1 = alloc() {temp = true} : memref<1024x16x64xf32>\r\n  %2 = alloc() {temp = true} : memref<f32>\r\n  %3 = alloc() {temp = true} : memref<1024x16x64xf32>\r\n  %4 = alloc() {temp = true} : memref<f32>\r\n  store %cst, %4[] : memref<f32>\r\n  store %cst_0, %2[] : memref<f32>\r\n  loop.for %arg2 = %c0 to %c1024 step %c1 {\r\n    loop.for %arg3 = %c0 to %c16 step %c1 {\r\n      loop.for %arg4 = %c0 to %c64 step %c1 {\r\n        %5 = subview %3[%arg2, %arg3, %arg4] [] [] : memref<1024x16x64xf32> to memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %6 = subview %0[%arg2, %arg3, %arg4] [] [] : memref<1024x16x64xf32> to memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %7 = subview %arg1[%arg2, %arg3, %arg4] [] [] : memref<1024x16x64xf32> to memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %8 = load %5[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %cst_1 = constant -0.00855816528 : f32\r\n        %9 = load %4[] : memref<f32>\r\n        store %cst_1, %5[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %10 = subview %arg0[%arg2, %arg3, %arg4] [] [] : memref<1024x16x64xf32> to memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %11 = subview %1[%arg2, %arg3, %arg4] [] [] : memref<1024x16x64xf32> to memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %12 = load %11[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %cst_2 = constant 0.0171163306 : f32\r\n        %13 = load %2[] : memref<f32>\r\n        store %cst_2, %11[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %14 = load %10[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %15 = load %6[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %16 = mulf %14, %cst_2 : f32\r\n        store %16, %6[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %17 = load %7[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %18 = addf %cst_1, %16 : f32\r\n        store %18, %7[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n      }\r\n    }\r\n  }\r\n  dealloc %4 : memref<f32>\r\n  dealloc %3 : memref<1024x16x64xf32>\r\n  dealloc %2 : memref<f32>\r\n  dealloc %1 : memref<1024x16x64xf32>\r\n  dealloc %0 : memref<1024x16x64xf32>\r\n  return\r\n}\r\n```\r\n\r\n*** IR Dump After xla::mlir_gpu::{anonymous}::DeadTempBufferRemoval ***\r\n```\r\nfunc @fusion.95(%arg0: memref<1024x16x64xf32>, %arg1: memref<1024x16x64xf32>) {\r\n  %c0 = constant 0 : index\r\n  %c1 = constant 1 : index\r\n  %cst = constant -0.00855816528 : f32\r\n  %cst_0 = constant 0.0171163306 : f32\r\n  %c1024 = constant 1024 : index\r\n  %c16 = constant 16 : index\r\n  %c64 = constant 64 : index\r\n  loop.for %arg2 = %c0 to %c1024 step %c1 {\r\n    loop.for %arg3 = %c0 to %c16 step %c1 {\r\n      loop.for %arg4 = %c0 to %c64 step %c1 {\r\n        %0 = subview %arg1[%arg2, %arg3, %arg4] [] [] : memref<1024x16x64xf32> to memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %cst_1 = constant -0.00855816528 : f32\r\n        %1 = subview %arg0[%arg2, %arg3, %arg4] [] [] : memref<1024x16x64xf32> to memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %cst_2 = constant 0.0171163306 : f32\r\n        %2 = load %1[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %3 = mulf %2, %cst_2 : f32\r\n        %4 = load %0[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n        %5 = addf %cst_1, %3 : f32\r\n        store %5, %0[%c0, %c0, %c0] : memref<1x1x1xf32, affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + s0 + d1 * 64 + d2)>>\r\n      }\r\n    }\r\n  }\r\n  return\r\n}\r\n```", "This optimization is essentially the same as the store forwarding pass, except that you allow forwarding stores (to the constant buffer) across other stores and loads. This is only correct if the store of the constant value is the only write to the given buffer, which is not checked due to the lack of an alias analysis. However, if we had an alias analysis, then the normal store forwarding pass would solve this issue, as well.\r\nSo instead of adding this, I'd prefer to extend the general store forwarding pass with some aliasing information.", "@xinan-jiang Can you please check reviewer comments and keep us posted. Thanks!", "> This optimization is essentially the same as the store forwarding pass, except that you allow forwarding stores (to the constant buffer) across other stores and loads. This is only correct if the store of the constant value is the only write to the given buffer, which is not checked due to the lack of an alias analysis. However, if we had an alias analysis, then the normal store forwarding pass would solve this issue, as well.\r\n> So instead of adding this, I'd prefer to extend the general store forwarding pass with some aliasing information.\r\n\r\nThanks. I will see how to alias analysis for cross BB store.", "Closing this, as we agreed on a different approach."]}, {"number": 37969, "title": "Memory leak in TensorFlow 2.0 DataSet when using group_by_window.", "body": "**System information**\r\n- OS Platform: - Google Cloud Linux Ubuntu 16.04 \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.01\r\n- GPU model and memory: Tesla P100, 16GB\r\n- Running on GRAPH MODE:- YES\r\n\r\nThis is creating memory leak.\r\n```\r\ndef pairwise_batch_iterator(tf_records,\r\n                           no_threads=14,\r\n                           batch_size=64,\r\n                           num_epochs=50):\r\n    \r\n    dataset = make_dataset(tf_records, no_threads)\r\n    dataset = dataset.repeat(num_epochs)\r\n    \r\n    dataset = dataset.apply(tf.data.experimental.group_by_window(\r\n        key_func=lambda elem, *args: elem,\r\n        reduce_func=lambda _, window: window.batch(batch_size),\r\n        window_size=batch_size))\r\n    \r\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n    \r\n    return dataset\r\n```\r\n\r\nThis works fine\r\n```\r\ndef pairwise_batch_iterator(tf_records,\r\n                           no_threads=14,\r\n                           batch_size=64,\r\n                           num_epochs=50):\r\n    \r\n    dataset = make_dataset(tf_records, no_threads)\r\n    dataset = dataset.repeat(num_epochs)\r\n    \r\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n    \r\n    return dataset\r\n```\r\n***\r\nI am training a pairwise ranking model, where I have 100k TF Records each file has all pairs belongs to one query id, While training I have to group by query id that's why I am using tf.data.experimental.group_by_window and this is creating a memory leak. (**This only happens when I have a huge number of TF records** ) If I use the second version of code I don't face any issue but I have to group by query id.\r\n***\r\n\r\n", "comments": ["@akanyaani \r\n\r\nLooks like code is incomplete. Please, share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "Hi @ravikyram \r\n\r\nI am facing some issues in the Colab notebook, so I am posting a sample code here.\r\n\r\n```\r\ndense = tf.random.uniform([500000, 300])\r\nids = tf.random.uniform([500000], maxval=1000, dtype=tf.int64)\r\n\r\nbatch_size = 768\r\n\r\n\r\ndense = tf.data.Dataset.from_tensor_slices(dense) \r\nids = tf.data.Dataset.from_tensor_slices(ids) \r\n\r\ndataset = tf.data.Dataset.zip((ids, dense))\r\n\r\ndataset = dataset.apply(tf.data.experimental.group_by_window(\r\n       key_func=lambda elem, *args: elem,\r\n      reduce_func=lambda _, window: window.batch(batch_size),\r\n      window_size=batch_size))\r\n\r\ndataset = dataset.repeat(100)\r\ndataset = dataset.prefetch(50)\r\n\r\nfor i in dataset:\r\n    train_func()\r\n```\r\nBut I am unable to replicate the issue when creating dataset using random vectors. Still, you can replicate the issue if you read data from huge numbers of TF records.\r\n\r\n**In my case, I have 100k TF records and every file contains training data for one query id.\r\nI am still facing the issue when reading these many TF records and finally grouping all pairs for particular id in one batch.**\r\n\r\n\r\n", "@akanyaani \r\n\r\nI tried to reproduce the issue but i am seeing the below error message `NameError: name 'train_func' is not defined`.Please, share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "Hi @ravikyram,\r\n\r\nPlease run this code you will be able to generate the issue, I am still facing some issue with Colab notebook that's why I have written normal python code.\r\n\r\nFirst I am creating TF records with similar data which I have and then just iterating over. Using this code you will see a continuous increase in RAM usage and after couple of iteration, it will get killed because of this.\r\n```\r\nimport tensorflow as tf\r\nfrom datetime import datetime\r\nimport random\r\nimport numpy as np\r\nimport glob\r\n\r\noutput_dir = \"./test/\"\r\n\r\ndef create_int_feature(values):\r\n    feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\r\n    return feature\r\n\r\n\r\ndef create_float_feature(values):\r\n    feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))\r\n    return feature\r\n\r\n\r\ndef serialize_example(q_id, q, s1, s2):\r\n    feature = {\r\n        'q_id' : create_int_feature([q_id]),\r\n        'q': create_float_feature(q),\r\n        's1' : create_float_feature(s1),\r\n        's2' : create_float_feature(s1)\r\n    }\r\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\r\n    return example_proto.SerializeToString()\r\n\r\ndef writ_tf(q_id, q, s1, s2):\r\n    filename = output_dir + str(datetime.now().timestamp()) + \".tfrecord\"\r\n    tf_writer = tf.io.TFRecordWriter(filename)\r\n    for i in range(len(q_id)):\r\n        example = serialize_example(q_id[i], q[i], s1[i], s2[i])\r\n        tf_writer.write(example)\r\n    tf_writer.close()\r\n\r\nfor i in range(5500):\r\n    batch = random.randrange(250, 768)\r\n    q_id = [i+1]*batch\r\n    q = np.random.rand(batch, 786)\r\n    s1 = np.random.rand(batch, 1536)\r\n    s2 = np.random.rand(batch, 1536)\r\n    \r\n    writ_tf(q_id, q, s1, s2)\r\n\r\nNO_THREADS = 12\r\n\r\ndef load_tf_records(filenames, no_threads):\r\n    if type(filenames) is str:\r\n        filenames = [filenames]\r\n    return tf.data.TFRecordDataset(filenames, buffer_size=500, num_parallel_reads=no_threads)\r\n\r\n\r\ndef parse_example(serialized_example):\r\n    data_fields = {\r\n        \"q_id\": tf.io.VarLenFeature(tf.int64),\r\n        \"q\": tf.io.VarLenFeature(tf.float32),\r\n        \"s1\": tf.io.VarLenFeature(tf.float32),\r\n        \"s2\": tf.io.VarLenFeature(tf.float32)}\r\n    \r\n    parsed = tf.io.parse_single_example(serialized_example, data_fields)\r\n    q_id = tf.cast(tf.sparse.to_dense(parsed[\"q_id\"]), tf.int64)[0]\r\n    q = tf.cast(tf.sparse.to_dense(parsed[\"q\"]), tf.float32)\r\n    p_su = tf.cast(tf.sparse.to_dense(parsed[\"s1\"]), tf.float32)\r\n    n_su = tf.cast(tf.sparse.to_dense(parsed[\"s2\"]), tf.float32)\r\n\r\n    return (q_id, q, p_su, n_su)\r\n\r\n\r\ndef make_dataset(tf_files, no_threads=NO_THREADS):\r\n    dataset = load_tf_records(tf_files, no_threads)\r\n    dataset = dataset.apply(tf.data.experimental.ignore_errors())\r\n    dataset = dataset.map(parse_example, num_parallel_calls=no_threads)\r\n    return dataset\r\n   \r\n    \r\ndef batch_iterator(tf_records,\r\n                   no_threads=12,\r\n                   batch_size=768,\r\n                   num_epochs=100):\r\n    \r\n    dataset = make_dataset(tf_records, no_threads)\r\n\r\n    \r\n    \r\n    dataset = dataset.apply(tf.data.experimental.group_by_window(\r\n        key_func=lambda elem, *args: elem,\r\n        reduce_func=lambda _, window: window.batch(batch_size),\r\n        window_size=batch_size,\r\n    ))\r\n    \r\n    dataset = dataset.repeat(num_epochs)\r\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n    \r\n    return dataset\r\n\r\ntf_records = glob.glob((output_dir + \"*tfrecord\"))\r\ndataset = batch_iterator(tf_records)\r\n\r\n#Just iterate over dataset\r\nfor i in dataset:\r\n    q_id, q, p_su, n_su = i\r\n    target_p = tf.reshape(p_su, [-1])\r\n    target_n = tf.reshape(n_su, [-1])\r\n```\r\n\r\n", "I have tried to reproduce the issue in colab  but it is taking forever to run. Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/87e63e3218b52873242f3e837951a910/untitled761.ipynb) Thanks!", "Hi @ravikyram \r\n\r\nYa, it takes some time to create 5500 TF Records because it does not give an error with less number of the file.\r\nI ran this Colab and was able to produce memory error I have attached the screenshot of the error.\r\nColab Gist:- https://colab.research.google.com/drive/1j2chtHRpLHjq-dWRsn7Lktgm3I3MgKul\r\n\r\n<img width=\"1440\" alt=\"Screenshot 2020-04-05 at 6 31 21 PM\" src=\"https://user-images.githubusercontent.com/11317416/78499407-263f1e00-776e-11ea-87ba-33341e37b311.png\">\r\n", "This is not a memory leak and is working as intended. `group_by_window` will maintain up to `num_keys * window_size` elements buffered in memory. If you have too many keys or too many windows, you will use a lot of memory.\r\n\r\nIIUC, your program is effectively trying to do an in-memory sort using the query ID as the key. If your dataset does not fit into memory this will not be possible.\r\n\r\nWhat is not clear to me is why do you need to use `group_by_window` in the first place, if your individual TFRecord files are already grouped by query ID. Is it perhaps because your `make_dataset` method is interleaving entries from different files? If so, I suggest changing that part of your input pipeline.", "Hi @jsimsa \r\n\r\nYes, But I have another reason also, make_datet is interleaving entries from different files, and the second problem is all queries can have a variable number of documents but I want to use all the documents which belong to one query in a batch. Could you please help me out.\r\n", "The easiest option is to specify `num_parallel_reads=None` when calling the `TFRecordDataset`. That way your input pipeline will be processing files in a sequential order instead of interleaving their elements.\r\n\r\nIf you would like to batch contents of each file into a single batch, you can do something along the following lines:\r\n\r\n```\r\nfilenames = ...\r\n\r\ndef file_as_batch(filename):\r\n  ds = tf.data.Dataset.TFRecordDataset(filename, ...)\r\n  return ds.batch(batch_size=MAX_INT)\r\n\r\ndataset = filenames.flat_map(file_as_batch)\r\n# At this point `dataset` produces one batch for each input file\r\n```\r\n\r\nNote that the downstream computation will need to be able to handle variable length batches.", "Hi @jsimsa,\r\n\r\nI have updated the code according to your suggestion and it returns the output as an expectation but this pipeline is very slow and GPU has to wait after every step.\r\n\r\n```\r\n_READ_RECORD_BUFFER = 1000\r\n\r\ndef parse_example(serialized_example):\r\n    data_fields = {\r\n        \"q_id\": tf.io.VarLenFeature(tf.int64),\r\n        \"q\": tf.io.VarLenFeature(tf.float32),\r\n        \"s1\": tf.io.VarLenFeature(tf.float32),\r\n        \"s2\": tf.io.VarLenFeature(tf.float32)}\r\n    \r\n    parsed = tf.io.parse_single_example(serialized_example, data_fields)\r\n    q_id = tf.cast(tf.sparse.to_dense(parsed[\"q_id\"]), tf.int64)[0]\r\n    q = tf.cast(tf.sparse.to_dense(parsed[\"q\"]), tf.float32)\r\n    p_su = tf.cast(tf.sparse.to_dense(parsed[\"s1\"]), tf.float32)\r\n    n_su = tf.cast(tf.sparse.to_dense(parsed[\"s2\"]), tf.float32)\r\n\r\n    return (q_id, q, p_su, n_su)\r\n\r\ndef file_as_batch(filename):\r\n    dataset = tf.data.TFRecordDataset(filename, buffer_size=_READ_RECORD_BUFFER,\r\n                                      num_parallel_reads=12)\r\n    dataset = dataset.map(parse_example, num_parallel_calls=12)\r\n    return dataset.batch(batch_size=768)\r\n\r\n\r\n\r\n\r\ndataset = tf.data.Dataset.list_files(path + \"/*tfrecord\")\r\ndataset = dataset.flat_map(file_as_batch)\r\ndataset = dataset.filter(lambda x, *args: tf.shape(x)[0] >= 64)\r\ndataset = dataset.repeat(100)\r\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n```", "You should be able to do something along the following lines:\r\n\r\n```\r\nPARALLELISM = 10\r\n\r\nfilenames = tf.data.Dataset.list_files(path + \"/*tfrecord\")\r\nindices = tf.data.Dataset.range(PARALLELISM)\r\n\r\ndef make_dataset(shard_index):\r\n  dataset = filenames.shard(PARALLELISM, index)\r\n  dataset = dataset.flat_map(file_as_batch)\r\n  dataset = dataset.filter(lambda x, *args: tf.shape(x)[0] >= 64)\r\n  return dataset.repeat(100)\r\n\r\ndataset = indices.interleave(make_dataset, num_parallel_calls=PARALLELISM)\r\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n```\r\n\r\nThe above will run `PARALLELISM` copies of the input pipeline, each reading from a different shard of filenames, interleaving the output of these parallel pipelines. Greater values of `PARALLELISM` are expected to speed up your input pipeline (up to the number of available CPU cores) but will also result in high memory usage.", "Hi @jsimsa \r\n\r\nThis one worked perfectly, Thankyou very much for your help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37969\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37969\">No</a>\n"]}, {"number": 37968, "title": "tf.keras.models.Model.fit strange behaviour after upgrading from 2.1 to 2.2-rc1", "body": "I was experimenting with `tf.keras.applications.inception_v3.InceptionV3` for classifying skin cancer lesions. It was going smooth since when Colaboratory decided to upgrade its VM's TF version from 2.1 to 2.2-rc1.\r\n\r\nNow when loading the model from disk it says:\r\n```text\r\nWARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\r\n```\r\nBut most importantly the `Model.fit` is not able anymore to properly process a `keras.utils.Sequence`! Indeed during training the steps per epoch are no more inferred from the `Sequence` object showing `1/Unknown`, also it does not actually terminate the epoch!\r\n\r\nSnippet to reproduce the issue:\r\nColab code: https://colab.research.google.com/drive/1wdlWES83ibvLCwzpHrhJsQBNep-Aycqj\r\nHAM1000 dataset: https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000\r\nISIC dataset: https://www.isic-archive.com/#!/topWithHeader/wideContentTop/main\r\n\r\nThe code stopped working overnight after Colab updated their VM images.\r\n\r\nIn the code the HAM dataset is automatically downloaded and rearranged provided you have `kaggle.json` file with the API Token. The ISIC dataset needs to be downlaoded manually from their official website or from my GDrive [here](https://drive.google.com/drive/folders/1E_xIwuA3HkovP4F_cDDzD1K-nEl0TNUC?usp=sharing)\r\n\r\n**NB**: Also even if I create a new network and start training it with a `Sequence` the steps per epochs are not inferred as well. ", "comments": ["I have same issue ", "Can you test whether rc2 still has the same issue? We have done a few more cherry-picks and want to check if these also fix this", "Just tried `2.2.0-rc2`, error stays the same. ", "Am having exactly the same issue. I tried changing from h5 to tf file storage, but that only succeeded in removing the warning message. The optimizer still can't be loaded, but now fails silently.\r\n\r\nRunning `model.optimizer.get_weights()` after reloading the model gives me `[]`.\r\n\r\nI should get something more like `[643, array([[[[-5.82830864e-04,  5.68370800e-04, -2.78998603e-04,\r\n            1.33891811e-03, -2.89490534e-04,  3.54266516e-03,\r\n            5.05483190e-11,  3.68256005e-03, -3.21522425e-03,\r\n            9.00787301e-04, -2.92667857e-04,  7.44584668e-03,\r\n           -8.97338055e-03,  9.71068395e-04, -1.64545365e-02,\r\n            1.86355431e-02, -3.00804037e-03,  3.82911152e-04,\r\n           -4.87643149e-04,  1.59692019e-03, -2.45196698e-03,\r\n           -1.29825426e-02, -2.17458670e-04, -9.59509239e-03,\r\n           -3.59311025e-03, -2.34026265e-05,  5.58328955e-03,\r\n            8.98914295e-04,  1.44025474e-03,  1.85646396e-02,\r\n           -3.81471426e-03, -9.47254710e-03],\r\n          [ 3.06007161e-04, -6.80638070e-04,  2.04466240e-04,\r\n            4.62170545e-04, -1.00154630e-05,  2.06052428e-04,\r\n            7.26481453e-09,  8.34458694e-03, -1.16562960e-03,\r\n            2.11097838e-04, -1.14672934e-04,  7.56402314e-03,\r\n           -9.89045482e-03, -1.20888278e-03, -1.09558310e-02,\r\n            1.33366007e-02,  3.07402760e-03, -7.10781242e-05,\r\n           -3.85403604e-04,  4.09532833e-04,  3.30612151e-04,\r\n           -8.95670522e-03, -3.33556207e-04, -9.61863622e-03,\r\n           -5.05439332e-03, -1.13033391e-04,  6.13513310e-03,\r\n            1.88108839e-04,  1.44792779e-04,  1.49300322e-02,\r\n           -3.55098210e-03, -9.76313185e-03],\r\n          [ 2.15022286e-04, -6.72350463e-04,  5.12044353e-05,\r\n            6.01820939e-04,  6.96969568e-04, -4.09249496e-03,\r\n            9.14621179e-09,  5.68558183e-03, -3.26973875e-03,\r\n           -1.09577522e-04, -9.90386688e-05,  7.76778394e-03,\r\n           -5.82592143e-03,  1.29449691e-04, -3.73055600e-03,\r\n            1.32836401e-03, -6.25009136e-03, -2.75031198e-04,\r\n            5.25884447e-04, -7.52125517e-04,  6.06745540e-04,\r\n           -8.11741501e-03, -5.46381052e-04, -8.79659690e-03,\r\n           -4.84210020e-03, -4.62682547e-05,  6.59690751e-03,\r\n           -5.15048916e-04, -5.88135117e-05,  9.80930589e-03,\r\n           -3.64231272e-03, -9.75961983e-03]],`", "@lamba92 @DanielMorton thanks for reporting the issue, we are looking into it. ", "This issue has been fixed with [2.2.0-rc3](https://pypi.org/project/tensorflow/2.2.0rc3/#files).\r\n@lamba92 @DanielMorton Can you please check  and if so close this issue ?", "The loading of the model seems fixed, the step per epoch definitely not!\r\n```text\r\nEpoch 1/100\r\n      1/Unknown - 0s 31us/step - loss: 0.6810 - accuracy: 0.7969\r\n```\r\nTested on the same notebook above!", "@lamba92 Actually, an iterator generates data dynamically. So the length of a dataset iterator is unknown until you iterate through it at least once. You could pass steps_per_epoch argument to the `model.fit` as shown below. Then, it prints steps as you are expecting.\r\n\r\n\r\n```\r\ntraining_history = model.fit(\r\n    train_generator, steps_per_epoch=len(train_generator),\r\n    validation_data=validation_generator,\r\n    validation_steps=len(validation_generator),\r\n    epochs=100,\r\n    callbacks=[\r\n        ModelCheckpoint(model_checkpoint_path, save_best_only=True),\r\n        EarlyStopping(patience=10, restore_best_weights=True)\r\n    ],\r\n    workers=10\r\n)\r\n```\r\nCan you please verify above and close the issue if this was resolved for you. Thanks!", "I am passing a `keras.utils.Sequence` not a Python generator.\r\n\r\n`Sequence` has `__len__()` to know the actual number of batches. \r\n\r\nWhen using [`keras.model.Model.fit_generator()`](https://keras.io/models/model/#fit_generator)  it is stated that when using a `Sequence` the `steps_per_epoch` are taken from the `Sequence` itself.\r\nBut when using `fit_generator()` a warning is emitted stating that `fit()` will instead be used and that `fit_generator()` is deprecated and will be removed.\r\n\r\nTherefore I assumed that `fit()` as well was able to infer the `steps_per_epochs` from a `Sequence` and indeed until TF v2.1 it did!", "Thanks for the issue! This should be fixed in 2.2", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37968\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37968\">No</a>\n"]}, {"number": 37967, "title": "Using experimental_new_converter generates opcode that is not found in kernel", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): tf-nightly on colab\r\n- Tensorflow version (commit SHA if source): Version 2.2\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ARM Mbed OS (Cortex M K64F)\r\n\r\n**Describe the problem**\r\nI am trying the hello_world (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world).\r\nModel : https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb (with unroll set)\r\n\r\nThe model has operations SHAPE, GATHER, REDUCE_PROD etc which is not present in all_ops_resolver.cc. The Keras_lstm experimental example uses version tensorflowlite v2. I believe these operations are not supported in v1 . Any pointers on how to proceed. \r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nStep 1: Model generation using https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb with tf.keras.layers.LSTM(20, unroll=True). Without setting unroll, I get more than 1 subgraph error.\r\n\r\nStep 2: Use the example https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world to generate binary for Cortex-M \r\ngives error \"Didn't find op for builtin opcode 'SHAPE' version '1'\"\r\n\r\nMy requirement is a working example of LSTM for Cortex-M (K64F).", "comments": ["Hi,\r\n\r\nAfter looking at the code, I realised the operations SHAPE, GATHER, REDUCE_PROD are not supported on Tensorflow Lite micro yet and is available only on Tensorflow Lite. \r\n\r\nIs there any alternative to run LSTM inference on Microcontrollers?\r\n\r\nRegards,\r\nSeema", "@seemakumar8 This is a micro related issue,Please post this in [micro repository](https://github.com/tensorflow/tflite-micro/issues)  and move this issue to close status..Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37967\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37967\">No</a>\n"]}, {"number": 37965, "title": "Fix Bug in MixedPrecisionLossScaleOptimizer", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nThis pr mainly fixed the error that when disable eager execution and use the `MixedPrecisionLossScaleOptimizer` as the opt for keras model will trigger\r\n```bash\r\nFailedPreconditionError: Error while reading resource variable current_loss_scale from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/current_loss_scale/N10tensorflow3VarE does not exist.\r\n\t [[{{node training/ReadVariableOp_1}}]]\r\n```\r\nThe error is able to be replicated in tf-nightly, see gist [here](https://colab.research.google.com/gist/zhuzilin/f116e9689e2611333719fa753af31599/amp-opt-bug.ipynb)\r\n\r\nThe reason for this error is that the `MixedPrecisionLossScaleOptimizer` failed to pass the variables defined in its base optimizer and loss scale to keras backend and initialize them.\r\n\r\nThis PR fixed it by overwriting the `variables()` function, which is the function called to initialize variables.\r\n\r\nThank you for your time on reviewing this PR.", "comments": ["@reedwm Thank you for your review! I've removed the api exposed on loss_scale but not sure where should I put the test? Should it be in optimizers_test.py in keras or the loss_sacle_optimizer_test.py next to the file I modified?", "Add the test to optimizers_test.py, since we are trying to remove any dependencies from TF to Keras. optimizers_test.py already has a test that runs Model.fit with a non-Keras AdamOptimizer, so you can add a new test that runs with a MixedPrecisionLossScaleOptimizer.", "@reedwm I've added the test. Could you have a look? Thank you!\r\nBy the way, the long import line can pass the pylint since there is no resrict on the length of import.", "Thanks for the fix!", "@reedwm Just fixed the indentation error around the changed parenthesis. Could you please approve it again? Thank you for your time!", "@reedwm Could you help me merge this pr? Thank you!"]}, {"number": 37964, "title": "Warning caused by local-scope dataset.map() function", "body": "Just a quick observation (**tf = 2.1.0**)\r\n\r\nUsing `tf.data.Dataset.from_tensor_slices((filenames, labels)).map(FUNC)` when FUNC is in a local scope causes the warning:\r\n\r\n**No warning:**\r\n\r\n```\r\ndef mapper_func(f, l):\r\n    return foo(f, l)\r\n\r\ndef create_dataset(input):\r\n    return tf.data.Dataset.from_tensor_slices((input.filenames, input.labels)).map(mapper_func)\r\n```\r\n\r\n**Warning:**\r\n\r\n```\r\ndef create_dataset(input):\r\n\r\n    def mapper_func(f, l):\r\n        return foo(f, l)\r\n\r\n    return tf.data.Dataset.from_tensor_slices((input.filenames, input.labels)).map(mapper_func)\r\n```\r\n\r\n\r\n```\r\nW0327 05:10:18.360908  4452 ag_logging.py:146] AutoGraph could not transform <function create_datasets.<locals>.parse_img at 0x0000020603A95268> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: \r\nWARNING: AutoGraph could not transform <function create_datasets.<locals>.parse_img at 0x0000020603A95268> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: \r\n```", "comments": ["Actually I'm gonna pin this down to something else. Just tried reproducing it and it wasn't always predictable. Please disregard.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37964\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37964\">No</a>\n"]}, {"number": 37963, "title": "tf dataset cache behave differently with filter for TF 1.x", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: NA\r\n- TensorFlow installed from (source or\r\nbinary): binary (pip)\r\n- TensorFlow version (use command below): v1.15.0-rc3-22-g590d6eef7e 1.15.0\r\n- Python version: - Bazel\r\nversion (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from\r\nsource): NA\r\n- CUDA/cuDNN version: - GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen applying filter before cache on `tf.data.Dataset`, the outputs when caching to file compared to caching to memory are different.\r\n\r\n**Describe the expected behavior**\r\n\r\nThey should be the same.\r\n\r\nTF 2x is working as expected.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nCache to memory\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\ndata = tf.data.Dataset.from_tensor_slices(list(range(50)))\r\ndata = data.filter(lambda x: tf.random.uniform([]) < 0.5)\r\ndata = data.cache()\r\n\r\noutputs = [x for x in data]\r\noutputs_1 = [x for x in data]\r\n\r\nassert len(outputs) == len(outputs_1)\r\n\r\nfor x, y in zip(outputs, outputs_1):\r\n    assert x.numpy() == y.numpy()\r\n```\r\n\r\nCache to file\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\ndata = tf.data.Dataset.from_tensor_slices(list(range(50)))\r\ndata = data.filter(lambda x: tf.random.uniform([]) < 0.5)\r\ndata = data.cache('/tmp/dummy_data')\r\n\r\noutputs = [x for x in data]\r\noutputs_1 = [x for x in data]\r\n\r\nassert len(outputs) == len(outputs_1)\r\n\r\nfor x, y in zip(outputs, outputs_1):\r\n    assert x.numpy() == y.numpy()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with TF 1.15.2. Please find the Gist [here](https://colab.research.google.com/gist/amahendrakar/7714f97c7adc37db9f361bdc40fe9f5a/37963.ipynb). Thanks!", "Hi, I am facing a similar issue and I've managed to replicate the problem too. Is there a fix for this?", "In TF 1, `cache` does not work with eager iteration (i.e. `from elem in dataset`). You should either use TF 2 or append `repeat` to the end of your dataset consume multiple epochs.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37963\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37963\">No</a>\n", "Hi @jsimsa, thank you for your reply. I tried replicating it in non-eager and still experienced the same issue.\r\n\r\nWhy are you saying that `cache` does not work with eager mode in TF 1? By \"does not work\", do you mean it will not behave properly? Caching with file behaves as expected though.", "In TF 1, memory-based `cache` only supports reuse through subsequent `repeat`. This is because the implementation uses an in-memory cache that is owned by the iterator and thus different iterators will have different caches. This is not an issue for file-based caches because different iterators can access the same file-based cache.\r\n\r\nThis is fixed in TF 2 where a single in-memory `cache` is shared among all iterators created for a dataset."]}, {"number": 37962, "title": "Add support for large writes to Hadoop Filesystem", "body": "change append function to while loop .\r\nFixes #37961", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37962) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37962) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37962) for more info**.\n\n<!-- ok -->"]}, {"number": 37961, "title": "I meet a problem with HDFSWritableFile::Append", "body": "**System information** \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux centos 7\r\n- TensorFlow installed from (source or\r\nbinary): - find in tf_1.10, and recurrent in master.\r\n- Python version: find in python2.7, recurrent in python3.6.5 \r\n- Bazel version :find in 0.15.2, recurrent:2.0.0,\r\n- GCC/Compiler version find in 4.8.5, recurrent:7.3.0\r\n\r\nI meet a problem with HDFSWritableFile::Append\r\nBackground 1\uff1a     I save model and checkpoint in HDFS.\r\nBackground 2: \tMy users want to add a big dict(30millon data, above 3GB) to graph.\r\nThe Problem:\t\tHDFS abort quit when TF saves graph.txt to HDFS.\r\nPart of logs:\r\n```\r\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 450, in after_create_session\r\n    \"graph.pbtxt\")\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/graph_io.py\", line 71, in write_graph\r\n    text_format.MessageToString(graph_def))\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 434, in atomic_write_string_to_file\r\n    write_string_to_file(temp_pathname, contents)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 314, in write_string_to_file\r\n    f.write(file_content)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 111, in write\r\n    compat.as_bytes(file_content), self._writable_file, status)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\nInvalidArgumentError: viewfs://hadoop-meituan/xxxx01/user/hadoop-waimai/xxxx/model/model//date/graph.pbtxt.tmp04d0a32366f548ec9f3aa629600fa19f; Invalid argument\r\n```\r\n\r\nI deal with this question by logs, then I get a result that the graph is too big to save. \r\nproblem code:\r\n```\r\nStatus HDFSWritableFile::Append(StringPiece data) {\r\n    if (libhdfs()->hdfsWrite(fs_, file_, data.data(),\r\n                             static_cast<tSize>(data.size())) == -1) {\r\n       return IOError(filename_, errno);\r\n}\r\n```\r\ndata.size() return uint64_t, but hdfsWrite only accept int, so there are some questions when append a big string(len > INT_MAX)\r\n\r\nSo I change HDFSWritableFile::Append function to solve my question, and successfully solve it.\r\nso I want to make a pull request .", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37961\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37961\">No</a>\n"]}, {"number": 37960, "title": "TF 1.15.2 Memory Leak ", "body": "**System information** \r\n\r\n- OS Platform and Distribution : macOS Catalina 10.15.3\r\n\r\n- TensorFlow installed from : binary\r\n\r\n- TensorFlow version : 1.15.2\r\n- Python version: 3.7.3\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am running simple tensorflow  sess.run as below -\r\n\r\n`from tensorflow.python.framework import ops`\r\n`import gc`\r\n`import psutil`\r\n`process = psutil.Process(os.getpid())`\r\n\r\n\r\n`N_REPS = 10000`\r\n`sess = tf.Session(graph=tf.Graph())`\r\n`tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)`\r\n`for i in range(N_REPS):`\r\n        &nbsp;&nbsp;&nbsp;&nbsp;`sess.graph.finalize()`\r\n\t&nbsp;&nbsp;&nbsp;&nbsp;`tf.reset_default_graph()`\r\n        &nbsp;&nbsp;&nbsp;&nbsp;`with tf.Graph().as_default() as graph:`\r\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`tf.get_default_graph().finalize()`\r\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`scores = self.sess.run(['<loss>'], feed_dict={'k':v})`\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`memoryUsed = process.memory_info().rss`\r\n\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`del v`\r\n\t&nbsp;&nbsp;&nbsp;&nbsp;`gc.collect()`\r\n\r\nThe memory is increasing from 0th iteration till i reach 10000 iteration.\r\n\r\nI also tried to see if any operation is added to graph by using `self.sess.graph.finalize()` and it run perfectly well without throwing error which means no new operation is added to graph .\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect memory to be constant. \r\n\r\nHow to solve this memory issue ? ", "comments": ["any updates ?", "@17patelumang Looks like the code is incomplete. Can you please share the complete code with saved model file for us to reproduce this issue. Thanks!", "@gowthamkpr I cannot unfortunately share the saved mode, but for the code the saved model we have TF api ported  inside saved model", "Can you share a dummy simple code for us to reproduce this issue as it it hard to look into it if we cannot reproduce it. Thanks!", "@gowthamkpr thank you for reply below is the code\r\n\r\n\r\n`import tensorflow as tf`\r\n`from tensorflow.python.framework import ops`\r\n`import gc`\r\n`import psutil`\r\n`process = psutil.Process(os.getpid())`\r\n\r\n`N_REPS = 1000`\r\n`sess = tf.Session(graph=tf.Graph())`\r\n`tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)`\r\n`text=\"this is dummy string\"`\r\n\r\n`for i in range(N_REPS):`\r\n    `sess.graph.finalize()`\r\n    `tf.reset_default_graph()`\r\n    `with tf.Graph().as_default() as graph:`\r\n        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`tf.get_default_graph().finalize()`\r\n        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`scores = sess.run(['loss/Sigmoid:0'], feed_dict={'text':text})`\r\n        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`memoryUsed = process.memory_info().rss`\r\n    `del text`\r\n    `gc.collect()`", "I am not able to reproduce the issue using the above code @17patelumang. `saved_model_path` is still missing.", "how to share dummy saved_model ? ", "Can you please share a saved model with which I can reproduce this issue. Thanks!", "@17patelumang \r\nI also met this problem. \r\nHow did you sovle this problem? Thanks a lot", "@mitsuix  1. I checked if i was adding any ops to metagraph every time i do iteration\r\n2. I was finalizing graph after every iteration  `sess.graph.finalize()` \r\n\r\nI was not able to solve it fully however i mitigated it ? , if you can share the code and savedmodel (which i cannot) it would be helpful . ", "@mitsuix @17patelumang Without reproducing the issue, its hard to find the root cause. I would recommend you guys to please share a colab notebook that reproduces the problem. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue as it is not reproducible. Please add additional comments for us to open this issue again. Thanks!", "> @mitsuix @17patelumang Without reproducing the issue, its hard to find the root cause. I would recommend you guys to please share a colab notebook that reproduces the problem. Thanks!\r\n\r\nI am using c++ API, I also encountered this problem in version 1.15.4, session.run will make the memory grow all the time, in addition, I also tested version 2.4.1, 2.5.1, there is the same problem.My model was trained and exported under version 1.13.I can provide a model for you to test, thank you!", "@lichengcheng5945 I met the same problem! Did you solve it ?"]}, {"number": 37959, "title": "ImportError: cannot import name 'load_from_saved_model'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): na\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): colab/ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): pip\r\n- Python version: - Bazel\r\nversion (if compiling from source):1.4.0\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nError raised once \"import tensorflow as tf\"\r\nfrom tensorflow.python.keras.saving.saved_model import load_from_saved_model\r\nImportError: cannot import name 'load_from_saved_model'\r\n\r\nIt seems like it happens today, which \"import tensorflow\" will raise this error once I \"pip install tensorflow-gpu==1.14.0\". \r\n\r\n**Describe the expected behavior**\r\n\r\nTry to run \"from tensorflow.python.keras.saving.saved_model import load_from_saved_model\r\n\"\r\nIt should import correctly without any error.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["BTW, this is the colab link: https://colab.research.google.com/drive/1Jr0j_I_npKQxcQ1ggJQFHTqoJHmt9Bqk", "try `tf.saved_model.load` or`tf.keras.models.load_model`", "@Sanqiang I was able to import tensorflow using tensorflow 1.15. Can you please install tensorflow -gpu 1.15. Also check if any other versions of tensorflow exists. Thanks!", "Close ticket.\r\nIt is because my origiinal TF2.0 is not removed cleanly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37959\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37959\">No</a>\n"]}, {"number": 37958, "title": "Can I get a output shape( or size) of each node from NodeDef ?", "body": "TF version: 1.13.1 (so I am using not eager execution for my project.)\r\n\r\nI want to know the output tensor shape or size of each node before actually executing graph.\r\n(meaning, Graph building part in tensorflow before going into executor part.)\r\n\r\nI guess I can do it by extracting proper information from NodeDef protobuf. There is \"attr\" and I guess it possibly contains the output tensor shape information. However, I don't see it. I printed \"SummarizeAttrs(node_def)\" which is returning \"attr\" in string format. But I can't find the output size info from it. \r\n\r\nThere is some way to get the output shape in python api level. (ref: [)](https://stackoverflow.com/questions/46127471/how-to-get-weights-from-pb-model-in-tensorflow)\r\n\r\nBut I can't do the same in TF source code (c++ core part of tf) level.\r\n\r\nI am pretty sure there is a way. Could anybody know how to do it?\r\n\r\nThank you in advance.\r\n\r\nP.S I looked up other documents and articles, but couldn't find the answer.", "comments": ["this is a part of SummarizeAttr(node_def).\r\n\r\n`node name: MatMul,           Attr: T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"`\r\n\r\nno output shape info...", "The `NodeDef` is a protobuf message defined in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/node_def.proto\r\n\r\nIt does not contain output of the node and the inner implementation largely depends on this...", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37958\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37958\">No</a>\n"]}, {"number": 37957, "title": "segmentation fault", "body": "/home/tf_mtrl# python tf_train.py\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nRetrying in 1 seconds\r\nLoading configuration... done.\r\n/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\r\n  out=out, **kwargs)\r\n/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\r\n  ret = ret.dtype.type(ret / rcount)\r\n~~~ Well done ~~~\r\n~~~ Well done ~~~\r\n~~~ Well done ~~~\r\n~~~ Well done ~~~\r\n~~~ Well done ~~~\r\n~~~ Well done ~~~\r\n!!! Collision !!!\r\n~~~ Well done ~~~\r\n~~~ Well done ~~~\r\nError: tcpip::Socket::recvAndCheck @ recv: peer shutdown\r\nQuitting (on error).\r\nSegmentation fault (core dumped)\r\n", "comments": ["I get callstack as below.\r\nFatal Python error: Segmentation fault\r\n\r\nThread 0x0000ffffbb0d8b20 (most recent call first):\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1443 in _call_tf_sessionrun\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1350 in _run_fn\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1365 in _do_call\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1359 in _do_run\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1180 in _run\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 956 in run\r\n  File \"/home/tf_mtrl_origin_1/tf_adnagent.py\", line 282 in _update_network\r\n  File \"/home/tf_mtrl_origin_1/tf_adnagent.py\", line 236 in train\r\n  File \"tf_train.py\", line 19 in <module>\r\nError: tcpip::Socket::recvAndCheck @ recv: peer shutdown\r\nQuitting (on error).\r\nSegmentation fault (core dumped)", "@tiantaohuawei, Please provide the complete standalone code and Tensorflow version. Thanks!", "standalone is not available. the environment is a bit complicated\u3002tht tf version is 1.15. if you want any log,I can provide.", "@tiantaohuawei, It would be hard to find the root cause of the issue without the standalone code. Thanks!", "Please fill in issue template (close this and reopen a new one if that is easier, tag me in it).\r\n\r\nAlso, please use ` ``` ` around code blocks to make them easier to read. We prioritize issues based on the readability of their description and the presence of a reproducer too", "@tiantaohuawei, Please update the issue with standalone code. Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37957\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37957\">No</a>\n", "@tiantaohuawei . Hi, I also have trouble like you. And now, Have you already solved the problem ? I need help . Thank you.\r\nMy call stack is below: \r\n```shell\r\nThread 0x0000007f96fb5010 (most recent call first):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1443 in _call_tf_sessionrun\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1350 in _run_fn\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1365 in _do_call\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1359 in _do_run\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1180 in _run\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 956 in run\r\n  File \"/home/vdsmbox/VDSMART/Project/ai-eyepro-offline/face_detector.py\", line 52 in __call__\r\n```"]}, {"number": 37956, "title": "Severe performance drop after updating", "body": "**System information** \r\n- Fairly basic Keras/python script\r\n- OS: Manjaro, up to date (as of 3/26)\r\n- Device: Running on CPU (Intel 8th gen)\r\n- TensorFlow (opt-cuda) installed from PacMan\r\n-TF version: 2.1.0-4\r\n- Python version: 3.8.2-1\r\n\r\n**Explanation of issue**\r\nI updated my system earlier today. A few minutes ago I started training a small feed-forward network using a python/Keras script. I had run this script before on a 95% identical training dataset, each epoch took about five seconds according to logs I saved. This time they took about four times as long. No other settings were changed. I double checked with a non-modified dataset and the results were the same.\r\n\r\nAccording to logs, TensorFlow was previously on version 2.1.0-2 (now 2.1.0-4) The performance delta isn't caused by having other programs competing for CPU/RAM, or frequency/thermal throttling. I suppose minor updates to the kernel could cause some performance changes, but I don't see these causing such a profound difference.\r\n\r\nI'd normally not care about a small performance drop, but since this was quite sizable I decided it was worth reporting.", "comments": ["@TheProgrammerIncarnate\r\nCould you please share the code run by you along with complete error logs for us to analyse", "My script is about 350 LoC total, so I condensed it down into only the training-related sections. I'll get back to you shortly with any error logs\r\n\r\n```\r\n#Hyperparameter & constant definitions\r\nself.trainingBatchSize = 256\r\nself.inferenceBatchSize = 65536\r\nself.viewRadius = 4\r\nself.hiddenLayers = 8\r\nself.networkFraction = 0.8\r\nself.dropoutFraction = 0.0#Dropout caused problems with this particular dataset\r\nself.valSplit = 0.3\r\nself.trainingPasses = 50\r\n\r\n#Create four arrays for training and testing the network\r\n#trainingPixels is based on input image\r\nd = self.viewRadius * 2 + 1\r\ntrainingInputs = numpy.zeros((int((self.trainingPixels * (1 - self.valSplit))) +50,d,d,3),dtype=numpy.uint8)\r\ntrainingAnswers = numpy.zeros(int(self.trainingPixels * (1 - self.valSplit)) + 50)\r\ntestingInputs = numpy.zeros((int((self.trainingPixels * self.valSplit)) + 50,d,d,3),dtype=numpy.uint8)\r\ntestingAnswers = numpy.zeros(int(self.trainingPixels * self.valSplit) + 50)\r\n\r\n#==Loops that fill arrays with data here==\r\n\r\n#Setup model & data augmentation then run training\r\nself.model = Sequential()\r\nself.model.add(Flatten(input_shape=(d, d, 3)))\r\nself.model.add(GaussianNoise(0.5))\r\nwhile(self.hiddenLayers>0):\r\n\tself.hiddenLayers = self.hiddenLayers - 1\r\n\tself.model.add(Dense(int(totalInputs*self.networkFraction), activation='relu'))\r\n\tself.model.add(Dropout(self.dropoutFraction))\r\nself.model.add(Dense(len(self.targetColors)+1, activation='softmax'))\r\ndg = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,data_format=\"channels_last\")\r\nself.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\nmodelInfo = self.model.fit_generator(\r\n\t\t\tdg.flow(trainingInputs,trainingAnswers,self.trainingBatchSize),\r\n\t\t\tvalidation_data=(testingInputs,testingAnswers),\r\n\t\t\tsteps_per_epoch=math.ceil(self.trainingPixels / self.trainingBatchSize),\r\n\t\t\tepochs=trainingPasses,\r\n\t\t\tverbose=2)\r\n```", "Here's the terminal output from running it (up to a few epochs)\r\n```\r\n==Top of log==\r\nUsing TensorFlow backend.\r\n2020-03-27 13:08:53.606732: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2020-03-27 13:08:53.612068: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz\r\n2020-03-27 13:08:53.613027: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562af8ccec80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-03-27 13:08:53.613041: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-03-27 13:08:53.613151: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nEpoch 1/50\r\n - 23s - loss: 0.4055 - accuracy: 0.8453 - val_loss: 0.2400 - val_accuracy: 0.9239\r\nEpoch 2/50\r\n - 21s - loss: 0.2194 - accuracy: 0.9296 - val_loss: 0.1794 - val_accuracy: 0.9424\r\nEpoch 3/50\r\n\r\n```\r\nSome other notes, the message about supported vector instructions has always shown up (I have not compiled from source to target this CPU.) The message about creating a thread pool is new however. Note: my task monitor shows all threads being fully utilized when training, same as before.", "@TheProgrammerIncarnate \r\ni do not follow what is meant by versions 2.1.0-2 (now 2.1.0-4) , i have replicated the code shared by you, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/ebb83ff7ae4edc55d6f7792c91beb719/37956.ipynb)\r\n\r\nplease share complete details for us to replicate the issue. if possible please share a colab gist for us to analyse the issue faced.", "@Saduf2019 \r\nAfter poking around a little more, I believe the issue is out of Tensorflow's scope. The drop in performance is apparently caused by updating the python-tensorflow package from release 2 to release 4 (source version is 2.1.0 either way.) Thus this is something I need to talk to the package maintainer about (perhaps something to do with default optimization settings?)", "@TheProgrammerIncarnate\r\nIn that case please confirm if we may move this issue to closed status", "Ok, closing."]}, {"number": 37955, "title": "pathlib.Path support for keras ", "body": "this adds pathlib support to keras functions taking paths as arguments, see #37357", "comments": ["I think we should mostly do the conversion just before calling the filesystem layer (e.g., in `tf.io.gfile....` methods).\r\n\r\nFor the cases where those methods are not used and developers used the natural Python functions, I think we should first attempt a conversion to the wrapped ones. Only if this conversion does not make sense should we try to convert path-like objects to string in the other cases.\r\n\r\nThe utility can reside in TF proper, might need a public `tf.` name (via `tf_export` decorator).\r\n\r\nWe should also get a review from the API owners", "I have added a commit that should add (partial) support for python 3.4/3.5. I have made the import to `os` unconditional, since I expect this to be fast (lookup, since this will be loaded elsewhere), and put the `pathlib` import for the 3.4/3.5 implementation into the function, so that this does not affect import times.", "@ngc92 Can you please address Ubuntu Sanity errors? Thanks!", "I found that tensorflow offers a public API function `tf.compat.path_to_str` which mostly does the same thing my function does, only completely bypassing the python standard library. \r\n\r\nShould I remove my function in favour of the function that is already there?\r\nI think the `compat` implementation does not support `pathlib` for python versions older than the fspath protocol, though (so 3.4 and 3.5).\r\n\r\nThe implementation there is simply\r\n```\r\nif hasattr(path, '__fspath__'):\r\n    path = as_str_any(path.__fspath__())\r\nreturn path\r\n```", "I think that if we find a difference between the two function we can decide then which one to support and which one to deprecate.\r\n\r\nIf, however, both functions behave the same then we should use the existing one.", "there should only be a difference for python 3.4/3.5, because the `__fspath__()` attribute does not exist there.\r\n\r\napart from that, the question is whether it is cleaner to do things with python standard library functions instead of recoding them, i.e. the relevant documentation is\r\n```\r\nos.fspath(path)\r\n    Return the file system representation of the path.\r\n    If str or bytes is passed in, it is returned unchanged. Otherwise __fspath__() is called and its value is returned as long as it is a str or bytes object. In all other cases, TypeError is raised.\r\n    New in version 3.6.\r\n\r\nclass os.PathLike\r\n    An abstract base class for objects representing a file system path, e.g. pathlib.PurePath.\r\n    New in version 3.6.\r\n    abstractmethod __fspath__()\r\n        Return the file system path representation of the object.\r\n        The method should only return a str or bytes object, with the preference being for str.\r\n```\r\n\r\nThe only problem is python 3.5 compatibility.", "Let's keep both for now as we still release 3.5 pips. Once we can deprecate python 3.5 pips we will deduplicate."]}, {"number": 37954, "title": "[Intel MKL] Fixing bfloat16 integration in MatMul and BatchMatMul for\u2026", "body": "\u2026 DNNL1.2\r\n\r\nThis PR corrects bfloat16 integration in MatMul and BatchMatMul kernels for Intel MKL backend.", "comments": ["Thanks for review!"]}, {"number": 37953, "title": "Add Dataset_Ops C++ API for building dataflow graph", "body": "This PR provides a flexible way to build dataflow graphs by using Dataset_Ops C++ API. With this, the predefined `map_graph_def.pbtxt` file will not be needed.\r\n\r\ncc: @aaudiber @jsimsa ", "comments": ["@feihugis Here are the pros/cons I see for using a GraphDef dynamically generated using the C++ API vs using a static GraphDef generated using the Python API:\r\n\r\nPros for C++ API:\r\n- Dynamically generated GraphDefs are always at the latest version. Static graphdefs could become outdated in a new major version.\r\n- It's easier to make small changes to a dynamically generated GraphDef.\r\n\r\nCons for C++ API:\r\n- Internal C++ API is not guaranteed to stay the same, so the test could break due to C++ API changes.\r\n- Writing tests with C++ API requires understanding many internal details of tf.data, such as the presence of the \"RetVal\" node, how to build the function library, how to set the right attrs for each node.\r\n- Readers of the test are more familiar with Python API than C++ API.\r\n\r\nTensorflow [guarantees](https://www.tensorflow.org/guide/versions#compatibility_of_savedmodels_graphs_and_checkpoints) that GraphDefs generated using non-experimental APIs in TF 2.x can be executed through all of TF 2.x as well as 3.x. This mitigates the concern that a static file will become outdated. Given this, I would prefer to stick to the public Python API for generating test GraphDefs, so that we don't need to update the test when the less-stable C++ API changes.", "Thanks very much for the detailed explanation, @aaudiber! I agreed with you. At the beginning, I just considered the flexibility with using C++ API here, but did not realize the cons you mentioned.  Maybe we can close this PR at this moment.\r\n\r\nBTW, do you know what's the plan for the C++ API? I did not see people work on that part. Also, `visibility` in the dataset api defs is set to `HIDDEN` as [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_RangeDataset.pbtxt#L3). Do you know what's the purpose?\r\n", "@feihugis I don't know of any plans to make the C++ API compatible across versions - right now only the C API is covered (https://www.tensorflow.org/guide/versions#what_is_covered). \r\n\r\nI believe the reason for setting Dataset API op defs to HIDDEN is that we don't want users to depend on the op def API, and instead stick to the Python API. In general the quality of our Python API documentation is higher than op documentation, since there is very little interest in the op-level documentation.", "I appreciate the idea of switching to C++ API since it definitely gives some nice benefits, but since there are also significant downsides I agree we should close this PR.", "@aaudiber Gotcha. I will close this PR."]}]