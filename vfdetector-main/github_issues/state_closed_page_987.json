[{"number": 23793, "title": "Tensorflow exception no module found with pyspark", "body": "When I run a program with tensorflow an pyspark together with spark-submit sample.py it says no module named tensorflow but when I run activate tensorflow and run with python sample.py it says no module named pyspark.\r\n\r\nimport tensorflow as tf\r\n\r\nimport pyspark # only run after findspark.init()\r\n\r\nfrom pyspark.sql import SparkSession\r\n\r\nspark = SparkSession.builder.getOrCreate()\r\n\r\ndf = spark.sql('''select 'spark' as hello ''')\r\n\r\ndf.show()", "comments": ["Just want to check. Did you install pyspark on your system before importing?\r\n> pip install pyspark", "Now I have downloaded pyspark and set global variables. Installed\ntensorflow using pip3 as well as anaconda but I am facing the issue\n\nOn Sat, Nov 17, 2018, 4:32 AM ymodak <notifications@github.com> wrote:\n\n> Just want to check. Did you install pyspark on your system before\n> importing?\n>\n> pip install pyspark\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23793#issuecomment-439554617>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALtaV-JbY9oVyiw85LZ2dZFl9LRmzer8ks5uv0P0gaJpZM4Ylsu0>\n> .\n>\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23792, "title": "How can I use tf.assign under mirroredstrategy", "body": "When I used tf.assign under MirroredStrategy, I got valueError(you must specify an aggregation method to update a mirroredVariavle in towercontexy) \r\nHow can I fix it? ", "comments": ["This question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there."]}, {"number": 23791, "title": "TFLite support float input with uint8 inference", "body": "", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23790, "title": "fix tf.Saver save and restore bug under distributed setting(with ps, master and worker)", "body": "This pull request allow using more than one PS in distributed training, saving and restore models without using any shared file stream. The following issues will be fixed, although it is closed:\r\n   https://github.com/tensorflow/tensor2tensor/issues/394\r\n   https://github.com/tensorflow/tensorflow/issues/6374\r\n\r\n@tobyyouup @mrry  @ericyue ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it.", "I signed it.", "CLAs look good, thanks!\n\n<!-- ok -->", "Transferring my review request to @allenlavoie, since this is related to saving checkpoints.", "One high-level question is how this relates to `Saver(sharded=False)`.\r\n\r\nAnother question is what this does to users of shared filesystems who are using the current sharded behavior. Doesn't this create a bunch of extra communication and a bottleneck?", "@allenlavoie\r\n    This modification does not affect tf.Saver(sharded=False), the sharded controls whether the model are saved in one file or multiple files. I test with both sharded = True and sharded = False, both settings work with the current modification. \r\n    As for extra communication, shared file system also involves lots of communication. Moreover, pull parameters from sever is necessary for every iteration. \r\n    I experiment with two PS, the saving time does not increase significantly(13s vs 12s). Experiment with nvidia GPU M40, RFCN model, Resnet-50 backbone, pascal voc datasets, batch_size = 1.\r\n** Two PS after modification: \r\n[INFO] 2018-11-18 16:32:52,305 tf_logging.py:115 : Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into pascal_resnet50_rfcn_model_dis_more/model.ckpt.\r\n[INFO] 2018-11-18 16:33:05,211 tf_logging.py:115 : Saving checkpoints for 0 into pascal_resnet50_rfcn_model_dis_more/model.ckpt.\r\n** One PS before modification:\r\n[INFO] 2018-10-23 11:57:29,569 tf_logging.py:115 : Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into pascal_resnet50_rfcn_model_dis/model.ckpt.\r\n[INFO] 2018-10-23 11:57:41,622 tf_logging.py:115 : Saving checkpoints for 0 into pascal_resnet50_rfcn_model_dis/model.ckpt.", "@allenlavoie I look into the details of tf.train.Saver, the non sharded case are not affected, because no tf.device is set on the call path, so it is default to job:master. I log out the device placement, and it shows that both restore and save operation are placed on job:master. \r\n\r\ntensorflow/core/common_runtime/placer.cc:935] save/SaveV2: (SaveV2)/job:master/replica:0/task:0/device:CPU:0\r\ntensorflow/core/common_runtime/placer.cc:935] save/RestoreV2: (RestoreV2)/job:master/replica:0/task:0/device:CPU:0", "I meant more that this seems like it's re-implementing non-sharded save in the `sharded=True` path. Why doesn't `sharded=False` fix issues caused by not having a shared filesystem?", "@allenlavoie What is the purpose to use the sharded parameter? \r\nIn my understanding, it just split the checkpoint data into multiple file parts.\r\nI do not think a shared file system could avoid network communication overhead. \r\nBesides, many code using tf.Saver default to set sharded = True, such as tf.estimator.\r\nIf sharded is not necessary, should this parameter be dropped?", "My impression is that `sharded=True` is mainly useful if you can't fit all of your variables in memory on a single machine.\r\n\r\nSo you shard the variables among a bunch of parameter servers. But now if you go to save without sharding, you copy all of the variables to a single machine and run out of memory. Theoretically this could be serialized so each shard gets saved before the next gets copied to the machine doing the saving, but that seems very complicated and pretty slow.\r\n\r\nSo instead the assumption is that a distributed filesystem handles the shards, and each parameter server can just dump whatever they have to it. This assumption is annoying if there is no distributed filesystem, but I don't think copying all the shards to a single machine helps. Maybe the discussion should be around whether `sharded=True` is a good default, or whether it needs to be clearer in the documentation that a shared filesystem is an assumption for distributed `Estimator`?\r\n\r\nFWIW `tf.train.Saver` will probably not be in TensorFlow 2.x. We haven't yet settled on an API to expose this kind of sharding behavior.", "I close the pull request. If anyone need my fix, can pull from my forked respository."]}, {"number": 23789, "title": "Dataset.map() with random_shuffle() and num_parallel_calls=1 has non-deterministic result", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 22\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n`tf.data.Dataset.range(100).batch(2).map(lambda x: tf.random_shuffle(x), num_parallel_calls=1)` is non-deterministic. Different runs of the test program produce different output. It's as if the `random_shuffle` is ignoring the random seed. Although adding a seed argument to random_shuffle makes the problem go away, random_shuffle should still use the graph-level random seed when the seed argument is unspecified.\r\n\r\n**Describe the expected behavior**\r\nTwo different runs should always have the same output.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n#!/usr/bin/env python3\r\n\r\nimport tensorflow as tf\r\n\r\ntf.random.set_random_seed(0)\r\nrds = tf.data.Dataset.range(100).batch(2).map(\r\n    lambda x: tf.random_shuffle(x), num_parallel_calls=1)\r\nr = rds.make_one_shot_iterator().get_next()\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    for i in range(4):\r\n        x, = sess.run([r])\r\n        print(x)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n$ py3/rds.py\r\n2018-11-16 15:11:02.272589: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n[1 0]\r\n[3 2]\r\n[4 5]\r\n[7 6]\r\n$ py3/rds.py\r\n2018-11-16 15:11:04.833767: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n[0 1]\r\n[2 3]\r\n[5 4]\r\n[6 7]\r\n```", "comments": ["It seems that both I and the OP are experiencing unexpected nondeterminism from `tf.data.Dataset.map`, although my usage scenario is slightly different.\r\n\r\nI'm using `tf.data.Dataset.map` to sample from `tf.random.uniform(())`.  According to #13932, random ops that are nested in `map` should be deterministic as long as a seed has been set and `num_parallel_calls=1`. However this is not the case for me.  \r\n\r\n```python\r\n#!/usr/bin/env python\r\n\"\"\"Test deterministic behavior of tf.data.Dataset.map when num_parallel_calls=1\r\n\"\"\"\r\nimport tensorflow as tf\r\n\r\n\r\ndef test_0():\r\n    \"\"\"Use Dataset.map to sample from uniformly random distribution.\r\n    \"\"\"\r\n    tf.reset_default_graph()\r\n    tf.random.set_random_seed(9778)\r\n    dset = tf.data.Dataset.range(1)\r\n    dset = dset.map(lambda num: tf.random.uniform(()), num_parallel_calls=1)\r\n    iterator = dset.make_one_shot_iterator()\r\n    sample = iterator.get_next()\r\n    with tf.train.MonitoredSession() as sess:\r\n        print(sess.run(sample))\r\n\r\n\r\ndef test_1():\r\n    \"\"\"Sample from uniformly random distribution without Dataset.map.\r\n    \"\"\"\r\n    tf.reset_default_graph()\r\n    tf.random.set_random_seed(9778)\r\n    sample = tf.random.uniform(())\r\n    with tf.train.MonitoredSession() as sess:\r\n        print(sess.run(sample))\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(\"Using tf.data.Dataset.map with no parallelism:\")\r\n    test_0()\r\n    test_0()\r\n    print(\"Not using tf.data.Dataset.map:\")\r\n    test_1()\r\n    test_1()\r\n```\r\n\r\nOutput:\r\n```bash\r\nUsing tf.data.Dataset.map with no parallelism:\r\n2018-11-29 16:45:40.489347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-11-29 16:45:40.489702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 11.17GiB freeMemory: 504.56MiB\r\n2018-11-29 16:45:40.489728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2018-11-29 16:45:40.823917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-29 16:45:40.823978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2018-11-29 16:45:40.823995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2018-11-29 16:45:40.824164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 234 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n0.8847439\r\n2018-11-29 16:45:40.876451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2018-11-29 16:45:40.876491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-29 16:45:40.876506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2018-11-29 16:45:40.876513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2018-11-29 16:45:40.876616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 234 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n0.66879976\r\nNot using tf.data.Dataset.map:\r\n2018-11-29 16:45:40.904989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2018-11-29 16:45:40.905024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-29 16:45:40.905038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2018-11-29 16:45:40.905059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2018-11-29 16:45:40.905165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 234 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n0.5011252\r\n2018-11-29 16:45:40.935271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2018-11-29 16:45:40.935316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-29 16:45:40.935337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2018-11-29 16:45:40.935353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2018-11-29 16:45:40.951288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 234 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n0.5011252\r\n```\r\n\r\nMy details:\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.19.1\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10 / 7\r\n- GPU model and memory: Tesla K80 / 12GB\r\n", "@jchia I suggest the title be modified to \"Dataset.map() with random ops are non-deterministic with num_parallel_calls=1 and user-defined seed\"", "@shivaniag Assigning this to you, because I think the fix here will depend on switching to the new Python function implementation, and making sure that it respects seeds from the outer context. We should definitely add a test to confirm this.", "This will mostly get fixed with switching to the new python function implementation, which we are expecting to be in by end of this month.  Thank you for the issue. \r\n\r\n@feihugis I will keep this PR on hold for now, since the issue will get fixed with the changes I mentioned above. Thank you for the PR though. ", "@shivaniag Thanks for your updates. Would you mind to share what is the new python function implementation about? So that I could avoid the contribution conflicts.", "@shivaniag Is the \"new python function implementation\" in yet? Will it make it into 1.13?", "@jchia, The new python function implementation is in and this issue has been fixed with that. It will not make it into 1.13, but it is available with nightly build.", "@shivaniag do you know which nightly build it was fixed in?  having trouble running the most recent nightly build, so would like to try an exact one for this fix"]}, {"number": 23788, "title": "fix tf.Saver save and restore bug under distributed setting(with ps, master and worker)", "body": "This pull request allow using more than one PS in distributed training, saving and restore models without using any shared file stream. The following issues will be fixed, although it is closed:\r\n   https://github.com/tensorflow/tensor2tensor/issues/394\r\n   https://github.com/tensorflow/tensorflow/issues/6374\r\n\r\n@tobyyouup @mrry  @ericyue ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "I signed it."]}, {"number": 23787, "title": "fix tf.Saver save and restore bug under distributed setting(with ps, master and worker)", "body": "https://github.com/tensorflow/tensorflow/issues/6374\r\nhttps://github.com/tensorflow/tensor2tensor/issues/394\r\n\r\nThis fix allow distribute training to save and restore models on master without use of any shared filesystem.\r\n@mrry @rsepassi @tobyyouup @ericyue ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it."]}, {"number": 23786, "title": "Update graph_transformations.h", "body": "typo", "comments": ["Nagging Reviewer @martinwicke: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 15 days with no activity and the `awaiting review` label has been applied."]}, {"number": 23785, "title": "Infinite loop when printing CSV data in eager execution mode ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X Mojave\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nbinary via pip (1.12.0)\r\n- TensorFlow version (use command below):\r\nv1.12.0-rc2-3-ga6d8ffae09 1.12.0\r\n- Python version:\r\nPython 3.6.5\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nFollowing the docs at: https://www.tensorflow.org/tutorials/eager/eager_basics\r\nWhen using `ds_file = tf.data.TextLineDataset(filename)` the docs works as expected, but when using `tf.data.experimental.make_csv_dataset` function then then the first statement in the print loop is repeated infinitely.\r\n\r\nExample output\r\n```\r\n1.12.0\r\n\r\nElements in ds_file:\r\nOrderedDict([('Line 1', ), ('1', )])\r\nOrderedDict([('Line 1', ), ('1', )])\r\nOrderedDict([('Line 1', ), ('1', )])\r\nOrderedDict([('Line 1', ), ('1', )])\r\nOrderedDict([('Line 1', ), ('1', )])\r\nOrderedDict([('Line 1', ), ('1', )])\r\nOrderedDict([('Line 1', ), ('1', )])\r\nOrderedDict([('Line 1', ), ('1', )])\r\n... etc\r\n```\r\n\r\n**Describe the expected behavior**\r\nI expect the data API to allow me exploring data in eager execution mode even if I use the CsvDataset class. I can't use `TextLineDataset` as my CSV file contain multi line values in quotation marks. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tempfile\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\ntf.enable_eager_execution()\r\n_, filename = tempfile.mkstemp()\r\n\r\nwith open(filename, 'w') as f:\r\n  f.write(\"\"\"\"Line 1\",1\r\n\"Line 2\",2\r\n\"Line 3\",3\"\"\")\r\n#ds_file = tf.data.TextLineDataset(filename)\r\nds_file = tf.data.experimental.make_csv_dataset(filename, 1)\r\n\r\nprint('\\nElements in ds_file:')\r\nfor x in ds_file:\r\n  print(x)\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nSee above.", "comments": ["This issue is caused by the parameter `num_epochs` for `make_csv_dataset ()`. The default value for `num_epochs` is `None`, which will make the dataset cycle forever as stated [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/readers.py#L383-L384). One solution is to specify the value of `num_epochs` as following:\r\n`ds_file = tf.data.experimental.make_csv_dataset(filename, 1, num_epochs=1)`.\r\n\r\nUsers may easily forget to set `num_epochs` when using `make_csv_dataset ()`. However, the default value of `num_epochs` will cause the infinite loop and break the following pipelines if users don't recognize it. It may be better to remove the default value of `num_epochs` and force users to specify a number for it?\r\n\r\n@mrry @jsimsa @yongtang Do you have any suggestions/comments?\r\n\r\n ", "Changing `num_epochs` to default to 1 makes sense to me, but it would break anyone using the API in Estimator input functions that expect an infinitely repeated stream of inputs.", "Did not realize the Estimator-related part before. Thanks for your explanation, @mrry!", "So I made the suggested change and I'm still trying to replace `tf.data.TextLineDataset(filename)` with `tf.data.experimental.make_csv_dataset(...)` as before. \r\n\r\nMy modified example is not looping forever now, but it still doesn't work as expected in relation to the documentation.\r\n\r\n```python\r\nimport tempfile\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\ntf.enable_eager_execution()\r\n_, filename = tempfile.mkstemp()\r\n\r\nwith open(filename, 'w') as f:\r\n  f.write(\"\"\"\"Line 1\",1\r\n\"Line 2\",2\r\n\"Line 3\",3\"\"\")\r\n#ds_file = tf.data.TextLineDataset(filename)\r\nds_file = tf.data.experimental.make_csv_dataset(\r\n    filename, batch_size=1, num_epochs=1)\r\n\r\nprint('\\nElements in ds_file:')\r\nfor x in ds_file:\r\n  print(x)\r\n\r\n```\r\n\r\nPrints out below.  \r\n```\r\n1.12.0\r\ntf.Tensor(-1, shape=(), dtype=int64)\r\n{'_buffer_size': ,\r\n '_input_dataset': }\r\n\r\nElements in ds_file:\r\nOrderedDict([('Line 1', ), ('1', )])\r\nOrderedDict([('Line 1', ), ('1', )])\r\n```\r\n\r\nAgain my expectation based on the example I'm trying to modify would be\r\n\r\n1. To get three objects printed `OrderedDict([('Line 1', ), ('1', )])` `OrderedDict([('Line 2', ), ('2', )])` etc. \r\n2. Not have the same line printed twice. If I change the batch_size argument to something > 1 the behavior changes to only print a single line.", "When I run your program, I get:\r\n\r\n```\r\nElements in ds_file:\r\nOrderedDict([('Line 1', <tf.Tensor: id=210, shape=(1,), dtype=string, numpy=array(['Line 3'], dtype=object)>), ('1', <tf.Tensor: id=209, shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>)])\r\nOrderedDict([('Line 1', <tf.Tensor: id=214, shape=(1,), dtype=string, numpy=array(['Line 2'], dtype=object)>), ('1', <tf.Tensor: id=213, shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>)])\r\n```\r\n\r\nThis indicates that the first line of the CSV file is interpreted as column names and so you only get two results.\r\n\r\nIndeed, if you read the documentation for [tf.data.experimental.make_csv_dataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset), you will find this behavior documented -- in particular, see the `header` and `column_names` arguments. If I add `header=False` and `column_names=[\"string\", \"number\"]` as arguments to the `make_csv_dataset` call in your program, I get the following input:\r\n\r\n```\r\nElements in ds_file:\r\nOrderedDict([('string', <tf.Tensor: id=322, shape=(1,), dtype=string, numpy=array(['Line 1'], dtype=object)>), ('number', <tf.Tensor: id=321, shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>)])\r\nOrderedDict([('string', <tf.Tensor: id=326, shape=(1,), dtype=string, numpy=array(['Line 3'], dtype=object)>), ('number', <tf.Tensor: id=325, shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>)])\r\nOrderedDict([('string', <tf.Tensor: id=330, shape=(1,), dtype=string, numpy=array(['Line 2'], dtype=object)>), ('number', <tf.Tensor: id=329, shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>)])\r\n```", "Thanks for trying it out. Interesting enough, it seems to be behaving differently for you. Are you using the same version as I outlined in my initial post? \r\n \r\nAs you saw I got the same rows output twice (that was not a typo) and I still get that when I try again. \r\n\r\nGood input about the `column_names`. I went ahead and added the `column_names` argument as you suggested. I now get:\r\n```bash\r\n1.12.0\r\nv1.12.0-rc2-3-ga6d8ffae09 1.12.0\r\n\r\nElements in ds_file:\r\nOrderedDict([('string', ), ('number', )])\r\nOrderedDict([('string', ), ('number', )])\r\n```", "Did you also set `header=False`?", "You're right, I missed that argument. I now get three lines. Still it prints the same output `OrderedDict([('string', ), ('number', )])` and not the data of each line.\r\n\r\nThis issue has been the same throughout: It seems that I only ever get the first line when as output. Regardless if it is an infinite cycle of the dataset or if the dataset is only iterated a single time I only.\r\n\r\nJust to be clear: my intention is not to make this into a support case. I just want to be assured that this behavior is not a bug before I spend more time. \r\n\r\nI will then leave you guys to figure out wether to use `None` as a default value or not. :)\r\n\r\n@jsimsa you do seem to get the expected behavior when printing. What output does below code get you?\r\n\r\n```python\r\nimport tempfile\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\nprint(tf.GIT_VERSION, tf.VERSION)\r\n\r\ntf.enable_eager_execution()\r\n_, filename = tempfile.mkstemp()\r\n\r\nwith open(filename, 'w') as f:\r\n  f.write(\"\"\"\"Line 1\",1\r\n\"Line 2\",2\r\n\"Line 3\",3\"\"\")\r\n#ds_file = tf.data.TextLineDataset(filename)\r\nds_file = tf.data.experimental.make_csv_dataset(\r\n    filename, \r\n    batch_size=1, \r\n    num_epochs=1,\r\n    header=False,\r\n    column_names=[\"string\", \"number\"]\r\n  )\r\n\r\nprint('\\nElements in ds_file:')\r\nfor x in ds_file:\r\n  print(x)\r\n```", "Running your program in an internal version of colab.research.google.com, I get the following output:\r\n\r\n```\r\n1.12.0\r\n('unknown', '1.12.0')\r\n\r\nElements in ds_file:\r\nOrderedDict([('string', <tf.Tensor: id=48, shape=(1,), dtype=string, numpy=array(['Line 2'], dtype=object)>), ('number', <tf.Tensor: id=47, shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>)])\r\nOrderedDict([('string', <tf.Tensor: id=52, shape=(1,), dtype=string, numpy=array(['Line 1'], dtype=object)>), ('number', <tf.Tensor: id=51, shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>)])\r\nOrderedDict([('string', <tf.Tensor: id=56, shape=(1,), dtype=string, numpy=array(['Line 3'], dtype=object)>), ('number', <tf.Tensor: id=55, shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>)])\r\n```\r\n\r\n", "I tried the same code, Its working with my setup as well.\r\n\r\n> 1.12.0\r\n> b'v1.12.0-5399-g37326d1497' 1.12.0\r\n> \r\n> Elements in ds_file:\r\n> WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Colocations handled automatically by placer.\r\n> OrderedDict([('string', <tf.Tensor: id=48, shape=(1,), dtype=string, numpy=array([b'Line 3'], dtype=object)>), ('number', <tf.Tensor: id=47, shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>)])\r\n> OrderedDict([('string', <tf.Tensor: id=52, shape=(1,), dtype=string, numpy=array([b'Line 2'], dtype=object)>), ('number', <tf.Tensor: id=51, shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>)])\r\n> OrderedDict([('string', <tf.Tensor: id=56, shape=(1,), dtype=string, numpy=array([b'Line 1'], dtype=object)>), ('number', <tf.Tensor: id=55, shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>)])\r\n> ", "Hi @magnusart!  your last edited code is not producing  infinite loop issue neither in 1.13 or 2.6 , attached [gist](https://colab.research.google.com/gist/mohantym/7175c5de19129389fd8d0fb3389c16e8/github_23785.ipynb) for reference.\r\n\r\nWe  also see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23785\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23785\">No</a>\n"]}, {"number": 23784, "title": "Bazel 0.19.1 fails: Toolchain identifier 'local' was not found, valid identifiers are [local_linux, local_darwin, local_windows]", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2226a4d599224d4759844db5c80460fafd87145f\r\n- Python version: 2.7.12\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.19.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0\r\n- CUDA/cuDNN version: 9.0/7.2.1.38-1+cuda9.0\r\n- GPU model and memory: V100-PCIE-32GB\r\n\r\n**Describe the problem**\r\n\r\n```\r\n$ bazel clean\r\nStarting local Bazel server and connecting to it...\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\n$ ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.19.1 installed.\r\nPlease specify the location of python. [Default is /home/byronyi/tf/bin/python]:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: 'module' object has no attribute 'getsitepackages'\r\nFound possible Python library paths:\r\n  /home/byronyi/tf/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/byronyi/tf/lib/python2.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]:\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]:\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]:\r\n\r\n\r\nPlease specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]:\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the locally installed NCCL version you want to use. [Default is to use https://github.com/nvidia/nccl]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 7.0\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]:\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]:\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=noignite    \t# Disable Apacha Ignite support.\r\n\t--config=nokafka     \t# Disable Apache Kafka support.\r\nConfiguration finished\r\n$ bazel build -c opt --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nERROR: /home/byronyi/.cache/bazel/_bazel_byronyi/fe336f47afec8b033fb4b29a6628548f/external/local_config_cc/BUILD:57:1: in cc_toolchain rule @local_config_cc//:cc-compiler-k8: Error while selecting cc_toolchain: Toolchain identifier 'local' was not found, valid identifiers are [local_linux, local_darwin, local_windows]\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_cc//:cc-compiler-k8' failed; build aborted\r\nINFO: Elapsed time: 2.701s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (228 packages loaded, 1224 targets configured)\r\n    currently loading: tensorflow/core ... (6 packages)\r\n```\r\n\r\n**Any other info / logs**\r\nManually changing \r\n\r\n```/home/byronyi/.cache/bazel/_bazel_byronyi/fe336f47afec8b033fb4b29a6628548f/external/local_config_cc/BUILD:57``` \r\n\r\nfrom `local` to `local_linux` fix the problem.", "comments": ["#23719 seems to be same issue on Windows with Bazel 0.19.", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This is fixed in master. Closing for now.", "Changing to `local_linux works` with Bazel 0.19.1\r\nI had to come back to this version from Bazel 0.20 as there was another issue with that:\r\n`bazel build issue (does not contain a toolchain for CPU 'k8')`\r\n\r\nAnd of course can't do a pip install because CUDA 9.1 is not supported. And let's not even talk about Python 3.7 compatibility issues.\r\n\r\nSeriously, if any of the (no doubt good) folks at TF are reading this, the barriers to entry / using TensorFlow regularly are so high and painful compared to PyTorch, it's a big turn off.\r\n\r\nI use PyTorch for most of my work, but occasionally I find a repo only in TF and am forced to use it. I regret it every time.", "@cbasavaraj Hi, in which file do I need to make this fix ? I am facing the same issue and when I grep \"local\" in the directory it appears in hundreds of files", "@rohit-gupta See @byronyi 's original post: \r\n`/home/byronyi/.cache/bazel/_bazel_byronyi/fe336f47afec8b033fb4b29a6628548f/external/local_config_cc/BUILD:57 `\r\nIt will be the equivalent location for you", "Or you could also try upgrading your bazel installation. See https://github.com/bazelbuild/bazel/issues/6662", "> Changing to `local_linux works` with Bazel 0.19.1\r\n> I had to come back to this version from Bazel 0.20 as there was another issue with that:\r\n> `bazel build issue (does not contain a toolchain for CPU 'k8')`\r\n> \r\n> And of course can't do a pip install because CUDA 9.1 is not supported. And let's not even talk about Python 3.7 compatibility issues.\r\n> \r\n> Seriously, if any of the (no doubt good) folks at TF are reading this, the barriers to entry / using TensorFlow regularly are so high and painful compared to PyTorch, it's a big turn off.\r\n> \r\n> I use PyTorch for most of my work, but occasionally I find a repo only in TF and am forced to use it. I regret it every time.\r\n\r\nthanks Dude! And true, using/installing pytorch is way to easy comparing with TF... :/", "@cbasavaraj that is so true, same issues, not support to CUDA 9.1, everytime is a struggle with Bazel such as [#24385](https://github.com/tensorflow/tensorflow/issues/24385). There is a great comment in that linked thread. \"[...] Tensorflow stuffs Bazel down our throat. If CMake was a first class citizen a lot of us wouldn't touch bazel with a 10foot pole :)\"", "Hey @edumucelli and others, I installed tf-gpu once recently. Was pleasantly surprised to find the installation went smoothly. I followed the instructions here: https://www.tensorflow.org/install/gpu See in particular https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_10 for the CUDA etc depedencies. \r\n\r\nI still use PyTorch for 95% of my work though. There are a couple of pretrained nets I use tf for, and find that it throws a lot of warnings and messes up my command line logs! That apart, I haven't myself coded up / trained a TF model recently, so I don't know if it's become any easier that it used to be. I guess TF 2.0 will be pretty good."]}, {"number": 23782, "title": "Object detection time increases as the image resolution increase.", "body": "<em> The Object detection inference time  for different Object detection techniques increases as  the increase in Image size(resolution). I did bit of research and came to know that detection api resizes image to certain resolution and pass the resized image to detection process. In case of SSD the images are resized to 300*300 and in case of Faster R-CNN image are resize  between resolution 600 and 1024. However, if this is the case, then such increase in processing time with respect to image size should not make sense as any sized image  will be eventually processed at the standard size  which is 300*300 in case of SSD and  between 600 & 1024 in case of Faster R-CNN. I have experimental result that shows the increase in inference with respect to image size. I am not able to put conclusion for this behavior in words. I know the reason behind this for Yolo but not for other detection framework</em>\r\n**This is my experimental result for SSD, Faster RCNN and Yolo is as follows**\r\n![result](https://user-images.githubusercontent.com/41212548/48574000-3cbfad80-e8cb-11e8-9332-816435401420.png)\r\n\r\n\r\n\r\n", "comments": ["Could you share your pipeline config files for Faster R-CNN, SSD, and Yolo? Can I also check what are the backbone networks you are using for each of them?", "@asguradian Could you fill the [template](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md) with your system details. Also, is it possible for you to check with latest Tensorflow 2.0. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23782\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23782\">No</a>\n"]}, {"number": 23781, "title": "changed the download version of protobuf to 3.6.0", "body": "Fixed issue #22536 ", "comments": ["@angersson  Changes in pr as discussed in #23744 "]}, {"number": 23780, "title": "Repeated \"Already exists: Resource\" errors when I increase my model size", "body": "**System information**\r\n- Custom model built by creating convolutional network tf graph with SGD optimizer and running\r\n- Windows 10\r\n- TensorFlow installed from pip\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6\r\n- Running on cpu\r\n\r\nI've created a convolutional network by making a custom graph for feed forward and using a tensorflow optimizer to train it (I've tried with SGD and Adam). \r\n\r\n```\r\n`sess = tf.Session()\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=.01)\r\n\r\ninput_size = 16\r\ninput_matrix = tf.placeholder(tf.float32, shape=[1,input_size,input_size])\r\nlabel_ph = tf.placeholder(tf.float32, shape=[28*28])\r\nmodel_out = Model.feed_forward(input_matrix)\r\n\r\ninit = tf.global_variables_initializer()\r\nsess.run(init)\r\n\r\nloss = tf.losses.mean_squared_error([tf.convert_to_tensor(label_ph, tf.float32)], [model_out])\r\ntrain = optimizer.minimize(loss)\r\nsess.graph.finalize()`\r\n```\r\n\r\nThis is my graph definition. I know Model.feed_forward is opaque but it's basically a series of functions that get variable weights and feed input through layers. Just to give you an idea here's an example of a convolutional feed forward:\r\n\r\n```\r\n`def feed_input(input_tensors, filters, biases, stride, zero_pad=True):\r\n        if zero_pad:\r\n            pad_size = int((int(filters[0].shape[0]) + 1)/2 - 1)\r\n            input_tensors = tf.map_fn(lambda inp:\r\n                tf.pad(inp, [[pad_size, pad_size], [pad_size, pad_size]]), input_tensors)\r\n\r\n        _, shape_x, _ = input_tensors.shape\r\n\r\n        unflattened_output = tf.map_fn(lambda input_tensor: tf.convert_to_tensor([[[tf.add(tf.tensordot(curr_filter,\r\n            ConvLayer.__get_input_slice__(input_tensor, x, y, tf.shape(curr_filter)[0]), 2), biases[filter_ind])\r\n         for x in range(0, shape_x - curr_filter.shape[0] + 1, stride)]\r\n         for y in range(0, shape_x - curr_filter.shape[0] + 1, stride)]\r\n         for filter_ind, curr_filter in enumerate(filters)], dtype=tf.float32), input_tensors)\r\n\r\n        shape_1, shape_2, shape_3, shape_4 = unflattened_output.shape\r\n\r\n        return tf.reshape(unflattened_output, [shape_1 * shape_2, shape_3, shape_4])`\r\n\r\n```\r\nAfter defining my graph I am running a loop that gets some input and runs sess.run(train, feed_dict={input_matrix:img, label_ph:label_image}). If I make input_size smaller (e.g. 4) or if I reduce the number of layers in my network, everything runs fine and it does exactly what I expect it to do. However, when my network grows in the ways I just mentioned it consistently spits out these errors during sess.run(train, feed_dict={input_matrix:img, label_ph:label_image}):\r\n\r\n> 2018-11-15 00:46:31.069913: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.069914: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.069914: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.069927: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.070138: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.078467: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.080434: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.081575: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.083125: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.085370: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.089281: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.090932: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.091080: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.091398: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.091916: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.092735: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.098503: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.094920: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.099036: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.101236: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.101988: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.101966: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.103264: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.102945: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.106092: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.106858: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.108494: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.109495: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.109987: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.112326: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.112525: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.113595: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.113831: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.115342: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.115418: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.120685: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.122569: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.122708: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.123904: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.124832: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.126926: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.127336: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.129575: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.128498: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.115477: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.130180: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.130601: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.131469: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.135938: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.132834: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.139111: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_20/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.139728: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.140827: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.141429: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.142202: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.140004: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.142621: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.144039: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.147757: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.145043: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.149502: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.146723: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.148971: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.145861: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.149850: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.151642: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.152947: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.154227: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.155666: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.157249: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.157877: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.159295: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_24/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.160204: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.163481: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.163750: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot_256/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.164518: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.166533: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/strided_slice_1/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.172400: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.174768: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n2018-11-15 00:46:31.174793: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_4/gradients/map_22/while/Tensordot/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Aidan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\Aidan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\Users\\Aidan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n\t [[{{node gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var}} = TemporaryVariable[dtype=DT_FLOAT, shape=[5,5], var_name=\"gradients/...dd/tmp_var\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^gradients/map_24/while/Tensordot_580/Reshape_grad/Reshape)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Aidan/Documents/PycharmProjects/SketchIt/src/network/FeedImages.py\", line 57, in <module>\r\n    sess.run(train, feed_dict={input_matrix:img, label_ph:label_image})\r\n  File \"C:\\Users\\Aidan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\Aidan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\Aidan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\Aidan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError: Resource __per_step_4/gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/struct tensorflow::TemporaryVariableOp::TmpVar\r\n\t [[{{node gradients/map_24/while/Tensordot_512/transpose/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var}} = TemporaryVariable[dtype=DT_FLOAT, shape=[5,5], var_name=\"gradients/...dd/tmp_var\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^gradients/map_24/while/Tensordot_580/Reshape_grad/Reshape)]]\r\n\r\nAnyone know what might be the cause? Maybe some concurrency issue since its telling me things that shouldn't exist have already been created? Help would be much appreciated, I'm close to making this thing work! Thanks for your time.", "comments": ["What is \"AddOpsRewrite_Add/tmp_var/struct\" in your graph? Why are you using temporary variables? They are not compatible with while loops, which is what you're seeing in the error message.\r\n\r\n(I thought temporary variables had been removed as part of tf's API a long time ago)", "Thanks for the response.\r\n\r\nThose are created by sess.run(train) where train is GradientDescentOptimizer.minimize. So tensorflow is generating them, not me.\r\n\r\nAlso I should update that I rolled back to tensorflow 1.8 (haven't tried 9, 10, or 11 just due to lack of time) and the error no longer comes up.", "Issue close was accidental", "What is ArithmeticOptimizer in your code? These ops don't look like they were added by TF's backward pass.", "Hm this might come from grappler.", "@rmlarsen I see that the memory optimizer adds temporaryvariable. This is not safe to do inside a while loop because the temporaryvariable op has weird issues with the resource manager.", "ArithmeticOptimizer comes from running GradientDescentOptimizer.minimize as well. I'm running train in a few nested python for loops. I figured that's super typical, but should I be making a list of inputs and feeding them in with one call instead?\r\n\r\nIf anyone's interested you can see exactly what I'm running here. I already uploaded the network on a different account. https://github.com/awoods12/Sketch_It ", "Could it be an issue about `tf.map_fn` rather than optimizer?\r\nI met the same errors and the core part of my code was like\r\n```python\r\ny = tf.map_fn(some_function, x)\r\n```\r\nI found it worked when `x` had shape `[1]`, but got errors otherwise.\r\nAnd then I found another way without `map_fn` and everything works well now.", "The bug seems to be in a Grappler optimization.\n\nOn Mon, Dec 3, 2018 at 7:07 AM yangyangxcf <notifications@github.com> wrote:\n\n> Could it be an issue about tf.map_fn rather than optimizer?\n> I met the same errors and the core part of my code was like\n>\n> y = tf.map_fn(some_function, x)\n>\n> I found it worked when x had shape [1], but got errors otherwise.\n> And then I found another way without map_fn and everything works well now.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23780#issuecomment-443740562>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxY7VtRUpvdBNn-8g8JG3V6oETeCJks5u1T42gaJpZM4Yg0tH>\n> .\n>\n\n\n-- \n - Alex\n", "I have the same problem on TF 1.12 + Ubuntu 16.04 and my custom code. Weirdly enough, even though I share the `vact_scope` code in different networks, the problem does not happen for all of them (It might be due to calling `tf.nn.dynamic_rnn` in the same scope twice with `scope.reuse_variables()` between the calls)\r\n\r\n@ezhulenev, any news on the problem? Any workarounds? Maybe there's a way to disable the `ArithmeticOptimizer`? (or whole Grappler? What is the expected performance hit then?)\r\n\r\n```\r\n2019-02-07 17:26:05.004296: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn1/while/vact_scope/vact_iteration/prior_halting_logits/dense/BiasAdd/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19Te\r\nmporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.030579: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn2/while/vact_scope/vact_iteration/vact_iteration/output_projection_wrapper/BiasAdd_10/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/\r\nN10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.031806: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn2/while/vact_scope/vact_iteration/vact_iteration/output_projection_wrapper/BiasAdd_10/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/\r\nN10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.048779: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn1/while/vact_scope/vact_iteration/prior_halting_logits/dense/BiasAdd/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19Te\r\nmporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.052690: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn2/while/vact_scope/vact_iteration/vact_iteration/output_projection_wrapper/BiasAdd_10/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/\r\nN10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.054059: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn2/while/vact_scope/vact_iteration/vact_iteration/output_projection_wrapper/BiasAdd_10/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/\r\nN10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.056958: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn1/while/vact_scope/vact_iteration/prior_halting_logits/dense/BiasAdd/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19Te\r\nmporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.062997: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn1/while/vact_scope/vact_iteration/prior_halting_logits/dense/BiasAdd/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19Te\r\nmporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.074335: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn2/while/vact_scope/vact_iteration/vact_iteration/output_projection_wrapper/BiasAdd_10/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/\r\nN10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.093691: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn1/while/vact_scope/vact_iteration/prior_halting_logits/dense/BiasAdd/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19Te\r\nmporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.106734: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn2/while/vact_scope/vact_iteration/vact_iteration/output_projection_wrapper/BiasAdd_10/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/\r\nN10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.128811: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn1/while/vact_scope/vact_iteration/prior_halting_logits/dense/BiasAdd/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19Te\r\nmporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.145847: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn1/while/vact_scope/vact_iteration/prior_halting_logits/dense/BiasAdd/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19Te\r\nmporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.147453: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn2/while/vact_scope/vact_iteration/vact_iteration/output_projection_wrapper/BiasAdd_10/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/\r\nN10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.164530: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn1/while/vact_scope/vact_iteration/prior_halting_logits/dense/BiasAdd/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19Te\r\nmporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.168118: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn2/while/vact_scope/vact_iteration/vact_iteration/output_projection_wrapper/BiasAdd_10/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/\r\nN10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.174878: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn2/while/vact_scope/vact_iteration/vact_iteration/output_projection_wrapper/BiasAdd_10/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/\r\nN10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.181530: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn1/while/vact_scope/vact_iteration/prior_halting_logits/dense/BiasAdd/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19Te\r\nmporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.242049: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn1/while/vact_scope/vact_iteration/prior_halting_logits/dense/BiasAdd/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19Te\r\nmporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.287269: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn2/while/vact_scope/vact_iteration/vact_iteration/output_projection_wrapper/BiasAdd_10/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/\r\nN10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.375503: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn1/while/vact_scope/vact_iteration/prior_halting_logits/dense/BiasAdd/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19Te\r\nmporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.737421: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn1/while/vact_scope/vact_iteration/prior_halting_logits/dense/BiasAdd/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19Te\r\nmporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.737421: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn2/while/vact_scope/vact_iteration/vact_iteration/output_projection_wrapper/BiasAdd_10/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/\r\nN10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.756781: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn1/while/vact_scope/vact_iteration/prior_halting_logits/dense/BiasAdd/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19Te\r\nmporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.757632: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn2/while/vact_scope/vact_iteration/vact_iteration/output_projection_wrapper/BiasAdd_10/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/\r\nN10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-02-07 17:26:05.915909: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_186/OptimizeLoss/gradients/network/rnn2/while/vact_scope/vact_iteration/vact_iteration/output_projection_wrapper/BiasAdd_10/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/\r\nN10tensorflow19TemporaryVariableOp6TmpVarE\r\n```\r\n\r\nWeirdly, I see strange nested scopes (__per_step_186/OptimizeLoss/gradients/network/rnn2/while/vact_scope/**vact_iteration/vact_iteration**/output_projection_wrapper/BiasAdd_10/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/\r\nN10tensorflow19TemporaryVariableOp6TmpVarE), which I do not create in the code (the GLOBAL_VARIABLES collection does not contain any variables with `vact_iteration/vact_iteration` in the name)", "You can disable all optimizations in arithmetic optimizer with something like:\r\n\r\n```py\r\n    from google3.third_party.tensorflow.core.protobuf import rewriter_config_pb2\r\n    \r\n    config_proto = tf.ConfigProto()\r\n\r\n    off = rewriter_config_pb2.RewriterConfig.OFF\r\n    config_proto.graph_options.rewrite_options.arithmetic_optimization = off\r\n    \r\n    session = tf.Session(config=config_proto)\r\n```\r\n\r\nanother example in https://github.com/tensorflow/tensorflow/blob/b17d53c0cd07bbae46a55dc9c12d3f48a1c19604/tensorflow/python/debug/lib/debug_graph_reconstruction_test.py#L45-L51\r\n\r\nBut I suspect that the real problem is in memory optimizer, you can disable it with this option: https://github.com/tensorflow/tensorflow/blob/4e19402174593095a0837d6e7a8ca3f686b67543/tensorflow/core/protobuf/rewriter_config.proto#L122\r\n\r\nCan you try if disabling one of them helps.", "Either `arithmetic_optimization = off` or `memory_optimization = off` does the job, thanks!\r\n\r\n**UPD 8/02**: my problem was also caused by the model size, disregard the \"calling `tf.nn.dynamic_rnn` in the same scope twice\" in my previous comment.", "@artsobolev Out of curiosity, what was the issue with the model size?  \r\n\r\nI was running into the same issue as described but was able to 'fix' it by disabling arithmetic optimizations via \r\n\r\n```\r\n    from google3.third_party.tensorflow.core.protobuf import rewriter_config_pb2\r\n    \r\n    config_proto = tf.ConfigProto()\r\n\r\n    off = rewriter_config_pb2.RewriterConfig.OFF\r\n    config_proto.graph_options.rewrite_options.arithmetic_optimization = off\r\n    \r\n    session = tf.Session(config=config_proto)\r\n```\r\nwithout having to disable memory optimization.\r\n\r\nAlso, I was wondering if anyone might have any additional insight into this issue and/or any ideas about possibly fixing it instead of just disabling arithmetic optimizations?", "Do you have minimal reproducible example? I have a few theories, but small example would help.", "@saforem2, disabling optimizations worked for me as well, but so did decreasing the hidden layer size in the LSTM from 512 to 128. Apparently, the optimizer behaves differently in these cases.\r\n\r\n@ezhulenev, unfortunately, I do not have such example.", "@ezhulenev Unfortunately I don't have a minimal reproducible example either.  \r\n\r\nIf you really think it would help, I can work on writing up a gist or walking through the steps to reproduce the error, but I think it would require cloning a (moderately sized) repository of mine.", "I'll try to work on a fix, I'll let you know when it will be in master, so you could test it.", "Hello,\r\nI am facing a similar issue but I can't seem to find this ```google3``` library to import the ```off ``` constant mentioned by the comments above.\r\n\r\nCan anyone tell me where to get ```google3``` ?\r\nThank you.", "I also meet the same issue with tf.nn.conv1d, by using\r\n```\r\n    config_proto = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\r\n    off = rewriter_config_pb2.RewriterConfig.OFF\r\n    config_proto.graph_options.rewrite_options.arithmetic_optimization = off\r\n\r\n    sess = tf.Session(config=config_proto)\r\n```\r\n\r\nthe issue is fixed. So i think maybe tf1.13 has some inner bug.", "I had the same problem about import rewriter_config_pb2 from google3\r\n ```\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\n\r\nconfig_proto = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\r\n\r\noff = rewriter_config_pb2.RewriterConfig.OFF\r\n\r\nconfig_proto.graph_options.rewrite_options.arithmetic_optimization = off\r\n\r\nsession = tf.Session(config=config_proto)\r\n\r\n\r\n```", "Just remove \"google3\" from the example and it should work.", "But I had another problem I used python3.6 and I countered this problem \"AlreadyExists error\"\r\n", "I tried to disable the arithmetic optimizer but I don't know where does it ? in  version python3.7, TF1.13.1\r\nI access the arithmetic optimizer.h and disabled all variables. But it hasn't effect on the error.\r\n\r\n```\r\n\r\n2019-05-10 23:23:48.280599: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.280882: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.281480: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.281866: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.282864: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.283515: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.284000: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.284436: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.284976: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.285390: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.285765: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.286219: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.286526: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.287017: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.287346: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.287845: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.288143: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.288607: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.289050: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.289331: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.289795: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.290269: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.290532: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.291102: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.292775: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.293485: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.293981: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.294523: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.295292: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.296044: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2019-05-10 23:23:48.296854: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n\t [[{{node training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"Generator2.py\", line 232, in <module>\r\n    estimator_model.train(input_fn=customTrain, steps=100)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 358, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1124, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model_default\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1407, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 676, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1171, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1270, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python3.7/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1255, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1327, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1091, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError: Resource __per_step_12/training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n\t [[{{node training/TFOptimizer/gradients/lstm/while/ReadVariableOp_8/Enter_grad/ArithmeticOptimizer/AddOpsRewrite_Add/tmp_var}}]]\r\n'''\r\n```", "If you are using the estimators API, try something like this:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\nsession_config = tf.ConfigProto()\r\noff = rewriter_config_pb2.RewriterConfig.OFF\r\nsession_config.graph_options.rewrite_options.arithmetic_optimization = off\r\nrun_config = tf.estimator.RunConfig(session_config=session_config)\r\n...\r\nestimator = tf.estimator.Estimator(model_fn, model_dir, run_config)\r\n```\r\n\r\nNote to developers: this issue is not fixed. Theses are just workarounds as far as I can understand. Could you please re-open the issue?", "Thank so much. I will try then If it's still existing. I will re-open issue @183amir ", "Perfect solution. I ran my model on non GPU machine and ended up with this error, on GPU machine it works totally fine.. Does presence/absence of GPU anything to do with arithmetic optimization?", "Same problem here. Workaround is not a fix.\r\nReopen it.", "Thank you for the report. We will fix the corresponding rewrite in Grappler. It looks like it generates non-unique node names.", "I have this exact same issue in tf 2.0 beta0.  Is there a workaround for that?\r\n\r\nI tried:\r\n```\r\ntf.config.optimizer.set_experimental_options({'arithmetic_optimization': False})\r\n```\r\nand\r\n```\r\ntf.config.optimizer.set_experimental_options({'arithmetic_optimization': rewriter_config_pb2.RewriterConfig.OFF})\r\n```\r\nbut it didn't help.\r\n\r\nE: For me issue happens only with AdamOptimizer. RMSProp is fine", "I'm having the same issue and I can't get the workaround to work. Do I do something with the `tf.Session` instance after creating it or will it automatically be used?\r\n\r\nFor now I just try different sequence lengths in my model, some work, some don't.\r\n\r\nEDIT: Figured it out: To make keras use the model call `tensorflow.keras.backend.set_session` with the model instance.", "tensorflow.keras.backend.set_session worked. using TF 1.13", "For me, either reducing the graph size or `memory_optimization  = off` helped. TF 1.14\r\n\r\nAs follows: \r\n```\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\n\r\nconfig_proto = tf.ConfigProto()\r\noff = rewriter_config_pb2.RewriterConfig.OFFrite_options.arithmetic_optimization = off\r\nconfig_proto.graph_options.rewrite_options.memory_optimization  = off\r\n```", "Here is the corrected code example:\r\n\r\n```python\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\nconfig = tf.ConfigProto()\r\noff = rewriter_config_pb2.RewriterConfig.OFF\r\nconfig.graph_options.rewrite_options.memory_optimization  = off\r\nreturn tf.Session(config=config)\r\n```", "For me making the second parameter of Embedding layer smaller (LSTM) worked.", "@aidanw95 \r\nIt looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version (2.6.0) and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23780\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23780\">No</a>\n", "This is definitely an issue in Tensorflow V2.6.0. \r\nI'm using python 3.9.\r\nTensorflow installed using binaries(pip).\r\nCode runs on a CPU.\r\n\r\nLike the comments above suggest, turning off arithmetic optimization was also the ~fix~ workaround for me. But their code is not compatible with V2, so I'll post a snippet with what worked. I used the docs on graph optimization for reference: https://www.tensorflow.org/guide/graph_optimization.\r\n\r\n```\r\nimport contextlib\r\n@contextlib.contextmanager\r\ndef options(options):\r\n  old_opts = tf.config.optimizer.get_experimental_options()\r\n  tf.config.optimizer.set_experimental_options(options)\r\n  try:\r\n    yield\r\n  finally:\r\n    tf.config.optimizer.set_experimental_options(old_opts)\r\n\r\ndef test_function(body):\r\n  with options({'arithmetic_optimization': False}):\r\n          tf_tensor = tf.constant([body])\r\n          if model:\r\n              tensor_object = model(tf_tensor)\r\n.....\r\n```\r\nI'm not sure if this is an issue because I'm running this on a CPU but it seems like if the code isn't able to detect a GPU then these should be sensibly turned off. "]}, {"number": 23779, "title": "Error on Label's datatype when enable eager execution in Colab ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.14.1 - Colab\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None\r\n- **TensorFlow installed from (source or binary)**: Colab build in\r\n- **TensorFlow version (use command below)**: 1.12.0\r\n- **Python version**: Colab Python2\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\n\r\n### Describe the problem\r\nWhen I run the simple DNN on MNIST without eager execution, it works fine. However, when I enable eager execution then fit the model, I got the error said that my `train_labels` and `test_labels` has to be `int64` tensor which are originally `uint8` tensor. The error is shown as below:\r\n\r\n`InvalidArgumentError: cannot compute Equal as input #0(zero-based) was expected to be a int64 tensor but is a uint8 tensor [Op:Equal]`\r\n\r\nAlthough, I can fix it by using `train_labels = train_labels.astype(np.int64)` ,It looks like the error should be fixed in the source code.\r\n\r\n\r\n\r\n### Source code to reproduce the issue in Colab\r\n\r\n```python\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.enable_eager_execution()\r\n\r\nmnist = tf.keras.datasets.mnist\r\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28,28)),\r\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(128, activation=tf.nn.relu),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\n\r\nmodel.compile(optimizer = tf.train.AdamOptimizer(0.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\r\n\r\nmodel.summary()\r\n\r\nhistory = model.fit(x = train_images,y = train_labels,validation_split=0.1,batch_size = 256,verbose=2,shuffle = True,epochs=10)\r\n\r\n```\r\n\r\n\r\n### Other info / logs\r\n\r\n\r\n```Python traceback\r\n\r\nInvalidArgumentErrorTraceback (most recent call last)\r\n<ipython-input-6-7bdf2a937371> in <module>()\r\n      5                     verbose=2,\r\n      6                     shuffle = True,\r\n----> 7                     epochs=10)\r\n      8 ###########################################\r\n .\r\n .\r\n .\r\n .\r\n .\r\n .     \r\n      \r\n/usr/local/lib/python2.7/dist-packages/six.pyc in raise_from(value, from_value)\r\n    735 else:\r\n    736     def raise_from(value, from_value):\r\n--> 737         raise value\r\n    738 \r\n    739 \r\n\r\nInvalidArgumentError: cannot compute Equal as input #0(zero-based) was expected to be a int64 tensor but is a uint8 tensor [Op:Equal]\r\n\r\n```\r\n\r\n\r\n\t\t", "comments": ["@dailiang18bb I am able to reproduce with Tensorflow 1.12 but not tf-nightly. Can you give tf-nightly a try and see if the issue has been fixed?", "An easy workaround it to change the types in y_train and y_test to np.int64.\r\n\r\n```import tensorflow as tf\r\nimport numpy as np\r\nmnist = tf.keras.datasets.mnist\r\n\r\ntf.enable_eager_execution()\r\n\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\ny_train = y_train.astype(np.int64)\r\ny_test = y_test.astype(np.int64)\r\n\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(),\r\n  tf.keras.layers.Dense(512, input_shape=(784,), activation=tf.nn.relu),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, input_shape=(512,), activation=tf.nn.softmax)\r\n])\r\nmodel.compile(optimizer=tf.train.AdamOptimizer(),\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\nmodel.evaluate(x_test, y_test)```", "@yongtang Thanks for your reply, I tried install tf.nightly in Colab by the following code, problem solved! Looks like the bug has been fixed in the nightly version. \r\n```\r\n!pip uninstall tensorflow -y\r\n!pip install --upgrade tf-nightly\r\nimport tensorflow as tf\r\ntf.VERSION\r\n```\r\n* TensorFlow version: `1.13.0-dev20181128`\r\n\r\n------\r\n\r\n@ThomasHagebols Thanks for your reply, have a great day!\r\n\r\n------\r\n\r\nThis issue can be closed."]}, {"number": 23778, "title": "cannot conver mobilenet pb to tflite", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.12.0\r\n- Python version:2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nIm trying to convert pb to tflite, i run my model using Mobilenet v2, \r\nwhen trying to convert im getting the following error.\r\n\r\nbazel-bin/tensorflow/contrib/lite/toco/toco --input_file=/home/dell/TFObject-detection/tensorflow/models/research/object_detection/Output/frozen_inference_graph.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=frozen_inference_graph.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=ImageTensor --output_arrays=SemanticPredictions --input_shapes=1,512,512,3 --allow_nonexistent_arrays\r\n\r\n\r\n2018-11-15 09:54:38.341689: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 0 operators, 2 arrays (0 quantized)\r\n2018-11-15 09:54:38.341711: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:135] Model is empty!!!\r\n2018-11-15 09:54:38.341737: W tensorflow/contrib/lite/toco/tooling_util.cc:1246] Fixing constant output array SemanticPredictions by inserting a copy. This is not optimal.\r\n2018-11-15 09:54:38.341749: F ./tensorflow/contrib/lite/toco/model.h:1984] Check failed: has_shape() \r\nAborted (core dumped)\r\n\r\n\r\nis that related to this issue? any hint?", "comments": ["using this tutorial i converted my model\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md", "> using this tutorial i converted my model\r\n> https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md\r\n\r\n\r\n\r\n> _Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template_\r\n> \r\n> **System information**\r\n> \r\n> * Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n> * TensorFlow installed from (source or binary):binary\r\n> * TensorFlow version (use command below):1.12.0\r\n> * Python version:2.7\r\n> * Bazel version (if compiling from source):\r\n> * GCC/Compiler version (if compiling from source):\r\n> * CUDA/cuDNN version:\r\n> * GPU model and memory:\r\n> \r\n> You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n> You can also obtain the TensorFlow version with\r\n> python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n> \r\n> Im trying to convert pb to tflite, i run my model using Mobilenet v2,\r\n> when trying to convert im getting the following error.\r\n> \r\n> bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=/home/dell/TFObject-detection/tensorflow/models/research/object_detection/Output/frozen_inference_graph.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=frozen_inference_graph.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=ImageTensor --output_arrays=SemanticPredictions --input_shapes=1,512,512,3 --allow_nonexistent_arrays\r\n> \r\n> 2018-11-15 09:54:38.341689: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 0 operators, 2 arrays (0 quantized)\r\n> 2018-11-15 09:54:38.341711: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:135] Model is empty!!!\r\n> 2018-11-15 09:54:38.341737: W tensorflow/contrib/lite/toco/tooling_util.cc:1246] Fixing constant output array SemanticPredictions by inserting a copy. This is not optimal.\r\n> 2018-11-15 09:54:38.341749: F ./tensorflow/contrib/lite/toco/model.h:1984] Check failed: has_shape()\r\n> Aborted (core dumped)\r\n> \r\n> is that related to this issue? any hint?\r\n\r\nHI, how did you solved your problem?"]}, {"number": 23777, "title": "[tflite] set eigen threads to 0 when recommended_num_threads == 1", "body": "Avoid creating unused idle thread", "comments": ["@mzakharocsc Can you please solve the merge conflicts to move ahead with this PR? Thanks", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Thread creation is now lazy. I believe you can avoid the Eigen thread creation by setting the thread count to 1, and it will avoid creating the Eigen thread pool entirely. That should be safer than trusting Eigen to properly handle a value of 0."]}, {"number": 23776, "title": "Unicode error while testing", "body": "Error for inverted question mark : UnicodeEncodeError: 'ascii' codec can't encode character u'\\xbf' in position 8: ordinal not in range(128)\r\nError for not adding u' ' : TypeError: normalize() argument 2 must be unicode, not str", "comments": ["![screen shot 2018-11-15 at 7 34 11 pm](https://user-images.githubusercontent.com/17671208/48558479-61d60f80-e90f-11e8-954f-e04e4e99bece.png)\r\n![screen shot 2018-11-15 at 7 35 50 pm](https://user-images.githubusercontent.com/17671208/48558480-626ea600-e90f-11e8-9fb4-a27a366b92bb.png)\r\n"]}, {"number": 23775, "title": "Patch 1", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 23774, "title": "\"ValueError: list.remove(x): x not in list\" in eager mode for decorated keras method", "body": "The below runs without the decorator but when the decorator is added then it fails with the above error.\r\n\r\ntf.__version__, keras.__version__ = ('1.12.0', '2.1.6-tf')\r\nubuntu 16.04\r\n```\r\nimport tensorflow.keras.layers as KL\r\nfrom functools import wraps\r\n\r\ndef decorator(f):\r\n    @wraps(f)\r\n    def wrapper(*args, **kwargs):\r\n        return f(*args, **kwargs)\r\n    return wrapper\r\n\r\nclass MyLayer(KL.Layer):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n\r\n    @decorator\r\n    def call(self, inputs):\r\n        pass\r\na=MyLayer()\r\n```", "comments": ["I discovered a solution. Just pip install decorator; then decorate my decorator with @decorator.decorator. This keeps the original argument names for inspect to find.", "On second thoughts this should be fixed in tensorflow as workarounds are fiddly. Typically a decorator would take *args, **kwargs if it could be used on a function or class method. In function_utils.py line 58 it has  args.remove('self'). Changing that to args.pop(0) would I think resolve.", "I'm afraid `args.pop(0)` cannot solve it, could you explain why to use decorator for `__call__` method?", "I have a decorator that saves the parameters and results of decorated\nfunctions. This is useful to compare two versions. For example I have\nworking versionA and decorate it so it saves baseline inputs and results\n(in eager mode). Then I use those inputs on versionB and compare the\nresults. This allows me to simplify or make code faster but to test it\nstill produces same results; or to port between keras/pytorch and compare\nthe two versions.\n\nWhy wouldn't args.pop(0) solve it? It is only executed for a bound\nfunction.\n\nOn Fri, 16 Nov 2018 at 01:44, Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> I'm afraid args.pop(0) cannot solve it, could you explain why to use\n> decorator for __call__ method?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23774#issuecomment-439252689>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABJN6VjKYxbuhqbZ4zhsC9jcj6gk7Ayjks5uvhiSgaJpZM4Yf3JY>\n> .\n>\n", "This is fixed in latest tf-nightly version 1.15.0-dev20190725. Thanks!\r\nhttps://github.com/tensorflow/tensorflow/blob/4ad169057cf0adb51767914e84c300c95ce748d5/tensorflow/python/util/function_utils.py#L62"]}, {"number": 23773, "title": "ERRoR in running with Gpu", "body": "How to solve this Error: \r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation init: Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.", "comments": ["Do you have any other logs? Are you using Linux or Windows? What version of tensorflow? Nvidia or AMD GPU? It's easier to help you if you are as descriptive as possible. Thanks!", "Make sure you have tensorflow GPU version installed and/or its point to the write env which has GPU version installed\r\n Also check what devices are getting recognized . If you don't see GPU then its not the right env or TF-gpu.\r\n uninstall tensorflow and reinstall tensorflow-gpu\r\n//////////// Try this below \r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import device_lib\r\nprint(device_lib.list_local_devices())\r\n/////////// You should get logs like below with various devices getting recognized\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 17806258439xxxxxxxxx \r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 3170530099\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 141133486494xxxxxxxxxxxxxxx\r\nphysical_device_desc: \"device: 0, name: GeForce GTX 970, pci bus id: 0000:02:00.0, compute capability: 5.2\"\r\n, name: \"/device:GPU:1\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 3170530099\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 163427xxxxxxxxxxxxxxxxxxxxxxxx \r\nphysical_device_desc: \"device: 1, name: GeForce GTX 970, pci bus id: 0000:03:00.0, compute capability: 5.2\"\r\n]\r\n## Try this too \r\nif tf.test.gpu_device_name():\r\n    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\r\nelse:\r\n    print(\"Please install GPU version of TF\")\r\n////////////////////// output\r\nDefault GPU Device: /device:GPU:0\r\n/////////////////////////////\r\nas n0t0ri0us mentioned please provide more details about the environment(linux,osx,windows,gpu card, pip/conda etc) and show some complete logs. Lets us know what worked .", "*Hi*\n*thanks for your answer*\n*I tried that statement*\n*the output is:*\n\nname: \"/device:CPU:0\"\n2018-11-17 17:25:27.712583: I\ntensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports\ninstructions that this TensorFlow binary was not compiled to use: AVX2\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 12899223527737949935\n]\nPlease install GPU version of TF\n\n*But my syatem has Gpu*\n*what can I do?*\n\nOn Fri, Nov 16, 2018 at 12:00 PM Simhan Ramakrishnan <\nnotifications@github.com> wrote:\n\n> Make sure you have tensorflow GPU version installed or its point to the\n> write env which has GPU version installed\n> You also check what devices are getting recognized . If you don't see GPU\n> then its not the right env or TF. uninstall tensorflow and reinstall\n> tensorflow-gpu\n> //////////// Try this below\n> import tensorflow as tf\n> from tensorflow.python.client import device_lib\n> print(device_lib.list_local_devices())\n> /////////// You should get something logs like below\n> [name: \"/device:CPU:0\"\n> device_type: \"CPU\"\n> memory_limit: 268435456\n> locality {\n> }\n> incarnation: 17806258439xxxxxxxxx\n> , name: \"/device:GPU:0\"\n> device_type: \"GPU\"\n> memory_limit: 3170530099\n> locality {\n> bus_id: 1\n> links {\n> }\n> }\n> incarnation: 141133486494xxxxxxxxxxxxxxx\n> physical_device_desc: \"device: 0, name: GeForce GTX 970, pci bus id:\n> 0000:02:00.0, compute capability: 5.2\"\n> , name: \"/device:GPU:1\"\n> device_type: \"GPU\"\n> memory_limit: 3170530099\n> locality {\n> bus_id: 1\n> links {\n> }\n> }\n> incarnation: 163427xxxxxxxxxxxxxxxxxxxxxxxx\n> physical_device_desc: \"device: 1, name: GeForce GTX 970, pci bus id:\n> 0000:03:00.0, compute capability: 5.2\"\n> ]\n> Try this too\n>\n> if tf.test.gpu_device_name():\n> print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n> else:\n> print(\"Please install GPU version of TF\")\n> ////////////////////// output\n> Default GPU Device: /device:GPU:0\n> /////////////////////////////\n> as n0t0ri0us mentioned please provide more details about the\n> environment(linux,osx,windows,gpu card etc) and show some complete logs.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23773#issuecomment-439317783>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AqoKEn5ZIRyfCaa5G3DfYAWlM6fcuD0iks5uvneIgaJpZM4Yf3BI>\n> .\n>\n", "As it says \"Please install GPU version of TF\"\r\nTensorflow magically cannot run on GPU and i don't know much about your environment . You also have to make sure correct nvidia drivers have been installed.  \r\nPlease follow these instructions from here and for the right OS. Use anaconda if possible. \r\nhttps://www.tensorflow.org/install/   and post your comments on stack overflow instead of raising an issue", "*I'm using Windows10*\n*version of tensorflow 1.12*\n*Nvidia GPU*\n*I installed tensorflow-gpu but  i have these message in my running\nenvironment:*\n2018-11-18 11:31:18.966801: I\ntensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports\ninstructions that this TensorFlow binary was not compiled to use: AVX AVX2\n2018-11-18 11:31:20.253752: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with\nproperties:\nname: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493\npciBusID: 0000:01:00.0\ntotalMemory: 4.00GiB freeMemory: 3.31GiB\n2018-11-18 11:31:20.254787: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu\ndevices: 0\n2018-11-18 11:31:21.153266: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect\nStreamExecutor with strength 1 edge matrix:\n2018-11-18 11:31:21.153661: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\n2018-11-18 11:31:21.153904: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\n2018-11-18 11:31:21.154529: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow\ndevice (/device:GPU:0 with 3026 MB memory) -> physical GPU (device: 0,\nname: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n*[name: \"/device:CPU:0\"device_type: \"CPU\"memory_limit: 268435456locality\n{}incarnation: 4749799083155869145, name: \"/device:GPU:0\"device_type:\n\"GPU\"memory_limit: 3173357977locality {  bus_id: 1  links {  }}incarnation:\n769163103206302902*\n\n*physical_device_desc: \"device: 0, name: GeForce GTX 1050, pci bus id:\n0000:01:00.0, compute capability: 6.1\"]*\n2018-11-18 11:31:21.160132: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu\ndevices: 0\n2018-11-18 11:31:21.160490: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect\nStreamExecutor with strength 1 edge matrix:\n2018-11-18 11:31:21.160798: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\n2018-11-18 11:31:21.161001: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\n2018-11-18 11:31:21.161334: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow\ndevice (/device:GPU:0 with 3026 MB memory) -> physical GPU (device: 0,\nname: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-11-18 11:31:21.163951: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu\ndevices: 0\n2018-11-18 11:31:21.164348: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect\nStreamExecutor with strength 1 edge matrix:\n2018-11-18 11:31:21.164715: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\n2018-11-18 11:31:21.164968: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\n2018-11-18 11:31:21.165295: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow\ndevice (/device:GPU:0 with 3026 MB memory) -> physical GPU (device: 0,\nname: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\n*Default GPU Device: /device:GPU:0*\n\n\n\nOn Thu, Nov 15, 2018 at 8:55 PM n0t0ri0us <notifications@github.com> wrote:\n\n> Do you have any other logs? Are you using Linux or Windows? What version\n> of tensorflow? Nvidia or AMD GPU? It's easier to help you if you are as\n> descriptive as possible. Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23773#issuecomment-439119890>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AqoKEkslfdTetPHNreb3t6lNMuITGOLrks5uvaOAgaJpZM4Yf3BI>\n> .\n>\n", " *I'm using Windows10*\n*version of tensorflow 1.12*\n*Nvidia GPU*\n*I installed tensorflow-gpu but  i have these message in my running\nenvironment:*\n2018-11-18 11:31:18.966801: I\ntensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports\ninstructions that this TensorFlow binary was not compiled to use: AVX AVX2\n2018-11-18 11:31:20.253752: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with\nproperties:\nname: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493\npciBusID: 0000:01:00.0\ntotalMemory: 4.00GiB freeMemory: 3.31GiB\n2018-11-18 11:31:20.254787: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu\ndevices: 0\n2018-11-18 11:31:21.153266: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect\nStreamExecutor with strength 1 edge matrix:\n2018-11-18 11:31:21.153661: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\n2018-11-18 11:31:21.153904: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\n2018-11-18 11:31:21.154529: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow\ndevice (/device:GPU:0 with 3026 MB memory) -> physical GPU (device: 0,\nname: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n*[name: \"/device:CPU:0\"device_type: \"CPU\"memory_limit: 268435456locality\n{}incarnation: 4749799083155869145, name: \"/device:GPU:0\"device_type:\n\"GPU\"memory_limit: 3173357977locality {  bus_id: 1  links {  }}incarnation:\n769163103206302902*\n\n*physical_device_desc: \"device: 0, name: GeForce GTX 1050, pci bus id:\n0000:01:00.0, compute capability: 6.1\"]*\n2018-11-18 11:31:21.160132: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu\ndevices: 0\n2018-11-18 11:31:21.160490: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect\nStreamExecutor with strength 1 edge matrix:\n2018-11-18 11:31:21.160798: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\n2018-11-18 11:31:21.161001: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\n2018-11-18 11:31:21.161334: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow\ndevice (/device:GPU:0 with 3026 MB memory) -> physical GPU (device: 0,\nname: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-11-18 11:31:21.163951: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu\ndevices: 0\n2018-11-18 11:31:21.164348: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect\nStreamExecutor with strength 1 edge matrix:\n2018-11-18 11:31:21.164715: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\n2018-11-18 11:31:21.164968: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\n2018-11-18 11:31:21.165295: I\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow\ndevice (/device:GPU:0 with 3026 MB memory) -> physical GPU (device: 0,\nname: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\n*Default GPU Device: /device:GPU:0*\n\nOn Sun, Nov 18, 2018 at 1:33 AM Simhan Ramakrishnan <\nnotifications@github.com> wrote:\n\n> As it says \"Please install GPU version of TF\"\n> Tensorflow magically cannot run on GPU and i don't know much about your\n> environment . You also have to make sure correct nvidia drivers have been\n> installed.\n> Please follow these instructions from here and for the right OS. Use\n> anaconda if possible.\n> https://www.tensorflow.org/install/ and post your comments on stack\n> overflow instead of raising an issue\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23773#issuecomment-439650709>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AqoKEnQJp7K_S1hF_WvlnAsmKguzpcvIks5uwIe4gaJpZM4Yf3BI>\n> .\n>\n", "@sobhansepid The above message tells you that you have successfully installed TensorFlow GPU and it was able to locate GPU availability on your system.\r\nHowever the initial error message may have arisen due to ongoing computations on your system. Not sure exactly what can trigger. Try terminate all processes and give a fresh try for your GPU computation process.", "*thanks a lot for your answer*\nDoes not the following message make any problem? and what can I do?\n*I tensorflow/core/platform/cpu_**feature_guard.cc:141] Your CPU supports\ninstructions that this TensorFlow binary was not compiled to use: AVX AVX2 *\n\n\nOn Tue, Nov 20, 2018 at 1:38 AM ymodak <notifications@github.com> wrote:\n\n> @sobhansepid <https://github.com/sobhansepid> The above message tells you\n> that you have successfully installed TensorFlow GPU and it was able to\n> locate GPU availability on your system.\n> However the initial error message may have arisen due to ongoing\n> computations on your system. Not sure exactly what can trigger. Try\n> terminate all processes and give a fresh try for your GPU computation\n> process.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23773#issuecomment-440057885>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AqoKEmshA7yO70nHasUHwBQbSSEtm85tks5uwyvBgaJpZM4Yf3BI>\n> .\n>\n", "Nagging Assignee @ymodak: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Apologies for the delay in response.\r\nThe above warning message tells you that AVX AVX2* instruction sets are not enabled. To optimize the CPU performance you have to build TF from sources.\r\nHowever I won't recommend doing that because you are using TensorFlow GPU version and most expensive ops will be computed on GPU anyway.\r\nSo you can safely ignore this warning message.\r\nYou can avoid seeing those warning messages you can try:\r\n>import os\r\n>os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 23772, "title": "Failed to convert .pb to .tflite", "body": "When I convert a pb file to tflite, I meet a error as follow.  what's wrong?\r\n\r\n(tensorflow) XXXX :~/tensorflow/tensorflow-master$ bazel-bin/tensorflow/contrib/lite/toco/toco  \\\r\n>   --input_file=./headpose.pb \\\r\n>   --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\r\n>   --output_file=./headpose.tflite --inference_type=FLOAT \\\r\n>   --input_type=FLOAT --input_arrays=input \\\r\n>   --output_arrays=output \u2013input_shapes=1,416,416,3\r\n2018-11-15 21:19:52.503499: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:282] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.\r\n2018-11-15 21:19:52.616030: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.633837: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.633907: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.633920: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.634233: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.634249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.634269: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.634280: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.634410: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.634424: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.634444: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.634454: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.634767: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.634782: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.634801: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.634811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.635636: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.635656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.635686: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.635705: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.639048: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.639092: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.639121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.639131: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.652115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.652212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.652237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.652246: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.682724: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.682769: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.682804: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: RefSwitch\r\n2018-11-15 21:19:52.682826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1110] Converting unsupported operation: AssignSub\r\n2018-11-15 21:19:52.691380: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 437 operators, 737 arrays (0 quantized)\r\n2018-11-15 21:19:52.699329: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 293 operators, 513 arrays (0 quantized)\r\n2018-11-15 21:19:52.706308: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 293 operators, 513 arrays (0 quantized)\r\n2018-11-15 21:19:52.706484: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:98] Check failed: other_op->type == OperatorType::kMerge Found BatchNormalization as non-selected output from Switch, but only Merge supported.\r\nAborted (core dumped)\r\n\r\n", "comments": ["Can you try with latest tf-nightly build? Also can you please provide **System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n", "OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows10\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\nTensorFlow installed from (source or binary):Binary\r\nTensorFlow version:1.11.0\r\n", "Nagging Assignee @ymodak: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is this still an issue for you? Did you try latest tf-nightly build?\r\nYou can try setting virtual env and test if haven't already:\r\n> virtualenv -p python3 venv-tf-nightly                          \r\n> source venv-tf-nightly/bin/activate                             \r\n> pip3 install tf-nightly                                                   ", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "HI, @ymodak I used the nightly build\uff0c same error occurred.\r\n\r\n", "@jiangxiluning Can you please post a new issue describing your problem? Also make sure you provide all the information asked by the template. Thanks!", "I have almost the same problem with @jiangxiluning, tflite does not support switching and merging, can you please give some inspiration? Here are the details of my issue. [LINK](https://github.com/tensorflow/tensorflow/issues/27241)"]}, {"number": 23771, "title": "Build failure on TensorFlow v1.12.0 on s390x", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04, 18.04 s390x\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary):  source \r\n- TensorFlow version: v1.12.0\r\n- Python version:  2.7.x\r\n- Installed using virtualenv? pip? conda?:  Building from source \r\n- Bazel version (if compiling from source): v0.15.0\r\n- GCC/Compiler version (if compiling from source): 7.0.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n**Describe the problem**\r\n\r\nWe are building TensorFlow v1.12.0 from source on Ubuntu 16.04, 18.04 on s390x platform .\r\nBuild fails with an error: \"`Unknown Target CPU`\" \r\n\r\nBulid command: \r\n  `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package `\r\n\r\n@gunan We haven't observed this Boringssl error with current master build and it's building successfully on s390x. CI link: https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/\r\n\r\nHowever while building v1.12.0 (since it is latest stable) , we came across this issue.  \r\nLooks like some code changes made in master which resolves issues related to Boringssl w.r.t. s390x are missing in v1.12.0.\r\n\r\nWill the Tensorflow v1.13.0 have the required changes to skip Boringssl for s390x?  Or is there a way to fix build issue in v1.12.0 ? Please let us know.\r\n\r\n\r\n\r\n\r\n", "comments": ["@gunan Could you please provide your inputs on this.", "I think this commit is missing from the release branch.\r\nhttps://github.com/tensorflow/tensorflow/commit/3914ff714ba4561ecff329012d02c3de027ee4a5\r\nDoes this commit fix the issues for you for 1.12?", "It is possible even https://github.com/tensorflow/tensorflow/commit/3437098ba5b111817ef6ac5906d86934168704b7 is missing from the release and that should definitely exclude all cloud capabilities from s390x builds.", "@gunan the commits that you have mentioned are applicable to master branch.  In [v1.12.0]( https://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/BUILD) ,  gcp or aws support configs are not available.  \r\n\r\nFor v1.12.0, I have disabled Apache Ignite support throgh ./configure and I could build TensorFlow\r\n\r\n`./configure`\r\n```\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.15.0- (@non-git) installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nDo you wish to build TensorFlow with Apache Ignite support? [Y/n]: N\r\nNo Apache Ignite support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: N\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: N\r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: N\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\nConfiguration finished\r\n```\r\n\r\nNow I will be continuing with test case execution using command: \r\n`bazel  test --test_timeout 300,450,1200,3600 --build_tests_only -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/core/platform/cloud/... -//tensorflow/contrib/lite/... -//tensorflow/contrib/cloud/... -//tensorflow/java/...`\r\n\r\n* Excluding` //tensorflow/java` as it fails with an error '`Building Java resource jar failed`' [#19770]\r\n* Excluding `//tensorflow/core/platform/cloud/` and `//tensorflow/contrib/cloud/` as it gives Boringssl issue for s390x. \r\n", "Closing this as TensorFlow can be built on s390x by disabling Apache Ignite support through ./configure "]}, {"number": 23770, "title": "The distribute strategy is not compatible with tf.contrib.slim.batch_norm() and tf.contrib.layers.batch_norm()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **_no_**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _**Ubuntu 16.04.5 LTS**_\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**_no_**\r\n- TensorFlow installed from (source or binary):**_binary_**\r\n- TensorFlow version (use command below):**_1.12.0_**\r\n- Python version: **_Python 3.5.2_**\r\n- Bazel version (if compiling from source): _**None**_\r\n- GCC/Compiler version (if compiling from source):  **_5.4.0_**\r\n- CUDA/cuDNN version: **_cuda-9.0.176/cudnn-7.2.1_**\r\n- GPU model and memory: **_2 same GTX Titan X (Pascal), 12GB_** \r\n\r\nI just use the offical docker image of the tensorflow, the tag is `1.12.0-devel-gpu-py3`\r\n\r\n**Problem**\r\n\r\nThe code is shown below\r\n```\r\n#coding=utf-8\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport argparse\r\n\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--num_gpus', type=int, default=2,\r\n\t\t\t\t\t\thelp='The number of the GPUs.')\r\nFLAGS, unparsed = parser.parse_known_args()\r\n\r\n\r\ndef input_fn():\r\n    return (\r\n        tf.data.Dataset.from_tensor_slices([0])\r\n        .map(lambda _: tf.random_uniform([1], 0, np.pi * 2))\r\n        .map(lambda x: (x, tf.sin(x)))\r\n        .repeat()\r\n        .batch(10)\r\n    )\r\n\r\ndef model_fn(features, labels, mode):\r\n    net = tf.layers.dense(features, units=20)\r\n    net = tf.nn.tanh(net)\r\n    \r\n    #net = tf.contrib.layers.batch_norm(net)\r\n    net = tf.contrib.slim.batch_norm(net)\r\n    #net = tf.keras.layers.BatchNormalization()(net)\r\n    #net = tf.layers.batch_normalization(inputs=net)\r\n\r\n    net = tf.layers.dense(net, units=20)\r\n    net = tf.nn.tanh(net)\r\n    output = tf.layers.dense(net, units=1)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        loss = tf.reduce_mean(tf.pow(output - labels, 2))\r\n        loss = tf.identity(loss, name='loss')\r\n        train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss, global_step=tf.train.get_global_step())\r\n        return tf.estimator.EstimatorSpec(tf.estimator.ModeKeys.TRAIN, loss=loss, train_op=train_op)\r\n\r\nsession_config = tf.ConfigProto(allow_soft_placement=True,\r\n                                    gpu_options=tf.GPUOptions(allow_growth=True, \r\n                                                force_gpu_compatible=True))\r\ndistribution = tf.contrib.distribute.MirroredStrategy(num_gpus=FLAGS.num_gpus)\r\nconfig = tf.estimator.RunConfig(keep_checkpoint_max=0, \r\n\t\t\t\t\t\t\t\ttrain_distribute=distribution)\r\nestimator = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\ntensors_to_log = {'loss':'loss'}\r\nlogging_hook = tf.train.LoggingTensorHook(\r\n        \t\t\ttensors=tensors_to_log, every_n_iter=100)\r\ntrain_hooks=[logging_hook]\r\nprint('tensorflow version: %s' % tf.__version__)\r\nestimator.train(input_fn=input_fn, steps=1000, hooks=train_hooks)\r\n```\r\n\r\nWhen use `tf.contrib.layers.batch_norm`  and `tf.contrib.slim.batch_norm` will raise error\r\n```\r\n name: GeForce GTX TITAN X, pci bus id: 0000:18:00.0, compute capability: 5.2)\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Error reported to Coordinator:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"estimator_multi_gpu.py\", line 27, in model_fn\r\n    net = tf.contrib.slim.batch_norm(net)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 182, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 596, in batch_norm\r\n    scope=scope)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 416, in _fused_batch_norm\r\n    is_training, _delay_updates, moving_vars_fn)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/utils.py\", line 214, in smart_cond\r\n    return static_cond(pred_value, fn1, fn2)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/utils.py\", line 192, in static_cond\r\n    return fn1()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 410, in _delay_updates\r\n    moving_mean, mean, decay, zero_debias=zero_debias_moving_mean)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py\", line 84, in assign_moving_average\r\n    with ops.colocate_with(variable):\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 59, in __enter__\r\n    return next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 4094, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 59, in __enter__\r\n    return next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 4146, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1307, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 439, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError\r\n```\r\n\r\nBut if I use `tf.keras.layers.BatchNormalization()(net)` or   `tf.layers.batch_normalization(inputs=net)`,\r\nit is totally OK.\r\n\r\nI see the source code of the `tf.layers.batch_normalization` and find it is inherited form `keras.layers.BatchNormalization`\r\n```\r\nfrom tensorflow.python.keras import layers as keras_layers\r\nfrom tensorflow.python.layers import base\r\nfrom tensorflow.python.ops import init_ops\r\nfrom tensorflow.python.util.tf_export import tf_export\r\n\r\n@tf_export('layers.BatchNormalization')\r\nclass BatchNormalization(keras_layers.BatchNormalization, base.Layer):\r\n```\r\nAnd I also find some similar issues\r\nhttps://github.com/tensorflow/tensorflow/issues/20509\r\nhttps://github.com/tensorflow/tensorflow/issues/20874\r\nhttps://github.com/tensorflow/tensorflow/issues/21968\r\nhttps://github.com/tensorflow/tensorflow/issues/22399\r\n\r\n_**There are many pretrained models in the model zoo of `slim`, the bn layer are all not compatible with the distribute stratege. It is not convinient for us to use them in other tasks with `tf.estimator`.**_\r\n\r\n\r\n**Other info / logs**\r\nThere are also two samll problems in the docker images....\r\n\r\n**_ONE:_**\r\n```\r\n__new__() got an unexpected keyword argument 'serialized_options'\r\n```\r\nI solvd it by \r\n```\r\nRUN pip3 install --upgrade pip\r\n```\r\nwhen build the image.\r\n\r\nThe `pip install -U protobuf ` did not work.\r\n\r\n**_TWO:_**\r\n```\r\n\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpggc6_3sr/model.ckpt.\r\nINFO:tensorflow:model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.\r\nUnexpected end of /proc/mounts line `overlay / overlay rw,relatime,lowerdir=/var/lib/docker/overlay2/l/GFPVUUB6DIRSEA556PJ7UZSB2U:/var/lib/docker/overlay2/l/Z7PKFCBJ2NCRWLDUMR7HOGXHY5:/var/lib/docker/overlay2/l/XJZAHQOGE7RXOAHTAKELNSYBZ5:/var/lib/docker/overlay2/l/ESG4TTNOTM5ZINQ5O4JCYNFODB:/var/lib/docker/overlay2/l/6ZQZQCRBLVJTIVAKYNIA2VNCNX:/var/lib/docker/overlay2/l/ZIWP422R2WQJXJQWA6BWTKSVP2:/var/lib/docker/overlay2/l/EZJIAHDCQ5P5GASJZR3GCECEE6:/var/lib/docker/overlay2/l/OA2WSFXRNJQI36B7ZILMTVTD5V:/var/lib/docker/overlay2/l/XITXCQNT5YIY7'\r\nUnexpected end of /proc/mounts line `YSSGYLTAEXMQX:/var/lib/docker/overlay2/l/EHORMZU5A6H3H6NAL34XTIYBEA:/var/lib/docker/overlay2/l/E34DRW27EHWOG6HEQCYM2LKYJX:/var/lib/docker/overlay2/l/Y4SABJDSTVMRHZ2SC6QYSK4NP6:/var/lib/docker/overlay2/l/BWHO3NPMOZYCZ33PIH4XXQH3Z6:/var/lib/docker/overlay2/l/YLLBXOSS6TVCINXJNKTZX7J6PQ:/var/lib/docker/overlay2/l/QQ2FBR7QT4NFNYR4UZUXOBD2K3:/var/lib/docker/overlay2/l/SSP6MK4IQCUV2N5DSY5BO4EZO7:/var/lib/docker/overlay2/l/XUONJPPZG5ANA2ELQECHOIY7KU:/var/lib/docker/overlay2/l/ZOCEMSU7RIV6OH6QZNTKKIGACB:/var/lib/do'\r\n```\r\nIt seems that this is harmless.\r\nhttps://stackoverflow.com/questions/46138549/docker-openmpi-and-unexpected-end-of-proc-mounts-line", "comments": ["Reassigning to @josh11b, since this is about distribution strategies.", "Why? ", "Any updates on this? Till this is resolved fully, can a known list of ops to substitute with keras equivalents be put out?\r\n\r\nThe problem is that extensive network libraries used by object detection and quite a few other modules in `models` are based on slim. There doesn't seem to be any substitute for that.", "I don't think we have any current effort to add distribute strategy support to slim, and it seems unlikely to become a priority due to the general TF move away from contrib.", "> Any updates on this? Till this is resolved fully, can a known list of ops to substitute with keras equivalents be put out?\r\n> \r\n> The problem is that extensive network libraries used by object detection and quite a few other modules in `models` are based on slim. There doesn't seem to be any substitute for that.\r\n\r\n@varun19299 \r\nI think you can place your variables and models manually like\r\nhttps://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py\r\nhttps://github.com/tensorflow/models/blob/master/tutorials/image/cifar10_estimator/cifar10_main.py\r\n\r\n@josh11b \r\nNow I agree with you and I also think the contrib's compatibility with distribute strategy is not necessary.\r\nIt is also a hard work to make the contrib compatible with the distribute strategy.\r\n\r\nI will close this issue soon.\r\n"]}, {"number": 23769, "title": "Error when feeding tf.keras model in custom loss function", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux openSUSE Leap 42.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.4.6\r\n- Bazel version (if compiling from source): none\r\n- GCC/Compiler version (if compiling from source): none\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to add adversarial training to my `tf.keras` model. For this, I added a custom loss function, which feeds the model the adversarially perturbed input and adds the cross-entropy to the loss. \r\n\r\nThe problem is that feeding the model a tensor in the custom loss function leads to a `TypeError: argument of type 'NoDependency' is not iterable`. I have reduced it to a minimal example that simply feeds the model zeros below. \r\n\r\nIf I call `model(model.input)` before the custom loss function is called, it surprisingly works.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe expected behavior would be the model compiling and fitting as it does with the `model(model.input)` line uncommented. The example below would also work when defining the `y_pred_zeros = model(tf.zeros((32, 20)))` outside of the `loss` function. But for adversarial training, I need to call the model inside of the loss function.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Dense(10, activation='relu', input_shape=(20,)),\r\n    tf.keras.layers.Dense(5)\r\n])\r\n\r\n# With the following line uncommented, it works.\r\n# model(model.input)\r\n\r\ndef loss(y_true, y_pred):\r\n    loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\r\n\r\n    # Feed zeros to the model and add the mean of the output to the loss\r\n    y_pred_zeros = model(tf.zeros((32, 20)))\r\n\r\n    return loss + tf.reduce_mean(y_pred_zeros)\r\n\r\nmodel.compile('sgd', loss=loss)\r\n\r\nx_train, y_train = np.random.randn(100, 20), np.random.randn(100, 5)\r\nmodel.fit(x_train, y_train)\r\n```\r\n\r\n**Other info / logs**\r\n```python\r\n/home/kilian/Desktop/venv/bin/python /home/kilian/Desktop/tmp.py\r\nTraceback (most recent call last):\r\n  File \"/home/kilian/Desktop/tmp.py\", line 20, in <module>\r\n    model.compile('sgd', loss=loss)\r\n  File \"/home/kilian/Desktop/venv/lib/python3.4/site-packages/tensorflow/python/training/checkpointable/base.py\", line 474, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File \"/home/kilian/Desktop/venv/lib/python3.4/site-packages/tensorflow/python/keras/engine/training.py\", line 617, in compile\r\n    output_loss = weighted_loss(y_true, y_pred, sample_weight, mask)\r\n  File \"/home/kilian/Desktop/venv/lib/python3.4/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 598, in weighted\r\n    score_array = fn(y_true, y_pred)\r\n  File \"/home/kilian/Desktop/tmp.py\", line 16, in loss\r\n    y_pred_zeros = model(tf.zeros((32, 20)))\r\n  File \"/home/kilian/Desktop/venv/lib/python3.4/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 704, in __call__\r\n    if ('mask' in self._call_fn_args and 'mask' not in kwargs and\r\nTypeError: argument of type 'NoDependency' is not iterable\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["@fchollet  Tried at my end and could reproduce the same issue. Please take a look.", "I confirm the code to reproduce the issue works on the latest TF 2 GPU nightly build (as of today). You can close here.\r\n\r\nPython 3.7, Ubuntu 19.04, Nvidia 1050 ti.", "This is fixed in latest tf-nightly version 1.15.0-dev20190725. Thanks!"]}, {"number": 23768, "title": "TFLite not support variable batch_size of input", "body": "I use `tf.estimator.export.build_raw_serving_input_receiver_fn` api with `default_batch_size=None` to export a savedModel, which input shape maybe [None, 3000, 40, 3].  But after convert the model to tflite model , the input shape is [1, 3000, 40, 3].  How can I using tflite with variable batch_size of input.", "comments": ["Depending on the your model, it might be impossible to create a tflite version that allows resizing of the input tensor.\r\n\r\nAlthough the tflite interpreter allows you to resize the input tensor, the conversion process sometimes hardcodes shapes (supposedly for efficiency reasons). However, it should be possible to convert the model with a fixed batch size, say 5, and use that at inference time.", "@andrehentz So, how can I using with fixed batch size , like 5 ?", "Try running tflite_convert again, with --input_shapes=5,3000,40,3", "@andrehentz I have tried, and see #23799"]}, {"number": 23767, "title": "Tensorflow calculates incorrect loss for `tf.keras` models when using Weights.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (No):\r\n- OS Platform and Distribution (e.g., MacOS + Google Cloud VMs):\r\n- TensorFlow installed from (source):\r\n- TensorFlow version (1.10 and 1.12):\r\n- Python version: 3.5\r\n\r\n**Describe the current behavior**\r\nThe loss calculation is not correct when working with `tf.keras.`. After building the model, [`tf.keras.fit_generator`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model) should accept `(inputs, targets, sample_weights)` as inputs. However, if I multiply the `sample_weights` by 10000,  the loss doesn't change.\r\n\r\nThe bug seems to appear from 1.10 version of Tensorflow onwards e.g. (1.11, 1.12)\r\n\r\n**Describe the expected behavior**\r\nIf I increase the weighting by a certain factor, the overall loss of the model should increase by the same factor. Given the model is doing random guessing. \r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nWEIGHT_VARIABLE = 1\r\n\r\nno_of_features = 10\r\ntimesteps = 3\r\nbatch_size = 32\r\n\r\ndef data_gen():\r\n\r\n    while True:\r\n        numerical = np.random.randint(5, size=(batch_size, timesteps, no_of_features))\r\n        y = np.random.randint(2, size=batch_size)\r\n        w = np.ones(batch_size) * WEIGHT_VARIABLE # or np.where() for imbalanced datasets\r\n\r\n        yield {'numeric_input': numerical}, y, w\r\n\r\n\r\ndef build_model():\r\n    numerical_input = tf.keras.layers.Input(shape=(timesteps, no_of_features), name='numeric_input')\r\n    rnn_out = tf.keras.layers.GRU(32, return_sequences=False)(numerical_input)\r\n    dense = tf.keras.layers.Dense(1, activation='sigmoid', name='main_output')(rnn_out)\r\n\r\n    model = tf.keras.models.Model(numerical_input, dense)\r\n\r\n    params = {\r\n        'loss': 'binary_crossentropy',\r\n        'optimizer': tf.keras.optimizers.Adam(),\r\n        'metrics': [tf.keras.metrics.binary_crossentropy, tf.keras.metrics.binary_accuracy]\r\n    }\r\n    model.compile(**params)\r\n\r\n    return model\r\n\r\n\r\ndef train_model():\r\n    gen1 = data_gen()\r\n    model = build_model()\r\n\r\n    model.fit_generator(gen1, epochs=30, steps_per_epoch=10)\r\n\r\n\r\nif __name__ == '__main__':\r\n    train_model()\r\n```\r\n\r\nIn the above code, you simply need to change the `WEIGHT_VARIABLE = 1` From 1 to 100000 and rerun the file.\r\n\r\n\r\n**Other info / logs**\r\n### v1.10\r\n\r\n`WEIGHT_VARIABLE = 1`\r\n10/10 [==============================] - 1s 128ms/step - loss: 0.7407 - binary_crossentropy: 0.7407 - binary_accuracy: 0.5031\r\nEpoch 2/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 0.7043 - binary_crossentropy: 0.7043 - binary_accuracy: 0.5125\r\nEpoch 3/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 0.7055 - binary_crossentropy: 0.7055 - binary_accuracy: 0.5219\r\nEpoch 4/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 0.7002 - binary_crossentropy: 0.7002 - binary_accuracy: 0.5250\r\nEpoch 5/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 0.6944 - binary_crossentropy: 0.6944 - binary_accuracy: 0.5375\r\n\r\n`WEIGHT_VARIABLE = 10000`\r\n\r\n10/10 [==============================] - 1s 131ms/step - loss: 7235.5976 - binary_crossentropy: 0.7236 - binary_accuracy: 0.4562\r\nEpoch 2/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 7271.9184 - binary_crossentropy: 0.7272 - binary_accuracy: 0.4844\r\nEpoch 3/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 7276.9147 - binary_crossentropy: 0.7277 - binary_accuracy: 0.4500\r\nEpoch 4/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 7052.0121 - binary_crossentropy: 0.7052 - binary_accuracy: 0.4625\r\nEpoch 5/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 7187.0285 - binary_crossentropy: 0.7187 - binary_accuracy: 0.4969\r\n\r\n\r\n### v1.12\r\n\r\n`WEIGHT_VARIABLE = 1`\r\n10/10 [==============================] - 1s 68ms/step - loss: 0.7188 - binary_crossentropy: 0.7188 - binary_accuracy: 0.5312\r\nEpoch 2/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 0.7044 - binary_crossentropy: 0.7044 - binary_accuracy: 0.4969\r\nEpoch 3/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 0.7086 - binary_crossentropy: 0.7086 - binary_accuracy: 0.4844\r\nEpoch 4/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 0.7075 - binary_crossentropy: 0.7075 - binary_accuracy: 0.4500\r\nEpoch 5/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 0.6950 - binary_crossentropy: 0.6950 - binary_accuracy: 0.5187\r\n\r\n`WEIGHT_VARIABLE = 10000`\r\n\r\n10/10 [==============================] - 1s 74ms/step - loss: 0.9084 - binary_crossentropy: 0.9084 - binary_accuracy: 0.4719\r\nEpoch 2/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 0.7120 - binary_crossentropy: 0.7120 - binary_accuracy: 0.5062\r\nEpoch 3/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 0.7024 - binary_crossentropy: 0.7024 - binary_accuracy: 0.5344\r\nEpoch 4/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 0.7257 - binary_crossentropy: 0.7257 - binary_accuracy: 0.4500\r\nEpoch 5/5\r\n10/10 [==============================] - 0s 4ms/step - loss: 0.7013 - binary_crossentropy: 0.7013 - binary_accuracy: 0.4844", "comments": ["I can also confirm this issue, looks like it was introduced in ``Tensorflow==1.11.0``\r\n\r\nThe same issue effects class weights too:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import Input, Dense\r\nfrom tensorflow.python.keras.models import Model\r\n\r\nprint (tf.__version__)\r\nbatch_size = 10\r\nnum_input = 5\r\nnum_output = 2\r\npositive_weight = 10000\r\n\r\n\r\nfrom tensorflow.python.keras import backend as K\r\n\r\n\r\ndef weighted_cross_entropy(y_true, y_pred, class_weights=[1, positive_weight]):\r\n    \"\"\" Sample weights/class weights are broken in TensorFlow 1.12.\r\n\r\n    See issue: https://github.com/tensorflow/tensorflow/issues/23767\r\n    \"\"\"\r\n    y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\r\n    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\r\n    return -K.sum(y_true * K.log(y_pred) * K.variable(class_weights), -1)\r\n\r\ndef _to_one_hot(y, num_classes):\r\n    \"\"\" Compute one-hot encoding of an array. \"\"\"\r\n    y_one_hot = np.zeros((len(y), num_classes)).astype(np.float32)\r\n    y_one_hot[np.arange(len(y)), y.squeeze()] = 1\r\n\r\n    return y_one_hot\r\n\r\n\r\nclass DummyGenerator(object):\r\n\r\n    def class_weights(self):\r\n        np.random.seed(7)\r\n        while True:\r\n            X = np.random.rand(batch_size, num_input)\r\n            y = np.random.randint(num_output, size=batch_size)\r\n            y = _to_one_hot(y, 2)\r\n            yield X, y\r\n\r\n    def sample_weights(self):\r\n        np.random.seed(7)\r\n        while True:\r\n            X = np.random.rand(batch_size, num_input)\r\n            y = np.random.randint(num_output, size=batch_size)\r\n            w = y * (positive_weight - 1) + 1\r\n            y = _to_one_hot(y, 2)\r\n            yield X, y, w\r\n\r\n\r\ndef dummy_model():\r\n    inputs = Input(shape=(num_input,))\r\n    outputs = Dense(2, activation=\"softmax\")(inputs)\r\n    model = Model(inputs=inputs, outputs=outputs)\r\n    return model\r\n\r\n\r\nmodel = dummy_model()\r\nmodel.save_weights(\"~/fixed.hdf5\")\r\ngen = DummyGenerator()\r\n\r\nprint(\"Training with sample_weights\")\r\nmodel.load_weights(\"~/fixed.hdf5\")\r\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\r\nmodel.fit_generator(gen.sample_weights(), steps_per_epoch=1000, epochs=1)\r\n\r\nprint(\"Training with class_weights\")\r\nmodel.load_weights(\"~/fixed.hdf5\")\r\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\r\nmodel.fit_generator(gen.class_weights(), steps_per_epoch=1000, epochs=1, class_weight={0:1, 1:positive_weight})\r\n\r\nprint(\"Training with loss_weights\")\r\nmodel.compile(loss=weighted_cross_entropy, optimizer=\"adam\")\r\nmodel.load_weights(\"~/fixed.hdf5\")\r\nmodel.fit_generator(gen.class_weights(), steps_per_epoch=1000, epochs=1)\r\n\r\n```", "Confirmed that we ran into the same issue. \ud83d\udc4d ", "Same issue here \ud83d\udc4d \r\n", "Can we get an acknowledgement of this issue? I know @fchollet is very busy, maybe @pavithrasv could give it a quick look?", "I do not think it has been fixed. I can still see it happens in 1.14 and even tf2.0 beta. I think it was fixed in 1.13 but re-introduced afterwards.", "This was resolved already. I cannot reproduce the issue when I use tf-nightly. Please check the [gist with TF1.14.0](https://colab.sandbox.google.com/gist/jvishnuvardhan/2d385f53d15b4e024d5d56a02892cab6/tf_23767_keras_weights.ipynb) and [gist with [`tf-nightly`](https://colab.sandbox.google.com/gist/jvishnuvardhan/75d60f494371d04184a98ab217c07f69/tfnightly_22207_keras_datasets.ipynb). Thanks!\r\n\r\n## output with `WEIGHT_VARIABLE = 1`\r\n```\r\nEpoch 1/5\r\n10/10 [==============================] - 2s 199ms/step - loss: 0.7167 - binary_crossentropy: 0.7167 - binary_accuracy: 0.4750\r\nEpoch 2/5\r\n10/10 [==============================] - 0s 6ms/step - loss: 0.6973 - binary_crossentropy: 0.6973 - binary_accuracy: 0.5031\r\nEpoch 3/5\r\n10/10 [==============================] - 0s 6ms/step - loss: 0.6901 - binary_crossentropy: 0.6901 - binary_accuracy: 0.5437\r\nEpoch 4/5\r\n10/10 [==============================] - 0s 7ms/step - loss: 0.6977 - binary_crossentropy: 0.6977 - binary_accuracy: 0.4781\r\nEpoch 5/5\r\n10/10 [==============================] - 0s 6ms/step - loss: 0.7034 - binary_crossentropy: 0.7034 - binary_accuracy: 0.4938\r\n```\r\n\r\n\r\n\r\n## output with `WEIGHT_VARIABLE = 1000`\r\n\r\n```\r\nEpoch 1/5\r\n10/10 [==============================] - 2s 166ms/step - loss: 722.5937 - binary_crossentropy: 0.7226 - binary_accuracy: 0.5188\r\nEpoch 2/5\r\n10/10 [==============================] - 0s 6ms/step - loss: 704.5393 - binary_crossentropy: 0.7045 - binary_accuracy: 0.4812\r\nEpoch 3/5\r\n10/10 [==============================] - 0s 6ms/step - loss: 718.4684 - binary_crossentropy: 0.7185 - binary_accuracy: 0.4812\r\nEpoch 4/5\r\n10/10 [==============================] - 0s 7ms/step - loss: 710.2846 - binary_crossentropy: 0.7103 - binary_accuracy: 0.4719\r\nEpoch 5/5\r\n10/10 [==============================] - 0s 6ms/step - loss: 698.0638 - binary_crossentropy: 0.6981 - binary_accuracy: 0.5281\r\n```\r\n\r\nI am closing this issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!"]}, {"number": 23766, "title": "TfLite performance is way worse comapred to Tensorflow mobile", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S8\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): On PC: current tf-nightly (14.11.2018), on phone current org.tensorflow:tensorflow-lite:0.0.0-nightly\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source): - \r\n- GCC/Compiler version (if compiling from source): - \r\n- CUDA/cuDNN version: - \r\n- GPU model and memory: CPU only\r\n\r\n**Describe the current behavior**\r\nI build a model with this code:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\n\r\nclass AlexNet(object):\r\n    \"\"\"Implementation of the AlexNet.\"\"\"\r\n\r\n    def __init__(self, x, keep_prob, num_classes, skip_layer,\r\n                 weights_path='DEFAULT'):\r\n        \"\"\"Create the graph of the AlexNet model.\r\n        Args:\r\n            x: Placeholder for the input tensor.\r\n            keep_prob: Dropout probability.\r\n            num_classes: Number of classes in the dataset.\r\n            skip_layer: List of names of the layer, that get trained from\r\n                scratch\r\n            weights_path: Complete path to the pretrained weight file, if it\r\n                isn't in the same folder as this code\r\n        \"\"\"\r\n        # Parse input arguments into class variables\r\n        self.X = x\r\n        self.NUM_CLASSES = num_classes\r\n        self.KEEP_PROB = keep_prob\r\n        self.SKIP_LAYER = skip_layer\r\n\r\n        if weights_path == 'DEFAULT':\r\n            self.WEIGHTS_PATH = 'bvlc_alexnet.npy'\r\n        else:\r\n            self.WEIGHTS_PATH = weights_path\r\n\r\n        # Call the create function to build the computational graph of AlexNet\r\n        self.create()\r\n\r\n    def create(self):\r\n        \"\"\"Create the network graph.\"\"\"\r\n        # 1st Layer: Conv (w ReLu) -> Lrn -> Pool\r\n        conv1 = conv(self.X, 11, 11, 96, 4, 4, padding='VALID', name='conv1')\r\n        norm1 = lrn(conv1, 2, 2e-05, 0.75, name='norm1')\r\n        pool1 = max_pool(norm1, 3, 3, 2, 2, padding='VALID', name='pool1')\r\n\r\n        # 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups\r\n        conv2 = conv(pool1, 5, 5, 256, 1, 1, groups=2, name='conv2')\r\n        norm2 = lrn(conv2, 2, 2e-05, 0.75, name='norm2')\r\n        pool2 = max_pool(norm2, 3, 3, 2, 2, padding='VALID', name='pool2')\r\n\r\n        # 3rd Layer: Conv (w ReLu)\r\n        self.conv3 = conv(pool2, 3, 3, 384, 1, 1, name='conv3')\r\n\r\n        # 4th Layer: Conv (w ReLu) splitted into two groups\r\n        conv4 = conv(self.conv3, 3, 3, 384, 1, 1, groups=2, name='conv4')\r\n\r\n        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\r\n        conv5 = conv(conv4, 3, 3, 256, 1, 1, groups=2, name='conv5')\r\n        self.pool5 = max_pool(conv5, 3, 3, 2, 2, padding='VALID', name='pool5')\r\n\r\n        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\r\n        self.flattened = tf.reshape(self.pool5, [1, 9216], name='output')\r\n\r\n    def load_initial_weights(self, session):\r\n        \"\"\"Load weights from file into network.\r\n        As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\r\n        come as a dict of lists (e.g. weights['conv1'] is a list) and not as\r\n        dict of dicts (e.g. weights['conv1'] is a dict with keys 'weights' &\r\n        'biases') we need a special load function\r\n        \"\"\"\r\n        # Load the weights into memory\r\n        weights_dict = np.load(self.WEIGHTS_PATH, encoding='bytes').item()\r\n\r\n        # Loop over all layer names stored in the weights dict\r\n        for op_name in weights_dict:\r\n\r\n            # Check if layer should be trained from scratch\r\n            if op_name not in self.SKIP_LAYER:\r\n\r\n                with tf.variable_scope(op_name, reuse=True):\r\n\r\n                    # Assign weights/biases to their corresponding tf variable\r\n                    for data in weights_dict[op_name]:\r\n\r\n                        # Biases\r\n                        if len(data.shape) == 1:\r\n                            var = tf.get_variable('biases', trainable=False)\r\n                            session.run(var.assign(data))\r\n\r\n                        # Weights\r\n                        else:\r\n                            var = tf.get_variable('weights', trainable=False)\r\n                            session.run(var.assign(data))\r\n\r\n\r\ndef conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,\r\n         padding='SAME', groups=1):\r\n    \"\"\"Create a convolution layer.\r\n    Adapted from: https://github.com/ethereon/caffe-tensorflow\r\n    \"\"\"\r\n    # Get number of input channels\r\n    input_channels = int(x.get_shape()[-1])\r\n\r\n    # Create lambda function for the convolution\r\n    convolve = lambda i, k: tf.nn.conv2d(i, k,\r\n                                         strides=[1, stride_y, stride_x, 1],\r\n                                         padding=padding)\r\n\r\n    with tf.variable_scope(name) as scope:\r\n        # Create tf variables for the weights and biases of the conv layer\r\n        weights = tf.get_variable('weights', shape=[filter_height,\r\n                                                    filter_width,\r\n                                                    input_channels/groups,\r\n                                                    num_filters])\r\n        biases = tf.get_variable('biases', shape=[num_filters])\r\n\r\n    if groups == 1:\r\n        conv = convolve(x, weights)\r\n\r\n    # In the cases of multiple groups, split inputs & weights and\r\n    else:\r\n        # Split input and weights and convolve them separately\r\n        input_groups = tf.split(axis=3, num_or_size_splits=groups, value=x)\r\n        weight_groups = tf.split(axis=3, num_or_size_splits=groups,\r\n                                 value=weights)\r\n        output_groups = [convolve(i, k) for i, k in zip(input_groups, weight_groups)]\r\n\r\n        # Concat the convolved output together again\r\n        conv = tf.concat(axis=3, values=output_groups)\r\n\r\n    # Add biases\r\n    bias = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))\r\n\r\n    # Apply relu function\r\n    relu = tf.nn.relu(bias, name=scope.name)\r\n\r\n    return relu\r\n\r\n\r\ndef fc(x, num_in, num_out, name, relu=True):\r\n    \"\"\"Create a fully connected layer.\"\"\"\r\n    with tf.variable_scope(name) as scope:\r\n\r\n        # Create tf variables for the weights and biases\r\n        weights = tf.get_variable('weights', shape=[num_in, num_out],\r\n                                  trainable=True)\r\n        biases = tf.get_variable('biases', [num_out], trainable=True)\r\n\r\n        # Matrix multiply weights and inputs and add bias\r\n        act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\r\n\r\n    if relu:\r\n        # Apply ReLu non linearity\r\n        relu = tf.nn.relu(act)\r\n        return relu\r\n    else:\r\n        return act\r\n\r\n\r\ndef max_pool(x, filter_height, filter_width, stride_y, stride_x, name,\r\n             padding='SAME'):\r\n    \"\"\"Create a max pooling layer.\"\"\"\r\n    return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\r\n                          strides=[1, stride_y, stride_x, 1],\r\n                          padding=padding, name=name)\r\n\r\n\r\ndef lrn(x, radius, alpha, beta, name, bias=1.0):\r\n    \"\"\"Create a local response normalization layer.\"\"\"\r\n    return tf.nn.local_response_normalization(x, depth_radius=radius,\r\n                                              alpha=alpha, beta=beta,\r\n                                              bias=bias, name=name)\r\n\r\n\r\ndef dropout(x, keep_prob):\r\n    \"\"\"Create a dropout layer.\"\"\"\r\n    return tf.nn.dropout(x, keep_prob)\r\n\r\n\r\n\r\n\r\ng = tf.Graph()\r\nwith g.as_default():\r\n    # Initialize all variables\r\n    x = tf.placeholder(tf.float32, [1, 227, 227, 3])\r\n    y = tf.placeholder(tf.float32, [1, 1000])\r\n    keep_prob = tf.placeholder(tf.float32)\r\n    alex_net = AlexNet(x, keep_prob, 1000, [\"fc6\", \"fc7\", \"fc8\"])\r\n```\r\n\r\nThen I froze the model. In the first try I converted it to TFlite directly, in a second try I set the `converter.post_training_quantize = True` option in the conversion process to quantize my model and in a second try I optimized my model for inference before converting and quantizing.\r\nThe code for the tflite conversion is the following:\r\n```\r\nimport tensorflow as tf\r\n\r\ngraph_def_file = \"alex_frozen_optimized.pb\"\r\ninput_arrays = [\"Placeholder\"]\r\noutput_arrays = [\"output\"]\r\n\r\nconverter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(\r\n    graph_def_file, input_arrays, output_arrays, input_shapes={\"Placeholder\" : [1, 227, 227, 3]})\r\nconverter.post_training_quantize = True\r\ntflite_model = converter.convert()\r\nopen(\"save_path/alex_frozen_optimized_quantized.tflite\", \"wb\").write(tflite_model)\r\n```\r\nThe code for optimizing for inference was the following terminal call: `python -m tensorflow.python.tools.optimize_for_inference --input alex_frozen.pb --output alex_frozen_optimized.pb --input_names=Placeholder --output_names=output`\r\n\r\nAll my models, including the plain, unconverted model is uploaded here for testing: [models.zip](https://www.dropbox.com/s/gyu9cfkn3yn07oc/models.zip?dl=0)\r\n\r\n**Describe the expected behavior**\r\nI deployed those models to my phone. I get runtime of around 290 ms for the unconverted model, while having runtimes of about 420 ms for the quantized tflite, the optimized and quantized tflite and the plain tflite model. That can't be right, can it?\r\n\r\n**Code to reproduce the issue**\r\nThe code for running inference on the phone for the tflite models is the following:\r\n```\r\nimport android.content.res.AssetFileDescriptor;\r\nimport android.content.res.AssetManager;\r\nimport android.graphics.Bitmap;\r\nimport android.os.Trace;\r\nimport java.io.FileInputStream;\r\nimport java.io.IOException;\r\nimport java.nio.ByteBuffer;\r\nimport java.nio.ByteOrder;\r\nimport java.nio.MappedByteBuffer;\r\nimport java.nio.channels.FileChannel;\r\nimport one.realnote.app.PpmItem;\r\nimport one.realnote.app.Util;\r\nimport org.tensorflow.lite.Interpreter;\r\nimport org.tensorflow.lite.Interpreter.Options;\r\n\r\npublic class AlexFeatureExtractionLite {\r\n\r\n  private static final String TAG = \"TFLiteAPIAlex\";\r\n\r\n  // Only return this many results.\r\n  private boolean isModelQuantized;\r\n  // Float model\r\n  // Number of threads in the java app\r\n  private static final int NUM_THREADS = 4;\r\n  // Config values.\r\n  private int inputSize;\r\n  // Pre-allocated buffers.\r\n  private float[][] outputVector;\r\n  private int[] intValues;\r\n\r\n  private static PpmItem ppm_tfRun = PpmItem.createDebugItem(\"TFA\", \"runAlex\", String.class);\r\n\r\n  private ByteBuffer imgData;\r\n  private Interpreter tfLite;\r\n\r\n  /**\r\n   * Memory-map the model file in Assets.\r\n   */\r\n  private static MappedByteBuffer loadModelFile(AssetManager assets)\r\n      throws IOException {\r\n    AssetFileDescriptor fileDescriptor = assets.openFd(\"tf/alex_frozen_optimized_quantized.tflite\");\r\n    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n    FileChannel fileChannel = inputStream.getChannel();\r\n    long startOffset = fileDescriptor.getStartOffset();\r\n    long declaredLength = fileDescriptor.getDeclaredLength();\r\n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n  }\r\n\r\n  /**\r\n   * Initializes a native TensorFlow session for classifying images.\r\n   *\r\n   * @param assetManager The asset manager to be used to load assets.\r\n   * @param inputSize The size of image input\r\n   * @param isQuantized Boolean representing model is quantized or not\r\n   */\r\n  public static AlexFeatureExtractionLite create(final AssetManager assetManager,\r\n      final int inputSize, final boolean isQuantized) {\r\n    final AlexFeatureExtractionLite d = new AlexFeatureExtractionLite();\r\n\r\n    d.inputSize = inputSize;\r\n    Interpreter.Options options = new Options();\r\n    options.setUseNNAPI(false);\r\n    options.setAllowFp16PrecisionForFp32(false);\r\n    options.setNumThreads(4);\r\n    try {\r\n      d.tfLite = new Interpreter(loadModelFile(assetManager), options);\r\n    } catch (Exception e) {\r\n      Util.logError(e);\r\n      throw new RuntimeException(e);\r\n    }\r\n\r\n    d.isModelQuantized = isQuantized;\r\n    // Pre-allocate buffers.\r\n    int numBytesPerChannel;\r\n    if (isQuantized) {\r\n      numBytesPerChannel = 1; // Quantized\r\n    } else {\r\n      numBytesPerChannel = 4; // Floating point\r\n    }\r\n    d.imgData = ByteBuffer.allocateDirect(1 * d.inputSize * d.inputSize * 3 * numBytesPerChannel);\r\n    d.imgData.order(ByteOrder.nativeOrder());\r\n    d.intValues = new int[d.inputSize * d.inputSize];\r\n    d.outputVector = new float[1][9216];\r\n\r\n    //d.tfLite.setNumThreads(NUM_THREADS);\r\n    return d;\r\n  }\r\n\r\n  private AlexFeatureExtractionLite() {\r\n  }\r\n\r\n\r\n  public float[] calcFeatureVector(final Bitmap bitmap) {\r\n    // Log this method so that it can be analyzed with systrace.\r\n    Trace.beginSection(\"recognizeImage\");\r\n\r\n    Trace.beginSection(\"preprocessBitmap\");\r\n    // Preprocess the image data from 0-255 int to normalized float based\r\n    // on the provided parameters.\r\n    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n\r\n    imgData.rewind();\r\n    for (int i = 0; i < inputSize; ++i) {\r\n      for (int j = 0; j < inputSize; ++j) {\r\n        int pixelValue = intValues[i * inputSize + j];\r\n        if (isModelQuantized) {\r\n          // Quantized model\r\n          imgData.put((byte) ((pixelValue >> 16) & 0xFF));\r\n          imgData.put((byte) ((pixelValue >> 8) & 0xFF));\r\n          imgData.put((byte) (pixelValue & 0xFF));\r\n        } else { // Float model\r\n          imgData.putFloat((((pixelValue >> 16) & 0xFF) - 104f));\r\n          imgData.putFloat((((pixelValue >> 8) & 0xFF) - 117f));\r\n          imgData.putFloat(((pixelValue & 0xFF) - 124f));\r\n        }\r\n      }\r\n    }\r\n    Trace.endSection(); // preprocessBitmap\r\n\r\n    // Copy the input data into TensorFlow.\r\n    Trace.beginSection(\"feed\");\r\n    outputVector = new float[1][9216];\r\n\r\n    Trace.endSection();\r\n\r\n    // Run the inference call.\r\n    Trace.beginSection(\"run\");\r\n    ppm_tfRun.start();\r\n    tfLite.run(imgData, outputVector);\r\n    ppm_tfRun.stop();\r\n    Trace.endSection();\r\n\r\n    Trace.endSection(); // \"recognizeImage\"\r\n    return outputVector[0];\r\n  }\r\n}\r\n```\r\n\r\nThe code to run inference for the unconverted model is:\r\n```\r\nimport android.content.res.AssetManager;\r\nimport android.graphics.Bitmap;\r\nimport android.os.Trace;\r\nimport one.realnote.app.PpmItem;\r\nimport org.tensorflow.Graph;\r\nimport org.tensorflow.Operation;\r\nimport org.tensorflow.contrib.android.TensorFlowInferenceInterface;\r\n\r\npublic class AlexFeatureExtraction {\r\n\r\n    private static final String TAG = \"AlexFeatureExtraction\";\r\n\r\n    // Config values.\r\n    private String inputName;\r\n    private int inputSize;\r\n\r\n    // Pre-allocated buffers.\r\n    private float[] outputVector;\r\n\r\n    private int[] intValues;\r\n    private float[] floatValues;\r\n\r\n    private TensorFlowInferenceInterface inferenceInterface;\r\n\r\n    public static AlexFeatureExtraction create(\r\n        final AssetManager assetManager) {\r\n\r\n        final AlexFeatureExtraction d = new AlexFeatureExtraction();\r\n\r\n\r\n        final String modelFilename = \"file:///android_asset/tf/alex.pb\";\r\n        final int inputSize = 227;\r\n\r\n        d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);\r\n\r\n        final Graph g = d.inferenceInterface.graph();\r\n\r\n        d.inputName = \"Placeholder\"; // ssd: \"image_tensor\";\r\n\r\n        // The inputName node has a shape of [N, H, W, C], where\r\n        // N is the batch size\r\n        // H = W are the height and width\r\n        // C is the number of channels (3 for our purposes - RGB)\r\n        final Operation inputOp = g.operation(d.inputName);\r\n        if (inputOp == null) {\r\n            throw new RuntimeException(\"Failed to find input Node '\" + d.inputName + \"'\");\r\n        }\r\n        d.inputSize = inputSize;\r\n        // The outputScoresName node has a shape of [N, NumLocations], where N\r\n        // is the batch size.\r\n\r\n        // ssd: final Operation outputOp1 = g.operation(\"detection_scores\");\r\n        final Operation outputOp1 = g.operation(\"output\");\r\n        if (outputOp1 == null) {\r\n            throw new RuntimeException(\"Failed to find output Node 'detection_scores'\");\r\n        }\r\n\r\n        // Pre-allocate buffers.\r\n        d.outputVector = new float[9216];\r\n\r\n        d.intValues = new int[d.inputSize * d.inputSize];\r\n        d.floatValues = new float[d.inputSize * d.inputSize * 3];\r\n\r\n        return d;\r\n    }\r\n\r\n\r\n    private AlexFeatureExtraction() {\r\n    }\r\n\r\n\r\n    private static PpmItem ppm_tfFeed = PpmItem.createDebugItem(\"TFA\", \"feed\", String.class);\r\n    private static PpmItem ppm_tfRun = PpmItem.createDebugItem(\"TFA\", \"runAlex\", String.class);\r\n\r\n\r\n    public float[] calcFeatureVector(final Bitmap bitmap) {\r\n        // Log this method so that it can be analyzed with systrace.\r\n        Trace.beginSection(\"recognizeImage\");\r\n\r\n        Trace.beginSection(\"preprocessBitmap\");\r\n        // Preprocess the image data from 0-255 int to normalized float based\r\n        // on the provided parameters.\r\n        bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n\r\n        for (int i = 0; i < intValues.length; ++i) {\r\n            floatValues[i * 3 + 2] = (float) (intValues[i] & 0xFF) - 124f;\r\n            floatValues[i * 3 + 1] = (float) ((intValues[i] >> 8) & 0xFF) - 117f;\r\n            floatValues[i * 3 + 0] = (float) ((intValues[i] >> 16) & 0xFF) -104f;\r\n        }\r\n\r\n        Trace.endSection(); // preprocessBitmap\r\n\r\n        // Copy the input data into TensorFlow.\r\n        Trace.beginSection(\"feed\");\r\n        ppm_tfFeed.start();\r\n        inferenceInterface.feed(inputName, floatValues, 1, inputSize, inputSize, 3);\r\n        ppm_tfFeed.stop();\r\n        Trace.endSection();\r\n\r\n        // Run the inference call.\r\n        Trace.beginSection(\"run\");\r\n        ppm_tfRun.start();\r\n        inferenceInterface.run(new String[]{\"output\"}, false);\r\n        ppm_tfRun.stop();\r\n        Trace.endSection();\r\n\r\n        // Copy the output Tensor back into the output array.\r\n        Trace.beginSection(\"fetch\");\r\n\r\n\r\n        inferenceInterface.fetch(\"output\", outputVector);\r\n\r\n        Trace.endSection();\r\n\r\n        Trace.endSection(); // \"recognizeImage\"\r\n\r\n        return outputVector;\r\n    }\r\n\r\n    public void close() {\r\n        inferenceInterface.close();\r\n    }\r\n}\r\n```\r\n\r\n**Other info / logs**\r\nOutput behavior is as expected for all models. Activating the useNNAPI option doesn't really do anything performance wise aswell as setting the number of threads.\r\n", "comments": ["This likely isn't the cause of the performance difference, but I should note that you can now use ByteBuffer for the outputs with TensorFlow Lite. You should also avoid allocating a new array for every inference invocation.\r\n\r\nAre you measuring the average latency? Or just the initial latency?", "Thanks, I will use this information!\r\n\r\nI'm measuring the average latency not just the initial one, although I wonder why the latency is decreasing over the first few iterations until it settles? ", "The initial inference run can take longer than subsequent runs, as it can trigger one-time execution of memory allocation and op preparation. This is expected.\r\n\r\nLet us know if using a ByteBuffer helps. Otherwise, we'd be happy to take a closer look if you're willing to privately attach and send the model you're using. A good subset of TensorFlow Lite ops are highly optimized, but not all of them, and it's possible there's a bottlenecking op in your graph.", "I already attached the models I'm using in a zip file in Dropbox here: https://www.dropbox.com/s/gyu9cfkn3yn07oc/models.zip?dl=0", "Regarding the use of ByteBuffers for the output, does it make a dofference performance wise if I use ByteBuffers or FloatBuffers? Because in the case of ByteBuffers, I would have to convert the content into floats to use it by calling toFloatBuffer(), correct?\r\nAnd also, if I run a network with multiple outputs can I just put the *Buffers in the output map to run the runForMultipleOutputs() method? ", "We're working to add direct support for Float/Int/LongBuffer types. The problem with multi-dimensional arrays is that we can't perform a bulk copy from the output tensor to the output multi-dimensional array; we have to copy every single sub-array directly (e.g., if your output array is [1000][3], we have to perform a copy of length 3, 1000 times). [ByteBuffer#asFloatBuffer()](https://developer.android.com/reference/java/nio/ByteBuffer#asFloatBuffer()) should be an extremely cheap operation, though, as it's just creating a new view of the same underlying buffer. In your case, since your buffer is [1][9216], then it should require just one copy, so I suspect the benefit from using ByteBuffer should be minimal.\r\n\r\n> And also, if I run a network with multiple outputs can I just put the *Buffers in the output map to run the runForMultipleOutputs() method?\r\n\r\nYep!", "As expected, using ByteBuffer does not lead to significant improvements. ", "Hi @Noltibus, would you mind trying one thing? We recently discovered a regression in our multi-threaded conv implementation, can you try again but using just 1 thread for TensorFlow Lite?", "Ok: on the first Try, I tried to run it with the gradle dependency ` 'org.tensorflow:tensorflow-lite:+' ` , which results in a native SIGSEV at app start. I already discovered that problem, which only happens if I use TfLite with the model I provided. In my app I use two different TfLite models, where the other one is the MobileNet SSD TfLite from the example TfLite app from the tensorflow repository. The MobileNet TfLite interface can be run without problems using the gradle dependency I mentioned above. My own net as TfLite can only run using the `'org.tensorflow:tensorflow-lite:0.0.0-nightly'` dependency.\r\n\r\nI run your test and assigned only one thread to both TfLite interfaces I use, which resulted in a performance decrease of about 33 % for the MobileNet TfLite Interface and a decrease of about 25 % for the TfLite interface, which uses my own model. This test was done using the nightly dependency to get my own model to work at last. Any other ideas?", "Any word on this? It seems, that using the normal, not the nightly dependency of tflite now runs the model, but still way slower than the not-lite model", "We're still investigating, apologies for the delay (holiday availability has been a bit spotty).", "I was finally able to repro this.\r\n\r\nBy default, TensorFlow (Mobile) uses ~4 threads, whereas TensorFlow Lite only uses 1. If you increase the thread count to 2, or 4, you should see a modest speedup (On a Pixel 2 it went from ~420ms to ~250ms when going from 1 to 4 threads).\r\n\r\nThe TensorFlow (Mobile) execution is still slightly faster, as it has multi-threading support for the lrn op, whereas TensorFlow Lite only multi-threads the conv op at the moment. We're actively working on expanded multi-threaded support across more kernels.\r\n\r\nAll that said, the quantized path should offer some amount of speedup. I've filed a bug internally to track that.", "Thanks for looking into it!", "Over to suharshs@ who is working on quantized optimizations.", "We have added many quantization performance improvements, is this still in the new versions of TF", "Glad to hear from you again! I will test it today and report the results back to you. ", "```import tensorflow as tf\r\nfrom tensorflow.lite.python.lite import Optimize\r\n\r\nfrom tensorflow.lite.python.lite_constants import QUANTIZED_UINT8, FLOAT\r\n\r\ngraph_def_file = \"save_path/smaller_alex.pb\"\r\ninput_arrays = [\"Placeholder\"]\r\noutput_arrays = ['output']\r\nalex_size = 114\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n    graph_def_file, input_arrays, output_arrays,\r\n    input_shapes={\"Placeholder\": [1, alex_size, alex_size, 3]})\r\n\r\nconverter.inference_type = QUANTIZED_UINT8\r\nconverter.inference_input_type = QUANTIZED_UINT8\r\nconverter.quantized_input_stats = {\"Placeholder\": (128., 128.)}\r\nconverter.default_ranges_stats = (0, 6)\r\nconverter.change_concat_input_ranges = False\r\n\r\ntflite_model = converter.convert()\r\nopen(\"output/frozen_quantized_alex_new.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nIf I use this code to convert the model, I get the following message: `Unimplemented: this graph contains an operator of type LocalResponseNormalization for which the quantized form is not yet implemented.`\r\nSo that operation needs to be implemented.\r\n\r\nIf I use the following code to convert and (try to) quantize:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.lite.python.lite import Optimize\r\n\r\nfrom tensorflow.lite.python.lite_constants import QUANTIZED_UINT8, FLOAT\r\n\r\ngraph_def_file = \"save_path/smaller_alex.pb\"\r\ninput_arrays = [\"Placeholder\"]\r\noutput_arrays = ['output']\r\nalex_size = 114\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n    graph_def_file, input_arrays, output_arrays,\r\n    input_shapes={\"Placeholder\": [1, alex_size, alex_size, 3]})\r\n\r\nconverter.post_training_quantize = True\r\nconverter.optimizations = [Optimize.OPTIMIZE_FOR_SIZE]\r\n\r\ntflite_model = converter.convert()\r\nopen(\"output/frozen_quantized_alex_new.tflite\", \"wb\").write(tflite_model)\r\n```\r\nThe conversion is successful, but the inference time is about 5x slower than using the plain `.pb` file. In addition to that I can`t run the model under Android with the message \r\n```\r\njava.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '2'  Registration failed.\r\n```\r\nSo is there any way to convert my model to tflite and quantize t to significantely improve my inference time?", "Testing relative performance between TF and TFLite via Python won't give you a meaningful reflection of actual on-device performance, as TFLite is specifically optimized for arm/arm64 devices, not (x86) desktops.\r\n\r\nAs for the Android failure, make sure you're using the latest TFLite nightly build (org.tensorflow:tensorflow-lite:0.0.0-nightly), and be sure to [clear your gradle cache](https://stackoverflow.com/questions/23025433/how-to-clear-gradle-cache) if necessary.", "Changing the gradle dependency makes the model converted with the second described method work. It runs a tiny bit faster on Android than the plain `.pb` file, so that is a success. However, how to quantize the model? Is the method I posted correct? Is the unimplemented operation the only thing hindering the conversion process?", "Yes, I would recommend using post training quantization https://www.tensorflow.org/lite/performance/post_training_quantization\r\n\r\nInterpreter: Didn't find op for builtin opcode 'CONV_2D' version '2'  Registration failed.\r\n\r\nmeans that you need to upgrade your interpreter version.", "Marking issue as resolved due to inactivity. Feel free to re-open this if it's unresolved or file a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose)"]}, {"number": 23765, "title": "Help !  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package", "body": "mac OS  \uff0c\r\nI'm stuck in this for a long time\uff01 \r\n\r\nZBMAC-C02VQ1YYH:tensorflow tianchuangxin1$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: Processed legacy workspace file /Users/tianchuangxin1/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\r\nERROR: /Users/tianchuangxin1/tensorflow/tensorflow/python/BUILD:5810:1: no such package '@cython//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/cython/cython/archive/0.28.4.tar.gz, https://github.com/cython/cython/archive/0.28.4.tar.gz] to /private/var/tmp/_bazel_tianchuangxin1/ef464020c356ceb4fa8b131168a2e389/external/cython/0.28.4.tar.gz: All mirrors are down: [] and referenced by '//tensorflow/python:framework/fast_tensor_util.pyx_cython_translation'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@cython//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/cython/cython/archive/0.28.4.tar.gz, https://github.com/cython/cython/archive/0.28.4.tar.gz] to /private/var/tmp/_bazel_tianchuangxin1/ef464020c356ceb4fa8b131168a2e389/external/cython/0.28.4.tar.gz: All mirrors are down: []\r\nINFO: Elapsed time: 48.999s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)", "comments": ["\r\nIs there a problem with my bazel version?", "Second time!\r\n\r\nAnalysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@com_google_absl//absl/strings': java.io.IOException: thread interrupted", "ERROR: /Users/tianchuangxin1/tensorflow/tensorflow/BUILD:562:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_tianchuangxin1/ef464020c356ceb4fa8b131168a2e389/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/private/var/tmp/_bazel_tianchuangxin1/ef464020c356ceb4fa8b131168a2e389/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 81, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/private/var/tmp/_bazel_tianchuangxin1/ef464020c356ceb4fa8b131168a2e389/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py\", line 29, in <module>\r\n    from tensorflow.python.keras import datasets\r\n  File \"/private/var/tmp/_bazel_tianchuangxin1/ef464020c356ceb4fa8b131168a2e389/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/keras/datasets/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras.datasets import imdb\r\n  File \"/private/var/tmp/_bazel_tianchuangxin1/ef464020c356ceb4fa8b131168a2e389/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/keras/datasets/imdb.py\", line 25, in <module>\r\n    from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq\r\n  File \"/private/var/tmp/_bazel_tianchuangxin1/ef464020c356ceb4fa8b131168a2e389/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/keras/preprocessing/__init__.py\", line 21, in <module>\r\n    import keras_preprocessing\r\nImportError: No module named keras_preprocessing\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 40.438s, Critical Path: 8.51s\r\nINFO: 3 processes: 3 local.\r\nFAILED: Build did NOT complete successfully\r\n", "ImportError: No module named keras_preprocessing\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build", "ERROR: /Users/tianchuangxin1/tensorflow/tensorflow/python/BUILD:3960:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1)\r\nld: can't open -exported_symbols_list file: -filelist\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.667s, Critical Path: 0.25s\r\nINFO: 1 process: 1 local.\r\nFAILED: Build did NOT complete successfully", "@T-chuangxin  Could you please provide all the details asked in the new issue template. That would help us to look into the issue.", "```\r\n: All mirrors are down: []\r\n```\r\nLooks like the problem is your internet connection or firewall.\r\nYou cannot access any of the mirrors for the cython library.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "yes. still issue."]}, {"number": 23764, "title": "How to know the value of the deterministic operation seed of tf.nn.dropout when we have already set the graph level seed and do not specify any seed in the op", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23763, "title": "Eager Execution doesnt save .meta file", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): example code for cnn network \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows with anaconda\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version :1.12\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:9.0/7.5\r\n- GPU model and memory:rtx 2080 \r\n\r\n\r\n**Current behavior**\r\n\r\ntraining of model in eager execution mode and save checkpoint using tfe.saver.But it only saves the following files:\r\n-checkpoint\r\n-checkpoint.ckpt-00-data\r\n-checkpoint.ckpt-00-index\r\n\r\nThere is no .meta file being saved\r\n\r\nDue to this i am not able to restore my checkpoint file and create a graph from it or use it create a .npy file.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should have saved three .ckpt files namely:\r\n-checkpoint\r\n-checkpoint.ckpt-00.data\r\n-checkpoint.ckpt-00.index\r\n-checkpoint.ckpt-00.meta\r\n\r\n\r\n**Code to reproduce the issue**\r\nhttps://github.com/madalinabuzau/tensorflow-eager-tutorials/blob/master/07_convolutional_neural_networks_for_emotion_recognition.ipynb\r\nThis code is similar to my implementation\r\n\r\n", "comments": ["Eager execution cannot save a .meta file because it's a data structure that doesn't exist in eager (there's no graph with eager execution, after all).\r\n\r\nHowever, loading the variables of a model should not require importing a metagraph, which is the only thing that needs a .meta file. Can you provide more information about how you're trying to load this model?", "@alextp below is the code which i used to save the model and restore.\r\n\r\n**Training and saving the model**\r\n```\r\nclass EmotionRecognitionCNN(tf.keras.Model):\r\n    \r\n    def __init__(self, num_classes, device='cpu:0', checkpoint_directory=None):\r\n        ''' Define the parameterized layers used during forward-pass, the device\r\n            where you would like to run the computation on and the checkpoint\r\n            directory.\r\n            \r\n            Args:\r\n                num_classes: the number of labels in the network.\r\n                device: string, 'cpu:n' or 'gpu:n' (n can vary). Default, 'cpu:0'.\r\n                checkpoint_directory: the directory where you would like to save or \r\n                                      restore a model.\r\n        ''' \r\n        super(EmotionRecognitionCNN, self).__init__()\r\n        \r\n        # Initialize layers\r\n        self.conv1 = tf.layers.Conv2D(16, 5, padding='same', activation=None)\r\n        self.batch1 = tf.layers.BatchNormalization()\r\n        self.conv2 = tf.layers.Conv2D(16, 5, 2, padding='same', activation=None)\r\n        self.batch2 = tf.layers.BatchNormalization()\r\n        self.conv3 = tf.layers.Conv2D(32, 5, padding='same', activation=None)\r\n        self.batch3 = tf.layers.BatchNormalization()\r\n        self.conv4 = tf.layers.Conv2D(32, 5, 2, padding='same', activation=None)\r\n        self.batch4 = tf.layers.BatchNormalization()\r\n        self.conv5 = tf.layers.Conv2D(64, 3, padding='same', activation=None)\r\n        self.batch5 = tf.layers.BatchNormalization()\r\n        self.conv6 = tf.layers.Conv2D(64, 3, 2, padding='same', activation=None)\r\n        self.batch6 = tf.layers.BatchNormalization()\r\n        self.conv7 = tf.layers.Conv2D(64, 1, padding='same', activation=None)\r\n        self.batch7 = tf.layers.BatchNormalization()\r\n        self.conv8 = tf.layers.Conv2D(128, 3, 2, padding='same', activation=None)\r\n        self.batch8 = tf.keras.layers.BatchNormalization()\r\n        self.conv9 = tf.layers.Conv2D(256, 1, padding='same', activation=None)\r\n        self.batch9 = tf.keras.layers.BatchNormalization()\r\n        self.conv10 = tf.layers.Conv2D(128, 3, 2, padding='same', activation=None)\r\n        self.conv11 = tf.layers.Conv2D(256, 1, padding='same', activation=None)\r\n        self.batch11 = tf.layers.BatchNormalization()\r\n        self.conv12 = tf.layers.Conv2D(num_classes, 3, 2, padding='same', activation=None)\r\n        \r\n        # Define the device \r\n        self.device = device\r\n        \r\n        # Define the checkpoint directory\r\n        self.checkpoint_directory = checkpoint_directory\r\n       \r\n    def predict(self, images, training):\r\n        \"\"\" Predicts the probability of each class, based on the input sample.\r\n            \r\n            Args:\r\n                images: 4D tensor. Either an image or a batch of images.\r\n                training: Boolean. Either the network is predicting in\r\n                          training mode or not.\r\n        \"\"\"\r\n        x = self.conv1(images)\r\n        x = self.batch1(x, training=training)\r\n        x = self.conv2(x)\r\n        x = self.batch2(x, training=training)\r\n        x = tf.nn.relu(x)\r\n        x = tf.layers.dropout(x, rate=0.4, training=training)\r\n        x = self.conv3(x)\r\n        x = self.batch3(x, training=training)\r\n        x = self.conv4(x)\r\n        x = self.batch4(x, training=training)\r\n        x = tf.nn.relu(x)\r\n        x = tf.layers.dropout(x, rate=0.3, training=training)\r\n        x = self.conv5(x)\r\n        x = self.batch5(x, training=training)\r\n        x = self.conv6(x)\r\n        x = self.batch6(x, training=training)\r\n        x = tf.nn.relu(x)\r\n        x = tf.layers.dropout(x, rate=0.3, training=training)\r\n        x = self.conv7(x)\r\n        x = self.batch7(x, training=training)\r\n        x = self.conv8(x)\r\n        x = self.batch8(x, training=training)\r\n        x = tf.nn.relu(x)\r\n        x = tf.layers.dropout(x, rate=0.3, training=training)\r\n        x = self.conv9(x)\r\n        x = self.batch9(x, training=training)\r\n        x = self.conv10(x)\r\n        x = self.conv11(x)\r\n        x = self.batch11(x, training=training)\r\n        x = self.conv12(x)\r\n        return tf.layers.flatten(x)\r\n    \r\n    def loss_fn(self, images, target, training):\r\n        \"\"\" Defines the loss function used during \r\n            training.         \r\n        \"\"\"\r\n        preds = self.predict(images, training)\r\n        loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=preds)\r\n        return loss\r\n    \r\n    def grads_fn(self, images, target, training):\r\n        \"\"\" Dynamically computes the gradients of the loss value\r\n            with respect to the parameters of the model, in each\r\n            forward pass.\r\n        \"\"\"\r\n        with tfe.GradientTape() as tape:\r\n            loss = self.loss_fn(images, target, training)\r\n        return tape.gradient(loss, self.variables)\r\n    \r\n    def restore_model(self):\r\n        \"\"\" Function to restore trained model.\r\n        \"\"\"\r\n        with tf.device(self.device):\r\n            # Run the model once to initialize variables\r\n            dummy_input = tf.constant(tf.zeros((1,48,48,1)))\r\n            dummy_pred = self.predict(dummy_input, training=False)\r\n            # Restore the variables of the model\r\n            saver = tfe.Saver(self.variables)\r\n            saver.restore(tf.train.latest_checkpoint\r\n                          (self.checkpoint_directory))\r\n    \r\n    def save_model(self, global_step=0):\r\n        \"\"\" Function to save trained model.\r\n        \"\"\"\r\n        tfe.Saver(self.variables).save(self.checkpoint_directory, \r\n                                       global_step=global_step)   \r\n    \r\n    def compute_accuracy(self, input_data):\r\n        \"\"\" Compute the accuracy on the input data.\r\n        \"\"\"\r\n        with tf.device(self.device):\r\n            acc = tfe.metrics.Accuracy()\r\n            for images, targets in tfe.Iterator(input_data):\r\n                # Predict the probability of each class\r\n                logits = self.predict(images, training=False)\r\n                # Select the class with the highest probability\r\n                preds = tf.argmax(logits, axis=1)\r\n                # Compute the accuracy\r\n                acc(tf.reshape(targets, [-1,]), preds)\r\n        return acc\r\n        \r\n    def fit(self, training_data, eval_data, optimizer, num_epochs=500, \r\n            early_stopping_rounds=10, verbose=10, train_from_scratch=False):\r\n        \"\"\" Function to train the model, using the selected optimizer and\r\n            for the desired number of epochs. You can either train from scratch\r\n            or load the latest model trained. Early stopping is used in order to\r\n            mitigate the risk of overfitting the network.\r\n            \r\n            Args:\r\n                training_data: the data you would like to train the model on.\r\n                                Must be in the tf.data.Dataset format.\r\n                eval_data: the data you would like to evaluate the model on.\r\n                            Must be in the tf.data.Dataset format.\r\n                optimizer: the optimizer used during training.\r\n                num_epochs: the maximum number of iterations you would like to \r\n                            train the model.\r\n                early_stopping_rounds: stop training if the loss on the eval \r\n                                       dataset does not decrease after n epochs.\r\n                verbose: int. Specify how often to print the loss value of the network.\r\n                train_from_scratch: boolean. Whether to initialize variables of the\r\n                                    the last trained model or initialize them\r\n                                    randomly.\r\n        \"\"\" \r\n    \r\n        if train_from_scratch==False:\r\n            self.restore_model()\r\n        \r\n        # Initialize best loss. This variable will store the lowest loss on the\r\n        # eval dataset.\r\n        best_loss = 999\r\n        \r\n        # Initialize classes to update the mean loss of train and eval\r\n        train_loss = tfe.metrics.Mean('train_loss')\r\n        eval_loss = tfe.metrics.Mean('eval_loss')\r\n        \r\n        # Initialize dictionary to store the loss history\r\n        self.history = {}\r\n        self.history['train_loss'] = []\r\n        self.history['eval_loss'] = []\r\n        \r\n        # Begin training\r\n        with tf.device(self.device):\r\n            for i in range(num_epochs):\r\n                # Training with gradient descent\r\n                for images, target in tfe.Iterator(training_data):\r\n                    grads = self.grads_fn(images, target, True)\r\n                    optimizer.apply_gradients(zip(grads, self.variables))\r\n                    \r\n                # Compute the loss on the training data after one epoch\r\n                for images, target in tfe.Iterator(training_data):\r\n                    loss = self.loss_fn(images, target, False)\r\n                    train_loss(loss)\r\n                self.history['train_loss'].append(train_loss.result().numpy())\r\n                # Reset metrics\r\n                train_loss.init_variables()\r\n                \r\n                # Compute the loss on the eval data after one epoch\r\n                for images, target in tfe.Iterator(eval_data):\r\n                    loss = self.loss_fn(images, target, False)\r\n                    eval_loss(loss)\r\n                self.history['eval_loss'].append(eval_loss.result().numpy())\r\n                # Reset metrics\r\n                eval_loss.init_variables()\r\n                \r\n                # Print train and eval losses\r\n                if (i==0) | ((i+1)%verbose==0):\r\n                    print('Train loss at epoch %d: ' %(i+1), self.history['train_loss'][-1])\r\n                    print('Eval loss at epoch %d: ' %(i+1), self.history['eval_loss'][-1])\r\n\r\n                # Check for early stopping\r\n                if self.history['eval_loss'][-1]<best_loss:\r\n                    best_loss = self.history['eval_loss'][-1]\r\n                    count = early_stopping_rounds\r\n                else:\r\n                    count -= 1\r\n                if count==0:\r\n                    break\r\n\r\n\r\ncheckpoint_directory = 'models_checkpoints/EmotionCNN/'\r\n\r\n# Use the GPU if available.\r\ndevice = 'gpu:0' if tfe.num_gpus()>0 else 'cpu:0'\r\n\r\n# Define optimizer.\r\noptimizer = tf.train.AdamOptimizer()\r\n\r\n# Instantiate model. This doesn't initialize the variables yet.\r\nmodel = EmotionRecognitionCNN(num_classes=7, device=device, \r\n                              checkpoint_directory=checkpoint_directory)\r\n\r\nmodel.fit(training_data, eval_data, optimizer, num_epochs=500, \r\n          early_stopping_rounds=5, verbose=10, train_from_scratch=False)\r\nmodel.save_model()\r\n```\r\n\r\n\r\n**For restoring the checkpoint:**\r\n\r\n```\r\nmodel =EmotionRecognitionCNN(num_classes=7, device=device, \r\n                              checkpoint_directory=checkpoint_directory)\r\n\r\nmodel.restore_model()\r\nmodel.predict(test_image)\r\n```\r\n\r\nWhile restoring the model it throws me error such as no variable found or unable to load.\r\n\r\n**What could be the issue.Also how do i extract weight from a checkpoint file in eager mode**\r\n", "The variables in this model only get built when the layers are used, not when the layers are instantiated, which is why you're getting this error.\r\n\r\n@allenlavoie do you think it's worth putting a better error message here? Or should the checkpointable API be used instead?", "There is a `self.predict` call in load which looks like it should call the layers. But it won't create slot variables in the optimizer, which might be the issue.\r\n\r\n@vishalghor can you use [`tf.keras.Model.save_weights(..., save_format='tf')`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#save_weights) and `Model.load_weights` instead? Or `tf.train.Checkpoint` if you need more control. With those there's no need to create variables before loading them, they'll be restored on creation. `tfe.Saver` and `tf.train.Saver` are not going to be part of the 2.x API.", "@allenlavoie  with the method suggested will i be able to access weight matrix by their name and use it . How does inference in eager execution is achieved with checkpoint files.\r\nKindly help me with this. in the same time i will try the method suggested by you. ", "@vishalghor I have the same problem as you. Sometimes it does not save .meta file, and once it instead saved ckp.meta.tmp854491c52a564b4a910a8836b8cb2734 which was an empty file. I have no idea with it. Have you got some solutions now?", "Using tf.saved_model.save in nightly should fix this issue. Closing."]}]