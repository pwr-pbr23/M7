[{"number": 28902, "title": "INTERNAL ERROR reported while trying to apply GpuDelegate to tflite", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: Yes, I am using my own training model based on MobileNetV2. \r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- Mobile device: XiaoMe 8, Android 9\r\n- tflite installed from (source or binary): nightly AAR (org.tensorflow:tensorflow-lite:0.0.0-nightly)\r\n- tflite-gpu installed from (source or binary): nightly AAR (org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly)\r\n- GPU model and memory: \r\n\r\n**Describe the current behavior**\r\nWhen I attempt to load my .tflite model on the Android (after applying the GpuDelegate), it fails saying \"INTERNAL ERROR\" (see traceback below). What does it mean for this error? And what shall I do? Thanks!\r\n\r\n**Describe the expected behavior**\r\nThe model should be loaded correctly and then run the Inference.\r\n\r\n**Code to reproduce the issue**\r\nI just modify the downloaded [TensorFlow Lite Android image classification example](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android), and add one more class extended from class Classifier to load my own trained model which is based on MobileNetV2. The model loads and runs fine if with CPU, i.e. I do not attempt to apply the GPU delegate before the attempting to load it. The only difference in the code is the following lines:\r\n\r\n`tfliteOptions = new Interpreter.Options();`\r\n`gpuDelegate = new GpuDelegate();`\r\n`tfliteOptions.addDelegate(gpuDelegate);`\r\n\r\n**Other info / logs**\r\n\r\n    --------- beginning of crash\r\n2019-05-21 08:27:02.183 13748-13769/org.tensorflow.lite.examples.classification E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.classification, PID: 13748\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Prepare: Shader compilation failed: ERROR: 0:6: 'data' : Syntax error:  syntax error\r\n    INTERNAL ERROR: no main() function!\r\n    ERROR: 1 compilation errors.  No code generated.\r\n    \r\n    Node number 73 (TfLiteGpuDelegate) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:83)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:60)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)\r\n        at org.tensorflow.lite.examples.classification.tflite.Classifier.<init>(Classifier.java:195)\r\n        at org.tensorflow.lite.examples.classification.tflite.ClassifierNote.<init>(ClassifierNote.java:41)\r\n        at org.tensorflow.lite.examples.classification.tflite.Classifier.create(Classifier.java:107)\r\n        at org.tensorflow.lite.examples.classification.ClassifierActivity.recreateClassifier(ClassifierActivity.java:167)\r\n        at org.tensorflow.lite.examples.classification.ClassifierActivity.lambda$onInferenceConfigurationChanged$0$ClassifierActivity(ClassifierActivity.java:146)\r\n        at org.tensorflow.lite.examples.classification.-$$Lambda$ClassifierActivity$83lGy2TUjuj0M5n4BhMB9qlLgSY.run(Unknown Source:8)\r\n        at android.os.Handler.handleCallback(Handler.java:873)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:201)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n", "comments": ["@valwang \r\n\r\nCould you elaborate what you mean by \"written custom code\"?  Did you just re-architect MobileNetV2, or did you actually try to write shader code?", "@impjdi I just re-architect MobileNetv2. My training model is based on MobileNetV2, but the input and output is modified.", "Today I tried to use the libtensorflowlite_gpu_gl.so for the same purpose, still got the similar errors: \r\n++++++++++++++++++\r\n2019-06-02 13:59:36.044 6836-6836/com.example.testmodel I/tflite: Created TensorFlow Lite delegate for GPU.\r\n2019-06-02 13:59:36.055 6836-6836/com.example.testmodel D/libEGL: eglInitialize: enter\r\n2019-06-02 13:59:36.045 6836-6836/com.example.testmodel W/ample.testmodel: type=1400 audit(0.0:8458): avc: denied { search } for name=\"ctx\" dev=\"debugfs\" ino=13889 scontext=u:r:untrusted_app:s0:c213,c256,c512,c768 tcontext=u:object_r:qti_debugfs:s0 tclass=dir permissive=0\r\n2019-06-02 13:59:36.055 6836-6836/com.example.testmodel D/libEGL: eglInitialize: exit(res=1)\r\n2019-06-02 13:59:36.055 6836-6836/com.example.testmodel E/libEGL: call to OpenGL ES API with no current context (logged once per thread)\r\n2019-06-02 13:59:36.142 6836-6836/com.example.testmodel I/Adreno: ERROR: 0:6: 'data' : Syntax error:  syntax error\r\n    INTERNAL ERROR: no main() function!\r\n    ERROR: 1 compilation errors.  No code generated.\r\n2019-06-02 13:59:36.143 6836-6836/com.example.testmodel E/tflite: TfLiteGpuDelegate Prepare: Shader compilation failed: ERROR: 0:6: 'data' : Syntax error:  syntax error\r\n    INTERNAL ERROR: no main() function!\r\n    ERROR: 1 compilation errors.  No code generated.\r\n2019-06-02 13:59:36.143 6836-6836/com.example.testmodel E/tflite: Node number 73 (TfLiteGpuDelegate) failed to prepare.\r\n2019-06-02 13:59:36.143 6836-6836/com.example.testmodel E/TestModel: test: ModifyGraphWithDelegate failed!\r\n2019-06-02 13:59:36.241 6836-6861/com.example.testmodel D/OpenGLRenderer: endAllActiveAnimators on 0x77921fb900 (DropDownListView) with handle 0x7791b85e80\r\n+++++++++++++++++++++++++++\r\n\r\nThe code I am using to initialize the GPU Delegate is as below:\r\n+++++++++++++++++++++++++++\r\nconst TfLiteGpuDelegateOptions options = {\r\n           .metadata = NULL,\r\n           .compile_options = {\r\n                .precision_loss_allowed = 1,  // FP16  \r\n                .preferred_gl_object_type = TFLITE_GL_OBJECT_TYPE_FASTEST,\r\n                .dynamic_batch_enabled = 0,   // Not fully functional yet\r\n            },\r\n};\r\ndelegate = TfLiteGpuDelegateCreate(&options);\r\nif (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) {\r\n     LOGE(\"%s: ModifyGraphWithDelegate failed!\", __FUNCTION__);\r\n     return 0;\r\n}\r\n+++++++++++++++++++++++++++\r\n\r\n", "@valwang \r\n\r\nHm.\r\n\r\n> 2019-06-02 13:59:36.055 6836-6836/com.example.testmodel E/libEGL: call to OpenGL ES API with no current context (logged once per thread)\r\n\r\nis interesting.  Are you certain that your device supports OpenGL ES 3.1?\r\n\r\n", "My device support OpenGL ES 3.2. I have run [tensorflowlite image_classification demo](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android) with GPU correctly on my device. But when I changed to my own model, I got the errors.", "Hi,\r\nI'm having the same issue, also with our own model:\r\n```\r\nI/Adreno: ERROR: 0:6: 'data' : Syntax error:  syntax error\r\n    INTERNAL ERROR: no main() function!\r\n    ERROR: 1 compilation errors.  No code generated.\r\nE/AndroidRuntime: FATAL EXCEPTION: ScreenshotProcessingBackground\r\n    Process: [REDUCTED]\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Prepare: Shader compilation failed: ERROR: 0:6: 'data' : Syntax error:  syntax error\r\n    INTERNAL ERROR: no main() function!\r\n    ERROR: 1 compilation errors.  No code generated.\r\n    \r\n    Node number 12 (TfLiteGpuDelegate) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:83)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:60)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)\r\n```", "Hi @impjdi ..\r\nIam facing similar issue with the latest classification demo app (tf-nightly gpu delegate) with GPU.\r\nI am using a custom model. Even though all nodes are GPU supported, it shows another error  ... \r\n      Node number 9 (TfLiteGpuDelegate) failed to prepare. \r\nHere is the error log\r\n\r\n```\r\n\r\n    Process: com.example.anilsathyan7.sketch, PID: 8757\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.anilsathyan7.sketch/com.example.anilsathyan7.sketch.MainActivity}: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Prepare: Shader compilation failed: ERROR: 0:6: 'unknown' : not a legal layout qualifier id \r\n    ERROR: 0:6: 'unknown' : Syntax error:  syntax error\r\n    INTERNAL ERROR: no main() function!\r\n    ERROR: 2 compilation errors.  No code generated.\r\n    \r\n    Node number 9 (TfLiteGpuDelegate) failed to prepare.\r\n    \r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3037)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3172)\r\n        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:78)\r\n        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108)\r\n        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1906)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:193)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6863)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:537)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:858)\r\n     Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Prepare: Shader compilation failed: ERROR: 0:6: 'unknown' : not a legal layout qualifier id \r\n    ERROR: 0:6: 'unknown' : Syntax error:  syntax error\r\n    INTERNAL ERROR: no main() function!\r\n    ERROR: 2 compilation errors.  No code generated.\r\n    \r\n    Node number 9 (TfLiteGpuDelegate) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:83)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:60)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)\r\n        at com.example.anilsathyan7.sketch.Classifier.<init>(Classifier.java:185)\r\n        at com.example.anilsathyan7.sketch.ClassifierFloatMobileNet.<init>(ClassifierFloatMobileNet.java:41)\r\n        at com.example.anilsathyan7.sketch.Classifier.create(Classifier.java:97)\r\n        at com.example.anilsathyan7.sketch.MainActivity.recreateClassifier(MainActivity.java:72)\r\n        at com.example.anilsathyan7.sketch.MainActivity.onCreate(MainActivity.java:37)\r\n        at android.app.Activity.performCreate(Activity.java:7149)\r\n        at android.app.Activity.performCreate(Activity.java:7140)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1288)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3017)\r\n        \t... 11 more\r\n```\r\n\r\nHere is the model :-\r\n\r\n![model_sketch](https://user-images.githubusercontent.com/1130185/60041364-d27cb000-96d8-11e9-8c0e-8722e3e6e80a.png)\r\n\r\n\r\n\r\nUsing android version 9.0", "@valwang @anilsathyan7 \r\n\r\nIt's not always the case, but often, this error message comes from when our OpenGL shader generators were not able to handle a certain case, but proceeds.  For example, if the settings told a `CONV_2D` shader generator to work on texture with FP64, but it doesn't know how to do it, it will most probably generate garbage or not generate anything for `data` at all.  Then you can encounter this.\r\n\r\nIs it possible for you to bisect which op is causing this under what circumstances? ", "As far as I've looked on the internet, this seems to stem from some dimension mismatch between input tensor and expected input tensor just before a dense layer (perhaps the input is from a conv layer which needs to be flattened?).\r\nCheck out [https://github.com/tensorflow/tensorflow/issues/30303](url) - this might be of help.", "We are also hitting this when trying to get a (slightly modified to be able to get GPU delegation to start working) DeepSpeech model exported for TFLite.\r\n\r\nOutput from TFLite `benchmark_model`:\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Next operations are not supported by GPU delegate:\r\nCUSTOM AudioSpectrogram: Operation is not supported.\r\nCUSTOM Mfcc: Operation is not supported.\r\nSPLIT: Operation is not supported.\r\nFirst 5 operations will run on the GPU, and the remaining 18 on the CPU.\r\nINFO: Replacing 5 node(s) with delegate (TfLiteGpuDelegate) node.\r\nERROR: 0:6: 'unknown' : not a legal layout qualifier id \r\nERROR: 0:6: 'unknown' : Syntax error:  syntax error\r\nINTERNAL ERROR: no main() function!\r\nERROR: 2 compilation errors.  No code generated.\r\n\r\nERROR: TfLiteGpuDelegate Prepare: Shader compilation failed: ERROR: 0:6: 'unknown' : not a legal layout qualifier id \r\nERROR: 0:6: 'unknown' : Syntax error:  syntax error\r\nINTERNAL ERROR: no main() function!\r\nERROR: 2 compilation errors.  No code generated.\r\n\r\n\r\nERROR: Node number 23 (TfLiteGpuDelegate) failed to prepare.\r\n```", "> We are also hitting this when trying to get a (slightly modified to be able to get GPU delegation to start working) DeepSpeech model exported for TFLite.\r\n> \r\n> Output from TFLite `benchmark_model`:\r\n> \r\n> ```\r\n> INFO: Initialized TensorFlow Lite runtime.\r\n> INFO: Created TensorFlow Lite delegate for GPU.\r\n> ERROR: Next operations are not supported by GPU delegate:\r\n> CUSTOM AudioSpectrogram: Operation is not supported.\r\n> CUSTOM Mfcc: Operation is not supported.\r\n> SPLIT: Operation is not supported.\r\n> First 5 operations will run on the GPU, and the remaining 18 on the CPU.\r\n> INFO: Replacing 5 node(s) with delegate (TfLiteGpuDelegate) node.\r\n> ERROR: 0:6: 'unknown' : not a legal layout qualifier id \r\n> ERROR: 0:6: 'unknown' : Syntax error:  syntax error\r\n> INTERNAL ERROR: no main() function!\r\n> ERROR: 2 compilation errors.  No code generated.\r\n> \r\n> ERROR: TfLiteGpuDelegate Prepare: Shader compilation failed: ERROR: 0:6: 'unknown' : not a legal layout qualifier id \r\n> ERROR: 0:6: 'unknown' : Syntax error:  syntax error\r\n> INTERNAL ERROR: no main() function!\r\n> ERROR: 2 compilation errors.  No code generated.\r\n> \r\n> \r\n> ERROR: Node number 23 (TfLiteGpuDelegate) failed to prepare.\r\n> ```\r\n\r\nFTR, forcing some reshape on the input tensor did work, as suggested in https://github.com/tensorflow/tensorflow/issues/30303#issuecomment-508117448", "@valwang We are checking to see if you still need help on this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "OK, I will heck the newer version for this. Thanks!\r\n\r\nFrom: kumariko ***@***.***>\r\nSent: 2021\u5e749\u67089\u65e5 13:56\r\nTo: tensorflow/tensorflow ***@***.***>\r\nCc: Wang, Gang (NSB - CN/Hangzhou) ***@***.***>; Mention ***@***.***>\r\nSubject: [Suspected Marketing Mail] Re: [tensorflow/tensorflow] INTERNAL ERROR reported while trying to apply GpuDelegate to tflite (#28902)\r\n\r\n\r\n@valwang<https://github.com/valwang> We are checking to see if you still need help on this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/28902#issuecomment-915785475>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ACLXVRQQISP5WEPD4TYKGD3UBBD6NANCNFSM4HOLCG3Q>.\r\nTriage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\r\n", "@valwang \r\nIs there any update on this, could you please verify on the latest version.", "Yes, with newer version, this error disappear. Now it can work correctly.\r\n\r\nFrom: Saduf2019 ***@***.***>\r\nSent: 2021\u5e7410\u670818\u65e5 16:11\r\nTo: tensorflow/tensorflow ***@***.***>\r\nCc: Wang, Gang (NSB - CN/Hangzhou) ***@***.***>; Mention ***@***.***>\r\nSubject: Re: [tensorflow/tensorflow] INTERNAL ERROR reported while trying to apply GpuDelegate to tflite (#28902)\r\n\r\n\r\n@valwang<https://github.com/valwang>\r\nIs there any update on this, could you please verify on the latest version.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/28902#issuecomment-945484442>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ACLXVRSTXCHDM3TLKHGINNDUHPJCXANCNFSM4HOLCG3Q>.\r\nTriage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\r\n", "@valwang \r\nThank you for your update, glad the issue is resolved, please move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28902\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28902\">No</a>\n"]}, {"number": 28901, "title": "[TF 2.0] Model not converging when trained with custom training loop", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.02 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary from Pypi\r\n- TensorFlow version (use command below): 2.0 alpha0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GTX1070 8GB\r\n\r\n**Describe the current behavior**\r\n\r\nI have created a simple regression model and trained it using keras and also using a custom training loop. What I noticed is that the model only converges when using the keras fit function. If I use the custom training loop, which manually computes gradients and applies them via an optimizer, the model does not converge. The same problem occurred not only on this very simple regression task but also on a model trained on the UTK Faces data set for age regression.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect both, the model trained with keras fit and the model trained with a custom training loop, to converge.\r\n\r\n**Code to reproduce the issue**\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\n# Define constants\r\nN_train_samples = 600\r\nN_validation_samples = 100\r\nN_evaluation_samples = 100\r\nnoise_sig = 0.1\r\nN_epochs = 100\r\nrandom_seed = 100\r\nbatch_size = 16\r\nlearning_rate = 0.1\r\n\r\n# Create target signal\r\nx = np.linspace(0.0, 3.0, N_train_samples+N_validation_samples+N_evaluation_samples, dtype=np.float32)\r\ny = np.sin(1 + x*x) + noise_sig*np.random.randn(N_train_samples+N_validation_samples+N_evaluation_samples).astype(np.float32)\r\ny_true = np.sin(1 + x*x)\r\n\r\n\r\n# Define model\r\nclass MyModel(object):\r\n    def __init__(self):\r\n        # Create model variables\r\n        self.W0 = tf.Variable(tf.random.normal([1, 10]), name=\"W0\")\r\n        self.b0 = tf.Variable(tf.zeros(10), name=\"b0\")\r\n        self.W1 = tf.Variable(tf.random.normal([10, 1]), name=\"W1\")\r\n        self.b1 = tf.Variable(tf.zeros(1), name=\"b1\")\r\n        self.trainable_variables = [self.W0, self.b0, self.W1, self.b1]\r\n\r\n    def __call__(self, inputs):\r\n        output = tf.reshape(inputs, [-1, 1])\r\n        output = tf.nn.sigmoid(tf.add(tf.matmul(output, self.W0), self.b0))\r\n        return tf.add(tf.matmul(output, self.W1), self.b1)\r\n\r\n\r\nclass MyKerasModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyKerasModel, self).__init__()\r\n        # Create model variables\r\n        self.dense0 = tf.keras.layers.Dense(10, activation=\"sigmoid\")\r\n        self.dense1 = tf.keras.layers.Dense(1, activation=\"linear\")\r\n\r\n    def call(self, inputs):\r\n        output = tf.reshape(inputs, [-1, 1])\r\n        output = self.dense0(output)\r\n        return self.dense1(output)\r\n\r\n\r\n\r\n# Define training step\r\n@tf.function\r\ndef train_step(model, optimizer, x, y):\r\n    with tf.GradientTape() as tape:\r\n        tape.watch(model.trainable_variables)\r\n        y_pred = model(x)\r\n        loss_val = tf.reduce_mean(tf.square(y-y_pred))\r\n    grads = tape.gradient(loss_val, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n    return loss_val\r\n\r\n\r\n# Shuffle and partition data set\r\nshuffle_idx = np.arange(0, N_train_samples+N_validation_samples+N_evaluation_samples)\r\nnp.random.shuffle(shuffle_idx)\r\nx_shuffled = x[shuffle_idx]\r\ny_shuffled = y[shuffle_idx]\r\nx_train = x_shuffled[0:N_train_samples]\r\ny_train = y_shuffled[0:N_train_samples]\r\nx_validation = x_shuffled[N_train_samples:N_train_samples+N_validation_samples]\r\ny_validation = y_shuffled[N_train_samples:N_train_samples+N_validation_samples]\r\nx_evaluation = x_shuffled[N_train_samples+N_validation_samples:N_train_samples+N_validation_samples+N_evaluation_samples]\r\ny_evaluation = y_shuffled[N_train_samples+N_validation_samples:N_train_samples+N_validation_samples+N_evaluation_samples]\r\n\r\n# Create data sets\r\ntrain_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(N_train_samples).batch(batch_size).repeat()\r\nvalidation_ds = tf.data.Dataset.from_tensor_slices((x_validation, y_validation)).batch(N_validation_samples)\r\nevaluation_ds = tf.data.Dataset.from_tensor_slices((x_evaluation, y_evaluation)).batch(N_evaluation_samples)\r\n\r\n# Create model and optimizer\r\nmdl = MyModel()\r\nopt = tf.optimizers.Adam(learning_rate)\r\n\r\nkeras_mdl = MyKerasModel()\r\nkeras_mdl.compile(optimizer=tf.optimizers.Adam(learning_rate), loss=\"mse\", metrics=[\"mae\"])\r\n\r\n# Train model\r\nepoch = 0\r\ntrain_iters = 0\r\ntrain_loss = 0.0\r\nfor x_feed, y_feed in train_ds:\r\n    train_loss += train_step(mdl, opt, x_feed, y_feed)\r\n    train_iters += 1\r\n    if (train_iters >= int(N_train_samples/batch_size)):\r\n        for x_feed, y_feed in validation_ds:\r\n            y_pred = mdl(x_feed)\r\n            validation_loss = tf.reduce_mean(tf.square(y-y_pred))\r\n        print(\"Epoch: {} Train loss: {:.5} Validation loss: {:.5}\".format(epoch, train_loss/train_iters, validation_loss))\r\n        train_iters = 0\r\n        train_loss = 0.0\r\n        epoch += 1\r\n    if (epoch == N_epochs):\r\n        break\r\n\r\nkeras_mdl.fit_generator(train_ds, epochs=N_epochs, steps_per_epoch=int(N_train_samples/batch_size),\\\r\n        validation_data=validation_ds, validation_steps=1)\r\n\r\n# Predict with model and plot results\r\ny_pred = mdl(x)\r\ny_keras_pred = keras_mdl(x)\r\nplt.plot(x, y)\r\nplt.plot(x, y_true)\r\nplt.plot(x, y_pred.numpy())\r\n\r\n```", "comments": ["@FelixWiewel Able to reproduce the issue with the provided code and visualized the graph", "Hi @FelixWiewel What if using `MyKerasModel` but with the custom training loop?", "@FelixWiewel @llan-ml \r\nI can reproduce the bug as well and already tried to use the `MyKerasModel` but with the same wrong behavior.  ", "> Hi @FelixWiewel What if using `MyKerasModel` but with the custom training loop?\r\n\r\nHi @llan-ml,\r\n\r\nI tried that and it gives the same wrong behavior. The only way I could make the model converge with a custom training loop was to use the MyKerasModel and train it with it's train_on_batch function.\r\n", "There is a bug in your code. In `train_step`, the shape of `y` is `(16,)`, while the shape of `y_pred` is `(16,1)`.\r\n\r\nYou should try this:\r\n<pre>\r\ndef train_step(model, optimizer, x, y):\r\n    y = y[:, tf.newaxis]\r\n    ...\r\n</pre>", "@llan-ml Thanks for pointing that out. Indeed this seems to solve the issue. But is this intended behavior? I mean shouldn't tensorflow be able to produce the same results regardless of this difference in shape?", "This is about array/tensor broadcast, similar to numpy. If you want to write customized training loops using low-level apis, you should take care and debug more.", "@llan-ml Thanks. I guess the issue can be closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28901\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28901\">No</a>\n"]}, {"number": 28900, "title": "undefined symbol: _ZTVN6icu_639ErrorCodeE while building v1.12.2", "body": "**System information**\r\n- Debian 10.0\r\n- TensorFlow installed from source\r\n- TensorFlow version: v1.12.2\r\n- Python version: 3.5.4\r\n- Bazel version: 0.15.2\r\n- GCC/Compiler version (if compiling from source): gcc 7.4.0\r\n- CUDA 9.2, cuDNN 7\r\n- GPU model and memory: GeForce 940MX, 2Go\r\n\r\nThe build fails with : \r\n```\r\nERROR: /opt/tensorflow/tensorflow/BUILD:533:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash failed: error executing command \r\n  (cd /home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda/lib64 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc-7 \\\r\n    NCCL_HDR_PATH=/usr/include \\\r\n    NCCL_INSTALL_PATH=/usr/lib/x86_64-linux-gnu \\\r\n    PATH=/home/yves/anaconda3/envs/digicampipe/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/yves/anaconda3/bin:/home/yves/coin/monero/monero/build/release/bin/ \\\r\n    PYTHON_BIN_PATH=/home/yves/anaconda3/envs/digicampipe/bin/python \\\r\n    PYTHON_LIB_PATH=/home/yves/anaconda3/envs/digicampipe/lib/python3.5/site-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=7.0 \\\r\n    TF_CUDA_VERSION=9.2 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_NCCL_VERSION=2 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/k8-opt/genfiles/tensorflow_api/v1/ --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow._api.v1 bazel-out/k8-opt/genfiles/tensorflow/_api/v1/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/app/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/bitwise/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/data/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/data/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/debugging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/distributions/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/dtypes/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/errors/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/feature_column/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/gfile/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/graph_util/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/initializers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/activations/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/densenet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_v3/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet_v2/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/nasnet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/resnet50/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg16/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg19/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/xception/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/backend/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/callbacks/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/constraints/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/boston_housing/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar10/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar100/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/imdb/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/mnist/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/reuters/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/estimator/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/initializers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/layers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/losses/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/metrics/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/models/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/optimizers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/sequence/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/text/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/regularizers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/wrappers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/wrappers/scikit_learn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/layers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/linalg/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/logging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/losses/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/manip/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/math/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/metrics/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/nn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/nn/rnn_cell/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/profiler/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/python_io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/quantization/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/random/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/resource_loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/strings/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/builder/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/main_op/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/signature_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/signature_def_utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/tag_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sparse/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/spectral/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/summary/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sysconfig/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/test/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/train/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/train/queue_runner/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/user_ops/__init__.py')\r\nTraceback (most recent call last):\r\n  File \"/home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/yves/anaconda3/envs/digicampipe/lib/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/yves/anaconda3/envs/digicampipe/lib/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTVN6icu_639ErrorCodeE\r\n During handling of the above exception, another exception occurred:\r\n Traceback (most recent call last):\r\n  File \"/home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/yves/anaconda3/envs/digicampipe/lib/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/yves/anaconda3/envs/digicampipe/lib/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTVN6icu_639ErrorCodeE\r\n Failed to load the native TensorFlow runtime.\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem** :\r\n```\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n```\r\n\r\n**Any other info / logs**\r\n```\r\n$ldd /home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\tlinux-vdso.so.1 (0x00007ffe132c4000)\r\n\tlibtensorflow_framework.so => /home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so (0x00007f3a9c170000)\r\n\tlibcublas.so.9.2 => /home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcublas.so.9.2 (0x00007f3a983a0000)\r\n\tlibcusolver.so.9.2 => /home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcusolver.so.9.2 (0x00007f3a90f5f000)\r\n\tlibcudart.so.9.2 => /home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib/libcudart.so.9.2 (0x00007f3a90cf5000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f3a90cc0000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f3a90c9f000)\r\n\tlibgomp.so.1 => /usr/lib/x86_64-linux-gnu/libgomp.so.1 (0x00007f3a90c6c000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f3a90ae9000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f3a90adf000)\r\n\tlibstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f3a9095b000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f3a90941000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f3a90780000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f3aaa5db000)\r\n\tlibcuda.so.1 => /usr/lib/x86_64-linux-gnu/libcuda.so.1 (0x00007f3a8f625000)\r\n\tlibcudnn.so.7 => /usr/local/cuda/lib64/libcudnn.so.7 (0x00007f3a7e694000)\r\n\tlibcufft.so.9.2 => /usr/lib/x86_64-linux-gnu/libcufft.so.9.2 (0x00007f3a7903a000)\r\n\tlibcurand.so.9.2 => /usr/lib/x86_64-linux-gnu/libcurand.so.9.2 (0x00007f3a750fc000)\r\n\tlibnvidia-fatbinaryloader.so.418.56 => /usr/lib/x86_64-linux-gnu/libnvidia-fatbinaryloader.so.418.56 (0x00007f3a74eae000)\r\n```\r\n\r\n```\r\n$nm -A /usr/lib/x86_64-linux-gnu/libicu*.a | grep _ZTVN6icu_639ErrorCodeE\r\n/usr/lib/x86_64-linux-gnu/libicui18n.a:decimfmt.ao:                 U _ZTVN6icu_639ErrorCodeE\r\nnm: wintzimpl.ao: no symbols\r\nnm: windtfmt.ao: no symbols\r\nnm: winnmfmt.ao: no symbols\r\n/usr/lib/x86_64-linux-gnu/libicui18n.a:number_mapper.ao:                 U _ZTVN6icu_639ErrorCodeE\r\n/usr/lib/x86_64-linux-gnu/libicuuc.a:errorcode.ao:0000000000000000 V _ZTVN6icu_639ErrorCodeE\r\nnm: cwchar.ao: no symbols\r\nnm: wintz.ao: no symbols\r\nnm: icuplug.ao: no symbols\r\n```\r\n\r\n```\r\nnm /home/yves/.cache/bazel/_bazel_yves/fbc06f9baef46cade6e35d9e4137e37c/execroot/org_tensorflow/bazel-out/host/bin/external/icu/libicuuc.pic.a | grep _ZTVN6icu_ | grep ErrorCode\r\n0000000000000000 V _ZTVN6icu_629ErrorCodeE\r\n```", "comments": ["For anyone having the same issue, I succeeded to compile Tensorflow by modifying third_party/icu/workspace.bzl in such a way that bazel would install the same version as the one already installed on the system.\r\n\r\nStill there must be something wrong somewhere which make the building of Tensorflow using system icu while it tries to use the icu from bazel during linking.\r\n\r\n", "Can you post the diff of the change you did to `third_party/icu/workspace.bzl`? This will ensure people hitting the same issue can find the workaround", "Sure, here it is:\r\n\r\n```diff\r\ndiff --git a/third_party/icu/workspace.bzl b/third_party/icu/workspace.bzl\r\nindex bfebf4219b..a243b83ec0 100644\r\n--- a/third_party/icu/workspace.bzl\r\n+++ b/third_party/icu/workspace.bzl\r\n@@ -5,11 +5,11 @@ load(\"//third_party:repo.bzl\", \"third_party_http_archive\")\r\n def repo():\r\n     third_party_http_archive(\r\n         name = \"icu\",\r\n-        strip_prefix = \"icu-release-62-1\",\r\n-        sha256 = \"e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761\",\r\n+        strip_prefix = \"icu-release-63-2\",\r\n+        sha256 = \"2234d9a2bfc5869f9aa9da9ce8d0b14803fcdf71c5a426b112dce8bf6dc79e34\",\r\n         urls = [\r\n-            \"https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz\",\r\n-            \"https://github.com/unicode-org/icu/archive/release-62-1.tar.gz\",\r\n+            \"https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-63-2.tar.gz\",\r\n+            \"https://github.com/unicode-org/icu/archive/release-63-2.tar.gz\",\r\n         ],\r\n         build_file = \"//third_party/icu:BUILD.bazel\",\r\n     )\r\n```\r\n\r\nIn order to get what to put after \"sha256 =\", I downloaded the release-63-2.tar.gz file and used sha256sum on it.", "Thanks.\r\n\r\nLooking again at the stack trace and error message I think this is caused by something escaping the conda environment.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Recently I encoutered this issue too on deepin linux 15.11 (like debian 9).\r\nMy workaround is removing system-wide libicu-dev package so that TF build system can't detect it:\r\n```\r\nsudo dpkg -r --force-depends libicu-dev\r\n```\r\nNote that `bazel clean --expunge` is required before continuing TF build.\r\nAfter TF was built, libicu-dev may be reinstalled."]}, {"number": 28899, "title": " Cannot register 2 metrics with the same name: /tensorflow/cc/saved_model/load_attempt_count ", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["1"]}, {"number": 28898, "title": "Analysis of target '//tensorflow/contrib/tensor_forest:model_ops_test' failed; build aborted: for tensorflow/contrib/tensor_forest/libutils.so", "body": "env:\r\nOS: windows 10\r\nbazel version: 0.24.1\r\ntensorflow version:r1.14\r\npython:3.6.4\r\nCUDA:9.0\r\ncuDNN: 7.4\r\n\r\n----------------------------------\r\n\r\nsend under command in gitBash could complete successfully\r\n```\r\n$ bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n----------------------------------\r\n\r\nbut send bazel test command encounter error, details as following.:\r\ncommand(in gitBash):\r\n\r\n```\r\n$ bazel test tensorflow/contrib/tensor_forest:model_ops_test\r\n\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nLoading:\r\nLoading: 0 packages loaded\r\nAnalyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (1 packages loaded, 0 targets configured)\r\nAnalyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (33 packages loaded, 352 targets configured)\r\nAnalyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (58 packages loaded, 2552 targets configured)\r\nAnalyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (82 packages loaded, 4125 targets configured)\r\nAnalyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (83 packages loaded, 4154 targets configured)\r\nERROR: file 'tensorflow/contrib/tensor_forest/libutils.so' is generated by these conflicting actions:\r\nLabel: //tensorflow/contrib/tensor_forest:model_ops_test\r\nRuleClass: cc_test rule\r\nConfiguration: c9dc3b2a2ac77c2e5a3c9175e9c4aa9e\r\nMnemonic: Symlink\r\nAction key: 0ac20b0e60679a64dc9111fddc500670\r\nProgress message: Copying Execution Dynamic Library\r\nPrimaryInput: File:[[<execution_root>]bazel-out/x64_windows-opt/bin]tensorflow/core/grappler/clusters/libutils.so, File:[[<execution_root>]bazel-out/x64_windows-opt/bin]tensorflow/core/grappler/costs/libutils.so\r\nPrimaryOutput: File:[[<execution_root>]bazel-out/x64_windows-opt/bin]tensorflow/contrib/tensor_forest/libutils.so\r\nERROR: Analysis of target '//tensorflow/contrib/tensor_forest:model_ops_test' failed; build aborted: for tensorflow/contrib/tensor_forest/libutils.so, previous action: action 'Copying Execution Dynamic Library', attempted action: action 'Copying Execution Dynamic Library'\r\nINFO: Elapsed time: 10.421s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (84 packages loaded, 4219 targets configured)\r\nERROR: Couldn't start the build. Unable to run tests\r\nFAILED: Build did NOT complete successfully (84 packages loaded, 4219 targets configured)\r\n```", "comments": ["To add on:\r\nI execute py_test suits by \"bazel test\" is successful, but execute tf_cc_test suits will occur above exceptions.\r\nSo i think maybe problems about c compiler. Hope you can help me to fix it. thanks.", "I also find there are files suffix is not .so and instead of .a file :libutils.a, libutils.a-2.params  in the target folder in windows10,  but have correct so files in Ubuntu18 env. so how to change it. Do y think upgrade cuda support version could solve this issue. ", "Did you try following the steps from [TensorFlow website](https://www.tensorflow.org/install/source_windows). Also check [GPU support](https://www.tensorflow.org/install/gpu#hardware_requirements) for CUDA compatibility with TensorFlow version.", "> Did you try following the steps from [TensorFlow website](https://www.tensorflow.org/install/source_windows). Also check [GPU support](https://www.tensorflow.org/install/gpu#hardware_requirements) for CUDA compatibility with TensorFlow version.\r\n\r\nThank you for your reply.\r\nI did the steps follow official guide, following is my develop environment\uff1a\r\n\r\nOS: windows 10\r\nbazel version: 0.24.1\r\ntensorflow version:r1.14\r\npython:3.6.4\r\nCUDA:9.0\r\ncuDNN: 7.4.1\r\n\r\nWhy is the .a file generated instead of the .so file? Do y think that is GPU version not compatible for versions of CUDA and Tensorflow\uff1fI think you can help me to check the version combination.\r\n", "Can you try with higher version of CUDA and let us know if that helps. For your reference TensorFlow supports CUDA 10.0 (TensorFlow >= 1.13.0). Have a look on [GPU support](https://www.tensorflow.org/install/gpu#software_requirements) link, Thanks!", "> Can you try with higher version of CUDA and let us know if that helps. For your reference TensorFlow supports CUDA 10.0 (TensorFlow >= 1.13.0). Have a look on [GPU support](https://www.tensorflow.org/install/gpu#software_requirements) link, Thanks!\r\n\r\nHi achandraa,\r\nI update the CUDA 10.0 and driver to 419.17 , cudnn 7.5, Tensorflow 2.0,  but use under command still encounter the error, i don't know why is libutils.a generated instead of libutils.so file in windows environment(In GitBash):\r\n\r\n```\r\n$ bazel test tensorflow/contrib/tensor_forest:model_ops_test\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nLoading:\r\nLoading: 0 packages loaded\r\nDEBUG: C:/users/.../_bazel_.../vmio52nn/external/build_bazel_rules_apple/apple/repositories.bzl:35:5:\r\nWARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.\r\n\r\nAnalyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (0 packages loaded, 0 targets configured)\r\nERROR: file 'tensorflow/contrib/tensor_forest/libutils.so' is generated by these conflicting actions:\r\nLabel: //tensorflow/contrib/tensor_forest:model_ops_test\r\nRuleClass: cc_test rule\r\nConfiguration: 6f9afee38480077f2e1d7771555e31bc\r\nMnemonic: Symlink\r\nAction key: 87cfd65dad66dcad42fb5f761552ee9c\r\nProgress message: Copying Execution Dynamic Library\r\nPrimaryInput: File:[[<execution_root>]bazel-out/x64_windows-opt/bin]tensorflow/core/grappler/clusters/libutils.so, File:[[<execution_root>]bazel-out/x64_windows-opt/bin]tensorflow/core/grappler/costs/libutils.so\r\nPrimaryOutput: File:[[<execution_root>]bazel-out/x64_windows-opt/bin]tensorflow/contrib/tensor_forest/libutils.so\r\nERROR: Analysis of target '//tensorflow/contrib/tensor_forest:model_ops_test' failed; build aborted: for tensorflow/contrib/tensor_forest/libutils.so, previous action: action 'Copying Execution Dynamic Library', attempted action: action 'Copying Execution Dynamic Library'\r\nINFO: Elapsed time: 0.398s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\nERROR: Couldn't start the build. Unable to run tests\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n```\r\n", "Here is my  configuration of  /etc/profile in MSYS2:\r\n\r\n```\r\nexport BAZEL_SH=\"$(cygpath -m $(realpath $(which bash)))\"\r\nexport JAVA_HOME=\"/c/Program Files/Java/jdk1.8.0_73\"\r\nexport JRE_HOME=$JAVA_HOME/jre\r\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib\r\nexport PATH=$JAVA_HOME/bin:$PATH\r\nexport PATH=\"/c/Python/Anaconda3:$PATH\"\r\n\r\nexport PATH=\"/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin:$PATH\"\r\nexport PATH=\"/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/libnvvp:$PATH\"\r\nexport PATH=\"/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/extras/CUPTI/libx64:$PATH\"\r\nset BAZEL_VS=\"/C/Program Files (x86)/Microsoft Visual Studio 14.0\"\r\nset BAZEL_VC=\"/C/Program Files (x86)/Microsoft Visual Studio 14.0/VC\"\r\n```\r\nI think i am following the operation of  tensorflow websit, but i don't know why i tried same error when i tried more methods, The libutils.a is generated in target folder instead of libutils.so file. Can you check and tell me where maybe occur the error.", "I found if use cc_test instead of tf_cc_test for model_ops_test, then command:\r\nbazel test tensorflow/contrib/tensor_forest:model_ops_test to be success.\r\nso maybe some wrong about tf_cc_test. kindly to check this information.\r\n\r\n```\r\ncc_test(\r\n    name = \"model_ops_test\",\r\n    size = \"small\",\r\n    srcs = [\r\n        \"kernels/model_ops_test.cc\",\r\n        \"ops/model_ops.cc\",\r\n    ],\r\n    deps = [\r\n        \":forest_proto_impl\",\r\n        \":model_ops_lib\",\r\n        \"//tensorflow/contrib/tensor_forest/kernels/v4:decision-tree-resource_impl\",\r\n        \"//tensorflow/core:framework_headers_lib\",\r\n        \"//tensorflow/core:test\",\r\n        \"//tensorflow/core:test_main\",\r\n        \"//tensorflow/core:testlib\",\r\n    ],\r\n)\r\n```\r\n\r\nExecution Result:\r\n```\r\n$bazel test //tensorflow/contrib/tensor_forest:model_ops_test\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nLoading:\r\nLoading: 0 packages loaded\r\nDEBUG: C:/users/l00446796/_bazel_l00446796/vmio52nn/external/build_bazel_rules_apple/apple/repositories.bzl:35:5:\r\nWARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.\r\n\r\nAnalyzing: target //tensorflow/contrib/tensor_forest:model_ops_test (0 packages loaded, 0 targets configured)\r\nINFO: Analysed target //tensorflow/contrib/tensor_forest:model_ops_test (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 test target...\r\n[0 / 2] [-----] BazelWorkspaceStatusAction stable-status.txt\r\nTarget //tensorflow/contrib/tensor_forest:model_ops_test up-to-date:\r\n  C:/users/.../_bazel_.../vmio52nn/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/contrib/tensor_forest/model_ops_test\r\nINFO: Elapsed time: 0.806s, Critical Path: 0.00s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\n//tensorflow/contrib/tensor_forest:model_ops_test               (cached) PASSED in 1.9s\r\n\r\nExecuted 0 out of 1 test: 1 test passes.\r\nINFO: Build completed successfully, 1 total action\r\n```", "You have successfully installed TF however you are running into issues when implementing [TensorForest](https://github.com/tensorflow/tensorflow/tree/r1.14/tensorflow/contrib/tensor_forest). Is that correct?\r\nI suspect that the specified test target is incorrect.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28897, "title": " Undefined symbol: tflite::InterpreterBuilder::~InterpreterBuilder() for tensorflow r1.14", "body": "hello, I meet a problem:\r\nwhen I use tensorflow1 r1.9 to build a IOS app using xcode,  it is OK. But when I use tensorflow r1.14\uff0cwhen building, it show error:\r\n\r\n`:-1: Undefined symbol: tflite::InterpreterBuilder::~InterpreterBuilder()`\r\n\r\nWhat causes it?  How to fix it? \r\nAnyone can give some advises? Thank you very much~\r\n \r\nenv: xcode10.2.1  iphone7  arm64", "comments": ["sorry, I use different version of tensorflow, which cause this problem.  Close issue now."]}, {"number": 28896, "title": "Fix an error in doc string of conv2d_transpose_v2", "body": "It seems the doc string of conv2d_transpose_v2 is copied from conv2d_transpose, but there is a minor error.\r\nThe first parameter of conv2d_transpose_v2 is changed to 'input' from 'value' comparing to conv2d_transpose, that change should also be done in the doc string.", "comments": []}, {"number": 28895, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version:1.3.1\r\n- Python version:3.7.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.5\r\n- GPU model and memory: GeForce RTX 2080 / 24GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI can't see to run any file using tensorflow-gpu\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nAfter writing the code, running the file results in the following logs\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\Projects\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\Projects\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\Projects\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\Projects\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\Projects\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/user/Dropbox/2019 Spring Semester/Advanced Intelligence/TAsession1/TA1-1_student.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\Projects\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\Projects\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\Projects\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\Projects\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\Projects\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\Projects\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\Projects\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\Projects\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["@ekuama The Tensorflow 1.3.1 supports CUDA 8 and cuDNN 6. And The Tensorflow 1.13.1 supports CUDA 10.0 and cuDNN 7.5. Please verify your Tensorflow version and install accordingly. Thanks! ", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28894, "title": "deployment of isolation Forest with TensorFlow serving", "body": "Hi All,\r\n\r\nI have developed anomaly detection model using sklearn Isolation Forest Algorithm, now would like to go for live inference.\r\ncan i use tensorflow service for live inference, if it possible , can you please provide me an example for same.\r\n\r\n", "comments": ["@yarapathineni This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "@yarapathineni Please post your query in the [TF Serving repsoitory link](https://github.com/tensorflow/serving/issues/new), Closing this issue since it is not a bug or feature request"]}, {"number": 28893, "title": "Installation Error", "body": "**System information**\r\n- Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: Alpha 2.0\r\n- Python version: 3.7.3\r\n- Installed using: pip\r\n- GPU model and memory: GTX 1060, 16GB ram\r\n\r\n\r\n\r\nMy import tensorflow statement is not working. Here is the entire stack trace:\r\n\r\n```\r\n(base) D:\\Project>python tf.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Joshua\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Joshua\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Joshua\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Joshua\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Joshua\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tf.py\", line 13, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Joshua\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Joshua\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Joshua\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Joshua\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Joshua\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Joshua\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Joshua\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Joshua\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```", "comments": ["How did you install tensorflow ?", "I solved the issue! I installed tensorflow 2.0, which I understand is currently in alpha. I was running into issues with tensorboard and I accidentally uninstalled the wrong version. So i fixed it by uninstalling everything and installing an older, more stable version of tensorflow. Thanks for the response.", "Good to know the issue is resolved."]}, {"number": 28892, "title": "Possible ODR violations for eigen's scalar_cast_op", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): debian stretch\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v.1.13.1\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.19.0\r\n- GCC/Compiler version (if compiling from source): clang 3.7\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI have the following target:\r\n```python\r\ntf_cc_binary(\r\n    name = \"tf_sample\",\r\n    srcs = [\r\n        \"tf_sample.cc\"\r\n    ],\r\n    deps = [\r\n        \"@org_tensorflow//tensorflow/core:tensorflow\",\r\n    ],\r\n)\r\n```\r\nwith the following contents:\r\n```cpp\r\n#include <stdio.h>\r\n#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/public/session_options.h\"\r\n\r\nint main(int argc, char **argv) {\r\n  ::tensorflow::Session *session;\r\n  ::tensorflow::Status status = NewSession(tensorflow::SessionOptions(), &session);\r\n  ::std::cout << \"Used TF! \" << status.ToString() << ::std::endl;\r\n  return 0;\r\n}\r\n```\r\nand it gives me the following linker error:\r\n```console\r\n$ bazel build //vehicle/vision:tf_sample \r\nINFO: Analysed target //vehicle/vision:tf_sample (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Writing explanation of rebuilds to '/tmp/bazel_explain_phil.log'\r\nINFO: From Linking vehicle/vision/tf_sample:\r\nexternal/amd64_compilers_repo/usr/bin/ld.gold: warning: while linking bazel-out/k8-fastbuild/bin/vehicle/vision/tf_sample: symbol 'Eigen::internal::scalar_cast_op<int, std::complex<float> >::operator()(int const&) const' defined in multiple places (possible ODR violation):\r\n  external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/functors/UnaryFunctors.h:155 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/tile_ops/tile_ops_cpu_impl_1.pic.o\r\n  external/org_tensorflow/tensorflow/core/kernels/cast_op.h:251 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_int32.pic.o\r\nexternal/amd64_compilers_repo/usr/bin/ld.gold: warning: while linking bazel-out/k8-fastbuild/bin/vehicle/vision/tf_sample: symbol 'Eigen::internal::scalar_cast_op<double, std::complex<double> >::operator()(double const&) const' defined in multiple places (possible ODR violation):\r\n  external/eigen_archive/Eigen/src/Core/functors/UnaryFunctors.h:155 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/matrix_square_root_op/matrix_square_root_op.pic.o\r\n  external/org_tensorflow/tensorflow/core/kernels/cast_op.h:251 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_double.pic.o\r\nexternal/amd64_compilers_repo/usr/bin/ld.gold: warning: while linking bazel-out/k8-fastbuild/bin/vehicle/vision/tf_sample: symbol 'Eigen::internal::scalar_cast_op<float, std::complex<float> >::operator()(float const&) const' defined in multiple places (possible ODR violation):\r\n  external/org_tensorflow/tensorflow/core/kernels/cast_op.h:251 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_float.pic.o\r\n  external/eigen_archive/Eigen/src/Core/functors/UnaryFunctors.h:155 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/matrix_square_root_op/matrix_square_root_op.pic.o\r\nexternal/amd64_compilers_repo/usr/bin/ld.gold: warning: while linking bazel-out/k8-fastbuild/bin/vehicle/vision/tf_sample: symbol 'Eigen::internal::scalar_cast_op<std::complex<double>, std::complex<double> >::operator()(std::complex<double> const&) const' defined in multiple places (possible ODR violation):\r\n  external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/functors/UnaryFunctors.h:155 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/save_restore_tensor/save_restore_tensor.pic.o\r\n  external/org_tensorflow/tensorflow/core/kernels/cast_op.h:260 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_complex128.pic.o\r\nexternal/amd64_compilers_repo/usr/bin/ld.gold: warning: while linking bazel-out/k8-fastbuild/bin/vehicle/vision/tf_sample: symbol 'Eigen::internal::scalar_cast_op<tensorflow::bfloat16, float>::operator()(tensorflow::bfloat16 const&) const' defined in multiple places (possible ODR violation):\r\n  external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/functors/UnaryFunctors.h:155 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/resize_bilinear_op/resize_bilinear_op.pic.o\r\n  external/org_tensorflow/tensorflow/core/kernels/cast_op.h:288 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_bfloat.pic.o\r\nexternal/amd64_compilers_repo/usr/bin/ld.gold: warning: while linking bazel-out/k8-fastbuild/bin/vehicle/vision/tf_sample: symbol 'Eigen::internal::scalar_cast_op<int, std::complex<double> >::operator()(int const&) const' defined in multiple places (possible ODR violation):\r\n  external/org_tensorflow/tensorflow/core/kernels/cast_op.h:251 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_int32.pic.o\r\n  external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/functors/UnaryFunctors.h:155 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/tile_ops/tile_ops_cpu_impl_1.pic.o\r\nexternal/amd64_compilers_repo/usr/bin/ld.gold: warning: while linking bazel-out/k8-fastbuild/bin/vehicle/vision/tf_sample: symbol 'Eigen::internal::scalar_cast_op<std::complex<float>, std::complex<float> >::operator()(std::complex<float> const&) const' defined in multiple places (possible ODR violation):\r\n  external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/functors/UnaryFunctors.h:155 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/save_restore_tensor/save_restore_tensor.pic.o\r\n  external/org_tensorflow/tensorflow/core/kernels/cast_op.h:260 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/cast_op/cast_op_impl_complex64.pic.o\r\nTarget //vehicle/vision:tf_sample up-to-date:\r\n  bazel-bin/vehicle/vision/tf_sample\r\nINFO: Elapsed time: 81.008s, Critical Path: 80.69s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 17 processes: 17 linux-sandbox.\r\nINFO: Build completed successfully, 18 total actions\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI already fixed another linker error:\r\n```console\r\nexternal/amd64_compilers_repo/usr/bin/ld.gold: warning: while linking bazel-out/k8-fastbuild/bin/vehicle/vision/tf_sample: symbol 'Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<unsigned short, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op<unsigned short>, Eigen::TensorMap<Eigen::Tensor<unsigned short, 1, 1, long>, 1\r\n6, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, false, false>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<unsigned short, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op<unsigned short>, Eigen::TensorMap<Eigen::Tensor<unsigned short, 1, 1, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&)' defined i\r\nn multiple places (possible ODR violation):\r\n  external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:43 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/data/_objs/optional_ops/optional_ops.pic.o\r\n  external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:227 from bazel-out/k8-fastbuild/bin/external/org_tensorflow/tensorflow/core/kernels/_objs/broadcast_to_op/broadcast_to_op.pic.o\r\n```\r\n\r\nby applying the following patch:\r\n```patch\r\ndiff --git a/third_party/tensorflow/tensorflow/core/kernels/data/optional_ops.cc b/third_party/tensorflow/tensorflow/core/kernels/data/optional_ops.cc\r\nindex a406f74..56f3316 100644\r\n--- a/third_party/tensorflow/tensorflow/core/kernels/data/optional_ops.cc\r\n+++ b/third_party/tensorflow/tensorflow/core/kernels/data/optional_ops.cc\r\n@@ -12,6 +12,9 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n See the License for the specific language governing permissions and\r\n limitations under the License.\r\n ==============================================================================*/\r\n+\r\n+#define EIGEN_USE_THREADS\r\n+\r\n #include \"tensorflow/core/kernels/data/optional_ops.h\"\r\n \r\n #include \"tensorflow/core/common_runtime/dma_helper.h\"\r\n```\r\n\r\nAny thoughts? I'm not very good with partial template specializations.", "comments": ["@philsc Is this still an issue with latest TF versions 1.14, 1.15 (tf-nightly) and 2.0.0b1? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28892\">No</a>\n"]}, {"number": 28891, "title": "tf.contrib.distribute.CollectiveAllReduceStrategy always failed for OS error or socket closed", "body": "When using tf.contrib.distribute.CollectiveAllReduceStrategy to distribute train on 16 workers with 2 GPU each, my job always failed with OS error or socket closed after running for several thousands of steps. I also tried 8 workers and different batch size, the failure always reproduce. And when using parameter server strategy, all my jobs can run successfully and I believe this was not caused by my cluster.\r\n\r\nThank you so much for helping investigating this problem.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n  using estimator and train_and_evaluate\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n  when training\r\n- TensorFlow installed from (source or binary):\r\n   source\r\n- TensorFlow version (use command below):\r\n   tf 1.13\r\n- Python version:\r\n  python 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n   CUDA 10\r\n- GPU model and memory:\r\n  P100, 16G\r\n\r\n\r\n**Describe the current behavior**\r\nWhen using tf.contrib.distribute.CollectiveAllReduceStrategy to distribute train on 16 workers with 2 GPU each, my job always failed with OS error or socket closed after running for several thousands of steps. I also tried 8 workers and different batch size, the failure always reproduce.\r\n\r\n**Describe the expected behavior**\r\nThe job should keep training without failure\r\n\r\n**Code to reproduce the issue**\r\n\r\n**Other info / logs**\r\n\r\n> \r\n\r\n2019-05-19T15:07:45.079Z: [1,13]<stderr>:INFO:tensorflow:loss = 3.7297182, step = 10300 (50.597 sec)\r\n2019-05-19T15:07:45.080Z: [1,14]<stderr>:INFO:tensorflow:loss = 3.7297182, step = 10300 (50.608 sec)\r\n2019-05-19T15:07:45.081Z: [1,14]<stderr>:INFO:tensorflow:global_step/sec: 1.97598\r\n2019-05-19T15:08:13.129Z: [1,4]<stderr>:2019-05-19 15:08:13.128241: E tensorflow/core/common_runtime/ring_reducer.cc:369] Aborting RingReduce with Unavailable: Socket closed\r\n2019-05-19T15:08:13.129Z: [1,4]<stderr>:2019-05-19 15:08:13.128323: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Unavailable: Socket closed\r\n2019-05-19T15:08:14.047Z: [1,1]<stderr>:Segmentation fault (core dumped)\r\n\r\nor os error\r\n\r\n2019-05-20T12:05:35.816Z: [1,12]<stderr>:INFO:tensorflow:loss = 5.7242765, step = 3400 (57.129 sec)\r\n2019-05-20T12:05:35.817Z: [1,14]<stderr>:INFO:tensorflow:loss = 5.7242765, step = 3400 (57.107 sec)\r\n2019-05-20T12:05:41.291Z: [1,7]<stderr>:2019-05-20 12:05:41.289749: E tensorflow/core/common_runtime/ring_reducer.cc:369] Aborting RingReduce with Unavailable: OS Error\r\n2019-05-20T12:05:41.291Z: [1,7]<stderr>:2019-05-20 12:05:41.289831: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Unavailable: OS Error\r\n2019-05-20T12:05:41.774Z: [1,1]<stderr>:Segmentation fault (core dumped)\r\n\r\n", "comments": ["Ping @dubey", "There was a bug in grpc which has been fixed recently. Could you try a newer version of TF?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28891\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28891\">No</a>\n", "@yuefengz One question for your answer 'There was a bug in grpc which has been fixed recently. Could you try a newer version of TF?': Does TF 1.14 contains this bug fix?"]}, {"number": 28890, "title": "XLA_GPU_JIT Slows Down GNMT (when large clusters are formed)", "body": "**System information**\r\n- Have I written custom code: Use models from NVIDIA [OpenSeq2Seq](https://github.com/NVIDIA/OpenSeq2Seq)\r\n- OS Platform and Distribution:  Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): upstream-base-779-g83909d2 1.13.1\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 5.4.0 20160609\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: NVIDIA GV100\r\n\r\n**Code to reproduce the issue**\r\nNote that let's assume a context where the deadness analysis is disabled for forming large clusters. In such scenarios, XLA_GPU_JIT slows down this reported GNMT implementation by around 30%.\r\n\r\nReproduction steps:\r\n\r\n1. Download the training dataset from the following link. Assuming putting it under /wmt16/\r\nhttps://drive.google.com/open?id=1ooQiWhmzmYsk2qMOfaunjTlx_z6lcUyO\r\n2. `git clone https://github.com/NVIDIA/OpenSeq2Seq.git`\r\n3. `cd OpenSeq2Seq`\r\n4. `git apply gnmt_config.txt` ([file here](https://github.com/tensorflow/tensorflow/files/3200298/gnmt_config.txt)). Assuming the dataset is placed under /wmt16/.\r\n5. Run command:\r\n`TF_XLA_FLAGS=\"--tf_xla_disable_deadness_safety_checks_for_debugging=true \" CUDA_VISIBLE_DEVICES=0 python run.py --config_file=example_configs/text2text/en-de/en-de-gnmt-like-4GPUs.py --benchmark --bench_start 50 --bench_steps 100 --use_xla_jit`\r\n6. Remove `--use_xla_jit` to run with native Tensorflow.\r\n\r\n**Observed slowdown**\r\nThe performance measured on my GV100 (1 GPU):\r\nWith XLA_JIT:\r\n    Avg time per step: 1.000s\r\nWith native Tensorflow:\r\n    Avg time per step: 0.760s\r\n\r\nThis is around 30% slowdown, comparing XLA_JIT to native TF.\r\n\r\n**A Theory of why such a slowdown**\r\nMy theory of why XLA slows down is as follows. Ideally, the TF runtime (host) should be able to run ahead of the device, so that the host overhead can be hidden. To achieve that, the TF runtime usually executes the dataflow ops in parallel on multiple CPUs to feed computation to one GPU stream. For example, when ops related to an LSTM cell are executed, the tf.while ops (e.g., Merge/Switch, etc.) could be executed in parallel, so that the loop related latencies are hidden. This, however, is not the case observed in XLA_JIT on the forward path of the GNMT.\r\n\r\nA possible explanation of why the host does not run ahead is a result of interaction between host-device synchronization needed by tf.while and long-latency _XlaRun ops. The tf.while ops involve synchronization between host and device as they need to copy some computation results back to the host to make decisions about when to exit the loop. For example, a typical op sequence of tf.while is Add, Less, LoopCond, and Switch; the result of Add is on device but TF computes Less, LoopCond, and Switch on host. These synchronization latencies are better hidden in pure TF executions, as there are more parallel ops and each op is of lower latency. In contrast, scheduling a long-latency op such as _XlaRun along with the op sequence of tf.while on the same GPU stream can introduce extra dependency (established through the runtime scheduling order on the single GPU stream) and force the host to wait for the completion of this long-latency op even if they have no dependencies in the dataflow graph. Often, this is the case observed.\r\n\r\n**Other info**\r\nFurther reference for script options:\r\nhttps://nvidia.github.io/OpenSeq2Seq/html/machine-translation.html\r\n\r\n\r\n", "comments": ["@sanjoy @jlebar ", "Trent, thank you for filing this bug with so many details.\r\n\r\nDo you all at nvidia have cycles to dig into this?  If so we can provide suggestions for how to check your hypothesis of the etiology and how to fix it.", "Yes, nvidia is happy to help with the investigation and a fix.", "Following up the post.\r\n\r\n**Experiments**\r\nI did some experiments to confirm the issues. I noticed some nodes of the i++ cycles are clustered into XLA clusters, and so some _hack_ is done to decluster them. After that, the execution time improves as shown below:\r\n\r\nAvg time per step: 0.814s\r\n\r\nThen I noticed that a cuStreamSync is always invoked at the end of GpuExecutable::Execute(), which is not necessary in the case of single compute stream. The behavior was introduced recently by Commit c7b255ae3505ea009 on May 4 by @hawkinsp. I disabled the change for experiments. The measured execution time is shown below.\r\n\r\nAvg time per step: 0.676s\r\n\r\nSo, combining the two changes, the execution time improves from 1 sec to 0.676 sec (**+48%**) and it is **+12%** on top of native TF.\r\n\r\n\r\n**Update on Etiology**\r\nTo describe the reported issue more precisely, it is because some nodes in the i++ cycle are clustered into the XLA clusters. It introduces extra dependencies and blocks the progression of the i++ cycles.\r\n\r\nNote that saying the result of the Add node is on device in the main description of this post is not accurate. The result of the Add node is on host. However, the i++ cycles also contain LoopAnd, and one of the i++ cycles even contains (reduction) All and LogicalNot. So, the host-device synchronization remains.\r\n\r\nPlease feel free to let me know if you'd like to see any further information? If it all sounds good, I will try to leverage the tf2xla declustering pass to decluster the nodes related to the i++ cycles. @sanjoy\r\n", "> Following up the post.\r\n> \r\n> **Experiments**\r\n> I did some experiments to confirm the issues. I noticed some nodes of the i++ cycles are clustered into XLA clusters, and so some _hack_ is done to decluster them. After that, the execution time improves as shown below:\r\n> \r\n> Avg time per step: 0.814s\r\n> \r\n> Then I noticed that a cuStreamSync is always invoked at the end of GpuExecutable::Execute(), which is not necessary in the case of single compute stream. The behavior was introduced recently by Commit [c7b255a](https://github.com/tensorflow/tensorflow/commit/c7b255ae3505ea009c291b738ef22fbe943be6f1) on May 4 by @hawkinsp. I disabled the change for experiments. The measured execution time is shown below.\r\n\r\nThat seems wrong -- we should be calling the `ExecuteOnStreamAsync` from `XlaRunOp`.  I'll take a look to see what I can do.\r\n\r\n> Avg time per step: 0.676s\r\n> \r\n> So, combining the two changes, the execution time improves from 1 sec to 0.676 sec (**+48%**) and it is **+12%** on top of native TF.\r\n> \r\n> **Update on Etiology**\r\n> To describe the reported issue more precisely, it is because some nodes in the i++ cycle are clustered into the XLA clusters. It introduces extra dependencies and blocks the progression of the i++ cycles.\r\n> \r\n> Note that saying the result of the Add node is on device in the main description of this post is not accurate. The result of the Add node is on host. However, the i++ cycles also contain LoopAnd, and one of the i++ cycles even contains (reduction) All and LogicalNot. So, the host-device synchronization remains.\r\n> \r\n> Please feel free to let me know if you'd like to see any further information? If it all sounds good, I will try to leverage the tf2xla declustering pass to decluster the nodes related to the i++ cycles. @sanjoy\r\n\r\nOk, great.\r\n\r\nBtw, I added some auto-clustering \"integration\" tests to: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/jit/tests  You should be able to add an integration test with GNMT in that directory.", "\r\nThanks for the instant reply.\r\n\r\n> Btw, I added some auto-clustering \"integration\" tests to: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/jit/tests You should be able to add an integration test with GNMT in that directory.\r\n\r\nThanks for this. The real pbtxt from the GNMT is 35MB. I guess we don't want to commit it. I will see if I can find a smaller yet representative model.\r\n", "> Thanks for this. The real pbtxt from the GNMT is 35MB. I guess we don't want to commit it. I will see if I can find a smaller yet representative model.\r\n\r\nI think the file size limit is 2MB.  I'd be fine if you wanted to check in the same large model in a more compact representation (gziped binary protobuf perhaps?) if that makes it fit.  Of course you'll then have to change the test harness to decompress and parse the binary proto instead of pbtxt.", "\r\nYa, it is quite small after gzip-ed. I will commit it as an integration test along with the future change to the declustering.\r\n\r\nBTW, I attach the dump from mark_for_compilation for your reference. Guess you may be interested in taking a look of the graph.\r\n[mark_for_compilation_annotated_4.pbtxt.gz](https://github.com/tensorflow/tensorflow/files/3238673/mark_for_compilation_annotated_4.pbtxt.gz)", "> I will commit it as an integration test along with the future change to the declustering.\r\n\r\nUp to you, but you should feel free to commit it before so the effect of the declustering change is clearer (e.g. see https://github.com/tensorflow/tensorflow/commit/004f4645f6d5ab71a24319dbfe727ebb70d47767#diff-578512bc12b9e17a62969910f46211d7 )", "> > I will commit it as an integration test along with the future change to the declustering.\r\n> \r\n> Up to you, but you should feel free to commit it before so the effect of the declustering change is clearer (e.g. see [004f464#diff-578512bc12b9e17a62969910f46211d7](https://github.com/tensorflow/tensorflow/commit/004f4645f6d5ab71a24319dbfe727ebb70d47767#diff-578512bc12b9e17a62969910f46211d7) )\r\n\r\nSee your point. Will commit the test before the declustering change.\r\n", "@trentlo I see that the associated PR is merged. Can we close this issue if it's resolved?", "\r\nThanks @ymodak for the follow-up.\r\n\r\n#29470 addresses only half the issue.\r\n\r\nTo close this issue (and see performance), we need another patch to remove the cuStreamSync at the end of every ExecuteOnStream. Softly ping @hawkinsp as I heard he may already have some codes dealing with this issue.\r\n", "CC @akuegel.  I can't find Alexander on Github -- Adrian do you know his handle if he has one?", "So in the end I decided to submit the patch that reverts it to the previous state, see 5a142145f86377ed1ffc1ac19d1d6d6853f8f355\r\nWe still don't understand why asynchronous execution seems to be worse in some cases.\r\nUnfortunately neither me nor Alex @pifon2a (who looked at this) know this part of the code.\r\n", "> @pifon2a\r\n\r\nThanks for taking care of this. It is indeed strange that switching from async to sync can see performance degradation.\r\n\r\nIf you can find a way to give me a reproducer, I am happy to profile the case to see if I am able to understand the cause.\r\n ", "To clarify the issue: Does your patch see the performance degradation? Or the degradation is only  seen when switching to use ExecuteAsyncOnStream?", "Closing this issue. Thanks everyone for taking care of this.\r\n\r\n@pifon2a: let me know if you'd like me to help with the performance degradation. We can track it in another issue if needed though.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28890\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28890\">No</a>\n"]}, {"number": 28889, "title": "3 cherrypicks for 1.14", "body": "", "comments": []}, {"number": 28888, "title": "r1.14 CUDA 10.1 build still broken and needs find_cuda_config.py from master", "body": "r1.14 still needs last 1-liner fix to find_cuda_config.py from master\r\n", "comments": []}, {"number": 28887, "title": "Cherrypicks: error rewriting and fix for deprecation warning", "body": "", "comments": []}, {"number": 28886, "title": "Allow importing of V1 SavedModels that output SparseTensors", "body": "If one attempts to load a V1 SavedModel that outputs a `SparseTensor`, the import code fails with an internal error. This PR adds the necessary hooks to the import code to allow it to load models with `SparseTensor` outputs.\r\n\r\nThe changes are centered in `WrappedFunction.prune()`. I also added a regression test in `load_v1_in_v2_test.py`.", "comments": []}, {"number": 28885, "title": " Pull 1.14 up to a071e8434eb695e61e4f70e8c001e775387c5e2a", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28885) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28885) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 28884, "title": "Pull 1.14 up to a071e8434eb695e61e4f70e8c001e775387c5e2a", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28884) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28884) for more info**.\n\n<!-- cla_yes -->", "Somehow the 1.14 branch does not have a commit it's supposed to have:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/257e37a8c4856ae327768060f5e73ececd19c036\r\n\r\nr1.14 doesn't have it: https://github.com/tensorflow/tensorflow/commits/r1.14/tensorflow/stream_executor/host/host_gpu_executor.cc\r\n\r\nr1.14-pull-2 (which I used git merge to build) doesn't have it:\r\nhttps://github.com/tensorflow/tensorflow/commits/r1.14-pull-2/tensorflow/stream_executor/host/host_gpu_executor.cc\r\n\r\nMaybe it's worth doing a safety check to compare differences between r1.14 and master up to a071e8434eb695e61e4f70e8c001e775387c5e2a\r\n\r\nIn the meantime I manually merged that commit and created another PR (Github is giving me a hard time with pushing to a branch if it isn't new - something about protected branches...):\r\nhttps://github.com/tensorflow/tensorflow/pull/28885"]}, {"number": 28883, "title": "sync master", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28883) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 28882, "title": "added an example for math.argmax", "body": "added an example for math.argmax", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28882) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 28881, "title": "added an example for math.argmax", "body": "added an example for math.argmax", "comments": ["closing this PR as the changes are meant to Master"]}, {"number": 28880, "title": "Pull 1.14 up to a071e8434eb695e61e4f70e8c001e775387c5e2a", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28880) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28880) for more info**.\n\n<!-- cla_yes -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28880) for more info**.\n\n<!-- cla_yes -->", "Can you make sure to include 0b0bc908dffed711e59c0676d6dcbf8c90f721a0?", "Replaced by https://github.com/tensorflow/tensorflow/pull/28884 since I don't know how to push to a protected branch."]}, {"number": 28879, "title": "Fix two linter warnings", "body": "I've been seeing the following linter warnings in my test builds for a few days now:\r\n```\r\nFAIL: Found 2 non-whitelited pylint errors:\r\ntensorflow/python/platform/googletest.py:28: [W0622(redefined-builtin), ] Redefining built-in 'xrange'\r\n\r\ntensorflow/examples/saved_model/integration_tests/integration_scripts.py:64: [E1111(assignment-from-no-return), MaybeRunScriptInstead] Assigning to function call which doesn't return\r\n```\r\n\r\nThis PR fixes the two issues that cause these warnings.", "comments": ["I wonder if `sys.exit` in `integration_scripts.py` is still needed (may be with a constant exit code).\r\n@andresusanopinto Andre, can you take a look?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Replaced the fix for the second warning with a hopefully less-controversial version.\r\n\r\nBy the way, these linter errors are still showing up on the master branch. To replicate the problem, run:\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ntensorflow/tools/ci_build/ci_build.sh CPU tensorflow/tools/ci_build/ci_sanity.sh\r\n```\r\nYou'll see the following output:\r\n```\r\n[...]\r\n=== Sanity check step 2 of 14: do_pylint PYTHON2 (Python 2 pylint) ===\r\n[...]\r\nFAIL: Found 2 non-whitelited pylint errors:\r\ntensorflow/examples/saved_model/integration_tests/integration_scripts.py:64: [E1111(assignment-from-no-return), MaybeRunScriptInstead] Assigning to function call which doesn't return\r\n\r\ntensorflow/python/platform/googletest.py:28: [W0622(redefined-builtin), ] Redefining built-in 'xrange'\r\n```\r\n\r\n", "Any idea what is causing those two failed checks (Windows Bazel GPU and import/copybara)? I don't have any visibility into the build results."]}, {"number": 28878, "title": "[TF-TRT] Avoid padding calculations when not needed", "body": "PR #27988 added the following:\r\n* In TRT >= 5.1.3, use `setPaddingMode` so TRT will automatically calculate padding.\r\n* In TRT >= 5.1.0, use `setPostPadding/setPostPadding` to perform asymmetric padding without an additional `IPaddingLayer` (there was a bug using this for convolutions which was fixed in 5.1.3).\r\n\r\nFor TRT < 5.1.3, we have to compute padding values manually based on the input dimensions. This PR makes it so these calculations are no longer performed for TRT >= 5.1.3 where they are not used.\r\nThis PR also simplifies some of the logic determining how to do the padding based on TRT version and adds comments explaining why.", "comments": []}, {"number": 28877, "title": "Opening bug #27833 again, since not able to compile tensorflow on debian 9", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): debian 9\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.0, 1.14\r\n- Python version: 3.6.5 (pyenv)\r\n- Installed using virtualenv? pip? conda?: virtualenv (python -m venv)\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source):  gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n\r\n**Describe the problem**\r\n\r\nNot able to compile tensorflow 2.0, 1.12, ... with bazel on debian system.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nactivate a virtualenv with:\r\nKeras-Applications==1.0.6\r\nmock==3.0.5\r\nnumpy==1.16.3\r\nsix==1.12.0\r\n\r\ngit co r1.14\r\n./configure\r\nbazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --config=mkl -k //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n```\r\ntensorflow/core/kernels/BUILD:4615:1: Couldn't build file tensorflow/core/kernels/_objs/sparse_reduce_op/sparse_reduce_op.pic.o: C++ compilation of rule '//tensorflow/core/kernels:sparse_reduce_op' failed (Exit 1): gcc failed: error executing command\r\n  (cd /home/f43995/.cache/bazel/_bazel_f43995/3010ddbda2b99362ce3481040aa5c4dc/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/opt/py_envs/tfopt/bin:/home/f43995/.pyenv/shims:/home/f43995/.pyenv/bin:/home/f43995/.pyenv/shims:/home/f43995/.pyenv/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/f43995/.dotnet/tools:/opt/mssql-tools/bin:/opt/mssql-tools/bin:/home/f43995/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/tensorflow/core/kernels/_objs/sparse_reduce_op/sparse_reduce_op.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/kernels/_objs/sparse_reduce_op/sparse_reduce_op.pic.o' -fPIC -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -isystem third_party/eigen3/mkl_include -isystem bazel-out/host/genfiles/third_party/eigen3/mkl_include -isystem bazel-out/host/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -g0 -g0 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DINTEL_MKL=1' -DEIGEN_USE_VML -DENABLE_MKL -fopenmp -msse3 -pthread '-DINTEL_MKL=1' -DENABLE_MKL -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/kernels/sparse_reduce_op.cc -o bazel-out/host/bin/tensorflow/core/kernels/_objs/sparse_reduce_op/sparse_reduce_op.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n```\r\n\r\n", "comments": ["@tjhgit Please refer the [link](https://github.com/tensorflow/tensorflow/issues/27575). Can you please follow the steps [install TF from sources](https://www.tensorflow.org/install/source#setup_for_linux_and_macos) ", "I followed the instructions closely and it did always run into the above error. I tried different versions of bazel and different versions of tensorflow and also to compile with jobs=2, fearing the errors were due to a lack of memory.\r\n\r\nCan you tell me a few commands on how to debug the compile process? I am completely tapping in the dark right now ...", "Only compilation without optimizations works, like for instance: `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`", "@tjhgit Please downgrade your gcc version to 4.8 and bazel to 0.15.0", "@tjhgit Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28876, "title": "Make vscode's pylint integration work on TensorFlow source", "body": "If you point a copy of Visual Studio Code at a TensorFlow source tree, the editor's built-in linter displays a large number of spurious errors and misses errors that would show up in CI builds. This PR adds a symbolic link to the `pylintrc` file for CI builds at the root of the source code tree. After this change, the default linter output of Visual Studio Code is much closer to what one sees when running `tensorflow/tools/ci_build/ci_sanity.sh`.", "comments": ["Can one of the admins verify this patch?", "Is there another way to specify to vscode which file to use for linting? I'd like to avoid adding a file to the root directory if possible. I'd expect vscode to have a configuration option somewhere.", "Per the VS Code [documentation](https://code.visualstudio.com/docs/python/linting#_commandline-arguments-and-configuration-files), you can configure the editor's pylint integration in two ways:\r\n* Set the `python.linting.pylintArgs` parameter in VS Code's `settings.json` file\r\n* Put a file called either `.pylintrc` or `pylintrc` in the root directory of the workspace\r\n\r\nI'm not aware of any other undocumented ways of configuring the linter.", "@aaudiber Can you please take a look on this PR? Thanks!", "Thanks @frreiss! This PR will need a manual import. Let me do that.", "Thanks @yifeif!"]}, {"number": 28875, "title": "Fix bazel dependencies for cublas 10.1", "body": "Attention @chsigg\r\n\r\nThese three dependency tweaks are needed to build against CUDA 10.1.", "comments": ["Thanks Nathan. Unfortunately this PR cannot be imported into our internal repository. I will need to fix this from our side."]}, {"number": 28874, "title": " Enable DatasetOpsTestBase to execute a function and tests for OptimizeDatasetOp", "body": "This PR includes:\r\n1) https://github.com/tensorflow/tensorflow/commit/0ef640bc893b842d6047e7116ace164131d4b565: add `RunFunction()` for `DatasetOpsTestBase` to exectue a function;\r\n\r\n2)  https://github.com/tensorflow/tensorflow/commit/2e7e3122653e62d8221d9c5fe264daf729e56512: add functions to create RangeDataset and FilterDataset. With `RunFunction()`, it will be easy to create the chained datasets for `OptimizeDataset`.\r\n\r\n3) https://github.com/tensorflow/tensorflow/commit/2f8fba157c50737590114b5381548a54ba2fe3f9: tests for `OptimizeDatasetOp`.\r\n\r\ncc: @jsimsa ", "comments": ["@jsimsa The internal checks failed. Could you please paste the log details here?", "```\r\nthird_party/tensorflow/core/kernels/data/dataset_test_base.cc:307:36: error: definition of 'VersionDef' must be imported from module '//third_party/tensorflow/core:protos_all_proto.ph.blaze-out/k8-py3-opt/genfiles/third_party/tensorflow/core/framework/versions.proto.h' before it is required\r\n  const int version = g->versions().producer();\r\n```", "@jsimsa The missing import for versions.pb.h is added (https://github.com/tensorflow/tensorflow/pull/28874/commits/c563485bb8bc71db7b8e5262af34a099d7c66fc4). Could you have a look at the change? Do you have any suggestions about how to check whether some imports/dependencies are missing?", "@jsimsa Sorry that the internal checks failed again. Could you help check and paste the logs here?", "the test failures do not seem to be related and the internal test is being re-ran", "@jsimsa This PR is rebased to solve the code conflicts. Could you have another look when you get a chance? Thanks!", "@jsimsa This PR has been pending for a while. Are there anything else I need to do?", "The internal copy of the CL seems to have failed with: \r\n\r\n```\r\nthird_party/tensorflow/core/kernels/data/optimize_dataset_op_test.cc:65:3: error: control may reach end of non-void function [-Werror,-Wreturn-type]\r\n  }\r\n```", "@jsimsa The failure seems to be caused by the missing return status. This commit (https://github.com/tensorflow/tensorflow/pull/28874/commits/443fba8256200502c5b2b21a03688a7908001a79) is submitted to fix it. It also makes TensorValue constructor explicit to follow the recent [changes](https://github.com/tensorflow/tensorflow/commit/ef0b1eff8d50cc0f58439adb6af979e9b39a0851#diff-4c807afc2044338c27e3f0dbf833ae4a). Please have a look at the change.", "@jsimsa The internal checks failed. Could you help paste the failure logs here? Thanks!", "The internal checks look good to me.", "Thanks for checking @jsimsa! It was in the FAIL status when I checked it. Now it changes to the PASS status."]}, {"number": 28873, "title": "[ROCm] Adding ROCm support for tile ops", "body": "This PR adds ROCm support for tile ops\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n--------------------------\r\n\r\n@tatianashp , @whchung", "comments": ["@chsigg can you please approve .thank you "]}]