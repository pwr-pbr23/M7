[{"number": 41817, "title": "TPU Error: Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: Socket closed", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nhttps://www.kaggle.com/ratthachat/flickr-image-captioning-tpu-tf2-glove\r\n\r\n- TensorFlow installed from (source or binary): installed source\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nI was running the provided code from Kaggle on a Google cloud TPU.\r\nHardware: v3-8\r\n\r\nIt was training fine till when I had 500000 Images. but the moment I increase the amount of data to 1 million. at almost the end of 1st epoch, I am getting the following error. I am new to TPUs so any help will be appreciated.\r\n\r\nTraceback (most recent call last):\r\n  File \"flickr-image-captioning-tpu-tf2-glove.py\", line 178, in <module>\r\n    for (batch, inputs) in tqdm_notebook(enumerate(train_dist_dataset)): # by .repeat() this will indefinitely run\r\n  File \"/usr/local/lib/python3.7/dist-packages/tqdm/notebook.py\", line 218, in __iter__\r\n    for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tqdm/std.py\", line 1129, in __iter__\r\n    for obj in iterable:\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 296, in __next__\r\n    return self.get_next()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 328, in get_next\r\n    global_has_value, replicas = _get_next_as_optional(self, self._strategy)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 192, in _get_next_as_optional\r\n    iterator._iterators[i].get_next_as_list(new_name))  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 1150, in get_next_as_list\r\n    strict=True,\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1204, in cond\r\n    if pred:\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 884, in __bool__\r\n    return bool(self._numpy())\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 929, in _numpy\r\n    six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnavailableError: Socket closed\r\nAdditional GRPC error information:\r\n{\"created\":\"@1595426699.706157313\",\"description\":\"Error received from peer ipv4:10.255.26.210:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\r\n2020-07-22 14:05:01.237755: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: Socket closed\r\nAdditional GRPC error information:\r\n{\"created\":\"@1595426699.706157313\",\"description\":\"Error received from peer ipv4:10.255.26.210:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@caffeine-lab \r\nCold you please share stand alone code with dependencies to replicate this issue or if possible share a colab gist with the error faced.\r\n\r\nWith respect to the error faced please find similar issues below:\r\n [link](https://www.kaggle.com/dimitreoliveira/bug-report-unavailableerror-socket-closed) [link1](https://github.com/tensorflow/tensorflow/issues/36136#issuecomment-627731715) ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41817\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41817\">No</a>\n"]}, {"number": 41816, "title": "TFLM 2.3 fails assert in CMSIS-NN fully_connected.cc", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Verified on Mbed OS 5.13\r\n- TensorFlow installed from (source or binary): Source\r\n- Tensorflow version (commit SHA if source): 0c43ad89f81b22d81d1894f5b53f9fbdda0b738a\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Mbed OS (ST IoT Discovery Kit)\r\n\r\n**Describe the problem**\r\n\r\nWe have a very simple fully connected network (33x20x10x4 neurons) that fails an assert when being invoked with CMSIS-NN kernels here (in **fully_connected.cc**):\r\n\r\n```\r\n    cmsis_nn_dims input_dims;\r\n    input_dims.n = batches;\r\n    input_dims.h = input_shape.Dims(1);\r\n    input_dims.w = input_shape.Dims(2); // <-- here\r\n```\r\n\r\nHere `input_shape` is the input layer (33 neurons) which does not have this dimension (`size_` is 2). Setting `input_dims.w` and `input_dims.c` to `1` solves the issue.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nSee above. Attached the tflite model.\r\n[ei-continuous-gestures-nn-classifier-tensorflow-lite-int8-quantized-model.lite.zip](https://github.com/tensorflow/tensorflow/files/4987903/ei-continuous-gestures-nn-classifier-tensorflow-lite-int8-quantized-model.lite.zip)\r\n\r\n\r\n", "comments": ["Hello @janjongboom,\r\nCould you have a look at my PR (#41285) ? I think it can solve this issue.\r\n\r\nRegards,\r\nBiagio.", "It looks like this is related to the linked PR (and should be fixed by it). Thanks for pointing out this issue. Let's verify that once the PR lands this is resolved.", "@dansitu ^", "PR https://github.com/tensorflow/tensorflow/pull/42314 provides a fix for this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41816\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41816\">No</a>\n"]}, {"number": 41815, "title": "SavedModel exporting fails on RNNs by setting wrong input dtypes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen trying to export concrete functions in a Keras model to the SavedModel format, if a function uses `tf.keras.layers.RNN`, the dtypes of the RNN cell input arguments are wrong. In particular, actual dtypes seem to be ignored and replaced with tf.float32. This does not happen when calling the same concrete function manually, but only when trying to export it.\r\n\r\n**Describe the expected behavior**\r\nThe input dtypes should be preserved, allowing to export the model. At the very least the behavior should be consistent with calling the concrete function.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass TestRNNCell(tf.keras.layers.Layer):\r\n  def __init__(self):\r\n    super().__init__()\r\n    self.units = 10\r\n    self.state_size = 20\r\n\r\n  def call(self, indices, state):\r\n    # This assertion fails.\r\n    assert indices.dtype == tf.int32\r\n    # If the assertion is removed, this line fails with:\r\n    # TypeError: Value passed to parameter 'indices' has DataType float32 not in list of allowed values: int32, int64\r\n    output = tf.gather(tf.range(5), indices)\r\n    return output, state\r\n\r\nclass TestModel(tf.keras.Model):\r\n  def __init__(self):\r\n    super().__init__()\r\n    self.rnn = tf.keras.layers.RNN(TestRNNCell())\r\n\r\n  @tf.function\r\n  def do_stuff(self, indices):\r\n    assert indices.dtype == tf.int32\r\n    return self.rnn(indices)\r\n\r\nmodel = TestModel()\r\ntf.saved_model.save(model, 'test_model', signatures={\r\n  'do_stuff': model.do_stuff.get_concrete_function(\r\n      indices=tf.TensorSpec([None, None, 5], tf.int32))\r\n})\r\n```\r\n\r\nUsing `model.save` instead of `tf.saved_model.save` does not help either. Here's a version that fails just in the same way.\r\n```python\r\nclass TestModel(tf.keras.Model):\r\n  # [Continuing the class above]\r\n\r\n  def call(self, indices):\r\n    assert indices.dtype == tf.int32\r\n    return self.rnn(indices)\r\n\r\nmodel = TestModel()\r\n\r\n# This works fine.\r\nmodel(tf.zeros([10, 10, 5], tf.int32))\r\n\r\n# This fails because the RNN inputs are now tf.float32.\r\nmodel.save('test_model', save_format='tf')\r\n```\r\n\r\nAny possible workaround for this would be much appreciated. I cannot figure out any since this problem only affects SavedModel exporting and not the actual concrete functions being exported.", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/2f6efc1ad81e0c6f239d0f9f0fd29805/41815-2-3.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/c2fd2680adb7cd75de9e8e455b1b4988/41815-tf-nightly.ipynb). Please find the attached gist. Thanks!", "The error is a bit hard to follow, but you won't get the error if you set the dtype of the RNN layer:\r\n\r\n```\r\n    self.rnn = tf.keras.layers.RNN(TestRNNCell(),dtype=tf.int32)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41815\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41815\">No</a>\n"]}, {"number": 41814, "title": "Is there any document for running a model which is built by tf.estimator by tf-serving ?", "body": "The same [stackoverflow](https://stackoverflow.com/questions/63069621/tensorflow-serving-can-run-a-model-built-by-tensorflow-estimator)", "comments": ["@DachuanZhao \r\nPlease refer to [this link](https://www.tensorflow.org/tfx/guide/serving) and confirm.", "> @DachuanZhao\r\n> Please refer to [this link](https://www.tensorflow.org/tfx/guide/serving) and confirm.\r\n\r\nThere isn't an example , either .", "> @DachuanZhao\r\n> Please refer to [this link](https://www.tensorflow.org/tfx/guide/serving) and confirm.\r\n\r\nI can find an example which shows how to running a model which is built by tf.keras by  tf-serving  at this [link](https://www.tensorflow.org/tfx/serving/serving_basic) , but I can't find a tf.estimator example .", "@DachuanZhao Please take a look at this [thread](https://branyang.gitbooks.io/tfdocs/content/get_started/export.html)\r\nAlso see section `Serving the exported model locally` in the above thread.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41813, "title": "Are break statements in tf.function supported?", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/distribute/input\r\n\r\n## Description of issue (what needs changing):\r\n\r\nhttps://www.tensorflow.org/tutorials/distribute/input#usages states\r\n> However break and return are currently not supported if the loop is placed inside a tf.function\r\n\r\nBut later the example at https://www.tensorflow.org/tutorials/distribute/input#use_iter_to_create_an_explicit_iterator shows:\r\n```\r\n@tf.function\r\ndef train_fn(distributed_iterator):\r\n  for _ in tf.range(steps_per_loop):\r\n    optional_data = distributed_iterator.get_next_as_optional()\r\n    if not optional_data.has_value():\r\n      break\r\n    per_replica_results = strategy.run(lambda x:x, args=(optional_data.get_value(),))\r\n    tf.print(strategy.experimental_local_results(per_replica_results))\r\n```\r\n\r\nThis shows the usage of a `break` statement in a `tf.function`.\r\n\r\nSo are those statements supported? Are there conditions under which they are supported?", "comments": ["Hi @Flamefire, if you run the following:\r\n\r\n```\r\nglobal_batch_size = 16\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\n\r\ndataset = tf.data.Dataset.from_tensors(([1.],[1.])).repeat(100).batch(global_batch_size)\r\ndist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\r\n\r\n@tf.function\r\ndef train_step(inputs):\r\n  features, labels = inputs\r\n  return labels - 0.3 * features\r\n\r\n@tf.function\r\ndef loop(dist_dataset):\r\n  for x in dist_dataset:\r\n    # train_step trains the model using the dataset elements\r\n    loss = mirrored_strategy.run(train_step, args=(x,))\r\n    print(\"Loss is \", loss)\r\n    break\r\n\r\nloop(dist_dataset)\r\n```\r\n\r\nYou'll see the error message,  `NotImplementedError: break and return statements are not yet supported in for ... in distributed input loops` (important thing to note here is that it's specifically not implemented for distributed input loops)\r\n\r\nI think the difference in the `train_fn(distributed_iterator)` example that you've pointed out from the docs is that the for loop is looping over a tf.range, and not a distributed dataset, so a break statement is allowed. I agree it's a bit confusing, but seems to be the break statements are supported in @tf.function, but not when your looping over distributed input", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I see, so I can't do `for x in distributed_dataset: ... break` but I can do `for something_else: x = next(distributed_iterator) ... break`\r\n\r\nThat's confusing, maybe the docs can be made more clear to point this out?", "Agreed, it is a bit confusing. Any suggestions on how to make the docs clearer here?", "Maybe:\r\n> However break and return are currently not supported when looping over a distributed dataset inside a tf.function. You can however create an iterator and call `next` on it inside a loop over some other collection(? is this the right term for python?) like `range`", "Change has been made to the docs\r\nhttps://www.tensorflow.org/tutorials/distribute/input#usages\r\n\r\n`Placing the loop inside a tf.function will give a performance boost. However, break and return are currently not supported for a loop over a tf.distribute.DistributedDataset that is placed inside of a tf.function.`"]}, {"number": 41812, "title": "[Doc] Are partial batches supported with MultiWorkerMirroredStrategy", "body": "## URL(s) with the issue:\r\n\r\n- https://github.com/tensorflow/tensorflow/releases/tag/v2.3.0\r\n- https://www.tensorflow.org/tutorials/distribute/input#partial_batches\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe release note states \r\n> tf.distribute.experimental.MultiWorkerMirroredStrategy adds support for partial batches.\r\n\r\nHowever the documentation/tutorial states\r\n> Currently this is supported for all strategies except tf.distribute.experimental.MultiWorkerMirroredStrategy.\r\n> [...] Partial batches are supported for all strategies except tf.distribute.experimental.MultiWorkerMirroredStrategy.\r\n\r\nThis sounds like those contradict each other. What is the actual reality? Can the documentation or the release notes be updated to clarify?", "comments": ["Hi @Flamefire, thanks for pointing this out. Partial batches are now supported with MWMS. A documentation change has just been submitted and I will update this thread when it goes through. ", "Closing this issue now because [documentation](https://www.tensorflow.org/tutorials/distribute/input#partial_batches) has been updated to be consistent with the release notes. Thanks again for spotting this one! "]}, {"number": 41811, "title": "Building Model with Attention Layer: Graph disconnected: cannot obtain value for tensor", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 \r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8.2\r\n\r\n\r\n**Describe the problem**\r\nI am trying to build a model with attention layer based on tutorial code as follows. But I am getting the below error.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n# Variable-length int sequences.\r\nquery_input = tf.keras.Input(shape=(None,), dtype='int32')\r\nvalue_input = tf.keras.Input(shape=(None,), dtype='int32')\r\n\r\n# Embedding lookup.\r\ntoken_embedding = tf.keras.layers.Embedding(encoder.vocab_size, 64)\r\n# Query embeddings of shape [batch_size, Tq, dimension].\r\nquery_embeddings = token_embedding(query_input)\r\n# Value embeddings of shape [batch_size, Tv, dimension].\r\nvalue_embeddings = token_embedding(value_input)\r\n\r\n# CNN layer.\r\ncnn_layer = tf.keras.layers.Conv1D(filters=100, kernel_size=4, padding='same')\r\n# Query encoding of shape [batch_size, Tq, filters].\r\nquery_seq_encoding = cnn_layer(query_embeddings)\r\n# Value encoding of shape [batch_size, Tv, filters].\r\nvalue_seq_encoding = cnn_layer(value_embeddings)\r\n\r\n# Query-value attention of shape [batch_size, Tq, filters].\r\nquery_value_attention_seq = tf.keras.layers.Attention()([query_seq_encoding, value_seq_encoding])\r\n\r\n# Reduce over the sequence axis to produce encodings of shape\r\n# [batch_size, filters].\r\nquery_encoding = tf.keras.layers.GlobalAveragePooling1D()(query_seq_encoding)\r\nquery_value_attention = tf.keras.layers.GlobalAveragePooling1D()(query_value_attention_seq)\r\n\r\n# Concatenate query and document encodings to produce a DNN input layer.\r\ninput_layer = tf.keras.layers.Concatenate()([query_encoding, query_value_attention])\r\n\r\noutput_layer = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(input_layer)\r\nmodel  = tf.keras.Model(inputs=input_layer, outputs=output_layer)\r\n```\r\n\r\n\r\n**Any other info / logs**\r\n```\r\nWARNING:tensorflow:Model inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_8\" was not an Input tensor, it was generated by layer concatenate_22.\r\nNote that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\r\nThe tensor that caused the issue was: concatenate_22/Identity:0\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-52-f66b4debcaf8> in <module>\r\n     33 \r\n     34 output_layer = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(input_layer)\r\n---> 35 model  = tf.keras.Model(inputs=input_layer, outputs=output_layer)\r\n     36 \r\n     37 #############################################################################\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in __init__(self, *args, **kwargs)\r\n    165 \r\n    166   def __init__(self, *args, **kwargs):\r\n--> 167     super(Model, self).__init__(*args, **kwargs)\r\n    168     _keras_api_gauge.get_cell('model').set(True)\r\n    169     # Model must be created under scope of DistStrat it will be trained with.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in __init__(self, *args, **kwargs)\r\n    171         'inputs' in kwargs and 'outputs' in kwargs):\r\n    172       # Graph network\r\n--> 173       self._init_graph_network(*args, **kwargs)\r\n    174     else:\r\n    175       # Subclassed network\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    454     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    455     try:\r\n--> 456       result = method(self, *args, **kwargs)\r\n    457     finally:\r\n    458       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in _init_graph_network(self, inputs, outputs, name, **kwargs)\r\n    305     # Keep track of the network's nodes and layers.\r\n    306     nodes, nodes_by_depth, layers, _ = _map_graph_network(\r\n--> 307         self.inputs, self.outputs)\r\n    308     self._network_nodes = nodes\r\n    309     self._nodes_by_depth = nodes_by_depth\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in _map_graph_network(inputs, outputs)\r\n   1790                              'The following previous layers '\r\n   1791                              'were accessed without issue: ' +\r\n-> 1792                              str(layers_with_complete_input))\r\n   1793         for x in nest.flatten(node.output_tensors):\r\n   1794           computable_tensors.add(id(x))\r\n\r\nValueError: Graph disconnected: cannot obtain value for tensor Tensor(\"input_50:0\", shape=(None, None), dtype=int32) at layer \"input_50\". The following previous layers were accessed without issue: []\r\n```\r\n\r\n", "comments": ["I might be wrong, but should your input not be the two Input layers? \r\n`model  = tf.keras.Model(inputs=[query_input, value_input], outputs=output_layer)`", "Thank you it solved the issue.  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41811\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41811\">No</a>\n", "Happy to hear :smile: "]}, {"number": 41810, "title": "Support for ragged tensor targets (variable-length y)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: v10.2\r\n- GPU model and memory: GeForce GTX 1070 - 8117MiB\r\n\r\n**Describe the current behavior**:\r\nIn compiling the model, the loss function fails to capture the shape of y_true as it is a ragged tensor. \r\nThis impedes sequence labelling tasks with both ragged inputs (x) and ragged sequential targets (y).\r\n\r\n**Describe the expected behavior**:\r\nI would expect to be able to train with both ragged inputs and targets (following the same internal structure).\r\nIn a sequence labelling task, e.g. named entity recognition, sentences and target vectors are variable-length-sequences. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nLink to the issue and reproducible code:\r\n[LSTM ragged targets sequence labelling](https://colab.research.google.com/drive/18P6gZQUlP6qxBq70UCRfI42RSilRJ2Mx?usp=sharing) ", "comments": ["@Jordy-VL \r\n\r\nPlease, grant me access for the colab link you have attached.Thanks!", "I updated the link to be public and editable for anyone, can you try again? :) thanks! ", "I have tried in colab with TF 2.3-rc2 ,nightly version(`2.4.0-dev20200728`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/9fdbbbeb20bc39bd6ead149bece7f945/untitled193.ipynb).Thanks!", "Any updates? \r\nThank you in advance :)", "I also would like to see this functionality soon for input-output sequences that are both ragged (was surprised that it was not supported). Thx\r\n", "Thanks for reporting the issue, I think there are a few issues within this report.\r\n\r\n1. In the example provided, the output of the lstm layer is a dense tensor with shape (batch, unit). There isn't a ragged dim involved in the model after the lstm layer, which is why the loss function failed afterwards (failed to compare the dense and ragged tensor). This is expected and working as intended.\r\n\r\n2. If we change your model's lstm layer to \"return_sequences=True\" and let it output the ragged output, the model failed at dense layer since it doesn't support ragged input. \r\n\r\n```\r\nValueError: Layer dense does not support RaggedTensors as input. Inputs received: tf.RaggedTensor(values=Tensor(\"lstm/RaggedFromTensor/boolean_mask/GatherV2:0\", shape=(None, 32), dtype=float32), row_splits=Tensor(\"lstm/RaggedFromTensor/concat:0\", shape=(None,), dtype=int64)). You can try converting your input to an uniform tensor.\r\n```\r\n\r\nSo far adding more support for ragged and composite input/output is one of the item on the keras road map. We will probably address this issue in near future.", "> Thanks for reporting the issue, I think there are a few issues within this report.\r\n> \r\n> 1. In the example provided, the output of the lstm layer is a dense tensor with shape (batch, unit). There isn't a ragged dim involved in the model after the lstm layer, which is why the loss function failed afterwards (failed to compare the dense and ragged tensor). This is expected and working as intended.\r\n> 2. If we change your model's lstm layer to \"return_sequences=True\" and let it output the ragged output, the model failed at dense layer since it doesn't support ragged input.\r\n> \r\n> ```\r\n> ValueError: Layer dense does not support RaggedTensors as input. Inputs received: tf.RaggedTensor(values=Tensor(\"lstm/RaggedFromTensor/boolean_mask/GatherV2:0\", shape=(None, 32), dtype=float32), row_splits=Tensor(\"lstm/RaggedFromTensor/concat:0\", shape=(None,), dtype=int64)). You can try converting your input to an uniform tensor.\r\n> ```\r\n> \r\n> So far adding more support for ragged and composite input/output is one of the item on the keras road map. We will probably address this issue in near future.\r\n\r\nNo problem. Would it be possible to share a link with regards to the Keras roadmap or some time estimate on addressing this issue? Then I know when to come back to the topic. ", "We don't have a public doc for that so far, but probably in next two releases (tf release every 3 months).", "Any news on this topic? Did you manage to trick TF in some way? \r\nFor now, it looks TF does not support ragged tensors on target variables.", "@MiquelFerriol  Hi Miquel, sorry to disappoint but I did not find a short-term workaround :/ \r\nDynamic padding and batching is the only solution I see for now... \r\n", "I don't see mention of this issue in the 2.4.0-rc0 release notes\r\n", "Note that supporting RaggedTensors as targets (e.g., in `model.fit`) is a frequent request, see #44988, #44112, #43591, #43093, #42320, #41810.\r\n\r\nSome partial progress is in pull requests #45060 and #45015, which will allow using custom losses in Keras model (but existing losses like MSE or (S)CE will still not work).", "Note that I created a new feature request #45403 to support RaggedTensors in standard Keras loss functions.", "The issue is now fixed in `tf-nightly-2.5.0.dev20210302`, see https://colab.research.google.com/drive/18P6gZQUlP6qxBq70UCRfI42RSilRJ2Mx?usp=sharing", "@Jordy-VL,\r\nCan you please confirm if we can close this issue as it has been fixed per [above comment](https://github.com/tensorflow/tensorflow/issues/41810#issuecomment-788876673)? Thanks! ", "@rmothukuru I confirm it works, thanks for the effort! ", "How do I utilize ragged labels? Just set the input to ragged=True?"]}, {"number": 41809, "title": "Is SYCL on Tensorflow officially dead?", "body": "It seems there is no community activity about SYCL for years and there are quite some untested dead code still living in the code base.\r\nI'm wondering is SYCL dead and will it be removed from TF?\r\n\r\nThanks", "comments": ["Adoption of SYCL is growing, in particular Intel's DPC++ compiler is a SYCL implementation with extensions and the Aurora supercomputer will use SYCL for its programming model. We (Codeplay) are also expanding the hardware support for SYCL with various manufacturers. \r\nAt the moment we have a branch of TensorFlow that while based on an older version it does work for specific hardware and driver combinations. It is tested using CI. \r\nhttps://github.com/codeplaysoftware/tensorflow\r\nhttps://developer.codeplay.com/products/computecpp/ce/guides/tensorflow-guide/tensorflow-native-compilation\r\n\r\nCurrently we are waiting for the TensorFlow runtime and modules architecture to be clearer and then there will be some progress again to adapt to that. We also expect that will lead to collaboration with hardware vendors.", "> Currently we are waiting for the TensorFlow runtime and modules architecture to be clearer\r\n\r\nAre you talking about these:\r\n - https://github.com/tensorflow/community/pull/262\r\n - https://github.com/tensorflow/community/pull/257\r\n\r\nand the [TFRT](https://blog.tensorflow.org/2020/04/tfrt-new-tensorflow-runtime.html) effort?\r\n\r\nIf so, would it make sense to remove SYCL support from TF head (it doesn't build today anyway) and reconsider adding it as an out of tree backend once these interfaces and/or TFRT has landed?", "> Adoption of SYCL is growing, in particular Intel's DPC++ compiler is a SYCL implementation with extensions and the Aurora supercomputer will use SYCL for its programming model. We (Codeplay) are also expanding the hardware support for SYCL with various manufacturers.\r\n> At the moment we have a branch of TensorFlow that while based on an older version it does work for specific hardware and driver combinations. It is tested using CI.\r\n> https://github.com/codeplaysoftware/tensorflow\r\n> https://developer.codeplay.com/products/computecpp/ce/guides/tensorflow-guide/tensorflow-native-compilation\r\n> \r\n> Currently we are waiting for the TensorFlow runtime and modules architecture to be clearer and then there will be some progress again to adapt to that. We also expect that will lead to collaboration with hardware vendors.\r\n\r\nThank you for the updates. \r\nMy bad. My question should be more specific. It's mainly about the uncompiled code in TF code base.\r\n\r\nKevin", "> \r\n> \r\n> > Currently we are waiting for the TensorFlow runtime and modules architecture to be clearer\r\n> \r\n> Are you talking about these:\r\n> \r\n>     * [tensorflow/community#262](https://github.com/tensorflow/community/pull/262)\r\n> \r\n>     * [tensorflow/community#257](https://github.com/tensorflow/community/pull/257)\r\n> \r\n> \r\n> and the [TFRT](https://blog.tensorflow.org/2020/04/tfrt-new-tensorflow-runtime.html) effort?\r\n> \r\n> If so, would it make sense to remove SYCL support from TF head (it doesn't build today anyway) and reconsider adding it as an out of tree backend once these interfaces and/or TFRT has landed?\r\n\r\nYes those are the most relevant discussions. We would be also be prepared to upstream the code we have working in our fork if we had a \"sponsor\" from Google that can help us get the code integrated into the main branch.", "Hi @rodburns,\r\n\r\nSorry for the delayed response.\r\n\r\nGenerally we're moving away from putting diverse HW backends integrated into the mainline.  Instead we want them to live out of tree for obvious maintainability benefits -- the TensorFlow team can only own so much code.  Moreover, this means you retain code ownership over the TF/SYCL integration (since it would no longer live in the TF repository), and don't need to go through Google's code review process to make changes.\r\n\r\nThis ability to have \"out of tree backends\" is a core design principle of TFRT (CC @mhong) and is also the motivation behind RFC's like https://github.com/tensorflow/community/pull/262 and https://github.com/tensorflow/community/pull/257 (CC @annarev).\r\n\r\nSo how about the following:\r\n\r\n - Let's drop SYCL support from the TF core repository.  I'll do this in the next week or so unless you have strong objections.\r\n - Let's work together on making sure the pluggable device API and/or TFRT works for SYCL.  IMO this will be a better use of eng resources on your side, as compared to upstreaming SYCL support to TF HEAD.", "We are keen to work with you and others on ensuring that SYCL can be integrated with TensorFlow.", "@rodburns I would be happy to help out with integrating with the new API. That being said, it is still work-in-progress but you can check RFCs @sanjoy linked and let us know if you have any questions.\r\nCurrent StreamExecutor C API is checked in here:\r\nhttps://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/stream_executor/stream_executor.h?q=stream_executor.h\r\n(note that Pluggable Device needs to be implemented first before this API is useful. Also, the API might still have some changes coming up.)", "Hi Anna,\n\nWhat is the difference between\nhttps://cs.opensource.google/tensorflow/tensorflow/\n<https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/stream_executor/stream_executor.h?q=stream_executor.h>\n and https://github.com/tensorflow ?\n\nThank you\nKevin\n\nAnna Revinskaya <notifications@github.com> \u4e8e2020\u5e749\u670822\u65e5\u5468\u4e8c \u4e0a\u534811:50\u5199\u9053\uff1a\n\n> @rodburns <https://github.com/rodburns> I would be happy to help out with\n> integrating with the new API. That being said, it is still work-in-progress\n> but you can check RFCs @sanjoy <https://github.com/sanjoy> linked and let\n> us know if you have any questions.\n> Current StreamExecutor C API is checked in here:\n>\n> https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/stream_executor/stream_executor.h?q=stream_executor.h\n> (note that Pluggable Device needs to be implemented first before this API\n> is useful. Also, the API might still have some changes coming up.)\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41809#issuecomment-696494374>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACDERVDMVRHRXZU5Z7HTPGDSHANJNANCNFSM4PKKQEIQ>\n> .\n>\n", "> Hi Anna, What is the difference between https://cs.opensource.google/tensorflow/tensorflow/ <https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/stream_executor/stream_executor.h?q=stream_executor.h> and https://github.com/tensorflow ? Thank you Kevin Anna Revinskaya <notifications@github.com> \u4e8e2020\u5e749\u670822\u65e5\u5468\u4e8c \u4e0a\u534811:50\u5199\u9053\uff1a\r\n> [\u2026](#)\r\n> @rodburns <https://github.com/rodburns> I would be happy to help out with integrating with the new API. That being said, it is still work-in-progress but you can check RFCs @sanjoy <https://github.com/sanjoy> linked and let us know if you have any questions. Current StreamExecutor C API is checked in here: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/stream_executor/stream_executor.h?q=stream_executor.h (note that Pluggable Device needs to be implemented first before this API is useful. Also, the API might still have some changes coming up.) \u2014 You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub <[#41809 (comment)](https://github.com/tensorflow/tensorflow/issues/41809#issuecomment-696494374)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/ACDERVDMVRHRXZU5Z7HTPGDSHANJNANCNFSM4PKKQEIQ> .\r\n\r\nhttps://github.com/tensorflow is the actual repo that contains TF code. cs.opensource.google just provides a UI to browse the TF github code (and other google OSS projects).\r\n\r\nEdit: In other words the following display the same file in github:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/experimental/stream_executor/stream_executor.h\r\nhttps://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/stream_executor/stream_executor.h?q=stream_executor.h\r\n"]}, {"number": 41807, "title": "TFLITE Relocate Tensor Fail", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): No \r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nSteps to generate .tflite-\r\n1. Train a ckpt\r\n2. Create saved_model.pb using None,None input parameter\r\n3. Generate .TFLITE saved .pb model by specifying default input because None,None doesnt works\r\n\r\nGenerated .tflite Input Details. \r\n{'shape': array([  1, 256, 256,   3], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 0, 'name': 'image'}\r\n{'shape': array([  1, 256, 256,   1], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 1, 'name': 'mask'}\r\n{'shape': array([ 1, 64, 64,  1], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 2, 'name': 'mask2'}\r\n{'shape': array([  1, 128, 128,   1], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 3, 'name': 'mask4'}\r\n\r\nActual Input Detail\r\n(1, 432, 492, 3) (1, 432, 492, 1) (1, 216, 246, 1) (1, 108, 123, 1)\r\n\r\nAllocating Tensors based on Actual Input Values\r\ninterpreter.resize_tensor_input(input_details[0]['index'], (1,h,w,3))\r\ninterpreter.resize_tensor_input(input_details[1]['index'], (1,h,w,1))\r\ninterpreter.resize_tensor_input(input_details[2]['index'], (1,int(h/4),int(w/4),1))\r\ninterpreter.resize_tensor_input(input_details[3]['index'], (1,int(h/2),int(w/2),1))\r\ninterpreter.allocate_tensors()\r\n\r\nERROR - \r\nFile \"testtliteNone.py\", line 141, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"/homelib/python3.5/site-packages/tensorflow/lite/python/interpreter.py\", line 95, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/home/lib/python3.5/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 106, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/kernel_util.cc:233 d1 == d2 || d1 == 1 || d2 == 1 was not true.Node number 4 (MUL) failed to prepare.\r\n\r\n**Describe the expected behavior**\r\nAllocation Should Be Done\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@DeepakG19 \r\n\r\nWill it be possible to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "> @DeepakG19\r\n> \r\n> Will it be possible to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n\r\nsorry Ravi. can not share the model file. can share commands that i used to generate the model files. Thanks for understanding.", "@DeepakG19 Can you please try against latest TF (2.3 or nightly) ? Thanks!", "It seems the allocation failed because of the resize input.\r\n\r\nCan you try not resize input?\r\n\r\nthanks", "> @DeepakG19 Can you please try against latest TF (2.3 or nightly) ? Thanks!\r\n\r\nTried with tf-nightly. Same issue.", "> It seems the allocation failed because of the resize input.\r\n> \r\n> Can you try not resize input?\r\n> \r\n> thanks\r\n\r\nHow does one make a tflite model independent of input size for inference?", "Not sure about what model you're using, but \r\n\r\n```\r\nActual Input Detail\r\n(1, 432, 492, 3) (1, 432, 492, 1) (1, 216, 246, 1) (1, 108, 123, 1)\r\n\r\nAllocating Tensors based on Actual Input Values\r\ninterpreter.resize_tensor_input(input_details[0]['index'], (1,h,w,3))\r\ninterpreter.resize_tensor_input(input_details[1]['index'], (1,h,w,1))\r\ninterpreter.resize_tensor_input(input_details[2]['index'], (1,int(h/4),int(w/4),1))\r\ninterpreter.resize_tensor_input(input_details[3]['index'], (1,int(h/2),int(w/2),1))\r\ninterpreter.allocate_tensors()\r\n\r\n```\r\n\r\nseems like the resize here may have some inconsistent shape.", "> Not sure about what model you're using, but\r\n> \r\n> ```\r\n> Actual Input Detail\r\n> (1, 432, 492, 3) (1, 432, 492, 1) (1, 216, 246, 1) (1, 108, 123, 1)\r\n> \r\n> Allocating Tensors based on Actual Input Values\r\n> interpreter.resize_tensor_input(input_details[0]['index'], (1,h,w,3))\r\n> interpreter.resize_tensor_input(input_details[1]['index'], (1,h,w,1))\r\n> interpreter.resize_tensor_input(input_details[2]['index'], (1,int(h/4),int(w/4),1))\r\n> interpreter.resize_tensor_input(input_details[3]['index'], (1,int(h/2),int(w/2),1))\r\n> interpreter.allocate_tensors()\r\n> ```\r\n> \r\n> seems like the resize here may have some inconsistent shape.\r\n\r\n![20200731_124638](https://user-images.githubusercontent.com/3595901/89010305-f07d0100-d32b-11ea-932d-391cfcc7552a.jpg)\r\n\r\nThe respective size ratio match, resize should work?", "sorry, I mean this:\r\n\r\n```\r\nGenerated .tflite Input Details.\r\n{'shape': array([ 1, 256, 256, 3], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 0, 'name': 'image'}\r\n{'shape': array([ 1, 256, 256, 1], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 1, 'name': 'mask'}\r\n{'shape': array([ 1, 64, 64, 1], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 2, 'name': 'mask2'}\r\n{'shape': array([ 1, 128, 128, 1], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 3, 'name': 'mask4'}\r\n\r\nActual Input Detail\r\n(1, 432, 492, 3) (1, 432, 492, 1) (1, 216, 246, 1) (1, 108, 123, 1)\r\n```\r\n\r\nAlso, it's possible your model has some fixed shape in the middle which can break the shape propagation.\r\n\r\nYou will need to build the model with dynamic shape in the first place.\r\n\r\nCan you try if not using resize input. and see if it works?", "> sorry, I mean this:\r\n> \r\n> ```\r\n> Generated .tflite Input Details.\r\n> {'shape': array([ 1, 256, 256, 3], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 0, 'name': 'image'}\r\n> {'shape': array([ 1, 256, 256, 1], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 1, 'name': 'mask'}\r\n> {'shape': array([ 1, 64, 64, 1], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 2, 'name': 'mask2'}\r\n> {'shape': array([ 1, 128, 128, 1], dtype=int32), 'quantization': (0.0, 0), 'dtype': <class 'numpy.float32'>, 'index': 3, 'name': 'mask4'}\r\n> \r\n> Actual Input Detail\r\n> (1, 432, 492, 3) (1, 432, 492, 1) (1, 216, 246, 1) (1, 108, 123, 1)\r\n> ```\r\n> \r\n> Also, it's possible your model has some fixed shape in the middle which can break the shape propagation.\r\n> \r\n> You will need to build the model with dynamic shape in the first place.\r\n> \r\n> Can you try if not using resize input. and see if it works?\r\n\r\nThat printed order is different, values are passed correctly, checked. \r\nThere were RESIZE ops, since downsampling is not supported, i am passing additional inputs. For upscaling I am using tf.keras.backend.resize_images, which has a *scaling* parameter, and hence i doubt there is any fixed size.", "Hi Karim, can you help take a look?\r\n\r\nThanks!", "Passing explicit shape during conversion doesn't guarantee you can resize input during inference, and you shouldn't try resizing the input.\r\n\r\nYou will need to pass None in the input for the dimensions that are dynamic. If your TF model is constructed already with None in shape, then converting the saved model will result in TFLite model with same shape.\r\nIf you had problems during conversion, then please reply with more details on what you tried and what was the error you got.\r\n\r\nThanks\r\n", "> Passing explicit shape during conversion doesn't guarantee you can resize input during inference, and you shouldn't try resizing the input.\r\n> \r\n> You will need to pass None in the input for the dimensions that are dynamic. If your TF model is constructed already with None in shape, then converting the saved model will result in TFLite model with same shape.\r\n> If you had problems during conversion, then please reply with more details on what you tried and what was the error you got.\r\n> \r\n> Thanks\r\n\r\n1. Ckpt is created from training using input 256x256\r\n2. saved_model.pb is generated, passing input params as None,None for all 4 inputs\r\n3. tflite_converter CLI is used to convert to tflite. Input shapes provided as None or -1 doesnt work.\r\n\r\nDynamic size input exists at inference time. The error occurs while using INTERPRETER.RELOCATE_TENSOR() after changing input tensor size.", "> > Passing explicit shape during conversion doesn't guarantee you can resize input during inference, and you shouldn't try resizing the input.\r\n> > You will need to pass None in the input for the dimensions that are dynamic. If your TF model is constructed already with None in shape, then converting the saved model will result in TFLite model with same shape.\r\n> > If you had problems during conversion, then please reply with more details on what you tried and what was the error you got.\r\n> > Thanks\r\n> \r\n> 1. Ckpt is created from training using input 256x256\r\n> 2. saved_model.pb is generated, passing input params as None,None for all 4 inputs\r\n> 3. tflite_converter CLI is used to convert to tflite. Input shapes provided as None or -1 doesnt work.\r\n> \r\n> Dynamic size input exists at inference time. The error occurs while using INTERPRETER.RELOCATE_TENSOR() after changing input tensor size.\r\n\r\nAlso, tflite created for passing any size ie 256x256 or 512x512 etc in tflite_converter works for input of that size but will fail when we try to relocate_tensor", "The failure during allocate_tensors is because a MUL op has operands that are not broadcastable. So either you're setting inputs that are invalid, or some other issue.\r\n\r\nCan you run the TF model with same shape ?\r\nCan you share the exact commands you used ?\r\n\r\nPlease try to share reproduce steps or better a code snippet with sample model that has issue.\r\n\r\nThanks", "> The failure during allocate_tensors is because a MUL op has operands that are not broadcastable. So either you're setting inputs that are invalid, or some other issue.\r\n> \r\n> Can you run the TF model with same shape ?\r\n> Can you share the exact commands you used ?\r\n> \r\n> Please try to share reproduce steps or better a code snippet with sample model that has issue.\r\n> \r\n> Thanks\r\n\r\nHi Karim,\r\n\r\nThe saved_model in pb [saved_model.pb and \\variables] runs without any issue for variable sized inputs. This was created after passing None,None in input placeholder dimension.\r\n\r\nThe above model is further used to create tflite, and since None or -1 or 1 is not a valid input, we specify a dummy size, for instance 512x512.\r\ntflite is created passing 512x512x3:512x512x1:128x128x1:256x256x1 as input for respective placeholders. Any input of the same size (512x512) executes without any error. But when the input size is changed, the error in allocate_tensor appears, as mentioned earlier.\r\n\r\nThe code used to generate tflite from saved_model is\r\ntflite_convert --saved_model_dir=\"/None_dir/\" --output_file=\"out.tflite\" --input_shapes=1,512,512,3:1,512,512,1;1,128,128,1:1,256,256,1 --input_arrays=image,mask,mask2,mask4 --output_arrays=out --enable_v1_converter\r\n\r\nHave also tried -\r\ntflite_convert --saved_model_dir=\"/None_dir/\" --output_file=\"out.tflite\" --input_shapes=1,512,512,3:1,512,512,1;1,128,128,1:1,256,256,1 --input_arrays=image,mask,mask2,mask4 --output_arrays=out --enable_v1_converter --experimental_new_converter=true\r\n", "Hi @DeepakG19 \r\n\r\nPassing dummy values and resize the model during inference to different size is not how you should specify dynamic shape, and not working is expected.\r\n\r\nThe issue is this part \"and since None or -1 or 1 is not a valid input,\" You should be able to use None as input.\r\nIf the model was saved with inputs as None, then please use our Python API to do the conversion by specifying only the saved_model path.\r\nExample\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"saved_model_dir_path\")\r\ntflite_model = converter.convert()\r\n# Save the TF Lite model.\r\nwith tf.io.gfile.GFile('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\nAfter doing the above, please \r\n1) Paste any errors you got during the conversion script above.\r\n2) If conversion passed and inference has issue, then paste code for inference that you tried and what was the error.\r\n3) Please share the model you are trying to convert.\r\n\r\nThanks", "> Hi @DeepakG19\r\n> \r\n> Passing dummy values and resize the model during inference to different size is not how you should specify dynamic shape, and not working is expected.\r\n> \r\n> The issue is this part \"and since None or -1 or 1 is not a valid input,\" You should be able to use None as input.\r\n> If the model was saved with inputs as None, then please use our Python API to do the conversion by specifying only the saved_model path.\r\n> Example\r\n> \r\n> # Convert the model.\r\n> converter = tf.lite.TFLiteConverter.from_saved_model(\"saved_model_dir_path\")\r\n> tflite_model = converter.convert()\r\n> \r\n> # Save the TF Lite model.\r\n> with tf.io.gfile.GFile('model.tflite', 'wb') as f:\r\n> f.write(tflite_model)\r\n> \r\n> After doing the above, please\r\n> \r\n> 1. Paste any errors you got during the conversion script above.\r\n> 2. If conversion passed and inference has issue, then paste code for inference that you tried and what was the error.\r\n> 3. Please share the model you are trying to convert.\r\n> \r\n> Thanks\r\n\r\nHi Karim,\r\nThanks for quick response. The solution you provided, works at the moment.The problem was using a dummy value (as you said) to save model as tflite, which I got from https://stackoverflow.com/a/55732431.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41807\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41807\">No</a>\n"]}, {"number": 41806, "title": "compile tensorflow fails with error: SWIGing tensorflow/python/tensorflow.i ... swig failed: error executing command", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 6\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):1.14, 1.15, 2.1, 2.2, 2.3\r\n- Bazel version (if compiling from source): 2.0.0\r\n\r\n**Describe the current behavior**\r\n\r\nRunning the build fails with a rather undescriptive:\r\n```\r\nERROR: /home/username/src/tensorflow/tensorflow/python/BUILD:2974:1: SWIGing tensorflow/python/tensorflow.i failed (Exit 1): swig failed: error executing command\r\n  (cd /tmp/_bazel_username/dcca333cc36b578f4473c754fbbc85ff/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/swig/swig -c++ -python -module pywrap_tensorflow_internal -o bazel-out/local-py3-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc -outdir bazel-out/local-py3-opt/bin/tensorflow/python -ltensorflow/python/client/device_lib.i -ltensorflow/python/client/events_writer.i -ltensorflow/python/client/tf_session.i -ltensorflow/python/client/tf_sessionrun_wrapper.i -ltensorflow/python/framework/cpp_shape_inference.i -ltensorflow/python/framework/python_op_gen.i -ltensorflow/python/grappler/cluster.i -ltensorflow/python/grappler/cost_analyzer.i -ltensorflow/python/grappler/item.i -ltensorflow/python/grappler/model_analyzer.i -ltensorflow/python/grappler/tf_optimizer.i -ltensorflow/python/lib/core/py_func.i -ltensorflow/python/lib/core/strings.i -ltensorflow/python/lib/io/file_io.i -ltensorflow/python/lib/io/py_record_reader.i -ltensorflow/python/lib/io/py_record_writer.i -ltensorflow/python/platform/base.i -ltensorflow/python/pywrap_tfe.i -ltensorflow/python/training/quantize_training.i -ltensorflow/python/training/server_lib.i -ltensorflow/python/util/kernel_registry.i -ltensorflow/python/util/port.i -ltensorflow/python/util/py_checkpoint_reader.i -ltensorflow/python/util/stat_summarizer.i -ltensorflow/python/util/tfprof.i -ltensorflow/python/util/transform_graph.i -ltensorflow/python/util/util.i -Ibazel-out/local-py3-opt/genfiles -Iexternal/eigen_archive -Iexternal/grpc -Iexternal/protobuf_archive -Iexternal/swig -Iexternal/boringssl -Ibazel-out/local-py3-opt/genfiles/external/local_config_python -Iexternal/nsync -Iexternal/gemmlowp -Iexternal/jpeg -Iexternal/com_googlesource_code_re2 -Iexternal/jsoncpp_git -Iexternal/zlib_archive -Iexternal/highwayhash -Iexternal/gif_archive -Ibazel-out/local-py3-opt/genfiles/external/jpeg -Iexternal/lmdb -Iexternal/png_archive -Iexternal/farmhash_archive -Iexternal/local_config_cuda -Iexternal/sqlite_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/python/tensorflow.i).\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nThis is as reported in #14211, looked like an issue in Bazel (https://github.com/bazelbuild/bazel/issues/4053) but was deemed a Bug in TensorFlow, hence requesting a fix here\r\n\r\n**Describe the expected behavior**\r\n\r\nBuild succeeds without need to apply patch from https://github.com/bazelbuild/bazel/issues/4053#issuecomment-343134886\r\n", "comments": ["SWIG no longer used. Closing", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41806\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41806\">No</a>\n"]}, {"number": 41805, "title": "incompatible with expected float_ref.", "body": "Traceback (most recent call last):\r\n  File \"convert_TFLite.py\", line 90, in <module>\r\n    convert_from_savedModel(savedModelDir, TFLiteFile, quan=True, integerOnly=True)\r\n  File \"convert_TFLite.py\", line 50, in convert_from_savedModel\r\n    TFLiteModel = converter.convert()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/lite.py\", line 459, in convert\r\n    self._funcs[0], lower_control_flow=False))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/convert_to_constants.py\", line 707, in convert_variables_to_constants_v2_as_graph\r\n    frozen_func = _construct_concrete_function(func, graph_def, converted_inputs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/convert_to_constants.py\", line 406, in _construct_concrete_function\r\n    new_output_names)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py\", line 633, in function_from_graph_def\r\n    wrapped_import = wrap_function(_imports_graph_def, [])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py\", line 611, in wrap_function\r\n    collections={}),\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py\", line 86, in __call__\r\n    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py\", line 92, in wrapped\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py\", line 631, in _imports_graph_def\r\n    importer.import_graph_def(graph_def, name=\"\")\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py\", line 405, in import_graph_def\r\n    producer_op_list=producer_op_list)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py\", line 501, in _import_graph_def_internal\r\n    raise ValueError(str(e))\r\nValueError: Input 0 of node conv21/pointwise/BatchNorm/cond/Assign/Switch was passed float from conv21/pointwise/BatchNorm/moving_mean:0 incompatible with expected float_ref.\r\n\r\nwhen I use \"tf.lite.TFLiteConverter.from_saved_model()\" to convert checkpoint to tflite\uff0cI got above err about BatchNorm, please help me, ths.\r\n\r\n_Originally posted by @ZhouKai90 in https://github.com/tensorflow/tensorflow/issues/3628#issuecomment-663382551_", "comments": ["Switch is not supported by TFLite. Please use the v2 control flow ops while creating a TF graph. https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2"]}, {"number": 41804, "title": "Error output shape of `tf.gather` in tflite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.15.0\r\n\r\n---\r\n\r\n`tf.gather` has error behavior in tflite model.\r\n\r\nMy script:\r\n\r\n```python\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import quantize as contrib_quantize\r\n\r\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\r\nMODEL = './test.tflite'\r\n\r\n\r\ndef convert():\r\n    with tf.Graph().as_default() as g:\r\n        with tf.Session() as sess:\r\n            data = tf.placeholder('float32', [1, 512, 2], name='data')\r\n            indices = tf.placeholder('int64', [1, 1], name='indices')\r\n            output = tf.gather(data, indices, batch_dims=1)\r\n\r\n            # Tensor(\"GatherV2:0\", shape=(1, 1, 2), dtype=float32)\r\n            print(output)\r\n            contrib_quantize.experimental_create_eval_graph(\r\n                input_graph=g)\r\n            converter = tf.lite.TFLiteConverter.from_session(\r\n                sess,\r\n                input_tensors=[data, indices],\r\n                output_tensors=[output])\r\n            tflite_model = converter.convert()\r\n            with open(MODEL, \"wb\") as w:\r\n                w.write(tflite_model)\r\n\r\n\r\ndef load():\r\n    interpreter = tf.lite.Interpreter(MODEL)\r\n    interpreter.allocate_tensors()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # [\r\n    # {'name': 'GatherV2', 'index': 0,\r\n    # 'shape': array([1, 1, 1, 2], dtype=int32),\r\n    # 'dtype': <class 'numpy.float32'>,\r\n    # 'quantization': (0.0, 0)}\r\n    # ]\r\n    print(output_details)\r\n\r\n\r\nif __name__ == '__main__':\r\n    convert()\r\n    load()\r\n\r\n```\r\n\r\nThe shape of `output` is `[1, 1, 2]` in graph, but its `[1, 1, 1, 2]` in tflite.\r\n\r\n**logs**\r\n\r\n```bash\r\n2020-07-28 16:41:55.462113: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-28 16:41:56.271049: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2020-07-28 16:41:56.271163: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ubun\r\n2020-07-28 16:41:56.271185: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ubun\r\n2020-07-28 16:41:56.271414: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 410.48.0\r\n2020-07-28 16:41:56.271483: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 410.48.0\r\n2020-07-28 16:41:56.271502: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 410.48.0\r\n2020-07-28 16:41:56.272061: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n2020-07-28 16:41:56.304875: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394405000 Hz\r\n2020-07-28 16:41:56.308426: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d426dd1c60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-28 16:41:56.308484: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n\r\nTensor(\"GatherV2:0\", shape=(1, 1, 2), dtype=float32)\r\n2020-07-28 16:41:56.323565: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-07-28 16:41:56.323821: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-07-28 16:41:56.327368: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2020-07-28 16:41:56.327398: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.008ms.\r\n2020-07-28 16:41:56.327411: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-07-28 16:41:56.332508: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-07-28 16:41:56.332641: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-07-28 16:41:56.342305: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2020-07-28 16:41:56.342358: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 4 nodes (0), 3 edges (0), time = 4.117ms.\r\n2020-07-28 16:41:56.342383: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 4 nodes (0), 3 edges (0), time = 0.212ms.\r\n[{'name': 'GatherV2', 'index': 0, 'shape': array([1, 1, 1, 2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\n```\r\n", "comments": ["I have tried in colab with TF version 1.15 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/84aa3653703cc8f9248d8416cd0ab8f3/untitled192.ipynb).Thanks!", "Can you try on 2.3 / nightly?", "@mihaimaruseac . I try the tf2.3 in colab[[here](https://colab.research.google.com/drive/11PhuxdgaZ9SlU0-cylp4K5zN_R61CP1b#scrollTo=3urVVjb9Ouu6)]. `tf.gather` works with no error occurred!\r\n\r\nIt seems this problem is disappeared in tf2.x. But in tf1.x, it is still confused.", "We won't be able to fix 1.x behavior as that is outside of the patch policy. Recommended path is to update to TF2.3", "Ok! I will try to update my code."]}, {"number": 41803, "title": "The version of CUB in your include path is not compatible with this release of Thrust", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.3\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11/8\r\n- GPU model and memory: 1070 ti\r\n\r\n i try\r\nbazel --output_base=c:/bazel/output_dir/ build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\nand have some error:\r\n#error The version of CUB in your include path is not compatible with this release of Thrust. CUB is now included in the CUDA Toolkit, so you no longer need to use your own checkout of CUB. Define THRUST_IGNORE_CUB_VERSION_CHECK to ignore this.\r\n", "comments": ["The same with me. When building tf2.3 with cuda11/cudnn8 on win10\u3002I have tried several times. Each time the error may show different details but they all about \"cub\" and \"cudnn\" indeed.", "Config: cuda 11, cudnn 8. I confirm this information. The errors are completely different. The error log is very large. But in the end - cub.\r\nConfig: cuda 10.2, cudnn 8 - also errors. different.", "ERROR: C:/tensorflow/tensorflow/core/kernels/BUILD:2677:1: C++ compilation of rule '//tensorflow/core/kernels:dynamic_partition_op_gpu' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/bazel/output_dir/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.26.28801\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.26.28801\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.26.28801\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Python38/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python38/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\Build\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_ENABLE_XLA=1\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=C:\\Users\\Build\\AppData\\Local\\Temp\r\n  C:/Python38/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/mkl_dnn_v1 /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1 /Iexternal/mkl_windows /Ibazel-out/x64_windows-opt/bin/external/mkl_windows /Iexternal/curl /Ibazel-out/x64_windows-opt/bin/external/curl /Iexternal/boringssl /Ibazel-out/x64_windows-opt/bin/external/boringssl /Iexternal/jsoncpp_git /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git /Iexternal/aws /Ibazel-out/x64_windows-opt/bin/external/aws /Iexternal/aws-c-common /Ibazel-out/x64_windows-opt/bin/external/aws-c-common /Iexternal/aws-c-event-stream /Ibazel-out/x64_windows-opt/bin/external/aws-c-event-stream /Iexternal/aws-checksums /Ibazel-out/x64_windows-opt/bin/external/aws-checksums /Iexternal/cub_archive /Ibazel-out/x64_windows-opt/bin/external/cub_archive /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cufft_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/curand_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/cub_archive/_virtual_includes/cub /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/mkl_dnn_v1/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/include /Iexternal/mkl_dnn_v1/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src /Iexternal/mkl_dnn_v1/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/common /Iexternal/mkl_dnn_v1/src/cpu /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu /Iexternal/mkl_dnn_v1/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu/gemm /Iexternal/mkl_dnn_v1/src/cpu/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu/xbyak /Iexternal/mkl_windows/include /Ibazel-out/x64_windows-opt/bin/external/mkl_windows/include /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cublas/include /Iexternal/local_config_cuda/cuda/cufft/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cufft/include /Iexternal/local_config_cuda/cuda/curand/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/curand/include /Iexternal/curl/include /Ibazel-out/x64_windows-opt/bin/external/curl/include /Iexternal/boringssl/src/include /Ibazel-out/x64_windows-opt/bin/external/boringssl/src/include /Iexternal/jsoncpp_git/include /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git/include /Iexternal/aws/aws-cpp-sdk-core/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-core/include /Iexternal/aws/aws-cpp-sdk-s3/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-s3/include /Iexternal/aws/aws-cpp-sdk-transfer/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-transfer/include /Iexternal/aws-c-common/include /Ibazel-out/x64_windows-opt/bin/external/aws-c-common/include /Iexternal/aws-c-event-stream/include /Ibazel-out/x64_windows-opt/bin/external/aws-c-event-stream/include /Iexternal/aws-checksums/include /Ibazel-out/x64_windows-opt/bin/external/aws-checksums/include /DCURL_STATICLIB /DPLATFORM_WINDOWS /DENABLE_CURL_CLIENT /DOPENSSL_IS_BORINGSSL /DTF_USE_SNAPPY /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /showIncludes /MD /O2 /DNDEBUG /w /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX2 /std:c++14 -x cuda -DGOOGLE_CUDA=1 -Xcuda-fatbinary=--compress-all --no-cuda-include-ptx=all --cuda-include-ptx=sm_61 --cuda-gpu-arch=sm_61 -DGOOGLE_CUDA=1 -DTENSORFLOW_USE_NVCC=1 -DTENSORFLOW_USE_XLA=1 -DINTEL_MKL=1 -DEIGEN_USE_VML -DENABLE_MKLDNN_V1 -DENABLE_INTEL_MKL_BFLOAT16 -DENABLE_MKL -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/dynamic_partition_op_gpu/dynamic_partition_op_gpu.cu.o /c tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : Command line warning D9002 : ignoring unknown option '--no-cuda-include-ptx=all'\r\ncl : Command line warning D9002 : ignoring unknown option '--cuda-include-ptx=sm_61'\r\ncl : Command line warning D9002 : ignoring unknown option '--cuda-gpu-arch=sm_61'\r\nbazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include\\thrust/system/cuda/config.h(78): fatal error C1189: #error:  The version of CUB in your include path is not compatible with this release of Thrust. CUB is now included in the CUDA Toolkit, so you no longer need to use your own checkout of CUB. Define THRUST_IGNORE_CUB_VERSION_CHECK to ignore this.", "> Config: cuda 10.2, cudnn 8 - also errors. different.\r\n\r\n@Expert73,\r\nCould you please try building TensorFlow using CUDA 10.1 and cuDNN 7.6 and check if you are facing the same issue.\r\n\r\nAlso, please share the error you are facing while building TensorFlow with CUDA 10.2, so that we can look into it. Thanks!", "CUDA 10.1 and cuDNN 7.6\r\n\r\nERROR: C:/tensorflow/tensorflow/core/kernels/sparse/BUILD:26:1: C++ compilation of rule '//tensorflow/core/kernels/sparse:kernels_gpu' failed (Exit 1): python.exe failed: error executing command\r\n  cd C:/bazel/output_dir/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.26.28801\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.26.28801\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.26.28801\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Python38/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python38/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\Build\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_ENABLE_XLA=1\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=C:\\Users\\Build\\AppData\\Local\\Temp\r\n  C:/Python38/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/mkl_dnn_v1 /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1 /Iexternal/mkl_windows /Ibazel-out/x64_windows-opt/bin/external/mkl_windows /Iexternal/curl /Ibazel-out/x64_windows-opt/bin/external/curl /Iexternal/boringssl /Ibazel-out/x64_windows-opt/bin/external/boringssl /Iexternal/jsoncpp_git /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git /Iexternal/aws /Ibazel-out/x64_windows-opt/bin/external/aws /Iexternal/aws-c-common /Ibazel-out/x64_windows-opt/bin/external/aws-c-common /Iexternal/aws-c-event-stream /Ibazel-out/x64_windows-opt/bin/external/aws-c-event-stream /Iexternal/aws-checksums /Ibazel-out/x64_windows-opt/bin/external/aws-checksums /Iexternal/cub_archive /Ibazel-out/x64_windows-opt/bin/external/cub_archive /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cufft_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/curand_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/cub_archive/_virtual_includes/cub /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cusolver_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cusparse_headers_virtual /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/mkl_dnn_v1/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/include /Iexternal/mkl_dnn_v1/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src /Iexternal/mkl_dnn_v1/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/common /Iexternal/mkl_dnn_v1/src/cpu /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu /Iexternal/mkl_dnn_v1/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu/gemm /Iexternal/mkl_dnn_v1/src/cpu/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu/xbyak /Iexternal/mkl_windows/include /Ibazel-out/x64_windows-opt/bin/external/mkl_windows/include /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cublas/include /Iexternal/local_config_cuda/cuda/cufft/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cufft/include /Iexternal/local_config_cuda/cuda/curand/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/curand/include /Iexternal/curl/include /Ibazel-out/x64_windows-opt/bin/external/curl/include /Iexternal/boringssl/src/include /Ibazel-out/x64_windows-opt/bin/external/boringssl/src/include /Iexternal/jsoncpp_git/include /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git/include /Iexternal/aws/aws-cpp-sdk-core/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-core/include /Iexternal/aws/aws-cpp-sdk-s3/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-s3/include /Iexternal/aws/aws-cpp-sdk-transfer/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-transfer/include /Iexternal/aws-c-common/include /Ibazel-out/x64_windows-opt/bin/external/aws-c-common/include /Iexternal/aws-c-event-stream/include /Ibazel-out/x64_windows-opt/bin/external/aws-c-event-stream/include /Iexternal/aws-checksums/include /Ibazel-out/x64_windows-opt/bin/external/aws-checksums/include /Iexternal/local_config_cuda/cuda/cusolver/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cusolver/include /Iexternal/local_config_cuda/cuda/cusparse/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cusparse/include /DCURL_STATICLIB /DPLATFORM_WINDOWS /DENABLE_CURL_CLIENT /DOPENSSL_IS_BORINGSSL /DTF_USE_SNAPPY /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /showIncludes /MD /O2 /DNDEBUG /w /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX2 /std:c++14 -x cuda -DGOOGLE_CUDA=1 -Xcuda-fatbinary=--compress-all --no-cuda-include-ptx=all --cuda-include-ptx=sm_61 --cuda-gpu-arch=sm_61 -DGOOGLE_CUDA=1 -DTENSORFLOW_USE_NVCC=1 -DTENSORFLOW_USE_XLA=1 -DINTEL_MKL=1 -DEIGEN_USE_VML -DENABLE_MKLDNN_V1 -DENABLE_INTEL_MKL_BFLOAT16 -DENABLE_MKL -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels_gpu/kernels_gpu.cu.o /c tensorflow/core/kernels/sparse/kernels_gpu.cu.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : Command line warning D9002 : ignoring unknown option '--no-cuda-include-ptx=all'\r\ncl : Command line warning D9002 : ignoring unknown option '--cuda-include-ptx=sm_61'\r\ncl : Command line warning D9002 : ignoring unknown option '--cuda-gpu-arch=sm_61'\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(114): warning: __host__ annotation is ignored on a function(\"no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(114): warning: __device__ annotation is ignored on a function(\"no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(115): warning: __host__ annotation is ignored on a function(\"no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(115): warning: __device__ annotation is ignored on a function(\"no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(115): warning: __host__ annotation is ignored on a function(\"~no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(115): warning: __device__ annotation is ignored on a function(\"~no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/Memory.h(85): warning: ignoring return value from routine declared with \"nodiscard\" attribute\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MathFunctions.h(1210): warning: calling a __host__ function from a __host__ __device__ function is not allowed\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/arch/Default/BFloat16.h(513): warning: calling a __host__ function from a __host__ __device__ function is not allowed\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/arch/Default/BFloat16.h(516): warning: calling a __host__ function from a __host__ __device__ function is not allowed\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/DenseBase.h(639): warning: __host__ annotation is ignored on a function(\"DenseBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/DenseBase.h(639): warning: __device__ annotation is ignored on a function(\"DenseBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(484): warning: __host__ annotation is ignored on a function(\"MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(484): warning: __device__ annotation is ignored on a function(\"MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(485): warning: __host__ annotation is ignored on a function(\"MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(485): warning: __device__ annotation is ignored on a function(\"MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(485): warning: __host__ annotation is ignored on a function(\"~MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(485): warning: __device__ annotation is ignored on a function(\"~MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(156): warning: __host__ annotation is ignored on a function(\"ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(156): warning: __device__ annotation is ignored on a function(\"ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(157): warning: __host__ annotation is ignored on a function(\"ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(157): warning: __device__ annotation is ignored on a function(\"ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(157): warning: __host__ annotation is ignored on a function(\"~ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(157): warning: __device__ annotation is ignored on a function(\"~ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseBinaryOp.h(105): warning: __host__ annotation is ignored on a function(\"CwiseBinaryOp\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseBinaryOp.h(105): warning: __device__ annotation is ignored on a function(\"CwiseBinaryOp\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(70): warning: __host__ annotation is ignored on a function(\"CwiseUnaryView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(70): warning: __device__ annotation is ignored on a function(\"CwiseUnaryView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(110): warning: __host__ annotation is ignored on a function(\"CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(110): warning: __device__ annotation is ignored on a function(\"CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(125): warning: __host__ annotation is ignored on a function(\"CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(125): warning: __device__ annotation is ignored on a function(\"CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(125): warning: __host__ annotation is ignored on a function(\"~CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(125): warning: __device__ annotation is ignored on a function(\"~CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(185): warning: __host__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(185): warning: __device__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(186): warning: __host__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(186): warning: __device__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(186): warning: __host__ annotation is ignored on a function(\"~MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(186): warning: __device__ annotation is ignored on a function(\"~MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(300): warning: __host__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(300): warning: __device__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(301): warning: __host__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(301): warning: __device__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(301): warning: __host__ annotation is ignored on a function(\"~MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(301): warning: __device__ annotation is ignored on a function(\"~MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Map.h(162): warning: __host__ annotation is ignored on a function(\"Map\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Map.h(162): warning: __device__ annotation is ignored on a function(\"Map\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Ref.h(90): warning: __host__ annotation is ignored on a function(\"RefBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Ref.h(90): warning: __device__ annotation is ignored on a function(\"RefBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Ref.h(232): warning: __host__ annotation is ignored on a function(\"Ref\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Ref.h(232): warning: __device__ annotation is ignored on a function(\"Ref\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(111): warning: __host__ annotation is ignored on a function(\"Block\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(111): warning: __device__ annotation is ignored on a function(\"Block\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(161): warning: __host__ annotation is ignored on a function(\"BlockImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(161): warning: __device__ annotation is ignored on a function(\"BlockImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(181): warning: __host__ annotation is ignored on a function(\"BlockImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(181): warning: __device__ annotation is ignored on a function(\"BlockImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(341): warning: __host__ annotation is ignored on a function(\"BlockImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(341): warning: __device__ annotation is ignored on a function(\"BlockImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/IndexedView.h(114): warning: __host__ annotation is ignored on a function(\"IndexedView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/IndexedView.h(114): warning: __device__ annotation is ignored on a function(\"IndexedView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(103): warning: __host__ annotation is ignored on a function(\"Reshaped\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(103): warning: __device__ annotation is ignored on a function(\"Reshaped\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(137): warning: __host__ annotation is ignored on a function(\"ReshapedImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(137): warning: __device__ annotation is ignored on a function(\"ReshapedImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(155): warning: __host__ annotation is ignored on a function(\"ReshapedImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(155): warning: __device__ annotation is ignored on a function(\"ReshapedImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(215): warning: __host__ annotation is ignored on a function(\"ReshapedImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(215): warning: __device__ annotation is ignored on a function(\"ReshapedImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(66): warning: __host__ annotation is ignored on a function(\"Transpose\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(66): warning: __device__ annotation is ignored on a function(\"Transpose\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(126): warning: __host__ annotation is ignored on a function(\"TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(126): warning: __device__ annotation is ignored on a function(\"TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(157): warning: __host__ annotation is ignored on a function(\"TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(157): warning: __device__ annotation is ignored on a function(\"TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(157): warning: __host__ annotation is ignored on a function(\"~TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(157): warning: __device__ annotation is ignored on a function(\"~TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Diagonal.h(78): warning: __host__ annotation is ignored on a function(\"Diagonal\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Diagonal.h(78): warning: __device__ annotation is ignored on a function(\"Diagonal\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(222): warning: __host__ annotation is ignored on a function(\"TriangularView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(222): warning: __device__ annotation is ignored on a function(\"TriangularView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(559): warning: __host__ annotation is ignored on a function(\"TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(559): warning: __device__ annotation is ignored on a function(\"TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(560): warning: __host__ annotation is ignored on a function(\"TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(560): warning: __device__ annotation is ignored on a function(\"TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(560): warning: __host__ annotation is ignored on a function(\"~TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(560): warning: __device__ annotation is ignored on a function(\"~TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reverse.h(90): warning: __host__ annotation is ignored on a function(\"Reverse\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reverse.h(90): warning: __device__ annotation is ignored on a function(\"Reverse\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayWrapper.h(47): warning: __host__ annotation is ignored on a function(\"ArrayWrapper\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayWrapper.h(47): warning: __device__ annotation is ignored on a function(\"ArrayWrapper\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayWrapper.h(145): warning: __host__ annotation is ignored on a function(\"MatrixWrapper\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayWrapper.h(145): warning: __device__ annotation is ignored on a function(\"MatrixWrapper\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\Eigen\\CXX11\\src/Tensor/TensorBlock.h(245): warning: invalid friend declaration\r\n\r\nC:\\bazel\\output_dir\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\Eigen\\CXX11\\src/Tensor/TensorBlock.h(709): warning: invalid friend declaration\r\n\r\nexternal/com_google_protobuf/src\\google/protobuf/map.h(1028): warning: invalid friend declaration\r\n\r\n.\\tensorflow/core/kernels/cuda_sparse.h(39): error: identifier \"cusparseDnMatDescr_t\" is undefined\r\n\r\n.\\tensorflow/core/kernels/cuda_sparse.h(40): error: identifier \"cusparseSpMatDescr_t\" is undefined\r\n\r\n.\\tensorflow/core/kernels/cuda_sparse.h(41): error: identifier \"cusparseSpMMAlg_t\" is undefined\r\n\r\n.\\tensorflow/core/lib/gtl/flatmap.h(157): warning: invalid friend declaration\r\n\r\n.\\tensorflow/core/platform/env.h(368): warning: overloaded virtual function \"tensorflow::Env::RegisterFileSystem\" is only partially overridden in class \"tensorflow::EnvWrapper\"\r\n\r\nexternal/com_google_absl\\absl/types/optional.h(428): warning: expression has no effect\r\n          detected during instantiation of \"const T &absl::lts_2020_02_25::optional<T>::operator*() const & [with T=stream_executor::dnn::AlgorithmDesc]\"\r\n.\\tensorflow/stream_executor/dnn.h(804): here\r\n\r\nexternal/com_google_absl\\absl/types/optional.h(428): warning: expression has no effect\r\n          detected during instantiation of \"const T &absl::lts_2020_02_25::optional<T>::operator*() const & [with T=size_t]\"\r\n.\\tensorflow/stream_executor/dnn.h(858): here\r\n\r\n3 errors detected in the compilation of \"C:/Users/Build/AppData/Local/Temp/nvcc_inter_files_tmp_dir/tmp47m_jwi8/kernels_gpu.cu.cpp1.ii\".", "CUDA 10.2 and cuDNN 7.6\r\n\r\nERROR: C:/tensorflow/tensorflow/core/kernels/BUILD:3644:1: C++ compilation of rule '//tensorflow/core/kernels:cuda_sparse' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/bazel/output_dir/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.26.28801\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.26.28801\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.26.28801\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Python38/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python38/lib/site-packages\r\n    SET TEMP=C:\\Users\\Build\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_ENABLE_XLA=1\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=C:\\Users\\Build\\AppData\\Local\\Temp\r\n  C:/Python38/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/mkl_dnn_v1 /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1 /Iexternal/mkl_windows /Ibazel-out/x64_windows-opt/bin/external/mkl_windows /Iexternal/cub_archive /Ibazel-out/x64_windows-opt/bin/external/cub_archive /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cusolver_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cufft_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/curand_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cusparse_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/cub_archive/_virtual_includes/cub /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/mkl_dnn_v1/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/include /Iexternal/mkl_dnn_v1/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src /Iexternal/mkl_dnn_v1/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/common /Iexternal/mkl_dnn_v1/src/cpu /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu /Iexternal/mkl_dnn_v1/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu/gemm /Iexternal/mkl_dnn_v1/src/cpu/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu/xbyak /Iexternal/mkl_windows/include /Ibazel-out/x64_windows-opt/bin/external/mkl_windows/include /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cublas/include /Iexternal/local_config_cuda/cuda/cusolver/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cusolver/include /Iexternal/local_config_cuda/cuda/cufft/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cufft/include /Iexternal/local_config_cuda/cuda/curand/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/curand/include /Iexternal/local_config_cuda/cuda/cusparse/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cusparse/include /DTF_USE_SNAPPY /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /showIncludes /MD /O2 /DNDEBUG /w /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX2 -nvcc_options=disable-warnings /std:c++14 -DGOOGLE_CUDA=1 -DTENSORFLOW_USE_NVCC=1 -DTENSORFLOW_USE_XLA=1 -DINTEL_MKL=1 -DEIGEN_USE_VML -DENABLE_MKLDNN_V1 -DENABLE_INTEL_MKL_BFLOAT16 -DENABLE_MKL -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY /DEIGEN_STRONG_INLINE=inline -DGOOGLE_CUDA=1 -DTENSORFLOW_USE_XLA=1 -DINTEL_MKL=1 -DENABLE_MKL /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/cuda_sparse/cuda_sparse.o /c tensorflow/core/kernels/cuda_sparse.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\n.\\tensorflow/core/kernels/cuda_sparse.h(39): error C2061: syntax error: identifier 'cusparseDnMatDescr_t'\r\n.\\tensorflow/core/kernels/cuda_sparse.h(40): error C2061: syntax error: identifier 'cusparseSpMatDescr_t'\r\n.\\tensorflow/core/kernels/cuda_sparse.h(41): error C2061: syntax error: identifier 'cusparseSpMMAlg_t'\r\n.\\tensorflow/core/kernels/cuda_sparse.h(290): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\n.\\tensorflow/core/kernels/cuda_sparse.h(290): error C2146: syntax error: missing ')' before identifier 'matA'\r\n.\\tensorflow/core/kernels/cuda_sparse.h(290): error C3646: 'matA': unknown override specifier\r\n.\\tensorflow/core/kernels/cuda_sparse.h(290): error C2988: unrecognizable template declaration/definition\r\n.\\tensorflow/core/kernels/cuda_sparse.h(290): error C2059: syntax error: ','\r\n.\\tensorflow/core/kernels/cuda_sparse.h(293): error C2059: syntax error: ')'\r\n.\\tensorflow/core/kernels/cuda_sparse.h(293): error C2238: unexpected token(s) preceding ';'\r\n.\\tensorflow/core/kernels/cuda_sparse.h(303): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\n.\\tensorflow/core/kernels/cuda_sparse.h(303): error C2146: syntax error: missing ')' before identifier 'matA'\r\n.\\tensorflow/core/kernels/cuda_sparse.h(303): error C3646: 'matA': unknown override specifier\r\n.\\tensorflow/core/kernels/cuda_sparse.h(303): error C2988: unrecognizable template declaration/definition\r\n.\\tensorflow/core/kernels/cuda_sparse.h(303): error C2059: syntax error: ','\r\n.\\tensorflow/core/kernels/cuda_sparse.h(306): error C2059: syntax error: ')'\r\n.\\tensorflow/core/kernels/cuda_sparse.h(306): error C2238: unexpected token(s) preceding ';'\r\ntensorflow/core/kernels/cuda_sparse.cc(448): error C2039: 'SpMMBufferSize': is not a member of 'tensorflow::GpuSparse'\r\n.\\tensorflow/core/kernels/cuda_sparse.h(189): note: see declaration of 'tensorflow::GpuSparse'\r\ntensorflow/core/kernels/cuda_sparse.cc(448): error C2988: unrecognizable template declaration/definition\r\ntensorflow/core/kernels/cuda_sparse.cc(448): error C2143: syntax error: missing ';' before '<'\r\ntensorflow/core/kernels/cuda_sparse.cc(448): error C2059: syntax error: '<'\r\ntensorflow/core/kernels/cuda_sparse.cc(448): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/cuda_sparse.cc(448): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/cuda_sparse.cc(448): error C2374: 'tensorflow::SpMMBufferSize': redefinition; multiple initialization\r\ntensorflow/core/kernels/cuda_sparse.cc(448): note: see declaration of 'tensorflow::SpMMBufferSize'\r\ntensorflow/core/kernels/cuda_sparse.cc(448): error C2059: syntax error: 'do'\r\ntensorflow/core/kernels/cuda_sparse.cc(448): error C2059: syntax error: 'if'\r\ntensorflow/core/kernels/cuda_sparse.cc(448): error C2059: syntax error: '}'\r\ntensorflow/core/kernels/cuda_sparse.cc(448): error C2059: syntax error: 'return'\r\ntensorflow/core/kernels/cuda_sparse.cc(448): error C2653: 'Status': is not a class or namespace name\r\ntensorflow/core/kernels/cuda_sparse.cc(448): error C2653: 'GpuSparse': is not a class or namespace name\r\ntensorflow/core/kernels/cuda_sparse.cc(448): error C2653: 'errors': is not a class or namespace name\r\ntensorflow/core/kernels/cuda_sparse.cc(464): error C2653: 'GpuSparse': is not a class or namespace name\r\ntensorflow/core/kernels/cuda_sparse.cc(464): error C2061: syntax error: identifier 'SpMM'\r\ntensorflow/core/kernels/cuda_sparse.cc(464): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/cuda_sparse.cc(464): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/cuda_sparse.cc(464): error C2988: unrecognizable template declaration/definition\r\ntensorflow/core/kernels/cuda_sparse.cc(464): error C2059: syntax error: 'do'\r\ntensorflow/core/kernels/cuda_sparse.cc(464): error C2059: syntax error: 'if'\r\ntensorflow/core/kernels/cuda_sparse.cc(464): error C2653: 'errors': is not a class or namespace name\r\ntensorflow/core/kernels/cuda_sparse.cc(464): error C2059: syntax error: '}'\r\ntensorflow/core/kernels/cuda_sparse.cc(464): error C2059: syntax error: 'return'\r\ntensorflow/core/kernels/cuda_sparse.cc(464): error C2653: 'Status': is not a class or namespace name\r\ntensorflow/core/kernels/cuda_sparse.cc(512): error C7525: inline variables require at least '/std:c++17'\r\ntensorflow/core/kernels/cuda_sparse.cc(512): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\ntensorflow/core/kernels/cuda_sparse.cc(512): error C2061: syntax error: identifier 'CsrmvExImpl'\r\ntensorflow/core/kernels/cuda_sparse.cc(519): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/cuda_sparse.cc(519): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/cuda_sparse.cc(551): error C7525: inline variables require at least '/std:c++17'\r\ntensorflow/core/kernels/cuda_sparse.cc(551): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\ntensorflow/core/kernels/cuda_sparse.cc(551): error C2374: 'Status': redefinition; multiple initialization\r\ntensorflow/core/kernels/cuda_sparse.cc(512): note: see declaration of 'Status'\r\ntensorflow/core/kernels/cuda_sparse.cc(551): error C2061: syntax error: identifier 'SpMVImpl'\r\ntensorflow/core/kernels/cuda_sparse.cc(558): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/cuda_sparse.cc(558): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/cuda_sparse.cc(612): error C2653: 'GpuSparse': is not a class or namespace name\r\ntensorflow/core/kernels/cuda_sparse.cc(612): error C2061: syntax error: identifier 'Csrmv'\r\ntensorflow/core/kernels/cuda_sparse.cc(612): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/cuda_sparse.cc(612): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/cuda_sparse.cc(612): error C2988: unrecognizable template declaration/definition\r\ntensorflow/core/kernels/cuda_sparse.cc(612): error C2059: syntax error: 'if'\r\ntensorflow/core/kernels/cuda_sparse.cc(612): error C2059: syntax error: '}'\r\ntensorflow/core/kernels/cuda_sparse.cc(659): error C7525: inline variables require at least '/std:c++17'\r\ntensorflow/core/kernels/cuda_sparse.cc(659): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\ntensorflow/core/kernels/cuda_sparse.cc(659): error C2374: 'Status': redefinition; multiple initialization\r\ntensorflow/core/kernels/cuda_sparse.cc(512): note: see declaration of 'Status'\r\ntensorflow/core/kernels/cuda_sparse.cc(659): error C2061: syntax error: identifier 'Csrgeam2Impl'\r\ntensorflow/core/kernels/cuda_sparse.cc(667): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/cuda_sparse.cc(667): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/cuda_sparse.cc(698): error C2653: 'GpuSparse': is not a class or namespace name\r\ntensorflow/core/kernels/cuda_sparse.cc(698): error C2061: syntax error: identifier 'Csrgeam'\r\ntensorflow/core/kernels/cuda_sparse.cc(698): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/cuda_sparse.cc(698): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/cuda_sparse.cc(698): error C2988: unrecognizable template declaration/definition\r\ntensorflow/core/kernels/cuda_sparse.cc(698): error C2059: syntax error: 'return'\r\ntensorflow/core/kernels/cuda_sparse.cc(698): error C2059: syntax error: '}'\r\ntensorflow/core/kernels/cuda_sparse.cc(720): error C7525: inline variables require at least '/std:c++17'\r\ntensorflow/core/kernels/cuda_sparse.cc(720): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\ntensorflow/core/kernels/cuda_sparse.cc(720): error C2374: 'Status': redefinition; multiple initialization\r\ntensorflow/core/kernels/cuda_sparse.cc(512): note: see declaration of 'Status'\r\ntensorflow/core/kernels/cuda_sparse.cc(720): error C2061: syntax error: identifier 'CsrgeamBufferSizeExtImpl'\r\ntensorflow/core/kernels/cuda_sparse.cc(728): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/cuda_sparse.cc(728): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/cuda_sparse.cc(759): error C2653: 'GpuSparse': is not a class or namespace name\r\ntensorflow/core/kernels/cuda_sparse.cc(759): error C2061: syntax error: identifier 'CsrgeamBufferSizeExt'\r\ntensorflow/core/kernels/cuda_sparse.cc(759): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/cuda_sparse.cc(759): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/cuda_sparse.cc(759): error C2988: unrecognizable template declaration/definition\r\ntensorflow/core/kernels/cuda_sparse.cc(759): error C2059: syntax error: 'return'\r\ntensorflow/core/kernels/cuda_sparse.cc(759): error C2059: syntax error: '}'\r\ntensorflow/core/kernels/cuda_sparse.cc(850): error C2653: 'GpuSparse': is not a class or namespace name\r\ntensorflow/core/kernels/cuda_sparse.cc(850): error C2061: syntax error: identifier 'CsrgemmBufferSize'\r\ntensorflow/core/kernels/cuda_sparse.cc(850): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/cuda_sparse.cc(850): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/cuda_sparse.cc(850): error C2988: unrecognizable template declaration/definition\r\ntensorflow/core/kernels/cuda_sparse.cc(850): error C2059: syntax error: 'do'\r\ntensorflow/core/kernels/cuda_sparse.cc(850): error C2059: syntax error: 'if'\r\ntensorflow/core/kernels/cuda_sparse.cc(850): error C2653: 'errors': is not a class or namespace name\r\ntensorflow/core/kernels/cuda_sparse.cc(850): error C2059: syntax error: '}'\r\ntensorflow/core/kernels/cuda_sparse.cc(850): error C2059: syntax error: 'return'\r\ntensorflow/core/kernels/cuda_sparse.cc(850): error C2825: 'Status': must be a class or namespace when followed by '::'\r\ntensorflow/core/kernels/cuda_sparse.cc(850): error C2510: 'Status': left of '::' must be a class/struct/union\r\ntensorflow/core/kernels/cuda_sparse.cc(852): error C2653: 'GpuSparse': is not a class or namespace name\r\ntensorflow/core/kernels/cuda_sparse.cc(852): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\ntensorflow/core/kernels/cuda_sparse.cc(852): error C2086: 'int Status': redefinition\r\ntensorflow/core/kernels/cuda_sparse.cc(512): note: see declaration of 'Status'\r\ntensorflow/core/kernels/cuda_sparse.cc(852): error C2146: syntax error: missing ';' before identifier 'CsrgemmNnz'\r\ntensorflow/core/kernels/cuda_sparse.cc(858): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/cuda_sparse.cc(858): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/cuda_sparse.cc(871): error C7525: inline variables require at least '/std:c++17'\r\ntensorflow/core/kernels/cuda_sparse.cc(871): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\ntensorflow/core/kernels/cuda_sparse.cc(871): error C2374: 'Status': redefinition; multiple initialization\r\ntensorflow/core/kernels/cuda_sparse.cc(512): note: see declaration of 'Status'\r\ntensorflow/core/kernels/cuda_sparse.cc(871): error C2061: syntax error: identifier 'CsrgemmImpl'\r\ntensorflow/core/kernels/cuda_sparse.cc(879): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/cuda_sparse.cc(879): fatal error C1003: error count exceeds 100; stopping compilation", "completely different CUDA related errors occur on different configurations (10.2/7.6, 10.2/8.0, 11.0/8.0).\r\nnone of the builds were successful. what's going on what to watch?", "@Expert73,\r\nPlease take a look at the [tested build configuration](https://www.tensorflow.org/install/source_windows#gpu) for the compatible dependencies required while installing TensorFlow. And provide the exact sequence of commands / steps that you executed before running into the problem.\r\n\r\nAlso, please attach the logs as a text file so that it would be easier for us to debug. Thanks!", "I have successfully built TensorFlow 2.3.0 with CUDA 11 and cuDNN 8 on Windows 10.\r\nNot sure if everything works correctly, but at least the build is successful. The simple CNN also worked fine.\r\n\r\nSince the error occurs in the CUDA SDK's `config.h`, you can avoid the problem by skipping the CUB_VERSION check.\r\nYou can avoid the checking process by building with the following command\r\n\r\n`bazel build --config=opt --config=avx2_win --config=short_logs --config=cuda --define=no_tensorflow_py_deps=true --copt=-DTHRUST_IGNORE_CUB_VERSION_CHECK --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nKey Points.\r\n* Use NumPy 1.18.5; 1.19.x will fail.\r\n* Add `--copt=-DTHRUST_IGNORE_CUB_VERSION_CHECK` to bazel parameter\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41803\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41803\">No</a>\n", "\n\nThanks!\u00a0\n\u00a0I also\u00a0 successfully built TensorFlow\u00a0 with CUDA 11 and cuDNN 8 on Windows 10 but in master branch.\n\u00a0", "build using numpy 1.19 also possible by following the solution in [https://github.com/tensorflow/tensorflow/issues/41086](https://github.com/tensorflow/tensorflow/issues/41086)"]}, {"number": 41801, "title": "Corrected citation: Graves et. al -2016 -> +2006", "body": "[Connectionist temporal classification](https://dl.acm.org/doi/10.1145/1143844.1143891) link and the paper suggests that the publication year is 2006 and not 2016.", "comments": []}, {"number": 41799, "title": "RuntimeError: external/org_tensorflow/tensorflow/lite/core/subgraph.cc:1044 required_bytes != bytes (602112 != 150528)", "body": "Training and conversion model equipment\uff1aubuntu18.04\r\ntf :2.2.0\r\ntb-nightly               1.14.0a20190603\r\ntf-estimator-nightly     1.14.0.dev2019060501\r\n\r\nI use Dev Board to predict\uff1a\r\nedgetpu  \uff1a 2.14.0       \r\ntflite-runtime\uff1a 2.1.0.post1  \r\n----------------------------------------------\r\nMy training and conversion model code:\r\n[2.2.0.ipynb.zip](https://github.com/tensorflow/tensorflow/files/4986035/2.2.0.ipynb.zip)\r\nhdf5 model:\r\n[0.03-0.98.hdf5.zip](https://github.com/tensorflow/tensorflow/files/4986041/0.03-0.98.hdf5.zip)\r\ntflite model:\r\n[st2_io.tflite.zip](https://github.com/tensorflow/tensorflow/files/4986048/st2_io.tflite.zip)\r\n\r\nPrediction program running on Dev Board\uff1a\r\n[classify_image.py.zip](https://github.com/tensorflow/tensorflow/files/4986066/classify_image.py.zip)\r\nedgetpu model(Model used for prediction\uff09:\r\n[st2_io_edgetpu.tflite.zip](https://github.com/tensorflow/tensorflow/files/4986053/st2_io_edgetpu.tflite.zip)\r\nlabel:\r\n[sparrow (1).txt](https://github.com/tensorflow/tensorflow/files/4986071/sparrow.1.txt)\r\nImage:\r\n[edgetest.zip](https://github.com/tensorflow/tensorflow/files/4986075/edgetest.zip)\r\n-------------------------------------------------------\r\nI got this error after executing classify_image.py on Dev Board\uff1a\r\n\r\nmendel@orange-eft:~/sparrow$ python3 classify_image.py --model st2_io_edgetpu.tflite \r\nTraceback (most recent call last):\r\n  File \"classify_image.py\", line 27, in <module>\r\n    main()\r\n  File \"classify_image.py\", line 21, in main\r\n    for result in engine.classify_with_image(img, top_k=3):\r\n  File \"/usr/lib/python3/dist-packages/edgetpu/classification/engine.py\", line 99, in classify_with_image\r\n    return self.classify_with_input_tensor(input_tensor, threshold, top_k)\r\n  File \"/usr/lib/python3/dist-packages/edgetpu/classification/engine.py\", line 123, in classify_with_input_tensor\r\n    input_tensor)\r\n  File \"/usr/lib/python3/dist-packages/edgetpu/basic/basic_engine.py\", line 136, in run_inference\r\n    result = self._engine.RunInference(input)\r\n  File \"/usr/lib/python3/dist-packages/edgetpu/swig/edgetpu_cpp_wrapper.py\", line 111, in RunInference\r\n    return _edgetpu_cpp_wrapper.BasicEnginePythonWrapper_RunInference(self, input)\r\nRuntimeError: external/org_tensorflow/tensorflow/lite/core/subgraph.cc:1044 required_bytes != bytes (602112 != 150528)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["is there any solution yet? I am facing the same problem here", "is there any solution yet? I am facing the same problem here", "@LINYOUWEI0804,\r\n\r\nWe are checking to see if this is still an issue, Can you upgrade your tensorflow to latest stable version i.e `2.6.0` and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41799\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41799\">No</a>\n"]}, {"number": 41797, "title": "Keras saves invalid JSON files containing Infinity", "body": "**Describe the current behavior**\r\n\r\nJSON saved by Keras contains `Infinity` which is invalid according to [RFC 7159](https://tools.ietf.org/html/rfc7159):\r\n\r\n> \"Numeric values that cannot be represented in the grammar below (such as Infinity and NaN) are not permitted.\"\r\n\r\n**Describe the expected behavior**\r\n\r\nKeras saves correct JSON format.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```\r\nfrom tensorflow import keras\r\ninput = keras.Input(shape=(1))\r\nx = keras.backend.sqrt(input)\r\nmodel = keras.Model(input, x)\r\nmodel.compile(optimizer='adam', loss='mse')\r\nwith open('repro.json', 'w') as json_file:\r\n    json_file.write(model.to_json())\r\n```\r\n\r\n```\r\n~ node\r\n> JSON.parse(require('fs').readFileSync('repro.json', 'utf-8'))\r\nUncaught SyntaxError: Unexpected token I in JSON at position 508\r\n```\r\n\r\n[repro.zip](https://github.com/tensorflow/tensorflow/files/4985883/repro.zip)\r\n\r\ntensorflow/tensorflow#37196\r\nlutzroeder/netron#553\r\n\r\n@goldiegadde @howl-anderson", "comments": ["@lutzroeder \r\n\r\nIn the JSON file you uploaded I see a value named `infinity `but there is no value `identity.`\r\nOn running the code in colab with TF 2.2 I see neither `infinity` nor `identity`. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/a9f1d86821c63ef8227129b3e710dcb2/untitled191.ipynb).Thanks!", "@ravikyram good catch, corrected to `Infinity`.\r\n\r\nThis does repro in `tensorflow 2.3.0` on macOS and @imxieyi reported lutzroeder/netron#553 for `Keras v2.2.4-tf`. The Python JSON parser by default (incorrectly) supports `Infinity` but other parsers like `JSON.parse` will fail. \r\n\r\nSee also https://docs.python.org/3/library/json.html:\r\n\r\n> If allow_nan is false (default: True), then it will be a ValueError to serialize out of range float values (nan, inf, -inf) in strict compliance of the JSON specification. If allow_nan is true, their JavaScript equivalents (NaN, Infinity, -Infinity) will be used.", "I think it is the same issue as #37196, I will have a deep look at it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41797\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41797\">No</a>\n"]}, {"number": 41795, "title": "Add qint8/qint16/quint8/quint16 support for tf.math.equal/tf/math.not_equal", "body": "This PR add qint8/qint16/quint8/quint16 support for tf.math.equal/tf/math.not_equal,\r\nas was requested in #26069.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 41793, "title": "Add transactional API as separate methods", "body": "This PR adds Transactions API as separate empty methods to workaround the issues described in #41615 . Follow up PRs will replace the #41615 in steps.", "comments": ["@mihaimaruseac, lets merge this and I will change it when I enable filesystem implementations.", "Yes, let's merge as is and fix in upcoming one", "Will import and fix internal tests manually", "Curious why only the windows presubmits failed with this error (internal clang compile also fails)\r\n\r\n```\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(115) : error C4716: 'tensorflow::FileSystem::NewAppendableFile': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(78) : error C4716: 'tensorflow::FileSystem::NewRandomAccessFile': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(147) : error C4716: 'tensorflow::FileSystem::FilesExist': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(157) : error C4716: 'tensorflow::FileSystem::GetChildren': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(138) : error C4716: 'tensorflow::FileSystem::FileExists': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(200) : error C4716: 'tensorflow::FileSystem::Stat': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(186) : error C4716: 'tensorflow::FileSystem::GetMatchingPaths': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(216) : error C4716: 'tensorflow::FileSystem::CreateDir': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(227) : error C4716: 'tensorflow::FileSystem::RecursivelyCreateDir': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(233) : error C4716: 'tensorflow::FileSystem::DeleteDir': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(266) : error C4716: 'tensorflow::FileSystem::DeleteRecursively': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(273) : error C4716: 'tensorflow::FileSystem::GetFileSize': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(280) : error C4716: 'tensorflow::FileSystem::RenameFile': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(286) : error C4716: 'tensorflow::FileSystem::CopyFile': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(309) : error C4716: 'tensorflow::FileSystem::IsDirectory': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(132) : error C4716: 'tensorflow::FileSystem::NewReadOnlyMemoryRegionFromFile': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(97) : error C4716: 'tensorflow::FileSystem::NewWritableFile': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\file_system.h(206) : error C4716: 'tensorflow::FileSystem::DeleteFile': must return a value\r\n```", "Btw, since I imported manually, I also fixed the token order"]}, {"number": 41791, "title": "Cache hashable input signature for _cache_key", "body": "Evaluations of with-signature benchmarks pending merge of PR #41714 ", "comments": ["About a 10us speedup from roughly 190us to 180us, observed as an approximate median over 5-10 trials with 100000 iterations each on benchmark_defun_matmul_2_by_2_CPU_with_signature"]}, {"number": 41790, "title": "Fix invalid fusion of Matmul and Mul", "body": "This PR fixes #39572 which lead to wrong operator fusion in the TFLite converter for some cases.\r\n\r\nThis is a resubmission of #40013 which has already be reviewed in detail there.\r\n\r\n@rthadur @liufengdb could you take a look?", "comments": ["@liufengdb Could you please take a look?", "Any updates on this?", "@joker-eph @liufengdb @karimnosseir @gbaned Any news on when this PR can get a review? I think this PR fixes a pretty serious bug in TFLite so it would be great if this could make it into the 2.4 release.", "Any updates on this? The main changes of this PR have already been reviewed in #40013 and discussed in #39572 which is a major issue for people using fully connected layers with batch normalisation in TFLite.", "@liufengdb @karimnosseir Any update on this PR? Please. Thanks!", "Any news?", "@mihaimaruseac Sorry to ping you here, it looks like this PR is stuck in the pipeline.\r\nIt fixes #39572 which has been a serious issue for us since TF 2.2 and is a resubmission of #40013 which has been abandoned but already reviewed in detail.\r\nIt would be really good to get this reviewed and merged into 2.4 before the release branch get's cut.", "That got lost for me. I am sorry. I think this pattern was added by @liufengdb, so deferring to him to review.\r\n@lgeiger if got delayed more, ping me and i will check.\r\nApologies again.", "Thanks for the response, looking forward to the review.", "@karimnosseir For reference, @liufengdb reviewed the content of this PR in #40013 which wasn't merged since https://github.com/tensorflow/tensorflow/pull/40013#discussion_r452659910 wasn't addressed and the PR abandoned by the original author.", "@karimnosseir Any updates?", "@karimnosseir Thanks for the review. I rebased the PR onto latest master in b72545ee21dfdc1fde0424a87523536c353f3ce3 and clarified the comments and cleaned up the unittests according the the review suggestions.\r\n\r\nSorry about some of the unclear descriptions, I adapted most of the code from #40013 without too much thinking. I hope this is now clearer to read.", "@gbaned looks like there are quite a few internal CI failures. Unfortunately I cannot access the logs, is there anything that is still preventing this from being merged from my side?", "@lgeiger  It is processing internally, we will let you know if anything required from you. Thank you!", "@gbaned any updates state of the internal processing?", "@lgeiger could you resolve the merge conflict?", "@lgeiger Nevermind. I will handle internal reviews."]}, {"number": 41789, "title": "Error computing gradient of a vectorized scan loop", "body": "This code snippet uses `tf.scan` to compute a sum of squared residuals, wraps it in `tf.vectorized_map`, and attempts to take the gradient:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nnp.random.seed(seed=42)\r\ndata = np.random.randn(100).astype(np.float32)\r\n\r\ndef log_prob(x):\r\n  return tf.reduce_sum(tf.scan(\r\n      lambda _, yi: (x - yi)**2,\r\n      elems=data,\r\n      initializer=tf.convert_to_tensor(0.),))\r\n\r\nx = tf.Variable(tf.random.normal([10]))\r\nv_log_prob = lambda x: tf.vectorized_map(log_prob, x)\r\nwith tf.GradientTape() as tape:\r\n  lp = tf.reduce_sum(v_log_prob(x))\r\ng = tape.gradient(lp, x)\r\n```\r\n\r\nI would expect the gradient to be a vector of the same size as `x`. Instead it raises an exception `InvalidArgumentError: Operation 'loop_body/scan/while/pfor/PartitionedCall' has no attr named '_XlaCompile'.`.\r\n\r\n<details><summary>Full stack trace</summary>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2512       with c_api_util.tf_buffer() as buf:\r\n-> 2513         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\n   2514         data = pywrap_tf_session.TF_GetBuffer(buf)\r\n\r\nInvalidArgumentError: Operation 'loop_body/scan/while/pfor/PartitionedCall' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n67 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    330     try:\r\n--> 331       xla_compile = op.get_attr(\"_XlaCompile\")\r\n    332       xla_separate_compiled_gradients = op.get_attr(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2516       # Convert to ValueError for backwards compatibility.\r\n-> 2517       raise ValueError(str(e))\r\n   2518     x = attr_value_pb2.AttrValue()\r\n\r\nValueError: Operation 'loop_body/scan/while/pfor/PartitionedCall' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2512       with c_api_util.tf_buffer() as buf:\r\n-> 2513         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\n   2514         data = pywrap_tf_session.TF_GetBuffer(buf)\r\n\r\nInvalidArgumentError: Operation 'while' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    330     try:\r\n--> 331       xla_compile = op.get_attr(\"_XlaCompile\")\r\n    332       xla_separate_compiled_gradients = op.get_attr(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2516       # Convert to ValueError for backwards compatibility.\r\n-> 2517       raise ValueError(str(e))\r\n   2518     x = attr_value_pb2.AttrValue()\r\n\r\nValueError: Operation 'while' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2512       with c_api_util.tf_buffer() as buf:\r\n-> 2513         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\n   2514         data = pywrap_tf_session.TF_GetBuffer(buf)\r\n\r\nInvalidArgumentError: Operation 'while/while_body/cond' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    330     try:\r\n--> 331       xla_compile = op.get_attr(\"_XlaCompile\")\r\n    332       xla_separate_compiled_gradients = op.get_attr(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2516       # Convert to ValueError for backwards compatibility.\r\n-> 2517       raise ValueError(str(e))\r\n   2518     x = attr_value_pb2.AttrValue()\r\n\r\nValueError: Operation 'while/while_body/cond' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2512       with c_api_util.tf_buffer() as buf:\r\n-> 2513         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\n   2514         data = pywrap_tf_session.TF_GetBuffer(buf)\r\n\r\nInvalidArgumentError: Operation 'while/while_body/cond/PartitionedCall' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    330     try:\r\n--> 331       xla_compile = op.get_attr(\"_XlaCompile\")\r\n    332       xla_separate_compiled_gradients = op.get_attr(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2516       # Convert to ValueError for backwards compatibility.\r\n-> 2517       raise ValueError(str(e))\r\n   2518     x = attr_value_pb2.AttrValue()\r\n\r\nValueError: Operation 'while/while_body/cond/PartitionedCall' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2512       with c_api_util.tf_buffer() as buf:\r\n-> 2513         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\n   2514         data = pywrap_tf_session.TF_GetBuffer(buf)\r\n\r\nInvalidArgumentError: Operation 'loop_body/scan/while/TensorArrayV2Write/TensorListSetItem/pfor/Tile' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    330     try:\r\n--> 331       xla_compile = op.get_attr(\"_XlaCompile\")\r\n    332       xla_separate_compiled_gradients = op.get_attr(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2516       # Convert to ValueError for backwards compatibility.\r\n-> 2517       raise ValueError(str(e))\r\n   2518     x = attr_value_pb2.AttrValue()\r\n\r\nValueError: Operation 'loop_body/scan/while/TensorArrayV2Write/TensorListSetItem/pfor/Tile' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-230-367bca75a1b1> in <module>()\r\n      1 with tf.GradientTape() as tape:\r\n----> 2   lp = v_log_prob(x)\r\n      3 g = tape.gradient(lp, x)\r\n\r\n<ipython-input-229-89f89e42229e> in <lambda>(x)\r\n      1 x = tf.Variable(tf.random.normal([10]))\r\n----> 2 v_log_prob = lambda x: tf.vectorized_map(log_prob, x)\r\n      3 print(v_log_prob(x))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py in vectorized_map(fn, elems, fallback_to_while_loop)\r\n    448     batch_size = array_ops.shape(first_elem)[0]\r\n    449   return pfor(loop_fn, batch_size,\r\n--> 450               fallback_to_while_loop=fallback_to_while_loop)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py in pfor(loop_fn, iters, fallback_to_while_loop, parallel_iterations)\r\n    202       def_function.run_functions_eagerly(False)\r\n    203     f = def_function.function(f)\r\n--> 204   outputs = f()\r\n    205   if functions_run_eagerly is not None:\r\n    206     def_function.run_functions_eagerly(functions_run_eagerly)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    794       else:\r\n    795         compiler = \"nonXla\"\r\n--> 796         result = self._call(*args, **kwds)\r\n    797 \r\n    798       new_tracing_count = self._get_tracing_count()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    860               *args, **kwds)\r\n    861       # If we did not create any variables the trace we have is good enough.\r\n--> 862       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n    863 \r\n    864     def fn_with_cond(*inner_args, **inner_kwds):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)\r\n   1856                            resource_variable_ops.BaseResourceVariable))],\r\n   1857         captured_inputs=self.captured_inputs,\r\n-> 1858         cancellation_manager=cancellation_manager)\r\n   1859 \r\n   1860   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1937         possible_gradient_type,\r\n   1938         executing_eagerly)\r\n-> 1939     forward_function, args_with_tangents = forward_backward.forward()\r\n   1940     if executing_eagerly:\r\n   1941       flat_outputs = forward_function.call(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in forward(self)\r\n   1442     \"\"\"Builds or retrieves a forward function for this call.\"\"\"\r\n   1443     forward_function = self._functions.forward(\r\n-> 1444         self._inference_args, self._input_tangents)\r\n   1445     return forward_function, self._inference_args + self._input_tangents\r\n   1446 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in forward(self, inference_args, input_tangents)\r\n   1194       (self._forward, self._forward_graph, self._backward,\r\n   1195        self._forwardprop_output_indices, self._num_forwardprop_outputs) = (\r\n-> 1196            self._forward_and_backward_functions(inference_args, input_tangents))\r\n   1197     return self._forward\r\n   1198 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _forward_and_backward_functions(self, inference_args, input_tangents)\r\n   1346     outputs = self._func_graph.outputs[:self._num_inference_outputs]\r\n   1347     return self._build_functions_for_outputs(\r\n-> 1348         outputs, inference_args, input_tangents)\r\n   1349 \r\n   1350 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _build_functions_for_outputs(self, outputs, inference_args, input_tangents)\r\n    904             self._func_graph.inputs,\r\n    905             grad_ys=gradients_wrt_outputs,\r\n--> 906             src_graph=self._func_graph)\r\n    907 \r\n    908       captures_from_forward = [\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    681                 # functions.\r\n    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 683                                          lambda: grad_fn(op, *out_grads))\r\n    684               else:\r\n    685                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    334       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    335     except ValueError:\r\n--> 336       return grad_fn()  # Exit early\r\n    337 \r\n    338   if not xla_compile:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in <lambda>()\r\n    681                 # functions.\r\n    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 683                                          lambda: grad_fn(op, *out_grads))\r\n    684               else:\r\n    685                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _rewrite_forward_and_call_backward(self, op, *doutputs)\r\n    717   def _rewrite_forward_and_call_backward(self, op, *doutputs):\r\n    718     \"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\r\n--> 719     forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n    720     if not backwards_function.outputs:\r\n    721       return backwards_function.structured_outputs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in forward_backward(self, num_doutputs)\r\n    626     if forward_backward is not None:\r\n    627       return forward_backward\r\n--> 628     forward, backward = self._construct_forward_backward(num_doutputs)\r\n    629     self._cached_function_pairs[num_doutputs] = (forward, backward)\r\n    630     return forward, backward\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _construct_forward_backward(self, num_doutputs)\r\n    674           args=[], kwargs={},\r\n    675           signature=signature,\r\n--> 676           func_graph=backwards_graph)\r\n    677       backwards_graph_captures = backwards_graph.external_captures\r\n    678       captures_from_forward = [\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _backprop_function(*grad_ys)\r\n    664             self._func_graph.inputs,\r\n    665             grad_ys=grad_ys,\r\n--> 666             src_graph=self._func_graph)\r\n    667 \r\n    668     with self._func_graph.as_default():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    681                 # functions.\r\n    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 683                                          lambda: grad_fn(op, *out_grads))\r\n    684               else:\r\n    685                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    334       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    335     except ValueError:\r\n--> 336       return grad_fn()  # Exit early\r\n    337 \r\n    338   if not xla_compile:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in <lambda>()\r\n    681                 # functions.\r\n    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 683                                          lambda: grad_fn(op, *out_grads))\r\n    684               else:\r\n    685                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py in _WhileGrad(op, *grads)\r\n    352   body_grad_graph, args = _create_grad_func(\r\n    353       ys, xs, non_none_grads, cond_graph, body_graph,\r\n--> 354       util.unique_grad_fn_name(body_graph.name), op, maximum_iterations)\r\n    355 \r\n    356   if body_grad_graph.while_op_needs_rewrite:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py in _create_grad_func(ys, xs, grads, cond_graph, body_graph, name, while_op, maximum_iterations)\r\n    624       func_graph=_WhileBodyGradFuncGraph(name, cond_graph, body_graph,\r\n    625                                          maximum_iterations, while_op,\r\n--> 626                                          body_graph_inputs, body_graph_outputs))\r\n    627 \r\n    628   # Update the list of outputs with tensors corresponding to the captured\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py in <lambda>(*args)\r\n    620   grad_func_graph = func_graph_module.func_graph_from_py_func(\r\n    621       name,\r\n--> 622       lambda *args: _grad_fn(ys, xs, args, body_graph),\r\n    623       args, {},\r\n    624       func_graph=_WhileBodyGradFuncGraph(name, cond_graph, body_graph,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py in _grad_fn(ys, xs, args, func_graph)\r\n    680   grad_outs = gradients_util._GradientsHelper(\r\n    681       ys, xs, grad_ys=grad_ys, src_graph=func_graph,\r\n--> 682       unconnected_gradients=\"zero\")\r\n    683 \r\n    684   # TODO(b/118712257): Handle the case when grad_outs has None's e.g. when there\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    681                 # functions.\r\n    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 683                                          lambda: grad_fn(op, *out_grads))\r\n    684               else:\r\n    685                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    334       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    335     except ValueError:\r\n--> 336       return grad_fn()  # Exit early\r\n    337 \r\n    338   if not xla_compile:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in <lambda>()\r\n    681                 # functions.\r\n    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 683                                          lambda: grad_fn(op, *out_grads))\r\n    684               else:\r\n    685                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py in _IfGrad(op, *grads)\r\n    119   # functions.\r\n    120   true_grad_graph = _create_grad_func(\r\n--> 121       true_graph, grads, util.unique_grad_fn_name(true_graph.name))\r\n    122   false_grad_graph = _create_grad_func(\r\n    123       false_graph, grads, util.unique_grad_fn_name(false_graph.name))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py in _create_grad_func(func_graph, grads, name)\r\n    382       name,\r\n    383       lambda: _grad_fn(func_graph, grads), [], {},\r\n--> 384       func_graph=_CondGradFuncGraph(name, func_graph))\r\n    385 \r\n    386 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py in <lambda>()\r\n    381   return func_graph_module.func_graph_from_py_func(\r\n    382       name,\r\n--> 383       lambda: _grad_fn(func_graph, grads), [], {},\r\n    384       func_graph=_CondGradFuncGraph(name, func_graph))\r\n    385 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py in _grad_fn(func_graph, grads)\r\n    372   result = gradients_util._GradientsHelper(\r\n    373       ys, func_graph.inputs, grad_ys=grad_ys,\r\n--> 374       src_graph=func_graph)\r\n    375 \r\n    376   return result\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    681                 # functions.\r\n    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 683                                          lambda: grad_fn(op, *out_grads))\r\n    684               else:\r\n    685                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    334       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    335     except ValueError:\r\n--> 336       return grad_fn()  # Exit early\r\n    337 \r\n    338   if not xla_compile:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in <lambda>()\r\n    681                 # functions.\r\n    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 683                                          lambda: grad_fn(op, *out_grads))\r\n    684               else:\r\n    685                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _rewrite_forward_and_call_backward(self, op, *doutputs)\r\n    717   def _rewrite_forward_and_call_backward(self, op, *doutputs):\r\n    718     \"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\r\n--> 719     forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n    720     if not backwards_function.outputs:\r\n    721       return backwards_function.structured_outputs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in forward_backward(self, num_doutputs)\r\n    626     if forward_backward is not None:\r\n    627       return forward_backward\r\n--> 628     forward, backward = self._construct_forward_backward(num_doutputs)\r\n    629     self._cached_function_pairs[num_doutputs] = (forward, backward)\r\n    630     return forward, backward\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _construct_forward_backward(self, num_doutputs)\r\n    674           args=[], kwargs={},\r\n    675           signature=signature,\r\n--> 676           func_graph=backwards_graph)\r\n    677       backwards_graph_captures = backwards_graph.external_captures\r\n    678       captures_from_forward = [\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _backprop_function(*grad_ys)\r\n    664             self._func_graph.inputs,\r\n    665             grad_ys=grad_ys,\r\n--> 666             src_graph=self._func_graph)\r\n    667 \r\n    668     with self._func_graph.as_default():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    681                 # functions.\r\n    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 683                                          lambda: grad_fn(op, *out_grads))\r\n    684               else:\r\n    685                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    334       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    335     except ValueError:\r\n--> 336       return grad_fn()  # Exit early\r\n    337 \r\n    338   if not xla_compile:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in <lambda>()\r\n    681                 # functions.\r\n    682                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 683                                          lambda: grad_fn(op, *out_grads))\r\n    684               else:\r\n    685                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py in _TileGrad(op, grad)\r\n    827         grad.values, math_ops.mod(grad.indices, input_shape_0), input_shape_0)\r\n    828     split_shape = array_ops.concat([[1], split_shape[1:]], axis=0)\r\n--> 829   input_grad = math_ops.reduce_sum(array_ops.reshape(grad, split_shape), axes)\r\n    830   # Fix shape inference\r\n    831   if not context.executing_eagerly():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in reduce_sum(input_tensor, axis, keepdims, name)\r\n   2001 \r\n   2002   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\r\n-> 2003                               _ReductionDims(input_tensor, axis))\r\n   2004 \r\n   2005 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in reduce_sum_with_dims(input_tensor, axis, keepdims, name, dims)\r\n   2012   return _may_reduce_to_scalar(\r\n   2013       keepdims, axis,\r\n-> 2014       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\r\n   2015 \r\n   2016 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in _sum(input, axis, keep_dims, name)\r\n  10537   _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n  10538         \"Sum\", input=input, reduction_indices=axis, keep_dims=keep_dims,\r\n> 10539                name=name)\r\n  10540   _result = _outputs[:]\r\n  10541   if _execute.must_record_gradient():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\r\n    607             _SatisfiesTypeConstraint(base_type,\r\n    608                                      _Attr(op_def, input_arg.type_attr),\r\n--> 609                                      param_name=input_name)\r\n    610           attrs[input_arg.type_attr] = attr_value\r\n    611           inferred_from[input_arg.type_attr] = input_name\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)\r\n     59           \"allowed values: %s\" %\r\n     60           (param_name, dtypes.as_dtype(dtype).name,\r\n---> 61            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\n     62 \r\n     63 \r\n\r\nTypeError: Value passed to parameter 'input' has DataType variant not in list of allowed values: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, float16, uint32, uint64\r\n```\r\n</details>\r\n\r\nHere's a colab notebook reproducing the issue with the current tf nightly:\r\nhttps://colab.sandbox.google.com/drive/14ytDF-74jvDYtJXff0IktJLBin_BuKYJ#scrollTo=GvZjAFbX_-Un\r\n\r\nAshish, any idea where this might be coming from?\r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/00a5a6362cb18c09336d2369af70b4f1/41789-2-3.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/4deafc99c656bbb30e95e48af55bd2f5/41789-tf-nightly.ipynb). \r\n\r\nWhereas, running the code with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/1a4b9e46d97c471cfd0390f83761d0ac/41789-2-2.ipynb#scrollTo=B8GR_KaP1brK) throws an error stating `    UnrecognizedFlagError: Unknown command line flag 'f'`\r\n\r\nPlease find the attached gist. Thanks!", "Hello @amahendrakar \r\nThanks for testing this!\r\nI remembered that I also encountered this `UnrecognizedFlagError: Unknown command line flag 'f'` error using TF v2.2 and solved it using discussions in #17702. [Colab notebook](https://colab.research.google.com/drive/1X7IJi78VRx-wvg74lVWpMjmi4zrGAznq?usp=sharing). But resolving `f flag` issue resulted in many other errors. ", "I have checked the code snippet with latest `'2.4.0-dev20200919'` tensorflow version and still I am getting the same error message. Any updates on this @agarwal-ashish . Thanks", "Thank you for the report. Should be fixed with https://github.com/tensorflow/tensorflow/commit/e01adec56da68c9335bb9a7736c82b0f66ddcc43", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41789\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41789\">No</a>\n"]}, {"number": 41788, "title": "[XLA] Make LLVM code easier to read.", "body": "We know which LLVM variable correspond to the tanh.", "comments": []}, {"number": 41787, "title": "RuntimeError: tensorflow/lite/kernels/detection_postprocess.cc:158 NumOutputs(node) != 4 (3 != 4)Node number 180 (TFLite_Detection_PostProcess) failed to prepare.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip install -ignore-installed --upgrade tensorflow-gpu==1.15\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nFULL ERROR\r\nC:\\Users\\Sreed\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\r\nC:\\Users\\Sreed\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\r\nC:\\Users\\Sreed\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\r\n  stacklevel=1)\r\n2020-07-27 23:06:48.543673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\nTraceback (most recent call last):\r\n  File \"TFLite_detection_webcam.py\", line 140, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"C:\\Users\\Sreed\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\lite\\python\\interpreter.py\", line 244, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"C:\\Users\\Sreed\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\lite\\python\\interpreter_wrapper\\tensorflow_wrap_interpreter_wrapper.py\", line 106, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/detection_postprocess.cc:158 NumOutputs(node) != 4 (3 != 4)Node number 180 (TFLite_Detection_PostProcess) failed to prepare.\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n\r\nCode from https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/TFLite_detection_webcam.py by someone else. \r\nRight now have a detect.tflite file as well as a labelmap.txt file", "comments": ["@Dasinator21 \r\nCan you please share simple indented stand alone code such that we can replicate the issue faced or if possible provide colab gist with the code and error.\r\n\r\nWith the error faced please refer to below issues:\r\n#26289 #22794 [as per these issues please verify your setting and requirements]", "https://github.com/Dasinator21/Replicate-Error\r\nThis will hopefully allow someone to get to the same error.", "@Dasinator21 \r\nPlease provide with a colab gist with the error faced for us to analyse the issue faced or simple stand lone code for us to replicate.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41786, "title": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FAKE_QUANT, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip installed\r\n- TensorFlow version (or github SHA if from source): 1.15\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nI can make the tflite file by adding --allow_custom_ops at the end of the conversion code. toco --graph_def_file=\"C:\\tensorflow5\\models\\research\\object_detection\\TFLite_model\\tflite_graph.pb\" --output_file=tflite_graph.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,300,300,3 --input_array=normalized_input_image_tensor --output_array=TFLite_Detection_PostProcess:2 --inference_type=FLOAT --input_type=FLOAT --allow_custom_ops \r\nhowever when I run the model for inference, I get an runtime error saying node number 180 (TFLite_Detection_PostProcess) failed to prepare.\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.[\r\n\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["Recommend trying in 2.3/nightly and also using a much readable title, not the full error message.", "You can also refer [tf lite object detection guide](https://www.tensorflow.org/lite/models/object_detection/overview) for your case.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41785, "title": "failed call to cuInit: UNKNOWN ERROR (-1)", "body": "Running the latest docker with:\r\n\r\n    docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu-jupyter jupyter notebook --notebook-dir=/tf --ip 0.0.0.0 --no-browser --allow-root --NotebookApp.allow_origin='https://colab.research.google.com'\r\n\r\ncode:\r\n\r\n    import tensorflow as tf\r\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n\r\ngives me:\r\n\r\n    2020-07-27 19:44:03.826149: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n    2020-07-27 19:44:03.826179: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (-1)\r\n    2020-07-27 19:44:03.826201: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\r\n\r\n\r\nI'm on Pop_OS 20.04, have tried installing the CUDA drivers from the Pop repository as well as from NVidia. No dice. Any help appreciated.\r\n\r\nRunning \r\n\r\n    docker run --gpus all nvidia/cuda:10.0-base nvidia-smi\r\n\r\ngives me:\r\n\r\n    +-----------------------------------------------------------------------------+\r\n    | NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |\r\n    |-------------------------------+----------------------+----------------------+\r\n    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n    |                               |                      |               MIG M. |\r\n    |===============================+======================+======================|\r\n    |   0  GeForce RTX 2080    On   | 00000000:09:00.0  On |                  N/A |\r\n    |  0%   52C    P5    15W / 225W |    513MiB /  7959MiB |     17%      Default |\r\n    |                               |                      |                  N/A |\r\n    +-------------------------------+----------------------+----------------------+\r\n                                                                                   \r\n    +-----------------------------------------------------------------------------+\r\n    | Processes:                                                                  |\r\n    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n    |        ID   ID                                                   Usage      |\r\n    |=============================================================================|\r\n    +-----------------------------------------------------------------------------+\r\n\r\n", "comments": ["@prismspecs,\r\nPlease check if [this comment](https://github.com/NVIDIA/nvidia-docker/issues/1330#issuecomment-664069187) from your issue in the NVIDIA repo works.\r\n\r\nAlso, please take a look at [this thread](https://gridforums.nvidia.com/default/topic/10801/general-discussion/unable-to-run-tensorflow-with-vgpu/post/16395/#16395) on the NVIDIA forum and let us know if it helps. Thanks!", "I simply needed to add --gpus all\r\n\r\nI feel like this should be default since it's a GPU based docker image no?\r\n\r\nThanks!", "@prismspecs,\r\nIs this still an issue? Always refer to the [official guide](https://www.tensorflow.org/install/docker#gpu_support) while installing TensorFlow. \r\n\r\nPlease feel free to close the issue if resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41785\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41785\">No</a>\n"]}, {"number": 41784, "title": "C++ compilation of rule '//tensorflow/core/kernels:strided_slice_op_gpu' failed (Exit 2)", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., windows 10):\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):2.0.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 10.1, cudnn 7.6.0\r\n- GPU model and memory:  2060rtx, 16GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npython -m pip install --upgrade pip\r\npython -m venv C:\\Users\\aya\\tensorflow-v2.2.0\\.venv\r\nC:\\Users\\aya\\tensorflow-v2.2.0\\.venv\\Scripts\\activate.bat\r\ncd C:\\Users\\aya\\tensorflow-v2.2.0\\.venv\r\ndir\r\n.\\Scripts\\activate\r\ncd C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\r\npython -m pip install --upgrade pip\r\npip install setuptools --upgrade\r\npip install six numpy wheel   #(numpy==1.17.0 not 1.19.0) pip install numpy==1.17.0\r\npip install keras_applications==1.0.8 --no-deps\r\npip install keras_preprocessing==1.1.1 --no-deps\r\npip install h5py\r\npip list\r\ncd C:\\Users\\aya\\tensorflow-v2.2.0\r\ngit clone https://github.com/tensorflow/tensorflow \r\ncd tensorflow\r\ngit checkout v2.2.0\r\ncd C:\\Users\\aya\\tensorflow-v2.2.0\\tensorflow\r\npython ./configure.py\r\nchoose: y for cuda    (for GPU) computaion capability for geforce rtx 2060: 7.5\r\noptimization flag:   /arch:AVX2\r\ncd C:\\Users\\aya\\tensorflow-v2.2.0\\tensorflow\r\n\r\nI run this:\r\nbazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\nor\r\n\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nand get the error:\r\n\r\n\r\nERROR: C:/users/aya/tensorflow-v2.2.0/tensorflow/tensorflow/core/kernels/BUILD:140:1: C++ compilation of rule '//tensorflow/core/kernels:strided_slice_op_gpu' failed (Exit 2)\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(114): warning: __host__ annotation is ignored on a function(\"no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(114): warning: __device__ annotation is ignored on a function(\"no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(115): warning: __host__ annotation is ignored on a function(\"no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(115): warning: __device__ annotation is ignored on a function(\"no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(115): warning: __host__ annotation is ignored on a function(\"~no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(115): warning: __device__ annotation is ignored on a function(\"~no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/Memory.h(85): warning: ignoring return value from routine declared with \"nodiscard\" attribute\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/DenseBase.h(639): warning: __host__ annotation is ignored on a function(\"DenseBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/DenseBase.h(639): warning: __device__ annotation is ignored on a function(\"DenseBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(484): warning: __host__ annotation is ignored on a function(\"MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(484): warning: __device__ annotation is ignored on a function(\"MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(485): warning: __host__ annotation is ignored on a function(\"MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(485): warning: __device__ annotation is ignored on a function(\"MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(485): warning: __host__ annotation is ignored on a function(\"~MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(485): warning: __device__ annotation is ignored on a function(\"~MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(156): warning: __host__ annotation is ignored on a function(\"ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(156): warning: __device__ annotation is ignored on a function(\"ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(157): warning: __host__ annotation is ignored on a function(\"ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(157): warning: __device__ annotation is ignored on a function(\"ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(157): warning: __host__ annotation is ignored on a function(\"~ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(157): warning: __device__ annotation is ignored on a function(\"~ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseBinaryOp.h(105): warning: __host__ annotation is ignored on a function(\"CwiseBinaryOp\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseBinaryOp.h(105): warning: __device__ annotation is ignored on a function(\"CwiseBinaryOp\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(70): warning: __host__ annotation is ignored on a function(\"CwiseUnaryView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(70): warning: __device__ annotation is ignored on a function(\"CwiseUnaryView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(110): warning: __host__ annotation is ignored on a function(\"CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(110): warning: __device__ annotation is ignored on a function(\"CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(125): warning: __host__ annotation is ignored on a function(\"CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(125): warning: __device__ annotation is ignored on a function(\"CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(125): warning: __host__ annotation is ignored on a function(\"~CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(125): warning: __device__ annotation is ignored on a function(\"~CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(185): warning: __host__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(185): warning: __device__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(186): warning: __host__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(186): warning: __device__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(186): warning: __host__ annotation is ignored on a function(\"~MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(186): warning: __device__ annotation is ignored on a function(\"~MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(300): warning: __host__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(300): warning: __device__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(301): warning: __host__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(301): warning: __device__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(301): warning: __host__ annotation is ignored on a function(\"~MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(301): warning: __device__ annotation is ignored on a function(\"~MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Map.h(162): warning: __host__ annotation is ignored on a function(\"Map\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Map.h(162): warning: __device__ annotation is ignored on a function(\"Map\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Ref.h(90): warning: __host__ annotation is ignored on a function(\"RefBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Ref.h(90): warning: __device__ annotation is ignored on a function(\"RefBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Ref.h(232): warning: __host__ annotation is ignored on a function(\"Ref\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Ref.h(232): warning: __device__ annotation is ignored on a function(\"Ref\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(111): warning: __host__ annotation is ignored on a function(\"Block\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(111): warning: __device__ annotation is ignored on a function(\"Block\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(161): warning: __host__ annotation is ignored on a function(\"BlockImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(161): warning: __device__ annotation is ignored on a function(\"BlockImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(181): warning: __host__ annotation is ignored on a function(\"BlockImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(181): warning: __device__ annotation is ignored on a function(\"BlockImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(341): warning: __host__ annotation is ignored on a function(\"BlockImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(341): warning: __device__ annotation is ignored on a function(\"BlockImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/IndexedView.h(113): warning: __host__ annotation is ignored on a function(\"IndexedView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/IndexedView.h(113): warning: __device__ annotation is ignored on a function(\"IndexedView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(103): warning: __host__ annotation is ignored on a function(\"Reshaped\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(103): warning: __device__ annotation is ignored on a function(\"Reshaped\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(137): warning: __host__ annotation is ignored on a function(\"ReshapedImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(137): warning: __device__ annotation is ignored on a function(\"ReshapedImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(155): warning: __host__ annotation is ignored on a function(\"ReshapedImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(155): warning: __device__ annotation is ignored on a function(\"ReshapedImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(215): warning: __host__ annotation is ignored on a function(\"ReshapedImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(215): warning: __device__ annotation is ignored on a function(\"ReshapedImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(66): warning: __host__ annotation is ignored on a function(\"Transpose\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(66): warning: __device__ annotation is ignored on a function(\"Transpose\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(126): warning: __host__ annotation is ignored on a function(\"TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(126): warning: __device__ annotation is ignored on a function(\"TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(157): warning: __host__ annotation is ignored on a function(\"TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(157): warning: __device__ annotation is ignored on a function(\"TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(157): warning: __host__ annotation is ignored on a function(\"~TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(157): warning: __device__ annotation is ignored on a function(\"~TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Diagonal.h(78): warning: __host__ annotation is ignored on a function(\"Diagonal\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Diagonal.h(78): warning: __device__ annotation is ignored on a function(\"Diagonal\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(222): warning: __host__ annotation is ignored on a function(\"TriangularView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(222): warning: __device__ annotation is ignored on a function(\"TriangularView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(559): warning: __host__ annotation is ignored on a function(\"TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(559): warning: __device__ annotation is ignored on a function(\"TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(560): warning: __host__ annotation is ignored on a function(\"TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(560): warning: __device__ annotation is ignored on a function(\"TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(560): warning: __host__ annotation is ignored on a function(\"~TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(560): warning: __device__ annotation is ignored on a function(\"~TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reverse.h(90): warning: __host__ annotation is ignored on a function(\"Reverse\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reverse.h(90): warning: __device__ annotation is ignored on a function(\"Reverse\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayWrapper.h(47): warning: __host__ annotation is ignored on a function(\"ArrayWrapper\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayWrapper.h(47): warning: __device__ annotation is ignored on a function(\"ArrayWrapper\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayWrapper.h(145): warning: __host__ annotation is ignored on a function(\"MatrixWrapper\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayWrapper.h(145): warning: __device__ annotation is ignored on a function(\"MatrixWrapper\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\Eigen\\CXX11\\src/Tensor/TensorBlock.h(245): warning: invalid friend declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\Eigen\\CXX11\\src/Tensor/TensorBlock.h(709): warning: invalid friend declaration\r\n\r\nexternal/com_google_protobuf/src\\google/protobuf/map.h(1027): warning: invalid friend declaration\r\n\r\nexternal/com_google_absl\\absl/strings/string_view.h(501): warning: expression has no effect\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(114): warning: __host__ annotation is ignored on a function(\"no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(114): warning: __device__ annotation is ignored on a function(\"no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(115): warning: __host__ annotation is ignored on a function(\"no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(115): warning: __device__ annotation is ignored on a function(\"no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(115): warning: __host__ annotation is ignored on a function(\"~no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/util/XprHelper.h(115): warning: __device__ annotation is ignored on a function(\"~no_assignment_operator\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/DenseBase.h(639): warning: __host__ annotation is ignored on a function(\"DenseBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/DenseBase.h(639): warning: __device__ annotation is ignored on a function(\"DenseBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(484): warning: __host__ annotation is ignored on a function(\"MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(484): warning: __device__ annotation is ignored on a function(\"MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(485): warning: __host__ annotation is ignored on a function(\"MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(485): warning: __device__ annotation is ignored on a function(\"MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(485): warning: __host__ annotation is ignored on a function(\"~MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MatrixBase.h(485): warning: __device__ annotation is ignored on a function(\"~MatrixBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(156): warning: __host__ annotation is ignored on a function(\"ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(156): warning: __device__ annotation is ignored on a function(\"ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(157): warning: __host__ annotation is ignored on a function(\"ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(157): warning: __device__ annotation is ignored on a function(\"ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(157): warning: __host__ annotation is ignored on a function(\"~ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayBase.h(157): warning: __device__ annotation is ignored on a function(\"~ArrayBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseBinaryOp.h(105): warning: __host__ annotation is ignored on a function(\"CwiseBinaryOp\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseBinaryOp.h(105): warning: __device__ annotation is ignored on a function(\"CwiseBinaryOp\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(70): warning: __host__ annotation is ignored on a function(\"CwiseUnaryView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(70): warning: __device__ annotation is ignored on a function(\"CwiseUnaryView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(110): warning: __host__ annotation is ignored on a function(\"CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(110): warning: __device__ annotation is ignored on a function(\"CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(125): warning: __host__ annotation is ignored on a function(\"CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(125): warning: __device__ annotation is ignored on a function(\"CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(125): warning: __host__ annotation is ignored on a function(\"~CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/CwiseUnaryView.h(125): warning: __device__ annotation is ignored on a function(\"~CwiseUnaryViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(185): warning: __host__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(185): warning: __device__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(186): warning: __host__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(186): warning: __device__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(186): warning: __host__ annotation is ignored on a function(\"~MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(186): warning: __device__ annotation is ignored on a function(\"~MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(300): warning: __host__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(300): warning: __device__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(301): warning: __host__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(301): warning: __device__ annotation is ignored on a function(\"MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(301): warning: __host__ annotation is ignored on a function(\"~MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/MapBase.h(301): warning: __device__ annotation is ignored on a function(\"~MapBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Map.h(162): warning: __host__ annotation is ignored on a function(\"Map\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Map.h(162): warning: __device__ annotation is ignored on a function(\"Map\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Ref.h(90): warning: __host__ annotation is ignored on a function(\"RefBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Ref.h(90): warning: __device__ annotation is ignored on a function(\"RefBase\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Ref.h(232): warning: __host__ annotation is ignored on a function(\"Ref\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Ref.h(232): warning: __device__ annotation is ignored on a function(\"Ref\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(111): warning: __host__ annotation is ignored on a function(\"Block\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(111): warning: __device__ annotation is ignored on a function(\"Block\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(161): warning: __host__ annotation is ignored on a function(\"BlockImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(161): warning: __device__ annotation is ignored on a function(\"BlockImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(181): warning: __host__ annotation is ignored on a function(\"BlockImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(181): warning: __device__ annotation is ignored on a function(\"BlockImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(341): warning: __host__ annotation is ignored on a function(\"BlockImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Block.h(341): warning: __device__ annotation is ignored on a function(\"BlockImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/IndexedView.h(113): warning: __host__ annotation is ignored on a function(\"IndexedView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/IndexedView.h(113): warning: __device__ annotation is ignored on a function(\"IndexedView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(103): warning: __host__ annotation is ignored on a function(\"Reshaped\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(103): warning: __device__ annotation is ignored on a function(\"Reshaped\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(137): warning: __host__ annotation is ignored on a function(\"ReshapedImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(137): warning: __device__ annotation is ignored on a function(\"ReshapedImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(155): warning: __host__ annotation is ignored on a function(\"ReshapedImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(155): warning: __device__ annotation is ignored on a function(\"ReshapedImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(215): warning: __host__ annotation is ignored on a function(\"ReshapedImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reshaped.h(215): warning: __device__ annotation is ignored on a function(\"ReshapedImpl_dense\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(66): warning: __host__ annotation is ignored on a function(\"Transpose\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(66): warning: __device__ annotation is ignored on a function(\"Transpose\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(126): warning: __host__ annotation is ignored on a function(\"TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(126): warning: __device__ annotation is ignored on a function(\"TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(157): warning: __host__ annotation is ignored on a function(\"TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(157): warning: __device__ annotation is ignored on a function(\"TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(157): warning: __host__ annotation is ignored on a function(\"~TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Transpose.h(157): warning: __device__ annotation is ignored on a function(\"~TransposeImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Diagonal.h(78): warning: __host__ annotation is ignored on a function(\"Diagonal\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Diagonal.h(78): warning: __device__ annotation is ignored on a function(\"Diagonal\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(222): warning: __host__ annotation is ignored on a function(\"TriangularView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(222): warning: __device__ annotation is ignored on a function(\"TriangularView\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(559): warning: __host__ annotation is ignored on a function(\"TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(559): warning: __device__ annotation is ignored on a function(\"TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(560): warning: __host__ annotation is ignored on a function(\"TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(560): warning: __device__ annotation is ignored on a function(\"TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(560): warning: __host__ annotation is ignored on a function(\"~TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/TriangularMatrix.h(560): warning: __device__ annotation is ignored on a function(\"~TriangularViewImpl\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reverse.h(90): warning: __host__ annotation is ignored on a function(\"Reverse\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/Reverse.h(90): warning: __device__ annotation is ignored on a function(\"Reverse\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayWrapper.h(47): warning: __host__ annotation is ignored on a function(\"ArrayWrapper\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayWrapper.h(47): warning: __device__ annotation is ignored on a function(\"ArrayWrapper\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayWrapper.h(145): warning: __host__ annotation is ignored on a function(\"MatrixWrapper\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/ArrayWrapper.h(145): warning: __device__ annotation is ignored on a function(\"MatrixWrapper\") that is explicitly defaulted on its first declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\Eigen\\CXX11\\src/Tensor/TensorBlock.h(245): warning: invalid friend declaration\r\n\r\nC:\\users\\aya\\_bazel_aya\\apeokjhk\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\Eigen\\CXX11\\src/Tensor/TensorBlock.h(709): warning: invalid friend declaration\r\n\r\nexternal/com_google_protobuf/src\\google/protobuf/map.h(1027): warning: invalid friend declaration\r\n\r\nexternal/com_google_absl\\absl/strings/string_view.h(501): warning: expression has no effect\r\n\r\nexternal/com_google_protobuf/src\\google/protobuf/repeated_field.h(397): error C2993: 'T': is not a valid type for non-type template parameter '__formal'\r\nexternal/com_google_protobuf/src\\google/protobuf/repeated_field.h(406): note: see reference to class template instantiation 'google::protobuf::internal::TypeImplementsMergeBehaviorProbeForMergeFrom<T>' being compiled\r\nexternal/com_google_protobuf/src\\google/protobuf/repeated_field.h(397): error C2065: 'RetType': undeclared identifier\r\nexternal/com_google_protobuf/src\\google/protobuf/repeated_field.h(397): error C2923: 'std::_Select<__formal>::_Apply': 'RetType' is not a valid template type argument for parameter '<unnamed-symbol>'\r\nexternal/com_google_protobuf/src\\google/protobuf/repeated_field.h(397): error C2062: type 'unknown-type' unexpected\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n\r\n", "comments": ["What is your compiler version? Please fill in the form fully.\r\nCurrently, I have no problems on my machine building any branches on windows.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41784\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41784\">No</a>\n"]}, {"number": 41783, "title": "TensorFlow 2.2 using tf.float16 executes only on CPU", "body": "I'm using [this code](https://github.com/vcadillog/PPO-Mario-Bros-Tensorflow-2) as a starting point for one of my projects. The only modification I did so far is switching to OpenAI Gym Atari because I'm running it on Windows.\r\n\r\nThe problem I'm having is when I use tf.keras.backend.set_floatx(tf.float16) the code gets stuck executing on CPU. I also changed all the explicit casts throughout the code. The idea is to utilize the tensor cores on my GPU. This only happens with this code. All my other projects work fine.\r\n\r\nI'm running it on TF 2.2. I tried removing tf.function annotations. I tried messing around with casts. For example, leaving all the data in fp32 and using fp16 weights in the model. Nothing helped. Honestly, I haven't tried much because I can't find any information on the problem and have no idea where to start debugging. There are no errors and everything works fine, except it runs on CPU instead of GPU. The only difference is in the mess that TF spews out when initializing I get this message:\r\n\r\n`W tensorflow/compiler/jit/xla_device.cc:398] XLA_GPU and XLA_CPU devices are deprecated and will be removed in subsequent releases. Instead, use either @tf.function(experimental_compile=True) for must-compile semantics, or run with TF_XLA_FLAGS=--tf_xla_auto_jit=2 for auto-clustering best-effort compilation.`\r\n\r\nI tried both options proposed in the message and nothing happened.\r\n\r\nAlso, I have tried [tf.device](https://www.tensorflow.org/api_docs/python/tf/device) but nothing changed\r\n\r\nI really need this to work. Not just for the memory and performance optimizations but part of my project is benchmarking the difference between the two.\r\n\r\nAny help would be appreciated or at least if someone could point me in the direction where to look for the information.\r\n\r\nThank you.", "comments": ["@aleksandarPlu \r\n\r\nRequest you to fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nRequest you to share simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram \r\n\r\nThanks for you reply. All other project are working fine, except this one. Unable to create any simpler version to reproduce this issue.\r\nPlatform details are:  TF 2.2, Win 10 (1909), nVidia driver 451.67, CUDA 10.1, Python 3.7, i7-4770k, RTX 2060 Super", "@aleksandarPlu Can you try using `tf.keras.mixed_precision.experimental.set_policy('mixed_float16')` This might help you to speed up training by 3 times on a modern GPU as mentioned [here](https://www.tensorflow.org/guide/mixed_precision#overview)", "@gowthamkpr thank you for your suggestion. Now at least was able to run the code on tf16 with your proposed solution. But we did not get any improvement in speed or memory. It is the same as when I used tf32", "@aleksandarPlu Glad you were able to run the code with proposed solution. As mentioned in the comment, it might help you speed up training but I didn't promise it would. \r\n\r\nAs mentioned [here](https://www.tensorflow.org/guide/mixed_precision#overview), NVIDIA GPUs can run operations in float16 faster than in float32. Therefore, these lower-precision dtypes should be used whenever possible on those devices. However, variables and a few computations should still be in float32 for numeric reasons so that the model trains to the same quality. The Keras mixed precision API allows you to use a mix of either float16 or bfloat16 with float32, to get the performance benefits from float16/bfloat16 and the numeric stability benefits from float32.\r\n\r\nSome ops are performed on CPU but not on GPU, this might be the reason for not seeing any change in speed.\r\n\r\nMoreover, As this issue is not a bug pr feature request can you please post this in stack overflow where community can help you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41783\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41783\">No</a>\n"]}, {"number": 41782, "title": "[ROCm][XLA] Fixing no_alias_test", "body": "Update file-check statement to match with function signature. Same as #35572.\r\n\r\n/cc @deven-amd", "comments": ["@jerryyin Can you please check @timshen91's comments and keep us posted ? Thanks!", "@gbaned Resolve and rebased in the latest commit. Thanks."]}, {"number": 41781, "title": "Timeseries example and warning message with latest code: Executor start aborting: Invalid argument:", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): Pulled from source on 27th EST\r\n- TensorFlow version (use command below): Latest from source\r\n- Python version: 3.8.1\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: TRX 2070\r\n\r\n**Describe the current behavior**\r\nWhen i run Timeseries example from Tensorflow.org, i get the following warning messages (a lot)\r\n\r\n2020-07-27 14:23:21.829669: W tensorflow/core/common_runtime/executor.cc:1086] [/device:CPU:0] :  You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [119759,72]\r\n\t [[{{node Placeholder/_1}}]]\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://www.tensorflow.org/tutorials/structured_data/time_series#part_2_forecast_a_multivariate_time_series\r\n", "comments": ["@summa-code,\r\nOn running the code with the latest TF-nightly, I did not face any errors or warnings. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/88001ef1ee580e1342e51cdf29bd6297/41781.ipynb). Thanks!", "I will try in the next build and let you know.", "They are gone, but another error or information started popping out,\r\n\r\nI tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)\r\n", "@amahendrakar This information is happening on TF 2.4, the original issue is gone. Should i file new one ?", "@summa-code,\r\nYes please. Please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}]