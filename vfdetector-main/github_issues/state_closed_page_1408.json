[{"number": 10742, "title": "The current makefile builds incomplete Tensorflow", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.1\r\n- **Bazel version (if compiling from source)**: not using bazel\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI have managed to cross compile using the Makefile approach but the API's are incomplete. I think it is because not all source files are included in the Makefile and the text files. Can someone provides a Makefile that is equivalent to bazel build //tensorflow:libtensorflow_cc.so? I want to cross compile a library and use that on my custom platform.\r\n\r\n### Source code / logs\r\n\r\n", "comments": ["The readme in the makefile page clearly states this is the case.\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile\r\n\r\nThe supported way to build tensorflow is to use bazel. Makefile is only maintained to build for iOS, and will provide an incomplete build of tensorflow."]}, {"number": 10741, "title": "[go] bug in Shape.size for dim == NumDimensions", "body": "### System information\r\nThis does not matter.\r\n\r\n### Describe the problem\r\n(go) when dim equals s.NumDimensions(), the function should return -1, instead it panics.\r\n\r\n### Source code / logs\r\nIn [shape.go](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/shape.go#L62), `Shape.Size` method\r\n\r\n        func (s Shape) Size(dim int) int64 {\r\n    ---   if dim < 0 || dim > s.NumDimensions() {\r\n            return -1\r\nshould be:\r\n\r\n        func (s Shape) Size(dim int) int64 {\r\n    +++   if dim < 0 || dim >= s.NumDimensions() {\r\n            return -1\r\n", "comments": ["Thanks for the report, please specify the filename (link is even better) in the future to make it clear what you are talking about.  Thanks!"]}, {"number": 10740, "title": "Type error occurring in the input pipeline should give more descriptive error messages than \"RandomShuffleQueue is closed and has insufficient elements\"", "body": "I don't have a particular code example, but I had the case that a `tf.py_func` node returned the wrong data type and thus caused the queue to stall. It was impossible to tell what the error was from the error messages that TensorFlow produced. What is the reason that there are no error messages about type mismatches?", "comments": ["It would be very helpful if you could create a reproducible test case. @ebrevdo, do you have any idea on this anyways?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 10739, "title": "Quantized graph using graph transform fails to work", "body": "**System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): v1.1\r\nBazel version (if compiling from source): N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce:**\r\n\r\nI am using a quantized graph created using following command:\r\n\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \r\n--in_graph=./frozen_model_inception_resnet_v2.pb \r\n--out_graph=./quantized_weights_and_nodes_inception_resnet_v2.pb \r\n--inputs='InputImage:0' \r\n--outputs='InceptionResnetV2/Logits/Predictions' \r\n--transforms='\r\nadd_default_attributes\r\nstrip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\nremove_nodes(op=Identity, op=CheckNumerics)\r\nfold_constants(ignore_errors=true)\r\nfold_batch_norms\r\nfold_old_batch_norms\r\nquantize_weights\r\nquantize_nodes\r\nstrip_unused_nodes\r\nsort_by_execution_order'\r\n\r\nI try to use the graph in a program as below:\r\n\r\ngraph_def = tf.GraphDef()\r\n    with open(os.path.join(FLAGS.model_dir, GRAPH_FILE), \"rb\") as f:\r\n        model_str = f.read()\r\n        graph_def.ParseFromString(model_str)\r\n        tf.import_graph_def(graph_def, name='')\r\n\r\nHowever, I get error \"ValueError: No op named QuantizedAdd in defined operations\" now when tf.import_graph_def(graph_def, name='') is called.\r\n\r\nI also tried to use :         dir(tf.contrib) as explained in one of the issues : https://github.com/tensorflow/tensorflow/issues/10130\r\n\r\ngraph_def = tf.GraphDef()\r\n    with open(os.path.join(FLAGS.model_dir, GRAPH_FILE), \"rb\") as f:\r\n        model_str = f.read()\r\n        graph_def.ParseFromString(model_str)\r\n        dir(tf.contrib)\r\n        tf.import_graph_def(graph_def, name='')\r\n\r\nbut this did not solve the problem for me, I still get same error.", "comments": ["@petewarden, do you have time to take a quick look.", "I'm guessing this is a mismatch between the 1.1 version of TensorFlow that you're using in Python (since you installed it from a binary) and the top-of-tree TensorFlow tool that it looks like you built from source (since I see blaze-bin in your command line). QuantizedAdd was introduced very recently, so it's not available in any binary builds yet as far as I know, so you'll need to build a pip package from the same source as your tool and install that into your Python environment.", "@petewarden : I did like you said. However, I get another error now for the quantized graph, when I run inference:\r\n\r\n File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 312, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2528, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1203, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): requested_output_max must be >= requested_output_min, but got nan and 0\r\n\t [[Node: InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/batchnorm/mul_1/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/batchnorm/mul_1/eightbit, InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/batchnorm/mul_1/eightbit:1, InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/batchnorm/mul_1/eightbit:2, InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/batchnorm/mul_1/eightbit/requant_range, InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/batchnorm/mul_1/eightbit/requant_range:1)]]\r\n\r\nI have taken the latest source code and installed tensorflow. Any inputs on this?\r\n", "I haven't tried quantization on InceptionResnetV2, so it's possible there's issues we haven't run across with that model. As a workaround, try changing the quantize_nodes transform to:\r\n\r\n```\r\nquantize_nodes(fallback_min=-10, fallback_max=10)\r\n```\r\n\r\nThis is likely to produce less accurate end results, but it will be helpful to see if it gets us past the import error.", "@petewarden The error was common across other inception models that I tried (i.e. Inception V3 and Inception V4) even for those models I got similar error as above: \r\n\r\n\r\n```\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 312, in import_graph_def\r\nop_def=op_def)\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2528, in create_op\r\noriginal_op=self._default_original_op, op_def=op_def)\r\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1203, in init\r\nself._traceback = self._graph._extract_stack() # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): requested_output_max must be >= requested_output_min, but got nan and 0\r\n[[Node: InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/batchnorm/mul_1/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/batchnorm/mul_1/eightbit, InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/batchnorm/mul_1/eightbit:1, InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/batchnorm/mul_1/eightbit:2, InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/batchnorm/mul_1/eightbit/requant_range, InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/batchnorm/mul_1/eightbit/requant_range:1)]]\r\n\r\n```\r\nHowever, I tried the solution you gave. i.e.  using: \r\nquantize_nodes(fallback_min=-10, fallback_max=10)\r\n\r\ngraph that is generated, the inference runs without error but the results is totally inaccurate results (0% accuracy). The results are very similar that I get when I remove \"quantize_nodes\" from \"transforms\".\r\n\r\nThis behavior is same also for inceptionV3 and inceptionV4 as that for inception_resnet_v2.\r\n", "Facing the same issue as well.\r\nStepping into the code, the error is raised because the operations dictionary doesn't have `QuantizedAdd` but has `QuantizedBiasAdd` instead, [line 279 of _importer.py_](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/importer.py#L279)\r\n\r\n[Contents of the operation dictionary](http://imgur.com/a/kvHIi)\r\n_It has 629 elements so this image previews the Q elements segment only_", "@eshirima  It appears because of the input tensor is empty. you  can add some codes to handle the empty tensor.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing due to inactivity. Please file a new issue if this problem is still occuring.", "The problem is still there. And nobody resolves it so you closed it?"]}, {"number": 10738, "title": "hexagon graph execution has put checks for inputs to be of square dimensions, why is it so?", "body": "OS: Ubuntu 16.04 64bits\r\nAndroid Version: 7.1 (Nougat)\r\nNDK Version: android-ndk-r12b\r\nHEXAGON SDK: 3.1\r\nnnlib source: https://source.codeaurora.org/quic/hexagon_nn/nnlib\r\n\r\nI want to test against input shape 1 640 480 3\r\nBelow check is returning with error as it is looking for square dimensions.\r\n(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc#L136)\r\n`CHECK(fsize >= (WIDTH + 1) * WIDTH * 3 + header_size);`\r\n\r\nI am not sure whether I can comment that and I did following that I got this error:\r\n(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc#L147)\r\n```\r\nCHECK(src_pos + 2 + header_size < fsize);\r\nCHECK(dst_pos + 2 < pixel_count);\r\n```\r\n\r\nAlso cannot give input graph as argument while running -> hexagon_graph_execution.\r\nit needs to be changed @ \r\n(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc#L52)\r\n```\r\n\r\nconstexpr const char* const IMAGE_FILENAME = \"/data/local/tmp/img_299x299.bmp\";\r\nconstexpr const char* const MODEL_FILENAME =\r\n\"/data/local/tmp/tensorflow_inception_v3_stripped_optimized_quantized.pb\";\r\n```\r\n\r\nany plans for standard interface to change \r\n1) input shape\r\n2) model filename\r\n3) Image filename\r\n\r\nthanks,", "comments": ["@satok16 , Would be glad and thankful, to provide workaround from your re-factoring code base.", "@kzos, hexagon_graph_execution_test.cc is a test application intended to run the inception models.  If you want to run an inference on HVX in a more generic way, you should write your own program to follow tensorflow inference steps.  Anyway, the upcoming refactoring will allow you to benchmark arbitrary graph on HVX by benchmark_model.cc without using hexagon_graph_execution_test.cc, and that will help you to understand these tensorflow steps.", "@satok16, Ok, Already I am making changes for my own interface, got stuck in few op's not being supported by hexagon. Will fix that first, meanwhile, I hope refactored changes would get pushed. BTW you have any tentative date for pushing?", "@kzos \r\nMost of the code is already submitted internally.  The remaining part will be submitted in a week, hopefully in a few days.  Stay tuned.", "@satok16 May I know currently whether I can run generic graph with HVX and any example for that?\r\nThanks.\r\n", "That's partially yes, but not easy.  You should transform a graph by using graph_transforms like:\r\n\r\nbazel run tensorflow/tools/graph_transforms:transform_graph -- \\\r\n--in_graph=\"${SRC_FILE_NAME}\" \\\r\n--out_graph=\"${DST_FILE_NAME}\" \\\r\n--inputs=\"${INPUTS}\" --outputs=\"${OUTPUTS}\" \\\r\n--transforms=\"\\\r\nfuse_remote_graph(\\\r\ninput_types=\\\"${INPUT_TYPES}\\\",\\\r\ninput_shapes=\\\"${INPUT_SHAPES}\\\",\\\r\nborder_inputs=\\\"${BORDER_INPUTS}\\\",\\\r\nborder_outputs=\\\"${BORDER_OUTPUTS}\\\",\\\r\nremote_fused_graph_executor_name=\\\"${REMOTE_FUSED_GRAPH_EXECUTOR_NAME}\\\",\\\r\nremote_fused_graph_node_name=\\\"${REMOTE_FUSED_GRAPH_NODE_NAME}\\\"\\\r\n)\"\r\n\r\nYou should specify inputs (border_inputs) and outputs (border_outputs) of a subgraph where all ops are supported by HVX.\r\n\r\nCurrently, I'm away from this work and there are no good examples.\r\n\r\nAnyway, the original topic has been already implemented which can be used via the command I noted above.\r\n\r\nIf you want to add further functionalities, contributions are always welcome.", "@satok16 Thanks.\r\nBut may I know whether fuse_remote_graph in transform_graph is necessary?\r\nAs my understanding, we can use graph transform to make the following transformations.\r\nFP32 model->quantized model->fused hexagon quantized model.\r\nCan I feed FP32 model or quantized model directly into tensorflow with hexagon support?\r\n\r\nThanks for your help.\r\n\r\n", "No, the transform is required in order to let the tensorflow know which ops should be run on HVX because some HVX ops are not optimized and may be much slower than CPU.  So, user needs to optimize the graph manually by using fuse_remote_graph."]}, {"number": 10736, "title": "seq2seq.BasicDecoder incompatible with seq2seq.ScheduledOutputTrainingHelper", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source \r\n- **TensorFlow version (use command below)**:  b'v1.1.0-0-g1ec6ed5' 1.1.0\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **CUDA/cuDNN version**: 5.1.3\r\n- **GPU model and memory**: TitanX 12GB\r\n- **Exact command to reproduce**: See script below\r\n\r\n### Describe the problem\r\n`seq2seq.ScheduledOutputTrainingHelper.sample(...)` outputs a `tf.bool` tensor but `seq2seq.BasicDecoder.output_dtype` assumes `tf.int32` output.\r\n\r\n### Source code / logs\r\nMinimal case script:\r\n```python\r\nimport tensorflow as tf\r\nwith tf.Graph().as_default():\r\n    batch_size = 32\r\n    nsteps = 100\r\n    ndims = 5\r\n    sequence_length = [nsteps] * batch_size\r\n    sampling_probability = 0.5\r\n    num_units = 20\r\n\r\n    cell = tf.contrib.rnn.BasicRNNCell(\r\n        num_units,\r\n    )\r\n\r\n    inputs = tf.random_uniform((batch_size, nsteps, ndims))\r\n\r\n    output, state = tf.nn.dynamic_rnn(\r\n        cell,\r\n        inputs,\r\n        dtype=tf.float32,\r\n    )\r\n\r\n    cell = tf.contrib.rnn.BasicRNNCell(\r\n        num_units,\r\n    )\r\n\r\n    helper = tf.contrib.seq2seq.ScheduledOutputTrainingHelper(\r\n        output,\r\n        sequence_length,\r\n        sampling_probability,\r\n    )\r\n\r\n    initial_state = tf.zeros((batch_size, num_units))\r\n    decoder = tf.contrib.seq2seq.BasicDecoder(\r\n        cell,\r\n        helper,\r\n        initial_state,\r\n    )\r\n\r\n    decoded = tf.contrib.seq2seq.dynamic_decode(\r\n        decoder,\r\n    )\r\n\r\n    run_ops = {'decoded': decoded}\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        ret = sess.run(run_ops)\r\n```\r\nOutput:\r\n```bash\r\n2017-06-15 12:28:39.868640: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-15 12:28:39.868659: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-15 12:28:39.868664: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-15 12:28:40.234562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-06-15 12:28:40.234768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties:\r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:01:00.0\r\nTotal memory: 11.92GiB\r\nFree memory: 384.19MiB\r\n2017-06-15 12:28:40.234817: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x31b49d0\r\n2017-06-15 12:28:40.315855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-06-15 12:28:40.316063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties:\r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.92GiB\r\nFree memory: 384.19MiB\r\n2017-06-15 12:28:40.318076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1\r\n2017-06-15 12:28:40.318084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y\r\n2017-06-15 12:28:40.318088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y\r\n2017-06-15 12:28:40.318093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)\r\n2017-06-15 12:28:40.318098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\r\n2017-06-15 12:28:40.927295: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: TensorArray dtype is int32 but Op is trying to write dtype bool.\r\n\t [[Node: decoder/while/TensorArrayWrite_1/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_BOOL, _class=[\"loc:@decoder/TensorArray_1\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](decoder/while/TensorArrayWrite_1/TensorArrayWriteV3/Enter, decoder/while/Identity/_47, decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperSample/Cast/_49, decoder/while/Identity_2/_51)]]\r\n2017-06-15 12:28:40.927295: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: TensorArray dtype is int32 but Op is trying to write dtype bool.\r\n\t [[Node: decoder/while/TensorArrayWrite_1/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_BOOL, _class=[\"loc:@decoder/TensorArray_1\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](decoder/while/TensorArrayWrite_1/TensorArrayWriteV3/Enter, decoder/while/Identity/_47, decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperSample/Cast/_49, decoder/while/Identity_2/_51)]]\r\n2017-06-15 12:28:40.927369: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: TensorArray dtype is int32 but Op is trying to write dtype bool.\r\n\t [[Node: decoder/while/TensorArrayWrite_1/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_BOOL, _class=[\"loc:@decoder/TensorArray_1\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](decoder/while/TensorArrayWrite_1/TensorArrayWriteV3/Enter, decoder/while/Identity/_47, decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperSample/Cast/_49, decoder/while/Identity_2/_51)]]\r\n2017-06-15 12:28:40.927384: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: TensorArray dtype is int32 but Op is trying to write dtype bool.\r\n\t [[Node: decoder/while/TensorArrayWrite_1/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_BOOL, _class=[\"loc:@decoder/TensorArray_1\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](decoder/while/TensorArrayWrite_1/TensorArrayWriteV3/Enter, decoder/while/Identity/_47, decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperSample/Cast/_49, decoder/while/Identity_2/_51)]]\r\n2017-06-15 12:28:40.927443: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: TensorArray dtype is int32 but Op is trying to write dtype bool.\r\n\t [[Node: decoder/while/TensorArrayWrite_1/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_BOOL, _class=[\"loc:@decoder/TensorArray_1\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](decoder/while/TensorArrayWrite_1/TensorArrayWriteV3/Enter, decoder/while/Identity/_47, decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperSample/Cast/_49, decoder/while/Identity_2/_51)]]\r\n2017-06-15 12:28:40.927469: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: TensorArray dtype is int32 but Op is trying to write dtype bool.\r\n\t [[Node: decoder/while/TensorArrayWrite_1/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_BOOL, _class=[\"loc:@decoder/TensorArray_1\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](decoder/while/TensorArrayWrite_1/TensorArrayWriteV3/Enter, decoder/while/Identity/_47, decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperSample/Cast/_49, decoder/while/Identity_2/_51)]]\r\n2017-06-15 12:28:41.647498: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: TensorArray dtype is int32 but Op is trying to write dtype bool.\r\n\t [[Node: decoder/while/TensorArrayWrite_1/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_BOOL, _class=[\"loc:@decoder/TensorArray_1\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](decoder/while/TensorArrayWrite_1/TensorArrayWriteV3/Enter, decoder/while/Identity/_47, decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperSample/Cast/_49, decoder/while/Identity_2/_51)]]\r\nTraceback (most recent call last):\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1039, in _do_call\r\n    return fn(*args)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1021, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray dtype is int32 but Op is trying to write dtype bool.\r\n\t [[Node: decoder/while/TensorArrayWrite_1/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_BOOL, _class=[\"loc:@decoder/TensorArray_1\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](decoder/while/TensorArrayWrite_1/TensorArrayWriteV3/Enter, decoder/while/Identity/_47, decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperSample/Cast/_49, decoder/while/Identity_2/_51)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tmp/scratch2.py\", line 46, in <module>\r\n    ret = sess.run(run_ops)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray dtype is int32 but Op is trying to write dtype bool.\r\n\t [[Node: decoder/while/TensorArrayWrite_1/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_BOOL, _class=[\"loc:@decoder/TensorArray_1\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](decoder/while/TensorArrayWrite_1/TensorArrayWriteV3/Enter, decoder/while/Identity/_47, decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperSample/Cast/_49, decoder/while/Identity_2/_51)]]\r\n\r\nCaused by op 'decoder/while/TensorArrayWrite_1/TensorArrayWriteV3', defined at:\r\n  File \"tmp/scratch2.py\", line 40, in <module>\r\n    decoder,\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 278, in dynamic_decode\r\n    swap_memory=swap_memory)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 267, in body\r\n    outputs_ta, emit)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 305, in map_structure\r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 305, in <listcomp>\r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 266, in <lambda>\r\n    outputs_ta = nest.map_structure(lambda ta, out: ta.write(time, out),\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 279, in write\r\n    name=name)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 2823, in _tensor_array_write_v3\r\n    name=name)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/sarroff/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): TensorArray dtype is int32 but Op is trying to write dtype bool.\r\n\t [[Node: decoder/while/TensorArrayWrite_1/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_BOOL, _class=[\"loc:@decoder/TensorArray_1\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](decoder/while/TensorArrayWrite_1/TensorArrayWriteV3/Enter, decoder/while/Identity/_47, decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperSample/Cast/_49, decoder/while/Identity_2/_51)]]\r\n```\r\n", "comments": ["Hi, I filed an issue too. We are running essentially the same script.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/10815", "I have the following workaround, which doesn't require rewriting any TF code.\r\n \r\nThe callable`dynamic_decode` returns `final_outputs`, `final_state`, and `final_sequence_lengths`. The output argument `final_outputs` itself is an instance of `BasicDecoderOutput`, which is essentially a named tuple having the elements `[rnn_output, sample_id]`. If no operations in your graph depend on `sample_id`, and if the input argument `impute_finished ` to `dynamic_decode` is set to `False` (the default), then no errors will be raised.\r\n\r\nI.e., following the example code above, one could write:\r\n```python\r\n    decoded = tf.contrib.seq2seq.dynamic_decode(\r\n        decoder,\r\n        impute_finished=False)\r\n    final_outputs, final_state, final_sequence_lengths = decoded\r\n    decoded = [[final_outputs[0]], final_state, final_sequence_lengths]\r\n\r\n    run_ops = {'decoded': decoded}\r\n```\r\nOf course, this assumes that `impute_finished` and `sample_id` are not necessary to your task. Otherwise, you'll probably have to modify `ScheduledOutputTrainingHelper.sample`", "@woodshop Good catch. This is very helpful!", "I believe this issue has been fixed by https://github.com/tensorflow/tensorflow/commit/c16de2979a9522e42a4b65cbdc20a05c731ec555#diff-282452f582e236ca10723e56a0b03945 "]}, {"number": 10735, "title": "[BUG] Get different image value each feed", "body": "### System information\r\n- OS: Centos 7\r\n- TensorFlow installed from source\r\n- Tensorflow version: both 1.1.0rc1 and 1.2.0rc1\r\n- Bazel version: 0.4.5-jdk7\r\n- CUDA 8.0/ cuDNN 5.1\r\n- GeForce GTX 1080 / 8G\r\n\r\n### Describe the problem\r\nGet different values of the same feeded image. \r\n\r\n### Source code\r\nBelow is a minimal script that reproduce the problem:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom scipy.misc import imread\r\n\r\ntest_im = some-test-img-path\r\nim = imread(test_im)\r\nim_batch = np.stack([im])\r\n\r\nimg_feed = tf.placeholder(tf.uint8, (None, None, None, 3))\r\nimg_mean = tf.reduce_mean(tf.to_float(img_feed))\r\n\r\nwith tf.Session() as sess:\r\n  for i in range(10):\r\n    img_mean_tf = sess.run(img_mean, feed_dict={img_feed: im_batch})\r\n    img_mean_np = np.mean(im_batch)\r\n    print('tf img mean: {}, np img mean: {}'.format(img_mean_tf, img_mean_np))\r\n```\r\n", "comments": ["It works as expected in CPU though. Looks like it is related to GPU computation.  ", "If the difference is not large this is not a bug. Reduction operations on GPU are non-deterministic.", "@gongbudaizhe, @ppwwyyxx is correct about reproducibility. Please guantify the difference.", "Ok, so this means GPU is inherently non-deterministic... that explains a lot of things... \r\n\r\nIn terms of reproducibility, for me, the effect turns out to be minimal. Thanks guys. "]}, {"number": 10734, "title": "Pin Java bindings to 1.7", "body": "The android_library bazel rule currently enforces Java 7\r\nhttps://github.com/bazelbuild/bazel/blob/6c1106b1a721516d3b3db54d2e1c31b44a76fbb1/src/main/java/com/google/devtools/build/lib/bazel/rules/android/BazelAndroidSemantics.java#L73\r\n\r\nMore generally, to enable broader use we pin the source and artifacts\r\nto 1.7 till Java 8 gains wider adoption across Android.", "comments": ["Can one of the admins verify this patch?", "Gentle ping, could an admin assign a reviewer? It would be great to have it in before some upcoming PRs - thanks!", "Jenkins, test this please"]}, {"number": 10733, "title": "Timeline incorrectly show most ops on gpu:0", "body": "Timeline show most ops on gpu:0, which disagree with nvidia-smi and device placement log.\r\n\r\nTF version v1.2.0-rc0-735-gf48673b (less than two weeks ago)\r\n\r\nAttachments (remove txt suffix)\r\n[chrome___tracing.pdf](https://github.com/tensorflow/tensorflow/files/1077664/chrome___tracing.pdf)\r\n[trace.json.txt](https://github.com/tensorflow/tensorflow/files/1077680/trace.json.txt)\r\n[jupyter.log.txt](https://github.com/tensorflow/tensorflow/files/1077682/jupyter.log.txt)\r\n[run_metadata.pb.txt](https://github.com/tensorflow/tensorflow/files/1077683/run_metadata.pb.txt)\r\n\r\n", "comments": []}, {"number": 10732, "title": "Padding queue support for tf.estimator generator input pipeline", "body": "So, as I desrcibe [here](https://github.com/tensorflow/tensorflow/issues/10680), there is no padding possibilities in current estimator's input pipelines.\r\n\r\nFor now I added simple possibility to you data have dynamic shape over last axis. \r\nIf such solution looks good, then I am looking forward for next contributing steps.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@Scitator can you use six instead?", "Six done. What test are required?\r\nWill be tests for `_fill_array` and `_pad_if_needed` enough?", "Yes, these are the new functions to be tested.\r\n\r\nIdeally, `_get_integer_next_batch` will also be exercised.", "Ping for @Scitator !", "@Scitator one more ping", "@rmlarsen @vrv @drpngx \r\nSorry guys, was out of the internet for about a mount (even now my connection is very slow).\r\nAdd docs and some tests.", "@Scitator Thanks! \r\n@drpngx @martinwicke could you take another look, please?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@martinwicke \r\nSo, add additional tests (bug found) + change logic for `pad_value` usage.  With changed logic code sometimes looks a bit ugly. \r\n\r\nMain functionality have no tests yet (but works it my personal projects). By the way, where are all generator input pipeline tests? ", "Jenkins, test this please.", "@martinwicke Update indents and generator IO, tested on my [repo](https://github.com/Scitator/YATS2S/blob/versions/tf_1.2/seq2seq_example.ipynb)", "Jenkins, test this please.", "@martinwicke Finally, all check have passed.", "Jenkins, test this please.", "@tensorflow-jenkins test this please.", "Yay! Thanks for the contribution."]}, {"number": 10731, "title": "scope reuse problem variable not exist for using Dense(in contirb.seq2seq.attention_wrapper)", "body": "tf version '1.2.0-rc0' \r\nin attention_wrapper.py(contirb.seq2seq.attention_wrapper), it use below\r\n      \r\n    from tensorflow.python.layers import core as layers_core \r\n    memory_layer = layers_core.Dense(10, name=\"memory_layer\", use_bias=False)  #line 416\r\n\r\nbut this usage will cause un expected result when trying to reuse memory_layer, see below\r\n\r\n    input = tf.constant([[1,2,3], [4,5,6]], dtype=tf.float32)\r\n    with tf.variable_scope('main') as scope:\r\n        memory_layer = layers_core.Dense(10, name=\"memory_layer\", use_bias=False)\r\n        x = memory_layer(input)\r\n        scope.reuse_variables()\r\n        memory_layer = layers_core.Dense(10, name=\"memory_layer\", use_bias=False)\r\n        y = memory_layer(input)\r\n\r\nValueError: Variable main/memory_layer_1/kernel does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n\r\nOne workaround is to change name=\"memory_layer\" to _scope=\"memory_layer\"\r\n\r\n    with tf.variable_scope('main') as scope:\r\n        memory_layer = layers_core.Dense(10, _scope=\"memory_layer\", use_bias=False)\r\n        x = memory_layer(input)\r\n        scope.reuse_variables()\r\n        memory_layer = layers_core.Dense(10, _scope=\"memory_layer\", use_bias=False)\r\n        y = memory_layer(input)\r\n\r\nI think this is a bug for atttention_wrapper.py ? since we can not reuse memory_layer, then we can not train/evaluate in one graph when using attention cell wrapper.\r\n", "comments": ["@fchollet might have some suggestions here.", "@fchollet might have some suggestions here.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@chenghuige Is this still a problem for you?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 112 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 10730, "title": "All the graph have an operation _Retval that makes the train loop slow", "body": "I have tried to train the model in [tf_cnn_benchmarks.py](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py) and [cifar10_multi_gpu_train.py](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py).\r\n\r\nAll the timelines results like the image bellow. There is an operation named '_Retval' in the end of training loop. But as we see, long idle time was leaved in the timeline. How can I make the training faster by getting rid of the operation '_Retval' ? Is there other method to let the gpu be at full power?\r\n\r\n![2017-06-15 19 57 31](https://user-images.githubusercontent.com/9522983/27180240-0e26c2f0-5205-11e7-9f7e-98be45a6a2b2.png)\r\n![2017-06-15 19 58 33](https://user-images.githubusercontent.com/9522983/27180242-120350fa-5205-11e7-89f1-5d3fe6261293.png)\r\n\r\nThe images above were got by the script:\r\n\r\npython tf_cnn_benchmarks.py local_parameter_device=cpu --num_gpus=2 --batch_size=64 --model=vgg16 --variable_update=independent --optimizer=sgd --trace_file=/home/zhaoerchao/timeline_benchmark.json --distortions", "comments": ["@zhangyaobit, could you take a look?", "Could you mark the idle period and  _Retval  in the two images?\r\n\r\nCould you also clarify what are the two timelines/images respectively for?  Are they for tf_cnn_benchmarks.py or cifar10_multi_gpu_train.py?", "Thank you for your apply.\r\n\r\nThe two images in the question are all for tf_cnn_benchmarks.py\r\n\r\nThe second one is zoomed in of the first one. The _Retval operation is in the end of the image. The idle period is the period without operation that is obviously.\r\n", "Could you upload the timeline file somewhere, so that someone of us can take a look?", "Sorry, I am on vacation now.  I will upload the timeline as soon as I come back.", "[timeline_benchmark.json.txt](https://github.com/tensorflow/tensorflow/files/1100812/timeline_benchmark.json.txt)\r\n\r\n@zhangyaobit The timeline file is uploaded, looking forward to your apply !", "@zhaoerchao could you use a python timer to measure the total time of, say 100 steps, and then compute time per step? This will help identify if this is  an issue of Timeline or tf_cnn_benchmarks.", "The script has measured the total time and the time per step, see [time measure code](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py#L670). I think it will have a significance performance improvement if the problem of the GPU idle time is solved.  Can you help to analysis the result of the script ?", "What is the time per step measured by the script? If it is the time without gap, it means the gap is an artifact/bug of Timeline; if it is the time including the gap, it means the gap is a performance issue of tf_cnn_benchmarks.py. ", "@zhangyaobit @zheng-xq  Thank you for your suggestion. The time per step measured includes the gap. I have doubted it is a performance issue of tf_cnn_benchmarks.py. But I tried many ways to train the model, all of them have this issue. But when I monitor the GPU usage by `watch -n 0.1 nvidia-smi`, gpu is always 95%+ used during training time. But the timeline still contains the long idle period. And [cifar10_multi_gpu_train.py](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py) also has this issue.\r\n\r\nBesides, when I run the script on one single GPU:\r\n`python tf_cnn_benchmarks.py local_parameter_device=cpu --num_gpus=\r\n1 --batch_size=64 --model=resnet50 --variable_update=parameter_server --optimizer=sgd`\r\n\r\nI got the timeline [timeline_benchmark_origin.json.txt](https://github.com/tensorflow/tensorflow/files/1107295/timeline_benchmark_origin.json.txt) when the script had no change and [timeline_benchmark_changed.json.txt](https://github.com/tensorflow/tensorflow/files/1107297/timeline_benchmark_changed.json.txt) when [this line](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py#L933) was replaced by `with tf.control_dependencies([]):`.\r\n\r\nThe performance is significantly improved when the operation `global_step.assign_add` has no dependencies. But the improvement is only useful on the special step:\r\n```text\r\nStarting real work at step 10 at time Wed Jun 28 09:49:39 2017\r\nDone warm up\r\nStep    Img/sec loss\r\n1       images/sec: 544.0 +/- 0.0 (jitter = 0.0)        6.776\r\n10      images/sec: 207.7 +/- 33.2 (jitter = 2.1)       5.985\r\n20      images/sec: 201.4 +/- 17.0 (jitter = 1.8)       5.563\r\n30      images/sec: 199.3 +/- 11.4 (jitter = 1.4)       5.343\r\n40      images/sec: 198.2 +/- 8.6 (jitter = 1.0)        5.255\r\n50      images/sec: 197.5 +/- 6.9 (jitter = 1.0)        5.196\r\n60      images/sec: 197.0 +/- 5.8 (jitter = 1.0)        5.167\r\n70      images/sec: 196.6 +/- 5.0 (jitter = 0.9)        5.145\r\n80      images/sec: 196.3 +/- 4.3 (jitter = 0.9)        5.125\r\n90      images/sec: 196.0 +/- 3.9 (jitter = 1.0)        5.111\r\nFinishing real work at step 109 at time Wed Jun 28 09:50:11 2017\r\n----------------------------------------------------------------\r\ntotal images/sec: 192.95\r\n----------------------------------------------------------------\r\n```\r\nMay I ask you to clone the repository [tensorflow high performance model benchmark](https://github.com/tensorflow/benchmarks) and run it to analysis the reason ? \r\n\r\nThank you very much!", "@zheng-xq @yzhwang, could you or re-assign this to someone who is familiar with tf_cnn_benchmarks.py to take a look?", "Can anyone look into this further? I'm also experiencing this issue, and it seems to me that this is either a serious issue with tensorflow's kernel execution/flow management (unlikely) or a bug of the profiler being unable to log certain operations (more probable). Either way, it is difficult to optimize a tensorflow-written pipeline when information provided from the profiling methods is opaque.\r\n![retval_log](https://user-images.githubusercontent.com/14655667/28632229-d6163a1c-71fd-11e7-8ea6-efdc0eb50930.png)\r\n(final op, in green, is _RetVal)", "Including @prb12 since it is related to timeline. Paul could you provide some advice here? Thank you!", "I agree that this looks strange.  The code is clearly waiting for a bunch of operations to complete via control dependencies (`group_deps_3`), and it's not clear to me from the timeline where those are executing or which one is 'slow'.\r\n\r\nThe reason for that seems to be that the device level GPU tracing (in the `/gpu:0/memcpy`  and `/gpu:0/stream:N` timelines) looks to be truncated after 12ms.  The Tensorflow GPU device is still busy queueing up CUDA kernel launches, so we know there ought to be more activity on the CUDA streams!\r\n\r\n![image](https://user-images.githubusercontent.com/11547801/28678542-81fd0462-72a5-11e7-8c18-0264b1c24fb3.png)\r\n\r\nThere is probably a *load* of work still running on the GPU, followed by some memcpyDtoHs to transfer the results back to the host.  Eventually TF will have to wait for all of these to complete before it can update your parameters or return fetched values to the caller of session.run.\r\n\r\nThe 'group_dependencies' you commented out are (intentionally) waiting for all of that work to complete.  `_Retval` ops are the ops that return results from a TensorFlow function, and they would also typically be the point where you would need to copy results back from the device and/or sync the GPU.  (I'm not sure why this model is using functions... @zheng-xq could probably comment here)\r\n\r\nSo, to me it looks like either something is broken in the GPUTracer, or possibly the version of libcupti or the cuda drivers on your machine.\r\n\r\nI would suggest a couple of experiments:\r\n1) Try capturing a trace using the [NVidia tools](https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-nvprof-your-handy-universal-gpu-profiler/)  -- e.g. `nvprof --print_gpu_trace` or use `nvvp` if you prefer.\r\n2) Turn on some logging in the GPUTracer https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/gpu_tracer.cc\r\n\r\nPaul", "I agree nvprof and nvvp provide very useful info.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing due to lack of activity, but please reopen if this still needs attention."]}, {"number": 10729, "title": "tf.nn.max_pool wrong docs?", "body": "### System information\r\nNot Applicable\r\n\r\n### Describe the problem\r\n[API](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) states that ksize has length >= 4, the size of window for each dimension of the input tensor. However, value is a 4-D Tensor so doesn't this mean that ksize should be length == 4? Same for strides.\r\n\r\nDigging into maxpooling_op.cc shows that there's some check that does `==`. Line 212:\r\n\r\n```\r\n    OP_REQUIRES(context, ksize_.size() == 4,\r\n                errors::InvalidArgument(\"Sliding window ksize field must \"\r\n                                        \"specify 4 dimensions\"));\r\n```\r\n\r\n\r\n", "comments": ["@martinwicke, @josh11b , this seems to come from the attribute specifier only allowing a >= optionally and not ==. \r\ni.e.\r\n```\r\nREGISTER_OP(\"AvgPool\")\r\n    .Input(\"value: T\")\r\n    .Output(\"output: T\")\r\n    .Attr(\"ksize: list(int) >= 4\")\r\n    .Attr(\"strides: list(int) >= 4\")\r\n    .Attr(GetPaddingAttrString())\r\n    .Attr(GetConvnetDataFormatAttrString())\r\n    .Attr(\"T: {half, float, double}\")\r\n    .SetShapeFn(shape_inference::AvgPoolShape)\r\n    .Doc(R\"doc(\r\n```\r\nWe can't make it list(int) == 4, because the op registration class doesn't support that for Attr. However, since the attribute constraint gets auto placed in the docs i.e. here:\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/max_pool\r\n\r\nIt is confusing, maybe we should\r\n\r\n1. not  include the list constraints in auto-generated documentation.\r\n2. augment list constraints to allow == as well as >=.\r\n\r\n", "Two things we can do: support more constraint types (good, but work), and add support for = at the beginning of the doc string for attrs suppressing the generated prefix (which currently works for tensor inputs).", "If you are accepting PRs for this and could point me to some resources to study how this works in detail (like the op registration class for one), I'd be happy to do one.", "If you want to fix the >= / = / == parsing issue, the code that processes that lives here in tensorflow/core/framework/op_def_builder.cc line 221ff. You could allow == as a token, and then we could use that in the Op registrations. There's no documentation for this, just the code.\r\n\r\nWe would love a PR for that.", "Josh also suggested there is a := syntax that lets you remove the constraint part in the docs and provide your own text completely.\r\n", "Added PR #11925 for the fix. As both `max_pool` and `avg_pool` are wrapped in the python code, the fix only changes the docstring in tensorflow/python/ops/nn_ops.py."]}, {"number": 10728, "title": "tensorflow.contrib.keras.python.keras.models throwing errors for a valid keras code", "body": "### System information\r\n== cat /etc/issue ===============================================\r\nLinux parikshit-XPS-L322X 4.4.0-79-generic #100-Ubuntu SMP Wed May 17 19:58:14 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux parikshit-XPS-L322X 4.4.0-79-generic #100-Ubuntu SMP Wed May 17 19:58:14 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.0)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.0rc1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.0-rc1\r\ntf.GIT_VERSION = v1.2.0-rc0-24-g94484aa\r\ntf.COMPILER_VERSION = v1.2.0-rc0-24-g94484aa\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\nKeras version 2.0.5 with Tensorflow backend\r\n\r\nUsing PyCharm Community edition 2017.1.3 as editor \r\n\r\n### Describe the problem\r\nI was trying to implement a toy example for One Shot Siamese paper (Gregory Koch etc.) using Keras and found difference in behaviour (errors) between tensorflow.contrib.keras.python.keras.models (i.e using Tensorflow's contrib library for Keras) and keras.models (i.e Keras library with tensorflow backend). Here we have to train two separate CNNs with tied weights and tensorflow contrib library for keras is throwing errors for valid Keras code. Please refer to the code below for difference in behaviour / error\r\n\r\n### Source code (using tensorflow contrib lib for keras)\r\n```python\r\nfrom __future__ import absolute_import, print_function, division\r\nfrom tensorflow.contrib.keras.python.keras.layers import LSTM, Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\r\nfrom tensorflow.contrib.keras.python.keras.models import Model, Sequential\r\nfrom tensorflow.contrib.keras.python.keras.regularizers import l2\r\nfrom tensorflow.contrib.keras.python.keras import backend as K\r\nfrom tensorflow.contrib.keras.python.keras.optimizers import SGD\r\nfrom tensorflow.contrib.keras.python.keras.initializers import RandomNormal\r\n\r\ninput_shape = (105, 105, 1)\r\nleft_input = Input(input_shape)\r\nright_input = Input(input_shape)\r\n\r\nw_init = RandomNormal(mean=0, stddev=1e-2)\r\nb_init = RandomNormal(mean=0.5, stddev=1e-2)\r\n\r\nconvnet = Sequential()\r\nconvnet.add(Conv2D(64, (10, 10), activation='relu', input_shape=input_shape,\r\n                   kernel_initializer=w_init, kernel_regularizer=l2(2e-4)))\r\nconvnet.add(MaxPooling2D())\r\nconvnet.add(Conv2D(128, (7, 7), activation='relu',\r\n                   kernel_regularizer=l2(2e-4), kernel_initializer=w_init,\r\n                   bias_initializer=b_init))\r\nconvnet.add(MaxPooling2D())\r\nconvnet.add(Conv2D(128, (4, 4), activation='relu', kernel_initializer=w_init,\r\n                   kernel_regularizer=l2(2e-4), bias_initializer=b_init))\r\nconvnet.add(MaxPooling2D())\r\nconvnet.add(Conv2D(256, (4, 4), activation='relu', kernel_initializer=w_init,\r\n                   kernel_regularizer=l2(2e-4), bias_initializer=b_init))\r\nconvnet.add(Flatten())\r\nconvnet.add(Dense(4096, activation=\"sigmoid\", kernel_regularizer=l2(1e-3),\r\n                  kernel_initializer=w_init, bias_initializer=b_init))\r\n\r\nl_side = convnet(left_input)\r\nr_side = convnet(right_input)\r\n```\r\n### Output/Error (tensorflow contrib lib for keras)\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/parikshit/PycharmProjects/Toy_example/one_shot.py\", line 53, in <module>\r\n    l_side = convnet(left_input)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/keras/python/keras/engine/topology.py\", line 432, in __call__\r\n    output = super(Layer, self).__call__(inputs, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 439, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/keras/python/keras/models.py\", line 560, in call\r\n    return self.model.call(inputs, mask)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/keras/python/keras/engine/topology.py\", line 1743, in call\r\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/keras/python/keras/engine/topology.py\", line 1957, in run_internal_graph\r\n    self.add_loss(layer.get_losses_for(None), None)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 254, in add_loss\r\n    self._losses += losses\r\nAttributeError: 'Model' object has no attribute '_losses'\r\n```\r\n### Source code (using Keras library with tensorflow backend)\r\n```python\r\nfrom __future__ import absolute_import, print_function, division\r\nfrom keras.layers import LSTM, Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\r\nfrom keras.models import Model, Sequential\r\nfrom keras.regularizers import l2\r\nfrom keras import backend as K\r\nfrom keras.optimizers import SGD\r\nfrom keras.initializers import RandomNormal\r\n\r\ninput_shape = (105, 105, 1)\r\nleft_input = Input(input_shape)\r\nright_input = Input(input_shape)\r\n\r\nw_init = RandomNormal(mean=0, stddev=1e-2)\r\nb_init = RandomNormal(mean=0.5, stddev=1e-2)\r\n\r\nconvnet = Sequential()\r\nconvnet.add(Conv2D(64, (10, 10), activation='relu', input_shape=input_shape,\r\n                   kernel_initializer=w_init, kernel_regularizer=l2(2e-4)))\r\nconvnet.add(MaxPooling2D())\r\nconvnet.add(Conv2D(128, (7, 7), activation='relu',\r\n                   kernel_regularizer=l2(2e-4), kernel_initializer=w_init,\r\n                   bias_initializer=b_init))\r\nconvnet.add(MaxPooling2D())\r\nconvnet.add(Conv2D(128, (4, 4), activation='relu', kernel_initializer=w_init,\r\n                   kernel_regularizer=l2(2e-4), bias_initializer=b_init))\r\nconvnet.add(MaxPooling2D())\r\nconvnet.add(Conv2D(256, (4, 4), activation='relu', kernel_initializer=w_init,\r\n                   kernel_regularizer=l2(2e-4), bias_initializer=b_init))\r\nconvnet.add(Flatten())\r\nconvnet.add(Dense(4096, activation=\"sigmoid\", kernel_regularizer=l2(1e-3),\r\n                  kernel_initializer=w_init, bias_initializer=b_init))\r\n\r\nl_side = convnet(left_input)\r\nr_side = convnet(right_input)\r\n```\r\n### Output/Error (Keras library with tensorflow backend)\r\n```\r\nUsing TensorFlow backend.\r\n\r\nProcess finished with exit code 0\r\n```", "comments": ["@fchollet , could you please take a look. Thanks!", "Thanks for the bug report. I've sent out a CL to fix it.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 10727, "title": "Redundant Computation of tf.cond()", "body": "### System information (Runs all right)\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n-  OS: Linux Ubuntu 14.04.01\r\n- **TensorFlow installed from pip:\r\n- **TensorFlow version (use command below)**:'v1.0.0-65-g4763edf-dirty', '1.0.1\r\n- **CUDA/cuDNN version: CUDA8.0/cuDNN5.1\r\n- **GPU model and memory**: GTX1060 6GB\r\n\r\n### Describe the problem\r\ntf.cond(pred, fn1, fn2) have redundant computation when fn1 and fn2 have dependencies on other tensors. See the example code, you'll know.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\ndef tf_fun1(a):\r\n    def func1(a):\r\n        print 'a1 %s\\n'% a\r\n        return a\r\n    r = tf.py_func(func1, [a], tf.int32)\r\n    return r\r\n\r\ndef tf_fun2(a):\r\n    def func2(a):\r\n        print 'a2 %s\\n'% a\r\n        return a\r\n    r = tf.py_func(func2, [a], tf.int32)\r\n    return r\r\n\r\ndef test():\r\n    is_training = tf.placeholder(tf.bool)\r\n    tensor_a = tf_fun1(1)\r\n    tensor_b = tf_fun2(2)\r\n    cond_tensor = tf.cond(is_training, lambda: tensor_a, lambda: tensor_b)\r\n    cond_func = tf.cond(is_training, lambda: tf_fun1(1),lambda: tf_fun2(2))\r\n    cond_func_with_tensor = tf.cond(is_training, lambda: tf_fun1(tensor_a),lambda: tf_fun2(tensor_b))\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        print 'run cond_tensor'\r\n        value = sess.run(cond_tensor,feed_dict={is_training:True})\r\n        print 'run cond_func'\r\n        value = sess.run(cond_func,feed_dict={is_training:True})\r\n        print 'run cond_func_with_tensor'\r\n        value = sess.run(cond_func_with_tensor,feed_dict={is_training:True})\r\n\r\n```\r\nThe result of running:\r\n```\r\nrun cond_tensor\r\na2 2\r\na1 1\r\nrun cond_func\r\na1 1\r\nrun cond_func_with_tensor\r\na2 2\r\na1 1\r\na1 1\r\n```\r\n", "comments": ["I don't see any bug here. You are creating two disctinct instances of tf_fun1 (and of tf_fun2).  One in tensor_a and one when constructing the condition. When run, it has to evaluate all arguments to tf.cond which runs tensor_a and tensor_b. Then is_training is True, so the subgraph created by \"lambda: tf_fun1(tensor_a)\" is executed which is similar to tensor_a but completely a distinct object, thus you get \"a1 1\" printed again. Closing for now."]}, {"number": 10726, "title": "in the test dataset the use of different batch_size will get different results. ,why?", "body": "\r\nWhen I used kears and tensorflow training after the checkpoint, after stopping the process, when I was found in the test, the use of different batch_size will get different results. The When batch_size = 1 when the error rate is high, but when the batch_size = 128, the error rate is not so high. The The The Why is this? The So what about my online service?", "comments": ["I'm sorry, but I am closing thi, since it is not a TensorFlow question, this is a machine learning algorithm question. Please read references on what batch size means, e.g. here is one of the first google results. \r\nhttps://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\r\n\r\nBest of luck. Thanks!"]}, {"number": 10725, "title": "tf.contrib.data.Dataset.filter kernel error on excluded element", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary (nightly build)\r\n- **TensorFlow version (use command below)**:  1.2.0-rc2 (git version v1.2.0-rc0-1066-g4c0052d)\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GTX 970\r\n- **Exact command to reproduce**: See script\r\n\r\n### Describe the problem\r\nI am trying to use Dataset's `filter()` function to exclude certain examples, but when the iterator hits the index of the first excluded example, a kernel error occurs and the program crashes.\r\n\r\n### Source code / logs\r\n\r\nThis example creates a data set of 100 integers and filters out every third one by checking `x % 3 != 2`. The first two calls to `sess.run()` produce the digits 0 and 1 and they get printed correctly. At the third `sess.run()` call, the program crashes.\r\n\r\nRunning this\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndataset = tf.contrib.data.Dataset.range(100)\r\ndataset = dataset.filter(lambda x: tf.not_equal(tf.mod(x, 3), 2))\r\n\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n\r\nsess = tf.Session()\r\nprint(sess.run(next_element))\r\nprint(sess.run(next_element))\r\nprint(sess.run(next_element))\r\n```\r\n\r\nIt results in this\r\n\r\n```\r\n2017-06-15 10:18:44.797870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-15 10:18:44.797884: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-15 10:18:44.797886: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-15 10:18:44.797888: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-15 10:18:44.797891: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-15 10:18:44.896171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-06-15 10:18:44.896367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] Found device 0 with properties: \r\nname: GeForce GTX 970\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.94GiB\r\nFree memory: 3.42GiB\r\n2017-06-15 10:18:44.896377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:959] DMA: 0 \r\n2017-06-15 10:18:44.896379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:969] 0:   Y \r\n2017-06-15 10:18:44.896384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1028] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)\r\n0\r\n1\r\n2017-06-15 10:18:45.918333: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Expects 1 arguments, but 2 is provided\r\nTraceback (most recent call last):\r\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1267, in _do_call\r\n    return fn(*args)\r\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1248, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Expects 1 arguments, but 2 is provided\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](OneShotIterator)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ede/.IntelliJIdea2017.1/config/scratches/scratch_6.py\", line 14, in <module>\r\n    print(sess.run(next_element))\r\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 896, in run\r\n    run_metadata_ptr)\r\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1108, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1261, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1280, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Expects 1 arguments, but 2 is provided\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](OneShotIterator)]]\r\n\r\nCaused by op 'IteratorGetNext', defined at:\r\n  File \"/home/ede/.IntelliJIdea2017.1/config/scratches/scratch_6.py\", line 8, in <module>\r\n    next_element = iterator.get_next()\r\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 247, in get_next\r\n    name=name))\r\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 282, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2528, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/ede/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1203, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Expects 1 arguments, but 2 is provided\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](OneShotIterator)]]\r\n```\r\n", "comments": ["@vrv, do you have any thoughts on this issue since @mrry is out?", "Thanks for the simple repro!  I'm not entirely familiar with the code unfortunately; I took a quick look at the code and nothing obvious popped out as to why there are two input tensors anywhere (range produces a single element, filter just passes on the type of its input), so we'll need to do some more debugging to figure out why this is happening.  Thanks for the report!", "Okay I think I found the bug and sent it out for review internally, not sure if it's the right solution given my lack of knowledge about the internals, but it at least solves the issue illustrated by this code."]}, {"number": 10724, "title": "Merge pull request #1 from tensorflow/master", "body": "update", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "update", "update", "Please stop spamming the issue tracker with pointless PR."]}, {"number": 10723, "title": "Poor Scalability of TensorFlow MultiGPU Training on a Single Machine [Performance Bug]", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 3.16.7\r\n- **TensorFlow installed from (source or binary)**: source, hash: b1e174e\r\n- **TensorFlow version (use command below)**: v1.1.0-rc2-119-gb1e174e 1.1.0-rc2\r\n- **Bazel version (if compiling from source)**:  0.4.5-jdk7\r\n- **CUDA/cuDNN version**: CUDA 8.0, cudnn-v5.1\r\n- **GPU model and memory**: Titan X with 12 GiB\r\n- **Exact command to reproduce**: ./tf_multiGPU.py\r\n\r\n- **tensorflow/benchmark version**: source, hash: 9165a70\r\n\r\n### Describe the problem\r\nRecently, we are testing the TensorFlow scalability on multiGPU machines. We use the official scripts provided in the [benchmarks](https://www.tensorflow.org/performance/benchmarks) using codes from GitHub repository tensorflow/benchmark. We execute the script according to the official website to test the scalability of TensorFlow on the machine equipped with 8 Titan X GPUs. We test the model VGG16 with batch size equaling to 64.\r\n\r\nThe results are shown in the following table:\r\n\r\n| Num of GPUs | Throughput (images/sec) |\r\n|-------------|-------------------------|\r\n| 1           | 83.13                   |\r\n| 2           | 155.06                  |\r\n| 3           | 211.8                   |\r\n| 4           | 278.51                  |\r\n| 5           | 265.53                  |\r\n| 6           | 268.19                  |\r\n| 7           | 272.8                   |\r\n| 8           | 302.27                  |\r\n\r\nWe are surprised to find that the total throughput of 5 GPUs is smaller than 4 GPUs, which means TensorFlow incurs significant overheads when the number of GPU is larger than 4. Because I don't know whether this performance issue belongs to the tensorflow/tensorflow repository or tensorflow/benchmark repository, I submit this issue here looking forward to the official response. The script for reproducing this issue can be found in Source code/logs section with the results.\r\n\r\nThe scalability is strongly related to the topology of GPU interconnection. In our machine, we have a tree topology for GPUs. The topology details can be found at the [nv-topo-matrix.txt](https://gist.github.com/583d689fd16ee24f1924b83ca3dea5b9.git)\r\n\r\nWe believe this is a performance bug since if more GPUs can not achieve higher throughput, at least they should obtain similar throughput. \r\n\r\n### Source code / logs\r\nSource code: [tf_multiGPU.py](https://gist.github.com/b4024e1d8bfbbdf0cf917798d677aaae.git)\r\nThe log after executing the script above: https://drive.google.com/file/d/0Bw3_V-EwBVToYXVqZDkzQkN6c3M/view?usp=sharing", "comments": ["@tfboyd , could you take a look and offer some suggestions?", "I would have to look closer but I do not find this completely surprising but your number is lower than I would expect given my experience doing the benchmarks..  I believe VGG16 has a lot of parameters.  Did you try different parameter server approaches?  On the DGX-1 replicated (with NCCL) was the best approach for VGG16.  parameter_server on gpu was only a good option on AWS where they have GPUDirect.    I would try the following options when using VGG16:\r\n\r\n```\r\n# My guess is with your ENV this will most likely to give you the best scaling for VGG16\r\n- variable_update = replicated  local_parameter_device = gpu  use_nccl = true\r\nor\r\n# this is a good fall back in most environments.  \r\n- variable_update = parameter_server  local_parameter_device = cpu\r\n```\r\n\r\nIf your Titan X setup is more similar to a DGX-1 in topology, I no longer have access and did not research it then the nccl setup will work best.  If peering is not setup perfectly then cpu is often the best option.  Based on your starting number I would guess you should be able to get over 500 images/sec with one of the options that works best with your topology.  \r\n\r\nAlso VGG16 does not scale as well as the other models as the GPUs speedup.  Here is the [graph](https://www.tensorflow.org/performance/benchmarks#results), with real data it starts to dip and you only hit something like 5.8 or so speedup.  On K80s (much slower GPUs)  the scaling is over 6x on GCE.  I also do not see many people showing VGG16 distributed numbers and from what I have read it is the same reason.  The large number of parameters makes it difficult to scale.  \r\n\r\nNCCL is suppose to \"magically\" solve these types of problems but that does not seem to be the case currently.  My testing showed that the best performance requires trying a few options.  \r\n\r\nI have run other platforms and this can happen on them as well if using an variable placement option that does not work with with the hardware platform.\r\n\r\nI want to stress, these are guesses based on my experience testing on a variety of systems but not remotely all of them.  When it is a system I can access, it is much easier to speak in absolutes.  I hope you get a better speedup trying the other options.  \r\n\r\n", "With \" variable_update = replicated  local_parameter_device = gpu  use_nccl = true\" I can get:\r\n4GPU: 270im/s\r\n8GPU: 530im/s\r\nwith Tesla M40s (roughly same speed as TitanX).\r\n\r\nWith \"parameter_server\" I can still get 4GPU:270im/s, 8GPU:470im/s which is faster than what's reported above. But my GPU topology may be better connected.", "Thanks @ppwwyyxx  It has been a while since we were on the same thread.  :-)  Off topic but DataSets is great if you are going multi-GPU and do not need/want to feed your data via a dict.  It is contrib but is moving to core very soon.  \r\n\r\nOhh did you use synthetic or real data? Just curious I want to use your number as a mental data point if people ask about M40 performance.  I have access to 4x Titan X Maxwells but not a box of  M40s.\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data", "Closing, please let me know if you still have issues.  One of the options I suggested should get you much closer to the published performance numbers.  ", "@tfboyd @ppwwyyxx \r\nHi Toby & Yuxin,\r\n\r\nThanks a lot for your patient explanation. I performed the multiGPU tests for three argument sets and the results are shown as the following tables:\r\nThe abbreviations for the parameter sets are as followed:\r\n1. replicated_gpu_nccl: ```variable_update=replicated local_parameter_device=gpu use_nccl=True```\r\n2. parameter_server_gpu: ```variable_update=parameter_server local_parameter_device=cpu use_nccl=False```\r\n3. parameter_server_gpu_nccl: ```variable_update=parameter_server local_parameter_device=gpu use_nccl=True```\r\n\r\n| Number of GPUs | replicated_gpu_nccl | parameter_server_cpu | parameter_server_gpu_nccl |\r\n|----------------|---------------------|----------------------|---------------------------|\r\n| 1              | 82.31               | 76.53                | 83.14                     |\r\n| 2              | 154.59              | 140.35               | 155.53                    |\r\n| 3              | 214.84              | 219.67               | 212.39                    |\r\n| 4              | 280.17              | 276.82               | 266.04                    |\r\n| 5              | 338.93              | 302.22               | 267.23                    |\r\n| 6              | 405.59              | 370.58               | 275.08                    |\r\n| 7              | 470.66              | 410.25               | 273.08                    |\r\n| 8              | 517.31              | 373.5                | 308.48                    |\r\n\r\nFrom the table presented above, I would like to conclude that the parameter set used in the second column is the best practice for my system and platform. Do the results in the second column achieve the expected speedup of VGG16 on 8 GPUs? \r\n\r\nThank you again for your helpful suggestions! I am still a little confused about two modes for variable_update. What's the difference between \"replicated\" and \"parameter_server\"? Comparing with the second column and the fourth column of the table above, these two modes result in significant performance difference even both of them use GPU as local parameter device and reduce parameters through NCCL. Can anyone help me?\r\n\r\nThanks,\r\nXinfeng", "You should read through the benchmarks page. It does contain detailed explanations to the difference between parameter update strategies.", "@byronyi \r\nThanks for your kind remind! I am sorry that I didn't claim the problem very clearly. To be more specific, my question is that what's the difference causing so significant performance difference. To my poor understanding, both of programs resulting in the second and fourth column data aggregate gradients on one GPU and use NCCL for the reduction and they have the same amount of data transfer. The program using replicated variable updates needs to transfer gradients from each GPU and aggregated gradients to each GPU. While the program using parameter server for variable updates needs to transfer gradients from each GPU and updated gradients from the parameter server. The only difference is that whether each GPU has a local copy of weights. I think the overhead of multiGPU mainly comes from the communication. However, with the amount of transferred data the same, these two programs still have so large performance difference.\r\n\r\nAbove stated is exactly what I am confused. I doubt I have an incorrect understanding of these two modes. Thanks for your patience! ", "First, I am really excited that having the benchmarks public helped us to have a reference point to figure this out (maybe a pat on the back but it justifies the time and money we spend running these tests).  Second, I am glad you were able to try multiple configurations and we have another semi-reference point that most likely M40s and decently connected Titan X (maxwells) likely behave like a DGX-1.  \r\n\r\nMy attempt (others have already given possibly better answers and I will add a few links).  \r\n\r\n- The [high performance models](https://www.tensorflow.org/performance/performance_models) page explains variable management better than I can.\r\n- [NCCL](https://github.com/NVIDIA/nccl) home page\r\n\r\nOne of your configs did not do what you might have expected:\r\n```\r\nparameter_server_gpu_nccl: variable_update=parameter_server local_parameter_device=gpu use_nccl=True\r\n```\r\n\r\nIn the above config what happened was the variables were **spread across the 8 GPUs** and NCCL was not involved.  NCCL is only a valid option if you set parameter_server=replicated and local_ps_device=gpu.  The settings can be confusing and we do not do any or very many checks because it is a tool and there are so many options.  \r\n\r\nDeeply understanding the differences is beyond me.  We had a long internal thread discussing why variable_update=parameter_server and local_parameter_device=cpu is faster on DGX-1 for inception and resnet.  For my level of understanding it seems that the more parameters there are the more likely NCCL will be the better option.  I also know VGG16 does not scale very well in distributed mode (across servers), which is also an indication.  \r\n\r\nI am sorry I cannot provide you a deeper technical explanation.  I would rather say I do not know than make something up.  :-)\r\n\r\n\r\n\r\n", "@tfboyd \r\nHi Toby,\r\n\r\nI am thankful for your patient explanations and helpful information! At least I confirmed that I had a misunderstanding about these modes. It's so great that TensorFlow provides these flexible options. Understanding system performance is always hard including TensorFlow. XD\r\nWe are making our steps forward to understand the performance of TensorFlow and so wonderful we can hear from TensorFlow experts for the insightful guide!\r\n\r\nThanks,\r\nXinfeng\r\n", "I was wondering if you could provide a tutorial to follow which we can train a model on local machine with multiple gpus using nccl library.\r\n\r\nThe only one i found so far is in Chinese:\r\nhttp://openresearch.ai/t/nccl-efficient-tensorflow-multigpu-training/159", "@kazemSafari \r\nI have asked the team to post better examples.  The new distribution strategies makes it very easy to take a model and scale it well across multiple gpus with very few changes if you are using estimator.  While cifar10 Resnet is not going to scale it is easier to follow because it trains quickly.\r\n\r\nhttps://github.com/tfboyd/models/tree/resnet_perf_tweaks/official/resnet\r\nThis [Pull Request ](https://github.com/tensorflow/models/pull/5243)has the commands I used when testing\r\n\r\nThe example is decent but you have to look around for the code because it was made overly modular and lots of functions passed around.  \r\n\r\nThis page explains the UI, I strongly suggest using the Estimator section unless you are already using Keras.  \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/README.md\r\n\r\nThere is an effort as we speak to update tensorflow.org to make this easier to find.  "]}, {"number": 10722, "title": "tf.image.resize_bicubic() produces artifacts in output image", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes. See below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: All platforms\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.0\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\ntf.image.resize_bicubic() produces weird artifacts in output image. To reproduce, please save this image as bbf.png, and run the program below:\r\n\r\nhttps://www.dramafever.com/st/news/images/e09901b7-bb86-4acd-94e3-ee93d2e301cc.png\r\n\r\nThis is what tf.image.resize_bicubic() produces:\r\n\r\n![output](https://user-images.githubusercontent.com/19349719/27169091-41a12772-515d-11e7-9e17-6240f4a07634.jpg)\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\nwith open('bbf.png', 'rb') as f:\r\n    image_bytes = f.read()\r\n\r\nimage = tf.image.decode_image(image_bytes)\r\nimage = tf.expand_dims(image, 0)\r\n\r\nresized_image = tf.image.resize_bicubic(image, [256, 256])\r\n\r\nresized_image = tf.cast(resized_image, tf.uint8)\r\nresized_image = tf.squeeze(resized_image)\r\nencoded_image = tf.image.encode_jpeg(resized_image)\r\n\r\nwith tf.Session() as sess:\r\n    jpg_image = sess.run(encoded_image)\r\n    with open('output.jpg', 'wb') as f:\r\n        f.write(jpg_image)\r\n```", "comments": ["Most bicubic interpolants are not guaranteed to be monotonic, since the cubic interpolant's extrema can exceed the original extrema in the given interval. Just clamp your image to allowable value after running the resize_cubic call. i.e. 0, 255 or 0, 1 depending on float or int types.  ", "@aselle Thanks for the solution. I tried tf.saturate_cast() and it worked beautifully. So is this an intended behavior? Anyway, I feel that it's really easy for anyone to fall into this pitfall. It would be really great if this behavior is well documented.", "Glad it worked. Writing a doc note is a great idea. If you could prepare a pull-request that would be awesome! Thanks!", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 10721, "title": "DataFeeder fails on default random_state", "body": "Fixes `len() of unsized object` error in [DataFeeder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py#L367) due to incorrect object type.\r\n\r\nTested with Python 3.5 and latest Tensorflow (1.1.0). Failed on\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.learn import KMeansClustering\r\nfrom tensorflow.contrib.learn import SKCompat\r\n\r\ndef my_input_fn(df):\r\n    return {k: tf.constant(df[k].values) for k in df.columns}\r\n\r\ndef train_input_fn():\r\n    return my_input_fn(pd.DataFrame(train_data)) # train_data is anything that can be passed to pd.DataFrame()\r\n\r\nmodel = SKCompat(KMeansClustering(num_clusters=k))\r\nmodel.fit(x = train_input_fn(), y = None)\r\n```\r\n\r\nResulted in:\r\n\r\n```\r\ntensorflow/contrib/learn/python/learn/learn_io/data_feeder.py in __init__(self, x, y, n_classes, batch_size, shuffle, random_state, epochs)\r\n    365         0] if x_is_dict else self._x.shape[0]\r\n    366     if self._shuffle:\r\n--> 367       self.indices = self.random_state.permutation(num_samples)\r\n    368     else:\r\n    369       self.indices = np.array(range(num_samples))\r\n\r\nmtrand.pyx in mtrand.RandomState.permutation (numpy/random/mtrand/mtrand.c:39959)()\r\n\r\nmtrand.pyx in mtrand.RandomState.shuffle (numpy/random/mtrand/mtrand.c:38843)()\r\n\r\nTypeError: len() of unsized object\r\n```\r\n\r\nCouldn't find any myself, but is there any workaround for this?", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "@yifeif my bad, I see that either a NumPy array (like in the failing tests) or a Tensor (like in [this](https://www.tensorflow.org/get_started/input_fn) example) can be passed to the feeder so it only should use `value` when Tensors are passed.", "Jenkins, test this please.", "ping @mdymczyk ", "one more ping for @mdymczyk -- can you address the review comments?", "@frankchn @vrv sure, sorry for the delay (long vacation).", "@mdymczyk Could you please address the comments?", "@rmlarsen I asked for clarification (above) 2 weeks ago and got no reply. Thought the way it is now covers all the cases already so I could use a pointer what I'm missing.", "@mdymczyk sorry, missed that.\r\n@drpngx Could you respond, please?", "@mdymczyk friendly ping if there's any update.", "@mdymczyk ping", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "\r\nJenkins, test this please.", "Jenkins, test this please.", "Ignoring CLA, it's confused because of my commits.", "Jenkins, test this please.", "multinomial_test is flaky (and unrelated), ignoring."]}, {"number": 10720, "title": "freeze_graph problem", "body": "When I use freeze_graph, the --input_checkpoint=model.ckpt-8361242, the data is V1 version. But I use the tensorflow v1.1,the data save as model.ckpt-569500.data-00000-of-00001,model.ckpt-569500.index,model.ckpt-569500.meta.How can I deal with?", "comments": ["I believe you can just give \"model\" and it will find the other files or provide the directory containing the model. Perhaps @concretevitamin can comment.", "The old way of doing things, `--input_checkpoint=model.ckpt-8361242` (namely, feeding the prefix of those files), still holds."]}, {"number": 10719, "title": "orthogonal_initializer bug fix", "body": "Fixed a bug in \"orthogonal_initializer\" function in init_ops.py file.\r\n\r\nThe original way of generating a random matrix(**random_uniform**) is not scaled to make the mean value to 0. \r\n- This will cause the value in the first \"block\" of the returned matrix, which is corresponding to the largest singular value, very close to each other. \r\n- In convolutional neural network, if the initial parameters of a kernel are similar to each other, then all the parameters will update very similarly, which is a really bad thing for training.\r\n\r\nThe bug is fixed by simply using the **random_normal** function to generate the random matrix. This will work because the default parameters for random_normal are mean=0.0, stddev=1.0", "comments": ["Can one of the admins verify this patch?", "@ningxynf01 thanks for the PR. I realized that the change is made to branch r0.12 and it has already been fixed in master branch. We don't usually patch released branches. Could you try a newer binary or build from a newer branch? Thanks!", "@yifeif Thank you for the reply! I used to think r0.12 is the latest version, thank you for letting me know!"]}, {"number": 10718, "title": "bazel build error: no such package '@protobuf//'", "body": "Build from master 0d2f691, but encounter with the error of \"/tensorflow/tools/pip_package/BUILD:94:1: no such package '@protobuf//': Traceback (most recent call last)\". They are operated in virtualenv with bazel 0.5.1 on a centos 7.3.1611. \r\n\r\nThe package can be downloaded in a browser from the url \"http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz\". I'm in China, where it is difficult to access the google service. I have no idea about whether the network is the problem.\r\n\r\n**Here is the terminal log** of brach version, configure and bazel build.\r\n\r\n`\r\n(tf-source) [huwh1@huwh1-centos tensorflow]$ git branch -v\r\n* master 0d2f691 [ahead 16] Create tf_env_collect.sh\r\n\r\n(tf-source) [huwh1@huwh1-centos tensorflow]$ ./configure \r\nYou have bazel 0.5.1 installed.\r\nPlease specify the location of python. [Default is /home/huwh1/virtualenv/tf-source/bin/python]: \r\nFound possible Python library paths:\r\n  /home/huwh1/virtualenv/tf-source/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/huwh1/virtualenv/tf-source/lib/python2.7/site-packages]\r\n\r\nUsing python library path: /home/huwh1/virtualenv/tf-source/lib/python2.7/site-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] \r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] \r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] \r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] \r\nnvcc will be used as CUDA compiler\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 3.0\r\nDo you wish to build TensorFlow with MPI support? [y/N] \r\nMPI support will not be enabled for TensorFlow\r\nConfiguration finished\r\n\r\n(tf-source) [huwh1@huwh1-centos tensorflow]$ bazel clean\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\n(tf-source) [huwh1@huwh1-centos tensorflow]$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package | tee\r\n____Loading package: tensorflow/tools/pip_package\r\n____Loading package: @bazel_tools//tools/cpp\r\n____Loading package: tensorflow/contrib/specs\r\n____Loading package: @zlib_archive//\r\n____Loading package: third_party/eigen3\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 87,161 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 311,205 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 1,475,383 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 1,957,187 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 4,274,206 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 2,138,182 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 1,875,852 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 4,264,858 bytes\r\nERROR: /home/huwh1/git/tensorflow/tensorflow/tools/pip_package/BUILD:94:1: no such package '@protobuf//': Traceback (most recent call last):\r\n\tFile \"/home/huwh1/git/tensorflow/tensorflow/workspace.bzl\", line 122\r\n\t\t_apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n\tFile \"/home/huwh1/git/tensorflow/tensorflow/workspace.bzl\", line 113, in _apply_patch\r\n\t\t_execute_and_check_ret_code(repo_ctx, cmd)\r\n\tFile \"/home/huwh1/git/tensorflow/tensorflow/workspace.bzl\", line 97, in _execute_and_check_ret_code\r\n\t\tfail(\"Non-zero return code({1}) when ..., <2 more arguments>))\r\nNon-zero return code(256) when executing 'patch -p1 -d /home/huwh1/.cache/bazel/_bazel_huwh1/571d325cc1cf529816d64b96c622cea4/external/protobuf -i /home/huwh1/git/tensorflow/third_party/protobuf/add_noinlines.patch':\r\nStdout: \r\nStderr: java.io.IOException: Cannot run program \"patch\" (in directory \"/home/huwh1/.cache/bazel/_bazel_huwh1/571d325cc1cf529816d64b96c622cea4/external/protobuf\"): error=2, No such file or directory and referenced by '//tensorflow/tools/pip_package:licenses'.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\n____Elapsed time: 3.457s\r\n`\r\n\r\n**Here is the system information:**\r\n\r\n`== cat /etc/issue ===============================================\r\nLinux huwh1-centos 3.10.0-514.21.1.el7.x86_64 #1 SMP Thu May 25 17:04:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7 (Core)\"\r\nVERSION_ID=\"7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux huwh1-centos 3.10.0-514.21.1.el7.x86_64 #1 SMP Thu May 25 17:04:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.0)\r\nprotobuf (3.3.0)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\nImportError: No module named pywrap_tensorflow_internal\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64:/opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2017.4.196/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2017.4.196/linux/mkl/lib/intel64_lin:/usr/local/cuda-8.0/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Jun 15 10:28:42 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro K420         Off  | 0000:01:00.0     Off |                  N/A |\r\n| 25%   44C    P8    N/A /  N/A |      9MiB /  1998MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      6899    C   /home/huwh1/virtualenv/tf-gpu/bin/python         7MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n`\r\n", "comments": ["Make sure you have the patch command installed.", "@ppedrioli Thank you for your reply! Could you please provide some more details? I have \"pip install --upgrade patch\" in the virtualenv of python, but the same problem remains ...", "@huwh1 The one you want is not the python version.\r\n\r\nYou should install it via your OS package manager (not the python one). \r\nFor instance for CentOS you will need something like:\r\n`sudo yum install patch`", "Thank you very much! @ppedrioli The build progress can be finished with the \"patch\" installed.", "try 'bazel clean' befoew /.configure. It worked for me", "Also, as suggested [here](https://github.com/tensorflow/serving/issues/579#issuecomment-326483655), try doing:\r\n\r\n    git submodule update --recursive"]}, {"number": 10717, "title": "To fix link error", "body": "To fix link error when building //tensorflow/cc:tutorials_example_trainer\r\nThe local environment is centos release 6.8 (Final), gcc 4.9.2, bazel 0.5.0.\r\nError detail: /opt/rh/devtoolset-3/root/usr/bin/ld: bazel-out/local_linux-opt/bin/tensorflow/cc/_objs/tutorials_example_trainer/tensorflow/cc/tutorials/example_trainer.o: undefined reference to symbol 'clock_gettime@@GLIBC_2.2.5'", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "`-lrt` doesn't work on MacOS. If you can guard this appropriately, it should be fine, there are other platform specific flags in the BUILD files.\r\n\r\nSee test log for MacOS.", "Commit again. Use the select in linkopts.", "Jenkins, test this please.", "Thanks!"]}, {"number": 10716, "title": "To fix link error when building //tensorflow/cc:tutorials_example_trainer", "body": "The local environment is centos release 6.8 (Final), gcc 4.9.2, bazel 0.5.0.\r\nError detail: /opt/rh/devtoolset-3/root/usr/bin/ld: bazel-out/local_linux-opt/bin/tensorflow/cc/_objs/tutorials_example_trainer/tensorflow/cc/tutorials/example_trainer.o: undefined reference to symbol 'clock_gettime@@GLIBC_2.2.5'", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Appears superceded by #10717."]}, {"number": 10715, "title": "Feature Request: Update tf.image.sample_distorted_bounding_box to use Scalar Tensor Inputs", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.1\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n8.0/5.1\r\n- **GPU model and memory**:\r\nTitan X\r\n- **Exact command to reproduce**:\r\n\r\nCan you please update the scalar inputs to tf.image.sample_distorted_bounding_box to also be accepted as Tensors.  I am dynamically creating min_object_covered values (random values) but currently can not input them to the function", "comments": ["This looks like it could be done w/o breaking shape inference, but it would need to be done w/o breaking graphdef compatibility by creating a duplicate of the sample bounding box op as a V2. I am labeling as contributions welcome since this is not likely going to be worked on soon.\r\n\r\nOne work around would be to make a few discrete values that would be acceptible and use a switch statement to select between several bounding box ops.", "Created a PR #10840 for this issue."]}, {"number": 10714, "title": "undefined symbol error when loading zero_out custom op", "body": "### System information\r\n- **Implement zero_out custom op as defined in tensorflow tutorial**: no custom code at all\r\n- **OS Platform and Distribution**: Ubuntu 16.04.2 LTS (Xenial Xerus) / 4.4.0-75-generic #96-Ubuntu SMP Thu Apr 20 09:56:33 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.1.0-rc2 / v1.1.0-rc2-817-geb11d6b\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n\r\n### The problem\r\nUndefined symbol error when using zero_out custom op. It appears after I follow the tensorflow tutorial:\r\n\r\n$ bazel build --config opt //tensorflow/core/user_ops:zero_out.so is OK\r\n> ____Loading complete.  Analyzing...\r\n____Found 1 target...\r\n____Building...\r\n____[0 / 1] BazelWorkspaceStatusAction stable-status.txt\r\nTarget //tensorflow/core/user_ops:zero_out.so up-to-date:\r\n  bazel-bin/tensorflow/core/user_ops/zero_out.so\r\n____Elapsed time: 0.464s, Critical Path: 0.01s\r\n\r\n$ python zero_output_op_test.py**:\r\n>======================================================================\r\nERROR: testZeroOut (__main__.ZeroOutTest)\r\nTraceback (most recent call last):\r\n  File \"zero_out_op_test.py\", line 5, in testZeroOut\r\n    zero_out_module = tf.load_op_library('/srv/workspace/tensorflow/bazel-bin/tensorflow/core/user_ops/zero_out.so')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/load_library.py\", line 64, in load_op_library\r\n    None, None, error_msg, error_code)\r\nNotFoundError: /srv/workspace/tensorflow/bazel-bin/tensorflow/core/user_ops/zero_out.so: undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringB5cxx11Ev\r\nRan 2 tests in 0.040s\r\nFAILED (errors=1)\r\n\r\n### Source code / logs\r\n- **zero_out.cc**:\r\n```\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nclass ZeroOutOp : public OpKernel {\r\n public:\r\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* context) override {\r\n    // Grab the input tensor\r\n    const Tensor& input_tensor = context->input(0);\r\n    auto input = input_tensor.flat<int32>();\r\n\r\n    // Create an output tensor\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\r\n                                                     &output_tensor));\r\n    auto output = output_tensor->flat<int32>();\r\n\r\n    // Set all but the first element of the output tensor to 0.\r\n    const int N = input.size();\r\n    for (int i = 1; i < N; i++) {\r\n      output(i) = 0;\r\n    }\r\n\r\n    // Preserve the first input value if possible.\r\n    if (N > 0) output(0) = input(0);\r\n  }\r\n};\r\n\r\nREGISTER_OP(\"ZeroOut\")\r\n    .Input(\"to_zero: int32\")\r\n    .Output(\"zeroed: int32\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n      c->set_output(0, c->input(0));\r\n      return Status::OK();\r\n    });\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);\r\n```\r\n\r\n-**BUILD**:\r\n```\r\nload(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\r\n\r\ntf_custom_op_library(\r\n    name = \"zero_out.so\",\r\n    srcs = [\"zero_out.cc\"],\r\n)\r\n```\r\n-**zero_out_op_test.py**:\r\n```import tensorflow as tf\r\n\r\nclass ZeroOutTest(tf.test.TestCase):\r\n  def testZeroOut(self):\r\n    zero_out_module = tf.load_op_library('zero_out.so')\r\n    with self.test_session():\r\n      result = zero_out_module.zero_out([5, 4, 3, 2, 1])\r\n      self.assertAllEqual(result.eval(), [5, 0, 0, 0, 0])\r\n\r\nif __name__ == \"__main__\":\r\n  tf.test.main()\r\n```\r\n\r\n\r\n", "comments": ["I had the same issue.\r\n\r\nSince your GCC version is > 5, you need to add the -D_GLIBCXX_USE_CXX11_ABI=0 flag.\r\n\r\nSee here: https://github.com/tensorflow/tensorflow/issues/9137#issuecomment-294097780", "@bbouffaut, could you see if this fixes it for you.", "Thanks @aselle @sampepose for the idea. I already tried this solution and it did not work, but I'm not sure where the option shall be applied: in the \"bazel build\" command of the custom op? or in the \"bazel buil\" of tensorflow? or elsewhere ? Many thanks for your help.", "Hi @aselle  @sampepose, I added --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" both on tensorflow and zero_out custom op compilation command (bazel build --config=opt  --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" ...\", then it works well.\r\n\r\nMany thanks for your help!\r\n\r\nBR,\r\nBaptiste."]}, {"number": 10713, "title": "[Documentation] cifar10_input.py is referenced but missing on master", "body": "### Describe the problem\r\nFile `cifar10_input.py` is referenced in documentation, but is missing on `master`.\r\n\r\nDocumentation piece:\r\nhttps://www.tensorflow.org/programmers_guide/reading_data\r\n\r\nr0.7 version\r\nhttps://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/models/image/cifar10/cifar10_input.py\r\n", "comments": ["@wolffg, please take a look!", "also missing on [cifar10 tutorial](https://www.tensorflow.org/tutorials/deep_cnn)\r\nin r1.2 version\r\n[tensorflow_models/tutorials/image/cifar10/](https://www.github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/image/cifar10/)", "see this issue: https://github.com/tensorflow/tensorflow/issues/10711 \r\nseems to need a fix on the tutorial doc ", "This is now fixed."]}, {"number": 10712, "title": "GPU Error with tf.losses.sparse_softmax_cross_entropy", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: See below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary (PIP)\r\n- **TensorFlow version (use command below)**: v1.1.0-rc0-61-g1ec6ed5 1.1.0\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: 8.0/5.1.5\r\n- **GPU model and memory**: NVIDIA Titan X - 12GB\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ny = np.random.randint(0, 10, size=40, dtype=np.int32)\r\nlogits = np.float32(np.random.rand(40, 10))\r\nweights = np.ones(40, dtype=np.float32)\r\n\r\ny_t = tf.placeholder(tf.int32)\r\nlogits_t = tf.placeholder(tf.float32)\r\n\r\ncost = tf.losses.sparse_softmax_cross_entropy(labels=y,logits=logits, weights=tf.ones(40,dtype=tf.float32))\r\nsess = tf.Session()\r\nsess.run(cost, feed_dict={y_t: y, logits_t: logits})\r\n```\r\n\r\n### Describe the problem\r\nAm I doing something wrong or does this function not have a GPU implementation yet?  I guess I can do this on the CPU, but I thought I would check.  It does work on the CPU, I did check that.\r\n\r\n### Source code / logs\r\nThe error message is attached.\r\n\r\n[error_message.txt](https://github.com/tensorflow/tensorflow/files/1075808/error_message.txt)\r\n\r\n", "comments": ["```\r\ncost = tf.losses.sparse_softmax_cross_entropy(labels=y,logits=logits, weights=tf.ones(40,dtype=tf.float32))\r\n```\r\nlabels should be `y_t` in this line I think. `y` is type int32, which is causing the error.\r\n", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}]