[{"number": 16595, "title": "Branch 183846994", "body": "", "comments": ["There was a merge conflict in tensorflow/compiler/tf2xla/kernels/pooling_ops.cc related to https://github.com/tensorflow/tensorflow/commit/995378c4c9ff156cae7a365cfdc1480a3ee6d0bf#diff-97f39b11a5fef52e6b277b89e298cffb\r\n\r\nI resolved it by taking internal version which seemed to be more recent. Pretty sure that is what I shoulod have done, but can you confirm Russell", "Confirmed.  We applied that patch manually to fix an internal breakage; we also needed to add support for the gradient operators.  Thanks for checking.", "Closing as it seems sync was handled by someone else today.\r\nhttps://github.com/tensorflow/tensorflow/pull/16604"]}, {"number": 16594, "title": "Layers created with tf.layers not listed with tf.contrib.framework.get_model_variables", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: Not Applicable\r\n- **GCC/Compiler version (if compiling from source)**: Not Applicable\r\n- **CUDA/cuDNN version**: 8/6\r\n- **GPU model and memory**: GeForce 940MX\r\n\r\n### Describe the problem\r\nCurrently I'm using [slim models](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models) to build my networks, but for extra layers I'm using the [tf.layers](https://www.tensorflow.org/api_docs/python/tf/layers) API. When I try to retrieve the list of variables with the function [`tf.contrib.framework.get_model_variables`](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/framework/get_model_variables)\r\nonly the layers created inside slim models are retrieved.\r\n\r\n### Minimal code to reproduce the issue\r\n\r\n```python\r\nimport tensorflow as tf\r\nx = tf.random_normal([2,5],name='x')\r\ny = tf.contrib.layers.fully_connected(x,10,scope='Contrib_Fully_Connected') # Retrieved\r\nz = tf.layers.dense(x,10, name='Layers_Fully_Connected') # Not Retrieved\r\nprint(tf.contrib.framework.get_model_variables())\r\n```\r\n\r\nOutput:\r\n\r\n```bash\r\n[<tf.Variable 'Contrib_Fully_Connected/weights:0' shape=(5, 10) dtype=float32_ref>, <tf.Variable 'Contrib_Fully_Connected/biases:0' shape=(10,) dtype=float32_ref>]\r\n```\r\n\r\n### Workaround:\r\n\r\nUse [`tf.get_collection`](https://www.tensorflow.org/api_docs/python/tf/get_collection)\r\n\r\n```python\r\nimport tensorflow as tf\r\nx = tf.random_normal([2,5],name='x')\r\ny = tf.contrib.layers.fully_connected(x,10,scope='Contrib_Fully_Connected') # Retrieved\r\nz = tf.layers.dense(x,10, name='Layers_Fully_Connected') # Not Retrieved\r\nprint(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\r\n```\r\nthe output is:\r\n```bash\r\n[<tf.Variable 'Contrib_Fully_Connected/weights:0' shape=(5, 10) dtype=float32_ref>,\r\n <tf.Variable 'Contrib_Fully_Connected/biases:0' shape=(10,) dtype=float32_ref>,\r\n <tf.Variable 'Layers_Fully_Connected/kernel:0' shape=(5, 10) dtype=float32_ref>,\r\n <tf.Variable 'Layers_Fully_Connected/bias:0' shape=(10,) dtype=float32_ref>]\r\n```\r\n\r\nIs it worth to bring this issue as a bug and improve the funcionality of `tf.contrib.framework.get_model_variables` or will this contrib function be discontinued?", "comments": ["I think it's worth sending a fix if you know what the problem is.", "I am not sure whether this is by design since in [get_model_variables](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/variables.py#L338) it set the collection as ops.GraphKeys.MODEL_VARIABLES instead of tf.GraphKeys.GLOBAL_VARIABLES as @Tauranis did for tf.get_collection above. \r\n@drpngx, could you please help confirm this?", "I see.\r\n\r\n@martinwicke do you have more context on the collection names and which one we're supposed to be using?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@allenlavoie is working on a better way of getting to variables. Even now, I would generally prefer the .variables property of layer objects.\r\n\r\nHowever, this is a bug, and we could add code to tf.layers.Layer to fix this. Marking contributions welcome."]}, {"number": 16593, "title": "contrib.tfgan: batch_norm is_training=True for both training and inferencing, non-slim version", "body": "Hi, I am exploring contrib.tfgan, such a great work @joel-shor .\r\n\r\n### batch_norm is_training=True for both training and inferencing\r\nHowever, when I see the example in source code of both generator and discriminator of MNIST, as below.\r\n\r\nhttps://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb\r\n\r\n`with slim.arg_scope(\r\n        [layers.fully_connected, layers.conv2d_transpose],\r\n        activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,\r\n        weights_regularizer=layers.l2_regularizer(weight_decay)):\r\n        net = layers.fully_connected(noise, 1024)\r\n        net = layers.fully_connected(net, 7 * 7 * 256)\r\n        net = tf.reshape(net, [-1, 7, 7, 256])`\r\n\r\nThe default argument of layers.batch_norm is set to True, and this gen_fn and dis_fn are used for  both training phase and generating test images phase (inferencing).\r\n\r\nIs it a bug or it is intended? If it is intended, can you explain why is that?\r\n\r\n### non-slim implementation\r\nIn addition, I don't really like slim, and I believe some people don't either. Can I use other model construction libraries like tf.layers or keras to build the network. Is tfslim a must?\r\n\r\nThank you,\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.5 and 1.4.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.7\r\n- **GCC/Compiler version (if compiling from source)**: 4.2\r\n- **CUDA/cuDNN version**:NA (CPU)\r\n- **GPU model and memory**:NA\r\n- **Exact command to reproduce**:", "comments": ["/CC @joel-shor ", "You're right; that's a bug. I'll fix it shortly.\r\n\r\nTo answer your other question: yes, you can define the functions however you'd like. I used slim to keep the definitions short, but TFGAN is framework-agnostic.", "Thank you very much,\r\n\r\nAlong with that, I can show me how or change the codes somehow that I can feed custom arguments to the graph that differs during training mode and inference mode. For example, dropout and batch norm behaves differently between two modes, or `tf.cond(is_training, true_fn, false_fn)`.", "Commit https://github.com/tensorflow/models/commit/5f99e5893d6d72e6161a3c32d3d6e3e7fc3bbe4d fixes the batch norm issues.\r\n\r\nIn regards to your question about differences in training/inference: the PR I am submitting demonstrates how to take advantage of a custom argument such as `is_training`. Note that this behavior is already supported in the GANEstimator, where a generator or discriminator that has a special argument called `mode` will have the mode passed to it.", "@joel-shor thanks for the great work on TFGAN.\r\n\r\nI was wondering the same as @nxphi47 because in the `gan_train` code I don't see a distinction between training and inference.\r\nI'd suspect during the generator step the discriminator should work in inference mode (passing `is_training=false` to batch norm and dropout) while during the discriminator step the generator should run in inference mode. I don't see this behavior enforced anywhere in the code. Am I missing something obvious or is this intended?\r\nUnfortunately PR #3370 doesn't exist and I couldn't find the commit you mentioned anywhere in the commit history, could you update the reference?", "I updated the reference. See the TFGAN MNIST example (https://github.com/tensorflow/models/tree/master/research/gan/mnist) for how to get different train and inference behavior.", "@joel-shor Thanks for the fast response and the updated link.\r\n\r\nSorry, I should've been more clear in my comment above. I was talking about the distinction between training and inference of the different models **during** adversarial training.\r\nMeaning the generator should work in inference mode during the discriminator updates and vice versa. This is also how [this DCGAN implementation](https://towardsdatascience.com/implementing-a-generative-adversarial-network-gan-dcgan-to-draw-human-faces-8291616904a) handles batch norm.\r\n\r\nShould TFGAN handle this the same way? I think it's easiest to explain this with some (pseudo) code:\r\n```patch\r\ncommit b2b6fbc857325298de402eef589281dcebd5ae5c\r\nAuthor: Lukas Geiger <lukas.geiger94@gmail.com>\r\nDate:   Sun Apr 1 14:39:53 2018 +0200\r\n\r\n    is_training\r\n\r\ndiff --git a/tensorflow/contrib/gan/python/train.py b/tensorflow/contrib/gan/python/train.py\r\nindex 73acd05b60..8cf2b316aa 100644\r\n--- a/tensorflow/contrib/gan/python/train.py\r\n+++ b/tensorflow/contrib/gan/python/train.py\r\n@@ -31,21 +31,26 @@ from __future__ import absolute_import\r\n from __future__ import division\r\n from __future__ import print_function\r\n\r\n+import functools\r\n+\r\n from tensorflow.contrib.framework.python.ops import variables as variables_lib\r\n from tensorflow.contrib.gan.python import losses as tfgan_losses\r\n from tensorflow.contrib.gan.python import namedtuples\r\n from tensorflow.contrib.slim.python.slim import learning as slim_learning\r\n from tensorflow.contrib.training.python.training import training\r\n+from tensorflow.python.framework import dtypes\r\n from tensorflow.python.framework import ops\r\n from tensorflow.python.ops import array_ops\r\n from tensorflow.python.ops import check_ops\r\n from tensorflow.python.ops import init_ops\r\n+from tensorflow.python.ops import math_ops\r\n from tensorflow.python.ops import variable_scope\r\n from tensorflow.python.ops.distributions import distribution as ds\r\n from tensorflow.python.ops.losses import losses\r\n from tensorflow.python.training import session_run_hook\r\n from tensorflow.python.training import sync_replicas_optimizer\r\n from tensorflow.python.training import training_util\r\n+from tensorflow.python.util import tf_inspect as inspect\r\n\r\n\r\n __all__ = [\r\n@@ -101,6 +106,14 @@ def gan_model(\r\n     ValueError: If the generator outputs a Tensor that isn't the same shape as\r\n       `real_data`.\r\n   \"\"\"\r\n+  is_generator_training = array_ops.placeholder(\r\n+      dtype=dtypes.bool, name='IS_GENERATOR_TRAINING')\r\n+  if 'is_training' in inspect.getargspec(generator_fn).args:\r\n+    generator_fn = functools.partial(\r\n+        generator_fn, is_training=is_generator_training)\r\n+  if 'is_training' in inspect.getargspec(discriminator_fn).args:\r\n+    discriminator_fn = functools.partial(\r\n+        discriminator_fn, is_training=math_ops.logical_not(is_generator_training))\r\n   # Create models\r\n   with variable_scope.variable_scope(generator_scope) as gen_scope:\r\n     generator_inputs = _convert_tensor_or_l_or_d(generator_inputs)\r\n@@ -792,7 +805,7 @@ def gan_train_ops(\r\n class RunTrainOpsHook(session_run_hook.SessionRunHook):\r\n   \"\"\"A hook to run train ops a fixed number of times.\"\"\"\r\n\r\n-  def __init__(self, train_ops, train_steps):\r\n+  def __init__(self, train_ops, train_steps, is_generator=True):\r\n     \"\"\"Run train ops a certain number of times.\r\n\r\n     Args:\r\n@@ -803,10 +816,11 @@ class RunTrainOpsHook(session_run_hook.SessionRunHook):\r\n       train_ops = [train_ops]\r\n     self._train_ops = train_ops\r\n     self._train_steps = train_steps\r\n+    self._is_generator = is_generator\r\n\r\n   def before_run(self, run_context):\r\n     for _ in range(self._train_steps):\r\n-      run_context.session.run(self._train_ops)\r\n+      run_context.session.run(self._train_ops, feed_dict={'IS_GENERATOR_TRAINING:0': self._is_generator})\r\n\r\n\r\n def get_sequential_train_hooks(train_steps=namedtuples.GANTrainSteps(1, 1)):\r\n@@ -821,9 +835,11 @@ def get_sequential_train_hooks(train_steps=namedtuples.GANTrainSteps(1, 1)):\r\n   \"\"\"\r\n   def get_hooks(train_ops):\r\n     generator_hook = RunTrainOpsHook(train_ops.generator_train_op,\r\n-                                     train_steps.generator_train_steps)\r\n+                                     train_steps.generator_train_steps,\r\n+                                     is_generator=True)\r\n     discriminator_hook = RunTrainOpsHook(train_ops.discriminator_train_op,\r\n-                                         train_steps.discriminator_train_steps)\r\n+                                         train_steps.discriminator_train_steps,\r\n+                                         is_generator=False)\r\n     return [generator_hook, discriminator_hook]\r\n   return get_hooks\r\n\r\n```", "@joel-shor Thanks for reopening.\r\n\r\nWould a change similar to the above be a welcome PR?\r\nIf yes, is there a way to do this without introducing a global variable that is referenced via it's name?", "Any updates on this?", "After testing this more it looks like `tfgan` might do the right thing. I misread the DCGAN implementation mentioned above: They're using the same behaviour as `tfgan`.\r\n\r\nSome quick tests on our side suggest that it is indeed better to not run batch norm in inference mode during adverserial training.\r\n\r\nIn any case it would be great to get some feedback from GAN experts.", "Apologies for the delay. I'm just now returning from extended vacation. I will investigate this and review your proposal by end-of-day Thursday.\r\n\r\nThanks for your patience.", "No Problem, thanks for your response.", "Hello, I had the same question. Once a network is created, and used within the _same_ session run in inference and training modes, I'm not sure how the `training` parameter of the batch norm can be set. In the examples given, it seems it is hard-coded as `True` upon creation ?", "I have the exact same question... awaiting response @joel-shor ", "> After testing this more it looks like `tfgan` might do the right thing. I misread the DCGAN implementation mentioned above: They're using the same behaviour as `tfgan`.\r\n> \r\n> Some quick tests on our side suggest that it is indeed better to not run batch norm in inference mode during adverserial training.\r\n> \r\n> In any case it would be great to get some feedback from GAN experts.\r\n\r\nSame observation here. It might be fine to hard code `is_training = True`. I think it works more like a weakened instance normalization.", "Hi @nxphi47 !It seems you are using older versions(1.x versions) of Tensorflow which is not supported any more.  Please visit this thread to upgrade your codebase .[link1](https://www.tensorflow.org/addons) . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 16592, "title": "Freeze-graph not working, allocating too much memory for inference", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.5.0-rc1-1783-g7d7dce1', '1.5.0-rc1')\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: Build label: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0\r\n- **CUDA/cuDNN version**: N/A (CPU only)\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\nsave_alexnet_checkpoint.py\r\n```\r\nimport os\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom alexnet import AlexNet\r\nfrom datagenerator import ImageDataGenerator\r\nfrom datetime import datetime\r\nfrom tensorflow.contrib.data import Iterator\r\n\r\n\"\"\"\r\nConfiguration Part.\r\n\"\"\"\r\n\r\nnum_classes = 1000\r\ncheckpoint_path = \"path/to/ckpt\"\r\n\r\n\r\n# Create parent path if it doesn't exist\r\nif not os.path.isdir(checkpoint_path):\r\n    os.mkdir(checkpoint_path)\r\n\r\n# TF placeholder for graph input and output\r\n\r\n\r\n# Initialize model\r\nx = tf.placeholder(tf.float32, [1, 227, 227, 3],name=\"input\")\r\nkeep_prob=tf.placeholder(tf.float32,[],name=\"keepProbs\")\r\nmodel = AlexNet(x, keep_prob, num_classes, [])\r\nsoftmax = tf.nn.softmax(model.fc8,name=\"softmax\")\r\n\r\n# Initialize an saver for store model checkpoints\r\nsaver = tf.train.Saver()\r\n\r\n# Start Tensorflow session\r\nwith tf.Session() as sess:\r\n\r\n\t# Validate the model on the entire validation set\r\n\tsess.run(tf.global_variables_initializer())\r\n\r\n\t# Load the pretrained weights into the non-trainable layer\r\n\tmodel.load_initial_weights(sess)\r\n\r\n\t# save checkpoint of the model\r\n\tcheckpoint_name = os.path.join(checkpoint_path,\r\n\t\t                       'original_alexnet.ckpt')\r\n\tsave_path = saver.save(sess, checkpoint_name)\r\n\r\n\ttf.train.write_graph(sess.graph_def, checkpoint_path, 'alexnet_def.pb',as_text=False)\r\n\r\n\tnames=[]\r\n\tfor n in tf.get_default_graph().as_graph_def().node:\r\n\t\tnames.append(str(n.name))\r\n\r\n    \tnames = sorted(names, key=str.lower)\r\n    \tfor n in names:\r\n\t\tprint n\r\n\r\n\r\n\tprint(\"Model checkpoint saved at {}\".format(checkpoint_name))\r\n```\r\n<br><br>\r\n\r\nFreeze saved graph_def and weights:\r\n```\r\nbazel build tensorflow/python/tools:freeze_graph\r\nbazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/path/to/alexnet_def.pb -- input_checkpoint=path/to/ckpt/original_alexnet.ckpt --output_graph=/path/to/frozen_alexnet.pb --output_node_names='softmax' --input_binary=True\r\n```\r\n<br><br>\r\n\r\nForward Inference:\r\n```\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nfrom alexnet import AlexNet\r\nfrom tensorflow.python.client import timeline\r\nfrom caffe_classes import class_names\r\n\r\nfrozen_graph='/path/to/frozen_alexnet.pb '\r\n\r\ndef load_graph(frozen_graph_filename):\r\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(graph_def, name=\"prefix\")\r\n    return graph\r\n\r\nimagenet_mean = np.array([104., 117., 124.], dtype=np.float32)\r\n\r\ncurrent_dir = os.getcwd()\r\nimage_dir = os.path.join(current_dir, 'images')\r\n\r\nimg_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpeg')]\r\n\r\nimgs = []\r\nfor f in img_files:\r\n    imgs.append(cv2.imread(f))\r\n\r\ng=load_graph(frozen_graph)\r\nwith tf.Session(graph=g) as sess:\r\n\r\n    softmax=g.get_tensor_by_name('prefix/softmax:0')\r\n    x = g.get_tensor_by_name(\"prefix/input:0\")\r\n    keep_prob = g.get_tensor_by_name(\"prefix/keepProbs:0\")\r\n\r\n    for i, image in enumerate(imgs):\r\n        \r\n        img = cv2.resize(image.astype(np.float32), (227,227))\r\n        img -= imagenet_mean\r\n        img = img.reshape((1,227,227,3))\r\n\r\n\toptions = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n\trun_metadata = tf.RunMetadata()\r\n        \r\n        print 'Begin session:'\r\n        probs = sess.run(softmax, feed_dict={x: img, keep_prob: 1}, \\\r\n\t\t\toptions=options, run_metadata=run_metadata)\r\n        print 'Ended session'\r\n\tprint 'Class: ' + str(class_names[np.argmax(probs)]) \\\r\n\t+ ', Prob: ' + str(probs[0,np.argmax(probs)])\r\n\tfetched_timeline = timeline.Timeline(run_metadata.step_stats)\r\n\tchrome_trace = fetched_timeline.generate_chrome_trace_format(show_memory=True)\r\n\twith open('./forward_timeline.json', 'w') as f:\r\n\t\tf.write(chrome_trace)\r\n\r\n```\r\n\r\nlog_alexnet.sh\r\n```\r\n#!/bin/bash\r\n\r\nlogpid() { while sleep 0.1; do ps -p $1 -o pcpu= -o pmem= ; done; }\r\npython frozen-infer.py &\r\nlogpid $! | tee ./pid.log\r\n```\r\n### Describe the problem\r\n\r\nI ran my own Alexnet frozen model and the forward inference script I had required a large amount of RAM, around 2.466 GB rather than the memory required to load model size which was (243.9 MB).\r\n\r\nEssentially I took scripts from [finetune_alexnet_with_tensorflow](https://github.com/kratzert/finetune_alexnet_with_tensorflow), including ```alexnet.py```, ```datagenerator.py```, ```caffe_classes.py```, and heavily modifying ```validate_alexnet_on_imagenet.ipynb``` to save a checkpoint of the Alexnet-implementation for TensorFlow. The weights were taken from this [link](http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/). I ran ```save_alexnet_checkpoint.py``` first to generate a binary proto graph definition of the AlexNet file and corresponding checkpoint files. Using the default ```freeze_graph``` tool from TensorFlow compiled by Bazel, I created a frozen graph. I then ran ```log_alexnet.sh``` to see how much of my RAM a forward inference uses on my laptop. The image I used was from the above mentioned github repo, llama. The script outputs the percentage of CPU and memory the python script ```frozen-infer.py```  uses. \r\n\r\nMy CPU: Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz\r\n\r\nMy RAM is 8 GB, but according to the free command, it is 7.909 GB, where htop reports it as 7.54 GB. The maximum RAM allocated by ```sess.run``` and the python script was thus, 0.327*7.54 = 2.466 GB (see below log). This is insanely larger than the 243.9 MB model I had. The ```chrome_trace``` I generated reports a maximum of around 235 MB, which is no where near what was actually happening.\r\n[forward_timeline.json.tar.gz](https://github.com/tensorflow/tensorflow/files/1678731/forward_timeline.json.tar.gz)\r\n\r\nMy question is why does the session run require such memory, and how am I able to reduce it down to only consume the required memory to load the model, which would be ~250 MB? I am concerned about this because I will to transferring this code to a Raspberry Pi 3 soon.\r\n\r\nIt seems someone else had the same issue on stackoverflow [here](https://stackoverflow.com/questions/46531213/after-freeze-graph-the-inference-is-slower-and-requires-more-memory).\r\n\r\n### Source code / logs\r\nThe output I got is such from log_alexnet.sh:\r\n\r\n```\r\n 0.0  0.7\r\n 0.0  1.0\r\n 0.0  1.2\r\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n 0.0  1.4\r\n 0.0  1.8\r\n 0.0  2.0\r\n 0.0  2.2\r\n 0.0  7.7\r\n 194  5.3\r\nBegin session:\r\n 204 10.4\r\n 215 16.8\r\n 226 17.7\r\n 236 22.1\r\n 248 25.8\r\n 259 30.7\r\n 269 32.7\r\n 280 27.7\r\n 145 29.4\r\n 151 27.3\r\n 156 29.2\r\n 162 24.6\r\n 167 28.9\r\n 172 27.0\r\n 178 27.8\r\n 183 17.1\r\n 188 18.5 \r\n 194 18.1\r\nEnded session\r\nClass: llama, Prob: 0.99927825\r\n 134 21.3\r\n```\r\n", "comments": ["Not sure what it could be. Could you run a memory profiler to help us tell what the problem is?\r\n\r\n@petewarden what do you think?", "TensorFlow is known to use a lot of memory when copying constants with frozen graphs. The best way to solve this is to memory-map the file, as described here under Reducing Memory Footprint:\r\nhttps://www.tensorflow.org/mobile/optimizing\r\n\r\nAlexnet is a very old model at this point though, and I'd highly recommend looking at a more modern version like MobileNet that can achieve better accuracy with a much smaller file size.\r\n", "We will be using smaller, mobile nets for our deployment. But I think it is important to consider and understand why `sess.run` uses this large amount of memory, even for AlexNet. We might come across this issue again, even if we were to move to a smaller model.\r\nI'll get to memory-mapping the model soon, here is the profiler output for the time being:\r\n\r\n```\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    10  194.477 MiB  194.477 MiB   @profile\r\n    11                             def infer():\r\n    12  194.477 MiB    0.000 MiB   \tfrozen_graph='frozen-graphs/frozen_alexnet.pb'\r\n    13                             \r\n    14  196.004 MiB    0.000 MiB   \tdef load_graph(frozen_graph_filename):\r\n    15  196.004 MiB    0.000 MiB   \t    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n    16  196.004 MiB    0.000 MiB   \t\tgraph_def = tf.GraphDef()\r\n    17  431.656 MiB  235.652 MiB   \t\tgraph_def.ParseFromString(f.read())\r\n    18                             \r\n    19  431.656 MiB    0.000 MiB   \t    with tf.Graph().as_default() as graph:\r\n    20  433.652 MiB    1.996 MiB   \t\ttf.import_graph_def(graph_def, name=\"prefix\")\r\n    21  433.652 MiB    0.000 MiB   \t    return graph\r\n    22                             \r\n    23  194.477 MiB    0.000 MiB   \timagenet_mean = np.array([104., 117., 124.], dtype=np.float32)\r\n    24                             \r\n    25  194.477 MiB    0.000 MiB   \tcurrent_dir = os.getcwd()\r\n    26  194.477 MiB    0.000 MiB   \timage_dir = os.path.join(current_dir, 'images')\r\n    27                             \r\n    28  194.477 MiB    0.000 MiB   \timg_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpeg')]\r\n    29                             \r\n    30  194.477 MiB    0.000 MiB   \timgs = []\r\n    31  196.004 MiB    0.000 MiB   \tfor f in img_files:\r\n    32  196.004 MiB    1.527 MiB   \t    imgs.append(cv2.imread(f))\r\n    33                             \r\n    34  433.652 MiB    0.000 MiB   \tg=load_graph(frozen_graph)\r\n    35  435.652 MiB    2.000 MiB   \twith tf.Session(graph=g) as sess:\r\n    36                             \r\n    37  435.652 MiB    0.000 MiB   \t    softmax=g.get_tensor_by_name('prefix/softmax:0')\r\n    38  435.652 MiB    0.000 MiB   \t    x = g.get_tensor_by_name(\"prefix/input:0\")\r\n    39  435.652 MiB    0.000 MiB   \t    keep_prob = g.get_tensor_by_name(\"prefix/keepProbs:0\")\r\n    40                             \r\n    41 1664.207 MiB    0.000 MiB   \t    for i, image in enumerate(imgs):\r\n    42                             \t\t\r\n    43  437.680 MiB    2.027 MiB   \t\timg = cv2.resize(image.astype(np.float32), (227,227))\r\n    44  437.680 MiB    0.000 MiB   \t\timg -= imagenet_mean\r\n    45  437.680 MiB    0.000 MiB   \t\timg = img.reshape((1,227,227,3))\r\n    46                             \r\n    47  437.680 MiB    0.000 MiB   \t\toptions = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    48  437.680 MiB    0.000 MiB   \t\trun_metadata = tf.RunMetadata()\r\n    49                             \t\t\r\n    50  437.680 MiB    0.000 MiB   \t\tprint 'Begin session:'\r\n    51  437.680 MiB    0.000 MiB   \t\tprobs = sess.run(softmax, feed_dict={x: img, keep_prob: 1}, \\\r\n    52 1663.305 MiB 1225.625 MiB   \t\t\t\toptions=options, run_metadata=run_metadata)\r\n    53 1663.305 MiB    0.000 MiB   \t\tprint 'Ended session'\r\n    54                             \t\tprint 'Class: ' + str(class_names[np.argmax(probs)]) \\\r\n    55 1663.305 MiB    0.000 MiB   \t\t+ ', Prob: ' + str(probs[0,np.argmax(probs)])\r\n    56 1663.305 MiB    0.000 MiB   \t\tfetched_timeline = timeline.Timeline(run_metadata.step_stats)\r\n    57 1664.207 MiB    0.902 MiB   \t\tchrome_trace = fetched_timeline.generate_chrome_trace_format(show_memory=True)\r\n    58 1664.207 MiB    0.000 MiB   \t\twith open('./forward_timeline.json', 'w') as f:\r\n    59 1664.207 MiB    0.000 MiB   \t\t\tf.write(chrome_trace)\r\n``` ", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Going to close this issue for now, I've gotten preoccupied with other things at the moment.", "I have the same problem. Session.run consumes more memory than it should."]}, {"number": 16591, "title": "Fix FutureWarning on issubdtype from float to np.floating", "body": "This is try to fix [#16587](https://github.com/tensorflow/tensorflow/issues/16587). \r\n```\r\nFutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated\r\n```\r\n\r\nBefore fix:\r\n```\r\n>>> np.issubdtype(np.integer, np.float)\r\n__main__:1: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\nFalse\r\n```\r\nAfter fix:\r\n```\r\n>>> np.issubdtype(np.integer, np.floating)\r\nFalse\r\n>>>\r\n```", "comments": ["@drpngx could you pls kindly take a look? It seems @inc0 approved but didn't have write access.", "This does not seem to fix the issue in #16587.\r\n\r\nFresh virtualenv with tf 1.6 and h5py, `import tensorflow` yields:\r\n```\r\n/Users/alex/.virtualenvs/test3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n```", "@cancan101 feel free to send a fix PR. Thanks!"]}, {"number": 16590, "title": "how to save model for tensroflwo serving for lstm in tensorflow/contrib/timeseries/examples/lstm.py", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nwhen I run lstm in tensorflow/contrib/timeseries/examples/lstm.py, I tried to add methods to save model into savedModel, but it gives back errors.\r\n\r\n  File \"/Users/yang/.local/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py\", line 504, in export_savedmodel\r\n    serving_input_receiver = serving_input_receiver_fn()\r\n  File \"/Users/yang/.local/lib/python3.4/site-packages/tensorflow/contrib/timeseries/python/timeseries/estimators.py\", line 133, in _serving_input_receiver_fn\r\n    self._model.initialize_graph()\r\nTypeError: initialize_graph() missing 1 required positional argument: 'input_statistics'\r\n\r\nThe issue I guess is that, in self._model.initialize_graph(), no parameters are given, but in \r\n\r\n    def initialize_graph(self, input_statistics):\r\n        \"\"\"Save templates for components, which can then be used repeatedly.\r\n        This method is called every time a new graph is created. It's safe to start\r\n        adding ops to the current default graph here, but the graph should be\r\n        constructed from scratch.\r\n        Args:\r\n          input_statistics: A math_utils.InputStatistics object.\r\n        \"\"\"\r\n        super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)\r\n        with tf.variable_scope(\"\", use_resource=True):\r\n          # Use ResourceVariables to avoid race conditions.\r\n          self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)\r\n          # Create templates so we don't have to worry about variable reuse.\r\n          self._lstm_cell_run = tf.make_template(\r\n              name_=\"lstm_cell\",\r\n              func_=self._lstm_cell,\r\n              create_scope_now_=True)\r\n          # Transforms LSTM output into mean predictions.\r\n          self._predict_from_lstm_output = tf.make_template(\r\n              name_=\"predict_from_lstm_output\",\r\n              func_=functools.partial(tf.layers.dense, units=self.num_features),\r\n              create_scope_now_=True)\r\n\r\none param input_statistics is asked. But how to fix this issue\r\n\r\n### Source code / logs\r\n    \r\n    serving_input_receiver_fn = estimator.build_raw_serving_input_receiver_fn()\r\n    estimator.export_savedmodel(\r\n        \"../model\",\r\n        serving_input_receiver_fn\r\n    )\r\n", "comments": ["Thank you for the report! Looks like there are a couple issues with the example. I have a fix in the works which will include exporting in the example.", "Could you please tell me how to fix this issue or, what I can do to get the correct version? Because I wanna put this model into savedModel or savedBundle.", "Getting the fix code reviewed is taking a bit longer than I was thinking. Here's the patch:\r\n\r\n```\r\ndiff --git a/tensorflow/contrib/timeseries/examples/lstm.py b/tensorflow/contrib/timeseries/examples/lstm.py\r\nindex c834430b9..630f4fc05 100644\r\n--- a/tensorflow/contrib/timeseries/examples/lstm.py\r\n+++ b/tensorflow/contrib/timeseries/examples/lstm.py\r\n@@ -20,12 +20,14 @@ from __future__ import print_function\r\n \r\n import functools\r\n from os import path\r\n+import tempfile\r\n \r\n import numpy\r\n import tensorflow as tf\r\n \r\n from tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimators\r\n from tensorflow.contrib.timeseries.python.timeseries import model as ts_model\r\n+from tensorflow.contrib.timeseries.python.timeseries import state_management\r\n \r\n try:\r\n   import matplotlib  # pylint: disable=g-import-not-at-top\r\n@@ -70,7 +72,7 @@ class _LSTMModel(ts_model.SequentialTimeSeriesModel):\r\n     self._lstm_cell_run = None\r\n     self._predict_from_lstm_output = None\r\n \r\n-  def initialize_graph(self, input_statistics):\r\n+  def initialize_graph(self, input_statistics=None):\r\n     \"\"\"Save templates for components, which can then be used repeatedly.\r\n \r\n     This method is called every time a new graph is created. It's safe to start\r\n@@ -168,12 +170,15 @@ class _LSTMModel(ts_model.SequentialTimeSeriesModel):\r\n \r\n \r\n def train_and_predict(\r\n-    csv_file_name=_DATA_FILE, training_steps=200, estimator_config=None):\r\n+    csv_file_name=_DATA_FILE, training_steps=200, estimator_config=None,\r\n+    export_directory=None):\r\n   \"\"\"Train and predict using a custom time series model.\"\"\"\r\n   # Construct an Estimator from our LSTM model.\r\n   estimator = ts_estimators.TimeSeriesRegressor(\r\n       model=_LSTMModel(num_features=5, num_units=128),\r\n-      optimizer=tf.train.AdamOptimizer(0.001), config=estimator_config)\r\n+      optimizer=tf.train.AdamOptimizer(0.001), config=estimator_config,\r\n+      # Set state to be saved across windows.\r\n+      state_manager=state_management.ChainingStateManager())\r\n   reader = tf.contrib.timeseries.CSVReader(\r\n       csv_file_name,\r\n       column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)\r\n@@ -192,6 +197,28 @@ def train_and_predict(\r\n   predicted_mean = numpy.squeeze(numpy.concatenate(\r\n       [evaluation[\"mean\"][0], predictions[\"mean\"]], axis=0))\r\n   all_times = numpy.concatenate([times, predictions[\"times\"]], axis=0)\r\n+\r\n+  # Export the model in SavedModel format.\r\n+  if export_directory is None:\r\n+    export_directory = tempfile.mkdtemp()\r\n+  input_receiver_fn = estimator.build_raw_serving_input_receiver_fn()\r\n+  export_location = estimator.export_savedmodel(\r\n+      export_directory, input_receiver_fn)\r\n+\r\n+  # Predict using the SavedModel\r\n+  with tf.Graph().as_default():\r\n+    with tf.Session() as session:\r\n+      signatures = tf.saved_model.loader.load(\r\n+          session, [tf.saved_model.tag_constants.SERVING], export_location)\r\n+      saved_model_output = (\r\n+          tf.contrib.timeseries.saved_model_utils.predict_continuation(\r\n+              continue_from=evaluation, signatures=signatures,\r\n+              session=session, steps=100))\r\n+      # The exported model gives the same results as the Estimator.predict()\r\n+      # call above.\r\n+      numpy.testing.assert_allclose(\r\n+          predictions[\"mean\"],\r\n+          numpy.squeeze(saved_model_output[\"mean\"], axis=0))\r\n   return times, observed, all_times, predicted_mean\r\n \r\n \r\ndiff --git a/tensorflow/contrib/timeseries/examples/lstm_test.py b/tensorflow/contrib/timeseries/examples/lstm_test.py\r\nindex 3cace5672..ca56e38ca 100644\r\n--- a/tensorflow/contrib/timeseries/examples/lstm_test.py\r\n+++ b/tensorflow/contrib/timeseries/examples/lstm_test.py\r\n@@ -36,7 +36,8 @@ class LSTMExampleTest(test.TestCase):\r\n   def test_periodicity_learned(self):\r\n     (observed_times, observed_values,\r\n      all_times, predicted_values) = lstm.train_and_predict(\r\n-         training_steps=100, estimator_config=_SeedRunConfig())\r\n+         training_steps=100, estimator_config=_SeedRunConfig(),\r\n+         export_directory=self.get_temp_dir())\r\n     self.assertAllEqual([100], observed_times.shape)\r\n     self.assertAllEqual([100, 5], observed_values.shape)\r\n     self.assertAllEqual([200], all_times.shape)\r\n\r\n```", "Awesome", "It's OK to create a savedModel, however, I still confused on what's the inputs and outputs for the savedModel. Let's assume that, someone saved the model, for me, I don't know what's the exactly input or output segment is, in this condition, how can I get the result?", "The inputs and outputs are exercised in saved_model_utils (called from the LSTM example): https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/timeseries/python/timeseries/saved_model_utils.py\r\n\r\nI made a change last week which makes it easier to cold-start from a SavedModel; until that lands in the next push (should be today?) you need the output of an Estimator to get started like in the examples, or to feed in state manually (ugly / not recommended). Is that the issue?", "It's synced: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/timeseries/examples/lstm.py#L239\r\n\r\nThere's no intermediate state saved with the model, so it does need a sequence as input in order to start making sensible predictions. Alternatively you could save the state spit out by Estimator.evaluate() if you know you'll be predicting starting from the end of the evaluation data.", "Nagging Assignee @allenlavoie: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think this is resolved, but feel free to follow up if something isn't clear."]}, {"number": 16589, "title": "TensorFlowLite cannot locate symbol \"__atomic_store_8\" crash on Android x86 devices ", "body": "### System information\r\n\r\n- **OS Platform and Distribution** : Android x86 v5.0+\r\n- **TensorFlow installed from** : built using gradle from maven repository: https://google.bintray.com/tensorflow\r\n- **TensorFlow version** : 1.2.0-rc0\r\n\r\n### Describe the problem\r\nWe migrated our app to use tensorflow lite but it crashes on x86 upon invoking the inference module. It works perfect on ARM v7a devices.\r\n\r\n### Source code / logs\r\nHere's the exception:\r\n```\r\n01-15 03:39:47.138  2020  2056 W System.err: TensorFlowLite: failed to load native library: dlopen failed: cannot locate symbol \"__atomic_store_8\" referenced by \"/data/app/com.XXXXXXX.XXXXXXXXXXXXXXXX-1/lib/x86/libtensorflowlite_jni.so\"...\r\n01-15 03:39:47.138  2020  2056 E art     : No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)\r\n01-15 03:39:47.183  2020  2056 E AndroidRuntime: FATAL EXCEPTION: XXXXXServiceThread\r\n01-15 03:39:47.183  2020  2056 E AndroidRuntime: Process: com.XXXXXXX.XXXXXXXXXXXXXXXX, PID: 2020\r\n01-15 03:39:47.183  2020  2056 E AndroidRuntime: java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)\r\n01-15 03:39:47.183  2020  2056 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(Native Method)\r\n01-15 03:39:47.183  2020  2056 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:47)\r\n01-15 03:39:47.183  2020  2056 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.<init>(Interpreter.java:77)\r\n01-15 03:39:47.183  2020  2056 E AndroidRuntime: \tat com.XXXXXXX.XXXXXXXXXXXXXXXX.utils.classifier.TensorflowLiteClassifier.<init>(TensorflowLiteClassifier.java:46)\r\n01-15 03:39:47.183  2020  2056 E AndroidRuntime: \tat com.XXXXXXX.XXXXXXXXXXXXXXXX.service.classifier.TensorFlowFileClassifier.initialize(TensorFlowFileClassifier.java:41)\r\n01-15 03:39:47.183  2020  2056 E AndroidRuntime: \tat com.XXXXXXX.XXXXXXXXXXXXXXXX.service.servicethread.ServiceThreadModel.onFileSystemScanStarted(ServiceThreadModel.java:92)\r\n01-15 03:39:47.183  2020  2056 E AndroidRuntime: \tat com.XXXXXXX.XXXXXXXXXXXXXXXX.service.servicethread.ServiceThread$1.handleMessage(ServiceThread.java:82)\r\n01-15 03:39:47.183  2020  2056 E AndroidRuntime: \tat android.os.Handler.dispatchMessage(Handler.java:102)\r\n01-15 03:39:47.183  2020  2056 E AndroidRuntime: \tat android.os.Looper.loop(Looper.java:154)\r\n01-15 03:39:47.183  2020  2056 E AndroidRuntime: \tat android.os.HandlerThread.run(HandlerThread.java:61)\r\n01-15 03:39:47.1\r\n```", "comments": ["/CC @petewarden", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "+1", "Sorry for the delay on this one! From a bit of digging, it looks like it may be related to this issue:\r\nhttps://github.com/alexa/avs-device-sdk/issues/380\r\n\r\nIn that case, it was resolved with an explicit addition of -latomic to the linker stage. Can you try editing build_def.bzl to add that flag:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/build_def.bzl#L48\r\nIn `def tflite_linkopts_unstripped()`, add `-Wl,-latomic` to the linker flags section.", "I have exactly the same issue, I added the dependency to the file build_def.bzl and the problem is still there. Once the line is added there is another thing that has to be done ?", "@koAlech did @petewarden's suggestion fix your problem? ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I apologize for the slow response. We have failed in following @petewarden's suggestion as we are unsure how to build the aar file for tensorflow lite (as opposed to tensorflow mobile which we migrated from).\r\n\r\nWe have followed the [tfilte guide](https://www.tensorflow.org/mobile/tflite/devguide#android) and are using the latest build from the [bintray repository](https://google.bintray.com/tensorflow) (currently v0.1.1). \r\n\r\nPlease advise how we should implement @petewarden's suggestion in this configuration.", "Update: tried the latest version (`0.1.8-rc0`) and it crashes with the same exception on x86 devices.", "You can build tf lite's aar with:\r\nbazel build -c opt --cxxopt=--std=c++11 \\\r\n    --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n    //tensorflow/contrib/lite/java:tensorflow-lite\r\n", "Tried @andrehentz solution but it failed to build an aar file. One tensorflowlibrary.jar file was built in the bazel-bin/tensorflow folder. Please advise", "Nagging Assignee @petewarden: It has been 23 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 37 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 52 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 67 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "What is the exact command you ran (i.e. full bazel command)? What version of the sdk did you run it on? Were there any errors produced?", "@suharshs I was about to paste the entire bazel command we used in order to build tflite from source but then realized that a new version (1.9.0) was released and decided to give it a try.\r\nAnd surprisingly this issue was resolved! We're so happy!", "Fantastic! :)"]}, {"number": 16588, "title": "IllegalArgumentException: Retval[0] does not have value", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: iMac (27-inch, Late 2013) OS: 10.13.3 (17D47)\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**:  Using TensorFlow backend.\r\n1.5.0-rc1\r\n- **Python version**: Python 3.6.4\r\n- **Bazel version (if compiling from source)**: \r\nBuild label: 0.9.0-homebrew\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Sun Jul 12 12:24:01 +49936 (1513677414241)\r\nBuild timestamp: 1513677414241\r\nBuild timestamp as int: 1513677414241\r\n\r\n- **GCC/Compiler version (if compiling from source)**: Xcode 9.2\r\nBuild version 9C40b\r\n- **CUDA/cuDNN version**: No (CPU only)\r\n- **GPU model and memory**: No\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nif I add GRU or LSTM to my code and try to export this model to Android I have got the exception:\r\n```\r\nSuccessfully loaded model from 'file:///android_asset/frozen_opt.pb'\r\n01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow I/TensorFlowImageClassif: Read 24 labels, output layer size is 32\r\n01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow I/tensorflow: ClassifierActivity: Camera orientation relative to screen canvas: 90\r\n01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow I/tensorflow: ClassifierActivity: Initializing at size 640x480\r\n01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow E/art: No implementation found for void ru.rimidalv.tensorflow.env.ImageUtils.convertYUV420SPToARGB8888(byte[], int[], int, int, boolean) (tried Java_ru_rimidalv_tensorflow_env_ImageUtils_convertYUV420SPToARGB8888 and Java_ru_rimidalv_tensorflow_env_ImageUtils_convertYUV420SPToARGB8888___3B_3IIIZ)\r\n01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow W/tensorflow: ImageUtils: Native YUV420SP -> RGB implementation not found, falling back to Java implementation\r\n01-30 07:45:35.714 25635-25666/ru.rimidalv.tensorflow E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[the_input], outputs:[softmax/truediv]\r\n01-30 07:45:35.714 25635-25666/ru.rimidalv.tensorflow E/AndroidRuntime: FATAL EXCEPTION: inference\r\n                                                                        Process: ru.rimidalv.tensorflow, PID: 25635\r\n                                                                        java.lang.IllegalArgumentException: Retval[0] does not have value\r\n                                                                            at org.tensorflow.Session.run(Native Method)\r\n                                                                            at org.tensorflow.Session.access$100(Session.java:48)\r\n                                                                            at org.tensorflow.Session$Runner.runHelper(Session.java:298)\r\n                                                                            at org.tensorflow.Session$Runner.run(Session.java:248)\r\n                                                                            at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:230)\r\n                                                                            at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n                                                                            at ru.rimidalv.tensorflow.TensorFlowImageClassifier.recognizeImage(TensorFlowImageClassifier.java:171)\r\n                                                                            at ru.rimidalv.tensorflow.ClassifierActivity$2.run(ClassifierActivity.java:175)\r\n                                                                            at android.os.Handler.handleCallback(Handler.java:739)\r\n                                                                            at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                                                                            at android.os.Looper.loop(Looper.java:158)\r\n                                                                            at android.os.HandlerThread.run(HandlerThread.java:61)\r\n```\r\n\r\ncode to export my model:\r\n\r\n```\r\nbazel build tensorflow/python/tools:freeze_graph\r\nbazel-bin/tensorflow/python/tools/freeze_graph \\\r\n--input_graph=$PB_MAIN/protobuf_path.pbtxt \\\r\n--input_checkpoint=$PB_MAIN/checkpoint_path.ckpt \\\r\n--output_graph=$PB_MAIN/frozen_graph.pb \\\r\n--output_node_names=softmax/truediv \\\r\n```\r\nand than\r\n\r\n```\r\n!bazel build tensorflow/tools/graph_transforms:transform_graph\r\n!bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=$PB_MAIN/frozen_graph.pb \\\r\n--out_graph=$PB_MAIN/frozen_opt.pb \\\r\n--inputs='the_input:0' \\\r\n--outputs='softmax/truediv:0' \\\r\n--transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"-1,128,64,1\") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_nodes round_weights strip_unused_nodes sort_by_execution_order'\r\n```\r\n\r\n\r\n### Source code / logs\r\nMy model:\r\n\r\n```python\r\n    input_shape = (128, 64, 1)\r\n    latent_dim = 128\r\n    decoder_inputs = Input(shape=(input_shape), name='the_input')\r\n    conv_to_rnn_dims = (32, 128*2 )\r\n    inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(decoder_inputs)\r\n    inner = GRU(latent_dim, return_sequences=True, kernel_initializer='he_normal')(inner)\r\n    decoder_dense = Dense(10, activation='softmax', name=\"softmax\")(inner)\r\n    model = Model(inputs=[decoder_inputs], outputs=decoder_dense)\r\n    model.compile(loss={'softmax': lambda y_true, y_pred: y_pred}, optimizer=\"adam\")\r\n```\r\n\r\nSave the model:\r\n\r\n```python\r\n\r\n    K.set_learning_phase(0)\r\n    model = load_model(os.path.join('model_data', 'model.h5'), compile=False)\r\n    model.load_weights(os.path.join('model_data', 'weights00.h5'))\r\n    sess = K.get_session()\r\n   \r\n    protobuf_path = os.path.join('tf-exports', 'protobuf_path.pbtxt')\r\n    checkpoint_path = os.path.join('tf-exports', 'checkpoint_path.ckpt')\r\n\r\n    tf.train.write_graph(sess.graph_def, '.', protobuf_path)\r\n\r\n    saver = tf.train.Saver()\r\n    saver.save(sess, save_path = checkpoint_path)\r\n\r\n```\r\n", "comments": ["You are missing the implementation of the kernel. You need to add it to the list and rebuild.\r\n\r\n@andrewharp do you have a pointer to the instructions?", "@drpngx if you say about ops_to_register I do this:\r\n\r\n```\r\nbazel-bin/tensorflow/python/tools/print_selective_registration_header \\\r\n--graphs=$PB_MAIN/frozen_opt.pb > $TFP/tensorflow/core/framework/ops_to_register.h\r\n```\r\nthen this ( tried with --copt=\"-D__ANDROID_TYPES_FULL__\" \\ and without it):\r\n```\r\nbazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" \\\r\n--copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" \\\r\n//tensorflow/contrib/android:libtensorflow_inference.so \\\r\n--host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n--crosstool_top=//external:android/crosstool --cpu=armeabi-v7a \r\n```\r\nand finaly:\r\n```\r\ncp $TFP/bazel-bin/tensorflow/contrib/android/libtensorflow_inference.so /Users/rimidalv/AndroidStudioProjects/agronumbers/libs/armeabi-v7a\r\n```\r\n\r\nAnd I tried next configuration of GRU:\r\n```\r\ndecoder_gru = GRU(latent_dim, return_sequences=False, kernel_initializer='zeros')(decoder_inputs)\r\n```\r\nand this:\r\n```\r\ndecoder_gru = GRU(latent_dim, return_sequences=False)(decoder_inputs)\r\n```", "I found solution! \r\n\r\nTo fix this error you should remove this: **op=Identity**  from \"remove_nodes\" inside --transforms\r\nAnd may be add --transforms='remove_attribute(attribute_name=_class)' if you get this error: https://github.com/tensorflow/tensorflow/issues/9494\r\n\r\nThis code is OK:\r\n\r\n```\r\n!bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=$PB_MAIN/frozen_graph.pb \\\r\n--out_graph=$PB_MAIN/frozen_opt.pb \\\r\n--inputs='the_input:0' \\\r\n--outputs='softmax/truediv:0' \\\r\n--transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"-1,128,64,1\") remove_nodes(op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_nodes round_weights strip_unused_nodes sort_by_execution_order'\r\n```\r\n\r\n"]}, {"number": 16587, "title": "Feature deprecated in h5py is used in TF1.5", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux, OS X\r\n- **TensorFlow installed from (source or binary)**: source and binary\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: 0.9\r\n- **GCC/Compiler version (if compiling from source)**: \r\n- **CUDA/cuDNN version**: 9.0 - 7.0\r\n- **GPU model and memory**: GTX1060, GTX 1050Ti \r\n- **Exact command to reproduce**:\r\n`sudo pip3 install h5py`\r\nrun python3, from there, type:\r\n`import tensorflow as tf`\r\n\r\n\r\n### Describe the problem\r\nA feature of h5py used in TF 1.5 is deprecated, in particular: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated.\r\n\r\n### Source code / logs\r\nWarning message: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n", "comments": ["Created a PR #16591 to fix the FutureWarning.", "Marking as contributions welcome since there is a PR.", "per https://github.com/tensorflow/tensorflow/pull/16591#issuecomment-371191027, this still seems to be an issue", "It seems I cannot repro this issue with TF1.6 on my side, @cancan101 could you pls help confirm your detailed info as below:\r\n#### OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n#### TensorFlow installed from (source or binary): \r\n#### TensorFlow version (use command below):\r\n#### Python version: \r\n#### Bazel version (if compiling from source):\r\n#### GCC/Compiler version (if compiling from source):\r\n", "Running `tf_env_collect.sh`:\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nDarwin Alexs-MacBook-Pro-5.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.6\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.0.0 (clang-900.0.39.2)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin Alexs-MacBook-Pro-5.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.1)\r\nprotobuf (3.5.2)\r\ntensorflow (1.6.0)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.6.0\r\ntf.GIT_VERSION = v1.6.0-0-gd2e24b6039\r\ntf.COMPILER_VERSION = v1.6.0-0-gd2e24b6039\r\nSanity check: array([1], dtype=int32)\r\n/Users/alex/.virtualenvs/test3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\r\n  return f(*args, **kwds)\r\n/Users/alex/.virtualenvs/test3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```", "i just compiled from source in hopes this was fixed, but alas it is not. in addition to the information below, my h5py version is 2.7.1:\r\n\r\n```\r\n== uname -a =====================================================\r\nLinux machine 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.2)\r\nnumpydoc (0.7.0)\r\nprotobuf (3.5.2.post1)\r\ntensorflow (1.7.0)\r\ntensorflow-gpu (1.7.0)\r\ntensorflow-tensorboard (0.1.6)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.7.0\r\ntf.GIT_VERSION = b'v1.7.0-1568-gf9c5e71'\r\ntf.COMPILER_VERSION = b'v1.7.0-1568-gf9c5e71'\r\nSanity check: array([1], dtype=int32)\r\n/home/kyle/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /home/kyle/torch/install/lib:/usr/local/cuda/lib64:\r\nDYLD_LIBRARY_PATH /home/kyle/torch/install/lib:/home/kyle/torch/install/lib:/home/kyle/torch/install/lib:\r\n\r\n== nvidia-smi ===================================================\r\nWed Apr 11 12:16:46 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:05:00.0  On |                  N/A |\r\n|  0%   55C    P0    76W / 280W |    190MiB / 11175MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:09:00.0 Off |                  N/A |\r\n|  0%   52C    P0    74W / 280W |      2MiB / 11178MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1103      G   /usr/lib/xorg/Xorg                           146MiB |\r\n|    0      2225      G   compiz                                        41MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.1/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.1/lib64/libcudart.so.9.1.85\r\n/usr/local/cuda-9.1/lib64/libcudart_static.a\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/lib64/libcudart_static.a\r\n```\r\n\r\n", "you need to use the new rc for h5py.", "i couldn't get it to build with the latest h5py. i thought it might be because i didn't run `bazel clean` in between builds, but i ran `bazel clean` and tried again and couldn't get it to work.\r\n\r\n```\r\n$ pip install git+https://github.com/h5py/h5py.git@2.8.0rc1`\r\n$ ./configure\r\n$ bazel build --config=opt --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n...\r\nERROR: /home/kyle/Documents/setup/tensorflow/tensorflow/tools/api/generator/BUILD:27:1: Executing genrule //tensorflow/tools/api/generator:python_api_gen failed (Aborted): bash failed: error executing com\r\nmand /bin/bash -c ... (remaining 1 argument(s) skipped)\r\nWarning! ***HDF5 library version mismatched error***\r\nThe HDF5 header files used to compile this application do not match\r\nthe version used by the HDF5 library to which this application is linked.\r\nData corruption or segmentation faults may occur if the application continues.\r\nThis can happen when an application was compiled by one version of HDF5 but\r\nlinked with a different version of static or shared HDF5 library.\r\nYou should recompile the application or check your shared library related\r\nsettings such as 'LD_LIBRARY_PATH'.\r\nYou can, at your own risk, disable this warning by setting the environment\r\nvariable 'HDF5_DISABLE_VERSION_CHECK' to a value of '1'.\r\nSetting it to 2 or higher will suppress the warning messages totally.\r\nHeaders are 1.8.16, library is 1.8.17\r\n            SUMMARY OF THE HDF5 CONFIGURATION \r\n            ================================= \r\n\r\nGeneral Information:\r\n-------------------\r\n                   HDF5 Version: 1.8.17\r\n                  Configured on: Sat May 27 16:16:57 UTC 2017\r\n                  Configured by: root@de9f433bbe70\r\n                 Configure mode: production   \r\n                    Host system: x86_64-unknown-linux-gnu\r\n              Uname information: Linux de9f433bbe70 3.13.0-119-generic #166-Ubuntu SMP Wed May 3 12:18:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n                       Byte sex: little-endian\r\n                      Libraries: static, shared\r\n             Installation point: /home/kyle/anaconda3\r\n\r\nCompiling Options:\r\n------------------\r\n               Compilation Mode: production   \r\n                     C Compiler: /opt/rh/devtoolset-2/root/usr/bin/gcc ( gcc (GCC) 4.8.2 20140120 )\r\n                         CFLAGS:  -m64\r\n                      H5_CFLAGS: -std=c99 -pedantic -Wall -Wextra -Wundef -Wshadow -Wpointer-arith -Wbad-function-cast -Wcast-qual -Wcast-align -Wwrite-strings -Wconversion -Waggregate-return -Wstrict-pro\r\ntotypes -Wmissing-prototypes -Wmissing-declarations -Wredundant-decls -Wnested-externs -Winline -Wfloat-equal -Wmissing-format-attribute -Wmissing-noreturn -Wpacked -Wdisabled-optimization -Wformat=2 -Wun\r\nreachable-code -Wendif-labels -Wdeclaration-after-statement -Wold-style-definition -Winvalid-pch -Wvariadic-macros -Winit-self -Wmissing-include-dirs -Wswitch-default -Wswitch-enum -Wunused-macros -Wunsaf\r\ne-loop-optimizations -Wc++-compat -Wstrict-overflow -Wlogical-op -Wlarger-than=2048 -Wvla -Wsync-nand -Wframe-larger-than=16384 -Wpacked-bitfield-compat -Wstrict-overflow=5 -Wjump-misses-init -Wdouble-pro\r\nmotion -Wsuggest-attribute=const -Wtrampolines -Wstack-usage=8192 -Wvector-operation-performance -Wsuggest-attribute=pure -Wsuggest-attribute=noreturn -Wsuggest-attribute=format -O3\r\n                      AM_CFLAGS:\r\n                       CPPFLAGS:\r\n                    H5_CPPFLAGS: -D_GNU_SOURCE -D_POSIX_C_SOURCE=200112L   -DNDEBUG -UH5_DEBUG_API\r\n                    AM_CPPFLAGS:  -I/home/kyle/anaconda3/include\r\n               Shared C Library: yes\r\n               Static C Library: yes\r\n  Statically Linked Executables: no\r\n                        LDFLAGS:\r\n                     H5_LDFLAGS:\r\n                     AM_LDFLAGS:  -L/home/kyle/anaconda3/lib\r\n                Extra libraries: -lrt -lpthread -lz -ldl -lm\r\n                       Archiver: ar\r\n                         Ranlib: ranlib\r\n              Debugged Packages:\r\n                    API Tracing: no\r\nLanguages:\r\n----------\r\n                        Fortran: yes\r\n               Fortran Compiler: /opt/rh/devtoolset-2/root/usr/bin/gfortran ( GNU Fortran (GCC) 4.8.2 20140120 )\r\n          Fortran 2003 Compiler: yes\r\n                  Fortran Flags:\r\n               H5 Fortran Flags:\r\n               AM Fortran Flags:\r\n         Shared Fortran Library: yes\r\n         Static Fortran Library: yes\r\n\r\n                            C++: yes\r\n                   C++ Compiler: /opt/rh/devtoolset-2/root/usr/bin/g++ ( g++ (GCC) 4.8.2 20140120 )\r\n                      C++ Flags:  -DBOOST_MATH_DISABLE_FLOAT128 -m64\r\n                   H5 C++ Flags:\r\n                   AM C++ Flags:\r\n             Shared C++ Library: yes\r\n             Static C++ Library: yes\r\n\r\nFeatures:\r\n---------\r\n                  Parallel HDF5: no\r\n             High Level library: yes\r\n                   Threadsafety: yes\r\n            Default API Mapping: v18\r\n With Deprecated Public Symbols: yes\r\n         I/O filters (external): deflate(zlib)\r\n                            MPE: no\r\n                     Direct VFD: no\r\n                        dmalloc: no\r\nClear file buffers before write: yes\r\n           Using memory checker: no\r\n         Function Stack Tracing: no\r\n      Strict File Format Checks: no\r\n   Optimization Instrumentation: no\r\nBye...\r\n/bin/bash: line 1:  5901 Aborted                 (core dumped) \r\n...\r\nINFO: Elapsed time: 2863.064s, Critical Path: 176.83s\r\nFAILED: Build did NOT complete successfully   \r\n```\r\n\r\ncurrently trying again, cleaning as much as possible:\r\n\r\n```\r\n$ bazel clean\r\n$ rm -rf /home/kyle/anaconda3/lib/python3.6/site-packages/tensor*\r\n$ cd .. ; rm -rf tensorflow # remove tensorflow repo\r\n$ pip install --upgrade --force-reinstall h5py==2.8.0rc1\r\n$ git clone https://github.com/tensorflow/tensorflow\r\n$ cd tensorflow\r\n$ ./configure\r\n$ bazel build --config=opt --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\n```\r\n\r\nthis time it worked! so it was something about not cleaning my installation completely before rebuilding with the new library.", "Hi @feranick ! 1.x versions are not supported anymore . Can you move this to closed status as it is not replicating in [2.8 version](https://colab.sandbox.google.com/gist/mohantym/7e50b763fc3bbc5fe46ee21f74a6ee63/github_16587.ipynb) ? Thanks!", "Closing. "]}, {"number": 16586, "title": "Force sorting of CUDA/Python headers to avoid spurious rebuilds", "body": "If one does try to re-use Bazel cache of a TensorFlow CUDA-enabled\r\nbuild, then it might happen that readdir() syscall behind the use of\r\nfind in _read_dir() will generate a different ordering of the very same\r\nlist of headers. This will make new genrules for symlinking the CUDA\r\nheaders and in the end it will result in different actionKey computed by\r\nBazel, hence invalidating the action cache.\r\n\r\nFixes #16585", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 16585, "title": "TensorFlow with CUDA or Python might rebuilds more than necessary instead of re-using bazel cache", "body": "Context: for DeepSpeech, we perform tensorflow builds and then keep the cache in a tar (capturing the whole of the home directory of the build user). We then untar it and the deepspeech build through `bazel build` picks the proper cached items so it does not rebuild anything.\r\n\r\nRecently, we started to have increased (2.5x) build time on CUDA-enabled builds. Debugging with Bazel showed that it was rebuilding because the `actionKey` computed for `stream_executor_impl` was different. Instrumenting Bazel to get more informations, I could get down to the reason of the different actionKey: the ordering of the CUDA includes was different. The list itself contained the exact same content, just a different ordering.\r\n\r\nThose includes are symlinks, and they are generated from a genrule. This is all taken care of by https://github.com/tensorflow/tensorflow/blob/ba64f5334d4bba31d22c30e09a96f806ea0e2f7e/third_party/gpus/cuda_configure.bzl#L915-L1035 which generated shell script for the genrules, that actually do perform the symlinks. Checking those shell scripts revealed the exact same and different ordering.\r\n\r\nChecking more carefully, one will see that the headers are discovered by `_read_dir` function: https://github.com/tensorflow/tensorflow/blob/ba64f5334d4bba31d22c30e09a96f806ea0e2f7e/third_party/gpus/cuda_configure.bzl#L891-L894, it does directly get the output of `find`. This is dependant on the ordering provided by `readdir` syscall.\r\n\r\nIn our case, the ordering on the filesystem before making the tar archive, and after untarring it would be different.\r\n\r\nOne simple fix for that is to force ordering the list of headers, this way we are sure the order is always the same and we are not dependant on what `readdir` is going to get us.\r\n\r\nIn the past, Bazel would force the ordering of the elements considered to compute the actionKey. This was removed with 0.3.0 but it might have make the issue hidden https://github.com/bazelbuild/bazel/commit/9dc32111d5b6c1c7c5eaf39efad5fef75327ee75", "comments": ["Marking as contributions welcome since there is a PR.", "Updating since I spotted similar code-path in `third_party/py/python_configure.bzl`."]}, {"number": 16584, "title": "TensorFlow op to copy weights of Keras model", "body": "I am doing a distributed calibration of an LSTM model (keras 2.0 + TensorFlow 1.0)\r\n\r\n    with tf.device(tf.train.replica_device_setter(...):\r\n          model = ##create model by keras\r\n          clone_model = ## create the same model by keras but now a stateful one\r\n\r\nafter calibration, I want my chief worker to use the clone_model, copy the weights the calibration reached in model, and make predictions on some test set, but simply calling\r\n\r\n     clone_model.set_weights(model.get_weights())\r\n\r\ndoes not work.\r\nI understand I need to define this weight copy as an op and then call session(run) of that op\r\n\r\nCan you please help with a TensorFlow op copying weights of a keras model to another (identical architecture) Keras model?\r\n\r\n", "comments": ["I'm not sure what problem you are running into. In general in Tensorflow, it's always possible to retrieve the values of `trainable_variables`, then use `tf.assign` to load them in a different graph, for as long a you can match them up (typically by var name).", "Thank you. I think it is `trainable_weights` instead of `trainable_variables` \r\nInspired by your answer this is what I did \r\n\r\n    clone_ops = []\r\n    for i, src in enumerate(model.trainable_weights):\r\n          op = tf.assign(clone_model.trainable_weights[i], src)\r\n          clone_ops.append(op)\r\n\r\nand then when I want to clone the weights\r\n\r\n    for op in clone_ops:\r\n        session.run(op)\r\n", "Glad you figured it out!"]}, {"number": 16583, "title": "Fix typo", "body": "fix typos", "comments": []}, {"number": 16582, "title": "Fix tfcompile module label.", "body": "Hi,\r\n\r\nThe tfcompile module label is incorrect in the docs; this PR fixes it.\r\n\r\nCheers,\r\n\r\nAndy", "comments": ["Ankur, FYI, this affects the merge back in", "Thanks for the fix!\r\n\r\nThis PR affects the pull from Public to Google Internal (as the /third_party/ prefix is essential for Google Internal). We will do the pull in the next day or so, and review this PR after that.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ankurtaly: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ankurtaly: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ankurtaly: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ankurtaly: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 16581, "title": "TFLite : Slice operation while using tf.nn.conv2d", "body": "input = tf.get_variable(\"input\",dtype = tf.float32,shape=(1,256,256,3))\r\nkernel = tf.get_variable(\"kernel\",initializer=[[[[ 0.0,  1,  2],[ 3,  4,  5],[ 6,  7,  8]],[[ 9, 10, 11],[12, 13, 14],[15, 16, 17]]],[[[18, 19, 20],[21, 22, 23],[24, 25, 26]],[[27, 28, 29],[30, 31, 32],[33, 34, 35]]]],dtype=tf.float32)\r\nA = tf.nn.conv2d(input=input,filter=kernel,strides=[1,1,1,1],padding=\"SAME\")\r\nA = tf.nn.softmax(A)\r\noutput = tf.add(A,1,name=\"output\")\r\n\r\nI am trying to convert a simple model to tflite. However I am hitting. \r\n**If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: Slice.\r\nAbort trap: 6**\r\n\r\nDoes conv2d internally calls Slice", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 16580, "title": "Ensure bash is invoked as a login shell on windows otherwise fixups fail.", "body": "Hi,\r\n\r\nThis PR contains a small fix to `repo.bzl` to ensure that bash is invoked as a login shell on Windows. Otherwise, depending on the `tf_http_archive` execution, bash fails to find `rm` and/or `patch`. Note this issue only manifests itself when Bazel is invoked from a Windows command prompt.\r\n\r\n**E.g.**\r\n```\r\nERROR: Skipping '//tensorflow:libtensorflow.so': error loading package 'tensorflow': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):\r\n        File \"<redacted>/third_party/repo.bzl\", line 88\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"<redacted>/third_party/repo.bzl\", line 59, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"<redacted>/third_party/repo.bzl\", line 44, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(127) when executing 'C:\\tools\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d <redacted>/4bwywwqm/external/protobuf_archive -i <redacted>/third_party/protobuf/add_noinlines.patch':\r\nStdout:\r\nStderr: /usr/bin/bash: patch: command not found\r\n```\r\n**Environment**\r\n\r\n||Version|\r\n|--|--|\r\n|OS|Win10-Ent|\r\n|Bazel|0.8.1|\r\n|msys2|msys2-x86_64-20161025|\r\n\r\nCheers,\r\n\r\nAndy", "comments": ["+ av8ramit who was seeing the issue of not finding \"patch\" command on windows", "@andykernahan Is this really necessary? I am using Bazel 0.9.0 with Bash from Git for Windows (`C:\\Git\\bin\\bash.exe` to be exact, `C:\\Git\\usr\\bin\\bash.exe` won't work) on Windows 10. It works without `-l` from Windows command prompt. I tested that `-l` will work too, but the bash launching time increases from < 1s to 2s, don't really like it.", "@rongjiecomputer The Bazel on Windows requirements [specify](https://docs.bazel.build/versions/master/windows.html) the msys2 shell; ideally the repo should build using the recommended Bazel installation/setup environment.\r\n\r\nI believe that any impact on performance will only be observed building from clean repo, or when a `tf_http_archive` rule is modified, and only for those `tf_http_archive` rules which have a `patch_file` or `delete` attribute (which, as of, [f1f2fff21](/tensorflow/tensorflow/blob/f1f2fff2c149ea3a848ec61b7d3fcd39a6dc3f06/tensorflow/workspace.bzl), is only two rules).", "@andykernahan Since it will not break my use case, I guess it is still fine for me. No more objection."]}, {"number": 16579, "title": "Add option to not include histograms", "body": "`add_gan_model_image_summaries` does the work of adding images to summaries, but then it also calls `add_gan_model_summaries` which dumps every trainable variable to histograms. It would be nice to be able to get the image summaries without the histograms.\r\n\r\nI would prefer to just delete that line, because it is weird that the two functions are tied. It wouldn't be hard to call both functions in your code if you wanted both.\r\n\r\nHowever, this preserves existing functionality. If you pass `model_summaries=False`, it does not call `add_gan_model_summaries`.\r\n\r\nCheers", "comments": []}, {"number": 16578, "title": "Different result between python and C++ with same pb file and same data.", "body": "I got different result between python and C++ using same pb file and same data, I print result below, the fisrt 10000 number's diff is close to zero, but it's get bigger then.\r\n\r\n|line|C++ result|Python result|diff|sum diff|\r\n|----|:----:|:----:|:----:|:----:|\r\n|0|-0.010966|-0.010966|-0.000000|-0.000000|\r\n|1|-0.050666|-0.050666|0.000000|0.000000|\r\n|2|-0.007573|-0.007573|0.000000|0.000000|\r\n|3|-0.498266|-0.498266|0.000000|0.000000|\r\n|4|-0.079290|-0.079290|-0.000000|0.000000|\r\n|5|0.044778|0.044778|-0.000000|0.000000|\r\n|6|0.003472|0.003472|-0.000000|0.000000|\r\n|7|-0.542518|-0.542518|-0.000000|-0.000000|\r\n|8|-0.087951|-0.087951|0.000000|-0.000000|\r\n|9|-0.035723|-0.035723|0.000000|-0.000000|\r\n|10000|-0.041655|-0.041655|0.000001|-0.000050|\r\n|10001|0.059743|0.059742|0.000001|-0.000049|\r\n|10002|0.003410|0.003410|-0.000000|-0.000049|\r\n|10003|-0.671292|-0.671294|0.000001|-0.000048|\r\n|10004|-0.117169|-0.117170|0.000001|-0.000047|\r\n|10005|0.144822|0.144821|0.000001|-0.000045|\r\n|10006|-0.001332|-0.001332|0.000000|-0.000045|\r\n|10007|-0.795729|-0.795729|0.000001|-0.000045|\r\n|10008|-0.058562|-0.058562|0.000000|-0.000045|\r\n|10009|0.080222|0.080223|-0.000000|-0.000045|\r\n|40000|-0.056272|0.261853|-0.318125|83.999641|\r\n|40001|0.001396|0.006732|-0.005336|83.994308|\r\n|40002|0.000540|-0.004921|0.005461|83.999771|\r\n|40003|-0.601443|-0.382969|-0.218474|83.781296|\r\n|40004|-0.088934|0.222222|-0.311156|83.470139|\r\n|40005|0.048725|0.100866|-0.052141|83.417999|\r\n|40006|-0.002078|-0.011184|0.009106|83.427109|\r\n|40007|-0.652765|-0.507083|-0.145682|83.281425|\r\n|40008|-0.079999|0.267384|-0.347384|82.934044|\r\n|40009|-0.027125|0.091063|-0.118188|82.815857|\r\n|64470|-0.001395|-0.003402|0.002008|673.607727|\r\n|64471|-0.001808|-0.013858|0.012051|673.619751|\r\n|64472|0.003523|-0.001226|0.004749|673.624512|\r\n|64473|0.001785|0.015721|-0.013935|673.610596|\r\n|64474|0.001010|-0.002219|0.003228|673.613831|\r\n|64475|-0.000174|-0.000121|-0.000053|673.613770|\r\n|64476|0.002843|0.005779|-0.002935|673.610840|\r\n|64477|0.002199|-0.007970|0.010169|673.621033|\r\n|64478|-0.002163|0.000151|-0.002314|673.618713|\r\n|64479|0.018321|0.215750|-0.197428|673.421265|", "comments": []}, {"number": 16577, "title": "By default, only download inception if it doesn't exist already", "body": "Hope this saves some bandwidth:\r\n\r\n- I updated `get_graph_def_from_url_tarball` to accept a default location and only download a file if the file has not already been downloaded. If you do not give it a default location, it will always download (preserving existing behavior).\r\n- I added a default location for the inception model as part of `_default_graph_def_fn`. This means you only download inception the first time you run `run_inception` instead of every time you start your script.\r\n\r\nCheers", "comments": []}, {"number": 16576, "title": "Feature request: tf.estimator hyperparameter tuning", "body": "I'm making an issue here because I'm sure this is being worked on somewhere in this huge repo and I've failed to find it by search.\r\n\r\nAlmost all models have hyperparameters that cannot be set by gradient descent (number of layers, for example). These needs to be tuned, preferably programatically with a smart strategy.\r\n\r\n**What's the canonical way of doing hyperparameter tuning with the tf.estimator API?**\r\n\r\n(also, how can we do early stopping with tf.estimator?)\r\n\r\nI'm currently wrapping around scikit-optimize which is ok, but then I'll never be able to run parallel experiments across workers, and it's a bit tricky to know if the hyperparameters will lead to OOM aside from using tf.profile.\r\n\r\n```python\r\nimport os\r\n\r\nfrom skopt import gp_minimize\r\nfrom skopt.space import Real, Categorical, Integer\r\nfrom skopt.utils import use_named_args\r\n\r\nlogdir = 'tensorboard/'\r\nspace = [\r\n    Real(0.0, 0.1, name='learning_rate'),\r\n    Categorical([True, False], name='skip_connections'),\r\n    Integer(1, 9, name='layers')]\r\n\r\n\r\n@use_named_args(space)\r\ndef score(**params):\r\n    model_dir = os.path.join(logdir, str(params))\r\n    estimator = tf.estimator.Estimator(model_fn, model_dir, params=params)\r\n    trainspec = tf.estimator.TrainSpec(train_input_fn)\r\n    evalspec = tf.estimator.EvalSpec(eval_input_fn)\r\n    try:\r\n        tf.estimator.train_and_evaluate(estimator, trainspec, evalspec)\r\n        metrics = estimator.evaluate(test_input_fn)\r\n        return metrics['loss']\r\n    except (tf.errors.ResourceExhaustedError, tf.train.NanLossDuringTrainingError):\r\n        return 1e9\r\n\r\n\r\ngp_minimize(score, space)\r\n```", "comments": ["We have been pretty successful with this [approach](https://research.google.com/pubs/pub46180.html) but it's not available publicly. It would be terrific if someone signed up to provide that.\r\n\r\nCC @sculleyd ", "I saw [the presentation on Vizier](https://www.youtube.com/watch?v=12OkQrJvyek) and it looks pretty great but it's unfortunate that it's closed source. A canonical open source alternative would be a very welcome addition to TensorFlow Estimator, even just random search.\r\n\r\nI find that particularly getting early stopping and [learning rate annealing as in Keras](https://keras.io/callbacks/#reducelronplateau) into tf.estimator is very messy, as the training and evaluation loop should run distributed so it's not as simple as just having learning rate as a tf.Variable.", "Yeah, any implementation in that direction would be useful, I think.", "Are you thinking about a package like this one (implemented in R): https://tensorflow.rstudio.com/tools/tfruns/articles/overview.html ?\r\nIt would be SO helpful !", "It's not open source, but Vizier is usable outside of Google as [\"hyperparameter tuning\"](https://cloud.google.com/ml-engine/docs/hyperparameter-tuning-overview) in Google Cloud's Machine Learning Engine.", "Just chiming in to let you all know that I too would find both early stopping and hyper parameter optimisation highly useful. Particularly early stopping, as hyper parameter optimisation is easier to implement on my own.", "I am also interested in how to do early stopping with Estimator. ", "Very much interested in at least one mechanism for hyper-parameter tuning with Estimator!!", "Related: https://github.com/tensorflow/tensorflow/issues/7868", "Related: https://github.com/tensorflow/tensorboard/issues/46", "@drpngx Sounds like Vizier would be a great addition to TensorBoard.", "@carlthome \r\nIs this still an issue?\r\nTf.estimator.Estimator uses Tensorflow 1 style of coding and is not recommended for Tensorflow 2.x.It can behave unexpectedly, especially when combined with TF 2 code. There will not be any new addition to the estimators. \r\nCould you please follow the detailed explanation [here](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) and move this issue to close status? Thanks!", "@sushreebarsa sure thing! This has gone quite stale by now."]}, {"number": 16575, "title": "There's no problem running on window, and there's a problem on Ubuntu", "body": "Traceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-fc398a1d9321>\", line 1, in <module>\r\n    runfile('/home/lab326/songpeng/anacoda\u9879\u76ee/tflearn-vgg1.py', wdir='/home/lab326/songpeng/anacoda\u9879\u76ee')\r\n\r\n  File \"/home/lab326/anaconda3/lib/python3.5/site-packages/spyder/utils/site/sitecustomize.py\", line 705, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"/home/lab326/anaconda3/lib/python3.5/site-packages/spyder/utils/site/sitecustomize.py\", line 102, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"/home/lab326/songpeng/anacoda\u9879\u76ee/tflearn-vgg1.py\", line 59, in <module>\r\n    files_extension=['.jpg'], filter_channel=True)\r\n\r\n  File \"/home/lab326/anaconda3/lib/python3.5/site-packages/tflearn/data_utils.py\", line 512, in image_preloader\r\n    flags=files_extension, filter_channel=filter_channel)\r\n\r\n  File \"/home/lab326/anaconda3/lib/python3.5/site-packages/tflearn/data_utils.py\", line 732, in directory_to_samples\r\n    classes = sorted(os.walk(directory).__next__()[1])\r\n\r\nStopIteration\r\n\r\nThere's no problem running on window, and there's a problem on Ubuntu\r\n\r\ncode:\r\nfrom tflearn.data_utils import image_preloader\r\ndata_dir = \"/home/songpeng/dataset\"\r\nX, Y = image_preloader(data_dir, image_shape=(224, 224), mode='folder',\r\n                       categorical_labels=True, normalize=True,\r\n                       files_extension=['.jpg'], filter_channel=True)\r\n\r\n\r\n", "comments": ["This looks like an issue with [TFLearn](https://github.com/tflearn/tflearn). Can you file an issue there instead?"]}, {"number": 16574, "title": "tf.contrib.metrics.streaming_precision raise an exception Attempting to use uninitialized value model/precision/true_positives/count", "body": "Please go to Stack Overflow for help and support:\r\n\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 16573, "title": "Remove duplicated identical lines", "body": "I believe this commit 32db18b4908ec514c5fff8db95e1d05574bb05bb introduced the duplicated lines to the file.", "comments": []}, {"number": 16572, "title": "tf.dynamic_placeholder gives inconsistent results in Tensorflow 1.5", "body": "I'm seeing inconsistencies when calling tf.dynamic_partition(...) with Tensorflow 1.5.\r\n\r\nI'm using tf.dynamic_partition(...) to select rows from two tensors. First is a 1D tensor of spare logits, second is a 2D tensor of one-hot encoded labels. My selector is a 1D tensor with entries made of 0s and 1s.\r\n\r\nSay `matches_mask` has shape (268800,) and 320 of its elements are set to 1, rest are 0.\r\n`scalar_labels` has shape(268800,) and `one_hot_encoded_logits` has shape (268800, 2).\r\n\r\nThe expected behaviour for\r\n\r\n```python\r\n_, selected_scalar_labels = tf.dynamic_partition(scalar_labels, matches_mask, num_partitions=2)\r\n_, selected_one_hot_encoded_logits = tf.dynamic_partition(one_hot_encoded_logits, matches_mask, num_partitions=2)\r\n\r\n```\r\n\r\nis that `selected_scalar_labels` has shape (320, ) and `selected_one_hot_encoded_logits` has shape (320, 2).\r\n\r\nThis code works perfectly fine on two machines, one running Ubuntu 16.4, Tensorflow 1.3 and Tensorflow 1.4 (tested both versions) and CUDA 8 and the other running Ubuntu 16.4, Tensorflow 1.4 and CUDA 8.\r\nHowever today I upgraded first machine to Ubuntu 16.4, Tensorflow 1.5 and CUDA 9 and above breaks.\r\nFor some reason while `selected_scalar_labels` are as expected, `selected_one_hot_encoded_logits` have some strange shapes, such as (268352, 2).\r\n\r\nI tried writing a minimal test script, such as \r\n\r\n```python\r\nlabels_placeholder = tf.placeholder(dtype=tf.int32, shape=[None])\r\nlogits_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 2])\r\n\r\nmask_placeholder = tf.placeholder(dtype=tf.int32, shape=[None])\r\n\r\n_, selected_labels_op = tf.dynamic_partition(labels_placeholder, mask_placeholder, num_partitions=2)\r\n_, selected_logits_op = tf.dynamic_partition(logits_placeholder, mask_placeholder, num_partitions=2)\r\n\r\nwith tf.Session() as session:\r\n\r\n    for _ in range(100):\r\n\r\n        print()\r\n\r\n        size = 268800\r\n\r\n        labels = np.random.randint(0, 2, size=size)\r\n        logits = np.random.uniform(0, 1, size=(size, 2))\r\n\r\n        mask = np.random.binomial(1, np.random.uniform(0, 1), size)\r\n\r\n        feed_dictionary = {labels_placeholder: labels, logits_placeholder: logits, mask_placeholder: mask}\r\n\r\n        selected_labels, selected_logits = session.run([selected_labels_op, selected_logits_op], feed_dictionary)\r\n\r\n        print(\"selected_labels: {}\".format(selected_labels.shape))\r\n        print(\"selected_logits: {}\".format(selected_logits.shape))\r\n```\r\n\r\nbut that gave correct results.\r\nProblem occurs in a more complicated pipeline that is a part of Single Shot Detector's loss computations.\r\nUnfortunately computing `matches_mask` is a complex task that depends on image being used on input and SSD configuration, so I can't really provide here a complete test case without posting a few hundreds lines of code that isn't for public use + images. `matches_mask` itself is very simple though, just a 1D tensor of 0s and 1s.\r\n\r\nI appreciate that without presenting a test case that fails it's unlikely anyone will look into this problem, but I decided to post the issue in case someone else sees similar behaviour - from my point of view it seems to be a new bug introduced by Tensorflow 1.5 or its dependencies.\r\n\r\nI have of course double checked that my`matches_mask`, `scalar_labels` and `one_hot_encoded_logits` have expected dimensions and dtypes before `dynamic_partition` is called and that `matches_mask` is made of 0s and 1s only.\r\n\r\nEnvironments on which code works as expected:\r\n1) Ubuntu 16.04, tensorflow 1.3 installed with `pip install tensorflow-gpu`, CUDA 8.0, cuDNN 6.0, GPU: Tesla V100\r\n2) Ubuntu 16.04, tensorflow 1.4 installed with `pip install tensorflow-gpu`, CUDA 8.0, cuDNN 6.0, GPU: GTX 1080 Ti\r\n\ttf.GIT_VERSION: v1.4.0-19-ga52c8d9\r\n\ttf.VERSION: 1.4.1\r\n3) Ubuntu 16.04, tensorflow 1.4 installed with `pip install tensorflow-gpu`, CUDA 8.0, cuDNN 6.0, GPU: Tesla V100\r\n\ttf.GIT_VERSION: v1.4.0-rc1-11-g130a514\r\n\ttf.VERSION: 1.4.0\r\n\r\nEnvironments on which code fails:\r\n4) Ubuntu 16.04, tensorflow 1.5 installed with `pip install tensorflow-gpu`, CUDA 9.0, cuDNN 7.0, GPU: Tesla V100\r\n\ttf.GIT_VERSION: v1.5.0-0-g37aa430d84\r\n\ttf.VERSION: 1.5.0\r\n\r\nEnvironments 1, 3 and 4 are the same machine, the only differences are in Tensorflow and CUDA versions and dependencies they install.", "comments": ["@itsmeolivia could you take a look?", "Is this still happening with the latest version?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 16571, "title": " Support ./configure [--help|-h] to show possible config options", "body": "This is to fix [16518](https://github.com/tensorflow/tensorflow/issues/16518), which would start the interactive prompt immediately even when issue \"./configure [--help|-h]\".\r\n\r\nThe fix is to show the possible bazel config options when the help/ h commands issued.\r\nFeel free to comment if any we needs to further improve.", "comments": ["Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @case540: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This issue seems to be fixed by https://github.com/tensorflow/tensorflow/pull/17172, thus I'd like to close this PR"]}, {"number": 16570, "title": "Fix typos.", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 16569, "title": " Support optionally passing mode to GANEstimator `discriminator_fn`.", "body": "This fulfills a feature request.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 16568, "title": "Removing duplicate code block that raises exception", "body": "For TensorFlow version 1.5.0-rc1, the code block below raises a `ValueError`. Simply remove the duplication (lines 274 - 277 are exactly the same) and the issue is resolved.\r\n\r\n```\r\n# Add returned summaries to writer in each step.\r\nwriter.add_summary(summary, step)\r\n# Add metadata to visualize the graph for the last run.\r\nif step == (num_steps - 1):\r\n      writer.add_run_metadata(run_metadata, 'step%d' % step)\r\n```", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 16567, "title": "Protobuf 3.5 currently need golang and asm compiler, how can I add that to CI environemnt?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nPlease refer to the pull request on https://github.com/tensorflow/tensorflow/pull/16480\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10/ Jenkins\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: MSVC 2015\r\n- **CUDA/cuDNN version**: CUDA 9.0 with CUDNN 7\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: by pull request\r\n\r\n### Describe the problem\r\nI want to update the cmake build with protobuf 3.5.1.1 with grpc support. However the latest grpc requires golang and asm compiler. Build fail on Jenkin's environment\r\n\r\n### Source code / logs\r\nhttps://source.cloud.google.com/results/invocations/76cb8643-d30a-45d6-9536-5372feb88e6d/log\r\n", "comments": ["/CC @mrry, can you resolve this?", "IIRC, these dependencies are inherited from BoringSSL via gRPC. We currently avoid this problem by [building only the `grpc++_unsecure` target](https://github.com/tensorflow/tensorflow/blob/ba64f5334d4bba31d22c30e09a96f806ea0e2f7e/tensorflow/contrib/cmake/external/grpc.cmake#L43), which doesn't depend on BoringSSL. Presumably the same approach is possible here.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 16566, "title": "add visibility to  //tensorflow/contrib/tensor_forest/proto:fertile_stats_proto", "body": "Since the proto is a public-visible and TreePath depending on the proto \r\n\r\nLet's make it public...", "comments": ["This doesn't seem right. That rule doesn't directly depend on the proto file.", "what?!?\r\n\r\n`TreePath` is being saved in protobuf, while there is no python package to deserialized it!\r\n\r\nSo that there is no way to a user to read tree_path from the prediction.", "https://github.com/tensorflow/tensorflow/blob/ced7455a9c3d2ff1570f9752c9ca432af1913167/tensorflow/contrib/tensor_forest/python/tensor_forest.py#L108  \r\nuser can specify `inference_tree_paths` and if so, it would https://github.com/tensorflow/tensorflow/blob/ced7455a9c3d2ff1570f9752c9ca432af1913167/tensorflow/contrib/tensor_forest/kernels/model_ops.cc#L237  SerializeAsString and output\r\n\r\nbut if there is no `fertile_stats_proto_pb2.py`, user from python side can never have a chance to really use it.", "Ah, thanks for the explanation."]}]