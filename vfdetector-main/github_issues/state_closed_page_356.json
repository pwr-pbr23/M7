[{"number": 43390, "title": "Solve leftover from merge conflict", "body": "", "comments": []}, {"number": 43389, "title": "Solve leftover from merge conflict", "body": "", "comments": []}, {"number": 43388, "title": "Solve leftover from merge conflict", "body": "", "comments": []}, {"number": 43387, "title": "So frustrating with this error: Failed to load the native TensorFlow runtime", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OS X\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version: 2.0.1\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nIt is so frustrating that when i google this error, so many webpages are found but no (working) solutions are provided. If this is such a common error, can you guys provide a fix or an official solution for fixing the problem? \r\nBasically, i am trying to install tensorflow on my Mac. So what i did is:\r\npip install tensorflow\r\nIt was finished without any error.\r\nBut when I try:\r\n import tensorflow as tf\r\n\r\nThe common import error came up: Failed to load the native TensorFlow runtime.\r\n\r\nI couldn't figure out what is the exact issue, is it because Python 3.7 does not work with tensorflow 2.0.1? I tried:\r\npip install tensorflow==1.15.0. \r\nBut it says no 1.15.0 found in 2.0.1, not sure what does that mean.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip install tensorflow\r\nimport tensorflow as tf\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n", "comments": ["@wggbullet,\r\nCould you please provide the complete log of the error you are facing? \r\n\r\nAlso, please check if you are facing the same issue with TensorFlow v2.3 as well? Make sure you have all the required dependencies as mentioned [here](https://www.tensorflow.org/install/pip#system-requirements). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43387\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43387\">No</a>\n", "> Closing as stale. Please reopen if you'd like to work on this further.\r\n\r\nHow can i reopen this issue? \r\nI am still unable to get tensorflow working on my computer.", "the full log/trace:\r\n(base) [deeplearning]$ python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\nTraceback (most recent call last):\r\n  File \"/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation\r\n  Referenced from: /Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security\r\n in /Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation\r\n  Referenced from: /Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security\r\n in /Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n(base) [deeplearning]$ \r\n", "Take a look at https://stackoverflow.com/questions/51765519/error-importing-tensorflow-in-anaconda-on-mac-osx .\r\n\r\nwhat version of OSX are you using? We require macOS 10.12.6 (Sierra) or later (64-bit) (no GPU support) [https://www.tensorflow.org/install/pip] ", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43387\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43387\">No</a>\n", "> Take a look at https://stackoverflow.com/questions/51765519/error-importing-tensorflow-in-anaconda-on-mac-osx .\r\n> \r\n> what version of OSX are you using? We require macOS 10.12.6 (Sierra) or later (64-bit) (no GPU support) [https://www.tensorflow.org/install/pip]\r\n\r\n@wggbullet,\r\nPlease take a look at @rohan100jain's comment and let us know if you are still facing the same issue. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43387\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43387\">No</a>\n", "Tensorflow 1.9.0 was the last version to be supported on OSX 10.11 El Capitan."]}, {"number": 43386, "title": "tf.lite.Optimize.DEFAULT - Hybrid models are not supported on TFLite Micro.", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.6\r\n- TensorFlow installed from (source or binary): 2.3.0\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to convert a model for tflite, but keep hitting:\r\n\r\n\"Hybrid models are not supported on TFLite Micro.\"\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nHere is my model:\r\n\r\n```\r\nmodel = Sequential([\r\n    Conv2D(4, 3, \r\n           padding='same',\r\n           activation='relu',\r\n           input_shape=(IMG_WIDTH, IMG_HEIGHT, 1),\r\n           name='conv_layer1'),\r\n    MaxPooling2D(name='max_pooling1'),\r\n    Conv2D(4, 3, \r\n           padding='same',\r\n           activation='relu',\r\n           name='conv_layer2'),\r\n    MaxPooling2D(name='max_pooling2', pool_size=(2,2)),\r\n    Flatten(),\r\n    Dense(\r\n        20,\r\n        activation='relu',\r\n        name='hidden_layer'\r\n    ),\r\n    Dense(1, activation='sigmoid', name='output')\r\n])\r\n```\r\n\r\nAnd here is the code I am using to convert the mode:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"checkpoint.model\")\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nmodel = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(model)\r\n```\r\n\r\nI have also tried `tf.lite.Optimize.OPTIMIZE_FOR_SIZE` which has the same issue. Removing all optimisations lets me \r\n\r\nIs there any way to avoid triggering this error with my model? Ideally, I would like to optimize my model to make it smaller.\r\n", "comments": ["See here:\r\nhttps://www.tensorflow.org/lite/performance/post_training_quantization\r\n\r\nGo for full integer quantization. The default optimization leaves part of the operations as floating point (hence hybrid) and mixing isn't supported in micro.\r\n", "@yair-ehrenwald \r\n\r\nGreat! Setting a value for the representative_dataset seems to fix it.\r\n\r\nThanks for the quick response.", "> See here:\r\n> https://www.tensorflow.org/lite/performance/post_training_quantization\r\n> \r\n> Go for full integer quantization. The default optimization leaves part of the operations as floating point (hence hybrid) and mixing isn't supported in micro.\r\n\r\nThanks for this you answered a question that I had been scratching my head about. Do you know why full integer quantization would be failing with Zero Padding ? https://github.com/tensorflow/tensorflow/issues/50725 "]}, {"number": 43385, "title": "TF 2.3: Learning rate is NOT printed in the training output when using \"ReduceLROnPlateau\" callback", "body": "**TensorFlow version:** 2.3.0\r\n**Python version:** 3.8.2\r\n**Issue:**\r\nGiven the TensorFlow version is upgraded to 2.3.0,\r\nGiven using \"ReduceLROnPlateau\" callback,\r\nWhen perform training on the model with learning rate decay,\r\nThen learning rate \"lr - xxx\" is NOT printed in the training output\r\n\r\n```\r\nreduce_learning_rate = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=5)\r\nhistory = model.fit(x_train, y_train, epochs=training_epochs, batch_size=batch_size, validation_data=(x_test, y_test), callbacks=[reduce_learning_rate])\r\n```\r\n\r\n**TensorFlow 2.2.0:** Showing learning rate in the output of each epoch\r\n`133/133 [==============================] - 2s 15ms/step - loss: 6.5944 - accuracy: 0.5781 - val_loss: 0.1361 - val_accuracy: 0.9640 - lr: 0.0200`\r\n\r\n**TensorFlow 2.3.0:** Not showing learning rate in the output of each epoch\r\n`133/133 [==============================] - 2s 15ms/step - loss: 6.5944 - accuracy: 0.5781 - val_loss: 0.1361 - val_accuracy: 0.9640`", "comments": ["@mikemikezhu \r\n\r\nCan you share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43385\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43385\">No</a>\n"]}, {"number": 43384, "title": "flatbuffers changes for SHAPE op in TFLM", "body": "Changes in flatbuffer_conversions.cc and flatbuffer_conversion.h needed for adding SHAPE to micro\r\n\r\nSee https://github.com/tensorflow/tensorflow/issues/43380#issuecomment-695801642\r\n", "comments": []}, {"number": 43383, "title": "Add TF_UpdateEdge C API.", "body": "In order to support more complicated conditional/loop statement flow. We have to expose `graph->graph.UpdateEdge` fucntion to [.NET binding](https://github.com/SciSharp/TensorFlow.NET). There still are several APIs needed to support [MNIST RNN/ LSTM](https://github.com/SciSharp/SciSharp-Stack-Examples/blob/master/src/TensorFlowNET.Examples/ImageProcessing/DigitRecognitionRNN.cs) model training that I'll submit in other PR once this one is approved. Thanks.", "comments": ["@saxenasaurabh Can you review this PR?", "Thanks for the PR, could we update the [UpdateEdge](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/python_api.cc;l=58;drc=9c75b7ffd22ead5842dcb47e7b037f3bea46b9a7) function in `tensorflow/c/python_api.cc` to call this? \r\n\r\nAlso could you please add a unit test?", "@saxenasaurabh I'll update `tensorflow/c/python_api.cc` to call this API. Can we skip unit test for this? Because basically I just copy and paste the code.", "I think it would be nice to have a test since this is a public API. You should be able to adapt an [existing C++ test](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/graph/graph_test.cc;l=541;drc=000c8f09eae7b266ebda7afb53a83fd65b461db5).", "I went through this testing code, looks like it not fit for testing public API. It's just for internal graph testing. I think this [test](https://github.com/tensorflow/tensorflow/blob/686912c768fbd845eca78ad16fd6a2074acb4f97/tensorflow/c/c_api_test.cc#L459) is better for graph's public API. Please advise.", "Yes, you should add your test to `c_api_test`. What I meant was that you can add a C equivalent of the C++ test for `UpdateEdge`.", "@saxenasaurabh @gbaned Please help reivew the unit test of `UpdateEdge`.", "Seems auto-merge is not happening but the changes are now committed, so we can close this. Thank you for the PR."]}, {"number": 43382, "title": "[TFLM] Added handling of tensors used as outputs for ops but then never used", "body": "Discussed in https://github.com/tensorflow/tensorflow/issues/43380\r\n\r\nA long term fix would probably eliminate the need for this, but at least in my tests it solves the issue for now.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "This failed some of my tests now, converting to draft, needs some work.", "@yair-ehrenwald This PR is in draft, any update on this? Please. Thanks!", "@yair-ehrenwald This PR is in draft, any update on this? Please. Thanks!", "@yair-ehrenwald This PR is in draft, any update on this? Please. Thanks!", "Closing this, going with @advaitjain 's suggestion in https://github.com/tensorflow/tensorflow/issues/43380\r\n\r\n"]}, {"number": 43381, "title": "Add shape op and tests to TFLM", "body": "Added SHAPE op and wrote a few tests.\r\nSHAPE is required in order to run keras GRUCell layers in TFLM.\r\n\r\nThe necessity is further explained in this issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/43380\r\n\r\n\r\nI left the switch/case for checking output types even though I removed support for int64 because I thought it might be convenient for adding types in the future (maybe even use int8?).\r\n\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@advaitjain @njeffrie \r\nAny chance of getting this moving? :)\r\n"]}, {"number": 43378, "title": "google colab crashed after run tensofrflow code which is upgraded from 1.8 to 2.2", "body": "I subscribed in colab pro and I have enough RAM but the code crashes the colab after upgrade it from TF1.8 to TF 2.2, how to solve this problem\r\n![colab](https://user-images.githubusercontent.com/26867350/93712611-952bfa00-fb89-11ea-9e6f-eb4df272bd9a.png)\r\n ", "comments": ["@RokiaAbdeen \r\n\r\nCan you please share the colab link you are referring to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43377, "title": "Extract Graph in TensorFlow 2", "body": "I'm wondering how one might extract a graph in Tensorflow2.\r\nSomething like [this](https://github.com/tensorflow/tensorflow/blob/ae3c8479f88da1cd5636b974f653f27755cb0034/tensorflow/tensorboard/components/tf-tensorboard/test/data/graph_run_run2.pbtxt) in the pbtxt format: \r\n```\r\nnode {\r\n  name: \"a\"\r\n  op: \"matmul\"\r\n}\r\nnode {\r\n  name: \"b\"\r\n  op: \"matmul\"\r\n  input: \"a:0\"\r\n}\r\nnode {\r\n  name: \"c\"\r\n  op: \"matmul\"\r\n  input: \"a:0\"\r\n  input: \"b:0\"\r\n}\r\n```\r\n\r\nIt was pretty straightforward in TF1.", "comments": ["https://www.tensorflow.org/api_docs/python/tf/io/write_graph", "I've seen that. I guess a better way to ask this question is\r\nhow to i convert an MNIST model into a graph object for the\r\nargument ``graph_or_graph_def`` in \r\n\r\n```python3\r\ntf.io.write_graph(\r\n    graph_or_graph_def, logdir, name, as_text=True\r\n)\r\n```", "I don't know why you need this but check https://stackoverflow.com/questions/63181951/how-to-get-graph-or-graphdef-from-a-given-model", "I designed a custom silicon accelerator.\r\nNow I'm trying to write a TF compiler for it.\r\nWas playing around with MLIR, but MLIR is missing\r\nsome basic documentation...", "Please use the MLIR channels usually they are very available:\r\nhttps://groups.google.com/a/tensorflow.org/g/mlir\r\nhttps://llvm.discourse.group/c/mlir/31\r\nhttps://discord.com/invite/xS7Z362", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43376, "title": "I have found this error. tensorflow.python.framework.errors_impl.InvalidArgumentError:  indices[5] = 1 is not in [0, 1)", "body": "```\r\n# set the matplotlib backend so figures can be saved in the background\r\nimport matplotlib\r\nmatplotlib.use(\"Agg\")\r\n# import the necessary packages\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.callbacks import LearningRateScheduler\r\nfrom tensorflow.keras.optimizers import Adagrad\r\nfrom tensorflow.keras.utils import to_categorical\r\nfrom sklearn.metrics import classification_report\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom keras.layers import Dense, Embedding\r\nfrom pyimagesearch.cancernet import CancerNet\r\nfrom pyimagesearch import config\r\nfrom imutils import paths\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport argparse\r\nimport os\r\n\r\n# determine the total number of image paths in training, validation,\r\n# and testing directories\r\ntrainPaths = list(paths.list_images(config.TRAIN_PATH))\r\ntotalTrain = len(trainPaths)\r\ntotalVal = len(list(paths.list_images(config.VAL_PATH)))\r\ntotalTest = len(list(paths.list_images(config.TEST_PATH)))\r\n# calculate the total number of training images in each class and\r\n# initialize a dictionary to store the class weights\r\ntrainLabels = [int(p.split(os.path.sep)[-2]) for p in trainPaths]\r\ntrainLabels = to_categorical(trainLabels)\r\nclassTotals = trainLabels.sum(axis=0)\r\nclassWeight = dict()\r\n# loop over all classes and calculate the class weight\r\nfor i in range(0, len(classTotals)):\r\n\tclassWeight[i] = classTotals.max() / classTotals[i]\r\n# construct the argument parser and parse the arguments\r\n\tap = argparse.ArgumentParser()\r\n\tap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\r\n\t\t\t\t\thelp=\"path to output loss/accuracy plot\")\r\n\targs = vars(ap.parse_args())\r\n\t# initialize our number of epochs, initial learning rate, and batch\r\n\t# size\r\n\tNUM_EPOCHS = 2\r\n\r\n\tINIT_LR = 1e-2\r\n\tBS = 32\r\n\t# initialize the training data augmentation object\r\n\ttrainAug = ImageDataGenerator(\r\n\t\trescale=1 / 255.0,\r\n\t\trotation_range=20,\r\n\t\tzoom_range=0.05,\r\n\t\twidth_shift_range=0.1,\r\n\t\theight_shift_range=0.1,\r\n\t\tshear_range=0.05,\r\n\t\thorizontal_flip=True,\r\n\t\tvertical_flip=True,\r\n\t\tfill_mode=\"nearest\")\r\n\t# initialize the validation (and testing) data augmentation object\r\n\tvalAug = ImageDataGenerator(rescale=1 / 255.0)\r\n\t# initialize the training generator\r\n\ttrainGen = trainAug.flow_from_directory(\r\n\t\tconfig.TRAIN_PATH,\r\n\t\tclass_mode=\"categorical\",\r\n\t\ttarget_size=(48, 48),\r\n\t\tcolor_mode=\"rgb\",\r\n\t\tshuffle=True,\r\n\t\tbatch_size=BS)\r\n\t# initialize the validation generator\r\n\tvalGen = valAug.flow_from_directory(\r\n\t\tconfig.VAL_PATH,\r\n\t\tclass_mode=\"categorical\",\r\n\t\ttarget_size=(48, 48),\r\n\t\tcolor_mode=\"rgb\",\r\n\t\tshuffle=False,\r\n\t\tbatch_size=BS)\r\n\t# initialize the testing generator\r\n\ttestGen = valAug.flow_from_directory(\r\n\t\tconfig.TEST_PATH,\r\n\t\tclass_mode=\"categorical\",\r\n\t\ttarget_size=(48, 48),\r\n\t\tcolor_mode=\"rgb\",\r\n\t\tshuffle=False,\r\n\t\tbatch_size=BS)\r\n\t# initialize our CancerNet model and compile it\r\n\tmodel = CancerNet.build(width=48, height=48, depth=3,\r\n\t\t\t\t\t\t\tclasses=2)\r\n\tmodel.add(Embedding(batch_size=32, input_shape=(classWeight,), input_dim=1024*1000, output_dim=256))\r\n\r\n\topt = Adagrad(lr=INIT_LR, decay=INIT_LR / NUM_EPOCHS)\r\n\tmodel.compile(loss=\"binary_crossentropy\", optimizer=opt,\r\n\t\t\t\t  metrics=[\"accuracy\"])\r\n\t# fit the model\r\n\tH = model.fit(\r\n\t\tx=trainGen,\r\n\t\tsteps_per_epoch=totalTrain // BS,\r\n\t\tvalidation_data=valGen,\r\n\t\tvalidation_steps=totalVal // BS,\r\n\t\tclass_weight=classWeight,\r\n\t\tepochs=NUM_EPOCHS)\r\n# reset the testing generator and then use our trained model to\r\n# make predictions on the data\r\nprint(\"[INFO] evaluating network...\")\r\ntestGen.reset()\r\npredIdxs = model.predict(x=testGen, steps=(totalTest // BS) + 1)\r\n# for each image in the testing set we need to find the index of the\r\n# label with corresponding largest predicted probability\r\npredIdxs = np.argmax(predIdxs, axis=1)\r\n# show a nicely formatted classification report\r\nprint(classification_report(testGen.classes, predIdxs,\r\n\ttarget_names=testGen.class_indices.keys()))\r\n# compute the confusion matrix and and use it to derive the raw\r\n# accuracy, sensitivity, and specificity\r\ncm = confusion_matrix(testGen.classes, predIdxs)\r\ntotal = sum(sum(cm))\r\nacc = (cm[0, 0] + cm[1, 1]) / total\r\nsensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\r\nspecificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\r\n# show the confusion matrix, accuracy, sensitivity, and specificity\r\nprint(cm)\r\nprint(\"accuracy: {:.4f}\".format(acc))\r\nprint(\"sensitivity: {:.4f}\".format(sensitivity))\r\nprint(\"specificity: {:.4f}\".format(specificity))\r\n# plot the training loss and accuracy\r\nN = NUM_EPOCHS\r\nplt.style.use(\"ggplot\")\r\nplt.figure()\r\nplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\r\nplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\r\nplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\r\nplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\r\nplt.title(\"Training Loss and Accuracy on Dataset\")\r\nplt.xlabel(\"Epoch #\")\r\nplt.ylabel(\"Loss/Accuracy\")\r\nplt.legend(loc=\"lower left\")\r\nplt.savefig(args[\"plot\"])\r\n```", "comments": ["@khalilur1 \r\nI ran the code shared and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/9db2165604c7a9cc5e3870e46a507dcf/untitled415.ipynb).\r\nAs per the error shared please refer to: [link](https://stackoverflow.com/questions/51223936/tensorflow-invalidargumenterror-indices-while-training-with-keras), #38613", "@Saduf2019 I used model.fit to load data from disk without using gpu.", "@khalilur1 \r\nI ran your code with out gpu, please share a colab gist with the error reported.", "@Saduf2019 I have used the Kagale data set (https://www.kaggle.com/paultimothymooney/breast-histopathology-images/download) please use it. find a \r\n[breast-cancer-classification.zip](https://github.com/tensorflow/tensorflow/files/5259757/breast-cancer-classification.zip)\r\ncomplete project. ", "@khalilur1 \r\nPlease provide with simple stand alone code such that we could replicate the issue faced, or if possible share a colab gist with error reported.", "@Saduf2019 Thanks for your support issue is resolved. I am closing it ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43376\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43376\">No</a>\n"]}, {"number": 43375, "title": "[RNN] Error while converting the Decoder in image-Captioning model to TFLite file.", "body": "**System information**\r\n-Google Colab\r\n-TensorFlow version: 2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(encoder)\r\ntflite_model = converter.convert()\r\nopen(\"decoder.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-35-c6f70bd3ca2b> in <module>()\r\n----> 1 tflite_model2 = converter2.convert()\r\n      2 open(\"decoder.tflite\", \"wb\").write(tflite_model2)\r\n\r\n11 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    802 \r\n    803     func = _saving_utils.trace_model_call(self._keras_model, input_signature)\r\n--> 804     concrete_func = func.get_concrete_function()\r\n    805     self._funcs = [concrete_func]\r\n    806 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)\r\n   1165       ValueError: if this object has not yet been called on concrete values.\r\n   1166     \"\"\"\r\n-> 1167     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n   1168     concrete._garbage_collector.release()  # pylint: disable=protected-access\r\n   1169     return concrete\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)\r\n   1071       if self._stateful_fn is None:\r\n   1072         initializers = []\r\n-> 1073         self._initialize(args, kwargs, add_initializers_to=initializers)\r\n   1074         self._initialize_uninitialized_variables(initializers)\r\n   1075 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    695     self._concrete_stateful_fn = (\r\n    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 697             *args, **kwds))\r\n    698 \r\n    699     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2853       args, kwargs = None, None\r\n   2854     with self._lock:\r\n-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2856     return graph_function\r\n   2857 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3211 \r\n   3212       self._function_cache.missed.add(call_context_key)\r\n-> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n   3214       self._function_cache.primary[cache_key] = graph_function\r\n   3215       return graph_function, args, kwargs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3073             arg_names=arg_names,\r\n   3074             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 3075             capture_by_value=self._capture_by_value),\r\n   3076         self._function_attributes,\r\n   3077         function_spec=self.function_spec,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    599         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    602 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py in _wrapped_model(*args)\r\n    132     with base_layer_utils.call_context().enter(\r\n    133         model, inputs=inputs, build_graph=False, training=False, saving=True):\r\n--> 134       outputs = model(inputs, training=False)\r\n    135 \r\n    136     # Outputs always has to be a flat dict.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    983 \r\n    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n--> 985           outputs = call_fn(inputs, *args, **kwargs)\r\n    986 \r\n    987         if self._activity_regularizer:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    300   def wrapper(*args, **kwargs):\r\n    301     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 302       return func(*args, **kwargs)\r\n    303 \r\n    304   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\nTypeError: call() missing 2 required positional arguments: 'features' and 'hidden'\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://colab.research.google.com/drive/106LqEnqXNpTp110ZW1JBguPE4RdiapIj?usp=sharing\r\n```\r\n\r\n**Failure details**\r\nTypeError: call() missing 2 required positional arguments: 'features' and 'hidden'\r\nI converted the encoder in the same fashion and I could obtain the corresponding TFLite file without errors.\r\nI believe this error is in some way related to how we are defining the decoder using RNN. (Please correct me if I'm wrong).\r\n\r\n", "comments": ["@amaterasu59 \r\n\r\nCan you add the below lines of code before converting and see if the issue still persists.\r\n\r\n```\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter=True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\nPlease, refer similar issue #35590 and see if it helps you.Thanks!", "@ravikyram the issue still persists without any change.\r\nAfter looking at the error traceback call, the error I believe is related to the RNN decoder class and how we are using it in the training step.", "How about creating a concrete function manually for your decoder? And try the from_concrete_functions converter API."]}, {"number": 43373, "title": "TF2.0, how to include feature processing code in saved model.", "body": "`<em>Please` make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution : macOS Catalina 10.15.3\r\n- TensorFlow installed from : binary\r\n- TensorFlow version : 2.2.0\r\n- Python version: 3.7.3\r\n\r\nHi, I have a working code (shown below): it processed the raw input (in tensor functions); then the processed input is used as the input of the keras model. I'm able to save the model, and also can make inference after loading model. But my problem is: this feature processing is done before inputing to the keras model, so it is not part of the saved model. When I run inference, I need to process the raw input first, then run model.predict on the processed data.\r\n\r\nBecause we would like to put the model on production, we need to include the feature processing code into the saved model, so the raw data can be fed into the saved model directly. I've tried different ways to make it work, but all failed. Please help, thanks!\r\n\r\nNote that in the following code, 'FeatureProcess' is the function processing the raw input, I don't show the details of this 'FeatureProcess' function due to privacy reason. Inside this 'FeatureProcess' function is all tensor functions and manipulations.\r\n\r\ndef line_to_multiple_features(inputString, mode):\r\n  inputString = [inputString]\r\n  batch_ids, input_ids_long, labels, input_ids = FeatureProcess(inputString)\r\n  d = {'ids':batch_ids[0], 'input_ids_long':input_ids_long[0], 'input_ids':input_ids[0]}\r\n  if mode == 'train':\r\n    return d, labels[0]\r\n  else:\r\n    return d\r\n\r\ndef get_unbatched_feature_label(filename, mode, epochs, feature_delimiter):\r\n  dataset = tf.data.TextLineDataset(filename)\r\n  if mode == 'train':\r\n    dataset = dataset.repeat(epochs).shuffle(10000)\r\n    # convert every line to features\r\n  unbatch_parse_data = dataset.map(lambda x: line_to_multiple_features(x))\r\n  return unbatch_parse_data\r\n\r\n# Dataset for input data\r\ndef get_feature_label(_input):\r\n  input_data = ctx.absolute_path(_input)\r\n  input_files = tf.data.Dataset.list_files(input_data)\r\n  datasets_unbatched = get_unbatched_feature_label(input_files, args.mode, args.epochs, '\\t')\r\n  data_feature_label = datasets_unbatched.batch(GLOBAL_BATCH_SIZE)\r\n  return data_feature_label\r\n\r\ndef build_keras_model():\r\n  vocab_size = 100000 \r\n  embedding_dim = args.hidden_size\r\n  num_filters = 512\r\n  filter_sizes = [2,3,4]\r\n  drop = 0.5\r\n\r\n  inputs = {'ids':Input(shape=(4, ), dtype='string'),\r\n              'input_ids_long':Input(shape=(args.seq_length_long, ), dtype='int32'),\r\n              'input_ids':Input(shape=(args.seq_length, ), dtype='int32'))\r\n             }\r\n\r\n  embedding_short = Embedding(input_dim=vocab_size, output_dim=embedding_dim, \r\n  input_length=args.seq_length)(inputs['input_ids'])\r\n  embedding_long = Embedding(input_dim=vocab_size, output_dim=embedding_dim, \r\n  input_length=args.seq_length_long)(inputs['input_ids_long'])\r\n  reshape_short = Reshape((args.seq_length, embedding_dim, 1))(embedding_short)\r\n  reshape_long = Reshape((args.seq_length_long, embedding_dim, 1))(embedding_long)\r\n  conv_0_short = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_short)\r\n  conv_1_short = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_short)\r\n  conv_2_short = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_short)\r\n\r\n  conv_0_long = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_long)\r\n  conv_1_long = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_long)\r\n  conv_2_long = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_long)\r\n\r\n  maxpool_0_short = MaxPool2D(pool_size=(args.seq_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0_short)\r\n  maxpool_1_short = MaxPool2D(pool_size=(args.seq_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1_short)\r\n  maxpool_2_short = MaxPool2D(pool_size=(args.seq_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2_short)\r\n\r\n  maxpool_0_long = MaxPool2D(pool_size=(args.seq_length_long - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0_long)\r\n  maxpool_1_long = MaxPool2D(pool_size=(args.seq_length_long - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1_long)\r\n  maxpool_2_long = MaxPool2D(pool_size=(args.seq_length_long - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2_long)\r\n\r\n  concatenated_tensor = Concatenate(axis=1)([maxpool_0_short, maxpool_1_short, maxpool_2_short, maxpool_0_long, maxpool_1_long, maxpool_2_long])\r\n  flatten = Flatten()(concatenated_tensor)\r\n  dropout = Dropout(drop)(flatten)\r\n  logits = Dense(units=args.num_classes)(dropout)\r\n  pred_probs = tf.keras.activations.sigmoid(logits)\r\n  model = Model(inputs=inputs, outputs=pred_probs)\r\n\r\n  def get_loss_fn():\r\n    def get_loss(labels, outputs):\r\n      logits = tf.math.log((outputs+0.0000001) / ( 1 - outputs+0.0000001))\r\n      loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)\r\n      cost = tf.reduce_mean(\r\n                tf.reduce_sum(loss, axis=1)\r\n            )\r\n      return cost\r\n    return get_loss\r\n\r\n  adam = Adam(lr=0.001)\r\n  model.compile(optimizer=adam,\r\n                  loss=get_loss_fn(),\r\n                  metrics=['accuracy'])\r\n  return model\r\n\r\ntf.io.gfile.makedirs(args.model_dir)\r\nfilepath = args.model_dir # + \"/weights-{epoch:04d}\"\r\ncallbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=filepath, monitor=\"val_accuracy\", verbose=1, save_best_only=True, mode='auto', save_freq=10)]\r\nsteps_per_epoch = 60000 / GLOBAL_BATCH_SIZE * 0.9\r\n\r\nif args.mode == 'train':\r\n  with strategy.scope():\r\n    multi_worker_model = build_keras_model()\r\n  train_feature_label = get_feature_label(args.input)\r\n  validation_feature_label = get_feature_label(args.val_path)\r\n  multi_worker_model.fit(x=train_feature_label, validation_data=validation_feature_label, \r\n                                        epochs=args.epochs, steps_per_epoch=steps_per_epoch, callbacks=callbacks)\r\n  results = multi_worker_model.evaluate(validation_feature_label, verbose=2)\r\n  multi_worker_model.save(args.model_dir)\r\nelse: \r\n  print('now is inference mode')\r\n  test_feature = get_feature_label(args.test_path)\r\n  reconstructed_model = tf.keras.models.load_model(args.model_dir, compile=False)\r\n  for elem in test_feature:\r\n    y_pred_elem = reconstructed_model.predict(elem)", "comments": ["sorry I put indented spaces for the code, but the indents were all gone after I submitted it. ", "> sorry I put indented spaces for the code, but the indents were all gone after I submitted it.\r\n\r\nI have the same [problem](https://github.com/tensorflow/tensorflow/issues/42552) . They suggest me use ```tf.keras.layers.experimental.preprocessing```.  : )", "@shangh1 \r\nPlease share a colab gist with error reported.", "@DachuanZhao @Saduf2019  Thank you both! I'll try tf.keras.layers.experimental.preprocessing, and update you.", "Hi @Saduf2019 @DachuanZhao, my issue is similar as this [problem](https://github.com/tensorflow/tensorflow/issues/42552) brought up by @DachuanZhao. But I just looked at functions in tf.keras.layers.experimental.preprocessing, it only has around 20 classes, seems very limited. My data processing code is written all in tf-functions, and I'm not able to convert all tf-functions in my data processing code to tf.keras.layers.experimental.preprocessing. Some of my tf functions in data processing code include: tf.strings.split, tf.lookup.StaticHashTable, tf.strings.to_number, tf.sparse.SparseTensor, tf.compat.v1.sparse_to_dense, tf.fill, tf.concat, tf.not_equal, tf.cast. ", "@Saduf2019 sorry I'm not able to share the data processing code as it belongs to the company. Did I describe my problem clearly? ", "@shangh1 \r\nIn this case, could you please provide us with dummy data with similar shape and size to reproduce the issue reported here.", "@Saduf2019 Thanks. I simplify the code, and put it here. https://github.com/shangh1/model_tf2/tree/master", "@Saduf2019 @gowthamkpr hello, is there any update on this? Thanks.", "> @Saduf2019 @gowthamkpr hello, is there any update on this? Thanks.\r\n\r\nHave you read the new document [https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers) ? It solves my problem .", "@shangh1 Please take a look at the above comment.Thanks!", "> @shangh1 Please take a look at the above comment.Thanks!\r\n\r\nHi , I want to know whether it will slow down performance  or not in inference status if a model contains a preprocessing layer s  , especially when the model which contains a preprocessing layer served in tf-serving . Is there any document for that ?\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43373\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43373\">No</a>\n"]}, {"number": 43372, "title": "Build from source for the Raspberry Pi: Bazel binary wasn't found", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Buster \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4 8Gb \r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: 2.3\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.5.0\r\n- GCC/Compiler version (if compiling from source): 8.3\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI had a similar issue as #43232, when I am trying to deploy a TF2 Mask RCNN model to the Pi. Hence, I am trying to follow https://www.tensorflow.org/install/source_rpi#python-3.7 to get the TF nightly to the Pi. \r\n\r\nWhen I am trying to run: \r\n`tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\\r\n    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh`\r\n\r\nThe following error appears:\r\n`/usr/local/bin/bazel: line 163: /usr/local/lib/bazel/bin/bazel-real: cannot execute binary file: Exec format error\r\nTF_BUILD_INFO = {container_type: \"pi-python37\", command: \"tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\", source_HEAD: \"da8558533d925694483d2c136a9220d6d49d843c\", source_remote_origin: \"https://github.com/tensorflow/tensorflow.git\", OS: \"Linux\", kernel: \"5.4.51-v7l+\", architecture: \"armv7l\", processor: \"ARMv7 Processor rev 3 (v7l)\", processor_count: \"4\", memory_total: \"7882804 kB\", swap_total: \"102396 kB\", Bazel_version: \"ERROR: The project you're trying to build requires Bazel 3.5.0 (specified in /workspace/.bazelversion), but it wasn't found in /usr/local/lib/bazel/bin.\", Java_version: \"1.8.0_265\", Python_version: \"2.7.12\", gpp_version: \"g++ (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\", swig_version: \"\", NVIDIA_driver_version: \"\", CUDA_device_count: \"0\", CUDA_device_names: \"\", CUDA_toolkit_version: \"\"}\r\nWARNING: current bazel installation is not a release version.\r\n\r\n...\r\n\r\nBuilding for the Pi Two/Three, with NEON acceleration\r\n/usr/local/bin/bazel: line 163: /usr/local/lib/bazel/bin/bazel-real: cannot execute binary file: Exec format error\r\nERROR: The project you're trying to build requires Bazel 3.5.0 (specified in /workspace/.bazelversion), but it wasn't found in /usr/local/lib/bazel/bin.\r\n\r\nBazel binaries for all official releases can be downloaded from here:\r\n  https://github.com/bazelbuild/bazel/releases\r\n\r\nPlease put the downloaded Bazel binary into this location:\r\n  /usr/local/lib/bazel/bin/bazel-3.5.0-linux-armv7l`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI have built the Bazel binary and copy it to both: /usr/local/lib/bazel/bin/bazel-3.5.0-linux-armv7l and  /usr/local/bin\r\n\r\n/tf_workspace ./configure gives:\r\n\r\nYou have bazel 3.5.0- (@non-git) installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: \r\n\r\n\r\nHowever, when I tried to run \r\n'tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\\r\n    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh'\r\n\r\nThe above error happens. \r\n\r\nAny help would be highly appreciated. \r\n\r\nKInd regards,\r\nBrian\r\n\r\n\r\n\r\n\r\n", "comments": ["You're building ARM binary on x86_64 machine. So the Bazel should be x86_64 version not armv7.", "For cross compiling for ARM, you can also use CMake.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake_arm", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43372\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43372\">No</a>\n"]}, {"number": 43371, "title": "whats the equivalent of torch.nn.Parameter() in TF ?", "body": "from my understanding `torch.tensor(5.5, requires_grad=True)` is equivalent to `tf.Variable(5.5, trainable=True)`\r\n\r\nhow about `torch.nn.Parameter(torch.zeros([1,1,1]))` ?", "comments": ["@turmeric-blend,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43370, "title": "Can lhlo_fuse_linalg support fusion like which in XLA ?", "body": "Hi , I found that lhlo_fuse_linalg can do some fusion at Linalg level, but it seems can not support fusion like which in XLA now.\r\nAbout fusion, have a design or completely plan at Linalg level ? \r\nThanks.", "comments": ["hi @pifon2a , maybe you can give me some details, thanks.", "@shanshanpt \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "@shanshanpt at the moment we are still relying on XLA to fuse the ops. We are relying on Linalg & ParallelLoops fusion for the ops that we know will fuse, e.g. output of XLA with LHLO fusion ops that already contain fusable ops inside. Of course, the goal is to have fusion passes in MLIR, but I am not sure if there are any concrete plans. \r\n\r\nYou might want to check Linalg-on-Tensors, since it supports fusion of ops bodies, not only tiling-and-fusion of surrounding loops like in Linalg-on-Buffers case. @nicolasvasilache ", "> @shanshanpt at the moment we are still relying on XLA to fuse the ops. We are relying on Linalg & ParallelLoops fusion for the ops that we know will fuse, e.g. output of XLA with LHLO fusion ops that already contain fusable ops inside. Of course, the goal is to have fusion passes in MLIR, but I am not sure if there are any concrete plans.\r\n> \r\n> You might want to check Linalg-on-Tensors, since it supports fusion of ops bodies, not only tiling-and-fusion of surrounding loops like in Linalg-on-Buffers case. @nicolasvasilache\r\n\r\nthanks @pifon2a ", "@shanshanpt Can we close this issue as it has been resolved. Thanks!"]}, {"number": 43368, "title": "Tim", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\n\n**System information**\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n- TensorFlow installed from (source or binary):\n- TensorFlow version:\n- Python version:\n- Installed using virtualenv? pip? conda?:\n- Bazel version (if compiling from source):\n- GCC/Compiler version (if compiling from source):\n- CUDA/cuDNN version:\n- GPU model and memory:\n\n\n\n**Describe the problem**\n\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\n\n\n**Any other info / logs**\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["@stiwe5,\r\nIn order to expedite the trouble-shooting process, could you please explain in detail the issue you are facing and fill in the issue template? Thanks!", "Seems spam"]}, {"number": 43367, "title": "Unable to build tensorflowlite.dll in Windows with FLEX delegate support - library limit of 65535 objects exceeded", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 build 19041.450\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.3\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.5.0\r\n- GCC/Compiler version (if compiling from source): Visual C++ 2019\r\n- CUDA/cuDNN version: No GPU support\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to build tensorflowlite.dll in Windows for integration with my C++ Visual Studio project. I can build it just fine without flex delegate support, but the exported model I'm trying to load uses several flex nodes, so the call to interpreter->AllocateTensors() fails. \r\n\r\nSo I added support for flex delegates and now I can't build the library. It keeps telling me I've exceeded the DLL limit of 65535 objects.\r\n\r\nI've made some research and apparently is a limitation of the PE header, but the only suggestion they have is to split the library into multiple, smaller libs. I've also tried to remove some dependencies and symbols but it hasn't worked so far. I'd really be thankful if someone has a workaround for this problem or can give me any guidance.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI'm compiling from \"rc2.3\" branch, but I've tried \"master\" before with the same result.\r\n\r\nI added --config=monolithic to my bazel build command and \"//tensorflow/lite/delegates/flex:delegate\" line to the \"deps\" section in the \"tensorflowlite\" target inside \"tensorflow/lite/BUILD\" file. \r\n\r\nThis is my command line:\r\n\r\n`bazel build --config=opt --config=monolithic --config=windows --config=v2 //tensorflow/lite:tensorflowlite`\r\n\r\nI've tried removing symbols modifying the EXCLUDE_RE reg expression in \"tools\\def_file_filter\\def_file_filter.py.tpl\" but apparently didn't work or wasn't enough.\r\n\r\n**Any other info / logs**\r\n\r\nThis is the full error. I get another error line complaining about multiple definition but that doesn't worry me as much since I don't remember getting it when I was trying to build from master.\r\n\r\n```\r\nERROR: D:/users/[...]/documents/projects/tensorflow/tensorflow/lite/BUILD:644:24: Linking of rule '//tensorflow/lite:tensorflowlite.dll' failed (Exit 1189): link.exe failed: error executing command\r\n  cd C:/users/[...]/_bazel_[...]/rwi2ipkq/execroot/org_tensorflow\r\n  SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/[...]/AppData/Local/Programs/Python/Python38-32/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/[...]/AppData/Local/Programs/Python/Python38-32/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\[...]\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_ENABLE_XLA=1\r\n    SET TMP=C:\\Users\\[...]\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.27.29110/bin/HostX64/x64/link.exe @bazel-out/x64_windows-opt/bin/tensorflow/lite/tensorflowlite.dll-2.params\r\nExecution platform: @local_execution_config_platform//:platform\r\nLINK : warning LNK4044: unrecognized option '/s'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lpthread'; ignored\r\ndelegate_only_runtime.lo.lib(delegate.obj) : error LNK2005: \"class std::unique_ptr<struct TfLiteDelegate,void (__cdecl*)(struct TfLiteDelegate *)> __cdecl tflite::AcquireFlexDelegate(void)\" (?AcquireFlexDelegate@tflite@@YA?AV?$unique_ptr@UTfLiteDelegate@@P6AXPEAU1@@Z@std@@XZ) already defined in framework_lib.lo.lib(interpreter_builder.obj)\r\nLINK : fatal error LNK1189: library limit of 65535 objects exceeded\r\nTarget //tensorflow/lite:tensorflowlite failed to build\r\nINFO: Elapsed time: 81.219s, Critical Path: 49.74s\r\nINFO: 1 process: 1 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["I'd really thank any help with a workaround, such as how to build the flex delegates part in a separate shared library and then inject them before calling allocate_tensors() ...", "Could you try to add \"--copt=/bigobj\" option to the blaze command?", "I tried, but it didn't work. Output error was exactly the same. My bazel command line now:\r\n\r\n`bazel build --config=opt --config=monolithic --config=windows --config=v2 --copt=/bigobj //tensorflow/lite:tensorflowlite`\r\n\r\nI think that option helps only for big .obj files, and our problem is a .dll with too many exports.\r\n\r\nI found this target in tensorflow/BUILD file, line 812 (I'm using r2.3 head, d71d3ce2520587b752e5d27b2d4a4ba8720e4bd5):\r\n\r\n```\r\ngenrule(\r\n    name = \"tensorflow_filtered_def_file\",\r\n    srcs = [\":tensorflow_def_file\"],\r\n    outs = [\"tensorflow_filtered_def_file.def\"],\r\n    cmd = select({\r\n        \"//tensorflow:windows\": \"\"\"\r\n              $(location @local_config_def_file_filter//:def_file_filter) \\\\\r\n              --input $(location :tensorflow_def_file) \\\\\r\n              --output $@\r\n          \"\"\",\r\n        \"//conditions:default\": \"touch $@\",  # Just a placeholder for Unix platforms\r\n    }),\r\n    tools = [\"@local_config_def_file_filter//:def_file_filter\"],\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n```\r\n\r\nIt seems it was specifically added to deal with this problem. The thing is, it is used for \"//tensorflow:tensorflow\" and \"//tensorflow:tensorflow_cc\" targets but not for \"//tensorflow/lite:tensorflowlite\".\r\n\r\nDoing some more investigation, I found that this rule generates a \"tensorflow_filtered_def_file.def\" in \"bazel-out\\x64_windows-opt\\bin\\tensorflow\" that in my machine weights about 7MB (WITHOUT flex delegate support, I wasn't using that option when I built the main tensorflow.dll file) compared to the 760MB of the \"tensorflowlite.dll.gen.def\" (WITH flex delegate).\r\n\r\nI guess if I could somehow add this filtered def file build target to the tensorflowlite target this could solve my problem, or put me in the right track to solve it. Any tips on how to do that are welcome, I'm kind of new to bazel.", "Did anyone get this to work?\r\n", "@raelimp Could you please try on the latest TF v2.6.0 and let us know if the issue still persists? Thank you!", "There is one workaround.\r\n\r\nTFLite has a logic to enable Flex delegate by loading Python library of TensorFlow.\r\nTo use Flex delegate, you just need normal TensorFlow Lite build with your program.\r\nFor Windows, you need the `_pywrap_tensorflow_internal.pyd` file with your executable.\r\nWhen the TFLite runtime is initialized, FlexDelegate will be enabled if the runtime can find the file.\r\n\r\nYou can file Python PIPs from the link. https://www.tensorflow.org/install/pip\r\nDownload the same version of TensorFlow WHL file and unzip it. You can find the shared library.", "@sushreebarsa I'm trying to compile the Windows C++ .dll for TF v2.6.0 with Flex delegates and receive the same error:\r\n\r\n LINK : fatal error LNK1189: library limit of 65535 objects exceeded \r\n\r\nThe .dll compiles with no issues without the flex delegate dependency added to the BUILD file. \r\n\r\n@terryheo If I add _pywrap_tensorflow_internal.pyd to the directory with my C++ executable flex delegates will automatically be enabled?", "Instead of building a single TensorFlow Lite library with Flex enabled, you can build another shared library for Flex delegate.\r\n\r\n1. Sync repo to have https://github.com/tensorflow/tensorflow/commit/ce3fed2f30c050123567d796d632d68d97940047\r\n2. Build `tensorflowlite_flex.dll` with `bazel build -c opt tensorflow/lite/delegates/flex:tensorflowlite_flex`\r\n3. You will have `tensorflowlite_flex.dll` under `bazel-bin/tensorflow/lite/delegates/flex`\r\n4. Use the dll with your application. If it's linked with your application, FlexDelegate will be loaded by [this logic](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/interpreter_builder.cc#L173).\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43367\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43367\">No</a>\n", "@terryheo I followed steps 1-3 of the workaround, added both `tensorflowlite.dll.if.lib` and `tensorflowlite_flex.dll.if.lib` as a Linker dependency and placed both `tensorflowlite.dll` and `tensorflowlite_flex.dll` in the application folder. \r\n\r\nI still receive the error message \r\n\r\n> ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n\r\nAm I missing anything I should be doing?"]}, {"number": 43366, "title": "Add Function.experimental_get_tracing_count fixing #37323", "body": "Issue #37323 requested a public API to know how many times a function was traced.\r\n\r\nThis PR turns the previously private `_get_tracing_count` method into a public API, documents it properly, and adds unit tests to check if it is working correctly.\r\n\r\nThe public API has been named `experimental_get_tracing_count` with the `experimental_` prefix to better control the adoption of the API.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43366) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43366) for more info**.\n\n<!-- ok -->", "@mdanatg should I do something about the CI checks failing? I see that on the main branch some commits fail some of them as well. So maybe I just forked the branch when those were already failing?", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 43365, "title": "Define __slots__ for object wrappers", "body": "This PR defines [`__slots__`](https://docs.python.org/3/reference/datamodel.html#slots) for the small classes which prevents the creation of an unnecessary `__dict__`  per class which can save a bit of memory since TensorFlow uses these wrappers in many places internally.", "comments": ["Sorry, I missed that some identity wrappers need to support weak references. I fixed that in the latest commit."]}, {"number": 43364, "title": "C++ compilation of rule '@nccl_archive//:device_lib' failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution Arch Linux\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): clang 10.0.1\r\n- CUDA/cuDNN version: 11.0/8.0.2\r\n- GPU model and memory: GTX1080 32Gib RAM\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nBuild failed with the following error present:\r\n\r\nERROR: /d/tensorflow/tensorflow/python/tools/BUILD:226:1 C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1)\r\n\r\n**Provide the exact sequence of commands/steps that you executed before running into the problem**\r\n\r\n/home/erina/Desktop/bazel-3.1.0-linux-x86_64 build --config=opt -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**Any other info/logs**\r\n\r\nThe full error is as follows:\r\n\r\n```log\r\nERROR: /home/erina/.cache/bazel/_bazel_erina/e4985779bd7c6b512f3eaa8e37e6fd2a/external/nccl_archive/BUILD.bazel:53:1: C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1)\r\nclang: warning: Unknown CUDA version 11.0. Assuming the latest supported version 10.1 [-Wunknown-cuda-version]\r\nclang: warning: argument unused during compilation: '-Xcuda-fatbinary=--compress-all' [-Wunused-command-line-argument]\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:446:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/bits/strings_fortified.h:23:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bcopy (const void *__src, void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:38:13: note: previous declaration is here\r\nextern void bcopy (const void *__src, void *__dest, size_t __n)\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:446:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/bits/strings_fortified.h:29:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:42:13: note: previous declaration is here\r\nextern void bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:519:\r\n/usr/include/bits/string_fortified.h:31:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:43:14: note: previous declaration is here\r\nextern void *memcpy (void *__restrict __dest, const void *__restrict __src,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:519:\r\n/usr/include/bits/string_fortified.h:38:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memmove (void *__dest, const void *__src, size_t __len))\r\n       ^\r\n/usr/include/string.h:47:14: note: previous declaration is here\r\nextern void *memmove (void *__dest, const void *__src, size_t __n)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:519:\r\n/usr/include/bits/string_fortified.h:45:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (mempcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:384:14: note: previous declaration is here\r\nextern void *mempcpy (void *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:519:\r\n/usr/include/bits/string_fortified.h:59:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memset (void *__dest, int __ch, size_t __len))\r\n       ^\r\n/usr/include/string.h:61:14: note: previous declaration is here\r\nextern void *memset (void *__s, int __c, size_t __n) __THROW __nonnull ((1));\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:519:\r\n/usr/include/bits/string_fortified.h:81:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (explicit_bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/string.h:450:13: note: previous declaration is here\r\nextern void explicit_bzero (void *__s, size_t __n) __THROW __nonnull ((1))\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:519:\r\n/usr/include/bits/string_fortified.h:88:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:125:14: note: previous declaration is here\r\nextern char *strcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:519:\r\n/usr/include/bits/string_fortified.h:95:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:475:14: note: previous declaration is here\r\nextern char *stpcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:519:\r\n/usr/include/bits/string_fortified.h:103:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncpy (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:128:14: note: previous declaration is here\r\nextern char *strncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:519:\r\n/usr/include/bits/string_fortified.h:117:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpncpy (char *__dest, const char *__src, size_t __n))\r\n       ^\r\n/usr/include/string.h:483:14: note: previous declaration is here\r\nextern char *stpncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:519:\r\n/usr/include/bits/string_fortified.h:127:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcat (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:133:14: note: previous declaration is here\r\nextern char *strcat (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/clang/10.0.1/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:519:\r\n/usr/include/bits/string_fortified.h:134:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncat (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:136:14: note: previous declaration is here\r\nextern char *strncat (char *__restrict __dest, const char *__restrict __src,\r\n             ^\r\n13 errors generated when compiling for sm_61.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /d/tensorflow/tensorflow/python/tools/BUILD:226:1 C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1)\r\nINFO: Elapsed time: 1867.652s, Critical Path: 127.88s\r\nINFO: 9857 processes: 9857 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Generally CUDA 11 requires Tensorflow master branch. I see you are on `TensorFlow version: 2.3.0`", "Oh, I'll try to switch to the master branch. Thanks! No wonder it's the device_lib that failed!", "It failed again with identical errors. How should I solve this?", "Do you mean that you still have `clang: warning: Unknown CUDA version 11.0` with master?", "Noe really, the device_lib failed again. I'd try to clone the repo again.", "@2403772980ygy \r\n\r\nCan you try with these tested build configurations from [here](https://www.tensorflow.org/install/source#gpu) and see if the problem still persists. Thanks!", "please wait some time, I need to wait for CUDA 10.01......", "Arch Linux seems to miss multiple Cuda version support, I'll use a docker image of ubuntu to install Cuda.", "Do I need --config=cuda after running configuration? I'm afraid I'm building it wrong......", "You find the command at https://www.tensorflow.org/install/source?hl=en", "It successed with docker build, closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43364\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43364\">No</a>\n", "I see the same error while building `//tensorflow/lite:libtensorflowlite.so`\r\n\r\nSystem information\r\n\r\n    OS Platform and Distribution: CentOS 8\r\n    TensorFlow installed from (source or binary): source\r\n    TensorFlow version: master branch (63c0cfbb3c56f857ab9247a1154e11433a80f8e1)\r\n    Python version: 3.8.2\r\n    Bazel version (if compiling from source): 3.1.0\r\n    GCC/Compiler version (if compiling from source): Tried with both clang 10.0.1 and the LLVM version encoded in the repo (50df5f24dc333e912f6eb8500ef84e648d43af93)\r\n    CUDA/cuDNN version: 10.1/7.6\r\n    GPU model and memory: GeForce GTX 1050 Ti (CUDA Version: 11.1) / 32G\r\n\r\nThe error happens while building //tensorflow/core/kernels/rnn:gru_ops_gpu\r\n\r\nI am using LLVM to compile cuda. I suspect it is due to this line in `llvmpath/lib/clang/12.0.0/include/__clang_cuda_runtime_wrapper.h`:\r\n```\r\n// __THROW is redefined to be empty by device_functions_decls.h in CUDA. Clang's\r\n// counterpart does not do it, so we need to make it empty here to keep\r\n// following CUDA includes happy.\r\n#undef __THROW\r\n#define __THROW\r\n```\r\n\r\nThis makes the exception specification in the libc be inconsistent for `__THROW` and `__NTH` macros defined in `/usr/include/sys/cdefs.h`.\r\n\r\nI am not sure if the issue is in llvm, tensorflow or my build setup but if anyone has any ideas I would really appreciate it. I have opened a ticket in the LLVM bug tracker:\r\nhttps://bugs.llvm.org/show_bug.cgi?id=47869\r\n\r\nIn the meantime I am going to re-defined the `__NTH` macro in `__clang_cuda_runtime_wrapper.h` to also be empty:\r\n```\r\n#undef __THROW\r\n#define __THROW\r\n#undef __NTH\r\n#define __NTH(fct) fct\r\n#undef __NTHNL\r\n#define __NTHNL(fct) fct\r\n```"]}, {"number": 43363, "title": "Module 'tensorflow' has no attribute 'get_default_session' ", "body": "Hi there,\r\ni got the error: if tf.get_default_session() is not None:\r\nAttributeError: module 'tensorflow' has no attribute 'get_default_session' \r\n\r\nsearched similar issues online, it was all related to the versions of the packages or python i have installed, but i have tried all the suggestions with no luck, just posted here, wondering if i can please get some help.. \r\n\r\npython version  3.7.8, \r\ntensorflow             2.2.0\r\ntensorflow-estimator   2.2.0\r\nKeras: 2.1.4 \r\n\r\nThank you so much", "comments": ["https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_default_session", "@lxin618,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using. \r\n\r\nAlso, please check @bhack's comment and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43361, "title": "tensorflow import error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): anaconda\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: conda\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am getting this error when importing tensorflow. I don't have gpu. And my system is not gpu enabled. But still I am getting this error. It says \" Ignore above cudart dlerror if you do not have a GPU set up on your machine.\" Please help to get rid of this and tell me how ignore this.\r\n\r\n2020-09-19 12:27:43.405252: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-09-19 12:27:43.413500: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\nAny help is appreciated :)\r\n", "comments": ["If you are on a CPU only machine you could use `tensorflow-cpu`", "Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43361\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43361\">No</a>\n"]}, {"number": 43360, "title": "TFLITE not enable RUY AVX runtime path ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.3.0\r\n- Tensorflow Lite version:  https://github.com/google/ruy/archive/34ea9f4993955fa1ff4eb58e504421806b7f2e8f.zip\r\n\r\n**Describe the current behavior**\r\nTensorflow Lite build with RUY, even so RUY will detect AVX feature: \r\n```\r\nconstexpr Path kDefaultPaths = Path::kStandardCpp | kDefaultArchPaths;\r\n```\r\nBut TFLITE not set the runtime to use with this API: https://github.com/google/ruy/blob/be065e42dd898f565c3e439b70957debb28dfa34/ruy/ctx.cc#L59-L61\r\n\r\nAnd thus RUY can only use the kStandardCpp PATH.\r\n\r\n**Describe the expected behavior**\r\nTFLITE should set the AVX runtime PATH and RUY can use it when detected.\r\n\r\nCommand to build tflite:\r\n```\r\nmake SHELL=/bin/bash BUILD_WITH_NNAPI=false BUILD_WITH_RUY=true -C /home/lesliefang/tflite/tensorflow_test/tensorflow-2.3.0 -f tensorflow/lite/tools/make/Makefile -j 4\r\n```\r\n", "comments": ["Hi @talumbau, are you working on the RUY AVX PATH feature enabling? Could you help to comment on this?"]}, {"number": 43359, "title": "micro_speech.bin file creation fails even if compiling is successful", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.15.2\r\n- TensorFlow installed from: source \r\n- Tensorflow version (commit SHA if source): 19 September 2020\r\n- Target platform: Sparkfun Edge\r\n\r\n**Describe the problem**\r\n\r\nI build micro_speech project:\r\ngmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge  micro_speech_bin\r\n\r\nlast line of the build:\r\ntensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/arm-none-eabi-objcopy tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech.bin -O binary\r\n\r\nThe result bin file in tensorflow-master/tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech.bin file is only 64 bytes which cannot correct. It should be about 250KB.\r\n\r\nI have GNU Make 4.2.1", "comments": ["This was a regression that slipped through our CI system.\r\n\r\nb3bacdc0f4f784769e7d8d196cbe3c654cf40b7a fixed this issue and ff9f5aab8b03ed3e36b4512b4c3fbd3e3bb29cd0 makes sure that a similar regression will be caught before it is merged.\r\n\r\nI'm closing the current issue but please reopen this or make a new issue if things do not work for you from tip of tree.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43359\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43359\">No</a>\n"]}, {"number": 43358, "title": "Patch for TF 2.3.1", "body": "Contains multiple cherry-picks.", "comments": []}, {"number": 43357, "title": "Patch for TF 2.2.1", "body": "Contains multiple cherry-picks.", "comments": []}, {"number": 43356, "title": "Patch for TF 2.1.2", "body": "Contains multiple cherry-picks.", "comments": []}]