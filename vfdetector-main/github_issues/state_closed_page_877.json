[{"number": 27181, "title": "[TF 2.0 alpha] Tutorial \"Using TFRecords and tf.Example\" can't run on GPU", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: Tensorflow 2.0 alpha\r\n- Doc Link: [Using TFRecords and tf.Example](https://www.tensorflow.org/alpha/tutorials/load_data/tf_records)\r\n\r\n\r\n**Describe the documentation issue**\r\nI don't know why this tutorial only works for tensorflow CPU version.\r\nIf I run on GPU I will have an error in the cell:\r\n`tf_serialize_example(f0,f1,f2,f3)`\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-14-406ee79a7f52> in <module>\r\n----> 1 tf_serialize_example(f0,f1,f2,f3)\r\n\r\n<ipython-input-13-3eb13f28c327> in tf_serialize_example(f0, f1, f2, f3)\r\n      3     serialize_example,\r\n      4     (f0,f1,f2,f3),  # pass these args to the above function.\r\n----> 5     tf.string)      # the return type is `tf.string`.\r\n      6   return tf.reshape(tf_string, ()) # The result is a scalar\r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py in eager_py_func(func, inp, Tout, name)\r\n    387     if `func` returns None.\r\n    388   \"\"\"\r\n--> 389   return _internal_py_func(func=func, inp=inp, Tout=Tout, eager=True, name=name)\r\n    390 \r\n    391 \r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py in _internal_py_func(func, inp, Tout, stateful, eager, is_grad_func, name)\r\n    276   if eager:\r\n    277     result = gen_script_ops.eager_py_func(\r\n--> 278         input=inp, token=token, Tout=Tout, name=name)\r\n    279   else:\r\n    280     if stateful:\r\n\r\nD:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_script_ops.py in eager_py_func(input, token, Tout, name)\r\n     64       else:\r\n     65         message = e.message\r\n---> 66       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n     67   # Add nodes to the TensorFlow graph.\r\n     68   token = _execute.make_str(token, \"token\")\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nUnknownError: RuntimeError: Error copying tensor to device: CPU:0. Can't copy 37 bytes of a tensor into another with 32 bytes buffer.\r\nTraceback (most recent call last):\r\n\r\n  File \"D:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 205, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"D:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 107, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"<ipython-input-7-60a59d21d73e>\", line 10, in serialize_example\r\n    'feature2': _bytes_feature(feature2),\r\n\r\n  File \"<ipython-input-3-8047e5ac1b09>\", line 7, in _bytes_feature\r\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\r\n\r\n  File \"D:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 732, in numpy\r\n    return self._cpu_nograd()._numpy()  # pylint: disable=protected-access\r\n\r\n  File \"D:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 899, in _cpu_nograd\r\n    return self._copy_nograd(context.context(), \"CPU:0\")\r\n\r\n  File \"D:\\Anaconda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 847, in _copy_nograd\r\n    new_tensor = self._copy_to_device(context=ctx._handle, device=device_name)\r\n\r\nRuntimeError: Error copying tensor to device: CPU:0. Can't copy 37 bytes of a tensor into another with 32 bytes buffer.\r\n\r\n [Op:EagerPyFunc]\r\n```\r\n\r\nDoes it have a specific thing only for CPU? \r\n\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@shaolinkhoa The tutorial works fine when executed on cpu as well as gpu. Can you please try using [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true) and confirm? Thanks!", "@ymodak Yes, I already tested on the google colab.\r\nCurrently, the tutorial is using tensorflow for cpu as you can see in the code `install tensorflow==2.0.0-alpha0` so google colab runs perfectly on cpu.\r\nIf I change to `!pip install tensorflow-gpu==2.0.0-alpha0` ( I also change the setting to GPU) then I get the error at this line `tf_serialize_example(f0,f1,f2,f3)`\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-14-406ee79a7f52> in <module>()\r\n----> 1 tf_serialize_example(f0,f1,f2,f3)\r\n\r\n<ipython-input-13-3eb13f28c327> in tf_serialize_example(f0, f1, f2, f3)\r\n      3     serialize_example,\r\n      4     (f0,f1,f2,f3),  # pass these args to the above function.\r\n----> 5     tf.string)      # the return type is `tf.string`.\r\n      6   return tf.reshape(tf_string, ()) # The result is a scalar\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py in eager_py_func(func, inp, Tout, name)\r\n    387     if `func` returns None.\r\n    388   \"\"\"\r\n--> 389   return _internal_py_func(func=func, inp=inp, Tout=Tout, eager=True, name=name)\r\n    390 \r\n    391 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py in _internal_py_func(func, inp, Tout, stateful, eager, is_grad_func, name)\r\n    276   if eager:\r\n    277     result = gen_script_ops.eager_py_func(\r\n--> 278         input=inp, token=token, Tout=Tout, name=name)\r\n    279   else:\r\n    280     if stateful:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_script_ops.py in eager_py_func(input, token, Tout, name)\r\n     63       else:\r\n     64         message = e.message\r\n---> 65       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n     66   # Add nodes to the TensorFlow graph.\r\n     67   token = _execute.make_str(token, \"token\")\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnknownError: RuntimeError: Error copying tensor to device: CPU:0. Can't copy 15 bytes of a tensor into another with 8 bytes buffer.\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 205, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 107, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"<ipython-input-7-60a59d21d73e>\", line 10, in serialize_example\r\n    'feature2': _bytes_feature(feature2),\r\n\r\n  File \"<ipython-input-3-56a99c09a572>\", line 5, in _bytes_feature\r\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 732, in numpy\r\n    return self._cpu_nograd()._numpy()  # pylint: disable=protected-access\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 899, in _cpu_nograd\r\n    return self._copy_nograd(context.context(), \"CPU:0\")\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 847, in _copy_nograd\r\n    new_tensor = self._copy_to_device(context=ctx._handle, device=device_name)\r\n\r\nRuntimeError: Error copying tensor to device: CPU:0. Can't copy 15 bytes of a tensor into another with 8 bytes buffer.\r\n\r\n [Op:EagerPyFunc]\r\n```", "@shaolinkhoa Apologies for the delay in response. Thanks for trying TF 2.0 alpha. I was able to reproduce reported behavior using TF 2.0 alpha gpu in google colab gpu environment.\r\n@Ayush517 Can you please take a look? Thanks!", "@ymodak I was also able to reproduce the issue. \r\nI got almost the same error but the number of bytes to be copied was different for the same code. According to @shaolinkhoa's error there are 15 bytes to be copied whereas my error shows 11 bytes to be copied.\r\nMine showed this -\r\n\"RuntimeError: Error copying tensor to device: CPU:0. Can't copy 11 bytes of a tensor into another with 8 bytes buffer.\"\r\nLooking into it.\r\nIn the meantime, I found another one like this #26951", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@tensorflowbutler I just tested on google colab with tensorflow-gpu. \r\nIt works fine. \r\nWhen did you guys fix this? \r\nDo you know what is the problem?", "Closing this issue since its resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27181\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27181\">No</a>\n", "I also have the same problem in tensorflow-gpu==2.0.0-beta\r\n\r\n---------------------------------------------------------------------------\r\n\r\nUnknownError                              Traceback (most recent call last)\r\n\r\n<ipython-input-24-0d21f525f9e6> in <module>\r\n----> 1 tf_serialize_example(f0,f1,f2,f3)\r\n      2 #tf.__version__\r\n\r\n<ipython-input-20-674875c41404> in tf_serialize_example(f0, f1, f2, f3)\r\n      2   tf_string = tf.py_function(serialize_example,\r\n      3                              (f0,f1,f2,f3),  # pass these args to the above function.\r\n----> 4                              tf.string)      # the return type is `tf.string`.\r\n      5   return tf.reshape(tf_string, ()) # The result is a scalar\r\n      6 \r\n\r\n~/tf20/lib/python3.5/site-packages/tensorflow/python/ops/script_ops.py in eager_py_func(func, inp, Tout, name)\r\n    390     if `func` returns None.\r\n    391   \"\"\"\r\n--> 392   return _internal_py_func(func=func, inp=inp, Tout=Tout, eager=True, name=name)\r\n    393 \r\n    394 \r\n\r\n~/tf20/lib/python3.5/site-packages/tensorflow/python/ops/script_ops.py in _internal_py_func(func, inp, Tout, stateful, eager, is_grad_func, name)\r\n    279   if eager:\r\n    280     result = gen_script_ops.eager_py_func(\r\n--> 281         input=inp, token=token, Tout=Tout, name=name)\r\n    282   else:\r\n    283     if stateful:\r\n\r\n~/tf20/lib/python3.5/site-packages/tensorflow/python/ops/gen_script_ops.py in eager_py_func(input, token, Tout, name)\r\n     63       else:\r\n     64         message = e.message\r\n---> 65       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n     66   # Add nodes to the TensorFlow graph.\r\n     67   token = _execute.make_str(token, \"token\")\r\n\r\n/usr/lib/python3/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnknownError: RuntimeError: Error copying tensor to device: CPU:0. Can't copy 11 bytes of a tensor into another with 8 bytes buffer.\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/jeonghwan/tf20/lib/python3.5/site-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"/home/jeonghwan/tf20/lib/python3.5/site-packages/tensorflow/python/ops/script_ops.py\", line 121, in __call__\r\n    self._convert(ret, dtype=self._out_dtypes[0]))\r\n\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n\r\n  File \"/home/jeonghwan/tf20/lib/python3.5/site-packages/tensorflow/python/eager/context.py\", line 586, in _mode\r\n    yield\r\n\r\n  File \"/home/jeonghwan/tf20/lib/python3.5/site-packages/tensorflow/python/ops/script_ops.py\", line 109, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"<ipython-input-18-aca852a2de23>\", line 10, in serialize_example\r\n    'feature2': _bytes_feature(feature2),\r\n\r\n  File \"<ipython-input-15-99e749525cc0>\", line 4, in _bytes_feature\r\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\r\n\r\n  File \"/home/jeonghwan/tf20/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 769, in numpy\r\n    maybe_arr = self._cpu_nograd()._numpy()  # pylint: disable=protected-access\r\n\r\n  File \"/home/jeonghwan/tf20/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 952, in _cpu_nograd\r\n    return self._copy_nograd(context.context(), \"CPU:0\")\r\n\r\n  File \"/home/jeonghwan/tf20/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 895, in _copy_nograd\r\n    new_tensor = self._copy_to_device(context=ctx._handle, device=device_name)\r\n\r\nRuntimeError: Error copying tensor to device: CPU:0. Can't copy 11 bytes of a tensor into another with 8 bytes buffer.\r\n\r\n [Op:EagerPyFunc]"]}, {"number": 27180, "title": "[TF 2.0 alpha] Fix \"Load images with tf.data\" to run on window", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: Tensorflow 2.0 alpha\r\n- Doc Link: Tutorial [Load images with tf.data](https://www.tensorflow.org/alpha/tutorials/load_data/images)\r\n\r\n**Describe the documentation issue**\r\nTutorial [Load images with tf.data](https://www.tensorflow.org/alpha/tutorials/load_data/images) will have an error if it runs on windows because of the backslash of the path in this cell.\r\n```\r\nimport IPython.display as display\r\n\r\ndef caption_image(image_path):\r\n    image_rel = pathlib.Path(image_path).relative_to(data_root)\r\n    return \"Image (CC BY 2.0) \" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1])\r\n```\r\n\r\nSo I fixed it by replacing the backslash with the slash.\r\nimport IPython.display as display\r\n\r\n```\r\ndef caption_image(image_path):\r\n    image_rel = pathlib.Path(image_path).relative_to(data_root)\r\n    image_rel = str(image_rel)\r\n    image_rel=image_rel.replace('\\\\','/')\r\n    str_return = \"Image (CC BY 2.0) \" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1]) \r\n    return str_return\r\n\r\n```\r\n\r\n\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["`Pathlib.Path` should create either a `WindowsPath` or a `PosixPath` based on your OS (takes care of slashes and backslashes under the hood). \r\n\r\nCan you share some information about your setup, like what python version you are running etc...\r\n", "Sure. I'm using Anaconda, python 3.6.8 on win10. I run the code on Jupyter notebook 5.7.4.\r\nPathlib takes care of the path but those picture's paths in \"LICENSE.txt\" are using the backslash.\r\nEx:\r\n```\r\nimport os\r\nattributions = (data_root/\"LICENSE.txt\").open(encoding='utf-8').readlines()[4:]\r\nattributions = [line.split(' CC-BY') for line in attributions]\r\nattributions = dict(attributions)\r\n```\r\nThe dict `attributions `has the key `daisy/7568630428_8cf0fc16ff_n.jpg`\r\n\r\nWhen we use  \r\n`str_return = \"Image (CC BY 2.0) \" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1])` \r\nto extract the author name of the picture by the key, it will get error because the **image_path** is still the `WindowsPath` `daisy\\7568630428_8cf0fc16ff_n.jpg` after we converted it to string.\r\nSo `attributions ` can't find the key `daisy\\7568630428_8cf0fc16ff_n.jpg` because the **image_path** is still using the backslash.\r\n\r\n\r\n\r\n", "@Ayush517 Can you please take a look at this issue? Thanks!", "This tutorial uses `os.path.sep` now: https://www.tensorflow.org/tutorials/load_data/images"]}, {"number": 27179, "title": "lite/toco/quantize: add missing space", "body": "/cc @dansitu", "comments": []}, {"number": 27178, "title": "tflite/micro/README: fix binary paths", "body": "/cc @dansitu", "comments": ["@petewarden Could you PTAL and approve.", "@petewarden, @MarkDaoust can you please review this PR ? "]}, {"number": 27177, "title": "TF Lite Updated activations.cc", "body": "Corrected log error in activations.cc", "comments": ["@renjie-liu Thanks for the review comments. I have updated accordingly. "]}, {"number": 27176, "title": "TensorflowLite Convert error", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.18\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, EXPAND_DIMS, FULLY_CONNECTED, MAX_POOL_2D, RESHAPE, SQUEEZE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: Merge, Switch.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["As the output suggests, you have to build a custom operator to add those ops. Please take a look at this [faq](https://www.tensorflow.org/lite/guide/faq#models_operations) to know more. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27175, "title": "Divnan", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27175) for more info**.\n\n<!-- need_sender_cla -->", "@shashvatshahi1998 please sign CLA", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27175) for more info**.\n\n<!-- ok -->", "Can you edit the title and add a description of what this is supposed to do, please? Will make reviewing much easier.", "Closing due to lack of activity. Please reopen with a more suggestive title and a description if you want to send these changes."]}, {"number": 27174, "title": "Logging functions modification #26143", "body": "[TF2.0] Error Logging for GradientTape  #26143 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27174) for more info**.\n\n<!-- need_sender_cla -->", "Please review it I have removed merge conflicts this time @alextp ", "@shashvatshahi1998 please sign CLA", "Please help me @gbaned  I have already signed it on march 8 . I have set already set my email and other info why it is still showing cla error", "@gbaned please review it,  now CLA is fine.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27174) for more info**.\n\n<!-- ok -->", "@alextp can you guide me , what should be there then if is_floating() is not the correct way.", "is_floating is the correct way but the tape itself cannot be floating. You need to look at the input tensors.", "So what we can do is we can take ant tensor from target and then check that whether that is floating or not. Like \r\nfor v in target:\r\n    if not is_floating(v):\r\n       raise ValueError(\"\")", "Yes\n\nOn Wed, Mar 27, 2019 at 9:51 AM Shashvat Chand Shahi <\nnotifications@github.com> wrote:\n\n> So what we can do is we can take ant tensor from target and then check\n> that whether that is floating or not. Like\n> for v in target:\n> if not is_floating(v):\n> raise ValueError(\"\")\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27174#issuecomment-477247691>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxeRgVKHFGCXjQfxFK1HMNqVO-UFWks5va6GpgaJpZM4cM3rg>\n> .\n>\n\n\n-- \n - Alex\n", "ok i will do that till tomorrow and create a pull request", "I have finally added what you said, you can review it now", "Ya I know he referenced me in the beginning of the comment that shashvatshahi have done some changes  so I am also adding some changes.", "@alextp is still there any problem or you can merge this??", "Please somebody reply about the pull request", "@superbobry please review my changes also", "what can I do to pass the ubuntu sanity test as it is required?", "@alextp done with changes\r\n", "@alextp please review", "@alextp please reply", "you mean that i have to shift the code added from line 929 to 950 completely.\r\n\r\nif output_gradients is not None:\r\n      output_gradients = [None if x is None else ops.convert_to_tensor(x)\r\n                          for x in nest.flatten(output_gradients)]\r\n\r\nbelow this???", "Yes\n\nOn Fri, Mar 29, 2019 at 9:54 AM Shashvat Chand Shahi <\nnotifications@github.com> wrote:\n\n> you mean that i have to shift the code added from line 929 to 950\n> completely.\n>\n> if output_gradients is not None:\n> output_gradients = [None if x is None else ops.convert_to_tensor(x)\n> for x in nest.flatten(output_gradients)]\n>\n> below this???\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27174#issuecomment-478070515>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxVu_QZILDq-xEwn9kzbEphC9PyeDks5vbkUmgaJpZM4cM3rg>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp  changed the location of code.", "Done with changes", "Please @alextp can we now merge this.", "Please @alextp review it once", "I approved but I like the approach in https://github.com/tensorflow/tensorflow/pull/27183 a little more (warnings being less intrusive than errors)", "Ok , by the way thanks for your constant guidance.", "@alextp what about checks", "What to do with sanity check for ubuntu,  can anybody please guide me?", "@rthadur what does ubuntu sanity test mean", "@alextp what to do about ubuntu sanity test, it is again failing.", "please reply @alextp ", "```\r\nFAIL: Found 1 non-whitelited pylint errors:\r\ntensorflow/python/eager/backprop.py:214: [C0330(bad-continuation), ] Wrong continued indentation (remove 1 space).\r\n\r\n```\r\n\r\nThe sanity error is available if you click through the link", "also import/copybara is also pending for more than 20 hours now", "@alextp what are these conflicting files", "@alextp what to do", "```\r\n    if not is_floating(v):\r\nNameError: name 'is_floating' is not defined\r\n```", "Please try to run the code in your PR before mailing it to us", "@alextp in backprop.py we have imported dtypes.py from tensorflow.python.framework  which contains is_floating function so why it is showing error.", "@shashvatshahi1998 it should be dtypes.is_floating then. But I think this issue has already been addressed by the PR which added the conflicting change, so I'm going to close this PR.\r\n\r\nThanks for the contribution, though!"]}, {"number": 27173, "title": "InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'import/Placeholder' with dtype uint8 and shape [1,30,300,3]", "body": "I try to convert a tensorflow model to tf-trt model ,but when running has errors:\r\nTraceback (most recent call last):\r\n  File \"./tranTFtoTRT.py\", line 27, in <module>\r\n    sess.run(output_node)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'import/Placeholder' with dtype uint8 and shape [1,30,300,3]\r\n\t [[node import/Placeholder (defined at ./tranTFtoTRT.py:26) ]]\r\n\r\nCaused by op u'import/Placeholder', defined at:\r\n  File \"./tranTFtoTRT.py\", line 26, in <module>\r\n    return_elements=['AttentionOcr_v1/predicted_chars:0'])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 442, in import_graph_def\r\n    _ProcessNewOps(graph)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 235, in _ProcessNewOps\r\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3433, in _add_new_tf_operations\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3325, in _create_op_from_tf_operation\r\n    ret = Operation(c_op, self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'import/Placeholder' with dtype uint8 and shape [1,30,300,3]\r\n\t [[node import/Placeholder (defined at ./tranTFtoTRT.py:26) ]]\r\n\r\nHere is my code:\r\n\r\n  1 #!/usr/bin/env python\r\n  2 # -*- coding: utf-8 -*-\r\n  3 \r\n  4 # Import TensorFlow and TensorRT\r\n  5 import tensorflow as tf\r\n  6 import tensorflow.contrib.tensorrt as trt\r\n  7 \r\n  8 # Inference with TF-TRT 'SavedModel' workflow :\r\n  9 graph = tf.Graph()\r\n 10 with graph.as_default():\r\n 11     with tf.Session() as sess:\r\n 12         # Create a TensorRT inference graph from a SavedModel:\r\n 13         trt_graph = trt.create_inference_graph(\r\n 14             input_graph_def=None,\r\n 15             outputs=None,\r\n 16             input_saved_model_dir=\"/workspace/models/ocrmodel\",\r\n 17             input_saved_model_tags=[\"serve\"],\r\n 18             max_batch_size=1,\r\n 19             #max_workspace_size_bytes=1 << 32,\r\n 20             max_workspace_size_bytes=1 << 8,\r\n 21             #precision_mode=\"FP32\")\r\n 22             precision_mode=\"INT8\")\r\n 23         # Import the TensorRT graph into a new graph and run:\r\n 24         output_node = tf.import_graph_def(\r\n 25             trt_graph,\r\n 26             return_elements=['AttentionOcr_v1/predicted_chars:0'])\r\n 27         sess.run(output_node)\r\n\r\nCan anyone help me!", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 27172, "title": "Bazel Build Error: Failed to build libtensorflow_inference.so", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No,it's during compiling\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:latest\r\n- Python version:2.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):0.23.2\r\n- GCC/Compiler version (if compiling from source):clang-1000.10.44.4\r\n\r\n\r\n**Describe the problem**\r\nI'm tring to build libtensorflow_inference.so using bazel following [this tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android) and encounter this error when running command \r\n~~~\r\nbazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n   --crosstool_top=//external:android/crosstool \\\r\n   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n   --cxxopt=-std=c++11 \\\r\n   --cpu=armeabi-v7a\r\n~~~\r\nError Message:\r\n~~~\r\nERROR: /private/var/tmp/_bazel_vigor/d97331ab44cbad00aa5fa19b76a95775/external/com_google_absl/absl/numeric/BUILD.bazel:25:1: C++ compilation of rule '@com_google_absl//absl/numeric:int128' failed (Exit 1)\r\nexternal/com_google_absl/absl/numeric/int128.cc:139:59: error: no member named 'trunc' in namespace 'std'; did you mean simply 'trunc'?\r\n  uint64_t w0 = static_cast<uint64_t>(static_cast<double>(std::trunc(v)));\r\n                                                          ^~~~~~~~~~\r\n                                                          trunc\r\nexternal/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:283:8: note: 'trunc' declared here\r\ndouble  trunc(double);\r\n        ^\r\nexternal/com_google_absl/absl/numeric/int128.cc:141:59: error: no member named 'trunc' in namespace 'std'; did you mean simply 'trunc'?\r\n  uint64_t w1 = static_cast<uint64_t>(static_cast<double>(std::trunc(v)));\r\n                                                          ^~~~~~~~~~\r\n                                                          trunc\r\nexternal/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:283:8: note: 'trunc' declared here\r\ndouble  trunc(double);\r\n        ^\r\nexternal/com_google_absl/absl/numeric/int128.cc:143:59: error: no member named 'trunc' in namespace 'std'; did you mean simply 'trunc'?\r\n  uint64_t w2 = static_cast<uint64_t>(static_cast<double>(std::trunc(v)));\r\n                                                          ^~~~~~~~~~\r\n                                                          trunc\r\nexternal/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:283:8: note: 'trunc' declared here\r\ndouble  trunc(double);\r\n        ^\r\n3 errors generated.\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 156.381s, Critical Path: 11.75s\r\nINFO: 602 processes: 602 local.\r\nFAILED: Build did NOT complete successfully\r\n~~~\r\nI'm using the WORKSPACE file as default, the android sdk and ndk version is like this:\r\n~~~\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 28,\r\n    build_tools_version = \"28.0.3\",\r\n    # Replace with path to Android SDK on your system\r\n    path = \"/Users/vigor/Library/Android/sdk\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n    path=\"/Users/vigor/Library/Android/ndk14/\",\r\n    api_level=14)\r\n\r\n~~~", "comments": ["Could you please upload compiled .jar and .so file so that I can use it directly?", "I have the same issues. The function \"trunc\" do not be available in android ndk std namespace but in math.h.  So, just update ndk version solves it.   ", "We're in the process of updating our docs, but we recommend NDK r17c. And if you hit issues there, you'll need to bump the NDK api_level to either 19 or 21."]}, {"number": 27171, "title": "Compute F1 score for multilabel classifier", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI am working with tf.contrib.metrics.f1_score in a metric function and call it using an estimator. I want to compute the F1 score for **multi label classifier** but this contrib function can not compute it. Please add this capability to this F1 ( computing macro and micro f1). I need it to compare the dev set and based on that keep the best model. I don't like to compute it using the sklearn\r\n\r\n**Will this change the current api? How?**\r\nIDK\r\n\r\n**Who will benefit with this feature?**\r\nEvery one who is trying to compute macro and micro f1 inside the Tensorflow function and not willing to use other python libraries. \r\n\r\n**Any Other info.**\r\nIf it is possible to compute macro f1 score in tensorflow using tf.contrib.metrics please let me know.\r\n\r\nThanks", "comments": ["@alextp there is no function like f1_score in tf.keras.metrics it is only in tf.contrib so where can we add functions for macros and micros, can you please guide me a little bit.", "Maybe this belongs in some other package like tensorflow/addons or tf-text?", "Where can we find macro f1 function? Is it anywhere in tf. ANYWHERE?! Is it developed or added or not?", "@MHDBST  f1_score is defined in https://github.com/tensorflow/tensorflow/blob/0fb46cf9139f1e8cf88195e0b8f555eb10d00b3b/tensorflow/contrib/metrics/python/metrics/classification.py#L70 However there is no feature to compute macro f1 score. PR #27171 addresses this feature. Thanks!", "Thanks @ymodak, this f1 function is not working for multiclass classification ( more than two labels). Is there any way to compute F1 for multi class classification?", "I will work on that surely\n\n\nOn Thu, 18 Apr 2019, 21:17 Mohadeseh Bastan, <notifications@github.com>\nwrote:\n\n> Thanks @ymodak <https://github.com/ymodak>, this f1 function is not\n> working for multiclass classification ( more than two labels). Is there any\n> way to compute F1 for multi class classification?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27171#issuecomment-484566192>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJLGBWGT4SCWGFS44TSEES3PRCJYVANCNFSM4HBS7LFQ>\n> .\n>\n", "I will work on that\r\n", "@MHDBST As a workaround, have you explored https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html. This can help you compute f1_score for binary as well as multi-class classification problems. Thanks!", "@ymodak This function is what I'm using now. But I think there should be a metric in Tensorflow like accuracy, or F1 ( for binary classification) to compute macro f1 (for multi class classification) independent from other libraries. "]}, {"number": 27170, "title": "Keras support for RaggedTensors", "body": "# System Information\r\n- TensorFlow version: 1.13.1 (issue present in 2.0 alpha\r\n- Are you willing to contribute it: Yes\r\n\r\n# Current State/Behaviour\r\n`tf.RaggedTensor`s do a fantastic job of masking their internal representation from the user, allowing them to be used as regular tensors in most contexts. This does not extend to keras models however. As an example:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input\r\nfrom tensorflow.keras.layers import Lambda\r\nfrom tensorflow.keras.models import Model\r\nif not hasattr(tf, 'nest'):\r\n    tf.nest = tf.contrib.framework.nest\r\n\r\n\r\nvalues = Input(shape=(10,), dtype=tf.float32)\r\n\r\n## non-ragged version\r\nindices = Input(shape=(5,), dtype=tf.int64)\r\ngathered = Lambda(lambda args: tf.gather(*args))([values, indices])\r\nModel(inputs=(values, indices), outputs=gathered)\r\n# works fine\r\n\r\n## ragged version\r\nindex_values = Input(shape=(), dtype=tf.int64)\r\nindex_row_splits = Input(shape=(), dtype=tf.int64)\r\n\r\nindices = tf.RaggedTensor.from_row_splits(index_values, index_row_splits)\r\ngathered = Lambda(lambda args: tf.gather(*args))([values, indices])\r\nModel(inputs=(values, indices), outputs=gathered)\r\n# raises\r\n# 1.13.1: ValueError: Input tensors to a Model must come from `tf.keras.Input`.\r\n# Received: tf.RaggedTensor(values=Tensor(\"input_3:0\", shape=(?,), dtype=int64),\r\n# row_splits=Tensor(\"input_4:0\", shape=(?,), dtype=int64))\r\n# (missing previous layer metadata).\r\n# 2.0: AttributeError: 'RaggedTensor' object has no attribute 'op'\r\n```\r\n\r\n# Workaround\r\nThe operations acting on the component tensors are fine. As a work around, one can do the following:\r\n\r\n```python\r\ndef ragged_tensor_from_row_lengths(values, row_lengths):\r\n    def components(args):\r\n        values, row_lengths = args\r\n        ragged = tf.RaggedTensor.from_row_lengths(values, row_lengths)\r\n        return ragged.values, ragged.row_splits\r\n\r\n    components = tf.keras.layers.Lambda(components)([values, row_lengths])\r\n    return tf.RaggedTensor.from_row_splits(*components)\r\n\r\n\r\ndef as_ragged_components(tensor):\r\n    if isinstance(tensor, tf.RaggedTensor):\r\n        return dict(values=tensor.values, row_splits=tensor.row_splits)\r\n    elif isinstance(tensor, (list, tuple)):\r\n        return tuple(as_ragged_components(t) for t in tensor)\r\n    elif isinstance(tensor, dict):\r\n        return {k: as_ragged_components(v) for k, v in tensor.items()}\r\n    else:\r\n        # leave unchanged\r\n        assert(isinstance(tensor, tf.Tensor))\r\n        return tensor\r\n\r\n\r\ndef as_ragged(components):\r\n    if isinstance(components, (list, tuple)):\r\n        return tuple(as_ragged(c) for c in components)\r\n    elif isinstance(components, dict):\r\n        if all(k in components for k in ('values', 'row_splits')):\r\n            return tf.RaggedTensor.from_row_splits(**components)\r\n        else:\r\n            return {k: as_ragged(v) for k, v in components.items()}\r\n    else:\r\n        assert(isinstance(components, tf.Tensor))\r\n        return components\r\n\r\n\r\ndef ragged_lambda(fn, args):\r\n    assert(isinstance(args, (list, tuple)))\r\n    if not any(isinstance(a, tf.RaggedTensor) for a in args):\r\n        out_components = tf.keras.layers.Lambda(fn)(args)\r\n    else:\r\n        components = as_ragged_components(args)\r\n        flat_args = tf.nest.flatten(components)\r\n\r\n        def actual_fn(flat_args):\r\n            args = tf.nest.pack_sequence_as(components, flat_args)\r\n            args = as_ragged(components)\r\n            out = fn(args)\r\n            return as_ragged_components(out)\r\n\r\n        out_components = tf.keras.layers.Lambda(actual_fn)(flat_args)\r\n    return as_ragged(out_components)\r\n\r\n\r\ngathered = ragged_lambda(\r\n    lambda args: tf.gather(*args), [values, indices])\r\n\r\ngathered_components = tf.nest.flatten(as_ragged_components(gathered))\r\nModel(\r\n    inputs=(values, index_values, index_row_splits),\r\n    outputs=gathered_components)\r\n```\r\n\r\nThis is, however, very convoluted, and largely defeats the purpose of having compound tensors like `RaggedTensor`s that should be able to be used transparently in place of regular tensors.\r\n\r\n# Change to API\r\nNothing, though will allow `RaggedTensor`s to be used in keras `Model`s.\r\n\r\n# Who will benefit\r\nPeople who appreciate clean code.", "comments": ["Thank you for your suggestion. We will have a look and revert on the same", "Seems that this issue is somewhere near https://github.com/tensorflow/tensorflow/issues/29113", "We would use this", "@fchollet \u2014 we\u2019re stuck with batch size 1 for a chem/biochem project because the molecules don\u2019t have consistent dimensions, and keras can\u2019t support ragged tensors. This addition would be a major game changer for NLP and bio/chem\r\n\r\nhow hard is it to support ragged tensors in keras? \r\n\r\n", "@bionicles you can safely pack/unpack ragged tensors inside `keras.layers.Lambda` functions and/or custom layer `call` functions (or outside if you stick to `rt.values` and `rt.row_splits`). It makes the code a bit uglier, but it shouldn't stop you from using them entirely. In 2.0 `Lambda` layers don't seem to be necessary so it's not quite as bad.\r\n\r\nThat said, it would be nice if this wasn't necessary at all...", "Support for feeding ragged tensors into Keras was added a couple weeks ago, in 7ed84ad8.  You can now use keras.layers.InputLayer(..., ragged=True) and keras.layers.Input(..., ragged=True).  Keras should already support passing ragged tensors between layers and outputting ragged tensors.  (E.g., output support was added in dda0e7ac.)", "@bionicles As @edloper mentioned, we're working to add support for RaggedTensors and SparseTensors as first-class citizens in the tf.keras ecosystem. Currently, you can pass them in (use ragged=True in your tf.keras.Input() call), pass them between layers, and return them from your model, but we're still working on adding support for RTs in standard Keras layers.", "@jackd @edloper @markomernick thanks, will check this out, sounds like progress ", "Ragged inputs to keras still look broken to me in the last two tf2 preview nightlies (if I'm using them right). Is this supposed to work?:\r\n```\r\nimport tensorflow as tf\r\n\r\nragged = True\r\ninput = tf.ragged.constant([[],[1],[1,2]]) if ragged else tf.constant([[0,0],[1,0],[1,2]])\r\nlabel = tf.constant([0,1,3])\r\n\r\nx = tf.keras.Input(shape=(None,), ragged=ragged)\r\ny = tf.reduce_sum(x,axis=1)\r\nmodel = tf.keras.Model(inputs=x,outputs=y)\r\n\r\nmodel.compile(loss='mae',optimizer='sgd')\r\nmodel.summary()\r\nmodel.fit(input, label)\r\n```\r\nFor me it works fine with `ragged = False`, otherwise it fails with this exception:\r\n```\r\ntensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: You must feed a value for placeholder tensor 'input_1/flat_values' with dtype float and shape [?]\r\n         [[{{node input_1/flat_values}}]]\r\n```", "In the latest tf2 nightly using this line works if the dtype of the constant is explicitly set to float32: \r\n```\r\ny = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x,axis=1))(x)\r\n```\r\nBut now it fails as soon as the rows in the input tensor exceeds the batch size:\r\n```\r\n2019-07-29 16:29:43.704191: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at strided_slice_op.cc:108 : Invalid argument: slice index 1 of dimension 0 out of bounds.\r\n```\r\nThere seems to be a matching TODO note: `# TODO(b/133517906): Add slicing support.`", "I was happy to see `tf.keras` finally managed to do away with the `Lambda` necessity. I was disappointed to see this hasn't reached `RaggedTensor`s yet.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass RaggedModelOutputTest(tf.test.TestCase):\r\n\r\n    def test_simple(self):\r\n        inp = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\r\n        output = tf.expand_dims(inp, axis=-1)\r\n        tf.keras.Model(inputs=inp, outputs=output)\r\n        # looks like we don't have to wrap everything in lambdas anymore!\r\n\r\n    def test_ragged_conversion_at_end(self):\r\n        inp = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\r\n        output = tf.RaggedTensor.from_tensor(inp)\r\n        tf.keras.Model(inputs=inp, outputs=output)  # <--- fails\r\n        # oh ffs\r\n\r\n    def test_ragged_wrapped_lambda(self):\r\n        inp = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\r\n        output = tf.keras.layers.Lambda(tf.RaggedTensor.from_tensor)(inp)\r\n        tf.keras.Model(inputs=inp, outputs=output)\r\n        # not the hardest work-around, but took me a while to work out...\r\n\r\n\r\ntf.test.main()\r\n```", "Is there a timeline for official RaggedTensor support throughout Keras?", "Weight saving also has serious issues with `1.15.0-dev20190821` (recent nightly build installed via `pip`).\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nflat = tf.keras.layers.Input(shape=(2,), ragged=True)\r\nrow_splits = tf.keras.layers.Input(shape=(), dtype=tf.int64)\r\nrt = tf.RaggedTensor.from_row_splits(flat, row_splits)\r\n\r\n# fails\r\n# ------------------------------------------------------------\r\n# x = tf.reduce_max(rt, axis=1)\r\n# ------------------------------------------------------------\r\n\r\n# fails\r\n# ------------------------------------------------------------\r\n# x = tf.keras.layers.Lambda(tf.reduce_max, arguments=dict(axis=1))(rt)\r\n# ------------------------------------------------------------\r\n\r\n# Succeeds\r\n# ------------------------------------------------------------\r\ndef ragged_max(args, axis):\r\n    flat_values, row_splits = args\r\n    return tf.reduce_max(\r\n        tf.RaggedTensor.from_row_splits(flat_values, row_splits), axis=axis)\r\n\r\n\r\nx = tf.keras.layers.Lambda(ragged_max, arguments=dict(axis=1))(\r\n    [flat, row_splits])\r\n# ------------------------------------------------------------\r\n\r\nmodel = tf.keras.models.Model(inputs=(flat, row_splits), outputs=x)\r\n\r\nmodel.save_weights('/tmp/ragged-model.h5')\r\nprint('Saved successfully')\r\n```\r\n\r\nFailing variants fail with `ValueError: Unable to create group (name already exists)`.", "Hey all,\r\n\r\nThank you for the reports! We are working on adding CompositeTensor support to more and more elements of the Keras framework, but as you can see there are some areas that aren't quite ready yet. I will route these sightings to the relevant folks.", "It would be nice to have loss/metric support as well :)", "Hi @jackd - we're working on losses and metrics as we speak!", "Hi, \r\nI saw in a [TFWorld conference](https://youtu.be/iu_OSAg5slY?t=954) that many Keras layers, including recurrent or embedding support RaggedTensors in TF2.0. The same info seemed to have been confirmed in this thread back in July:\r\n> Support for feeding ragged tensors into Keras was added a couple weeks ago, in [7ed84ad](https://github.com/tensorflow/tensorflow/commit/7ed84ad81476362eabca993a959f53dffcbdc51d). You can now use keras.layers.InputLayer(..., ragged=True) and keras.layers.Input(..., ragged=True). Keras should already support passing ragged tensors between layers and outputting ragged tensors.\r\n\r\nHowever, I find no documentation on how to use RaggedTensor in Keras layers, and it does not seem supported when I try it myself on tensorflow 2.0.0.\r\n\r\nHere's a minimal example with  a single embedding layer, which seem to work as expected with a Tensor, but to fail with a RaggedTensor.\r\nThe error is:\r\n`TypeError: Failed to convert object of type <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. Contents: tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"embedding_1/Cast_1:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"input_2/row_splits_1:0\", shape=(None,), dtype=int64)), row_splits=Tensor(\"input_2/row_splits_0:0\", shape=(None,), dtype=int64)). Consider casting elements to a supported type.\r\n`\r\n```python\r\nimport tensorflow as tf\r\n\r\nval_ragged = tf.ragged.constant([[1, 2, 3], [1, 2], [1, 2, 3, 4]])\r\n\r\nval_tensor = val_ragged.to_tensor()\r\n\r\ninputs = tf.keras.layers.Input(shape=(None, None,), ragged=False)\r\noutputs = tf.keras.layers.Embedding(5, 4)(inputs)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n# this model with normal tensor works\r\nprint(model(val_tensor))\r\n\r\ninputs_ragged = tf.keras.layers.Input(shape=(None, None,), ragged=True)\r\noutputs_ragged = tf.keras.layers.Embedding(5, 4)(inputs_ragged)\r\nmodel_ragged = tf.keras.Model(inputs=inputs_ragged, outputs=outputs_ragged)\r\n\r\n# this one with RaggedTensor doesn't\r\nprint(model_ragged(val_ragged))\r\n```\r\n", "@jcrousse \r\nin tf-nightly-build:2.1.0.dev20191111 & tf-nightly-gpu-build:2.1.0.dev20191111,\r\nyour code has no problem. It works.", "Hi, now it seem indeed possible to calculate the model output with Keras layers, but the Keras `fit` function is the next point of failure. \r\n\r\nThis is based on tf-nightly-build:2.1.0.dev20191203\r\n\r\n**Sample code that works with Tensors and fails with RaggedTensors:**\r\n```python\r\nimport tensorflow as tf\r\n\r\nval_ragged = tf.ragged.constant([[1, 2, 3], [1, 2], [1, 2, 3, 4]])\r\ny = tf.constant([[0.0, 1.0], [0.0, 1.0], [1.0, 0.0]])\r\nval_tensor = val_ragged.to_tensor()\r\n\r\n# Tensor version\r\ninputs = tf.keras.layers.Input(shape=(None, ), ragged=False)\r\nembeddings = tf.keras.layers.Embedding(5, 4)(inputs)\r\nlstm_out = tf.keras.layers.LSTM(8)(embeddings)\r\nclassifier = tf.keras.layers.Dense(\r\n        2, activation='softmax', name='classifier')(lstm_out)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=classifier)\r\nmodel.compile(\r\n    tf.keras.optimizers.Adam(0.01),\r\n    loss=tf.keras.losses.CategoricalCrossentropy())\r\n_ = model(val_tensor)\r\nmodel.fit(val_tensor, y, epochs=1)\r\n\r\n# Ragged Tensor version\r\ninputs = tf.keras.layers.Input(shape=(None, ), ragged=True)\r\nembeddings = tf.keras.layers.Embedding(5, 4)(inputs)\r\nlstm_out = tf.keras.layers.LSTM(8)(embeddings)\r\nclassifier = tf.keras.layers.Dense(\r\n        2, activation='softmax', name='classifier')(lstm_out)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=classifier)\r\nmodel.compile(\r\n    tf.keras.optimizers.Adam(0.01),\r\n    loss=tf.keras.losses.CategoricalCrossentropy())\r\n_ = model(val_ragged)  # WORKS UNTIL THIS POINT INCLUDED. We can calculate the model output\r\nmodel.fit(val_ragged, y, epochs=1)  # Fails here\r\n```\r\n\r\nAnd the Traceback:\r\n\r\n> Traceback (most recent call last):\r\n  File \"/home/john/miniconda3/envs/oreo/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\", line 330, in assert_same_structure\r\n    expand_composites)\r\nValueError: The two structures don't have the same nested structure.\r\n\r\n> \r\n> First structure: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([3, None]), tf.int32, 1, tf.int64)\r\n> \r\n> Second structure: type=RaggedTensor str=tf.RaggedTensor(values=Tensor(\"Placeholder:0\", shape=(None,), dtype=float32), row_splits=Tensor(\"Placeholder_1:0\", shape=(None,), dtype=int64))\r\n> \r\n> More specifically: Incompatible CompositeTensor TypeSpecs: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([3, None]), tf.int32, 1, tf.int64) vs. type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([None, None]), tf.float32, 1, tf.int64)\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"/home/john/miniconda3/envs/oreo/lib/python3.6/contextlib.py\", line 99, in __exit__\r\n>     self.gen.throw(type, value, traceback)\r\n>   File \"/home/john/miniconda3/envs/oreo/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 2805, in variable_creator_scope\r\n>     yield\r\n>   File \"/home/john/miniconda3/envs/oreo/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 235, in fit\r\n>     use_multiprocessing=use_multiprocessing)\r\n>   File \"/home/john/miniconda3/envs/oreo/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 593, in _process_training_inputs\r\n>     use_multiprocessing=use_multiprocessing)\r\n>   File \"/home/john/miniconda3/envs/oreo/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 646, in _process_inputs\r\n>     x, y, sample_weight=sample_weights)\r\n>   File \"/home/john/miniconda3/envs/oreo/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2350, in _standardize_user_data\r\n>     batch_size=batch_size)\r\n>   File \"/home/john/miniconda3/envs/oreo/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2414, in _standardize_tensors\r\n>     nest.assert_same_structure(a, b, expand_composites=True)\r\n>   File \"/home/john/miniconda3/envs/oreo/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\", line 337, in assert_same_structure\r\n>     % (str(e), str1, str2))\r\n> ValueError: The two structures don't have the same nested structure.\r\n> \r\n> First structure: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([3, None]), tf.int32, 1, tf.int64)\r\n> \r\n> Second structure: type=RaggedTensor str=tf.RaggedTensor(values=Tensor(\"Placeholder:0\", shape=(None,), dtype=float32), row_splits=Tensor(\"Placeholder_1:0\", shape=(None,), dtype=int64))\r\n> \r\n> More specifically: Incompatible CompositeTensor TypeSpecs: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([3, None]), tf.int32, 1, tf.int64) vs. type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([None, None]), tf.float32, 1, tf.int64)\r\n> Entire first structure:\r\n> .\r\n> Entire second structure:\r\n> .\r\n\r\nI  tried to also use a RaggedTensor for `y` but I get exactly the same error.", "Hi @jcrousse,\r\n\r\nIt looks like you're defining an integer ragged tensor and trying to pass that into a float input. (The error is not as helpful as it should be, I know - we're working on it.) The key is the first dtype in the 'more specifically' message: one has 'tf.int32' and the other 'tf.float32'.", "9 months on and keras [still doesn't recognize ragged tensors as tensors](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L983)...", "> Support for feeding ragged tensors into Keras was added a couple weeks ago, in 7ed84ad.  You can now use keras.layers.InputLayer(..., ragged=True) and keras.layers.Input(..., ragged=True).  Keras should already support passing ragged tensors between layers and outputting ragged tensors.  (E.g., output support was added in dda0e7a.)\r\n\r\nDoes tensorflow Keras Embedding layers support Ragged Tensors? \r\nI keep getting:\r\nTypeError: Failed to convert object of type <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. Contents: tf.RaggedTensor(values=Tensor(\"embedding/Cast_1:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"text_input/row_splits_0:0\", shape=(None,), dtype=int64)). Consider casting elements to a supported type.\r\n", "Also, will subclassing a `tf.keras.layers.Layer` allow passing ragged inputs?\r\n\r\nCurrently that's [not possible](https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/engine/base_layer.py#L738). \ud83d\ude15", "@nectario - This should be possible in nightly.\r\n\r\n@stefanondisponibile - It is possible, just set self._supports_ragged_inputs = True after you call super().__init__().", "Thank you @markomernick !", "How could I do this in subclassed models (which does not take tf.keras.layers.Inputs as input layers, if I'm not mistaken)?\r\n\r\nI tried setting self._supports_ragged_inputs=True in the subclassed model and subsequent layer classes but I keep getting the following error from training_utils.validate_input_types()\r\n\r\n    ValueError: Please provide as model inputs either a single array or a list of arrays. You passed: \r\n    inputs=<tf.RaggedTensor [[1, 2, 3], [1, 2, 3], [4, 5]]>\r\n", "uhm @sungholee95 , can you maybe provide a snippet? @markomernick solution worked for me.", "A simple model using one of the reduce operations (e.g., `tf.reduce_mean`) fails:\r\n```\r\nimport tensorflow as tf\r\n\r\ninputs = tf.keras.layers.Input(shape=(None,), ragged=True)\r\nembeddings = tf.keras.layers.Embedding(100, 4)(inputs)\r\npooled_embeddings = tf.reduce_mean(embeddings, axis=1)\r\nclassifier = tf.keras.layers.Dense(2, activation='softmax')(pooled_embeddings)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=classifier)\r\n```\r\nError:\r\n```\r\nInvalid argument:  You must feed a value for placeholder tensor 'Placeholder_1' with dtype int64 and shape [?]\r\n```\r\n\r\nIt's possible to workaround this issue by wrapping the operation into a custom layer. For the record, the following works fine:\r\n```\r\nimport tensorflow as tf\r\n\r\nclass CustomMean(tf.keras.layers.Layer):\r\n\r\n    def __init__(self, axis=None):\r\n        super(CustomMean, self).__init__()\r\n        self._supports_ragged_inputs = True\r\n        self.axis = axis\r\n\r\n    def call(self, inputs, **kwargs):\r\n        return tf.reduce_mean(inputs, axis=self.axis)\r\n\r\ninputs = tf.keras.layers.Input(shape=(None,), ragged=True)\r\nembeddings = tf.keras.layers.Embedding(100, 4)(inputs)\r\npooled_embeddings = CustomMean(axis=1)(embeddings)\r\nclassifier = tf.keras.layers.Dense(2, activation='softmax')(pooled_embeddings)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=classifier)\r\n```", "@stekiri I believe a simple `Lambda` layer would be sufficient.\r\n\r\n```python\r\npooled_embeddings = tf.keras.layers.Lambda(tf.reduce_mean, arguments=dict(axis=1))(embeddings)\r\n```", "@jackd, you're totally right. Thanks! \ud83d\udc4d It's way more convenient.", "Did you manage to save a model with ragged inputs as a keras model ? i'm hitting the following issue when reloading models with a custom layer or the lambda layer which takes a ragged inputs: \r\nhttps://github.com/tensorflow/tensorflow/issues/38998\r\n\r\nthanks ", "@edloper any prospect of having compound tensors output from keras layers automatically update their components' `_keras_history`? `TensorflowOpLayer`s are amazing, but they rarely work nicely with compound tensors. It would be awesome if the following could work without having ugly `Lambda`s everywhere.\r\n```python\r\ninp = tf.keras.Input((None,), ragged=True)\r\nmodel = tf.keras.Model(inp, tf.reduce_sum(inp, axis=1))\r\n```\r\n\r\nVice-versa would also be nice - have `create_keras_history` look at components of compound tensors that didn't come from layers themselves.\r\n```python\r\nvalues = tf.keras.Input(())\r\nrow_splits = tf.keras.Input((), dtype=tf.int64)\r\nrt = tf.RaggedTensor.from_row_splits(values, row_splits, validate=False)\r\nmodel = tf.keras.Model((values, row_splits), rt)\r\n```", "@edloper  I had been going through RaggedTensor documentation  and I could not find how to apply RaggedTensor.from_row_splits on batched values and row_splits, when shape of values is (None, time-steps, embedding size) and row_splits is (None, time-steps), i.e, when the row_splits op has to be applied to each row (forming values) in a batch. Could you please let me know if I have missed something and if there is a way to handle such a case?", "@jackd: Yes, I believe that this is being worked on.  @tomerk might have more details.\r\n\r\n@vyshnavigutta369: If I understand correctly, you have something like this:\r\n\r\n```\r\nbatch_size = 2\r\nembedding_size = 2\r\n\r\n# values.shape = [batch_size, (time_steps), embedding_size]\r\nvalues = tf.ragged.constant(\r\n    [[[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6]], [[7, 7], [8, 8], [9, 9], [0, 0]]],\r\n    ragged_rank=1)\r\n\r\n# row_splits.shape = [batch_size, (time_steps)]\r\nrow_splits = tf.ragged.constant([[0, 3, 4, 6], [0, 3, 4]], tf.int64)\r\n\r\n# want.shape = [batch_size, (time_steps), (rows_per_time_step), embedding_size]\r\nwant = tf.ragged.constant(\r\n    [[[[1, 1], [2, 2], [3, 3]], [[4, 4]], [[5, 5], [6, 6]]], \r\n     [[[7, 7], [8, 8], [9, 9]], [[0, 0]]]])``` \r\n```\r\n\r\nwhere `want` is the value you want to construct?  This is certainly possible to do, but we don't provide any direct way to do it.  I think the easiest approach would be to convert the `row_splits` to `row_lengths`, and then use those to construct the desired result:\r\n\r\n```\r\nrow_lengths = row_splits[:, 1:] - row_splits[:, :-1]\r\nwant = row_lengths.with_values(\r\n    tf.RaggedTensor.from_row_lengths(values.values, row_lengths.values))\r\n```\r\n\r\nAnother option would be to use `tf.map_fn`, but I suspect that it would be a little slower:\r\n\r\n```\r\nwant = tf.map_fn(\r\n    lambda x: tf.RaggedTensor.from_row_splits(x[0], x[1]),\r\n    (values, row_splits),\r\n    fn_output_signature=tf.RaggedTensorSpec(ragged_rank=1, dtype=tf.int32))\r\n```\r\n", "@edloper This is exactly what I wanted. Thanks a lot!! =D", "Hi @edloper . In build model, when using tf.ragged.boolean_mask(data,mask)[:,-2:].to_tensor() in tf.keras.layer.concatenate at a later step, was getting this error \r\n\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nTraceback (most recent call last):\r\n  File \"v3.py\", line 596, in <module>\r\n    model, params = build_model(params_path='paramsv3', use_gru=True, display_summary=False)\r\n  File \"v3.py\", line 557, in build_model\r\n    encoder_outs_window = concatenate([Lcntxt,encoder_outs_window],axis=1)\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/merge.py\", line 927, in concatenate\r\n    return Concatenate(axis=axis, **kwargs)(inputs)\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\", line 737, in __call__\r\n    base_layer_utils.create_keras_history(inputs)\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 186, in create_keras_history\r\n    _, created_layers = _create_keras_history_helper(tensors, set(), [])\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 246, in _create_keras_history_helper\r\n    constants[i] = backend.function([], op_input)([])\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3632, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1472, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,170]\r\n\t [[{{node input_1}}]]\r\n\r\nIs it possible to resolve the above without using session call?\r\n", "I am wondering if there are plans for Ragged tensors will be supported on TPU. It doesn't seem like there is currently, at least I had to remove them to get my model to train. ", "@vyshnavigutta369: try wrapping the ops that use RaggedTensors in a Keras Lambda layer. It's hard to tell from the stack trace, but that might help.\r\n\r\n@Santosh-Gupta: RaggedTensors are currently not supported on TPUs, unfortunately, but we're investigating ways to make this work!", "@markomernick  On putting tf.ragged.boolean_mask in lambda layer was getting this error.\r\n\r\nTraceback (most recent call last):\r\n  File \"v3.py\", line 599, in <module>\r\n    model, params = build_model(params_path='paramsv3', use_gru=True, display_summary=False)\r\n  File \"v3.py\", line 536, in build_model\r\n    masking_ragged_slice = Lambda(lambda x: tf.ragged.boolean_mask(x[0],x[1]).to_tensor(shape=[None,2,256])) (encoder_outs_window , input_mask_slice)\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\", line 778, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\", line 888, in call\r\n    result = self.function(inputs, **kwargs)\r\n  File \"v3.py\", line 536, in <lambda>\r\n    masking_ragged_slice = Lambda(lambda x: tf.ragged.boolean_mask(x[0],x[1]).to_tensor(shape=[None,2,256])) (encoder_outs_window , input_mask_slice)\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/ops/ragged/ragged_array_ops.py\", line 96, in boolean_mask\r\n    mask, dtypes.bool, name='mask')\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/ops/ragged/ragged_tensor.py\", line 2433, in convert_to_tensor_or_ragged_tensor\r\n    value=value, dtype=dtype, preferred_dtype=preferred_dtype, name=name)\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1317, in convert_to_tensor\r\n    (dtype.name, value.dtype.name, value))\r\nValueError: Tensor conversion requested dtype bool for Tensor with dtype float32: <tf.Tensor 'lambda_2/strided_slice_1:0' shape=(17, 256) dtype=float32>\r\n\r\nI tested normally too, on putting lambda around tf.ragged.boolean_mask was giving the same error.\r\nI believe that tf.ragged.boolean_mask is breaking up the graph. Is there a way to get around this?", "@vyshnavigutta369  What is the dtype of `x`?  `boolean_mask` expects its second argument to be a boolean tensor, so if `x` isn't boolean, then I'd expect `tf.ragged.boolean_mask(x[0],x[1])` to fail with this error message.  (The error is saying that it got something that was float32, but needed a bool.)\r\n\r\nAlso, it looks like you're passing in a 2D tensor (with shape 17,256), but you're calling .to_tensor() with a 3D shape?", "Hi. This is the correct traceback.\r\n\r\nTraceback (most recent call last):\r\n  File \"v3.py\", line 637, in <module>\r\n    model, params = build_model(params_path='paramsv3', use_gru=True, display_summary=False)\r\n  File \"v3.py\", line 573, in build_model\r\n    masking_ragged = Lambda(lambda x: tf.ragged.boolean_mask(x[0],x[1]))(encoder_outs_window , input_mask_slice)\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\", line 778, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\", line 888, in call\r\n    result = self.function(inputs, **kwargs)\r\n  File \"v3.py\", line 573, in <lambda>\r\n    masking_ragged = Lambda(lambda x: tf.ragged.boolean_mask(x[0],x[1]))(encoder_outs_window , input_mask_slice)\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/ops/ragged/ragged_array_ops.py\", line 96, in boolean_mask\r\n    mask, dtypes.bool, name='mask')\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/ops/ragged/ragged_tensor.py\", line 2433, in convert_to_tensor_or_ragged_tensor\r\n    value=value, dtype=dtype, preferred_dtype=preferred_dtype, name=name)\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1317, in convert_to_tensor\r\n    (dtype.name, value.dtype.name, value))\r\nValueError: Tensor conversion requested dtype bool for Tensor with dtype float32: <tf.Tensor 'lambda_2/strided_slice_1:0' shape=(17, 256) dtype=float32>\r\n\r\nThe shapes of the two in tf.ragged.boolean_mask op are \r\n**input_mask_slice Tensor(\"lambda_1/strided_slice:0\", shape=(?, 17), dtype=bool)\r\nencoder_outs_window Tensor(\"lambda/strided_slice:0\", shape=(?, 17, 256), dtype=float32)**\r\n\r\nThe second argument is of the type bool.  \r\nWhat I essentially want is this operation,\r\n **tf.ragged.boolean_mask(encoder_outs_window,input_mask_slice)[:,-2:].to_tensor(shape=[None,2,256]).** \r\nThe above was working without lambda wrapper, but was failing when being used as one of the args in tf.keras.layer.concatenate  later. Getting the error message that I mentioned in the beginning. \r\n**tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_1' with dtype float and shape**\r\n\r\nOn putting a lambda on the above, I was getting the error I mentioned above.\r\n**ValueError: Tensor conversion requested dtype bool for Tensor with dtype float32: <tf.Tensor 'lambda_2/strided_slice_1:0' shape=(17, 256) dtype** \r\nBelow is what I did with Lambda,\r\nmasking_ragged = Lambda(lambda x: tf.ragged.boolean_mask(x[0],x[1]))(encoder_outs_window , input_mask_slice)                          \r\nslce_tensor = Lambda(lambda x: x[:,-2:].to_tensor(shape=[None,2,256]))(masking_ragged)", "@vyshnavigutta369  \r\n\r\nI think you probably need to do this:\r\n\r\n```\r\nmasking_ragged = Lambda(lambda x: tf.ragged.boolean_mask(x[0],x[1]))((encoder_outs_window , input_mask_slice))\r\n```\r\n\r\nI.e., call the lambda layer with a single argument that's a tuple, rather than with two arguments.  Or perhaps this would work:\r\n\r\n```\r\nmasking_ragged = Lambda(lambda x, y: tf.ragged.boolean_mask(x, y)(encoder_outs_window , input_mask_slice)\r\n```\r\n\r\nI'm not actually all that familiar with how Lambda layers get used when you want multiple arguments; but the fact that `x[1]` appears to be a float32 tensor with shape [17, 256], combined with the fact that `encoder_outs_window` is a float32 tensor with shape [?, 17, 256], strongly suggests to me that `x` is just `encoder_outs_window`.  You could test this with something like this:\r\n\r\n```\r\ndef func(x):\r\n  print(\"x is\", x)\r\n  return tf.ragged.boolean_mask(x[0],x[1])\r\n\r\nmasking_ragged = Lambda(lambda x, y: tf.ragged.boolean_mask(x, y)((encoder_outs_window , input_mask_slice))\r\n```\r\n\r\nAnd I expect that `x` will be a single float32 Tensor, not a tuple of tensors.", "Yes, that solved the previous error I was getting, but still getting the below.\r\n\r\nTraceback (most recent call last):\r\n  File \"v3.py\", line 638, in <module>\r\n    model, params = build_model(params_path='paramsv3', use_gru=True, display_summary=False)\r\n  File \"v3.py\", line 573, in build_model\r\n    masking_ragged = Lambda(lambda x, y: tf.ragged.boolean_mask(x, y))((encoder_outs_window , input_mask_slice))\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\", line 737, in __call__\r\n    base_layer_utils.create_keras_history(inputs)\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 186, in create_keras_history\r\n    _, created_layers = _create_keras_history_helper(tensors, set(), [])\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 246, in _create_keras_history_helper\r\n    constants[i] = backend.function([], op_input)([])\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3632, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/home/iiit/revanth.parvathaneni/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1472, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,170]\r\n\t [[{{node input_1}}]]\r\n\r\nInput 1 is  one of the inputs using tf.keras.Layer.Input which I was using to input to the build model. What I understand is that unsuccessful wrapping of tf.ragged.boolean_mask or using that directly in further layers of keras is giving me a similar error message. \r\n\r\nWhen I removed Lambda completely, \r\ni.e, cntxt = tf.ragged.boolean_mask(encoder_outs_window,input_mask_slice)[:,-2:].to_tensor(shape=[None,2,256]),\r\nat tf.keras.layers.concatenate(cntxt,..) was giving me the same error. But when used with tf.concat, the operation was successful. My code has lots of keras.layers being used whose arguments are derived from cntxt. So I am getting the same error at the first layer which is being called. ", "Is it possible to support Python variable length  list-of-lists as an input to the Keras model? \r\nLet's say I can do this in TF:\r\n```\r\nimport tensorflow as tf\r\n\r\ndef multi_hot_encoding(columns):\r\n    inp = tf.one_hot(columns, depth=15)\r\n    return tf.math.reduce_sum(inp, axis=-2)\r\ny = multi_hot_encoding(tf.ragged.constant([[0, 2, 5], [1, 2, 4, 7]]))\r\nprint(y)\r\n```\r\n\r\nHowever, same as a Keras model will fail:\r\n```\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import backend as K\r\ninp = keras.layers.Input(shape=(None,), ragged=True, dtype='int32')\r\nlayer_ = keras.layers.Lambda(lambda x: K.sum(K.one_hot(x, num_classes=15), axis=-2))(inp)\r\nmodel = keras.Model(inp, layer_)\r\ny = model.predict([[0, 2, 5], [1, 2, 4, 7]])\r\nprint(y)\r\n```\r\n\r\nwith:\r\n```\r\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).\r\n```\r\n\r\nunless I wrap input `list` into `tf.ragged.constant()`", "The latest versions of 2.4 (nightly) appear to resolve my remaining concerns. In particular, factory constructors and attribute access can now be used to create models without explicit `Lambda` wrappers.\r\n\r\n```python\r\nvalues = tf.keras.Input(())\r\nrow_lengths = tf.keras.Input((), dtype=tf.int32)\r\nrt = tf.RaggedTensor.from_row_lengths(values, row_lengths)\r\nmodel = tf.keras.Model([values, row_lengths], rt.row_splits)\r\n\r\nmodel([tf.range(10), tf.constant([2, 3, 5])])  # [0, 2, 5, 10]\r\n```\r\n\r\nThanks to those that made this possible. I'm happy to close this if the above still works once 2.4 is stable.", "Update: while not strictly related to the issue as it's currently titled, the above example doesn't work with `tf.SparseTensor`s :(\r\n\r\n```python\r\nindices = tf.keras.Input((2,), dtype=tf.int64)\r\nvalues = tf.keras.Input((), dtype=tf.float32)\r\nargs = (indices, values)\r\ntf.keras.Model(args, tf.SparseTensor(*args, dense_shape=(5, 5)))\r\n```\r\n```txt\r\nTypeError: Cannot convert a symbolic Keras input/output to a numpy array. \r\nThis error may indicate that you're trying to pass a symbolic value to a NumPy \r\ncall, which is not supported. Or, you may be trying to pass Keras symbolic \r\ninputs/outputs to a TF API that does not register dispatching, preventing \r\nKeras from automatically converting the API call to a lambda layer in the \r\nFunctional Model.\r\n```\r\n\r\nCurrent dirty workaround it to `add_dispatch_support` manually.\r\n\r\n```python\r\nimport functools\r\nimport inspect\r\nimport tensorflow as tf\r\nfrom tensorflow.python.util import dispatch\r\n\r\n@dispatch.add_dispatch_support\r\n@functools.wraps(tf.SparseTensor)\r\ndef SparseTensor(*args, **kwargs):\r\n    return tf.SparseTensor(*args, **kwargs)\r\n\r\n\r\nSparseTensor.__signature__ = inspect.signature(tf.SparseTensor)\r\n```", "An important feature missing is allowing RaggedTensors as targets (e.g., in model.fit), which is a frequent request, see #44988, #44112, #43591, #43093, #42320, #41810.\r\n\r\nSome partial progress is in pull requests #45060 and #45015, which will allow using custom losses in Keras model (but existing losses like MSE or (S)CE will still not work).", "@jackd \r\n\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999"]}, {"number": 27169, "title": "[Intel MKL] Updating the MKL CI Infrastructure.", "body": "Adding the following features:\r\n- Dockerfile builds TensorFlow with parameterized bazel options\r\n- gcc version detected inside the container environment in order to set the correct compiler flags\r\n- Support for modern Intel platforms (and CPU instructions)\r\n- Support for alternate base containers", "comments": ["@gunan This is our first step to moving out of the old `docker` folder. The CPU Dockerfiles in the new directory work just fine for building for MKL because the TensorFlow build doesn't happen in those Dockerfiles. ", "Ya, @gunan will have to answer that question. I considered putting it in the dockerfiles location, but the two scripts made more sense here, and none of the Dockerfiles build TF as part of Docker build. This is essentially a layer on top of other containers that does the actual TF build after detecting the context of the base container. We currently use these for the MKL public CI, but we're aiming to single-source it with our internal CI. ", "Just to check.\r\nIs the docker container only used for CI, or do you want to publicize and use it for TF docker containers that have MKL enabled?\r\n\r\nIf 2nd is the case, we should find a way to have it as a part of the new dockerfiles.\r\nOtherwise, the changes are LGTM for the CI.", "Both. We'd like to use this Dockerfile for both CI and to publish our containers. @gunan Where do you recommend we put the files?", "For the one to distribute, I recommend creating  a dockerfile here:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/dockerfiles\r\n\r\nBut I think we can do that after this PR is merged.", "Sounds good. Where do you recommend that we put the python script that detects gcc version?", "Right now it fits well in ci_build, but perhaps if we move them out, it may make sense to put it in a includes directory under `dockerfiles`.", "OK LMK.", "Ok, lets go ahead and merge this PR for now, and then we can figure out the\nrest of the details.\n\nOn Thu, Apr 4, 2019, 11:48 AM Clayne Robison <notifications@github.com>\nwrote:\n\n> OK LMK.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27169#issuecomment-480017833>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOU6xvgR7xDep0AKbKfMgaC4_reXlks5vdkkKgaJpZM4cMiLO>\n> .\n>\n", "Ping @gbaned for merge", "@claynerobison thank you for your contribution . can you please check Ubuntu Sanity build failures.", "@gunan @angersson @gbaned This last commit looks frightening, but it's mostly pylint whitespace fixes. The more substantial commit is in tensorflow/tools/ci_build/linux/mkl/build-dev-container.sh.", "@rthadur @gbaned fixed remaining pylint errors. Ubuntu Sanity check is now passing.", "@gbaned Would you mind merging this?", "@gbaned @gunan Our internal and external CI are waiting for this. Would you mind flipping the switch?", "@claynerobison It looks like the PR quietly got stuck in internal CI because set-build-env.py is using Intel's license. Can you change the license to the same license used by other TF files, please?", "@angersson Done.", "@claynerobison I had to resolve a few straggling linter issues that only showed up on our end. Thanks for your patience!"]}, {"number": 27168, "title": "tf.dataset guide, TF 2.0 and one shot iterator", "body": "<\r\n\r\n**System information**\r\n- TensorFlow version: 2.0\r\n- Doc Link: https://www.tensorflow.org/guide/datasets\r\n\r\nIssue\r\n - The tf.dataset guide mentions using one shot iterator.  \r\n - The doc does not have a section to view by API release.  \r\n", "comments": ["@netskink I agree with you. But, outside TF website, there are other resources that talk about one-shot iterator. Thanks!", "No worries.\n\nOn Tue, Apr 2, 2019 at 7:14 PM Vishnuvardhan Janapati <\nnotifications@github.com> wrote:\n\n> @netskink <https://github.com/netskink> I agree with you. But, outside TF\n> website, there are other resources that talk about one-shot iterator.\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27168#issuecomment-479247458>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEN6HHw6Jt_5igrsKH_51OuSr2k8lihhks5vc-RbgaJpZM4cMgyI>\n> .\n>\n\n\n-- \nJohn F. Davis\n6 Kandes Court\nDurham, NC 27713\n919-888-8358\nPublic Profile https://www.linkedin.com/in/netskink\n\n\u72ec\u6811\u4e00\u5e1c\n", "Not applicable now, thanks", "You are welcome Billy\n\nOn Fri, Jul 12, 2019 at 18:02 Billy Lamberta <notifications@github.com>\nwrote:\n\n> Not applicable now, thanks\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27168?email_source=notifications&email_token=ABBXUHBGZ7U5WRHL4EBCMGTP7D5PPA5CNFSM4HBSBSEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ27C6Q#issuecomment-511046010>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABBXUHD5TBKWUTJVM6FFOADP7D5PPANCNFSM4HBSBSEA>\n> .\n>\n-- \nRocking the roll like a natural man!\n"]}, {"number": 27167, "title": "Add the support of cuDNN Batch Norm Ex Operation", "body": "(Solved the conflicts based on #27125, which has been closed) \r\nWe added the support to the new cuDNN batch norm ex operation (available form 7.4.2). This operation provides faster NHWC kernels but needs to use an additional reserve_space and workspace.\r\n\r\nWhat we've done:\r\n(1) Register a new operation FusedBatchNormV3 (and corresponding grad ops)\r\n(2) Change python API to call the FusedBatchNormV3\r\n(3) Support XLA\r\n(4) Support TensorRT\r\n(5) Support Grappler pass\r\n\r\nFYI. @nluehr", "comments": ["@timshen91 is a better reviewer for cuDNN related changes.", "I fixed a minor issue about the XLA tests on this _V3 op. Please check. Thanks.", "@houtoms can you please check build errors", "Add @chsigg for TF ops.", "@rthadur , I just checked the logs and it shows something like `ERROR: /tmpfs/src/github/tensorflow/tensorflow/core/BUILD:2650:1: error loading package 'tensorflow/tools/proto_text': Unable to load package for '//tools/build_defs/proto/cpp:cc_proto_library.bzl': BUILD file not found on package path and referenced by '//tensorflow/core:protos_all_proto_text_srcs'`. \r\n\r\nI don't know if this is related to my change. Can you provide some advice?", "> @rthadur , I just checked the logs and it shows something like `ERROR: /tmpfs/src/github/tensorflow/tensorflow/core/BUILD:2650:1: error loading package 'tensorflow/tools/proto_text': Unable to load package for '//tools/build_defs/proto/cpp:cc_proto_library.bzl': BUILD file not found on package path and referenced by '//tensorflow/core:protos_all_proto_text_srcs'`.\r\n> \r\n> I don't know if this is related to my change. Can you provide some advice?\r\n\r\n@smit-hinsu , @timshen91 can you please assist with these errors", "I am not sure if anyone is assigned to review the main part of my changes on the new batch norm. Any updates?", "Thanks @penpornk , I have make the changes accordingly.", "Hi @penpornk , I tried the following commands, but only some files like `tensorflow.raw_ops.pbtxt` are updated. Could you advise how to generate the file you mentioned? Or do I need to manually generate it?\r\n```\r\n$ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n$ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```", "@houtoms Oh I'm sorry I forgot to mention it. Yes, you need to write them manually.", "Thanks. I just added two pbtxt files for this new operation.", "Hi @houtoms, sorry for being late for review. I think things that are left are:\r\n\r\n* API review\r\n* GPU implementation of the v3 op (I did part of this, but will need @chsigg for a deeper look after you resolve my comments)\r\n* Interactions with FP16 (@reedwm)\r\n* Grappler support", "Hi @aaroey I have committed the changes based on your comments. Please take a look at them. Thanks. ", "Hi @aaroey , I fixed the problems you mentioned. Please check. Thanks.", "Please regenerate the API golden files", "@houtoms please regenerate the API golden files (and exclude the change to summary op).", "Yea. I updated the goldens just now.", "Hi @houtoms, would you please help to fix the broken tests?\r\nThanks.", "Sure. I found many tests are broken, such as \"Ubuntu Python3\". Could you advise how to fix them?", "In \"Ubuntu Sanity\", the log shows ```FAIL: Found 9 non-whitelited pylint errors:\r\ntensorflow/python/ops/nn_impl.py:1303: [C0330(bad-continuation), ] Wrong hanging indentation (remove 1 space).\r\ntensorflow/python/ops/nn_impl.py:1304: [C0330(bad-continuation), ] Wrong hanging indentation (remove 1 space).\r\ntensorflow/python/ops/nn_impl.py:1305: [C0330(bad-continuation), ] Wrong hanging indentation (remove 1 space).\r\ntensorflow/python/ops/nn_impl.py:1306: [C0330(bad-continuation), ] Wrong hanging indentation (remove 1 space).\r\ntensorflow/python/ops/nn_impl.py:1307: [C0330(bad-continuation), ] Wrong hanging indentation (remove 1 space).\r\ntensorflow/python/ops/nn_impl.py:1308: [C0330(bad-continuation), ] Wrong hanging indentation (remove 1 space).\r\ntensorflow/python/ops/nn_impl.py:1309: [C0330(bad-continuation), ] Wrong hanging indentation (remove 1 space).\r\ntensorflow/python/ops/nn_impl.py:1310: [C0330(bad-continuation), ] Wrong hanging indentation (remove 1 space).\r\ntensorflow/python/ops/nn_impl.py:1311: [C0330(bad-continuation), ] Wrong hanging indentation (remove 1 space).``` But I don't think I made changes on this part. Could you advise?", "I am wondering if this is because my commits are way far behind the latest changes. Do I need to merge the new master?", "I saw a bunch of compilation errors like: \r\n\r\n```\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:39:11: error: no member named 'StreamExecutor' in namespace 'stream_executor'\r\nusing se::StreamExecutor;\r\n      ~~~~^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:42:11: error: no member named 'DeviceMemory' in namespace 'stream_executor'\r\nusing se::DeviceMemory;\r\n      ~~~~^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:43:11: error: no member named 'DeviceMemoryBase' in namespace 'stream_executor'\r\nusing se::DeviceMemoryBase;\r\n      ~~~~^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:44:11: error: no member named 'dnn' in namespace 'stream_executor'\r\nusing se::dnn::ToDataType;\r\n      ~~~~^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:45:11: error: no member named 'ScratchAllocator' in namespace 'stream_executor'\r\nusing se::ScratchAllocator;\r\n      ~~~~^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:46:7: error: no member named 'port' in namespace 'stream_executor'; did you mean simply 'port'?\r\nusing se::port::StatusOr;\r\n```\r\n\r\nYou should be able to access the [log](https://source.cloud.google.com/results/invocations/734fa6ad-b590-4410-a51f-b68d11353549/log).\r\n\r\nFor the ubuntu sanity issues, I think this part of the code (starting from the second line below) has extra leading whitespace:\r\n\r\n```\r\n  y, batch_mean, batch_var, _, _, _ = gen_nn_ops.fused_batch_norm_v3(\r\n       x,\r\n       scale,\r\n       offset,\r\n       mean,\r\n       variance,\r\n       epsilon=epsilon,\r\n       data_format=data_format,\r\n       is_training=is_training,\r\n       name=name)\r\n```\r\n\r\nThere are also failures of related tests [here](https://source.cloud.google.com/results/invocations/3ca972c7-8620-4418-910d-ef02f71fa52e/targets), you should be able to click each failed target and see the log.\r\n\r\nThanks.", "@houtoms can you please check build errors.", "Hi @rthadur and @aaroey , the log says something like `tensorflow/core/kernels/fused_batch_norm_op.cc:39:11: error: no member named 'StreamExecutor' in namespace 'stream_executor'`. \r\nBut I have no problem building these tests on my local machine. I can guess this might be related to that I need to add the `stream_executor` dependency in the `BUILD`, since I've added the `stream_executor.h` header in `fused_batch_norm_op.cc`. (But still, I am not sure if this is a real fix.)", "Can someone enable the access to the logs? It should have some \"details\" link for each test?", "Hi @aaroey , I saw the root cause of the broken builds is `//tensorflow/core/kernels:fused_batch_norm_op` according to the report of Ubuntu CC. Then, I found I need to add `//tensorflow/core:stream_executor` to the `deps` for `fused_batch_norm_op` in `core/kernels/BUILD`, since I added `stream_executor.h` in the `fused_batch_norm_op.cc`.\r\nAfter this change, I can successfully `bazel build //tensorflow/core/kernels:fused_batch_norm_op` in my local container. But I don't know why the build is still a failure here. Could you advise what might be the reason?", "@houtoms sure. Below are the failures I can see:\r\n\r\n## //tensorflow/python:auto_mixed_precision_test_gpu: \r\n```\r\nTest AutoMixedPrecisionTest > test_conv_bn\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.4/unittest/case.py\", line 58, in testPartExecutor\r\n    yield\r\n  File \"/usr/lib/python3.4/unittest/case.py\", line 577, in run\r\n    testMethod()\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/auto_mixed_precision_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1167, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/auto_mixed_precision_test_gpu.runfiles/org_tensorflow/tensorflow/python/grappler/auto_mixed_precision_test.py\", line 354, in test_conv_bn\r\n    self._assert_output_fp16(node_map, 'FusedBatchNorm')\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/auto_mixed_precision_test_gpu.runfiles/org_tensorflow/tensorflow/python/grappler/auto_mixed_precision_test.py\", line 274, in _assert_output_fp16\r\n    self.assertEqual(node_map[node_name].output_info[output_port].dtype,\r\nKeyError: 'FusedBatchNorm'\r\n```\r\n## //tensorflow/python:auto_mixed_precision_test_gpu\r\n```\r\nTest AutoMixedPrecisionTest > test_conv_bn_dropout\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.4/unittest/case.py\", line 58, in testPartExecutor\r\n    yield\r\n  File \"/usr/lib/python3.4/unittest/case.py\", line 577, in run\r\n    testMethod()\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/auto_mixed_precision_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1167, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/auto_mixed_precision_test_gpu.runfiles/org_tensorflow/tensorflow/python/grappler/auto_mixed_precision_test.py\", line 377, in test_conv_bn_dropout\r\n    self._assert_output_fp16(node_map, 'FusedBatchNorm')\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/auto_mixed_precision_test_gpu.runfiles/org_tensorflow/tensorflow/python/grappler/auto_mixed_precision_test.py\", line 274, in _assert_output_fp16\r\n    self.assertEqual(node_map[node_name].output_info[output_port].dtype,\r\nKeyError: 'FusedBatchNorm'\r\n```\r\n\r\n## //tensorflow/python:layout_optimizer_test_gpu\r\n```\r\nTest LayoutOptimizerTest > testBinaryOpSecondPort\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.4/unittest/case.py\", line 58, in testPartExecutor\r\n    yield\r\n  File \"/usr/lib/python3.4/unittest/case.py\", line 577, in run\r\n    testMethod()\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/layout_optimizer_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1597, in decorated\r\n    return func(self, *args, **kwargs)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/layout_optimizer_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1167, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/layout_optimizer_test_gpu.runfiles/org_tensorflow/tensorflow/python/grappler/layout_optimizer_test.py\", line 1478, in testBinaryOpSecondPort\r\n    self._assert_trans_nhwc_to_nchw('FusedBatchNorm-0', nodes)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/layout_optimizer_test_gpu.runfiles/org_tensorflow/tensorflow/python/grappler/layout_optimizer_test.py\", line 213, in _assert_trans_nhwc_to_nchw\r\n    self.assertIn(name + '-TransposeNHWCToNCHW-LayoutOptimizer', nodes)\r\n  File \"/usr/lib/python3.4/unittest/case.py\", line 1056, in assertIn\r\n    self.fail(self._formatMessage(msg, standardMsg))\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/layout_optimizer_test_gpu.runfiles/absl_py/absl/testing/absltest.py\", line 1611, in fail\r\n    return super(TestCase, self).fail(self._formatMessage(prefix, msg))\r\n  File \"/usr/lib/python3.4/unittest/case.py\", line 642, in fail\r\n    raise self.failureException(msg)\r\nAssertionError: 'FusedBatchNorm-0-TransposeNHWCToNCHW-LayoutOptimizer' not found in ['_SOURCE', '_SINK', 'PermConstNCHWToNHWC-LayoutOptimizer', 'Add-1-ReshapeConst-LayoutOptimizer', 'Const_3', 'Const_2', 'Const_1', 'Const', 'PermConstNHWCToNCHW-LayoutOptimizer', 'truncated_normal/shape', 'truncated_normal/TruncatedNormal', 'FusedBatchNormV3-0-TransposeNHWCToNCHW-LayoutOptimizer', 'FusedBatchNormV3', 'Add-1-ReshapeNHWCToNCHW-LayoutOptimizer', 'Add', 'Add-0-0-TransposeNCHWToNHWC-LayoutOptimizer', 'Identity', 'Identity/_0', '_SOURCE', '_SINK', 'Identity/_1', '_retval_Identity_0_0']\r\n```\r\n\r\n## //tensorflow/python/ops/parallel_for:control_flow_ops_test_gpu\r\n```\r\nTest NNTest > test_fused_batch_norm\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.4/unittest/case.py\", line 58, in testPartExecutor\r\n    yield\r\n  File \"/usr/lib/python3.4/unittest/case.py\", line 577, in run\r\n    testMethod()\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/ops/parallel_for/control_flow_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1031, in decorated\r\n    f(self, *args, **kwargs)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/ops/parallel_for/control_flow_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/parallel_for/control_flow_ops_test.py\", line 394, in test_fused_batch_norm\r\n    self._test_loop_fn(loop_fn, 3, loop_fn_dtypes=[dtypes.float32] * 6)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/ops/parallel_for/control_flow_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/parallel_for/test_util.py\", line 56, in _test_loop_fn\r\n    parallel_iterations=parallel_iterations)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/ops/parallel_for/control_flow_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 164, in pfor\r\n    return f()\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/ops/parallel_for/control_flow_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 161, in f\r\n    return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/ops/parallel_for/control_flow_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 214, in _pfor_impl\r\n    outputs.append(converter.convert(loop_fn_output))\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/ops/parallel_for/control_flow_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/parallel_for/pfor.py\", line 1175, in convert\r\n    output = self._convert_helper(y)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/ops/parallel_for/control_flow_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/parallel_for/pfor.py\", line 1343, in _convert_helper\r\n    \"which may run slower\" % (y_op.type, y_op, converted_inputs))\r\nValueError: No converter defined for FusedBatchNormV3\r\nname: \"loop_body/FusedBatchNormV3\"\r\nop: \"FusedBatchNormV3\"\r\ninput: \"loop_body/GatherV2\"\r\ninput: \"random_uniform_1\"\r\ninput: \"random_uniform_2\"\r\ninput: \"loop_body/Const\"\r\ninput: \"loop_body/Const_1\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"U\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"data_format\"\r\n  value {\r\n    s: \"NHWC\"\r\n  }\r\n}\r\nattr {\r\n  key: \"epsilon\"\r\n  value {\r\n    f: 0.009999999776482582\r\n  }\r\n}\r\nattr {\r\n  key: \"is_training\"\r\n  value {\r\n    b: true\r\n  }\r\n}\r\ninputs: [WrappedTensor(t=<tf.Tensor 'random_uniform:0' shape=(3, 1, 5, 5, 2) dtype=float32>, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'random_uniform_1:0' shape=(2,) dtype=float32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'random_uniform_2:0' shape=(2,) dtype=float32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'loop_body/Const:0' shape=(0,) dtype=float32>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'loop_body/Const_1:0' shape=(0,) dtype=float32>, is_stacked=False, is_sparse_stacked=False)].\r\nEither add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower\r\n```\r\n\r\n## 939 broken targets caused by //tensorflow/core/kernels:fused_batch_norm_op\r\nFor example //tensorflow/c:c_api_experimental_test:\r\n```\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:39:11: error: no member named 'StreamExecutor' in namespace 'stream_executor'\r\nusing se::StreamExecutor;\r\n      ~~~~^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:42:11: error: no member named 'DeviceMemory' in namespace 'stream_executor'\r\nusing se::DeviceMemory;\r\n      ~~~~^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:43:11: error: no member named 'DeviceMemoryBase' in namespace 'stream_executor'\r\nusing se::DeviceMemoryBase;\r\n      ~~~~^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:44:11: error: no member named 'dnn' in namespace 'stream_executor'\r\nusing se::dnn::ToDataType;\r\n      ~~~~^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:45:11: error: no member named 'ScratchAllocator' in namespace 'stream_executor'\r\nusing se::ScratchAllocator;\r\n      ~~~~^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:46:7: error: no member named 'port' in namespace 'stream_executor'; did you mean simply 'port'?\r\nusing se::port::StatusOr;\r\n      ^~~~~~~~\r\n      port\r\n./tensorflow/core/platform/mem.h:25:11: note: 'port' declared here\r\nnamespace port {\r\n          ^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:46:17: error: no member named 'StatusOr' in namespace 'tensorflow::port'\r\nusing se::port::StatusOr;\r\n          ~~~~~~^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:75:1: error: no template named 'DeviceMemory'\r\nDeviceMemory<U> CastDeviceMemory(Tensor* tensor) {\r\n^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:76:10: error: no template named 'DeviceMemory'; did you mean 'CastDeviceMemory'?\r\n  return DeviceMemory<U>::MakeFromByteSize(\r\n         ^~~~~~~~~~~~\r\n         CastDeviceMemory\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:75:17: note: 'CastDeviceMemory' declared here\r\nDeviceMemory<U> CastDeviceMemory(Tensor* tensor) {\r\n                ^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:76:25: error: qualified name refers into a specialization of function template 'CastDeviceMemory'\r\n  return DeviceMemory<U>::MakeFromByteSize(\r\n         ~~~~~~~~~~~~~~~^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:75:17: note: function template 'CastDeviceMemory' declared here\r\nDeviceMemory<U> CastDeviceMemory(Tensor* tensor) {\r\n                ^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:85:46: error: expected class name\r\nclass CudnnBatchNormAllocatorInTemp : public ScratchAllocator {\r\n                                             ^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:87:36: error: only virtual member functions can be marked 'override'\r\n  ~CudnnBatchNormAllocatorInTemp() override = default;\r\n                                   ^~~~~~~~~\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:92:47: error: only virtual member functions can be marked 'override'\r\n  int64 GetMemoryLimitInBytes(Stream* stream) override {\r\n                                              ^~~~~~~~~\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:96:3: error: no template named 'StatusOr'\r\n  StatusOr<DeviceMemory<uint8>> AllocateBytes(Stream* stream,\r\n  ^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:96:12: error: no template named 'DeviceMemory'; did you mean 'CastDeviceMemory'?\r\n  StatusOr<DeviceMemory<uint8>> AllocateBytes(Stream* stream,\r\n           ^~~~~~~~~~~~\r\n           CastDeviceMemory\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:75:17: note: 'CastDeviceMemory' declared here\r\nDeviceMemory<U> CastDeviceMemory(Tensor* tensor) {\r\n                ^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:105:14: error: no viable conversion from returned value of type 'tensorflow::Status' to function return type 'int'\r\n      return allocation_status;\r\n             ^~~~~~~~~~~~~~~~~\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:111:12: error: no template named 'DeviceMemory'; did you mean 'CastDeviceMemory'?\r\n    return DeviceMemory<uint8>::MakeFromByteSize(\r\n           ^~~~~~~~~~~~\r\n           CastDeviceMemory\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:75:17: note: 'CastDeviceMemory' declared here\r\nDeviceMemory<U> CastDeviceMemory(Tensor* tensor) {\r\n                ^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:111:31: error: qualified name refers into a specialization of function template 'CastDeviceMemory'\r\n    return DeviceMemory<uint8>::MakeFromByteSize(\r\n           ~~~~~~~~~~~~~~~~~~~^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:75:17: note: function template 'CastDeviceMemory' declared here\r\nDeviceMemory<U> CastDeviceMemory(Tensor* tensor) {\r\n                ^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:133:48: error: expected class name\r\nclass CudnnBatchNormAllocatorInOutput : public ScratchAllocator {\r\n                                               ^\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n20 errors generated.\r\n```", "@aaroey , Thanks. What I just asked is about the last one \"939 broken targets caused by //tensorflow/core/kernels:fused_batch_norm_op\". I think I already added the correct deps to the BUILD, but why does it still fail?", "@houtoms I think one possibility is that you added `#include \"tensorflow/core/platform/stream_executor.h\"` inside `#if GOOGLE_CUDA`. Please try moving it outside. Also please try to build without CUDA enabled in your local environment.", "I see. This `GOOGLE_CUDA` suggestion is super helpful. Let me fix it.", "@aaroey , I fixed the issues mentioned above. Could you please help check?", "It seems I cannot put \"tensorflow/core/platform/stream_executor.h\" outside GOOGLE_CUDA macros.", "Hi @aaroey , I put the tensorflow/core/platform/stream_executor.h back to the GOOGLE_CUDA macros and use this macro to split the CPU/GPU paths.", "Oh, just realize that the CPU functor cannot use the ScratchAllocator anymore, meaning it has to explicitly do the dummy allocation for the 5th output.\r\n\r\nI've made those changes. Please check. ", "@aaroey , I made the changes of using a dummy class for ScratchAllocator (and its inheritance). Please check the updates.", "There is a new build error:\r\n```\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:20:32: fatal error: cuda/include/cudnn.h: No such file or directory\r\n```\r\nIt seems the paths are changed recently, please try with\r\n```\r\n#include \"third_party/gpus/cudnn/cudnn.h\"\r\n```", "\"Please implement the virtual destructor of ScratchAllocator\"\r\nDo you mean I put the dummy allocation in the destructor of ScratchAlloctor? If yes, I think it is wrong, since the CudnnBatchNormAllocatorInTemp also inherits ScratchAllocator and will be used in xxxGradOpV3, meaning it will unnecessarily allocate the 5th output for the xxxGradOpV3.\r\n\r\nThere is no third_party/gpus/cudnn/cudnn.h: https://github.com/tensorflow/tensorflow/tree/master/third_party/gpus", "@houtoms Sorry, by implementing the destructor I meant to provide a implementation a.k.a add `{}`. It's fine that third_party/gpus/cudnn/cudnn.h doesn't exist, bazel will fix that.", "Thanks for the clarification. I made those changes.", "@aaroey @penpornk Could you take a look at my last commit? (I think you might miss the last one and only approved the second to the last.)", "@aaroey , I fixed some FusedBatchNormV3 related issues in the tests of quantize ops. Please check.", "@aaroey , I added the forward compat checkings for the new op and relevant tests.", "@aaroey I've done using the `forward_compatible_horizon()`. Please check.", "@houtoms there are  two more failing targets:\r\n//tensorflow/python:nn_fused_batchnorm_test_gpu\r\n//tensorflow/python/ops/parallel_for:control_flow_ops_test_gpu\r\n\r\nLet me know if you cannot access the log. Thanks.", "Thanks @aaroey , I think I just fixed those two tests. Please have a look.", "@aaroey , any comments on the last changes?", "Hi @aaroey , there are still two tests failed. \"Windows Bazel GPU\": I checked the logs and it seems the error is not related to this PR. \"feedback/copybara\": I cannot access the internal logs.\r\n\r\nCan you help with these two tests?", "@houtoms sure I'll take care of that.", "@houtoms we got some new failures for these targets:\r\n\r\ntensorflow/contrib/quantize:fold_batch_norms_test\r\ntensorflow/contrib/quantize:graph_matcher_test\r\ntensorflow/contrib/quantize:quantize_graph_test\r\ntensorflow/contrib/quantize:quantize_parameterized_test\r\ntensorflow/python/kernel_tests/linalg:linear_operator_toeplitz_test\r\ntensorflow/python:nn_fused_batchnorm_test\r\ntensorflow/python:nn_fused_batchnorm_test\r\ntensorflow/python/ops/parallel_for:control_flow_ops_test\r\ntensorflow/python/ops/parallel_for:control_flow_ops_test\r\n\r\nThe error is like (use tensorflow/contrib/quantize:fold_batch_norms_test for example):\r\n```\r\n[ RUN      ] FoldBatchNormsTest.testFoldAtrousConv2d\r\nF0515 09:39:44.230122    9537 shape_inference.cc:168] Check failed: output(i).IsSet() 5 for test/BatchNorm/FusedBatchNormV3 of type FusedBatchNormV3\r\n```", "@aaroey , I cannot repro this error. I focused on `tensorflow/contrib/quantize:fold_batch_norms_test`. What I have tried are (1) directly run python script; (2) bazel test it (CUDA); (3) bazel test it (Non-CUDA). All of them got passed.\r\n\r\nFrom the log you pasted, it seems the error is still related to the 5th output of FBNV3, which becomes a nullptr. \r\n\r\nMy theory is that in some context, the TF will check if the output is nullptr or not *before* conducting the actual computation (like here https://github.com/tensorflow/tensorflow/blob/e086a09d3a549a98b192404ae1e34e373dc367d3/tensorflow/core/framework/shape_inference.cc#L166). But our strategy of allocating in destructor can only eliminate the 5th nullptr *after* the computation when the ScratchAllocator is no longer needed. \r\n\r\nSo, my guess is that we still need to allocate the dummy 5th output outside the ScratchAllocator and before the computation. What is your opinion? ", "@houtoms my understanding is that the shape inference code has nothing to do with op kernels. I think the cause of the problem is that we used [FusedBatchNormShape](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/common_shape_fns.cc#L873) when we register the new v3 op in tensorflow/core/ops/nn_ops.cc, which doesn't set the 5th input. We need to modify that shape function somehow to make that happen.", "@aaroey I see. I made changes to call the FusedBatchNormV3Shape for the FBNV3.", "@aaroey There are one more tests failed. Can you help check the logs?", "Sure @houtoms.", "One problem I found in \"Windows Bazel GPU\" test is `tensorflow/stream_executor/cuda/cuda_dnn.cc(46): fatal error C1083: Cannot open include file: 'third_party/gpus/cudnn/cudnn.h': No such file or directory`. But I don't think this is related to this PR.", "@aaroey Any new findings?", "@houtoms there is still some test failures locally. I'm trying to fix them. Meanwhile please don't push any new commits, and I'll let you know for any news, thanks!", "Hi @yifeif, this seems to be another instance of merged-but-not-shown problem. Do you know how to mark this as merged?", "@aaroey looks like it didn't get marked as merged because the commit history has changed. You can do a rollback on the cl and then re-import (which should have the updated commit history)."]}, {"number": 27166, "title": "[TF 2.0 API Docs] tf.bitcast and tf.bitwise.*", "body": "**System information**\r\n- TensorFlow version: 2.0\r\n- Doc Link: \r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/bitcast\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/bitwise/bitwise_and\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/bitwise/bitwise_or\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/bitwise/bitwise_xor\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/bitwise/bitwise_invert\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/bitwise/right_shift\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/bitwise/left_shift\r\n\r\n**Describe the documentation issue**\r\n* Correct links? No\r\n* Clear Description?  Yes\r\n* Usage Example? No\r\n* Parameters Defined? Yes\r\n* Returns defined? Yes\r\n* Raises listed and defined? No\r\n* Visuals, if applicable? No\r\n\r\nsame as #26492, #26532 documentation for tf.bitcast and tf.bitwise.* is created from a generated file, python/ops/gen_array_ops.py, and made incorrect link to source code.\r\nThe documentation can be modified by editing the appropriate .pbtxt files within the tensorflow/tensorflow/core/api_def/base_api directory of the source repository.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nyes", "comments": ["Ya link is incorrect in the documentation, but whats the correct location where i can add some example in tf.bitcast", "you can find *.pbtxt here tensorflow/tensorflow/core/api_def/base_api/\r\napi_def_Bitcast.pbtxt\r\napi_def_BitwiseAnd.pbtxt\r\napi_def_BitwiseOr.pbtxt\r\napi_def_BitwiseXor.pbtxt\r\n\r\nyou can see @dynamicwebpaige [comment](https://github.com/tensorflow/tensorflow/pull/26017#issuecomment-466734466)", "Ok thanks", "Closing this issue since the associated PR has been merged. Feel free to reopen if the problem still persists. Thanks!"]}, {"number": 27165, "title": "Error when compiling a keras model with top_k_categorical_accuracy metrics", "body": "I am trying to compile a KERAS model for multi-class segmentation of 2D images with top_k_categorical_accuracy metrics, i.e.\r\n`model.compile(optimizer='adadelta', \r\n              loss='categorical_crossentropy', \r\n              metrics=['accuracy', 'top_k_categorical_accuracy'])`\r\nThe error I get is:\r\n\r\n> ValueError: Shape must be rank 2 but is rank 4 for 'metrics_2/top_k_categorical_accuracy/in_top_k/InTopKV2' (op: 'InTopKV2') with input shapes: [?,?,?,10], [?,?,?], [].\r\n\r\nI am using one-hot encoding for the labels (10 is number of classes).\r\nInterestingly, trying to use sparse_top_k_categorical_accuracy (which I am not supposed to use given my labels are not flattened) gives exactly the same error... \r\n\r\nI am using tensorflow 1.10.0\r\n\r\nDoes top_k_categorical_accuracy has any assumptions I am not aware of?  How can I make top_k_categorical_accuracy work for my model?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 27164, "title": "Fix label for loaded package nccl", "body": "Fixes:\r\n$ bazel build //tensorflow/examples/tutorials/word2vec:word2vec_basic\r\nERROR: error loading package '': in /home/nicholas/tensorflow/tensorflow/workspace.bzl: Label '//third_party:nccl/nccl_configure.bzl' crosses boundary of subpackage 'third_party/nccl' (perhaps you meant to put the colon here: '//third_party/nccl:nccl_configure.bzl'?)\r\nERROR: error loading package '': in /home/nicholas/tensorflow/tensorflow/workspace.bzl: Label '//third_party:nccl/nccl_configure.bzl' crosses boundary of subpackage 'third_party/nccl' (perhaps you meant to put the colon here: '//third_party/nccl:nccl_configure.bzl'?)\r\nINFO: Elapsed time: 0.955s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)", "comments": ["This build break was fixed in https://cr/240489664 aka 2243bd6ba9b36d43dbd5c0ede313853f187f5dce."]}, {"number": 27163, "title": "std::terminate may be called when environment is misconfigured while using C API", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: N/A, C API is used\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): 6.4.0/5.3.0\r\n- CUDA/cuDNN version: libcuda comes from 418.43 drivers, cudnn 7.3.1\r\n- GPU model and memory: nvidia, model/memory irrelevant\r\n\r\n**Describe the current behavior**\r\n\r\nWhen attempting to run a session via C API on a TensorRT-using graph, it is possible for exceptions to not get caught and abort the whole process via `std::terminate`. Stack-trace of one such exception, which happens when cudnn initialization fails, is provided below. Then the execution fails with:\r\n\r\n```\r\nterminate called after throwing an instance of 'nvinfer1::CudaError'\r\n  what():  std::exception\r\nfish: \u201c./target/debug/foobar\" terminated by signal SIGABRT (Abort)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect C API for `TF_SessionRun` and similar to return an error and caller to have an option to handle the error in some way that does not involve crashing.\r\n\r\n**Code to reproduce the issue**\r\n\r\nN/A, this is a failure of libtensorflow to return an error for system configuration issue.\r\n\r\n**Other info / logs**\r\n\r\n```\r\n#0  0x00007fffca5d7b60 in __cxa_throw () from libstdc++.so.6\r\n#1  0x00007fffc494fc23 in nvinfer1::throwCudaError(char const*, char const*, int, int, char const*) () from libnvinfer.so.5\r\n#2  0x00007fffc495578e in nvinfer1::rt::initializeCommonContext(nvinfer1::rt::CommonContext&, nvinfer1::IGpuAllocator&, void*, void*, int) () from libnvinfer.so.5\r\n#3  0x00007fffc495a4d4 in nvinfer1::rt::Engine::initialize() () from libnvinfer.so.5\r\n#4  0x00007fffc495dc04 in nvinfer1::rt::Engine::deserialize(void const*, unsigned long, nvinfer1::IGpuAllocator&, nvinfer1::IPluginFactory*) () from libnvinfer.so.5\r\n#5  0x00007fffc4946b33 in nvinfer1::Runtime::deserializeCudaEngine(void const*, unsigned long, nvinfer1::IPluginFactory*) () from libnvinfer.so.5\r\n#6  0x00007fffd4d001e1 in tensorflow::tensorrt::TRTEngineOp::GetEngine(int, tensorflow::OpKernelContext*) () from libtftrt.so\r\n#7  0x00007fffd4d03b8b in tensorflow::tensorrt::TRTEngineOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>) () from libtftrt.so\r\n#8  0x00007fffd3bd9f11 in tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>) () from libtensorflow_framework.so\r\n#9  0x00007fffd3c20fbb in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) () from libtensorflow_framework.so\r\n#10 0x00007fffd3c212ef in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(absl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8ul, std::allocator<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode> > const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from libtensorflow_framework.so\r\n#11 0x00007fffd3c8a4c1 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) () from libtensorflow_framework.so\r\n#12 0x00007fffd3c880e7 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from libtensorflow_framework.so\r\n#13 0x00007fffca6026df in ?? () from libstdc++.so.6\r\n#14 0x00007fffd5b11244 in start_thread () from libpthread.so.0\r\n#15 0x00007fffd563bccf in clone () from libc.so.6\r\n```", "comments": ["@azaks2 who is looking at tensorrt these days?", "@pooyadavoodi @trevor-m any thoughts?", "Can you solve the problem?", "@aaroey Should TF-TRT try to catch exceptions from TRT library functions? Or this is more of TRT problem since they are not catching their exceptions?", "@trevor-m I think we should fix the problem in TRT library. In general TF prevents using exceptions.", "@nagisa \r\nAs 1.x is not supported actively now, could you please confirm if this is still an issue in 2.x", "No longer using tensorflow so somebody else will need to do so. Either way\nunwinding into FFI should be apparent, if there's any left, through basic\ncode review of the bindings.\n\nOn Wed, 20 Oct 2021, 19:52 Saduf2019, ***@***.***> wrote:\n\n> @nagisa <https://github.com/nagisa>\n> As 1.x is not supported actively now, could you please confirm if this is\n> still an issue in 2.x\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27163#issuecomment-947851554>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAFFZUVQBPYOZTLTO5CJ42LUH3XVFANCNFSM4HBRQ5TA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "@nagisa \r\nIn that case can you please move this to closed status.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27163\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27163\">No</a>\n"]}, {"number": 27162, "title": "[ROCm] Support HCC based on Clang9 from ROCm2.2", "body": "This PR is to add support of HCC compiler based on Clang-9.0, which has been included in the ROCm2.2 release. \r\nCC @parallelo , @deven-amd and @whchung ", "comments": []}, {"number": 27161, "title": "Documentation Request: WALSModel with shard", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: script built around docs\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: conda gpu version\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: 10\r\n- **GPU model and memory**: Quadro 16GiB\r\n- **Exact command to reproduce**: `ex.run()` (see comments below)\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThis request comes with an associated [colab](https://colab.research.google.com/drive/1oJDfvIWnf7sY5uFcJ7XhTSEdA1n_3Jxv#scrollTo=8pqa0A7BUrG-)\r\n\r\nFurther, this documentation request stems from @walidk 's response on issue #26928 where \r\nit is suggested to shard the input matrix based on [wals_test.py](https://github.com/tensorflow/tensorflow/blob/f07558116ac7c90858cf0572a1bca1e50e208a37/tensorflow/contrib/factorization/python/ops/wals_test.py#L141)\r\n\r\nWhile the code is relevant, it is not necessarily clear how to apply it to the [colab](https://colab.research.google.com/drive/1oJDfvIWnf7sY5uFcJ7XhTSEdA1n_3Jxv#scrollTo=8pqa0A7BUrG-)\r\n\r\nWhy?\r\n\r\n\r\n1. there is `WALSModel` (used in the [colab](https://colab.research.google.com/drive/1oJDfvIWnf7sY5uFcJ7XhTSEdA1n_3Jxv#scrollTo=8pqa0A7BUrG-) ) and `WALSMatrixFactorization` (used in the linked test code). They seem the same, but what is the difference? Is that code compatible with `WALSModel`?\r\n\r\n2. both `WALSModel` and `WALSMatrixFactorization` have `num_row_shards=1` / `num_col_shards=1` as arguments. It is unclear if these would automatically shard the input matrix.\r\n\r\n3. as test code, it is not necessarily commented for pedagogical purposes\r\n\r\n4. as test code, it is not exactly a MWE demonstrating just what is needed to handle shard an input matrix and train accordingly. Should both rows and cols be sharded at the same time? \r\n\r\n5. part of the test code may work against the goal of sharding (or it is also likely I do not full understand it):\r\n\r\ne.g.\r\n\r\n```python\r\n....\r\n def _fn():\r\n      ...\r\n      sp_mat = self.np_array_to_sparse(np_matrix)\r\n      sp_mat_t = sparse_ops.sparse_transpose(sp_mat) #<--- if goal is to shard input matrix because it is too large, why make 2?\r\n      ...\r\n...\r\n```\r\n\r\nI understand that this request it is a bit of an \"advanced\"  documentation and that those contributing to TF are very busy - especially with v2. However I feel like this is a valid request as:\r\n\r\n1. the current documentation for `WALSModel` and `WALSMatrixFactorization` is already more than MWE including session supervisors (although leaning more towards pseudo code in some places which may make it more confusing than need be)\r\n\r\n2. the current documentation for the built in arguments `num_row_shards` / `col` state \"number of shards to use\", which gives no indication that the linked test code is even needed or where it should be introduced\r\n\r\n3. the current documentation hints at sharding (e.g. `transposed_matrix_slices_from_queue_for_worker_shard`) but assumes the user knows how to set that up.\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@walidk I started a [colab](https://colab.research.google.com/drive/1xGe3lRiKnwPuhc1FfgdZiayEE8rQQwvM) trying to fill out / finish the pseudo-code for `WALSModel`\r\n", "One of the motivations for the request of WALS implementation in TF 2.0 I did few days ago in #27107 is precisely because of this (in the part I point to distribute the implementation where possible).\r\nIt will be great to try to solve this as it will smooth the move of this method to TF 2.0... ", "@walidk anything?", "Since `contrib` has been depreciated in `Tensorflow 2.x `and `WALS` is not supported as well in `TF 2.x`, you could try [Neural Collaborative Filtering (NCF)](https://github.com/tensorflow/models/tree/08bb9eb5ad79e6bceffc71aeea6af809cc78694b/official/recommendation) which is similar to that.\r\nFeel free to open a new request if you have any issues."]}, {"number": 27160, "title": "Fuse reordering of dot contracting dimensions into constant input", "body": "The basic idea of this algsimp optimization is that dot(reorder_column(A), constant1) can be replaced by dot(A, reorder_row(constant1) and then reorder_row(constant1) can be computed at compile time. reorder_row is an inverse operation of reorder_column.", "comments": ["Some implementation note can be found in [here](https://docs.google.com/document/d/10fL_YrO_ZVEHTlkoQRXJ8cwO-FyAhfVGHuKoDl6_hsw/edit?usp=sharing).", "@jlebar This implements the idea of dot + transpose folding you mentioned before.", "@BinFan can you please resolve conflicts.", "@BinFan are you ready for another round of reviews here, or are you still working on it?", "> @BinFan are you ready for another round of reviews here, or are you still working on it?\r\n\r\nJust fixed the merge conflicts. I think it is ready for another round of review. Thanks!", "> After pondering over this for a while, I'm still finding it pretty hard to grok.\r\n> \r\n> I wonder if a simpler way of doing this would be a kind of compromise between the original proposal and the proposal of supporting multiple contracting dims in XLA:GPU.\r\n> \r\n> The idea I have is, do everything inside of AlgebraicSimplifier just like you're doing here, but have a series of transformations:\r\n> \r\n> * If the input is `dot(constant, rhs)`, _create a new instruction_ `dot(rhs, constant)` with swapped dnums, but don't add it to the HloComputation.  Continue below but operate only on the new dot.\r\n> * Pattern match `Dot(Reshape(Transpose(input)), constant)`.\r\n> * Check that the reshape merely squishes together N dimensions, and that this new big dimension is the LHS contracting dim of the dot.  If so, create a new dot with multiple contracting dimensions, but don't add it to the HloComputation.  If not, bail.\r\n> * Check that the transpose reorders these contracting dims.  If so, create a new dot with reordered dnums.  Again, don't add it to the HloComputation.  (I'll omit this now, you get the picture.)  Now we have `dot(lhs, constant), lhs_contracting_dims=<a,b,c>, rhs_contracting_dims=<x,y,z>`.\r\n> * Reorder the contracting dims so that the LHS contracting dims are consecutive.  Again, create a new dot.\r\n> * Do the inverse transformations on the RHS.\r\n> \r\n> As a simplification to the above, I don't think it's actually necessary to create new HloInstructions. We can merely _imagine_ that we're doing so. All we need to do in actuality is keep track of this imaginary dot's LHS, RHS, and dnums.\r\n> \r\n> As a sketch:\r\n> \r\n> ```\r\n> StatusOr<HloInstruction*>\r\n> AlgebraicSimplifierVisitor::OptimizeDotOfReorderContractingDims(\r\n>     HloInstruction* dot) {\r\n>   auto* lhs = dot->operand(0);\r\n>   auto* rhs = dot->operand(1);\r\n>   auto dnums = dot->dot_dimension_numbers();\r\n> \r\n>   // Canonicalize dot(<constant>, rhs) to dot(rhs, <constant>) to make the\r\n>   // remainder of this function easier.\r\n>   if (dot->operand(0)->IsConstant()) {\r\n>     using std::swap;\r\n>     swap(lhs, rhs);\r\n>     swap(dnums.lhs_contracting_dimensions(),\r\n>          dnums.rhs_contracting_dimensions());\r\n>     swap(dnums.lhs_batch_dimensions(), dnums.rhs_batch_dimensions());\r\n>   }\r\n> \r\n>   // Pattern match dot(reshape(transpose(input)), constant).\r\n>   HloInstruction* reshape;\r\n>   HloInstruction* transpose;\r\n>   HloInstruction* input;\r\n>   if (!Match(lhs, m::Reshape(&reshape,\r\n>                              m::Transpose(&transpose,\r\n>                                           m::Op(&input).IsNonConstant()))) ||\r\n>       !rhs->IsConstant()) {\r\n>     return nullptr;\r\n>   }\r\n> \r\n>   // Check that the reshape squishes some number of dimensions down to one dim,\r\n>   // and that this one dim is the dot's LHS contracting dimension.  If so, the\r\n>   // dot is equivalent to a new dot with multiple contracting dims.\r\n> \r\n>   // if (...) { return nullptr; }\r\n>   lhs = lhs->operand(0);\r\n>   // modify dnums appropriately.\r\n> \r\n>   // Check that the transpose reorders only LHS contracting dimensions.\r\n>   // if (...) { return nullptr; }\r\n>   lhs = lhs->operand(0);\r\n>   // modify dnums appropriately.\r\n> \r\n>   // (If we get here, we know the transformation will succeed.)\r\n> \r\n>   // Permute contracting dims so that the LHS contracting dims are sorted.\r\n>   // ...\r\n>   \r\n>   // Add transpose to RHS.\r\n>   // Add reshape to RHS.\r\n> }\r\n> ```\r\n> WDYT?\r\n> \r\n> You may also find useful the `Permute`, `InversePermutation`, and `ComposePermutations` functions in util.h.\r\n\r\nThanks a lot for the comments. Your suggested implementation is indeed better organized. My implementation is more or less aligned with the comment of \"keep track of this imaginary dot's LHS, RHS, and dnums\". For example, is_eligible_reorder_op returns the contracting dim indices of the imaginary dot after handling transpose, and we use that to compute the inverted shape of transpose and reshape on RHS. If we break the is_eligible_reorder_op function to check reshape and transpose separately, then the flow is very like the one you suggest. I agree that the implementation of tracking the original reshape and transpose dims and computing of the inverted dims in this PR is not very clear. I'll try to clean it up by the helper functions you point.\r\n\r\nSome extra questions: \r\n1. When canonicalizing the dot, if we swap the lhs and rhs, shouldn't we also add a transpose? Or I guess you mean that after this algsimp optimization is done, if we have swapped lhs and rhs, we swap it back?\r\n2. You example implies that the number of contracting dims in the dot is 1. So after we sort the LHS contracting dims, should we add a reshape so that the number of LHS contracting dim is 1? I think it is not hard to support multiple contracting dimensions. Do we have a specification? For example, \r\n- Should they always compact? Is lhs_contracting_dims=<0,2> allowed?\r\n- Should they always consecutive? Is lhs_contracting_dims=<2,1> allowed?\r\n- Should the lhs and rhs order always match? Is lhs_contracting_dims=<2,1> and rhs_contracting_dims=<0,1> allowed, is lhs_contracting_dims=<1,2> and rhs_contracting_dims=<0> allowed?\r\nBut on the other hand, I think multiple contracting dimensions imply transpose and reshape. I'm wondering if we see such optimization opportunity, should we just \"push out\" the multiple contracting dims so that we still have a single contract dim?", "> When canonicalizing the dot, if we swap the lhs and rhs, shouldn't we also add a transpose? Or I guess you mean that after this algsimp optimization is done, if we have swapped lhs and rhs, we swap it back?\r\n\r\nYeah, I forgot about that transpose on the output.  But a comment to this effect is probably all we'd need.\r\n\r\n> You example implies that the number of contracting dims in the dot is 1. So after we sort the LHS contracting dims, should we add a reshape so that the number of LHS contracting dim is 1?\r\n\r\nYes, I missed that too.\r\n\r\n> I think it is not hard to support multiple contracting dimensions. Do we have a specification? For example, Should they always compact? Is lhs_contracting_dims=<0,2> allowed?\r\n\r\nThe restrictions on HLO are specified in shape_inference.cc.\r\n\r\nBackends may have additional restrictions, but they need to canonicalize input HLO to meet those restrictions.  For example, XLA:nvgpu might have the restriction that the contracting dims must be compact in the physical layout.  (This would mean that lhs_contracting_dims=<0,2> is allowed if and only if in the physical layout, dims 0 and 2 are adjacent.)\r\n\r\n> But on the other hand, I think multiple contracting dimensions imply transpose and reshape. I'm wondering if we see such optimization opportunity, should we just \"push out\" the multiple contracting dims so that we still have a single contract dim?\r\n\r\nIt seems to me that in the ideal situation, we \"push *in*\" transpose/reshape into the kDot operation.  The reason is, if I have `kDot lhs_contracting_dims=<0,2>, rhs_contracting_dims=<3,2>`, then an ideal layout assignment would understand that the restrictions here are:\r\n\r\n - LHS dimensions 0 and 2 must be either most minor or most major, but may be in any order.\r\n - RHS dimensions 2 and 3 must be either most minor or most major, but may be in any order.\r\n - The physical order of LHS and RHS contracting dims must match, i.e. must be `0,2 / 3,2` or `2,0 / 2,3`.\r\n\r\nThis last bullet point gives layout assignment flexibility that it wouldn't have if we spelled the dot using transposes and reshapes, where the *logical* order of the dimensions affects the final *physical* order.\r\n\r\nOn the other hand, layout assignment does not know how to utilize flexibility of this kind today, and I don't have benchmarks where it matters, other than this example with a constant on one side or the other.  So I am OK with hacking this into algsimp rather than going whole hog and fixing layout assignment.  OTOH I'd also be OK scrapping this approach and digging into layout assn.", "> > But on the other hand, I think multiple contracting dimensions imply transpose and reshape. I'm wondering if we see such optimization opportunity, should we just \"push out\" the multiple contracting dims so that we still have a single contract dim?\r\n> \r\n> It seems to me that in the ideal situation, we \"push _in_\" transpose/reshape into the kDot operation. The reason is, if I have `kDot lhs_contracting_dims=<0,2>, rhs_contracting_dims=<3,2>`, then an ideal layout assignment would understand that the restrictions here are:\r\n> \r\n> * LHS dimensions 0 and 2 must be either most minor or most major, but may be in any order.\r\n> * RHS dimensions 2 and 3 must be either most minor or most major, but may be in any order.\r\n> * The physical order of LHS and RHS contracting dims must match, i.e. must be `0,2 / 3,2` or `2,0 / 2,3`.\r\n> \r\n> This last bullet point gives layout assignment flexibility that it wouldn't have if we spelled the dot using transposes and reshapes, where the _logical_ order of the dimensions affects the final _physical_ order.\r\n> \r\n> On the other hand, layout assignment does not know how to utilize flexibility of this kind today, and I don't have benchmarks where it matters, other than this example with a constant on one side or the other. So I am OK with hacking this into algsimp rather than going whole hog and fixing layout assignment. OTOH I'd also be OK scrapping this approach and digging into layout assn.\r\n\r\nI think I finally understand the three bullet points in the above. Shuffling the physical order of LHS and RHS contracting dimensions can be imagined as reordering as well. So the flexibility of the 3rd point still comes from the fact that we can reorder the elements in contracting dimension on both side, as long as both side reorder in the same way, right? \r\n\r\nI studied the DotDecomposer pass and shape inference code for Dot, and now I understand the canonical format of the Dot and contracting dims. I will still be pursuing the single contracting dimensions assumption in algsimp.", "> So the flexibility of the 3rd point still comes from the fact that we can\nreorder the elements in contracting dimension on both side, as long as both\nside reorder in the same way, right?\n\nI think you've got it, yes!\n\nOn Thu, Apr 11, 2019 at 3:50 PM Bin Fan <notifications@github.com> wrote:\n\n> But on the other hand, I think multiple contracting dimensions imply\n> transpose and reshape. I'm wondering if we see such optimization\n> opportunity, should we just \"push out\" the multiple contracting dims so\n> that we still have a single contract dim?\n>\n> It seems to me that in the ideal situation, we \"push *in*\"\n> transpose/reshape into the kDot operation. The reason is, if I have kDot\n> lhs_contracting_dims=<0,2>, rhs_contracting_dims=<3,2>, then an ideal\n> layout assignment would understand that the restrictions here are:\n>\n>    - LHS dimensions 0 and 2 must be either most minor or most major, but\n>    may be in any order.\n>    - RHS dimensions 2 and 3 must be either most minor or most major, but\n>    may be in any order.\n>    - The physical order of LHS and RHS contracting dims must match, i.e.\n>    must be 0,2 / 3,2 or 2,0 / 2,3.\n>\n> This last bullet point gives layout assignment flexibility that it\n> wouldn't have if we spelled the dot using transposes and reshapes, where\n> the *logical* order of the dimensions affects the final *physical* order.\n>\n> On the other hand, layout assignment does not know how to utilize\n> flexibility of this kind today, and I don't have benchmarks where it\n> matters, other than this example with a constant on one side or the other.\n> So I am OK with hacking this into algsimp rather than going whole hog and\n> fixing layout assignment. OTOH I'd also be OK scrapping this approach and\n> digging into layout assn.\n>\n> I think I finally understand the three bullet points in the above.\n> Shuffling the physical order of LHS and RHS contracting dimensions can be\n> imagined as reordering as well. So the flexibility of the 3rd point still\n> comes from the fact that we can reorder the elements in contracting\n> dimension on both side, as long as both side reorder in the same way, right?\n>\n> I studied the DotDecomposer pass and shape inference code for Dot, and now\n> I understand the canonical format of the Dot and contracting dims. I will\n> still be pursuing the single contracting dimensions assumption in algsimp.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27160#issuecomment-482354905>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAJMhy9ckiNav0QB_dCdGaMkTDJnnkD5ks5vf7xAgaJpZM4cMNW5>\n> .\n>\n", "Just pushed a new revision. It is ready for another round of review. Thanks!", "A new revision is ready.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27160) for more info**.\n\n<!-- need_author_consent -->", "Something went wrong with my latest update. Trying to figure out what happend.", "absl::Span and RepeatedField should play nicely with each other.  Offhand,\nyou may need to use absl::MakeSpan and/or ensure that you have a\n`Span<const T>` rather than `const Span<T>`.  If it's not RepeatedField but\nRepeatedPtrField, that is more unfortunate, but I don't think that's the\ncase here.\n\nIf this absolutely cannot be simplified, perhaps it could be encapsulated\nin a separate function.  That might clarify the invariants.\n\nOn Thu, Apr 25, 2019 at 4:30 PM Bin Fan <notifications@github.com> wrote:\n\n> *@BinFan* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/compiler/xla/service/algebraic_simplifier.cc\n> <https://github.com/tensorflow/tensorflow/pull/27160#discussion_r278765840>\n> :\n>\n> > +  const auto& transpose_dims = lhs->dimensions();\n> +  for (int64 i = 0; i < transpose_dims.size(); ++i) {\n> +    if (transpose_dims[i] != i &&\n> +        !(absl::c_linear_search(lhs_contracting_dims, i) &&\n> +          absl::c_linear_search(lhs_contracting_dims, transpose_dims[i]))) {\n> +      return nullptr;\n> +    }\n> +  }\n> +  // Virtually pull the transpose into the dot. Now the dot is equivalent to\n> +  // a new dot with new lhs transpose dims equal to the transpose dims within\n> +  // the old lhs contracting dims.\n> +  auto old_lhs_contracting_dims = lhs_contracting_dims;\n> +  lhs_contracting_dims.Clear();\n> +  for (const auto dim : old_lhs_contracting_dims) {\n> +    lhs_contracting_dims.Add(lhs->dimensions(dim));\n> +  }\n>\n> I keep the implement here and add a comment. I do not use helper function\n> here since it would need some change, and also the helper function expect a\n> absl::Span and it is not very friendly to RepeatedField.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27160#discussion_r278765840>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABEZB4KYOZCQDH74LM5QCDPSI5IFANCNFSM4HBQ2W4Q>\n> .\n>\n", "My previous push probably screwed up the history. Let me close this PR for now while trying to fix the issue.", "> absl::Span and RepeatedField should play nicely with each other. Offhand, you may need to use absl::MakeSpan and/or ensure that you have a `Span<const T>` rather than `const Span<T>`. If it's not RepeatedField but RepeatedPtrField, that is more unfortunate, but I don't think that's the case here. If this absolutely cannot be simplified, perhaps it could be encapsulated in a separate function. That might clarify the invariants.\r\n\r\nIt seems that the reason I could not pass lhs_contracting_dims into foo(absl::Span<const int64> x) is because elements of lhs_contracting_dimensions is of type ::google::protobuf::int64, which is a long int type, but int64 defined in [integral_types.h]([https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/integral_types.h]) is long long int type. @jlebar Any trick to workaround this? I could create a function InputComposePermutations do something like this:\r\n```\r\nusing pbint64 = ::google::protobuf::int64;\r\nvoid InplaceComposePermutations(absl::Span<const pbint64>, absl::Span<const int64>);\r\nInplaceComposePermutations(lhs_contracting_dims, transpose_dims);\r\n```\r\nbut it does not look nice. Or would it simpler that we copy out lhs and rhs contracting dims into std::vector<int64> in the beginning and then use vectors throughout the implementation? We don't really update the dot contracting dims anyway.", "I did not find a way to fix the log history issue in this PR, so I moved this PR to [28170](https://github.com/tensorflow/tensorflow/pull/28170)."]}, {"number": 27159, "title": "Update README.md", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27159) for more info**.\n\n<!-- need_sender_cla -->", "@suvhradipghosh07 please sign CLA ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n"]}, {"number": 27158, "title": "[cherrypick] Move bazel.rc to workspace root to support bazel-0.18.0", "body": "Bazel 0.18.0 will contain a change for which rc files it accepts.\r\nhttps://github.com/bazelbuild/bazel/commit/ec83598cb6ee4136166bb562a24dc5dfa58921db\r\nhttps://github.com/bazelbuild/bazel/issues/4502\r\n\r\nOld bazel used to read %workspace%/tools/bazel.rc. New bazel will not\r\nread that and instead will only read %workspace%/.bazelrc.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27158) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27158) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 27157, "title": "Added support for list inputs", "body": " ```y_pred.shape.ndims``` -> Not supports **list** as y_pred variable\r\n Whereas ```array_ops.rank(y_pred).numpy()```  works for all (Tensors, Numpy arrays, lists)\r\n\r\nFrom #27150", "comments": ["> I think I have added the test case, any other thing that needed to be added? @pavithrasv\r\n> \r\n> Waiting eagerly for my first PR to merge in TF\r\n\r\nI am not seeing the test file in the PR. Did you forget to add that? ", "> > I think I have added the test case, any other thing that needed to be added? @pavithrasv\r\n> > Waiting eagerly for my first PR to merge in TF\r\n> \r\n> I am not seeing the test file in the PR. Did you forget to add that?\r\n\r\nyes, I forget to add the test file. Do I need to make a new PR with all these included in it?", "> > > I think I have added the test case, any other thing that needed to be added? @pavithrasv\r\n> > > Waiting eagerly for my first PR to merge in TF\r\n> > \r\n> > \r\n> > I am not seeing the test file in the PR. Did you forget to add that?\r\n> \r\n> yes, I forget to add the test file. Do I need to make a new PR with all these included in it?\r\n\r\nNo, you can just add it here.", "Thanks for the help @pavithrasv , I have addded the test file and made the changes. :smile: \r\nPreviously in the test file the list was converted into the tensor and then passed into the update_state method but now they are passed straight away to test the update_state of MeanIoU.", "ready to be reviewed:) @pavithrasv "]}, {"number": 27156, "title": "TFLite MicroInterpreter Crashes/Seg Faults", "body": "**System information**\r\n- Have I written custom code: Yes (custom model and custom code)\r\n- OS Platform and Distribution: Mac OS X 10.13.6\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.0-11117-gca1f0ce1b9 1.14.1-dev20190326\r\n- Python version: 3.7.2\r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.10.44.4)\r\n\r\n**Describe the current behavior**\r\n`./demo`\r\n\r\n```\r\nModel loaded!, Size:6052, Version:3\r\nModel description: TOCO Converted.\r\nTensor arena allocated on stack\r\nAllocator working (ish): Size=0\r\nSegmentation fault: 11\r\n```\r\n\r\nThe code fails when running this line: https://github.com/kmader/tflite_micro/blob/278c2899f05cc721133829d75f9e88ed87a3d33f/simple_app/demo.cc#L59-L60\r\n\r\n**Describe the expected behavior**\r\n`./demo`\r\n\r\n```\r\nModel loaded!, Size:6052, Version:3\r\nModel description: TOCO Converted.\r\nTensor arena allocated on stack\r\nAllocator working (ish): Size=0\r\nInterpreter loaded!\r\nInput #0: 1, 49, 40, 1, Type:UInt8, 3\r\nOutput #0: 1, 4, Type:UInt8, 3\r\nBad input tensor parameters in model\r\nModel Results: [0]=0.411765, [1]=0.223529, [2]=0.160784, [3]=0.207843, \r\n```\r\n\r\n**Code to reproduce the issue**\r\nThe repository has been simplified and binderized at https://github.com/kmader/tflite_micro/tree/master/simple_app. I have it setup so you can easily change the model from the example speech model `g_tiny_conv_micro_features_model_data` to a the model I am interested in `local_min_cnn_tflite`. The `demo.cc` is a modified and simplified version of https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/examples/micro_speech/main.cc\r\n\r\n\r\n**Other info / logs**\r\nA brief look at the lldb output suggests the problem might be related to parsing the https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/examples/micro_speech/main.cc g related to:\r\n```\r\n* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x200015b10)\r\n    frame #0: 0x000000010000463b demo`tflite::SimpleTensorAllocator::AllocateTensor(tflite::Tensor const&, int, int, flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer> > const*, tflite::ErrorReporter*, TfLiteTensor*) [inlined] flatbuffers::IndirectHelper<long long>::Read(i=0) at flatbuffers.h:68 [opt]\r\n   65  \t  typedef T mutable_return_type;\r\n   66  \t  static const size_t element_stride = sizeof(T);\r\n   67  \t  static return_type Read(const uint8_t *p, uoffset_t i) {\r\n-> 68  \t    return EndianScalar((reinterpret_cast<const T *>(p))[i]);\r\n   69  \t  }\r\n   70  \t};\r\n   71  \ttemplate<typename T> struct IndirectHelper<Offset<T>> {\r\nTarget 0: (demo) stopped.\r\n```\r\n- And the backtrace being\r\n```\r\n* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x200015b10)\r\n  * frame #0: 0x000000010000463b demo`tflite::SimpleTensorAllocator::AllocateTensor(tflite::Tensor const&, int, int, flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer> > const*, tflite::ErrorReporter*, TfLiteTensor*) [inlined] flatbuffers::IndirectHelper<long long>::Read(i=0) at flatbuffers.h:68 [opt]\r\n    frame #1: 0x000000010000463b demo`tflite::SimpleTensorAllocator::AllocateTensor(tflite::Tensor const&, int, int, flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer> > const*, tflite::ErrorReporter*, TfLiteTensor*) [inlined] flatbuffers::Vector<long long>::Get(i=0) const at flatbuffers.h:199 [opt]\r\n    frame #2: 0x000000010000463b demo`tflite::SimpleTensorAllocator::AllocateTensor(this=<unavailable>, flatbuffer_tensor=0x0000000100016280, create_before=<unavailable>, destroy_after=<unavailable>, buffers=0x000000010001585c, error_reporter=0x00007ffeefbf57a8, result=0x00007ffeefbf5840) at simple_tensor_allocator.cc:135 [opt]\r\n    frame #3: 0x00000001000032e5 demo`tflite::MicroInterpreter::MicroInterpreter(this=0x00007ffeefbf36a8, model=<unavailable>, op_resolver=<unavailable>, tensor_allocator=<unavailable>, error_reporter=0x00007ffeefbf57a8) at micro_interpreter.cc:89 [opt]\r\n    frame #4: 0x0000000100000eb8 demo`main(argc=1, argv=0x00007ffeefbff870) at demo.cc:57\r\n    frame #5: 0x00007fff78ccf015 libdyld.dylib`start + 1\r\n```\r\n\r\n", "comments": ["Issue is potentially related to https://github.com/tensorflow/tensorflow/issues/25614 but the model size here is much smaller (even smaller than the micro_speech demo)", "The 'binderized' version can be run by launching the binder and starting a terminal\r\nhttps://mybinder.org/v2/gh/kmader/tflite_micro/master?urlpath=terminals/0\r\nand then changing to the app directory and running `make` to see it fail and `lldb demo` to debug the error\r\n\r\n<img width=\"1141\" alt=\"Screen Shot 2019-03-27 at 10 37 02 AM\" src=\"https://user-images.githubusercontent.com/116120/55066868-f7806900-507e-11e9-9c97-0f2bb25392f1.png\">\r\n", "Could this be a problem where the allocator is too small for the model to fit?", "@wangtz could be, how can I increase it?\r\n", "Try make this larger and try again.", "oop. forgot the link: https://github.com/kmader/tflite_micro/blob/278c2899f05cc721133829d75f9e88ed87a3d33f/simple_app/demo.cc#L53", "Assigning to Tiezhen, thanks for your help!\r\n\r\nAny update @kmader?", "@petewarden @wangtz I have made the change and rebuilt the image (https://github.com/kmader/tflite_micro/pull/2) and the same issue still occurs (lldb also shows it crashing in the same spot)\r\n\r\n```\r\njovyan@jupyter-kmader-2dtflite-5fmicro-2dcn3r5v7y:~/simple_app$ ./demo\r\nModel loaded!, Size:6052, Version:3\r\nModel description: TOCO Converted.\r\nTensor arena allocated on stack\r\nAllocator working (ish): Size=0\r\nSegmentation fault (core dumped)\r\n```", "I have the same exact thing going on.\r\nIs there any additional info that would be useful to someone looking into this?", "Hi Kevin and Ty,\r\n\r\nA few changes have been made into the allocator, is the problem still persist?", "It seems to still be an issue, here is the github workflow which reproduces the seg-fault: https://github.com/kmader/tflite_micro/pull/3/checks?check_run_id=310501334", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27156\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27156\">No</a>\n"]}, {"number": 27155, "title": "Built TF 1.13.1 - libcublas.so.10.1: cannot open shared object file with classify_image.py", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Slackware Linux 64 -current (14.2+)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.2\r\n- Bazel version (if compiling from source): 0.21.0- (@non-git)\r\n- GCC/Compiler version (if compiling from source): gcc-8.3.0-x86_64\r\n- CUDA/cuDNN version: CUDA Version 10.1.105\r\n- GPU model and memory: TITAN X (Pascal) 12 GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nRunning a tutorial sample `tensorflow/models-git/tutorials/image/imagenet]$ python3 classify_image.py` causes error:\r\n\r\n```\r\n2019-03-26 08:22:44.464647: I tensorflow/stream_executor/dso_loader.cc:142] Couldn't open CUDA library libcublas.so.10.1. LD_LIBRARY_PATH: \r\n2019-03-26 08:22:44.465972: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Failed precondition: could not dlopen DSO: libcublas.so.10.1; dlerror: libcublas.so.10.1: cannot open shared object file: No such file or directory\r\nAborted\r\n```\r\n\r\n**Describe the expected behavior**\r\nSample tutorial runs\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n`tensorflow/models-git/tutorials/image/imagenet]$ python3 classify_image.py`\r\n\r\n**Other info / logs**\r\nThis issue seems different from the other hundreds (thousands?) of similar reports. Most other similar issues are due to a mismatch between a binary (pre-compiled) tensorflow and the CUDA version installed by the user.\r\n\r\nIn my case, I successfully compiled TF 1.13.1 from source with GPU support. The tensorflow module loads in python3.\r\n\r\nI can resolve the issue with running this sample by setting `LD_LIBRARY_PATH=/opt/nvidia/cuda/lib64` but this should NOT be necessary because I already configured this path in `/etc/ld.so.conf.d/cuda.conf` and ran `$ ldconfig`.\r\n\r\n```\r\n$ python3\r\nPython 3.7.2 (default, Feb 19 2019, 23:15:48) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>> \r\n```\r\n\r\n```\r\n$ cat /etc/ld.so.conf.d/cuda.conf \r\n/opt/nvidia/cuda/lib64\r\n/opt/nvidia/cuda/extras/CUPTI/lib64\r\n/opt/nvidia/nccl/lib\r\n```\r\n\r\n```\r\n$ ls -al /opt/nvidia/cuda/lib64/libcublas.so.10.1\r\nlrwxrwxrwx 1 root root 23 Mar 26 08:01 /opt/nvidia/cuda/lib64/libcublas.so.10.1 -> libcublas.so.10.1.0.105\r\n```\r\n\r\n```\r\n$ ldconfig -p | grep libcublas\r\n\tlibcublasLt.so.10 (libc6,x86-64) => /opt/nvidia/cuda/lib64/libcublasLt.so.10\r\n\tlibcublasLt.so (libc6,x86-64) => /opt/nvidia/cuda/lib64/libcublasLt.so\r\n\tlibcublas.so.10 (libc6,x86-64) => /opt/nvidia/cuda/lib64/libcublas.so.10\r\n\tlibcublas.so (libc6,x86-64) => /opt/nvidia/cuda/lib64/libcublas.so\r\n```", "comments": ["Same issue running most things with Tensorflow. I need to always set `LD_LIBRARY_PATH=/opt/nvidia/cuda/lib64/` as an env var, otherwise Tensorflow fails to work due to the  libcublas not found error.", "> Same issue running most things with Tensorflow. I need to always set `LD_LIBRARY_PATH=/opt/nvidia/cuda/lib64/` as an env var, otherwise Tensorflow fails to work due to the libcublas not found error.\r\n\r\nExactly the same problem, but running ArchLinux and a GTX1070 Max-Q, I will try this temporary fix though.\r\nEdit: Not working here", "> Exactly the same problem, but running ArchLinux and a GTX1070 Max-Q, I will try this temporary fix though.\r\n> Edit: Not working here\r\n\r\nA couple of things to check:\r\n\r\n- Make sure you're specifying the lib(64) directory where CUDA is installed on your system. I installed CUDA in `/opt/nvidia/cuda` but it may be somewhere else on your system (`/usr/local`, etc)\r\n- I had to create a few symlinks for a few CUDA library names that could not be found to make things work, may be required on your system also:\r\n```\r\nln -s libcublas.so.10.1.0.105 libcublas.so.10.1\r\nln -s libcusolver.so.10.1.105 libcusolver.so.10.1\r\nln -s libcurand.so.10.1.105   libcurand.so.10.1\r\nln -s libcufft.so.10.1.105    libcufft.so.10.1\r\n```\r\n\r\nCreate the symlinks according to what it's looking for/complaining about. If it's looking for `libcublas.so.10` for ex. then the symlink can be something like `ln -s libcublas.so.10.1.0.105 libcublas.so.10`", "Also note for TF 1.13.1 \"TensorFlow GPU binaries are now built against CUDA 10 and TensorRT 5.0\" so they may not work with CUDA 10.1. In my case I compiled TF 1.13.1 from source with CUDA 10.1 installed on my system.", "> Same issue running most things with Tensorflow. I need to always set `LD_LIBRARY_PATH=/opt/nvidia/cuda/lib64/` as an env var, otherwise Tensorflow fails to work due to the libcublas not found error.\r\n\r\nFor my case I found an easier workaround for now:\r\n```\r\nln -s /opt/nvidia/cuda/lib64/libcublas.so.10.1 /usr/lib64/libcublas.so.10.1\r\n```\r\n\r\nAs I document below I found out that it is looking in /usr/lib64 among other places, so I gave it what it wants.\r\nhttps://github.com/tensorflow/tensorflow/pull/26230#issuecomment-480608904", "> For my case I found an easier workaround for now:\r\n> \r\n> ```\r\n> ln -s /opt/nvidia/cuda/lib64/libcublas.so.10.1 /usr/lib64/libcublas.so.10.1\r\n> ```\r\nThat fixed it for me until a better solution comes along, although under Arch the path is a bit different since `cuda` is directly under `/opt/` instead of `/opt/nvidia` :  \r\n```\r\nln -s /opt/cuda/lib64/libcublas.so.10.1 /usr/lib64/libcublas.so.10.1\r\n```  \r\nPackage used for install: `community/python-tensorflow-cuda 1.13.1-4`", "For some reason, when installing Nvidia libraries from their own repositories, libcublas10 actually installs libcublas.so.10.2.XXX rather than 10.1 like it does for every other library in the CUDA Toolkit. I'm struggling to figure out how to get the 10.1 version of libcublas. Pulling my hair out because everything else is showing up.", "@mathemaphysics having the same issue with 10.2 cublas being installed, I assume it is compatible. Did you figure it out? \r\nHaving a problem getting a working pytorch build from source (1.5.0) with 10.2,  looking to find the root cause. ", "@rtarquini I had multiple versions installed. Unfortunately it seems Nvidia's apt repositories for Ubuntu 18.04 mixed up some minor version numbers that worked for some libraries (all of them other than cublas appeared to be okay).\n\nTo be clear the apt install routine for installing 10.1 and 10.2 don't play well and left behind mixed versions.\n\nI had to dig through the libraries and link the right ones and set the path right.\n\nI later upgraded to 11 and it all worked just fine. Using 11 now.", "Hi @edrozenberg !\r\nWe see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27155\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27155\">No</a>\n"]}, {"number": 27154, "title": "SmartReply  Tensorflow Lite ", "body": "I'm visited in github repo on tensorflow but there are showing source code batch files not found when i want try to clone repo...", "comments": ["@jvishnuvardhan This isn't a bug/build issue, right?", "@singh728om Please find instructions [here](https://help.github.com/en/articles/cloning-a-repository) to clone a repo on Github. \r\n\r\n@Ayush517 Correct. Thanks!\r\n\r\nIn future, please post this kind od support questions on [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Thankss..for This..but i know how to clone repo...i tell you just ..one\ntime i visited in Tensorflow lite  repo for auto reply system bit .in .that\nrepo no source code ..showing after cloning n also there were not available\nfor clone i was visited through path n raw file ..but it..didn't\nwork..that's why i discuss..with you sir...sorry..for...posting in git...\n\nOn Wed, 27 Mar 2019, 10:45 pm Vishnuvardhan Janapati, <\nnotifications@github.com> wrote:\n\n> Closed #27154 <https://github.com/tensorflow/tensorflow/issues/27154>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27154#event-2233871955>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Ambal2nFHJjUtPcV6FaHdKHBtYDepYZ1ks5va6c6gaJpZM4cLv4g>\n> .\n>\n", "@singh728om Try it again and let me know which links are not working, and please also post with more details so that we can resolve it faster. Thanks!", "sure...i will..thankss..lot..for..support..me..to ask doubts...\n\nOn Wed, 27 Mar 2019, 11:13 pm Vishnuvardhan Janapati, <\nnotifications@github.com> wrote:\n\n> @singh728om <https://github.com/singh728om> Try it again and let me know\n> which links are not working, and please also post with more details so that\n> we can resolve it faster. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27154#issuecomment-477272404>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AmbalxUidLEqm6x1Wmxw6iRCoYoBiLMwks5va629gaJpZM4cLv4g>\n> .\n>\n"]}, {"number": 27153, "title": "Added initialization operator", "body": "Added the bilinear initialization operator and some minor doc fixes.", "comments": ["Thanks for the contribution. We've [announced](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md) that `tensorflow/contrib` will be removed in a future version of TensorFlow, and so we're not accepting new contributions to that directory. As a result, I'm going to close this PR.\r\n\r\nThe https://github.com/tensorflow/addons project is probably the best place to submit this kind of change, and it is where much of the code from `tensorflow/contrib` has been moved in preparation for TensorFlow 2.0."]}, {"number": 27152, "title": "Error importing tensorflow on windows 10 ( Tensorflow 1.13.1  python3.7.2 )", "body": "System information\r\n\r\nOS Platform and Distribution : Windows 10, 64-bit\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version: 1.13.1\r\nPython version: 3.7.2\r\nInstalled using virtualenv? pip? conda?: pip on a conda env\r\n\r\nI want to run a python file that uses tensorflow, but I keep getting this error.\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\popco\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\popco\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\popco\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\popco\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\popco\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\popco\\DeepLearning\\Science Fair\\progressive_growing_of_gans\\dataset_tool.py\", line 16, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\popco\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\popco\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\popco\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\popco\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\popco\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\popco\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\popco\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'", "comments": ["@zhangtravis Could you follow the instruction provided [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12) to install tensorflow? Please let me know how it progresses. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}]