[{"number": 39769, "title": "RPi Zero Build instructions not working", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/lite/guide/build_rpi#compile_natively_on_raspberry_pi\r\n\r\n## Description of issue (what needs changing):\r\nGuide on natively compiling says \"tested on Raspberry Pi Zero\", but following the instructions on a Raspberry Pi Zero leads to a build for armv7l, not armv6 as specified.\r\nAdding TARGET_ARCH=armv6 to the command would likely work, as has been done in the cross-compile section to separate newer models from the RPi 1 / Zero. However, since me following these cross-compiling instructions resulted in a armv7l target as well (hard-float VFP ABI linking errors on Zero), I directly followed the tips from #30181 to be on the safe side.", "comments": ["@Seneral,\r\n\r\nWe are checking to see if this is still an issue, Can you try following this [guide](https://github.com/cloudwiser/TensorFlowLiteRPIZero) as a reference and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39769\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39769\">No</a>\n"]}, {"number": 39768, "title": "Tensorflow 1.x not available for python 3.8", "body": "\r\n**System information**\r\n- OS Platform and Distribution Ubuntu 20.04 (LTS)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.15\r\n- Python version: 3.8.2\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nIt seems that tensorflow 1.15.2 binaries are not available for Python 3.8 at this time.  This is at odds with the directions at https://www.tensorflow.org/install/pip state the system requirements for `tensorflow==1.15` are:\r\n\r\n> * Python 3.5\u20133.8\r\n> * pip 19.0 or later (requires manylinux2010 support)\r\n> * Ubuntu 16.04 or later (64-bit)\r\n\r\nGiven that Ubuntu 20.04 is now the current LTS release and it has Python 3.8, it would be great if we could still run Tensorflow 1.x on that platform for as long as Tensorflow 1.x isn't deprecated.  \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nJust follow the exact instructions on https://www.tensorflow.org/install/pip for current Ubuntu release. Thanks!  ", "comments": ["@cboettig \r\nTensorFlow 1.x even 2.1 does not support Python 3.8. Instead could you please try installing TensorFlow v2.2.0rc3 using the following command. or use Python3.7, (3.6, 3.5 also work) for that.\r\n\r\npip install tensorflow==2.2.0rc3\r\n\r\nplease refer to this [comment](https://github.com/tensorflow/tensorflow/issues/38750#issuecomment-623088620) and [this](https://github.com/tensorflow/tensorflow/issues/39380#issuecomment-626379710)\r\n", "We don't add new versions of Python for old releases. If you want to use TF 1.15, you have to use Python 3.7 or lower. If you want to use Python 3.8 you have to use TF 2.2 or newer.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39768\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39768\">No</a>\n", "@mihaimaruseac Thank you very much for clarifying that.  Can we convert this bug report into a documentation bug report?  The current documentation states otherwise.  Specifically, https://www.tensorflow.org/install/pip says that you support Python 3.5 - 3.8, (right below the instruction for installing 1.15) and does not indicate this is not true of 1.15.  \r\n\r\nI do rely on the current TF release whenever possible.  As you surely know, many packages (in my most recent case, `stable-baselines`) still run only on 1.x version, so upgrading is not always possible.  Therefore I'm glad you still provide a 1.5 version, and I can revert to an older python version to take advantage of it. Thanks for the fast reply, very impressive on a project of this scale!\r\n\r\n", "Made https://github.com/tensorflow/docs/pull/1581 to update documentation. Thank you for the suggestion.", "pip install tensorflow\r\n Downloading tensorflow-2.3.0-cp38-cp38-win_amd64.whl (342.5 MB)\r\nthis is not working for python 3.8 . what can I do ? please ", "@hafez-ahmad this issue is not the right place for that, you should have opened a new issue, filling in issue template.\r\n\r\nI gather you are on Windows. Is your python 64 bits? Did you install the latest MSCV redistributable? When you installed Python, did you get it from the Python website or from Windows Marketplace?", "Yesterday,I installed python 3.8. I have solved it. \r\nI just open cmd as Administrator. then, I have installed tensorflow in python 3.8.   ", "> We don't add new versions of Python for old releases.\r\n\r\nA bit opinionated IMO. Py 2.x was supported for a decade or so after its deprecation.\r\n\r\nBut I've found a workaround:\r\n```\r\npip install torch\r\n```\r\n"]}, {"number": 39767, "title": "Wider vector for FP16 RELU Grad on GPUs", "body": "This PR uses wider vector (8 FP16 values) for loading and storing in FP16 ReluGrad kernel to improve performance on Nvidia Ampere GPUs.\r\n\r\nFor older GPUs, the performance is expected to be unchanged.\r\n\r\n\r\nfyi @nluehr ", "comments": ["@chsigg Can you please review this PR ? Thanks!", "This had to be rolled back because it broke the ROCm build.  [This](https://gist.github.com/sanjoy/62cb3862dc48c0666ae4ca712693667f) is the error message.  Please fix the compile error and I'll re-approve the PR.\r\n\r\nLet me know if you need more help in narrowing down the issue.", "@sanjoy, It should be a simple fix. I pushed a new commit in my kaixih:relu_grad_vect_pr (https://github.com/kaixih/tensorflow/tree/relu_grad_vect_pr) branch but it doesn't show up here. Should I make another PR or what is your advice?", "Creating a new PR should be simpler."]}, {"number": 39766, "title": "Build Error fedora 32, gcc -10.xx", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Linux Fedora 32\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n\r\n\r\nERROR: /home/prashantkumar/.cache/bazel/_bazel_prashantkumar/bf7a6276eada3a0737ce3d01ac04c833/external/llvm-project/mlir/test/BUILD:146:1: C++ compilation of rule '@llvm-project//mlir/test:TestTransforms' failed (Exit 1)\r\nIn file included from /usr/lib/gcc/x86_64-redhat-linux/10/../../../../include/c++/10/bits/move.h:57,\r\n                 from /usr/lib/gcc/x86_64-redhat-linux/10/../../../../include/c++/10/bits/nested_exception.h:40,\r\n                 from /usr/lib/gcc/x86_64-redhat-linux/10/../../../../include/c++/10/exception:148,\r\n                 from /usr/lib/gcc/x86_64-redhat-linux/10/../../../../include/c++/10/new:41,\r\n                 from external/llvm-project/llvm/include/llvm/Support/Compiler.h:21,\r\n                 from external/llvm-project/llvm/include/llvm/Support/Casting.h:17,\r\n                 from external/llvm-project/mlir/include/mlir/Support/LLVM.h:24,\r\n                 from external/llvm-project/mlir/include/mlir/IR/MLIRContext.h:12,\r\n                 from external/llvm-project/mlir/include/mlir/IR/TypeSupport.h:16,\r\n                 from external/llvm-project/mlir/include/mlir/IR/Types.h:12,\r\n                 from external/llvm-project/mlir/include/mlir/IR/Value.h:16,\r\n                 from external/llvm-project/mlir/include/mlir/IR/BlockSupport.h:16,\r\n                 from external/llvm-project/mlir/include/mlir/IR/Block.h:16,\r\n                 from external/llvm-project/mlir/include/mlir/IR/Operation.h:16,\r\n                 from external/llvm-project/mlir/include/mlir/IR/OpDefinition.h:22,\r\n                 from external/llvm-project/mlir/include/mlir/Dialect/Traits.h:17,\r\n                 from external/llvm-project/mlir/test/lib/Dialect/Test/TestDialect.h:17,\r\n                 from external/llvm-project/mlir/test/lib/Transforms/TestInlining.cpp:15:\r\n/usr/lib/gcc/x86_64-redhat-linux/10/../../../../include/c++/10/type_traits: In instantiation of 'struct std::is_constructible<mlir::BlockAndValueMapping, llvm::iterator_range<llvm::detail::indexed_accessor_range_base<mlir::OperandRange, mlir::OpOperand*, mlir::Value, mlir::Value, mlir::Value>::iterator> >':\r\nexternal/llvm-project/llvm/include/llvm/ADT/STLExtras.h:1171:30:   required by substitution of 'template<class RangeT, class> llvm::detail::indexed_accessor_range_base<mlir::OperandRange, mlir::OpOperand*, mlir::Value, mlir::Value, mlir::Value>::operator RangeT<RangeT, <template-parameter-1-2> >() const [with RangeT = mlir::BlockAndValueMapping; <template-parameter-1-2> = <missing>]'\r\nexternal/llvm-project/mlir/test/lib/Transforms/TestInlining.cpp:49:75:   required from here\r\n/usr/lib/gcc/x86_64-redhat-linux/10/../../../../include/c++/10/type_traits:909:52: error: static assertion failed: template argument must be a complete class or an unbounded array\r\n  909 |       static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),\r\n\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@pashu123 \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose) .Also Provide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39766\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39766\">No</a>\n"]}, {"number": 39765, "title": "Make iOS delegates opt-in", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/39737", "comments": ["Added some invocation examples as comments, lmk if you had somewhere else in mind!", "I'll actually have to add another example after https://github.com/tensorflow/tensorflow/pull/38956 merges", "Thanks again for your help! Changes look good to me.\r\n\r\nWill let @teijeong take a final pass, when he has a chance.", "> Have you thought about this issue?\r\n\r\nThis is definitely a good callout. \r\n\r\n> How is your team going to handle this situation?\r\n\r\nAfter a few years of using bazel we have found there is absolutely no way to be successful without a significant amount of configuration in a root `.bazelrc` file, so I didn't even consider this as an issue since I assume any significantly complex iOS application will have to have the same. If that's true the burden of folks including that for their entire team would be very low (this is what we would do if we were planning on using the delegates, we actually want the codepath with none of them at least for now)\r\n\r\n> Do you have better suggestions for the app developers wanting to use the TensorFlowLite swift library? \r\n\r\nOff the top of my head I think we have 2 options:\r\n\r\n1. Switch to the blacklist approach. In this case you'd have the same issue, but with reverse defines, but maybe this would be good enough\r\n2. Create a new `swift_library` target that is virtually a duplicate of the current one except with none of the `select()` statements, so folks could depend on that directly for \"all\" the delegates. This would also only partially solve the issue if you wanted only 1 of them.\r\n\r\nIn general I think this just brings up a place where bazel lacks some configurability, and flags are our only option \ud83d\ude22 ", "> After a few years of using bazel we have found there is absolutely no way to be successful without a significant amount of configuration in a root `.bazelrc` file\r\n\r\nThis is a fair point, and I agree that lots of projects require additional parameters anyways.\r\n\r\nMy concern was also related to how our internal CI system works. There are existing apps already depending on the `TensorFlowLite` Swift library, and because some of them are using Metal delegate, those builds would probably break if we submitted this PR as is. There are ways to provide custom build flags for each project, but it's a bit more involved than it sounds.\r\n\r\n> 1. Switch to the blacklist approach. In this case you'd have the same issue, but with reverse defines, but maybe this would be good enough\r\n\r\nYes, I would actually much prefer an opt-out model, because it could still give you options to easily exclude unnecessary delegates, and when we don't really care about the project size, the default configuration would suffice. I believe this would be also good enough to meet your needs.\r\n\r\nMore specifically, we could rename the define variables to be `use_foo_delegate` -> `exclude_foo_delegate`, and reverse the select conditions.", "> Yes, I would actually much prefer an opt-out model, because it could still give you options to easily exclude unnecessary delegates, and when we don't really care about the project size, the default configuration would suffice. I believe this would be also good enough to meet your needs.\r\n\r\nThe current CocoaPods approach is to opt-in, where developers explicitly list in their Podfiles which delegates they want to include. Do we want Bazel to match that workflow?\r\n\r\n> There are existing apps already depending on the TensorFlow Lite Swift library, and because some of them are using Metal delegate, those builds would probably break if we submitted this PR as is. \r\n\r\nAgreed that this would need to be communicated to existing teams, but you can think about the reverse where that each time we add a new delegate to `TensorFlowLite` Swift library, teams will need to decide if they want to increase their binary size for one or more delegates that they don't plan to use. Opt-in approach breaks existing teams currently using Metal delegation, but hopefully long term will provide them with the flexibility to add only the delegates they plan to use and avoid unnecessary binary increases. \r\n\r\nIf the plan is to add more delegates in the future, I think opt-in for both CocoaPods and Bazel provides the most flexibility for consumers of the TensorFlow Lite Swift library.", "@tristane0 Okay, that makes sense to me. I'm convinced enough to proceed with opt-in approach. \ud83d\ude00", "@keith looks like this change will break the existing unit tests for [MetalDelegate](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/swift/Tests/MetalDelegateTests.swift)\r\n\r\nWe may need to exclude `MetalDelegateTests.swift` and do the selects here as well:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/swift/BUILD.apple#L70\r\n\r\nand add the `use_metal_delegate=1` define to where we invoke `bazel test //tensorflow/lite/experimental/swift/...`\r\n\r\nAny other options?", "I pushed what I think is the other option, which might be a bit nicer overall, lmk what you think or I can revert it and go down the other path. I think it kinda depends on how much of a pain it will be to keep the second one in sync, vs how much of a pain it'll be to run CI multiple times with different defines. I wish there was a way to force the defines on from the `swift_library` level or `select` on `features` but I can't think of another way to configure it.", "> I pushed what I think is the other option, which might be a bit nicer overall, lmk what you think or I can revert it and go down the other path. I think it kinda depends on how much of a pain it will be to keep the second one in sync, vs how much of a pain it'll be to run CI multiple times with different defines. I wish there was a way to force the defines on from the `swift_library` level or `select` on `features` but I can't think of another way to configure it.\r\n\r\nThank you @keith! This was one of the options we were discussing yesterday and we agreed that it was a simpler approach than dealing with the selects for the `Tests` library. I am fine with moving forward with this approach. \r\n\r\nWould it be possible to mark the \"all delegates\" `swift_library` as `testonly = 1`? I think this would also require updating the test app targets to include that as well? I would also be ok with having test app just depend on `:TensorFlowLite` as it doesn't reference any of the delegates and not sure if there are any plans to do so since TFLite has existing [example apps](https://github.com/tensorflow/examples/tree/master/lite/examples) that already demonstrate Metal delegate usage.\r\n\r\n@yyoon to confirm and approve.", "Updated!", "> Updated!\r\n\r\nLooks like we need to update the `:TestsLibrary` to depend on the \"all delegates\" target; otherwise, LGTM. Thank you!", "Sorry I swapped the wrong one, updated", "@keith can you please check sanity errors [here](https://source.cloud.google.com/results/invocations/cac62d44-f0db-4637-8652-c431f800792b/log) ", "thanks, ran buildifier on it locally"]}, {"number": 39764, "title": "Tf32", "body": "API to enable and disable TensorFloat32 execution on Nvidia Ampere GPUs.\r\n\r\nWe also remove the following FP16 TensorCore opt-out environment variables:\r\n* TF_DISABLE_CUBLAS_TENSOR_OP_MATH\r\n* TF_DISABLE_CUDNN_TENSOR_OP_MATH\r\n* TF_DISABLE_CUDNN_RNN_TENSOR_OP_MATH\r\n\r\nAttn: @reedwm ", "comments": ["If we want to expose a python API for it then we should also expose a C API and have python call the C API.", "@alextp, since we don't plan on exposing a Python API for 2.3 but will for 2.4, can we delay exposing a C API until after 2.3 is cut? That gives us more times to work out the details. Note Google Cloud does plan on exposing the Python API in 2.3, so for their release there would be a Python API without a C API.\r\n\r\nAs for the details: the function prototype should look something like\r\n\r\n```C\r\nvoid TFE_ContextOptionsSetTensorFloat32(TFE_ContextOptions*, bool tf32);\r\n```\r\n\r\nThe implementation and semantics are a lot trickier. We have several options\r\n\r\n1. Set a global variable, as is done in this PR. This is very simple, but adds a hidden direct dependency from the C API to the StreamExecutor. It also prevents multiple EagerContexts where one enables TF32 and the other does not.\r\n2. Store the boolean on each GPU's [Stream](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/stream_executor/stream.h;l=92;drc=83d65b152b6b1ed1e622c59d908e76d8d2e7d07b), and have Stream methods to get/set the boolean. A `tf32` boolean argument would be added to many functions in stream_executor so Stream could propagate it to the actual CUDA calls. RPCs [CreateContextRequest](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/protobuf/eager_service.proto;l=85;drc=8588e0aab8c1ef6a4214bcc2f7d0bb61578a88b3) and [UpdateContextRequest](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/protobuf/eager_service.proto;l=130;drc=8588e0aab8c1ef6a4214bcc2f7d0bb61578a88b3) would be used to enable TF32 on remote devices.\r\n    \r\n    This is also pretty simple and doesn't use global variables. One issue is that the boolean logically doesn't belong as a field on Stream, as typically information about how to execute each op, like the precision, is passed as inputs to its methods. Another issue is that if two clients have remote devices on the same server, that server will use the same per-device Stream for both clients, and so  both clients must have TF32 be enabled or disabled.\r\n3. Store whether TF32 is enabled per op, as a NodeDef attribute. We could modify EagerContext to add an attribute to every op, and modify the MatMul, Conv2d, and other OpKernels to pass the boolean to Stream, and modify Stream and many other functions to pass it down to the CUDA API. I'm unsure how to add the attribute to each op within functions.\r\n    \r\n    The advantage is this fully works with local and remote devices, and allows each client to enable/disable TF32 independently. The disadvantage is that we have to modify a lot of code as every OpKernel supporting TF32 needs to be modified. It also is hacky to modify EagerContext to do this, but perhaps there is a better way of adding an attribute to every op?\r\n\r\n@alextp, thoughts? I am not (yet) very familiar with the Eager runtime code, so it's likely there are approaches and pros/cons that I have not considered.", "Delaying the C API SGTM.\r\n\r\nI prefer to set it at the context instead of ContextOptions since if we set on ContextOptions users will only be able to set tf32 before the context is initialized, and sometimes it gets initialized on import.\r\n\r\nI think Stream is a good compromise between a global flag and a noisy per-op flag."]}, {"number": 39763, "title": "Ubuntu 18.04.4 LTS crashes while training with Tensorflow", "body": "#System information\r\n\r\nOS Platform and Distribution: Linux Ubuntu 18.04.4 LTS\r\nTensorFlow installed from (container): tensorflow/tensorflow:nightly-gpu\r\nTensorFlow version (use command below): Tensorflow: 2.3.0-dev20200514\r\nPython version: 3.6.9\r\nCUDA/cuDNN version: V10.1.243 / CUDNN=7.6.4.38-1\r\nGPU model and memory: RTX 2080 8GB\r\n\r\n### Describe the problem\r\nUbuntu gnome crashes or/and freezes while training any model with tensorflow. It is completely unable of finishing any training session...\r\n\r\n### Source code / logs\r\nTaken from /var/log/syslog:\r\n\r\n\r\nMay 14 20:44:09 APC-L gnome-shell[10844]: Some code accessed the property 'WindowPreviewMenu' on the module 'windowPreview'. That property was defined with 'let' or 'const' inside the module. This was previously supported, but is not correct according to the ES6 standard. Any symbols to be exported from a module must be defined with 'var'. The property access will work as previously for the time being, but please fix your code anyway.\r\nMay 14 20:45:08 APC-L gnome-session-binary[2276]: WARNING: Application 'org.gnome.Shell.desktop' killed by signal 11\r\nMay 14 20:45:08 APC-L gnome-session[2276]: gnome-session-binary[2276]: WARNING: Application 'org.gnome.Shell.desktop' killed by signal 11\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-0: disconnected\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-0: Internal DisplayPort\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-0: 2660.0 MHz maximum pixel clock\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0):\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-1: disconnected\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-1: Internal TMDS\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-1: 165.0 MHz maximum pixel clock\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0):\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-2: disconnected\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-2: Internal TMDS\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-2: 165.0 MHz maximum pixel clock\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0):\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-3: disconnected\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-3: Internal DisplayPort\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-3: 2660.0 MHz maximum pixel clock\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0):\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-4: disconnected\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-4: Internal TMDS\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-4: 165.0 MHz maximum pixel clock\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0):\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): GBT AORUS AD27QD (DFP-5): connected\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): GBT AORUS AD27QD (DFP-5): Internal DisplayPort\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): GBT AORUS AD27QD (DFP-5): 2660.0 MHz maximum pixel clock\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0):\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-6: disconnected\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-6: Internal TMDS\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-6: 165.0 MHz maximum pixel clock\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0):\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-7: disconnected\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-7: Internal DisplayPort\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0): DFP-7: 2660.0 MHz maximum pixel clock\r\nMay 14 20:45:17 APC-L /usr/lib/gdm3/gdm-x-session[2262]: (--) NVIDIA(GPU-0):\r\nMay 14 20:45:17 APC-L gsd-media-keys[2588]: g_variant_get_va: assertion 'value != NULL' failed\r\nMay 14 20:45:17 APC-L gsd-media-keys[2588]: g_variant_unref: assertion 'value != NULL' failed\r\nMay 14 20:45:17 APC-L org.gnome.Shell.desktop[12786]: current session already has an ibus-daemon.\r\nMay 14 20:45:17 APC-L dbus-daemon[1011]: [system] Activating via systemd: service name='org.freedesktop.GeoClue2' unit='geoclue.service' requested by ':1.155' (uid=1000 pid=12786 comm=\"/usr/bin/gnome-shell \" label=\"unconfined\")\r\nMay 14 20:45:17 APC-L systemd[1]: Starting Location Lookup Service...\r\nMay 14 20:45:17 APC-L dbus-daemon[1011]: [system] Successfully activated service 'org.freedesktop.GeoClue2'\r\nMay 14 20:45:17 APC-L systemd[1]: Started Location Lookup Service.\r\nMay 14 20:45:17 APC-L gnome-shell[12786]: Telepathy is not available, chat integration will be disabled.\r\nMay 14 20:45:18 APC-L gnome-shell[12786]: JS WARNING: [/usr/share/gnome-shell/extensions/ubuntu-dock@ubuntu.com/appIcons.js 1028]: unreachable code after return statement\r\nMay 14 20:45:18 APC-L gnome-shell[12786]: [AppIndicatorSupport-DEBUG] Registering StatusNotifierItem :1.96/org/ayatana/NotificationItem/software_update_available\r\nMay 14 20:45:18 APC-L gnome-shell[12786]: [AppIndicatorSupport-DEBUG] Registering StatusNotifierItem :1.96/org/ayatana/NotificationItem/livepatch\r\nMay 14 20:45:18 APC-L gnome-shell[12786]: Error looking up permission: GDBus.Error:org.freedesktop.portal.Error.NotFound: No entry for geolocation\r\n\r\n\r\nI think it might be a NVIDIA driver issue... I have installed the latest propietary driver from NVIDIA 440.82 in several ways (nvidia web page run file, ubuntu software&update or ppa package). I also tried with a previous version driver (435.21), with the same result...\r\n\r\nThanks in advance.", "comments": ["@alvarobasi,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.\r\n\r\nAlso, please check if you are able to `import tensorflow` so that we can confirm it's not a build/install issue. Thanks!", "Sure, [here](https://colab.research.google.com/drive/12sx2GQN5kwRke7fPkj5TkgtugN5y1X44?usp=sharing) you can find the code. I've made some research about this problem an I found out that reducing de clock of my memories from 3200Mhz to 2133Mhz makes it a little bit more stable (it doesn't freeze the OS or crash the entire system like before) and I could run a 3 epoch training process without any issues with the except of gnome restarting from time to time during training. However, I run it again in order to verify stability and it gave me the following error during the process:\r\n\r\n ` 7999/12662 [=================>............] - ETA: 9:48 - loss: 0.0147 \r\nEpoch 00001: saving model to ./outputs/checkpoints/SRResNet-MSE/weights.01-0.0147.hdf5\r\n\r\nEpoch 00001: loss improved from 0.01590 to 0.01473, saving model to ./outputs/checkpoints/SRResNet-MSE/best_weights.hdf5\r\n 9999/12662 [======================>.......] - ETA: 5:20 - loss: 0.0139\r\nEpoch 00001: saving model to ./outputs/checkpoints/SRResNet-MSE/weights.01-0.0139.hdf5\r\n\r\nEpoch 00001: loss improved from 0.01473 to 0.01390, saving model to ./outputs/checkpoints/SRResNet-MSE/best_weights.hdf5\r\n11999/12662 [===========================>..] - ETA: 1:17 - loss: 0.0133\r\nEpoch 00001: saving model to ./outputs/checkpoints/SRResNet-MSE/weights.01-0.0133.hdf5\r\n\r\nEpoch 00001: loss improved from 0.01390 to 0.01329, saving model to ./outputs/checkpoints/SRResNet-MSE/best_weights.hdf5\r\n12662/12662 [==============================] - 1466s 116ms/step - loss: 0.0131\r\nEpoch 2/3\r\n 1337/12662 [==>...........................] - ETA: 18:23 - loss: 0.0100\r\nEpoch 00002: saving model to ./outputs/checkpoints/SRResNet-MSE/weights.02-0.0100.hdf5\r\n\r\nEpoch 00002: loss improved from 0.01329 to 0.00996, saving model to ./outputs/checkpoints/SRResNet-MSE/best_weights.hdf5\r\n 3337/12662 [======>.......................] - ETA: 15:10 - loss: 0.0098\r\nEpoch 00002: saving model to ./outputs/checkpoints/SRResNet-MSE/weights.02-0.0098.hdf5\r\n\r\nEpoch 00002: loss improved from 0.00996 to 0.00984, saving model to ./outputs/checkpoints/SRResNet-MSE/best_weights.hdf5\r\n 3562/12662 [=======>......................] - ETA: 14:49 - loss: 0.0098Segmentation fault (core dumped)`\r\n\r\nI'm running an i9 9900K along with 16 GB of 3200MHz clockspeed and an RTX 2080 with a MSI Z370 PC PRO, which is an entry level mobo... May be possible that I'm getting unstability issues because of my hardware combination? I've tested the same code in a Ryzen 2600 system with an RTX 2070, 16GB 2133MHz and a B350 mobo and it worked like a charm. I already posted this question to MSI in case it is not a tensorflow or nvidia driver related issue... \r\n\r\nBesides, I initially started this project on Windows 10 Pro and didn't have any issue whatsoever with i9 9900K + 3200MHz memory.", "@alvarobasi,\r\nI am facing a `NonMatchingChecksumError` while loading the dataset. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1fbde998f49c33cf39e49dacbc42c491/39763.ipynb#scrollTo=P895a6UMnZSo).\r\n\r\nCould you please try running the code in a virtual environment and check if it still crashes Ubuntu. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Sorry for forgetting about this thread. A lot of work to do :D. I normally apply undervolting to my i9 9900K in order to lower frequencies and stay safer and cooler with my basic Z370 MB. I tried to execute some stability tests and it crashed on Windows. Increasing the Vcore, now I'm pretty stable in every operation performed in Windows. I've tried to train again in Ubuntu with this Vcore config and didn't get any crash since then. Initially, I'd say the problem is solved, however, further testing is required to be confident about that. In the meantime, I\u00b4d keep this thread closed. Thanks for the help."]}, {"number": 39762, "title": "include tf_types.def as part of the tf-nightly pip install", "body": "While trying to build mlir with tf-nightly, there are situations\r\n\r\n`tensorflow/compiler/mlir/tensorflow/ir/tf_types.h`\r\n\r\nneeds to be included. However, this file implicitly includes\r\n`tensorflow/compiler/mlir/tensorflow/ir/tf_types.def` which is not included.\r\n\r\nThe follow error thrown out:\r\n```\r\nIn file included from bazel-out/darwin-fastbuild/bin/external/local_config_tf/include/tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h:37:\r\nIn file included from bazel-out/darwin-fastbuild/bin/external/local_config_tf/include/tensorflow/compiler/mlir/tensorflow/ir/tf_traits.h:25:\r\nbazel-out/darwin-fastbuild/bin/external/local_config_tf/include/tensorflow/compiler/mlir/tensorflow/ir/tf_types.h:74:10: fatal error: 'tensorflow/compiler/mlir/tensorflow/ir/tf_types.def' file not found\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n1 error generated.\r\n```\r\n\r\nThis PR add `.def` file under `tensorflow/compiler` to be part of the pip install, so that\r\n`tf_types.h` could be used.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 39761, "title": "choosing bitrate when encoding audio", "body": "**In tf1.15 there was the support of  choosing bitrate when encoding audio**\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/contrib/ffmpeg/ffmpeg_ops.py#L101\r\n\r\n**Now the possibility is cut out**\r\n\r\nIt is very important feature in speech recognition tasks. So It will be great when you return back this option.", "comments": ["`contrib` module is deprecated and no longer part of TensorFlow.\r\nHave you tried using [`tf.audio.encode_wav`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/audio/encode_wav) ?", "> `contrib` module is deprecated and no longer part of TensorFlow.\r\n> Have you tried using [`tf.audio.encode_wav`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/audio/encode_wav) ?\r\n\r\nHi.\r\nI talk about bitrate, not sample rate.\r\n\r\nNeither of any encoders support the option now.\r\n\r\nAs I mentioned earlier it is very important feature in speech recognition tasks.\r\nAs a proof of my words you could look at paper from Google https://arxiv.org/pdf/1808.05312.pdf  p3.3", "Thanks for the feature request @avnx, but I'm confused. Setting a bitrate doesn't seem to have been part of the tf.contrib.ffmpeg public API? Do you mean you edited the source code?\r\n\r\nIt would be nice if encode/decode audio had this support. I think we could mark this contributions welcome, because I doubt the core team has bandwidth to look at it.", "> Thanks for the feature request @avnx, but I'm confused. Setting a bitrate doesn't seem to have been part of the tf.contrib.ffmpeg public API? Do you mean you edited the source code?\r\n> \r\n> It would be nice if encode/decode audio had this support. I think we could mark this contributions welcome, because I doubt the core team has bandwidth to look at it.\r\n\r\nHi,\r\n\r\nYes, I edited source code. It was not hard as you can see https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/contrib/ffmpeg/ffmpeg_ops.py#L101\r\n\r\nJust change one option and that's it.\r\n\r\nI tried to go deeper in current code tf2.2 and I can't imagine how I could \"turn on\" the option to encode with different bitrate.", "@avnx ffmpeg and all tf.contrib has been removed from tensorflow code tree and is not part of the distribution since TF 2.0.\r\n\r\nThey are being distributed to separate repos and pip. tensorflow-io are mostly for I/O, audio/video, tensorflow-addons are mostly for additional tf.keras related modules. You can open feature request issues to [tensorflow/io](https://github.com/tensorflow/io) if it is audio related.", "> @avnx ffmpeg and all tf.contrib has been removed from tensorflow code tree and is not part of the distribution since TF 2.0.\r\n> \r\n> They are being distributed to separate repos and pip. tensorflow-io are mostly for I/O, audio/video, tensorflow-addons are mostly for additional tf.keras related modules. You can open feature request issues to [tensorflow/io](https://github.com/tensorflow/io) if it is audio related.\r\n\r\nso what do you want to say? I don't get you.\r\n\r\nI opened feature request and attached info where you (Google) use bitrate manipulation during solving speech recognition tasks. I say that it is very important feature in speech recognition tasks and it is good to have it. Especially considering the fact that there was the opportunity to do that in older versions of tf.\r\n\r\nI would be happy to do that by myself but tf code became very complicated in processing an audio (since tf2) and I don't think that I am capable of adding this feature by myself.", "@avnx I was suggesting you to open a feature request issue in  [tensorflow/io](https://github.com/tensorflow/io), so that we can track the progress there. We will add support for your feature request if there is enough interests from overall community.\r\n\r\nSince  [tensorflow/io](https://github.com/tensorflow/io) and  [tensorflow/tensorflow](https://github.com/tensorflow/tensorflow) are two different repos, feature requests cross the repo might not have been noticed.", "@avnx I have opened a feature request in https://github.com/tensorflow/io/issues/994 .\r\n\r\nLet's continue the conversion there to track the progress.", "> @avnx I was suggesting you to open a feature request issue in [tensorflow/io](https://github.com/tensorflow/io), so that we can track the progress there. We will add support for your feature request if there is enough interests from overall community.\r\n> \r\n> Since [tensorflow/io](https://github.com/tensorflow/io) and [tensorflow/tensorflow](https://github.com/tensorflow/tensorflow) are two different repos, feature requests cross the repo might not have been noticed.\r\n\r\nI am sorry. Didn't get your idea first time.\r\nThank you!"]}, {"number": 39760, "title": "[INTEL MKL] Enable NCHW to NHWC conversion for CPU - part1.", "body": "This PR enables layout (data format) conversion from NCHW to NHWC on CPU. This is useful (1) when a model is trained on GPU and later doing inference/fine-tuning on CPU and (2) helps quantizing NCHW trained model for CPU.\r\n\r\nTo make the PR small, we have included a few unit tests in this **part1**. More unit tests will follow on future PRs.", "comments": ["@mdfaijul Can you please check @ezhulenev's comments and keep us posted. Thanks!", "@ezhulenev Sorry, I have been in vacation. I will resume working this PR shortly.", "@ezhulenev Thanks for the suggestions. I have added comments. Please check.", "@ezhulenev Any update on this PR?", "@ezhulenev Any update on this PR?", "@ezhulenev Did you have any chance looking into changes to address your comments?", "@ezhulenev @gbaned Did you have chance to look into the updates in response to review comments.", "This PR failed internally:\r\n\r\n```\r\ntensorflow/core/grappler/optimizers:generic_layout_optimizer_test | \u00a0\r\nBuild Flags: --config=cuda --compilation_mode=opt\r\n```\r\n\r\n```\r\nthird_party/tensorflow/core/grappler/optimizers/generic_layout_optimizer_test.cc:224: Failure\r\nExpected equality of these values:\r\n  attr->s()\r\n    Which is: \"NHWC\"\r\n  attr_value\r\n    Which is: \"NCHW\"\r\n```", "@mdfaijul  Can you please check @ezhulenev's comments and keep us posted ? Thanks!", "@gbaned Thanks. I am looking into the error. Will get back to you shortly.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39760) for more info**.\n\n<!-- need_author_cla -->", "@gbaned @ezhulenev @andyly When I committed a fix to the GPU failure from another machine, where git was not setup well, I got CLA issues. The PR #42448 is identical to this closed one except with a small fix to the GPU failure. Sorry for the inconvenience."]}, {"number": 39759, "title": "Failed to bazel build //third_party/gpus/cuda/...", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Ubuntu Linux 18.04, x86_64\r\n-  CUDA-10.2/CUDNN-7.6.5/TensorRT-7 installed\r\n- NVidia GeForce GTX 1070 GPU card, `nvidia-smi` works fine.\r\n- TensorFlow installed from (source or binary): Not installed yet\r\n- TensorFlow version:  tensorflow git repo on GitHub,  the latest master branch(CommitId: 880ea5d)\r\n- Python version: system-provided Python3 (3.6.9)\r\n- Bazel version (if compiling from source):  Bazel  `3.1.0` from bazel's official apt repo.\r\n- GCC/Compiler version (if compiling from source): `gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0`\r\n\r\n- CUDA/cuDNN version: CUDA-10.2.89-1, CUDNN-7.6.5.32-1+cuda10.2, TensorRT-7.0.0-1+cuda10.2\r\n- GPU model and memory: GeForce GTX 1070,  8114MiB\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git  tensorflow.git\r\ncd tensorflow.git\r\necho \"3.1.0\" > ./.bazelversion\r\n./configure # Step by step following the instructions, with only CUDA and TensorRT support enabled\r\nbazel build //third_party/gpus/cuda/...\r\n```\r\n\r\nExpected: Everything works fine.\r\nActual: Bazel build error, as can be seen from the following console log.\r\n```\r\nINFO: Found applicable config definition build:tensorrt in file /work/studio/github.com/tensorflow/tensorflow.git/.bazelrc: --action_env TF_NEED_TENSORRT=1\r\nINFO: Found applicable config definition build:cuda in file /work/studio/github.com/tensorflow/tensorflow.git/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /work/studio/github.com/tensorflow/tensorflow.git/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:linux in file /work/studio/github.com/tensorflow/tensorflow.git/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /work/studio/github.com/tensorflow/tensorflow.git/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed target //third_party/gpus/cuda:build_defs_bzl (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: missing input file 'third_party/gpus/cuda/build_defs.bzl', owner: '//third_party/gpus/cuda:build_defs.bzl'\r\nERROR: /work/studio/github.com/tensorflow/tensorflow.git/third_party/gpus/cuda/BUILD:3:1: //third_party/gpus/cuda:build_defs_bzl: missing input file 'LabelCause{label=//third_party/gpus/cuda:build_defs.bzl, msg=missing input file 'third_party/gpus/cuda/build_defs.bzl', owner: '//third_party/gpus/cuda:build_defs.bzl'}'\r\nTarget //third_party/gpus/cuda:build_defs_bzl failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /work/studio/github.com/tensorflow/tensorflow.git/third_party/gpus/cuda/BUILD:3:1 1 input file(s) do not exist\r\nINFO: Elapsed time: 0.112s, Critical Path: 0.00s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n$ lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 18.04.4 LTS\r\nRelease:\t18.04\r\nCodename:\tbionic\r\n\r\n$ nvidia-smi\r\nThu May 21 22:35:58 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    On   | 00000000:01:00.0  On |                  N/A |\r\n| 26%   45C    P0    35W / 151W |   1665MiB /  8114MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1276      G   /usr/lib/xorg/Xorg                            40MiB |\r\n|    0      2459      G   /usr/lib/xorg/Xorg                           789MiB |\r\n|    0      2615      G   /usr/bin/gnome-shell                         390MiB |\r\n|    0     17079      G   ...quest-channel-token=9704712962697071887   184MiB |\r\n|    0     21665      G   ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files   242MiB |\r\n+-----------------------------------------------------------------------------+\r\n```", "comments": ["Has any one  idea on how to fix this? @angerson @ravikyram ", "You shouldn't need to wildcard-build everything under //third_party/gpus/cuda/...\r\n\r\nThat said, thanks a lot for your PR, your change indeed fixes two incorrect BUILD files.\r\n", "Hi, I get the same error, is there any solution for this ?", "Hi @storypku! \r\nWe are checking to see whether you still need help in this issue . Have you tried tested configurations from [here.](https://www.tensorflow.org/install/source#gpu)? Thanks!", "> Hi @storypku! We are checking to see whether you still need help in this issue . Have you tried tested configurations from [here.](https://www.tensorflow.org/install/source#gpu)? Thanks!\r\n\r\nThank all for helping. I think this issue can be closed now. \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39759\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39759\">No</a>\n"]}, {"number": 39758, "title": "TF-TRT test SquaredDiff op conversion in dynamic shape mode", "body": "This PR adds  explicit batch and dynamic shape tests to the SquaredDiff op converter. Tagging @bixia1 for review.", "comments": []}, {"number": 39757, "title": "TfLite Micro - Conv2D layer not running on ESP32 board", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: \r\nCompiling for ESP32: PlattformIO w/ ESP-IDF\r\nCreating TfLite: Windows 10 w/ Anaconda and Python 3.7, \r\n- TensorFlow installed from (source or binary): \r\nCompiling C++: tflite/micro from hello-world example\r\nTtLite creation: Python pip in a Anaconda environment with Python 3.7\r\n- Tensorflow version (commit SHA if source): Tensorflow 2.2\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32CAM (ESP-IDF Compiler)\r\n\r\n**Describe the problem**\r\nUsing a tflite-Model with a `Conv2D `layer results in a crash of the c++ code running on an ESP32 system. Just exchanging the layer to a `MaxPool2D `layer let the model run smoothly. This gave me the idea, that the problem is in using the Conv2D layer rather than the c++ code or model training.\r\nThe model structure looks like following:\r\n```\r\nmodel = Sequential()\r\nmodel.add(InputLayer(input_shape=(32,20,3)))\r\nmodel.add(Conv2D(8, (3, 3)))\r\n# model.add(MaxPool2D(pool_size=(2,2)))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(11, activation = \"softmax\"))\r\n```\r\n\r\nThe error on the ESP32 monitoring is the following\r\n```\r\nInput loaded.\r\nGuru Meditation Error: Core  0 panic'ed (LoadProhibited). Exception was unhandled.\r\nCore 0 register dump:\r\nPC      : 0x40089191  PS      : 0x00060033  A0      : 0x80089913  A1      : 0x3ffb2f90\r\nA2      : 0x3ffb3094  A3      : 0x00000000  A4      : 0x00060021  A5      : 0x000000fe\r\nA6      : 0x00000001  A7      : 0x00000000  A8      : 0x00000000  A9      : 0x3ffb34f8\r\nA10     : 0x00000003  A11     : 0x00060023  A12     : 0x00060021  A13     : 0x000000fe\r\nA14     : 0x0000002a  A15     : 0x3ffb5370  SAR     : 0x0000001f  EXCCAUSE: 0x0000001c\r\nEXCVADDR: 0x00000050  LBEG    : 0x4008e610  LEND    : 0x4008e63e  LCOUNT  : 0x00000000\r\nCore 0 was running in ISR context:\r\nEPC1    : 0x40089191  EPC2    : 0x00000000  EPC3    : 0x00000000  EPC4    : 0x00000000\r\n```\r\n\r\nThe model training is done in a Jypiter Notebook on a Windows 10 system and the tflite file is created with the following conversion sequence:\r\n```\r\nname = \"conv2d\"\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nopen(name + \".tfl\", \"wb\").write(tflite_model)\r\n```\r\n\r\nThe coding for the ESP32 is done in C++ in ESP-IDF. The tflite micro library is copied from the hello-world example with the ESP-IDF compiler running on PlattformIO.\r\nThe simplified c++ code is:\r\n```\r\n...  INCLUDES ...\r\n\r\nextern \"C\" void app_main() {\r\n    static tflite::ErrorReporter* error_reporter = nullptr;\r\n    const tflite::Model* model = nullptr;\r\n    static tflite::MicroInterpreter* interpreter = nullptr;\r\n    TfLiteTensor* output = nullptr;     \r\n    static tflite::ops::micro::AllOpsResolver *resolver; \r\n    static tflite::MicroOpResolver<5> micro_op_resolver;\r\n    int kTensorArenaSize = 128 * 1024;\r\n    uint8_t *tensor_arena = new uint8_t[kTensorArenaSize];\r\n    TfLiteStatus allocate_status;\r\n    error_reporter = new tflite::MicroErrorReporter;\r\n\r\n    CAccessSDClass accessSD;    \r\n//    model = tflite::GetModel(accessSD.ReadFileToCharArray(\"/sdcard/maxpool.tfl\"));\r\n    model = tflite::GetModel(accessSD.ReadFileToCharArray(\"/sdcard/conv2d.tfl\"));    \r\n    printf(\"Model loaded.\\n\");       \r\n\r\n    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_RESHAPE,\r\n                                 tflite::ops::micro::Register_RESHAPE());\r\n    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_CONV_2D,\r\n                                 tflite::ops::micro::Register_CONV_2D());\r\n    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_FULLY_CONNECTED,\r\n                                 tflite::ops::micro::Register_FULLY_CONNECTED());\r\n    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_SOFTMAX,\r\n                                 tflite::ops::micro::Register_SOFTMAX());\r\n    micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_MAX_POOL_2D,\r\n                                 tflite::ops::micro::Register_MAX_POOL_2D());\r\n    interpreter = new tflite::MicroInterpreter(model, micro_op_resolver, tensor_arena, kTensorArenaSize, error_reporter);\r\n\r\n    allocate_status = interpreter->AllocateTensors();\r\n\r\n    CImageBasis cib(\"/sdcard/zif0.jpg\");\r\n    float* input_data_ptr = interpreter->typed_tensor<float>(0);\r\n    for (int x = 0; x < cib.width; ++x)\r\n        for (int y = 0; y < cib.height; ++y)\r\n            for (int ch = 0; ch < cib.channels; ++ch)\r\n            {\r\n                *(input_data_ptr) = (float) cib.GetPixelColor(x, y, ch);\r\n                input_data_ptr++;\r\n            }\r\n    printf(\"Input loaded.\\n\");\r\n\r\n    interpreter->Invoke();\r\n    printf(\"Invoke Done.\\n\");    \r\n\r\n    output = interpreter->output(0);\r\n    for (int i = 0; i < 11; ++i)\r\n    {\r\n        printf(\"Result %d: %f\\n\", i, output->data.f[i]);  \r\n    }    \r\n}\r\n```\r\n\r\n\r\nIf the Conv2D layer is exchanged by a MaxPooling layer the model is running technically smoothl on exactly the same c++ code. The only change in the model is commenting the `Conv2D` layer and activation the `MaxPool2D` layer:\r\n```\r\nmodel.add(InputLayer(input_shape=(32,20,3)))\r\n# model.add(Conv2D(8, (3, 3)))\r\nmodel.add(MaxPool2D(pool_size=(2,2)))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(11, activation = \"softmax\"))\r\n```\r\n\r\nThen the result is as expected w/o any error:\r\n```\r\nInput loaded.\r\nInvoke Done.\r\nResult 0: 0.000000\r\nResult 1: 0.000000\r\nResult 2: 0.000000\r\nResult 3: 0.000000\r\nResult 4: 0.000000\r\nResult 5: 0.000000\r\nResult 6: 0.000000\r\nResult 7: 0.000000\r\nResult 8: 0.000000\r\nResult 9: 0.000000\r\nResult 10: 1.000000\r\n```\r\n\r\n\r\n\r\n**Any idea or hint where the problem is?**\r\n\r\n**Remark**\r\nThis model is not doing any usefull anymore. I have reduced the model and the c++ code to a minimum for reproducing the problem. Final target is a image classification on a ESP32 with the inbuild camera. For the problem I need a dedicated CNN with several Conv2D layers.\r\n", "comments": ["I could debug the error into the function call: `TfLiteStatus MicroInterpreter::Invoke()` in the file `micro_interpreter.cc`.\r\nThere it happens at the following step registration->invoke(...):\r\n```\r\n    if (registration->invoke) {\r\n      TfLiteStatus invoke_status = registration->invoke(&context_, node);\r\n```\r\nMaybe this helps.\r\n", "I have a strong hint to the root cause!\r\n\r\nIt looks like, as if the Conv2D implementatin has in internal limit controlled by the paramter `kMaxChannels` in the file `conv.cc`. If I increase this from 1024 to 4096, I get a running code.\r\n`constexpr int kMaxChannels = 4096;`\r\nAnybody who can confirm this? Is there a plan, to check this parameter at the modul loading and give an error message in case of too big layers?", "I could confirm my idea. The problem is the variable `kMaxChannels`. This was just too small for my purpouse. I would suggest to put this in to size it on the fly or minimum give an error message in case of \"too small\"", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39757\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39757\">No</a>\n"]}, {"number": 39756, "title": "problem running visualize.py at import flatbuffersn", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Mac OS 10.15.3**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/a**\r\n- TensorFlow installed from (source or binary): **from source**\r\n- TensorFlow version (use command below): v1.12.1-32248-g8670c85844 **2.2.0**\r\n- Python version: **3.7.4**\r\n- Bazel version (if compiling from source): **3.0.0**\r\n- GCC/Compiler version (if compiling from source): **Apple clang version 11.0.3 (clang-1103.0.32.59) (from gcc --version)**\r\n- CUDA/cuDNN version: **n/a**\r\n- GPU model and memory: **n/a**\r\n\r\n**Describe the current behavior**\r\nvisualize.py script fails with this error:\r\n`ImportError: cannot import name 'flatbuffersn' from 'flatbuffers.python' (/private/var/tmp/_bazel_jeremy/95159cfd4782ce915016562181875cd6/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/lite/tools/visualize.runfiles/flatbuffers/python/__init__.py)`\r\n\r\nI am running it with the command \r\n`bazel run //tensorflow/lite/tools:visualize ~/dev/tfl_dev/mobilenet_v1_0.25_128_quant/mobilenet_v1_0.25_128_quant.tflite ~/tmp/mobnet.html\r\n`\r\nfrom the top of my tensorflow source directory.  The build phase seems to work fine, then the import error seems to happen on running visualize.py.  Full output in attached file.\r\n[vis_dump.txt](https://github.com/tensorflow/tensorflow/files/4662612/vis_dump.txt)\r\nThe target tflite file can be downloaded [here]( http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_128_quant.tgz), but I don't think the tflite file ever gets loaded, so I doubt that the specific file is relevant.\r\n\r\nThe directory where it is trying to import from is listed here.  There is no flatbuffersn file or directory and the __init__.py file is empty.\r\n```\r\nls -l /private/var/tmp/_bazel_jeremy/95159cfd4782ce915016562181875cd6/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/lite/tools/visualize.runfiles/flatbuffers/python/\r\ntotal 0\r\n-r-xr-xr-x   1 jeremy  wheel    0 May 20 16:15 __init__.py*\r\ndrwxr-xr-x   3 jeremy  wheel   96 May 21 09:39 __pycache__/\r\ndrwxr-xr-x  10 jeremy  wheel  320 May 20 16:15 flatbuffers/\r\n```\r\n\r\n**Describe the expected behavior**\r\nI would expect it to dump an html file to ~/tmp/mobnet.html containing a visualization of the mobilenet model in the target tflite file.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe command line above.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Yeah, I can actually reproduce this, and I think the import name should be `flatbuffers` but the following `\\n` is somehow not correctly escaped and ended up being interpreted as character `n`.\r\n\r\nThis must be a bug somewhere in the flatbuffer build rule, or the flatbuffer tool. Let me dig a bit more about it.", "Found the issue, which was caused by the different `sed` tool behavior between linux and mac, as described here: https://stackoverflow.com/a/22203933/922135\r\n\r\nI'll send a fix soon, but in the meantime you should be able to try the fix locally.\r\n\r\nIn the `tensorflow/third_party/flatbuffers/build_defs.bzl` file at line365, change:\r\n\r\n    \"import flatbuffers\\\\n/' > %s\"\r\n\r\nto \r\n\r\n    \"import flatbuffers\\\\'$'\\\\n/' > %s\"\r\n\r\nand try running the visualize tool again.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39756\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39756\">No</a>\n", "That fixed it for me.  Thanks!"]}, {"number": 39755, "title": "tf.keras/Tensorboard callback does not implement activation histogram (and says it does)", "body": "**System information**\r\n- TensorFlow version (you are using): r2.2\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe callback v2 does advertise that it can record activations (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L1722), but the code clearly shows it doesn't : the following only saves weights histograms.\r\n```\r\n    if self.histogram_freq and epoch % self.histogram_freq == 0:\r\n      self._log_weights(epoch)\r\n\r\n```\r\n\r\nThe callback in callbacks_v1 did implement activation summary writing. However, there is one limitation where you cannot use it with eager execution enabled.\r\n\r\nWe should :\r\n- either remove the comment saying that Tensorboard callback logs activation histogram (or at least add a note that it should be implemented in a future version)\r\n- or, preferably of course, implement the feature...", "comments": ["Same issue. No activation histograms. Any idea if it is being investigated?", "@goulou Sorry for the late response. Looks like interesting feature.\r\n\r\nAre you still interested in contributing? If yes, please feel free to open a PR in  [keras-team/keras repo.](https://github.com/keras-team/keras/issues) repository.\r\n\r\nPlease note that Keras development moved to keras-team/keras repository to focus on only keras. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "This feature should be reopened IMO, it would be a useful feature.", "@kydonian Can you please open it in keras-team/keras repo as mentioned in my last response. Keras team is focussing their efforts in that repo only. Thanks!", "I did, see https://github.com/keras-team/keras/issues/15972."]}, {"number": 39754, "title": "applications_load_weight_test testcases fail on s390x architecture", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution: Ubuntu 18.04 on s390x architecture\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.2.0-0-g2b96f3662b 2.2.0\r\n- Python version: Python 3.6.9\r\n- Bazel version (if compiling from source): bazel 2.0.0- (@non-git)\r\n- GCC/Compiler version (if compiling from source):  gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n`applications_load_weight_test` testcases fail on s390x architecture due to incompatible Keras applications models.  It appears that `h5py` fails to find `Conversion function` for `CSET H5T_CSET_ASCII` data types.  Error in the logs take the form of:\r\n```\r\n[  SKIPPED ] ApplicationsLoadWeightTest.test_session\r\n======================================================================\r\nERROR: test_application_pretrained_weights_loading (__main__.ApplicationsLoadWeightTest)\r\ntest_application_pretrained_weights_loading (__main__.ApplicationsLoadWeightTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/334416e3a0381b53511e68de49f4ac07/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/keras/applications/applications_load_weight_test_resnet_v2.runfiles/org_tensorflow/tensorflow/python/keras/applications/applications_load_weight_test.py\", line 104, in test_application_pretrained_weights_loading\r\n    model = app(weights='imagenet')\r\n  File \"/root/.cache/bazel/_bazel_root/334416e3a0381b53511e68de49f4ac07/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/keras/applications/applications_load_weight_test_resnet_v2.runfiles/org_tensorflow/tensorflow/python/keras/applications/resnet_v2.py\", line 55, in ResNet50V2\r\n    classifier_activation=classifier_activation,\r\n  File \"/root/.cache/bazel/_bazel_root/334416e3a0381b53511e68de49f4ac07/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/keras/applications/applications_load_weight_test_resnet_v2.runfiles/org_tensorflow/tensorflow/python/keras/applications/resnet.py\", line 210, in ResNet\r\n    model.load_weights(weights_path)\r\n  File \"/root/.cache/bazel/_bazel_root/334416e3a0381b53511e68de49f4ac07/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/keras/applications/applications_load_weight_test_resnet_v2.runfiles/org_tensorflow/tensorflow/python/keras/engine/training.py\", line 250, in load_weights\r\n    return super(Model, self).load_weights(filepath, by_name, skip_mismatch)\r\n  File \"/root/.cache/bazel/_bazel_root/334416e3a0381b53511e68de49f4ac07/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/keras/applications/applications_load_weight_test_resnet_v2.runfiles/org_tensorflow/tensorflow/python/keras/engine/network.py\", line 1266, in load_weights\r\n    hdf5_format.load_weights_from_hdf5_group(f, self.layers)\r\n  File \"/root/.cache/bazel/_bazel_root/334416e3a0381b53511e68de49f4ac07/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/keras/applications/applications_load_weight_test_resnet_v2.runfiles/org_tensorflow/tensorflow/python/keras/saving/hdf5_format.py\", line 659, in load_weights_from_hdf5_group\r\n    original_keras_version = f.attrs['keras_version'].decode('utf8')\r\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\r\n  File \"/usr/local/lib/python3.6/dist-packages/h5py/_hl/attrs.py\", line 81, in __getitem__\r\n    attr.read(arr, mtype=htype)\r\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/h5a.pyx\", line 355, in h5py.h5a.AttrID.read\r\n  File \"h5py/_proxy.pyx\", line 50, in h5py._proxy.attr_rw\r\n  File \"h5py/_proxy.pyx\", line 319, in h5py._proxy.needs_bkg_buffer\r\n  File \"h5py/_proxy.pyx\", line 316, in h5py._proxy.needs_bkg_buffer\r\nKeyError: 'Conversion function not found (no appropriate function for conversion path)'\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 1.612s\r\n\r\nFAILED (errors=1, skipped=1)\r\nFailed to find converter for 8 -> b'PYTHON:OBJECT'\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThese testcases should pass on s390x architecture.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nbazel --host_jvm_args=\"-Xms1024m\" --host_jvm_args=\"-Xmx2048m\" test  //tensorflow/python/keras/applications:applications_load_weight_test_resnet_v2\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe problem appears when `h5py` tries to process keras application models downloaded to `~.keras/models/resnet50v2_weights_tf_dim_ordering_tf_kernels.h5`.  These models seem to be generated on x86 architecture and they fail to get past `h5py` process.  In particular, `original_keras_version = h5_file.file.attrs['keras_version'].decode('utf8')` [line ](https://github.com/tensorflow/tensorflow/blob/52fd23939e65699f8c7d53850a17daab7cc83177/tensorflow/python/keras/saving/hdf5_format.py#L659)fails with the following error:\r\n```\r\n  File \"/root/.cache/bazel/_bazel_root/334416e3a0381b53511e68de49f4ac07/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/keras/applications/applications_load_weight_test_resnet_v2.runfiles/org_tensorflow/tensorflow/python/keras/saving/hdf5_format.py\", line 659, in load_weights_from_hdf5_group\r\n    original_keras_version = f.attrs['keras_version'].decode('utf8')\r\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\r\n  File \"/usr/local/lib/python3.6/dist-packages/h5py/_hl/attrs.py\", line 81, in __getitem__\r\n    attr.read(arr, mtype=htype)\r\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/h5a.pyx\", line 355, in h5py.h5a.AttrID.read\r\n  File \"h5py/_proxy.pyx\", line 50, in h5py._proxy.attr_rw\r\n  File \"h5py/_proxy.pyx\", line 319, in h5py._proxy.needs_bkg_buffer\r\n  File \"h5py/_proxy.pyx\", line 316, in h5py._proxy.needs_bkg_buffer\r\nKeyError: 'Conversion function not found (no appropriate function for conversion path)'\r\n```\r\nI suspect `keras_version` attribute has `CSET H5T_CSET_ASCII;` which `h5py` fails to convert to `UTF8` when architectures are different.\r\n\r\nIf  `h5py` is indeed the cause of these failures (which I suspect), then these testcases can be disabled on non-x86 archs or compatible Keras application models be used.", "comments": ["I notice that these test cases have `\"no_oss\"` defined in the BUILD file.  Perhaps not applicable on s390x?", "Closing as per `no_oss`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39754\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39754\">No</a>\n", "`no_oss` means that the test was broken in OSS at some point."]}, {"number": 39753, "title": "MultiWorkerMirroredStrategy assign a wrong device", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS Linux release 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: nan\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.6.10\r\n- Bazel version (if compiling from source): nan\r\n- GCC/Compiler version (if compiling from source): nan\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: nan\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nUse distribution strategy MultiWorkerMirroredStrategy in graph mode, throw an error\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation metrics/auc/Identity: {{node metrics/auc/Identity}} was explicitly assigned to /replica:0/task:0/device:CPU:0 but available devices are [ /job:worker/replica:0/task:1/device:CPU:0, /job:worker/replica:0/task:1/device:GPU:0, /job:worker/replica:0/task:1/device:XLA_CPU:0, /job:worker/replica:0/task:1/device:XLA_GPU:0 ]. Make sure the device specification refers to a valid device.`\r\n\r\nassign a wrong device which is `/replica:0/task:0/device:CPU:0` on task1\r\n\r\n**Describe the expected behavior**\r\n\r\ntrain normly\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\n# file demo.py\r\nimport sys\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\ngraph_context = tf.Graph().as_default()\r\nstrategy_context = strategy.scope()\r\n\r\nwith graph_context:\r\n    with strategy_context:\r\n        ip = tf.keras.layers.Input([2])\r\n        h = tf.keras.layers.Dense(10, activation='relu', input_dim=2)(ip)\r\n        out = tf.keras.layers.Dense(2, activation='softmax')(h)\r\n        model = tf.keras.models.Model(inputs=[ip], outputs=[out])\r\n\r\n        model.compile(optimizer='Adam',\r\n                loss='categorical_crossentropy',\r\n                metrics=[tf.keras.metrics.AUC(num_thresholds=100)])\r\n\r\n    x = np.random.randn(100, 2)\r\n    y = (x[:, 0] * x[:, 1]) > 0\r\n    model.fit(x, tf.keras.utils.to_categorical(y), epochs=1)\r\n```\r\n\r\nrun above script command:\r\n```\r\nTF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:2222\",\"localhost:2223\"]},\"task\":{\"type\":\"worker\",\"index\":0}}' CUDA_VISIBLE_DEVICES=0 python demo.py\r\nTF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:2222\",\"localhost:2223\"]},\"task\":{\"type\":\"worker\",\"index\":1}}' CUDA_VISIBLE_DEVICES=1 python demo.py\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1367, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\r\n    self._extend_graph()\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1390, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation metrics/auc/Identity: {{node metrics/auc/Identity}} was explicitly assigned to /replica:0/task:0/device:CPU:0 but available devices are [ /job:worker/replica:0/task:1/device:CPU:0, /job:worker/replica:0/task:1/device:GPU:0, /job:worker/replica:0/task:1/device:XLA_CPU:0, /job:worker/replica:0/task:1/device:XLA_GPU:0 ]. Make sure the device specification refers to a valid device.\r\n         [[metrics/auc/Identity]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"../x.py\", line 22, in <module>\r\n    model.fit(x, tf.keras.utils.to_categorical(y), epochs=1)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 790, in fit\r\n    *args, **kwargs)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 777, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 772, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 619, in fit\r\n    epochs=epochs)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2200, in _distribution_standardize_user_data\r\n    session = K.get_session()\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 496, in get_session\r\n    _initialize_variables(session)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 911, in _initialize_variables\r\n    [variables_module.is_variable_initialized(v) for v in candidate_vars])\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 960, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1183, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1361, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1386, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation metrics/auc/Identity: node metrics/auc/Identity (defined at ../x.py:18)  was explicitly assigned to /replica:0/task:0/device:CPU:0 but available devices are [ /job:worker/replica:0/task:1/device:CPU:0, /job:worker/replica:0/task:1/device:GPU:0, /job:worker/replica:0/task:1/device:XLA_CPU:0, /job:worker/replica:0/task:1/device:XLA_GPU:0 ]. Make sure the device specification refers to a valid device.\r\n         [[metrics/auc/Identity]]\r\n```", "comments": ["@Hacky-DH \r\nI ran your code on tf 2.1 and do  not face any error as reported by you, please refer to this [gist here](https://colab.sandbox.google.com/gist/Saduf2019/c590f7647c992bacefd61af1e474aa3c/untitled195.ipynb)", "@Saduf2019 use following commands to run the code\r\n```\r\nTF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:2222\",\"localhost:2223\"]},\"task\":{\"type\":\"worker\",\"index\":0}}' CUDA_VISIBLE_DEVICES=0 python demo.py\r\nTF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:2222\",\"localhost:2223\"]},\"task\":{\"type\":\"worker\",\"index\":1}}' CUDA_VISIBLE_DEVICES=1 python demo.py\r\n```\r\n\r\nJust running `python demo.py` is one worker trainning, not multi worker", "If you want to run with graph mode in TF2, please try the following and let us know if the issue is still there:\r\n\r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\nwith strategy.scope():\r\n  model = .... # rest of your code as-is.\r\n\r\n```", "@guptapriya \r\n\r\nI modify code as you say, an error occurs:\r\n```\r\nTraceback (most recent call last):\r\n  File \"demo.py\", line 14, in <module>\r\n    h = tf.keras.layers.Dense(10, activation='relu', input_dim=2)(ip)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 748, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2116, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py\", line 1113, in build\r\n    trainable=True)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 446, in add_weight\r\n    caching_device=caching_device)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\", line 744, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 142, in make_variable\r\n    shape=variable_shape if variable_shape else None)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\r\n    shape=shape)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1319, in creator_with_resource_vars\r\n    _require_strategy_scope_extended(self)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 253, in _require_strategy_scope_extended\r\n    _wrong_strategy_scope(strategy, context)\r\n  File \"/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 218, in _wrong_strategy_scope\r\n    (strategy,))\r\nRuntimeError: Need to be inside \"with strategy.scope()\" for <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategyV1 object at 0x7fcd8459c208>\r\n```", "Can you please provide the complete code you tried? from the error message it seems that it thinks you're no longer in the scope of the strategy. \r\nBTW do you need to use graph mode? Can you use TF2 without graph mode instead? We have done a lot more testing and improvements for `MultiWorkerMirroredStrategy` in TF2. ", "@guptapriya here is full code that I modified\r\n```\r\n# file demo.py\r\nimport sys\r\nimport numpy as np\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\ngraph_context = tf.Graph().as_default()\r\nstrategy_context = strategy.scope()\r\n\r\nwith graph_context:\r\n    with strategy_context:\r\n        ip = tf.keras.layers.Input([2])\r\n        h = tf.keras.layers.Dense(10, activation='relu', input_dim=2)(ip)\r\n        out = tf.keras.layers.Dense(2, activation='softmax')(h)\r\n        model = tf.keras.models.Model(inputs=[ip], outputs=[out])\r\n\r\n        model.compile(optimizer='Adam',\r\n                loss='categorical_crossentropy',\r\n                metrics=[tf.keras.metrics.AUC(num_thresholds=100)])\r\n\r\n    x = np.random.randn(100, 2)\r\n    y = (x[:, 0] * x[:, 1]) > 0\r\n    model.fit(x, tf.keras.utils.to_categorical(y), epochs=1)\r\n```\r\n\r\nyou can start with cmds:\r\n```\r\nTF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:2222\",\"localhost:2223\"]},\"task\":{\"type\":\"worker\",\"index\":0}}' CUDA_VISIBLE_DEVICES=0 python demo.py\r\nTF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:2222\",\"localhost:2223\"]},\"task\":{\"type\":\"worker\",\"index\":1}}' CUDA_VISIBLE_DEVICES=1 python demo.py\r\n```\r\n\r\nI tried to run without graph context, it works, but a warning message occurs\r\n```\r\nW tensorflow/core/grappler/optimizers/data/auto_shard.cc:428] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\r\n```\r\nHow to fix this?\r\n\r\nBTW, what are differences between with graph and without graph context using distribute strategy e.g. MultiWorkerMirroredStrategy or MirroredStrategy?\r\n\r\nThanks", "You don't need the graph context, I think make a separate graph is probably causing the problem you saw earlier. So running without that is the right thing to do.\r\n\r\nRegarding that warning: when using model.fit with MultiWorkerMirroredStrategy, we try to shard your data across workers by sharding the files across them. but when the dataset doesn't read from files like in this case, we cannot do that, and that's all that warning means. I am guessing this is a toy example, you should not see it with a realistic dataset that reads from files. \r\n\r\nYou can read more about this:\r\nhttps://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size\r\nhttps://www.tensorflow.org/api_docs/python/tf/distribute/Strategy?version=nightly#experimental_distribute_dataset\r\n\r\n", "I had another question: is keras model run in eager mode without graph context in MultiWorkerMirroredStrategy scope? if not, is it run in tf.function? \r\nDo I need pass run_eagerly=False to model.compile function?", "In TF2, keras model.fit uses tf.function under the hood automatically. So you don't need to pass run_eagerly=False, it is False by default. run_eagerly is only needed if you actually need op by op execution.\r\nHigh level question: why are you trying to use TF1 / legacy graph mode instead of just TF2 default?\r\n\r\n", "Thank you very much!\r\n\r\nI heard that it is much slower in eager mode than graph mode in TF2. So I put model.compile and dataset construction forcely in graph context.\r\n\r\nAs you said I will try remove the graph context in TF2.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39753\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39753\">No</a>\n"]}, {"number": 39752, "title": "unable to load model with custom objects on a Fresh Python Environment!", "body": "Hi all, i have an issue in loading a model with custom loss function in a fresh-separate IPython Environment. Following is a sample code i used.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\n\r\n\r\n# creating the model\r\nx = tf.keras.Input(shape = 10, name = \"Input\")\r\n\r\ndense1 = tf.keras.layers.Dense(units = 10, activation = None)(x)\r\ndense2 = tf.keras.layers.Dense(units = 10, activation = None)(dense1)\r\ndense3 = tf.keras.layers.Dense(units = 10, activation = None)(dense2)\r\noutput = tf.keras.layers.Dense(units = 10, activation = None)(dense3)\r\n\r\nmodel = tf.keras.Model(inputs = x, outputs = output)\r\n\r\n# custom loss function\r\ndef custom_loss_func(y_true, y_pred):\r\n    return K.mean(K.square(y_true - y_pred))\r\n\r\n# compiling the model\r\nmodel.compile(loss = custom_loss_func, optimizer = \"sgd\")\r\n\r\n# sample fit\r\nX = np.linspace(0,1,10).reshape(1,10)\r\nY = X**2\r\n\r\nmodel.fit(X,Y,epochs = 10, batch_size = 1)\r\n\r\nmodel.save(\"custom_model.h5\")\r\n\r\n# test code to load along with the model. and it works\r\n#  new_model = tf.keras.models.load_model(\"custom_model.h5\", custom_objects = {\"custom_loss_func\":custom_loss_func})\r\n```\r\nin the last line, if i uncomment and run the script means, it works fine as the function is already defined on top. \r\n```\r\n# test code to load along with the model. and it works\r\nnew_model = tf.keras.models.load_model(\"custom_model.h5\", custom_objects = {\"custom_loss_func\":custom_loss_func})\r\n```\r\n\r\nBut if i run above line on a separate fresh IPython environment which hasn't seen this code yet, it throws error as below.\r\n\r\n```\r\nIn [34]: new_model = tf.keras.models.load_model(\"custom_model.h5\", custom_objects = {\"custom_loss_func\":custom_loss_func})\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-34-77825814faa4> in <module>\r\n----> 1 new_model = tf.keras.models.load_model(\"custom_model.h5\", custom_objects = {\"custom_loss_func\":custom_loss_func})\r\n\r\nNameError: name 'custom_loss_func' is not defined\r\n```\r\nIts correct, because that function object isn't defined in the new environment. \r\nI tried by defining a dummy function in the same name and try to load model, it didn't work as well, the errors are about mismatched input parameters, then mismatched return object etc.. \r\n\r\nIs there any working way to load the model with custom objects back on a fresh environment?", "comments": ["@Ramkumar47 \r\n\r\nI have tried in colab with TF version 2.2 , and i am not seeing any issue after defining the custom loss function and loading the model.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/571c4ab9ac499b44ac1e0312e9157f50/untitled926.ipynb).Thanks!", "Now i understood, there is no way to load a model with custom objects, without pre-defining them in the new environment.. Thank you @ravikyram "]}, {"number": 39751, "title": "Non-deterministic behaviour: tf.math.unsorted_segment_sum uses CUDA Atomic Operations", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):v2.1.0-rc2-17-ge5bf8de and v2.2.0-rc4-8-g2b96f3662b\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1.105 and 7.6.5.32\r\n- GPU model and memory: RTX6000 24GB\r\n\r\n**Describe the current behavior**\r\nCurrently, tf.math.unsorted_segment_sum uses non-deterministic GPU kernels which lead significant failings in the TensorFlow determinism venture. Other TensorFlow functions make use of tf.math.unsorted_segment_sum such as tf.gather (on backprop).\r\n\r\nSome functions affected that I've discovered:\r\n - tfa.image.dense_image_warp (on backprop)\r\n - tf.gather (on backprop)\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen TF_DETERMINISTIC_OPS=1,  tf.math.unsorted_segment_sum should use deterministic GPU kernels leading to reproducibility.\r\n\r\n**Who will benefit from this bug fix correction?**\r\nDeterminism is an extremely important part of our venture into deep learning as a community. Without determinism, it is hard to reliably tune hyperparameters and conduct other types of investigations such ablation studies. Whilst many TensorFlow operations have a deterministic alternative upon setting the OS Environment variable TF_DETERMINISTIC_OPS=1, tf.math.unsorted_segment_sum seems to have fallen under the radar, perhaps because other operations took priority (such as tf.reduce_sum). \r\n\r\nIntroducing this level of determinism to TensorFlow will allow it to be a better candidate for deep learning deployments in more sensitive environments such as medical. I.e. it doesn't make sense that a radiologist will look at result during one scan and then conduct the same scan and get a different result. It also affects the public's trust in AI venture altogether. ~~As far as I'm aware, PyTorch offers full deterministic capabilities (perhaps due to the benefit of hindsight with TensorFlow not having it).~~ \r\n\r\n**Standalone code to reproduce the issue**\r\nCode to reproduce the issue:\r\n(Edit: Please see the code here instead: https://github.com/tensorflow/tensorflow/issues/39751#issuecomment-632590302\r\n\r\nI've added seed settings, TF_DETERMINISTIC_OPS, etc... and the issue still reproduces)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnum_segments = 4\r\ndata = tf.random.normal([30, 256, 256])\r\ndata = tf.constant(data)\r\nsegments = np.random.randint(low=0, high=num_segments, size=data.shape)\r\n\r\nfor i in range(5):\r\n    reduced_summed = tf.math.unsorted_segment_sum(data, segments, num_segments)\r\n    print(reduced_summed)\r\n```\r\nOutput:\r\n\r\ntf.Tensor([-273.92117  380.23163 1279.9718  -839.6437 ], shape=(4,), dtype=float32)\r\ntf.Tensor([-273.92395  380.22168 1279.9834  -839.62573], shape=(4,), dtype=float32)\r\ntf.Tensor([-273.91425  380.22177 1279.9773  -839.62976], shape=(4,), dtype=float32)\r\ntf.Tensor([-273.9177  380.2243 1279.9733 -839.6427], shape=(4,), dtype=float32)\r\ntf.Tensor([-273.91568  380.2217  1279.9747  -839.64166], shape=(4,), dtype=float32)\r\n\r\n# Note: all printed results are different but in reality, they should be the same\r\n\r\nColab notebook with this code can be found at : https://colab.research.google.com/drive/1HNHSfERQ_IDDDM7bgabii9TQPufpsutp?usp=sharing\r\n\r\n** Unit Tests **\r\nEssentially, the code above will produce the same result, rather than a different result every time it is executed in the for loop.\r\n\r\nComing soon.\r\n\r\n\r\n**Other info / logs**\r\n\r\nMore information about the GPU operation can be found at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/segment_reduction_ops_gpu.cu.cc\r\n\r\nMore information coming soon", "comments": ["@gavinlive,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "> @gavinlive,\r\n> In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!\r\n\r\nHi, the code to reproduce the issue has now been added to the first post and again here:\r\n(Edit:  Please see code in the next comment instead)\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnum_segments = 4\r\ndata = tf.random.normal([30, 256, 256])\r\ndata = tf.constant(data)\r\nsegments = np.random.randint(low=0, high=num_segments, size=data.shape)\r\n\r\nfor i in range(5):\r\n    reduced_summed = tf.math.unsorted_segment_sum(data, segments, num_segments)\r\n    print(reduced_summed)\r\n```\r\nOutput:\r\n\r\ntf.Tensor([-273.92117  380.23163 1279.9718  -839.6437 ], shape=(4,), dtype=float32)\r\ntf.Tensor([-273.92395  380.22168 1279.9834  -839.62573], shape=(4,), dtype=float32)\r\ntf.Tensor([-273.91425  380.22177 1279.9773  -839.62976], shape=(4,), dtype=float32)\r\ntf.Tensor([-273.9177  380.2243 1279.9733 -839.6427], shape=(4,), dtype=float32)\r\ntf.Tensor([-273.91568  380.2217  1279.9747  -839.64166], shape=(4,), dtype=float32)\r\n\r\n# Note: all printed results are different but in reality, they should be the same", "> @gavinlive,\r\n> In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!\r\n\r\nPlease add TF2.2 label.\r\n\r\nResults are reproducible in TF2.2 as well.\r\n\r\nFor completeness, I've added seeds and TF_DETERMINISTIC_OPS within Python script:\r\n\r\n```\r\nimport os\r\nseed=1114\r\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\r\nos.environ['PYTHONHASHSEED'] = str(seed)\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport random\r\nrandom.seed(seed)\r\nnp.random.seed(seed)\r\ntf.random.set_seed(seed)\r\n\r\nprint(tf.version.VERSION)\r\nprint(tf.version.GIT_VERSION)\r\n\r\n\r\n\r\nnum_segments = 4\r\ndata = tf.random.normal([30, 256, 256])\r\ndata = tf.constant(data).numpy()\r\nsegments = np.random.randint(low=0, high=num_segments, size=data.shape)\r\n\r\n\r\n\r\nfor i in range(5):\r\n    reduced_summed = tf.math.unsorted_segment_sum(data, segments, num_segments)\r\n    print(reduced_summed)\r\n\r\n# Note: all printed results are different but in reality, they should be the same\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n2.2.0\r\nv2.2.0-rc4-8-g2b96f3662b\r\ntf.Tensor([-504.54437 1059.4211  -496.0039  -987.54095], shape=(4,), dtype=float32)\r\ntf.Tensor([-504.54556 1059.4254  -496.00696 -987.52454], shape=(4,), dtype=float32)\r\ntf.Tensor([-504.54715 1059.4282  -496.00647 -987.53094], shape=(4,), dtype=float32)\r\ntf.Tensor([-504.54132 1059.4308  -496.00787 -987.5342 ], shape=(4,), dtype=float32)\r\ntf.Tensor([-504.53625 1059.4412  -496.01093 -987.5367 ], shape=(4,), dtype=float32)\r\n```", "Added a Colab notebook also: https://colab.research.google.com/drive/1HNHSfERQ_IDDDM7bgabii9TQPufpsutp\r\n\r\n^ It shows a temporary fix for anyone else experiencing this issue, is to increasing the precision (via tf.cast) and then use unsorted_segment_sum, and then immediately decrease the precision (to the original data type+precision).\r\n\r\n\r\nSo for example, to fix the tf.gather operation during backpropagation, you can edit  array_grad.py (tensorflow/python/ops), and replace `math_ops.unsorted_segment_sum` with `unsorted_segment_sum_fix`, and add something similar to the following code:\r\n```\r\ndef unsorted_segment_sum_fix(*args, **kwargs):\r\n  args = list(args)\r\n  params = args[0]\r\n  orig_dtype = params.dtype\r\n  if 'float' in str(params.dtype):\r\n    params = math_ops.cast(params, dtype=dtypes.float64)\r\n  elif 'complex' in str(params.dtype):\r\n    params = math_ops.cast(params, dtype=dtypes.complex128)\r\n  args[0] = params\r\n  result = math_ops.unsorted_segment_sum(*args, **kwargs)\r\n  result = math_ops.cast(result, dtype=orig_dtype)\r\n  return result\r\n```", "Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/17820dde35568591baa98206d1388a20/39751-2-1.ipynb), [TF v2.2](https://colab.research.google.com/gist/amahendrakar/fca593f80e7de9d32a3665bd98c1a531/39751-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/1d2d581d32e765c575bf52e19a1a7856/39751-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@gavinlive,\r\n\r\n> PyTorch offers full deterministic capabilities\r\n\r\nThis is not true. TensorFlow's support for GPU determinism is now at least comparable to that of PyTorch.\r\n\r\nBoth TensorFlow and PyTorch have (had) various ops that function non-deterministically on GPU, some that have already been addressed, some that are in line to be addressed, some that are coming into awareness (such as this one), and likely others that are still to be discovered.\r\n\r\nWe are gradually working though these non-deterministic ops, identifying them, confirming that they truly operate non-deterministically, and implementing/testing fully-functional deterministic versions of them. These deterministic versions are then either enabled via `TF_DETERMINISTIC_OPS` (or a future equivalent) or completely replace the original, if that does not negatively impact performance.\r\n\r\nThank you for bringing the non-determinism of `unsorted_segment_sum` to our attention.", "> @gavinlive,\r\n> \r\n> > PyTorch offers full deterministic capabilities\r\n> \r\n> This is not true. TensorFlow's support for GPU determinism is now at least comparable to that of PyTorch.\r\n> \r\n> Both TensorFlow and PyTorch have (had) various ops that function non-deterministically on GPU, some that have already been addressed, some that are in line to be addressed, some that are coming into awareness (such as this one), and likely others that are still to be discovered.\r\n> \r\n> We are gradually working though these non-deterministic ops, identifying them, confirming that they truly operate non-deterministically, and implementing/testing fully-functional deterministic versions of them. These deterministic versions are then either enabled via `TF_DETERMINISTIC_OPS` (or a future equivalent) or completely replace the original, if that does not negatively impact performance.\r\n> \r\n> Thank you for bringing the non-determinism of `unsorted_segment_sum` to our attention.\r\n\r\nThanks for that! :) I'll correct my comment. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39751\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39751\">No</a>\n", "By the way, I wouldn't call this a bug. This a feature request. This is a request to extend the functionality of `TF_DETERMINISTIC_OPS`.", "I've reproduced this nondeterminism with the code in [this gist](https://gist.github.com/duncanriach/136fe08905af67f30c4eb775b34290d7). There's an issue with your code above, @gavinlive, in that the size of segments is not equal to the size of the first dimension of data.  With this addressed, the GPU-specific non-determinism is still present, but the API of the op is being more correctly followed. The code in the gist also includes a post-reduction of the summed segments in order to demonstrate the variation of a final scalar result. The gist compares the functionality on the CPU and the GPU and produces the following output:\r\n\r\n```\r\non CPU: -1957.1707153320312\r\non CPU: -1957.1707153320312\r\non CPU: -1957.1707153320312\r\non CPU: -1957.1707153320312\r\non CPU: -1957.1707153320312\r\non GPU: -1957.1702117919922\r\non GPU: -1957.1699676513672\r\non GPU: -1957.1700134277344\r\non GPU: -1957.1698608398438\r\non GPU: -1957.1700744628906\r\n```", "This [other issue](https://github.com/NVIDIA/framework-determinism/issues/25) led me to look at `tf.math.segment_sum` and I've confirmed that it also injects nondeterminism when running on a GPU. I have updated the aforementioned [gist](https://gist.github.com/duncanriach/136fe08905af67f30c4eb775b34290d7) to repro nondeterminism in both ops and also to maximize the probability of nondeterminism and minimize the run time. The output looks like this:\r\n\r\n```\r\n-----------------------------------\r\ntf.math.unsorted_segment_sum on cpu\r\n\r\n  71.9827442169189\r\n  71.9827442169189\r\n  71.9827442169189\r\n  71.9827442169189\r\n  71.9827442169189\r\n-----------------------------------\r\ntf.math.unsorted_segment_sum on gpu\r\n\r\n  71.9828128814697\r\n  71.9828295707703\r\n  71.9827938079834\r\n  71.9828252792358\r\n  71.9828085899353\r\n-----------------------------------\r\ntf.math.segment_sum on cpu\r\n\r\n  71.9828114509583\r\n  71.9828114509583\r\n  71.9828114509583\r\n  71.9828114509583\r\n  71.9828114509583\r\n-----------------------------------\r\ntf.math.segment_sum on gpu\r\n\r\n  71.9828183650970\r\n  71.9828221797943\r\n  71.9828226566315\r\n  71.9828269481659\r\n  71.9828283786774\r\n-----------------------------------\r\n```\r\n\r\nBy the way, I'm assuming that this nondeterminism is related to the use of CUDA `atomicAdd` in `segment_reduction_ops.h` and `segment_reduction_ops_gpu.cu.cc` (which I've scouted-out in the past).", "@gavinlive \r\nThanks for your fix for `tf.gather`, I was also facing this issue. However, it seems like `tf.gather` doesn't explicitly call `unsorted_segment_sum` if `axis=0` in `tf.gather` https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/array_grad.py#L642. But `gen_math_ops.unsorted_segment_sum` does indeed get called at some point. I'm not familiar with how tensorflow code works and how gradients are automatically computed so I couldn't make the necessary changes in `array_grad.py`. Instead I just changed `unsorted_segment_sum` directly in `tensorflow/python/ops/gen_math_ops.py`, which seemed to do the trick for me!", "@dangthatsright,\r\n\r\nLooking back at the email conversation I had with @gavinlive before he created this issue, I can see that I traced down to [`_GatherGrad`](https://github.com/tensorflow/tensorflow/blob/v2.3.0-rc2/tensorflow/python/ops/array_grad.py#L543-L564) in `tensorflow/python/ops/array_grad.py`, which _always_ calls [`_IndexedSlicesToTensorNoWarning`](https://github.com/tensorflow/tensorflow/blob/v2.3.0-rc2/tensorflow/python/ops/array_grad.py#L529-L540) in the same file, which in turn calls `math_ops.unsorted_segment_sum` (the internal way of calling `tf.math.unsorted_segment_sum`).\r\n\r\nBased on your comments above, I see that for the TF2 API, the `tf.gather` forward path is implemented by [`gather_v2`](https://github.com/tensorflow/tensorflow/blob/v2.3.0-rc2/tensorflow/python/ops/array_ops.py#L4681-L4695) in `tensorflow/python/ops/array_ops.py`. which, incidentally, immediately calls the API 1 version of the gather functionality (named `gather`). However, as your comment implies, the gradient function in the TF 2 API is implemented by [`GatherV2`](https://github.com/tensorflow/tensorflow/blob/v2.3.0-rc2/tensorflow/python/ops/array_grad.py#L618-L688). Which, however, again also _always_ calls `_IndexedSlicesToTensorNoWarning` in the same file, which in turn (as before) calls `math_ops.unsorted_segment_sum` (the internal way of calling `tf.math.unsorted_segment_sum`).\r\n\r\nThis is why/how the API-equivalent version of `tf.math.unsorted_segment_sum` is _always_ called to calculate the backprop of `tf.gather`.\r\n\r\nThe python modules (files) with names starting with `gen_`, such as `gen_math_ops` are automatically generated by the TensorFlow build process (from protobuffer interface specifications), and are part of the linkage between the C++/CUDA code and the python API code. When the code in `tensorflow/python/ops` calls a function in `gen_*_ops`, it's calling down to that lower level.\r\n\r\nI can't find where `tf.math.segment_sum` or `tf.math.unsorted_segment_sum` are explicitly exported (with `@tf_export`), even in [`tensorflow/python/ops/math_ops.py`](https://github.com/tensorflow/tensorflow/blob/v2.3.0-rc2/tensorflow/python/ops/math_ops.py) and I'm assuming that the API linkage gets automatically generated (using [`tensorflow/core/api_def/python_api/api_def_UnsortedSegmentSum.pbtxt`](https://github.com/tensorflow/tensorflow/blob/v2.3.0-rc2/tensorflow/core/api_def/python_api/api_def_UnsortedSegmentSum.pbtxt)) and that there doesn't need to be any intermediate code.\r\n\r\nWhat specifically did you change about `unsorted_segment_sum` in `tensorflow/python/ops/gen_math_ops.py` to get deterministic functionality? I'm wondering if this change could be implemented in `tensorflow/python/ops/array_grad.py` (or somewhere else in the source code) under the control of `TF_DETERMINISTIC_OPS` as a temporary solution until we implement deterministic CUDA kernels for these ops.\r\n\r\n", "You are right that gather always calls `_IndexedSlicesToTensorNoWarning`, so I was curious as to why it wasn't giving me deterministic results still. After putting in prints right before https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/ops/array_grad.py#L533 and L538, I found that my code always takes the first return path. (Or maybe it doesn't and the print gets optimized out or something?). In fact I put prints before all the `math_ops.unsorted_segment_sum` in the file and I saw nothing so I am a bit at loss as to which part of my code is actually calling this function.\r\n\r\nAnyways, the change I made is to the `gen_math_ops.py` file is a bit hacky haha\r\nFollowing the hint: \r\nReplace the original `unsorted_segment_sum` with:\r\n\r\n```\r\ndef unsorted_segment_sum(data, segment_ids, num_segments, name=None):\r\n  orig_dtype = data.dtype\r\n  if 'float' in str(data.dtype):\r\n    data = cast(data, _dtypes.float64)\r\n  elif 'complex' in str(data.dtype):\r\n    data = cast(data, _dtypes.complex128)\r\n  result = unsorted_segment_sum_base(data, segment_ids, num_segments, name)\r\n  result = cast(result, orig_dtype)\r\n  return result\r\n```\r\n\r\nand then take the code previously in `unsorted_segment_sum` and put it in\r\n\r\n`\r\ndef unsorted_segment_sum_base(data, segment_ids, num_segments, name=None):\r\n`\r\n\r\nThis gave me deterministic results.\r\n\r\nIt could be possible that I have another function calling `unsorted_segment_sum` but that seems unlikely because calling `tf.gather` with cpu also allowed me to get deterministic results. ", "Ah, right. Yes, I forgot about the temporary solution that @gavinlive provided ([link to his comment](https://github.com/tensorflow/tensorflow/issues/39751#issuecomment-632618778) above). I reviewed this back when he wrote it and then I got side-tracked and I forgot to come back and comment on it. This is a super-accumulator style solution. It's good to know that it doesn't have a significant negative performance impact in the context of your application.\r\n\r\nWhen I looked at this briefly before, I was not convinced that this would provide a perfectly deterministic solution. I was concerned that there might be rare cases where it would produce a different result between runs. I've been thinking about this some more today and I'm still concerned. Here's a conceptual example using decimal arithmetic in which the super-accumulator has four decimal places and the input/output values have only two decimal places:\r\n\r\nSuper-Accumulator Run 1: 0.9999\r\nSuper-Accumulator Run 2: 1.0001\r\n\r\nOutput Run 1: 0.99\r\nOutput Run 2: 1.00\r\n\r\nThe nondeterminism in the super-accumulator still gets through, even though its variance is much smaller than the precision of the output data type.\r\n\r\nThe above assumes that `tf.cast` simply discards the least-significant bits, which I'm almost certain it does (though I have not checked). I suspect that it could be made more robust with rounding before truncating, and that would protect the output from variance that was smaller than half the LSB in the output precision.\r\n\r\nThat still assumes that you don't have enough accumulated 64-bit floating-point rounding errors to spill up into the 32-bit mantissa. My sense is that that would be true in many or most cases, but without an in-depth analysis I would not be willing to claim that the solution was robustly deterministic for a wide range of applications with massive amounts of compute.\r\n\r\nRegarding making sure that the changes get run. I notice that `_GatherGradV2` [calls](https://github.com/tensorflow/tensorflow/blob/v2.3.0-rc2/tensorflow/python/ops/array_grad.py#L677) [`_BatchGatherGrad`](https://github.com/tensorflow/tensorflow/blob/v2.3.0-rc2/tensorflow/python/ops/array_grad.py#L588-L615) when `axis_static != 0`. `_BatchGatherGrad` calls `math_ops.unsorted_segment_sum`. Perhaps that's the other path the was being taken into the nondeterministic version of `math_ops.unsorted_segment_sum`.\r\n\r\nYou, and others reading this, might be interested in the following: rather than modifying the source code in the auto-generated file `gen_math_ops.py`, it's possible to apply a dynamic patch from your python program, after importing TensorFlow. You can create a copy of the reference to the existing function, initially bound to `math_ops.unsorted_segment_sum`, and can then call that from your new wrapper function (similar to what you have provided above), which can then be bound to `math_ops.unsorted_segment_sum`. This kind of (dynamic programming) re-binding approach is used with the existing, and upcoming, `fwd9m.tensorflow` patches. Here is an [example](https://github.com/NVIDIA/framework-determinism/blob/b4ec855d5b72107218ed5980004955e89df572a4/fwd9m/tensorflow/patch.py#L84-L87) for `tf.nn.bias_add`, which demonstrates only function replacement and not the wrapping.\r\n\r\nThe following binding points (at least) are relevant (to address the segment ops and also the ops that depend on them for backprop, such as `tf.gather`):\r\n  * `tf.math.segment_sum`\r\n  * `tensorflow.python.math_ops.segment_sum`\r\n  * `tf.math.unsorted_segment_sum`\r\n  * `tensorflow.python.math_ops.unsorted_segment_sum`\r\n\r\nConclusion:\r\n\r\n  1. I recommend rounding rather than truncating. I think this will be more deterministic.\r\n  2. If this works consistently for your model, then that's awesome.\r\n  3. I'm going to reconsider whether to implement and deploy this temporary solution (with rounding; probably initially via a patch).\r\n  4. You, or others, might want to implement this temporary solution, for now, using the dynamic programming approach rather than modifying the auto-generated source code or post-bazel-built code and/or having to rebuild a wheel.\r\n", "Thank you for introducing me to what a super accumulator is!\r\n\r\nRegarding your edge case and rounding: that makes a lot of sense and I agree that it also depends on how tf.cast is implemented. I don't know how it's implemented, but trying these two examples out they seem to be the same\r\n```\r\ntf.cast(tf.constant([0.99999999], dtype=tf.float64),tf.float32)\r\ntf.cast(tf.constant([1.00000001], dtype=tf.float64),tf.float32)\r\n```\r\nso that's promising at least. That said rounding seems like a reasonable safety net.\r\n\r\nRegarding accumulating enough errors to spill into float32: while it is theortically possible, I am pretty sure it won't for our system, since it's pretty small. Looking at https://en.wikipedia.org/wiki/Kahan_summation_algorithm#Accuracy, it seems like it can handle 10^16 values? \r\n\r\nRegarding where `_GatherGradV2` actually calls `unsorted_segment_sum`: I know that my code always takes the L533 branch, that I mentioned above, so it is still a mystery to me when `unsorted_segment_sum` actually gets called. \r\n\r\nRegarding the dynamic rebinding of the functions: thanks for this, I was having a hard time figuring out how to incorporate this code into my codebase but this provides me a much better alternative approach.", "Got the patch to work for me! Just a modification: `tf.python.ops.math_ops.unsorted_segment_sum` should be `tf.python.math_ops.unsorted_segment_sum`", "Regarding the code path: in the version of `array_grad.py` that you link to above, there is no `_GatherGradV2`. It's in the V2 version of the grad function, when `axis_static != 0`, that first `_IndexedSlicesToTensorNoWarning` is [called](https://github.com/tensorflow/tensorflow/blob/v2.3.0-rc2/tensorflow/python/ops/array_grad.py#L668), and then `_BatchGatherGrad` is [called](https://github.com/tensorflow/tensorflow/blob/v2.3.0-rc2/tensorflow/python/ops/array_grad.py#L668). This is an example where the line you're mentioning could be traversed before `math_ops.unsorted_segment_sum` is called. My understanding, from our discussion on framework-determinism [issue 25](https://github.com/NVIDIA/framework-determinism/issues/25), is that you're using TensorFlow version 2.2.0. The version of [array_grad.py](https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/array_grad.py#L618-L687) under the `v2.2.0` tag definitely contains `_GatherGradV2`, which calls `_BatchGatherGrad`.\r\n\r\nRegarding your example related to truncation and rounding: thank you for checking. I used decimal examples earlier only to demonstrate the concept. To take it to a deeper level of rigor, I tested the hypothesis using decimal numbers that should exactly translate into floating-point representations with a clear precision difference. FP32 has 23 bits of mantissa; FP64 has 52 bits of mantissa. A difference from 1.0 that is definitely representable by FP64 but not by FP32 would be 0.5^40 (one bit shifted to the right 40 places). So I tested this:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef test_cast(n):\r\n  fp64 = tf.constant(n, dtype=tf.float64)\r\n  print(\"\\nFP64: %16.13f\" % fp64)\r\n  fp32 = tf.cast(fp64, dtype=tf.float32)\r\n  print(\"FP32: %16.13f\" % fp32)\r\n\r\nd = 0.5**40\r\nfor n in [1-d, 1+d, -1-d, -1+d]:\r\n  test_cast(n)\r\n```\r\n\r\nWhich produces:\r\n\r\n```\r\nFP64:  0.9999999999991\r\nFP32:  1.0000000000000\r\n\r\nFP64:  1.0000000000009\r\nFP32:  1.0000000000000\r\n\r\nFP64: -1.0000000000009\r\nFP32: -1.0000000000000\r\n\r\nFP64: -0.9999999999991\r\nFP32: -1.0000000000000\r\n```\r\n\r\nFor completeness, I also tested (results reasonable and expected):\r\n  * `d=0.5**23`: all the combinations (1-d, 1+d, -1-d, and -1+d) are exactly representable in both FP32 and FP64 formats (+/- `0.9999998807907` => +/- `0.9999998807907`).\r\n  * `d=0.5**24`: half the combinations (1-d, -1+d) were exactly representable in both FP32 and FP64 formats (+/- `0.9999999503954` => `0.9999999503954`) and the other half (1+d, -1-d) were not (+/- `1.0000000596046` => +/- `1.0000000000000`).\r\n * `d=0.5**25`: all the combinations round the same way (+/- `0.9999999701977` => +/- `1.0000000000000` and +/- `1.0000000298023` => +/- `1.0000000000000`).\r\n\r\nThis is solid confirmation that `tf.cast` does, in fact, round and does not truncate (as `cast` does in at least some other languages). So, I'm happy with using `tf.cast` without pre-rounding. Pre-rounding isn't necessary.\r\n\r\nRegarding accumulating enough errors to spill up into float32: Refining what you wrote, going from 52 to 23 mantissa bits, it would be necessary for there to be on the order of 2^(52-23) = 2^29 = 536,870,912 floating-point rounding errors that accumulated in the same direction (with no canceling-out, which would be extremely unlikely) for the error to spill up into the 32-bit result. I've also just recognized that this would be limited to each output element of the segment sum and not to the entire model. Even with the extremely unlikely case of all non-cancelling errors, I think there is almost zero probability of there being 536,870,912 of the same segment ID in the `segment_ids` list parameter. Therefore, I'm now also comfortable with this aspect of the (temporary) solution.\r\n\r\n> Got the patch to work for me! Just a modification: tf.python.ops.math_ops.unsorted_segment_sum should be tf.python.math_ops.unsorted_segment_sum\r\n\r\nThanks. I've edited that in [my comment](https://github.com/tensorflow/tensorflow/issues/39751#issuecomment-664764793) above.\r\n\r\nConclusion: I'm planning to add this patch, thoroughly tested (on all versions), to `framework-determinism.tensorflow` to enhance determinism in all versions of TensorFlow going back to at least version 1.14. Once that's done it may be possible/worth upstreaming this temporary solution.\r\n\r\nThis is great, collaborative work, @dangthatsright and @gavinlive. Thank you both and well done.", "For those who are interested, I've enhanced the analysis of `tf.cast` rounding in [this gist](https://gist.github.com/duncanriach/84b067302dad6c573ae9ebc8573dc489) to show the effects on the bits of the mantissa directly.", "We (@wenscarl and I) have isolated and reproduced a source of nondeterminism relevant to this issue. When `tf.gather` is used (such as when selecting embeddings for words in a sentence), backprop into a dense trainable variable (such as an embedding matrix) is nondeterministic, but not because the backprop of `tf.gather` is itself nondeterministic (as previously suggested). `tf.gather` produces sparse gradients, encoded using `tf.IndexedSlices`. When any given row in the embedding matrix is to be updated by more than two indexed slices in the sparse gradient tensor (more than two repeated indices), then the reduction via `tf.convert_to_tensor` introduces nondeterminism because it uses one of the segment sum ops, which have been confirmed to operate nondeterministically (see above).\r\n\r\nHere is [a gist](https://gist.github.com/duncanriach/c8155b34314ad829d9160fc2d5a80430) that repros this nondeterminism from `tf.convert_to_tensor`.\r\n\r\nWe are currently working on releasing a temporary dynamic patch for the segment sum ops in [GitHub/NVIDIA/framework-determinism](https://github.com/NVIDIA/framework-determinism), which will be upstreamed if possible in advance of a higher-performance and more complete (CUDA-level) solution (later).", "Note that [PR 47772](https://github.com/tensorflow/tensorflow/pull/47772), which has been merged and will be present in TensorFlow version 2.5 onwards, causes TensorFlow to throw `tf.errors.UnimplementedError` when `TF_DETERMINISTIC_OPS` is set to `\"true\"` or `\"1\"` (when determinism is expected) and the segment reduction ops are used in such a way that truly random noise will be injected into the model's functionality.\r\n\r\nThis exception-throwing functionality can currently be disabled by setting `TF_DISABLE_SEGMENT_REDUCTION_OP_DETERMINISM_EXCEPTIONS` to `\"true\"` or `\"1\"` in case you're using a temporary patch solution (such as [this one](https://github.com/NVIDIA/framework-determinism/blob/7b1878b2c87a5c4eb7f216f4627f07c3a3cd676f/fwd9m/tensorflow/enable_determinism.py#L65-L66)) that relies on higher-precision, nondeterministic underlying functionality to make the op function deterministically on the GPU.\r\n\r\nThis move to implement d9m-unimplemented exception throwing is orchestrated under this [determinism RFC](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md).", "Hey,\r\nThanks for the discussion. I found adding rounding + casting after unsorted_segment_sum is a temporary solution at least in my application:\r\n\r\nx= tf.math.unsorted_segment_sum(...)\r\nx = _my_tf_round(x,2)\r\ndef _my_tf_round(x, decimals=0):\r\n    multiplier = tf.constant(10 ** decimals, dtype=x.dtype)\r\n    return tf.cast(tf.cast(tf.round(x * multiplier),tf.int32),tf.float32) / multiplier \r\n\r\n", "@ChenchengLiang, thanks for the suggested work-around. Please will you put your code into a [fenced code block](https://docs.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks), with proper indenting, to make it easier to read?\r\n\r\nI believe that the work-around you have provided is robust in terms of making the overall operation perfectly reproducible, assuming that the maximum number of segments in a reduction is below an extremely large number (on the order of 2^17 = 131k). However, the rounding to two decimal places removes most of the precision, including the very high precision with very small numbers near zero. This means that this approach cannot be used where very high precision is required, such as when combining gradients to update a word embedding.", "This is fixed with https://github.com/tensorflow/tensorflow/pull/51861.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39751\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39751\">No</a>\n", "This still does not seemed to be fixed on Windows. Should this issue be re-opened? Or should a new issue be created?", "> This still does not seemed to be fixed on Windows. Should this issue be re-opened? Or should a new issue be created?\r\n\r\nI think a new issue should be opened."]}, {"number": 39750, "title": "`Tensor` slicing is ~300x slower than `numpy` slicing", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: macOS 10.14.5\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0, 2.2.0-dev20200405\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n\r\nSlicing a 1D `Tensor` is approximately 2 orders of magnitude slower than slicing a `numpy` `array`. Looking at the source code I'm not entirely sure whether this is a problem with `tensorflow` or `Eigen` so sorry if this in the wrong place!\r\n\r\n**Describe the expected behavior**\r\n\r\nI guess the expected behaviour is that tensorflow's slicing op has a competive performance to numpy. I'd be keen to implement a fix if its appropriate \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nHere's a colab link with the below code snippet: https://colab.research.google.com/drive/1XeAgdhBwzxAuy613myIGttKrQ8W_UNCK?usp=sharing\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\narr = np.random.random(10_000_000).astype(np.float32)\r\ntnsr = tf.constant(arr)\r\n\r\n%timeit _ = tnsr[:100]\r\n%timeit _ = arr[:100]\r\n```\r\n\r\nThis outputs:\r\n\r\n```\r\n50.7 \u00b5s \u00b1 668 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n163 ns \u00b1 2.52 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\r\n```\r\n\r\n", "comments": ["Maybe try using [ tf.slice](https://www.tensorflow.org/api_docs/python/tf/slice)?\r\n", "I only get _slightly_ better performance with `tf.slice` - `35us` instead of `50us` locally.", "Extra info: using CPU, not GPU.\r\n\r\nWas able to reproduce the problem", "@kieranricardo \r\nI ran the code shared, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/52d4d8b3c6057d48c866d6c3cb1f291e/untitled190.ipynb) and let us know if it replicates your issue.", "hi @Saduf2019, thanks that replicates my issue", "@Saduf2019 I was also able to replicate this on `tf-nightly`, I'll update my initial description", "The issue still persist; slicing tensors is an order of magnitude slower than slicing numpy arrays. \r\nI can see it under TF 2.4.0 on Colab.", "Could reproduce the issue in **`Tensorflow Version 2.5`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/c15402aeefe4319a756ab5fb91e745dd/untitled190.ipynb). Thanks!", "With the use of `tf.slice` in Tensorflow 2.7, there seems to be a huge improvement in performance, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/f25805975bb22b3ae8bd5e37ffe30bd9/39750.ipynb). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39749, "title": "PReLU operation throws error on tflite android benchmark tool with gpu", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Redmi Note 8 pro\r\n- TensorFlow installed from (source or binary): Google Colab Tensorflow Binary\r\n- TensorFlow version (use command below): tf  1.15 and tf 2.2\r\n- Python version: 3.6.9\r\n\r\n\r\n**Describe the current behavior**\r\nI converted a tf.keras model with PReLU operation into **tflite** format. The **android tflite benchmark tool** throws an error on the model with the **default PReLU settings of shared_axes=Null**\r\n\r\n\r\n```\r\nadb shell /data/local/tmp/benchmark_model_tf15 --graph=/data/local/tmp/prelu_model1.tflite --enable_op_profiling=true --use_gpu=true\r\nadb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)\r\nSTARTING!\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nMax runs duration (seconds): [150]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [/data/local/tmp/prelu_mod3.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nInput value ranges: []\r\nUse legacy nnapi : [0]\r\nAllow fp16 : [0]\r\nRequire full delegation : [0]\r\nEnable op profiling: [1]\r\nMax profiling buffer entries: [1024]\r\nCSV File to export profiling data to: []\r\nUse gpu : [1]\r\nAllow lower precision in gpu : [1]\r\nUse Hexagon : [0]\r\nHexagon lib path : [/data/local/tmp]\r\nHexagon Profiling : [0]\r\nUse nnapi : [0]\r\nUse xnnpack : [0]\r\nLoaded model /data/local/tmp/prelu_mod3.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\n**ERROR: TfLiteGpuDelegate Init: PRELU: Dimensions are not HWC**\r\nERROR: TfLiteGpuDelegate Prepare: delegate is not initialized\r\nERROR: Node number 2 (TfLiteGpuDelegateV2) failed to prepare.\r\n\r\nERROR: Restored previous execution plan after delegate application failure.\r\nFailed to apply GPU delegate.\r\nBenchmarking failed.\r\n```\r\n\r\nNow, with option **shared_axes=[1,2]**, it runs the model without any error on gpu.\r\nAlso, the model runs **without any error on CPU**  in both settings.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe tflite model with default PReLU settings (i.e no shared axes ), should run without any errors on gpu delegate in android.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nColab Notebook: https://colab.research.google.com/drive/1pZRaeUB7Gj1HU1BR9DcSI6SxHfdsWbD_#scrollTo=_63mXS3A1m0r\r\n\r\n**Other info / logs** \r\n\r\n* **prelu_model1.tflite**: No shared axes and **does not run on gpu**\r\n* **prelu_model2.tflite**: Has shared_axes=[1,2] and runs on gpu\r\n[prelu_models.zip](https://github.com/tensorflow/tensorflow/files/4662049/prelu_models.zip)\r\n\r\nThe behaviour seems to be same for tf 2.2 models and corresponding android benchmark tools.\r\nHow can we make it run on gpu with default settings ie. no shared _axes?", "comments": ["@anilsathyan7 \r\n\r\nRequest you to provide the access for  colab link you have shared.Thanks!", "Here is the updated link: https://colab.research.google.com/drive/1pZRaeUB7Gj1HU1BR9DcSI6SxHfdsWbD_?usp=sharing\r\n\r\nAre you able to view the notebook?", "> **System information**\r\n> \r\n> * Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Redmi Note 8 pro\r\n> * TensorFlow installed from (source or binary): Google Colab Tensorflow Binary\r\n> * TensorFlow version (use command below): tf  1.15 and tf 2.2\r\n> * Python version: 3.6.9\r\n> \r\n> **Describe the current behavior**\r\n> I converted a tf.keras model with PReLU operation into **tflite** format. The **android tflite benchmark tool** throws an error on the model with the **default PReLU settings of shared_axes=Null**\r\n> \r\n> ```\r\n> adb shell /data/local/tmp/benchmark_model_tf15 --graph=/data/local/tmp/prelu_model1.tflite --enable_op_profiling=true --use_gpu=true\r\n> adb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)\r\n> STARTING!\r\n> Min num runs: [50]\r\n> Min runs duration (seconds): [1]\r\n> Max runs duration (seconds): [150]\r\n> Inter-run delay (seconds): [-1]\r\n> Num threads: [1]\r\n> Benchmark name: []\r\n> Output prefix: []\r\n> Min warmup runs: [1]\r\n> Min warmup runs duration (seconds): [0.5]\r\n> Graph: [/data/local/tmp/prelu_mod3.tflite]\r\n> Input layers: []\r\n> Input shapes: []\r\n> Input value ranges: []\r\n> Use legacy nnapi : [0]\r\n> Allow fp16 : [0]\r\n> Require full delegation : [0]\r\n> Enable op profiling: [1]\r\n> Max profiling buffer entries: [1024]\r\n> CSV File to export profiling data to: []\r\n> Use gpu : [1]\r\n> Allow lower precision in gpu : [1]\r\n> Use Hexagon : [0]\r\n> Hexagon lib path : [/data/local/tmp]\r\n> Hexagon Profiling : [0]\r\n> Use nnapi : [0]\r\n> Use xnnpack : [0]\r\n> Loaded model /data/local/tmp/prelu_mod3.tflite\r\n> INFO: Initialized TensorFlow Lite runtime.\r\n> INFO: Created TensorFlow Lite delegate for GPU.\r\n> **ERROR: TfLiteGpuDelegate Init: PRELU: Dimensions are not HWC**\r\n> ERROR: TfLiteGpuDelegate Prepare: delegate is not initialized\r\n> ERROR: Node number 2 (TfLiteGpuDelegateV2) failed to prepare.\r\n> \r\n> ERROR: Restored previous execution plan after delegate application failure.\r\n> Failed to apply GPU delegate.\r\n> Benchmarking failed.\r\n> ```\r\n> \r\n> Now, with option **shared_axes=[1,2]**, it runs the model without any error on gpu.\r\nThis is an issue specific to the gpu delegate implementation. The error msg is emitted here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/common/model_builder_helper.cc#L252-L254\r\n\r\n> Also, the model runs **without any error on CPU** in both settings.\r\n> \r\n> **Describe the expected behavior**\r\n> \r\n> The tflite model with default PReLU settings (i.e no shared axes ), should run without any errors on gpu delegate in android.\r\n> \r\n> **Standalone code to reproduce the issue**\r\n> \r\n> Colab Notebook: https://colab.research.google.com/drive/1pZRaeUB7Gj1HU1BR9DcSI6SxHfdsWbD_#scrollTo=_63mXS3A1m0r\r\n> \r\n> **Other info / logs**\r\n> \r\n> * **prelu_model1.tflite**: No shared axes and **does not run on gpu**\r\n> * **prelu_model2.tflite**: Has shared_axes=[1,2] and runs on gpu\r\n>   [prelu_models.zip](https://github.com/tensorflow/tensorflow/files/4662049/prelu_models.zip)\r\n> \r\n> The behaviour seems to be same for tf 2.2 models and corresponding android benchmark tools.\r\n> How can we make it run on gpu with default settings ie. no shared _axes?\r\n\r\n", "@multiverse-tf So is it a bug? How can we fix it and properly run the model using gpu delegate ?", "We have a candidate and will check with your models.  Stay tuned :)", "Ok.  fix was submitted internally.  should go live sometime soon.  commit log should read something like \"TFLite GPU: Expand how an HWC tensor is read.\"  I'll close this issue for now; reopen if needed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39749\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39749\">No</a>\n"]}, {"number": 39748, "title": "ERROR: Encountered unresolved custom op: Dilation2D.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version : 2.1.0\r\n\r\n**Error Output**\r\n\r\n```\r\nERROR: Encountered unresolved custom op: Dilation2D.\r\nERROR: Node number 2 (Dilation2D) failed to prepare.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nlink to code: [code](https://github.com/pranv12/tflite_example_cc)\r\nlink to model: [model](https://drive.google.com/file/d/1a7HeTnzP9J2H5z6JQZAsoOLnVOsZiZt_/view?usp=sharing)\r\n\r\na command to reproduce error: `g++ *.cc  -ltensorflow -ltensorflowLite`\r\n\r\n**Any other info/logs**\r\nthere is no traceback in output.", "comments": ["Instead of using separate dilation ops, could you set the dilation parameter in the convolution ops?\r\n\r\nTFLite only supports a subset of TF ops, and `Dilation2D` op itself is not supported. Please refer to this compatibility guide: https://www.tensorflow.org/lite/guide/ops_compatibility.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39747, "title": "[Intel MKL] Vectorize FP32/BF16 RandomUniform process", "body": "Change the `Distribution` result length from fixed length `4` to `Eigen::internal::packet_traits<T>::size` and do computation with `Eigen::Tensor` to use Eigen packet feature for vectoring.\r\n\r\nFunctionality of this PR is independent, but its performance is depended on another public Eigen PR for BF16: https://gitlab.com/libeigen/eigen/-/merge_requests/84. So, currently this PR will only improve FP32 performance.\r\n\r\nSigned-off-by: Lu Teng teng.lu@intel.com", "comments": ["Added a new performance UT in `random_op_test.cc`. Below result is tested on an Intel X86 18 cores server:\r\n* FP32 is improved **~6%**\r\n* BF16 is not improved with this PR only\r\n* BF16 is improved **~30%** with this PR and Eigen PR\r\n\r\nBefore:\r\n```\r\nBM_cpu_RandomUniform_DT_FLOAT/1048576        995786        636\t 1053.0M items/s\r\nBM_cpu_RandomUniform_DT_FLOAT/2097152       1925301        365\t 1089.3M items/s\r\nBM_cpu_RandomUniform_DT_FLOAT/8388608       7276460        100\t 1152.8M items/s\r\nBM_cpu_RandomUniform_DT_BFLOAT16/1048576    1298896        559\t 807.3M items/s\r\nBM_cpu_RandomUniform_DT_BFLOAT16/2097152    2494553        282\t 840.7M items/s\r\nBM_cpu_RandomUniform_DT_BFLOAT16/8388608    9309730        100\t 901.1M items/s\r\n```\r\n\r\nAfter:\r\n```\r\nBM_cpu_RandomUniform_DT_FLOAT/1048576        935140        681\t 1121.3M items/s\r\nBM_cpu_RandomUniform_DT_FLOAT/2097152       1828172        373\t 1147.1M items/s\r\nBM_cpu_RandomUniform_DT_FLOAT/8388608       6838120        100\t 1226.7M items/s\r\nBM_cpu_RandomUniform_DT_BFLOAT16/1048576    1305523        524\t 803.2M items/s\r\nBM_cpu_RandomUniform_DT_BFLOAT16/2097152    2475075        280\t 847.3M items/s\r\nBM_cpu_RandomUniform_DT_BFLOAT16/8388608    9313620        100\t 900.7M items/s\r\n```\r\n\r\nWith Eigen PR:\r\n```\r\nBM_cpu_RandomUniform_DT_FLOAT/1048576        939294        683\t 1116.3M items/s\r\nBM_cpu_RandomUniform_DT_FLOAT/2097152       1827102        382\t 1147.8M items/s\r\nBM_cpu_RandomUniform_DT_FLOAT/8388608       6957620        100\t 1205.7M items/s\r\nBM_cpu_RandomUniform_DT_BFLOAT16/1048576     999265        713\t 1049.3M items/s\r\nBM_cpu_RandomUniform_DT_BFLOAT16/2097152    1917920        364\t 1093.5M items/s\r\nBM_cpu_RandomUniform_DT_BFLOAT16/8388608    7333820        100\t 1143.8M items/s\r\n```\r\n\r\n", "@Zantares Thank you for your contribution. Can you please sign CLA? Thanks!", "Added the `cla: yes` tag because I manually verified that the author has signed the CLA.", "Sorry for the new failure in CI, I saw the failure is `parameterized_truncated_normal_op_test` which is ignored in our internal test, I will check whether it's related to this PR, thanks.", "Figured out what the issue is, length is hard coded to `4` in `parameterized_truncated_normal`: https://github.com/tensorflow/tensorflow/blob/2b2df19f4b2a249821611e98786262585b57bebe/tensorflow/core/kernels/parameterized_truncated_normal_op.cc#L81\r\nThis is incompatible with changed length in Distribution.\r\n\r\nPlease hold on this PR for a while, we'll have a discussion and refine it in recent days.", "Added commit(https://github.com/tensorflow/tensorflow/pull/39747/commits/8b5a736628058dae19dcd1d868fc674dbba3f559) to fix CI failures:\r\n1. unstable order in BUILD file.\r\n2. hard code **4** breaks this optimization.\r\n\r\nWe have went through all places called Distribution to make sure correctness. The only uncertainty is that Generator is re-entrant  or not. This is because the Distribution will uses Generator 4 times per call, and other Distribution may call same Generator with conflicting sample index in parallel with current implementation:\r\n* Distribution 1 may uses Generator to generate index 1 ~ index 4 samples\r\n* Distribution 2 may uses Generator to generate index 2 ~ index 5 samples\r\nThe optimization is safe if this behaviour is acceptable, otherwise we need to redesign it. @penpornk do you have any opinion? ", "Still got 2 CI failures.\r\n\r\nThe 1st\r\n```\r\nerror: the value of 'size' is not usable in a constant expression\r\n```\r\nis my mistake, I used the fast committing of GitHub without local test then missed it. It's easy to be fixed.\r\n\r\nThe 2nd\r\n```\r\nERROR: T:/src/github/tensorflow/tensorflow/core/BUILD:446:1: in cc_library rule //tensorflow/core:framework: cycle in dependency graph:\r\n...\r\n.-> //tensorflow/core:framework\r\n|   //tensorflow/core:framework_internal\r\n|   //tensorflow/core:framework_internal_headers_lib\r\n|   //tensorflow/core:framework_internal_headers_lib_gather\r\n|   //tensorflow/core:lib\r\n|   //tensorflow/core:lib_internal\r\n|   //tensorflow/core:lib_internal_impl\r\n|   //tensorflow/core/lib/random:philox\r\n`-- //tensorflow/core:framework\r\n```\r\nreally confused me. I didn't see this error while building on Linux although I generated cycle dependency.\r\nI tried to remove it via calling low level Eigen API without framework Tensor definition but failed because currently there's no bfloat16 in Eigen. So, I need to find another way to resolve the dependency issue or await Eigen BF16 PR. I need to do more analysis, thanks\r\n\r\nIt will be very grateful if have any suggestion to solve the 2nd issue.\r\n  ", "@Zantares Sorry about the 1st failure. \r\n\r\nRe: 2nd failure: I can't look at it now but will try to do it this evening (Pacific time)!", "> @Zantares Sorry about the 1st failure.\r\n> \r\n> Re: 2nd failure: I can't look at it now but will try to do it this evening (Pacific time)!\r\n\r\nThanks for your quick reply! In fact the key point to me is how I reproduce this build error because it works well on Linux and I don't have Windows system to try... I have minimized the build dependency like this:\r\n```c\r\ndiff --git a/tensorflow/core/lib/random/BUILD b/tensorflow/core/lib/random/BUILD\r\nindex ba2c91e8b7..e6fefa249f 100644\r\n--- a/tensorflow/core/lib/random/BUILD\r\n+++ b/tensorflow/core/lib/random/BUILD\r\n@@ -40,7 +40,7 @@ cc_library(\r\n     deps = [\r\n         \":exact_uniform_int\",\r\n         \":philox_random\",\r\n-        \"//tensorflow/core:framework\",\r\n+        \"//tensorflow/core/framework:kernel_shape_util\",\r\n         \"//tensorflow/core/lib/bfloat16\",\r\n         \"//tensorflow/core/lib/gtl:array_slice\",\r\n         \"//tensorflow/core/platform:logging\",\r\n``` \r\nbut I'm not sure if it works.", "Very appreciate your kindly help and suggestions! Unit tests are passed with this solution. Since you have checked it on Windows, please take a look at the new commit when you get time, thanks!", "This optimization has impacted GPU unexpectedly. Although I got a fix, I need to discuss with my colleagues whether continue to do this optimization. I will update it later after making decision, thanks!", "@rthadur and @penpornk sorry to bother your again due to the internal error. I fixed it by separating CPU/GPU Distribution code with macro `__CUDA__ARCH__` like [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/random/philox_random.h#L216):\r\n```c\r\n#ifndef __CUDA_ARCH__\r\n    const uint64 product = static_cast<uint64>(a) * b;\r\n    *result_low = static_cast<uint32>(product);\r\n    *result_high = static_cast<uint32>(product >> 32);\r\n#else\r\n    *result_low = a * b;\r\n    *result_high = __umulhi(a, b);\r\n#endif\r\n```\r\n\r\nIn this way we can keep behaviour unchanged on GPU. \r\n", "@Zantares `RandomUniformTest.testCPUGPUMatch` failed on `Linux GPU`:\r\n```c++\r\nAssertionError:\r\nArrays are not equal\r\n\r\nnot equal where = (array([0, 0, 0, ..., 9, 9, 9]), array([125000, 125001, 125002, ..., 999997, 999998, 999999]))\r\nnot equal lhs = array([0.52766204, 0.69834983, 0.75964117, ..., 0.61097395, 0.84916604,\r\n       0.44497943])\r\nnot equal rhs = array([0.02572417, 0.84088182, 0.97234309, ..., 0.17118156, 0.46387005,\r\n       0.72925603])\r\n(mismatch 87.49999%)\r\n x: array([[0.674339, 0.23592 , 0.822064, ..., 0.617748, 0.301735, 0.971752],\r\n       [0.200281, 0.278307, 0.489626, ..., 0.011425, 0.856925, 0.108008],\r\n       [0.683009, 0.90563 , 0.523264, ..., 0.495125, 0.801837, 0.228648],...\r\n y: array([[0.674339, 0.23592 , 0.822064, ..., 0.470374, 0.703966, 0.920726],\r\n       [0.200281, 0.278307, 0.489626, ..., 0.981545, 0.246259, 0.460888],\r\n       [0.683009, 0.90563 , 0.523264, ..., 0.646483, 0.033159, 0.294284],...\r\n```\r\nFull log [here](https://source.cloud.google.com/results/invocations/4ff065c3-c0a3-4e06-b14d-fe09d346623b/targets/%2F%2Ftensorflow%2Fpython%2Fkernel_tests%2Frandom:random_ops_test_gpu/tests).", "Root cause: some op kernel need to skip specific samples to generate independent output while calling Generator, but this optimization just simply call the Generator sequentially without skipping.\r\n\r\nTo fix it I need pass the skipping step into the Distribution and it will change the function API. I'll have internal discussion and update it later, thanks! ", "@Zantares Thank you for the update!", "Made a new commit with below fixes:\r\n* Add template flag`IsVec` in `UniformDistribution` to control optimization scope, only `PhiloxRandomOp` on CPU will be optimized now. This is a flexible API can extend the optimization to other ops.\r\n* Fix the error caused by wrong Skip steps in `FillPhiloxRandomTask`\r\n* Revert the code of other Distribution ops(parameterized_truncated_normal_op) what are not optimized with `IsVec`\r\n\r\nSince the original implementation has impact too many ops, I minimized optimization scope for known bottleneck `RandomUniform` and make it become easy to track with flag `IsVec`. Please have a look whether this flag is acceptable, thanks.", "I made a new fix for the failure which caused by missing consider of `double` and `int64` in last commit... and finally I have set up GPU server to test UT and make sure this one can pass. Please take a look at the new fix, it should be alright now:)  ", "This PR caused some internal breakages and was automatically rolled back in https://github.com/tensorflow/tensorflow/commit/806a053eb5b54575480bd22609a4286ca929941e. I'll work on bringing it back.", "> This PR caused some internal breakages and was automatically rolled back in [806a053](https://github.com/tensorflow/tensorflow/commit/806a053eb5b54575480bd22609a4286ca929941e). I'll work on bringing it back.\r\n\r\nPlease let me know If I can provide any help.\r\nBesides, I think this optimization has changed a lots in order to keep the flexibility. I have another op level implementation which only affects `RandomUniform`. I can bring that one into discussion if current implementation is not accepted.", "Since this optimization was reverted and no more update now, I put an alternative implementation which only optimize the op `RandomUniform`\r\n```patch\r\ndiff --git a/tensorflow/core/kernels/random_op_cpu.h b/tensorflow/core/kernels/random_op_cpu.h\r\nindex 461de98ccac..ae290cfc5a2 100644\r\n--- a/tensorflow/core/kernels/random_op_cpu.h\r\n+++ b/tensorflow/core/kernels/random_op_cpu.h\r\n@@ -54,6 +54,83 @@ namespace functor {\r\n using random::PhiloxRandom;\r\n using random::SingleSampleAdapter;\r\n \r\n+template <class Generator, typename RealType, class Distribution>\r\n+class DistributionVec {\r\n+ public:\r\n+  explicit DistributionVec(Distribution* dist) { this->dist = dist; }\r\n+\r\n+  typename Distribution::ResultType operator()(Generator* gen) {\r\n+    return (*dist)(gen);\r\n+  }\r\n+\r\n+  void VecCopy(RealType* data, int64 length) {}\r\n+\r\n+ private:\r\n+  Distribution* dist;\r\n+};\r\n+\r\n+template <>\r\n+class DistributionVec<\r\n+    random::PhiloxRandom, bfloat16,\r\n+    random::UniformDistribution<random::PhiloxRandom, bfloat16>> {\r\n+ public:\r\n+  typedef random::UniformDistribution<random::PhiloxRandom, bfloat16>\r\n+      Distribution;\r\n+\r\n+  explicit DistributionVec(Distribution* dist) { this->dist = dist; }\r\n+\r\n+  typename Distribution::ResultType operator()(random::PhiloxRandom* gen) {\r\n+    typename random::PhiloxRandom::ResultType sample = (*gen)();\r\n+    typename Distribution::ResultType result;\r\n+\r\n+    for (int i = 0; i < Distribution::kResultElementCount; ++i) {\r\n+      result[i] = tensorflow::random::InternalUint16ToBfloat16(sample[i]);\r\n+    }\r\n+\r\n+    return result;\r\n+  }\r\n+\r\n+  void VecCopy(bfloat16* data, int64 length) {\r\n+    // The mantissa has an implicit leading 1, so the above code creates a value\r\n+    // in [1, 2). The minus will not cause a rounding that makes the result 1.\r\n+    // Instead it will just be close to 1.\r\n+    auto result_t = typename TTypes<bfloat16>::Tensor(data, length);\r\n+    result_t = result_t - bfloat16(1.0);\r\n+  }\r\n+\r\n+ private:\r\n+  Distribution* dist;\r\n+};\r\n+\r\n+template <>\r\n+class DistributionVec<\r\n+    random::PhiloxRandom, float,\r\n+    random::UniformDistribution<random::PhiloxRandom, float>> {\r\n+ public:\r\n+  typedef random::UniformDistribution<random::PhiloxRandom, float> Distribution;\r\n+\r\n+  explicit DistributionVec(Distribution* dist) { this->dist = dist; }\r\n+\r\n+  typename Distribution::ResultType operator()(random::PhiloxRandom* gen) {\r\n+    typename random::PhiloxRandom::ResultType sample = (*gen)();\r\n+    typename Distribution::ResultType result;\r\n+\r\n+    for (int i = 0; i < Distribution::kResultElementCount; ++i) {\r\n+      result[i] = tensorflow::random::InternalUint32ToFloat(sample[i]);\r\n+    }\r\n+\r\n+    return result;\r\n+  }\r\n+\r\n+  void VecCopy(float* data, int64 length) {\r\n+    auto result_t = typename TTypes<float>::Tensor(data, length);\r\n+    result_t = result_t - 1.0f;\r\n+  }\r\n+\r\n+ private:\r\n+  Distribution* dist;\r\n+};\r\n+\r\n // The default implementation of the functor, which should never be invoked\r\n // But we still need to provide implementation for now for the linker to work,\r\n // since we do not support all the distributions yet.\r\n@@ -90,18 +167,23 @@ struct FillPhiloxRandomTask<Distribution, false> {\r\n \r\n     // First fill all the full-size groups\r\n     int64 limit_group_full = std::min(limit_group, size / kGroupSize);\r\n+    DistributionVec<random::PhiloxRandom, T, Distribution> dist_vec(&dist);\r\n     for (int64 index = start_group; index < limit_group_full; ++index) {\r\n-      auto samples = dist(&gen);\r\n+      auto samples = dist_vec(&gen);\r\n       std::copy(&samples[0], &samples[0] + kGroupSize, data + offset);\r\n       offset += kGroupSize;\r\n     }\r\n \r\n     // If there are any remaining elements that need to be filled, process them\r\n+    int64 remaining_size = 0;\r\n     if (limit_group_full < limit_group) {\r\n-      int64 remaining_size = size - limit_group_full * kGroupSize;\r\n-      auto samples = dist(&gen);\r\n+      remaining_size = size - limit_group_full * kGroupSize;\r\n+      auto samples = dist_vec(&gen);\r\n       std::copy(&samples[0], &samples[0] + remaining_size, data + offset);\r\n     }\r\n+    dist_vec.VecCopy(\r\n+        data + start_group * kGroupSize,\r\n+        (limit_group_full - start_group) * kGroupSize + remaining_size);\r\n   }\r\n };\r\n \r\n@@ -125,6 +207,8 @@ struct FillPhiloxRandomTask<Distribution, true> {\r\n     // First fill all the full-size groups\r\n     int64 limit_group_full = std::min(limit_group, size / kGroupSize);\r\n     int64 group_index;\r\n+    DistributionVec<SingleSampleAdapter<PhiloxRandom>, T, Distribution>\r\n+        dist_vec(&dist);\r\n     for (group_index = start_group; group_index < limit_group_full;\r\n          ++group_index) {\r\n       // Reset the generator to the beginning of the output group region\r\n@@ -134,21 +218,25 @@ struct FillPhiloxRandomTask<Distribution, true> {\r\n       gen.Skip(group_index * kGeneratorSkipPerOutputGroup);\r\n       SingleSampleAdapter<PhiloxRandom> single_samples(&gen);\r\n \r\n-      auto samples = dist(&single_samples);\r\n+      auto samples = dist_vec(&single_samples);\r\n       std::copy(&samples[0], &samples[0] + kGroupSize, data + offset);\r\n       offset += kGroupSize;\r\n     }\r\n \r\n     // If there are any remaining elements that need to be filled, process them\r\n+    int64 remaining_size = 0;\r\n     if (limit_group_full < limit_group) {\r\n       PhiloxRandom gen = base_gen;\r\n       gen.Skip(group_index * kGeneratorSkipPerOutputGroup);\r\n       SingleSampleAdapter<PhiloxRandom> single_samples(&gen);\r\n \r\n-      int64 remaining_size = size - limit_group_full * kGroupSize;\r\n-      auto samples = dist(&single_samples);\r\n+      remaining_size = size - limit_group_full * kGroupSize;\r\n+      auto samples = dist_vec(&single_samples);\r\n       std::copy(&samples[0], &samples[0] + remaining_size, data + offset);\r\n     }\r\n+    dist_vec.VecCopy(\r\n+        data + start_group * kGroupSize,\r\n+        (limit_group_full - start_group) * kGroupSize + remaining_size);\r\n   }\r\n };\r\n \r\ndiff --git a/tensorflow/core/lib/random/random_distributions.h b/tensorflow/core/lib/random/random_distributions.h\r\nindex 79ca7247838..a0fdf40457e 100644\r\n--- a/tensorflow/core/lib/random/random_distributions.h\r\n+++ b/tensorflow/core/lib/random/random_distributions.h\r\n@@ -22,9 +22,9 @@ limitations under the License.\r\n #include <cmath>\r\n #include <type_traits>\r\n \r\n-#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n #include \"tensorflow/core/lib/random/philox_random.h\"\r\n #include \"tensorflow/core/platform/types.h\"\r\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n \r\n namespace tensorflow {\r\n namespace random {\r\n@@ -32,9 +32,13 @@ namespace random {\r\n // Helper function to convert a 16-bit integer to a half between [0..1).\r\n PHILOX_DEVICE_INLINE Eigen::half Uint16ToHalf(uint16 x);\r\n // Helper function to convert a 16-bit integer to a bfloat16 between [0..1).\r\n-PHILOX_DEVICE_INLINE bfloat16 Uint16ToGfloat16(uint16 x);\r\n+PHILOX_DEVICE_INLINE bfloat16 Uint16ToBfloat16(uint16 x);\r\n+// Helper function to convert a 16-bit integer to a bfloat16 between [1..2).\r\n+PHILOX_DEVICE_INLINE bfloat16 InternalUint16ToBfloat16(uint16 x);\r\n // Helper function to convert a 32-bit integer to a float between [0..1).\r\n PHILOX_DEVICE_INLINE float Uint32ToFloat(uint32 x);\r\n+// Helper function to convert a 32-bit integer to a float between [1..2).\r\n+PHILOX_DEVICE_INLINE float InternalUint32ToFloat(uint32 x);\r\n // Helper function to convert two 32-bit integers to a double between [0..1).\r\n PHILOX_DEVICE_INLINE double Uint64ToDouble(uint32 x0, uint32 x1);\r\n \r\n@@ -107,9 +111,11 @@ class UniformDistribution<Generator, bfloat16> {\r\n   ResultType operator()(Generator* gen) {\r\n     typename Generator::ResultType sample = (*gen)();\r\n     ResultType result;\r\n+\r\n     for (int i = 0; i < kResultElementCount; ++i) {\r\n-      result[i] = Uint16ToGfloat16(sample[i]);\r\n+      result[i] = Uint16ToBfloat16(sample[i]);\r\n     }\r\n+\r\n     return result;\r\n   }\r\n };\r\n@@ -763,9 +769,9 @@ PHILOX_DEVICE_INLINE Eigen::half Uint16ToHalf(uint16 x) {\r\n   return result - Eigen::half(1.0);\r\n }\r\n \r\n-// Helper function to convert an 16-bit integer to a bfloat16 between [0..1).\r\n-// This can create a uniform distribution of values between [0..1).\r\n-PHILOX_DEVICE_INLINE bfloat16 Uint16ToGfloat16(uint16 x) {\r\n+// Helper function to convert an 16-bit integer to a bfloat16 between [1..2).\r\n+// This can create a uniform distribution of values between [1..2).\r\n+PHILOX_DEVICE_INLINE bfloat16 InternalUint16ToBfloat16(uint16 x) {\r\n   // bfloat are formatted as follows (MSB first):\r\n   //    sign(1) exponent(8) mantissa(7)\r\n   // Conceptually construct the following:\r\n@@ -779,13 +785,20 @@ PHILOX_DEVICE_INLINE bfloat16 Uint16ToGfloat16(uint16 x) {\r\n   bfloat16 result;\r\n   memcpy(&result, &val, sizeof(val));\r\n   // The mantissa has an implicit leading 1, so the above code creates a value\r\n-  // in [1, 2). The minus will not cause a rounding that makes the result 1.\r\n+  // in [1, 2).\r\n+  return result;\r\n+}\r\n+\r\n+// Helper function to convert an 16-bit integer to a bfloat16 between [0..1).\r\n+// This can create a uniform distribution of values between [0..1).\r\n+PHILOX_DEVICE_INLINE bfloat16 Uint16ToBfloat16(uint16 x) {\r\n+  // The minus will not cause a rounding that makes the result 1.\r\n   // Instead it will just be close to 1.\r\n-  return result - bfloat16(1.0);\r\n+  return InternalUint16ToBfloat16(x) - bfloat16(1.0);\r\n }\r\n \r\n-// Helper function to convert an 32-bit integer to a float between [0..1).\r\n-PHILOX_DEVICE_INLINE float Uint32ToFloat(uint32 x) {\r\n+// Helper function to convert an 32-bit integer to a float between [1..2).\r\n+PHILOX_DEVICE_INLINE float InternalUint32ToFloat(uint32 x) {\r\n   // IEEE754 floats are formatted as follows (MSB first):\r\n   //    sign(1) exponent(8) mantissa(23)\r\n   // Conceptually construct the following:\r\n@@ -799,7 +812,12 @@ PHILOX_DEVICE_INLINE float Uint32ToFloat(uint32 x) {\r\n   // Assumes that endian-ness is same for float and uint32.\r\n   float result;\r\n   memcpy(&result, &val, sizeof(val));\r\n-  return result - 1.0f;\r\n+  return result;\r\n+}\r\n+\r\n+// Helper function to convert an 32-bit integer to a float between [0..1).\r\n+PHILOX_DEVICE_INLINE float Uint32ToFloat(uint32 x) {\r\n+  return InternalUint32ToFloat(x) - 1.0f;\r\n }\r\n \r\n // Helper function to convert two 32-bit integers to a double between [0..1).\r\n\r\n```\r\n"]}, {"number": 39746, "title": "Support \"depth_to_pace\" for tflite GPU delegate", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): a8001b9e8db92620603c3c0588d251192d327bae\r\n\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nCould you please add \"depth_to_pace\" support for tflite GPU delegate ?\r\n\r\n\r\n**Any other info / logs**\r\n\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Following operations are not supported by GPU delegate:\r\nDEPTH_TO_SPACE: Operation is not supported.\r\n", "comments": ["@jdduke \r\n\r\nAcknowledged, but I don't think the TFLite GPU team can cycles to take on individual requests on missing ops.  There should be a comprehensive and prioritized list of requested ops."]}, {"number": 39745, "title": "Handling Unbalanced dataset : ImageAugmentation using tf.keras.preprocessing.image.ImageDataGenerator and tf.datasets: model.fit()", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/keras/train_and_evaluate#using_sample_weighting_and_class_weighting\r\n\r\nhttps://www.tensorflow.org/tutorials/images/classification#data_augmentation\r\n\r\n## Description of the issue (what needs changing):\r\nAccording to the TensorFlow documentation we are not supposed to use class_weights parameter when training the models (model.fit()) using tf.datasets. So the suggested way is to use sample_weights. I was able to use sample_weights and create a dataset using sample_weights and passed it to the model.fit() method without using ImageDatGenerators.\r\n\r\na) But how can we do the same while using ImageDataGenerator class and flow_from_dataframe() data generator and using tf.datasets? \r\n\r\nb) Is there any option to pass sample_weights in tf.keras.preprocessing.image.ImageDataGenerator class's flow()/flow_from_dataframe() methods?\r\n\r\nc) The second URL which I have shared above also does not mention whether the ImageDataGenerator class has a way of balancing the unbalanced datasets.\r\n\r\n### Usage example\r\nCan you provide a usage example?\r\n", "comments": ["Any updates on this?", "Any updates on this?", "> According to the TensorFlow documentation we are not supposed to use class_weights parameter when training the models (model.fit()) using `tf.data`.\r\n\r\nThat doesn't sound right. I think it is supported, but that paragraph in `train_and_evaluate` is unclear/incorrect. Experimenting with `class_weights` and `tf.data` they seem to be working together.\r\n\r\nI'm sending a fix to that doc.\r\n\r\n>  ImageDataGenerator\r\n\r\nI usually just think of this as a simple data loader, and If i need to make modifications to the output  wrap it using `tf.data.Dataset.fromn_generator`, and then `.map` whatever I want.\r\n", "That's funny. I wonder why the commit in @lamberta's fork closed it instead of the one in tensorflow-docs.", "@MarkDaoust Looks like you marked this PR as fixed in cl/314982746", "(But yeah, weird that my fork was able to \"fix\" this. It probably doesn't care about org structure, maybe just looks at user privileges?) ", "Thankyou for the updates."]}, {"number": 39744, "title": "tensorflow for go install failed", "body": "MacOS version(10.15.4)\r\n\r\n1. I followed the instruction of  https://www.tensorflow.org/install/lang_c  ,I installed the c Library .but it doesn't word.the error is > hello_tf.c:2:10: fatal error: 'tensorflow/c/c_api.h' file not found\r\n\r\n2. github.com/tensorflow/tensorflow/tensorflow/go/saved_model.go:25:2: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any of:\r\n/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOROOT)\r\n/Users/moses/workspace/godemo/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOPATH)\r\n\r\nhow to solve this problem\r\n", "comments": ["I have also encountered this error on Linux:\r\n\r\n```bash\r\n~$ /tmp/tensor/hello_tf \r\nHello from TensorFlow C library version 1.15.0\r\n~$ go get github.com/tensorflow/tensorflow/tensorflow/go\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any of:\r\n\t/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOROOT)\r\n\t/home/dwhite/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOPATH)\r\n~$ uname -a\r\nLinux lab 5.2.0-3-amd64 #1 SMP Debian 5.2.17-1 (2019-09-26) x86_64 GNU/Linux\r\n~$ echo \"$GOROOT $GOPATH\"\r\n\r\n~$ go version\r\ngo version go1.12.2 linux/amd64\r\n```", "\r\n\r\n> I have also encountered this error on Linux:\r\n> \r\n> ```shell\r\n> ~$ /tmp/tensor/hello_tf \r\n> Hello from TensorFlow C library version 1.15.0\r\n> ~$ go get github.com/tensorflow/tensorflow/tensorflow/go\r\n> package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any of:\r\n> \t/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOROOT)\r\n> \t/home/dwhite/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOPATH)\r\n> ~$ uname -a\r\n> Linux lab 5.2.0-3-amd64 #1 SMP Debian 5.2.17-1 (2019-09-26) x86_64 GNU/Linux\r\n> ~$ echo \"$GOROOT $GOPATH\"\r\n> \r\n> ~$ go version\r\n> go version go1.12.2 linux/amd64\r\n> ```\r\n\r\nI got the same problem last night.\r\nI execute this and I fixxed it.\r\n`cd $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/go`\r\n`git checkout r1.11`\r\n", "> > I have also encountered this error on Linux:\r\n> > ```shell\r\n> > ~$ /tmp/tensor/hello_tf \r\n> > Hello from TensorFlow C library version 1.15.0\r\n> > ~$ go get github.com/tensorflow/tensorflow/tensorflow/go\r\n> > package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any of:\r\n> > \t/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOROOT)\r\n> > \t/home/dwhite/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOPATH)\r\n> > ~$ uname -a\r\n> > Linux lab 5.2.0-3-amd64 #1 SMP Debian 5.2.17-1 (2019-09-26) x86_64 GNU/Linux\r\n> > ~$ echo \"$GOROOT $GOPATH\"\r\n> > \r\n> > ~$ go version\r\n> > go version go1.12.2 linux/amd64\r\n> > ```\r\n> \r\n> I got the same problem last night.\r\n> I execute this and I fixxed it.\r\n> `cd $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/go`\r\n> `git checkout r1.11`\r\n\r\nSame problem, this \"fixed\" the problem at least temporarily.", "Worked for me also", "Worked for me too", "> I got the same problem last night.\r\n> I execute this and I fixxed it.\r\n> `cd $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/go`\r\n> `git checkout r1.11`\r\n\r\n`r2.0` seems to be the last version w/o bug.", "Fixed with https://github.com/tensorflow/tensorflow/commit/e5e495db7bee77cd0fd5dda3b06bd743cbcf1ef8", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39744\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39744\">No</a>\n", "seems still exist \r\n```\r\n~/go/src/github.com/tensorflow/tensorflow/tensorflow $ git log HEAD --oneline\r\n0de929fdc1 (HEAD -> master, origin/master, origin/HEAD) Make StatusHelper safer to use and fix a related memory leak.\r\n~/go/src/github.com/tensorflow/tensorflow/tensorflow $ go test github.com/tensorflow/tensorflow/tensorflow/go\r\ngo/saved_model.go:25:2: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any of:\r\n        /usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOROOT)\r\n        /home/pi/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOPATH)\r\n~/go/src/github.com/tensorflow/tensorflow/tensorflow $ go version\r\ngo version go1.12.17 linux/arm\r\n```", "I am also getting this issue when running `go get github.com/tensorflow/tensorflow/tensorflow/go`:\r\n```\r\nsaved_model.go:25:2: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any of:\r\n\t/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOROOT)\r\n\t/Users/juan/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOPATH)\r\n```\r\nI am using the following version of Go:\r\n```\r\ngo version go1.14.4 darwin/amd64\r\n```\r\n\r\nI get this with the latest version of Tensorflow, commit `01b030b77623c5fa00a43640f77af2a43572d02c`.\r\n\r\nAny help would be appreciated!", "This works to install the latest working version:\r\n```\r\ngo get github.com/tensorflow/tensorflow/tensorflow/go@v1.15.0\r\n```", "I get this same error. Any resolution?", "Same here:\r\n```\r\n$ go get github.com/tensorflow/tensorflow/tensorflow/go\r\ncannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any of:\r\n\t/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOROOT)\r\n\t/Users/gl/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOPATH)\r\n```", "Hey @jhseu , any fix ? I am facing same issue. r1.11 or r2.0 doesn't work either.\r\n```\r\nshankernaik$ go get github.com/tensorflow/tensorflow/tensorflow/go\r\npackage github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any of:\r\n\t/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOROOT)\r\n\t/Users/shankernaik/code/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOPATH)\r\n```\r\n\r\n\r\n```\r\nshankernaik$ uname -a\r\nDarwin  19.6.0 Darwin Kernel Version 19.6.0: Tue Jan 12 22:13:05 PST 2021; root:xnu-6153.141.16~1/RELEASE_X86_64 x86_64\r\nshankernaik$ echo \"$GOROOT $GOPATH\"\r\n/usr/local/go /Users/shankernaik/code/go\r\n```\r\n\r\nUpdate :\r\nwhile configuring the env variables for linker from https://www.tensorflow.org/install/lang_c. instead try Absolute path as below\r\n```\r\nexport LIBRARY_PATH=/mydir/lib\r\nexport DYLD_LIBRARY_PATH=/mydir/lib\r\n```\r\n\r\nThis worked for me, caching the go package here.\r\n```\r\nshankernaik$ go test github.com/tensorflow/tensorflow/tensorflow/go\r\n# github.com/tensorflow/tensorflow/tensorflow/go.test\r\nld: warning: directory not found for option '-L/Users/shankernaik/Users/shankernaik/libtensorflow-cpu-darwin-x86_64-2.4.0'\r\nok  \tgithub.com/tensorflow/tensorflow/tensorflow/go\t(cached)\r\n```"]}, {"number": 39743, "title": "fix GetOrCreateExecutors executors_.emplace a lot of keys to executors_", "body": "The function `GetOrCreateExecutors` emplaces a lot of keys to `executors_`, which leads to memory usage a lot in TF serving.\r\n\r\n```\r\n  // See if we already have the executors for this run.\r\n  {\r\n    mutex_lock l(executor_lock_);\r\n    auto it = executors_.find(sorted_key);\r\n    if (it != executors_.end()) {\r\n      *executors_and_keys = it->second.get();\r\n\r\n      // Insert this under the original key.  \r\n      executors_.emplace(key, it->second);  // need delete\r\n      return Status::OK();\r\n    }\r\n  }\r\n```\r\n\r\nIf number of input features is big, then `inputs` parameter `GetOrCreateExecutors` could be many kinds order.\r\n\r\nplease check attached file\r\n\r\n[serving_nommap.0042.heap.base0007.pdf](https://github.com/tensorflow/tensorflow/files/4661274/serving_nommap.0042.heap.base0007.pdf)\r\n", "comments": ["please also check https://github.com/tensorflow/serving/issues/1215", "I'm not sure fix TF code or TF serving code would be better, so I submitted another PR in TF serving \r\nhttps://github.com/tensorflow/serving/pull/1638\r\n\r\nplease help check.", "I think it is better to fix in tf serving, please refer to another PR in TF serving \r\nhttps://github.com/tensorflow/serving/pull/1638\r\nSo I close this PR."]}, {"number": 39742, "title": "Warning while running teaching sample", "body": "While running folloing sipmle code\r\n```python\r\nfrom tensorflow import data\r\ndataset = data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\r\nfor elem in dataset:\r\n    print(elem.numpy())\r\nit = iter(dataset)\r\nprint(next(it).numpy())\r\nprint(dataset.reduce(0, lambda state, value: state + value).numpy())\r\n```\r\nI have this warning\r\n```python\r\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x000001B252F02AF0> and \\\r\nwill run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on \\\r\nLinux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Unable to locate the source code of <function <lambda> at 0x000001B252F02AF0>. Note that\\\r\nfunctions defined in certain environments, like the interactive Python shell do not expose\\\r\ntheir source code. If that is the case, you should to define them in a .py source file. If you \\\r\nare certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert.\\\r\nOriginal error: could not get source code\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n```\r\nI knaw about https://github.com/tensorflow/tensorflow/issues/38691 issue, but warning repited on new TensorFlow  version\r\n**System information**\r\nWindows 10 64bit (no GPU machine)\r\nPython 3.8.2\r\nTensorFlow 2.2.0 (standard GPU version)\r\nrunning on venv environment, in PyCharm Python console", "comments": ["@pilot87,\r\nI was able to run the code without any warnings on TF v2.2. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/5b4cc64ca520b52cde18d862434d8c05/39742.ipynb).\r\n\r\nCould you please try changing the log level using the below code and let us know if it works.\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\nimport tensorflow as tf\r\n```\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Sorry for so long offline.\r\nWell this code\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\nimport tensorflow as tf\r\nfrom tensorflow import data\r\ndataset = data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\r\nfor elem in dataset:\r\n...     print(elem.numpy())\r\nit = iter(dataset)\r\nprint(next(it).numpy())\r\nprint(dataset.reduce(0, lambda state, value: state + value).numpy())\r\n```\r\nstill producing this warnings (I omitted standard output because result is right):\r\n```\r\n2020-05-30 07:18:55.357802: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call\r\nto cuInit: UNKNOWN ERROR (303)\r\n\r\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x000002ABA9897670> and\r\n will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on\r\n Linux, export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Unable to locate the source code of <function <lambda> at 0x000002ABA9897670>. Note that\r\nfunctions defined in certain environments, like the interactive Python shell do not expose their\r\nsource code. If that is the case, you should to define them in a .py source file. If you are\r\n certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert.\r\nOriginal error: could not get source code\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert```", "The warning is due to a known limitation of the Python runtime. See [this article](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#source-code-of-lambda-functions).\r\n\r\nThe known workaround is to write the code like this:\r\n\r\n```\r\nsum_up = lambda state, value: state + value\r\nprint(dataset.reduce(0, sum_up).numpy())\r\n```"]}, {"number": 39741, "title": "Cd Tiger not opening", "body": "\r\n![Screenshot_2020-05-21-11-34-45-27](https://user-images.githubusercontent.com/65701344/82530632-49ab0600-9b2d-11ea-8892-3e0cee5f445b.png)\r\n<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Anees11 \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nIs this issue related to Tensorflow build/install?\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "Not relevant for this repository", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39741\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39741\">No</a>\n"]}, {"number": 39740, "title": "cd Tiger build installation isssue when I'm using command cd Tiger it's retrun such a no file directory ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Anees11,\r\nPlease fill in the issue template and also provide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "Regarding the error you are facing, use `ls` command to list all the directories in that folder and then try to `cd` into one of the directory. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39740\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39740\">No</a>\n"]}]