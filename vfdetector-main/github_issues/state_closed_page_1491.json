[{"number": 8209, "title": "Initial support for cudnn v6?", "body": "we can download cudnn v6 rc from here\r\nhttp://blog.yannisassael.com/2017/02/cudnn-v6-0-rc/", "comments": ["@zheng-xq I'm assuming this will happen at some point? ;-)", "Yes, they will most definitely happen at some point. :)", "@zheng-xq I'll leave this as assigned to you so that it doesn't stay in the triage list.  ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "We are switching form cudnn6 to cudnn7. Closing this one. "]}, {"number": 8208, "title": "Feature Request: armv7k support for WatchOS", "body": "There is no build target for the armv7k architecture in the compile_ios_tensorflow.sh makefile. This means that tensorflow currently does not work on the Apple Watch. Having support for one of the most popular wearables would be a big boon to what developers could do with machine learning.\r\n\r\nI asked a question about this a month ago on Stack Overflow but only got crickets, so I'm asking here as a feature request.\r\n\r\nRelated Stack Overflow question:\r\nhttps://stackoverflow.com/questions/41990420/tensorflow-on-watchos", "comments": ["@petewarden  Could you please comment?", "We don't have WatchOS devices on the team to test with, or experience with them, so it's not been something I've been able to easily add. I'd love to see this support added though, and from what I've looked at it should mostly be a case of different sdk paths and some clang and linker flags.", "My team and I understand. We would like to help facilitate this and move this forward. What specific things can we do to help, including testing on device and working through the compiler/linker issues.", "I would take a look at the Makefile where we deal with iOS builds:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/Makefile#L288\r\n\r\nYou'll see there are already two platforms, 'real' devices and the simulator. I expect you'll need to add a third option for WatchOS, something like:\r\n\r\n```\r\n\tWATCHOS_PLATFORM := $(shell xcrun --sdk watchos --show-sdk-platform-path)\r\n\tWATCHOS_SYSROOT := $(shell xcrun --sdk watchos --show-sdk-path)\r\n```\r\nThen I'm not sure if you'll need a new architecture or not, but you'll have to add some section or conditional where you set compiler and linker flags like `-mwatchos-version-min` I think, like we do for `-miphoneos-version-min`.", "I made the changes to the makefile that you suggested, I got the compilation to at least start, but I've run into a some issues that I'm not sure how to deal with. First I'm getting clang warnings about using sysroot for 'iPhoneOS but targeting Watch'  I'm not sure if these warnings are fatal though. The second issue is that I'm getting errors that fork and execv are not available in watchOS.\r\n\r\nI have included the compilation warnings and errors below:\r\n\r\n```shell\r\nclang: warning: using sysroot for 'iPhoneOS' but targeting 'Watch'\r\ngcc --std=c++11 -DIS_SLIM_BUILD -fno-exceptions -DNDEBUG -O3 -mwatchos-version-min=1.0 -arch armv7k -fembed-bitcode -D__thread= -DUSE_GEMM_FOR_CONV -Wno-c++11-narrowing -mno-thumb -DTF_LEAN_BINARY -D__ANDROID_TYPES_SLIM__ -fno-exceptions -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS10.2.sdk -MT /Users/brianhayward/Documents/projects/tensorflow-master/tensorflow/contrib/makefile/gen/obj/ios_ARMV7K/tensorflow/core/util/bcast.o -MMD -MP -MF /Users/brianhayward/Documents/projects/tensorflow-master/tensorflow/contrib/makefile/gen/dep/ios_ARMV7K//tensorflow/core/util/bcast.Td -I. -I/Users/brianhayward/Documents/projects/tensorflow-master/tensorflow/contrib/makefile/downloads/ -I/Users/brianhayward/Documents/projects/tensorflow-master/tensorflow/contrib/makefile/downloads/eigen -I/Users/brianhayward/Documents/projects/tensorflow-master/tensorflow/contrib/makefile/downloads/gemmlowp -I/Users/brianhayward/Documents/projects/tensorflow-master/tensorflow/contrib/makefile/gen/proto/ -I/Users/brianhayward/Documents/projects/tensorflow-master/tensorflow/contrib/makefile/gen/proto_text/ -I/Users/brianhayward/Documents/projects/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf-host/include -I/usr/local/include -c tensorflow/core/util/bcast.cc -o /Users/brianhayward/Documents/projects/tensorflow-master/tensorflow/contrib/makefile/gen/obj/ios_ARMV7K/tensorflow/core/util/bcast.o\r\nclang: warning: using sysroot for 'iPhoneOS' but targeting 'Watch'\r\ntensorflow/core/platform/posix/subprocess.cc:204:10: error: call to unavailable function 'fork': not available on watchOS\r\n  pid_ = fork();\r\n         ^~~~\r\n/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS10.2.sdk/usr/include/unistd.h:446:8: note: candidate function has been explicitly made unavailable\r\npid_t    fork(void) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;\r\n         ^\r\ntensorflow/core/platform/posix/subprocess.cc:277:3: error: call to unavailable function 'execv': not available on watchOS\r\n  execv(exec_path_, exec_argv_);\r\n  ^~~~~\r\n/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS10.2.sdk/usr/include/unistd.h:443:6: note: candidate function has been explicitly made unavailable\r\nint      execv(const char * __path, char * const * __argv) __WATCHOS_PROHIBITED __TVOS_PROHIBITED;\r\n         ^\r\n2 errors generated.\r\nmake: *** [/Users/brianhayward/Documents/projects/tensorflow-master/tensorflow/contrib/makefile/gen/obj/ios_ARMV7K/tensorflow/core/platform/posix/subprocess.o] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\n+ '[' 2 -ne 0 ']'\r\n+ echo 'armv7k compilation failed.'\r\n\r\narmv7k compilation failed.\r\n+ exit 1\r\n```", "I'd need to see your full changes, but from looking at the command line and error I think you're using the wrong SDK. The -isysroot setting is pointing to the iPhone SDK, and this is set from the IPHONEOS_SYSROOT variable, like this:\r\n\r\n`IPHONEOS_SYSROOT := $(shell xcrun --sdk iphoneos --show-sdk-path)`\r\n\r\nI'm guessing you'll need to add a new version that looks like this:\r\n\r\n`WATCHOS_SYSROOT := $(shell xcrun --sdk watchos --show-sdk-path)`\r\n\r\nAnd then use that in the command you invoke.", "You were right, I did have the wrong SDK targeted in the makefile. I found where I missed that and re-ran the compilation. I'm no longer getting the sysroot warnings I was getting before, but I'm still getting the errors about fork and execv being unavailable.", "I would try putting #if !defined(<macro for detecting watchos>) around the lines in subprocess.cc that call these functions, and put LOG(FATAL) << \"fork() not available on this platform\"; error messages in the #else block.\r\n\r\nI don't believe these should be needed, so it seems reasonable to stub them out.", "I stubbed out the calls to fork() and execv() for the watch compilation and that worked for those errors. Now the problem is that WatchOS doesn't have the accelerate framework as part of its sdk.  I'm now getting a failure from files that are trying to include Accelerate.h. I'm guessing that any computations that require the functions from the accelerate framework would have to be re-written for WatchOS.\r\n\r\nThese are the error I'm getting now:\r\n```shell\r\ngcc --std=c++11 -DIS_SLIM_BUILD -fno-exceptions -DNDEBUG -O3 -mwatchos-version-min=1.0 -arch armv7k -fembed-bitcode -D__thread= -DUSE_GEMM_FOR_CONV -Wno-c++11-narrowing -mno-thumb -DTF_LEAN_BINARY -D__ANDROID_TYPES_SLIM__ -fno-exceptions -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/WatchOS.platform/Developer/SDKs/WatchOS3.1.sdk -MT /Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/obj/ios_ARMV7K/tensorflow/core/kernels/concat_lib_cpu.o -MMD -MP -MF /Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/dep/ios_ARMV7K//tensorflow/core/kernels/concat_lib_cpu.Td -I. -I/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/downloads/ -I/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/downloads/eigen -I/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/downloads/gemmlowp -I/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/proto/ -I/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/proto_text/ -I/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf-host/include -I/usr/local/include -c tensorflow/core/kernels/concat_lib_cpu.cc -o /Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/obj/ios_ARMV7K/tensorflow/core/kernels/concat_lib_cpu.o\r\nIn file included from tensorflow/core/kernels/conv_ops_using_gemm.cc:62:\r\n./tensorflow/core/kernels/gemm_functors.h:37:10: fatal error: 'Accelerate/Accelerate.h' file not found\r\n#include <Accelerate/Accelerate.h>\r\n         ^\r\n1 error generated.\r\ngcc --std=c++11 -DIS_SLIM_BUILD -fno-exceptions -DNDEBUG -O3 -mwatchos-version-min=1.0 -arch armv7k -fembed-bitcode -D__thread= -DUSE_GEMM_FOR_CONV -Wno-c++11-narrowing -mno-thumb -DTF_LEAN_BINARY -D__ANDROID_TYPES_SLIM__ -fno-exceptions -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/WatchOS.platform/Developer/SDKs/WatchOS3.1.sdk -MT /Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/obj/ios_ARMV7K/tensorflow/core/kernels/check_numerics_op.o -MMD -MP -MF /Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/dep/ios_ARMV7K//tensorflow/core/kernels/check_numerics_op.Td -I. -I/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/downloads/ -I/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/downloads/eigen -I/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/downloads/gemmlowp -I/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/proto/ -I/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/proto_text/ -I/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf-host/include -I/usr/local/include -c tensorflow/core/kernels/check_numerics_op.cc -o /Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/obj/ios_ARMV7K/tensorflow/core/kernels/check_numerics_op.o\r\nmake: *** [/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/obj/ios_ARMV7K/tensorflow/core/kernels/conv_ops_using_gemm.o] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\nIn file included from tensorflow/core/kernels/conv_ops_fused.cc:34:\r\n./tensorflow/core/kernels/gemm_functors.h:37:10: fatal error: 'Accelerate/Accelerate.h' file not found\r\n#include <Accelerate/Accelerate.h>\r\n         ^\r\n1 error generated.\r\nmake: *** [/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/obj/ios_ARMV7K/tensorflow/core/kernels/conv_ops_fused.o] Error 1\r\n+ '[' 2 -ne 0 ']'\r\n+ echo 'armv7k compilation failed.'\r\narmv7k compilation failed.\r\n+ exit 1\r\n```", "Try removing the USE_GEMM_FOR_CONV define in the build, since that will switch to non-Accelerate versions of the code.", "I got past the Accelerate hurdles and setup a slice in compile_ios_protobuf.sh, but now I'm getting an error stating that the C compiler cannot create executables. It seems to be occurring during protobuff setup.\r\n\r\nThis is the output I get from the console and I have attached the config.log from the error as well. I changed it to a .txt because GitHub wouldn't let me upload a .log\r\n[config.txt](https://github.com/tensorflow/tensorflow/files/855827/config.txt)\r\n\r\n```shell\r\nMaking distclean in conformance\r\n rm -f conformance-test-runner conformance-cpp conformance-objc\r\ntest -z \"conformance.pb.cc conformance.pb.h protoc_middleman javac_middleman conformance-java javac_middleman_lite conformance-java-lite conformance-csharp conformance-php conformance-php-c conformance_pb2.py Conformance.pbobjc.h Conformance.pbobjc.m conformance_pb.rb com/google/protobuf/Any.java com/google/protobuf/AnyOrBuilder.java com/google/protobuf/AnyProto.java com/google/protobuf/BoolValue.java com/google/protobuf/BoolValueOrBuilder.java com/google/protobuf/BytesValue.java com/google/protobuf/BytesValueOrBuilder.java com/google/protobuf/conformance/Conformance.java com/google/protobuf/DoubleValue.java com/google/protobuf/DoubleValueOrBuilder.java com/google/protobuf/Duration.java com/google/protobuf/DurationOrBuilder.java com/google/protobuf/DurationProto.java com/google/protobuf/FieldMask.java com/google/protobuf/FieldMaskOrBuilder.java com/google/protobuf/FieldMaskProto.java com/google/protobuf/FloatValue.java com/google/protobuf/FloatValueOrBuilder.java com/google/protobuf/Int32Value.java com/google/protobuf/Int32ValueOrBuilder.java com/google/protobuf/Int64Value.java com/google/protobuf/Int64ValueOrBuilder.java com/google/protobuf/ListValue.java com/google/protobuf/ListValueOrBuilder.java com/google/protobuf/NullValue.java com/google/protobuf/StringValue.java com/google/protobuf/StringValueOrBuilder.java com/google/protobuf/Struct.java com/google/protobuf/StructOrBuilder.java com/google/protobuf/StructProto.java com/google/protobuf/Timestamp.java com/google/protobuf/TimestampOrBuilder.java com/google/protobuf/TimestampProto.java com/google/protobuf/UInt32Value.java com/google/protobuf/UInt32ValueOrBuilder.java com/google/protobuf/UInt64Value.java com/google/protobuf/UInt64ValueOrBuilder.java com/google/protobuf/Value.java com/google/protobuf/ValueOrBuilder.java com/google/protobuf/WrappersProto.java com/google/protobuf_test_messages/proto3/TestMessagesProto3.java google/protobuf/any.pb.cc google/protobuf/any.pb.h google/protobuf/any.rb google/protobuf/any_pb2.py google/protobuf/duration.pb.cc google/protobuf/duration.pb.h google/protobuf/duration.rb google/protobuf/duration_pb2.py google/protobuf/field_mask.pb.cc google/protobuf/field_mask.pb.h google/protobuf/field_mask.rb google/protobuf/field_mask_pb2.py google/protobuf/struct.pb.cc google/protobuf/struct.pb.h google/protobuf/struct.rb google/protobuf/struct_pb2.py google/protobuf/TestMessagesProto3.pbobjc.h google/protobuf/TestMessagesProto3.pbobjc.m google/protobuf/test_messages_proto3.pb.cc google/protobuf/test_messages_proto3.pb.h google/protobuf/test_messages_proto3_pb.rb google/protobuf/test_messages_proto3_pb2.py google/protobuf/timestamp.pb.cc google/protobuf/timestamp.pb.h google/protobuf/timestamp.rb google/protobuf/timestamp_pb2.py google/protobuf/wrappers.pb.cc google/protobuf/wrappers.pb.h google/protobuf/wrappers.rb google/protobuf/wrappers_pb2.py Conformance/ConformanceRequest.php Conformance/ConformanceResponse.php Conformance/WireFormat.php GPBMetadata/Conformance.php GPBMetadata/Google/Protobuf/Any.php GPBMetadata/Google/Protobuf/Duration.php GPBMetadata/Google/Protobuf/FieldMask.php GPBMetadata/Google/Protobuf/Struct.php GPBMetadata/Google/Protobuf/TestMessagesProto3.php GPBMetadata/Google/Protobuf/Timestamp.php GPBMetadata/Google/Protobuf/Wrappers.php Google/Protobuf/Any.php Google/Protobuf/BoolValue.php Google/Protobuf/BytesValue.php Google/Protobuf/DoubleValue.php Google/Protobuf/Duration.php Google/Protobuf/FieldMask.php Google/Protobuf/FloatValue.php Google/Protobuf/Int32Value.php Google/Protobuf/Int64Value.php Google/Protobuf/ListValue.php Google/Protobuf/NullValue.php Google/Protobuf/StringValue.php Google/Protobuf/Struct.php Google/Protobuf/Timestamp.php Google/Protobuf/UInt32Value.php Google/Protobuf/UInt64Value.php Google/Protobuf/Value.php Protobuf_test_messages/Proto3/ForeignEnum.php Protobuf_test_messages/Proto3/ForeignMessage.php Protobuf_test_messages/Proto3/TestAllTypes_NestedEnum.php Protobuf_test_messages/Proto3/TestAllTypes_NestedMessage.php Protobuf_test_messages/Proto3/TestAllTypes.php\" || rm -f conformance.pb.cc conformance.pb.h protoc_middleman javac_middleman conformance-java javac_middleman_lite conformance-java-lite conformance-csharp conformance-php conformance-php-c conformance_pb2.py Conformance.pbobjc.h Conformance.pbobjc.m conformance_pb.rb com/google/protobuf/Any.java com/google/protobuf/AnyOrBuilder.java com/google/protobuf/AnyProto.java com/google/protobuf/BoolValue.java com/google/protobuf/BoolValueOrBuilder.java com/google/protobuf/BytesValue.java com/google/protobuf/BytesValueOrBuilder.java com/google/protobuf/conformance/Conformance.java com/google/protobuf/DoubleValue.java com/google/protobuf/DoubleValueOrBuilder.java com/google/protobuf/Duration.java com/google/protobuf/DurationOrBuilder.java com/google/protobuf/DurationProto.java com/google/protobuf/FieldMask.java com/google/protobuf/FieldMaskOrBuilder.java com/google/protobuf/FieldMaskProto.java com/google/protobuf/FloatValue.java com/google/protobuf/FloatValueOrBuilder.java com/google/protobuf/Int32Value.java com/google/protobuf/Int32ValueOrBuilder.java com/google/protobuf/Int64Value.java com/google/protobuf/Int64ValueOrBuilder.java com/google/protobuf/ListValue.java com/google/protobuf/ListValueOrBuilder.java com/google/protobuf/NullValue.java com/google/protobuf/StringValue.java com/google/protobuf/StringValueOrBuilder.java com/google/protobuf/Struct.java com/google/protobuf/StructOrBuilder.java com/google/protobuf/StructProto.java com/google/protobuf/Timestamp.java com/google/protobuf/TimestampOrBuilder.java com/google/protobuf/TimestampProto.java com/google/protobuf/UInt32Value.java com/google/protobuf/UInt32ValueOrBuilder.java com/google/protobuf/UInt64Value.java com/google/protobuf/UInt64ValueOrBuilder.java com/google/protobuf/Value.java com/google/protobuf/ValueOrBuilder.java com/google/protobuf/WrappersProto.java com/google/protobuf_test_messages/proto3/TestMessagesProto3.java google/protobuf/any.pb.cc google/protobuf/any.pb.h google/protobuf/any.rb google/protobuf/any_pb2.py google/protobuf/duration.pb.cc google/protobuf/duration.pb.h google/protobuf/duration.rb google/protobuf/duration_pb2.py google/protobuf/field_mask.pb.cc google/protobuf/field_mask.pb.h google/protobuf/field_mask.rb google/protobuf/field_mask_pb2.py google/protobuf/struct.pb.cc google/protobuf/struct.pb.h google/protobuf/struct.rb google/protobuf/struct_pb2.py google/protobuf/TestMessagesProto3.pbobjc.h google/protobuf/TestMessagesProto3.pbobjc.m google/protobuf/test_messages_proto3.pb.cc google/protobuf/test_messages_proto3.pb.h google/protobuf/test_messages_proto3_pb.rb google/protobuf/test_messages_proto3_pb2.py google/protobuf/timestamp.pb.cc google/protobuf/timestamp.pb.h google/protobuf/timestamp.rb google/protobuf/timestamp_pb2.py google/protobuf/wrappers.pb.cc google/protobuf/wrappers.pb.h google/protobuf/wrappers.rb google/protobuf/wrappers_pb2.py Conformance/ConformanceRequest.php Conformance/ConformanceResponse.php Conformance/WireFormat.php GPBMetadata/Conformance.php GPBMetadata/Google/Protobuf/Any.php GPBMetadata/Google/Protobuf/Duration.php GPBMetadata/Google/Protobuf/FieldMask.php GPBMetadata/Google/Protobuf/Struct.php GPBMetadata/Google/Protobuf/TestMessagesProto3.php GPBMetadata/Google/Protobuf/Timestamp.php GPBMetadata/Google/Protobuf/Wrappers.php Google/Protobuf/Any.php Google/Protobuf/BoolValue.php Google/Protobuf/BytesValue.php Google/Protobuf/DoubleValue.php Google/Protobuf/Duration.php Google/Protobuf/FieldMask.php Google/Protobuf/FloatValue.php Google/Protobuf/Int32Value.php Google/Protobuf/Int64Value.php Google/Protobuf/ListValue.php Google/Protobuf/NullValue.php Google/Protobuf/StringValue.php Google/Protobuf/Struct.php Google/Protobuf/Timestamp.php Google/Protobuf/UInt32Value.php Google/Protobuf/UInt64Value.php Google/Protobuf/Value.php Protobuf_test_messages/Proto3/ForeignEnum.php Protobuf_test_messages/Proto3/ForeignMessage.php Protobuf_test_messages/Proto3/TestAllTypes_NestedEnum.php Protobuf_test_messages/Proto3/TestAllTypes_NestedMessage.php Protobuf_test_messages/Proto3/TestAllTypes.php\r\nrm -rf .libs _libs\r\nrm -f *.o\r\nrm -f ../objectivec/*.o\r\nrm -f google/protobuf/*.o\r\nrm -f third_party/jsoncpp/*.o\r\nrm -f *.lo\r\nrm -f *.tab.c\r\ntest -z \"\" || rm -f \r\ntest . = \".\" || test -z \"\" || rm -f \r\nrm -f ../objectivec/.deps/.dirstamp\r\nrm -f ../objectivec/.dirstamp\r\nrm -f google/protobuf/.deps/.dirstamp\r\nrm -f google/protobuf/.dirstamp\r\nrm -f third_party/jsoncpp/.deps/.dirstamp\r\nrm -f third_party/jsoncpp/.dirstamp\r\nrm -f TAGS ID GTAGS GRTAGS GSYMS GPATH tags\r\nrm -rf ../objectivec/.deps ./.deps google/protobuf/.deps third_party/jsoncpp/.deps\r\nrm -f Makefile\r\nMaking distclean in benchmarks\r\n rm -f generate-datasets cpp-benchmark\r\ntest -z \"benchmarks.pb.cc benchmarks.pb.h benchmark_messages_proto3.pb.cc benchmark_messages_proto3.pb.h benchmark_messages_proto2.pb.cc benchmark_messages_proto2.pb.h protoc_middleman protoc_middleman2 dataset.*\" || rm -f benchmarks.pb.cc benchmarks.pb.h benchmark_messages_proto3.pb.cc benchmark_messages_proto3.pb.h benchmark_messages_proto2.pb.cc benchmark_messages_proto2.pb.h protoc_middleman protoc_middleman2 dataset.*\r\nrm -rf .libs _libs\r\nrm -f *.o\r\nrm -f *.lo\r\nrm -f *.tab.c\r\ntest -z \"\" || rm -f \r\ntest . = \".\" || test -z \"\" || rm -f \r\nrm -f TAGS ID GTAGS GRTAGS GSYMS GPATH tags\r\nrm -rf ./.deps\r\nrm -f Makefile\r\nrm -rf .libs _libs\r\nCleaning any ObjC pyc files\r\nrm -f *.lo\r\ntest -z \"protobuf.pc protobuf-lite.pc\" || rm -f protobuf.pc protobuf-lite.pc\r\ntest . = \".\" || test -z \"\" || rm -f \r\nrm -f config.h stamp-h1\r\nrm -f libtool config.lt\r\nrm -f TAGS ID GTAGS GRTAGS GSYMS GPATH tags\r\nrm -f cscope.out cscope.in.out cscope.po.out cscope.files\r\nrm -f config.status config.cache config.log configure.lineno config.status.lineno\r\nrm -f Makefile\r\n+ ./configure --host=armv7k-apple-darwin14.0.0 --with-protoc=/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf-host/bin/protoc --disable-shared --prefix=/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf_ios/lib/ios_arm7k --exec-prefix=/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf_ios/lib/ios_arm7k 'CFLAGS=-DNDEBUG -Os -pipe -fPIC -fno-exceptions -mwatchos-version-min= -arch armv7k -fembed-bitcode -isysroot ' CXX= 'CXXFLAGS=-DNDEBUG -Os -pipe -fPIC -fno-exceptions -std=c++11 -stdlib=libc++ -mwatchos-version-min= -arch armv7k -fembed-bitcode -isysroot ' 'LDFLAGS=-arch armv7k -fembed-bitcode -mwatchos-version-min= -stdlib=libc++' 'LIBS=-lc++ -lc++abi'\r\nchecking whether to enable maintainer-specific portions of Makefiles... yes\r\nchecking build system type... x86_64-apple-darwin16.4.0\r\nchecking host system type... armv7k-apple-darwin14.0.0\r\nchecking target system type... armv7k-apple-darwin14.0.0\r\nchecking for a BSD-compatible install... /usr/bin/install -c\r\nchecking whether build environment is sane... yes\r\nchecking for armv7k-apple-darwin14.0.0-strip... no\r\nchecking for strip... strip\r\nchecking for a thread-safe mkdir -p... ./install-sh -c -d\r\nchecking for gawk... no\r\nchecking for mawk... no\r\nchecking for nawk... no\r\nchecking for awk... awk\r\nchecking whether make sets $(MAKE)... yes\r\nchecking whether make supports nested variables... yes\r\nchecking whether UID '501' is supported by ustar format... yes\r\nchecking whether GID '20' is supported by ustar format... yes\r\nchecking how to create a ustar tar archive... gnutar\r\nchecking for armv7k-apple-darwin14.0.0-gcc... no\r\nchecking for gcc... gcc\r\nchecking whether the C compiler works... no\r\nconfigure: error: in `/Volumes/MacintoshHD2/tensorflow-master/tensorflow/contrib/makefile/downloads/protobuf':\r\nconfigure: error: C compiler cannot create executables\r\nSee `config.log' for more details\r\n```\r\n\r\n", "The relevant error lines in the config.log are these:\r\n\r\n```\r\nconfigure:3873: gcc -DNDEBUG -Os -pipe -fPIC -fno-exceptions -mwatchos-version-min= -arch armv7k -fembed-bitcode -isysroot   -arch armv7k -fembed-bitcode -mwatchos-version-min= -stdlib=libc++ conftest.c -lc++ -lc++abi >&5\r\nclang: error: no such file or directory: 'armv7k'\r\nclang: warning: no such sysroot directory: '-arch'\r\nclang: error: invalid version number in '-mwatchos-version-min='\r\n```\r\n\r\nI'm not sure where these errors are coming from, but hopefully you can trace them back (I think it should be -arch=armv7k for example).", "So I was able to get tensorflow to compile for armv7k and I even got it to run on an apple watch, but I'm getting an error about \"No session factory registered for the given session options\". I found this issue about that warning https://github.com/tensorflow/tensorflow/issues/3308, in that it says to add the -all_load option, but I don't know where to add that flag.", "Sorry for the slow response, I was traveling. You should add that flag to the \"Other Linker Flags\" section of your xcode project. If you look at the 'simple' or 'camera' examples in contrib/ios_examples you should be able to see where they do it.", "No worries, I was out sick for the better part of last week so I wasn't making a lot of progress during that time either.\r\n\r\nSo I tried with the the linker flags and I got two different outcomes.\r\n\r\nIf I use the -force_load option like in the examples I'm getting a clang error saying no such file or directory: 'WatchKit'\r\n\r\nIf I used -all_load I get errors about undefined symbols for armv7k\r\n\r\n```clang\r\nUndefined symbols for architecture armv7k:\r\n  \"_deflate\", referenced from:\r\n      google::protobuf::io::GzipOutputStream::Deflate(int) in libprotobuf.a(gzip_stream.o)\r\n  \"_inflate\", referenced from:\r\n      google::protobuf::io::GzipInputStream::Inflate(int) in libprotobuf.a(gzip_stream.o)\r\n  \"_inflateInit2_\", referenced from:\r\n      google::protobuf::io::GzipInputStream::Inflate(int) in libprotobuf.a(gzip_stream.o)\r\n      google::protobuf::io::GzipInputStream::Next(void const**, int*) in libprotobuf.a(gzip_stream.o)\r\n  \"_deflateEnd\", referenced from:\r\n      google::protobuf::io::GzipOutputStream::Close() in libprotobuf.a(gzip_stream.o)\r\n  \"_deflateInit2_\", referenced from:\r\n      google::protobuf::io::GzipOutputStream::Init(google::protobuf::io::ZeroCopyOutputStream*, google::protobuf::io::GzipOutputStream::Options const&) in libprotobuf.a(gzip_stream.o)\r\n  \"_inflateEnd\", referenced from:\r\n      google::protobuf::io::GzipInputStream::~GzipInputStream() in libprotobuf.a(gzip_stream.o)\r\n      google::protobuf::io::GzipInputStream::Next(void const**, int*) in libprotobuf.a(gzip_stream.o)\r\nld: symbol(s) not found for architecture armv7k\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\nI think -all_load is the right flag to go with, but I'm not sure what I need to change in the tensorflow build script to get symbols for these.", "Great to see the progress! That looks like you just need to include zlib, probably with -lz in the linker flags.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It's been a while since there's been any updates on this bug, and nobody on the internal TF team has been working on it, so closing for now. Please reopen if it should still be active."]}, {"number": 8207, "title": "Operation Documentation has \" within it", "body": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/array_ops.cc#L1154\r\n\r\nThis wasn't causing a problem because TF was stripping out the documentation before hitting Python and the Python Protobuf text_format.py https://github.com/google/protobuf/issues/2798. I wanted to expose the documentation in the Python (A patch I am NOT recommending!!) per http://stackoverflow.com/questions/42521166/tensorflow-operation-documentation, but ran into numerous issues in the Protobuf seen in the above issue posted to Protobuf. The last problem I had stemmed from TF's misuse of \" within the documentation\r\n\r\nI did a batch replacement with a small script,\r\n```\r\nimport os, re\r\n\r\nfor fname in os.listdir('./'):\r\n    if fname[-3:] != \".cc\":\r\n        print \"Skipping\", fname\r\n        continue\r\n\r\n    with open(fname) as f:\r\n        code = f.read()\r\n\r\n    quotes = 0\r\n    blocks = re.findall('\\\"[Dd]oc\\((?:(?!REG).|\\n)*\\)[Dd]oc\\\"', code)\r\n    for sub in blocks:\r\n        if sub.count('\"') <= 2:\r\n            continue\r\n\r\n        s = sub.find('\"')\r\n        e = sub.rfind('\"')\r\n        new_sub = sub[:s+1] + sub[s+1:e].replace('\"', \"'\") + sub[e:]\r\n        quotes += sub.count('\"') - new_sub.count('\"')\r\n        code = code.replace(sub, new_sub)\r\n\r\n    print \"Replacing\", quotes, \"in\", fname\r\n    f = open(fname, 'w')\r\n    f.write(code)\r\n    f.close()\r\n```\r\n\r\nI can submit a pull request if you like, but running that script in `tensorflow/tensorflow/core/ops` achieves the same thing.\r\n", "comments": ["I put this into a Dockerfile and it built,\r\n\r\n```\r\nFROM tensorflow/tensorflow:nightly-devel\r\n# Remove misused quotes in documentation in Python Protobuf\r\n# https://github.com/tensorflow/tensorflow/issues/8207\r\nCOPY dequote.py /tensorflow/tensorflow/core/ops/\r\nRUN cd /tensorflow/tensorflow/core/ops/ && \\\r\n    python dequote.py &&\\\r\n    rm dequote.py\r\n... Rebuild commands ...\r\n```\r\n\r\n`dequote.py` is the code above", "If you want to fix it and send a pull request that would be fantastic!", "Please open a PR with commits from authors who have signed the CLA:\r\nhttps://github.com/tensorflow/tensorflow/pull/8221\r\n\r\nClosing for now."]}, {"number": 8206, "title": "Fix compile warnings.", "body": "Fixes instances of comparisons between 'signed' and 'unsigned'.", "comments": ["Can one of the admins verify this patch?", "@cwhipkey please review. More compiler warning fixes. Maybe we should have a check for compiler warnings before commit?", "@tensorflow-jenkins Test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 8205, "title": "`tf.losses.cosine_distance` still uses `dim`", "body": "- Version: current HEAD on master\r\n\r\nThe loss `tf.losses.cosine_distance` only accepts `dim`, not `axis` as the convention suggests for TF 1.0. It internally passes `axis=(dim,)` to `reduce_sum` though, so there may be a reason to keep the name I'm not getting.", "comments": ["@sherrym @josh11b This is a suggested API consistency change."]}, {"number": 8204, "title": "CMake: use copy_if_different to replace copy", "body": "For speedup edit-compile-run loop", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please."]}, {"number": 8203, "title": "Changed inconsistent conv1d to use 'filter' param instead of 'filters'", "body": "Updated the docs and all occurrences of 'filters' when used as input to conv1d. Partial fix for #6379.", "comments": ["Can one of the admins verify this patch?", "@martinwicke This is technically a backwards-incompatible API change (b.c. ppl may use keyword params) and therefore inadmissible, yes?", "Indeed, that is sad. We can accept a second parameter called filter, which\ntakes filters' spot, and we can deprecate filters, which would move to the\nend. That would be backward compatible. This change we cannot make because\nit may well break current users.\n", "I can change it to be a second parameter. Should there be a deprecation message when using filters?\r\nWhy was is it ok to do this in #7883 ?  Didn't that break the API as well?", "@JulienSiems The PR you referred was only documentation change. The first argument name change is not as big as this one. A longer warning period before removal would be more appropriate, e.g. remove when it gets to next major release. ", "Should I close this pull request? If it's better solved for the next major release?", "You could close the PR, or make the change in a backwards-compatible way as martin suggested.", "I used the deprecated_args decorator to deprecate filters. Is that the way it should be done? "]}, {"number": 8202, "title": "build_config.bzl file modified by configure", "body": "`configure` modifies `build_config.bzl` to set the `WITH_JEMALLOC` flag.  This is inconvenient when using git, since it means I can't pull after doing configure without manually clearing the file.\r\n\r\nCan the setup be modified so that `configure` makes a file not under git control?", "comments": ["Cc @jart. ", "`git status` after running `configure`:\r\n\r\n    shimura:tensorflow% git st\r\n    On branch master\r\n    Your branch is behind 'origin/master' by 445 commits, and can be fast-forwarded.\r\n      (use \"git pull\" to update your local branch)\r\n    Changes not staged for commit:\r\n      (use \"git add <file>...\" to update what will be committed)\r\n      (use \"git checkout -- <file>...\" to discard changes in working directory)\r\n\r\n      modified:   tensorflow/core/platform/default/build_config.bzl\r\n\r\n    Untracked files:\r\n      (use \"git add <file>...\" to include in what will be committed)\r\n\r\n      tensorflow/core/platform/default/build_config_root.bzl-e\r\n\r\n    no changes added to commit (use \"git add\" and/or \"git commit -a\")", "We received a pull request adding the -e file to gitignore in https://github.com/tensorflow/tensorflow/pull/7978, but I'd like to understand more why it's there in the first place, so I'll take a look now.", "As for build_config.bzl being modified, I have an idea which I think might work. Let me see if I can code that up quickly.", "PS, the workaround  to avoid overwriting ./configure results on each rebase/pull:\r\n\r\n```\r\ngit stash\r\ngit rebase origin/master\r\ngit stash pop\r\ngit checkout --theirs -- tensorflow/core/platform/default/build_config.bzl\r\n```"]}, {"number": 8201, "title": "Make summarize_graph recognize VariableV2", "body": "#8054", "comments": ["Can one of the admins verify this patch?", "Can you add a test please?", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 8200, "title": "Add cmake build for benchmark_model", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 8199, "title": "Tensorboard scalar summary graph distorts after resizing and toggling log scale on y-axis ", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone although a search for distorted image tensorboard doesn't help much...\r\n\r\n### Environment info\r\nOperating System: 16.04 LTS\r\nFirefox: 51.0.1 (64-bit)\r\nTF: 1.0 (installed via pip)\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\n$> sudo ls -l /usr/local/cudnn/*\r\n/usr/local/cudnn/include:\r\ntotal 100\r\n-r--r--r-- 1 root root 99658 Feb 20 11:27 cudnn.h\r\n\r\n/usr/local/cudnn/lib64:\r\ntotal 150908\r\nlrwxrwxrwx 1 root root       13 Feb 20 11:27 libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       35 Feb 16 17:01 libcudnn.so.4 -> /usr/local/cuda/lib64/libcudnn.so.4\r\nlrwxrwxrwx 1 root root       39 Feb 16 17:01 libcudnn.so.4.0.7 -> /usr/local/cuda/lib64/libcudnn.so.4.0.7\r\nlrwxrwxrwx 1 root root       18 Feb 20 11:27 libcudnn.so.5 -> libcudnn.so.5.1.10\r\n-rwxr-xr-x 1 root root 84163560 Feb 20 11:27 libcudnn.so.5.1.10\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nStandard TF pip url.\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n```\r\n$> python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.0\r\n```\r\n\r\n### Steps to reproduce (Firefox only)\r\n1. On the 'Scalars' tab for tensorboard, view the graph of a scalar summary (eg _cost_ or _accuracy_) by **expanding the tab**. ![tensorboard - mozilla firefox_027](https://cloud.githubusercontent.com/assets/252960/23708510/0bcd8b5a-040e-11e7-8a19-e2eb7ee208af.png)\r\n2. **Click on the expand icon** ![tensorboard - mozilla firefox_028](https://cloud.githubusercontent.com/assets/252960/23708524/1227e784-040e-11e7-9c5c-e8f466df5581.png)\r\n3. **Enable** log scale of y-axis ![tensorboard - mozilla firefox_029](https://cloud.githubusercontent.com/assets/252960/23708530/162e403a-040e-11e7-8628-fbd04ae05642.png)\r\n4. **Disable** log scale of y-axis (note the bug happens regardless of whether you do this) ![tensorboard - mozilla firefox_028](https://cloud.githubusercontent.com/assets/252960/23708524/1227e784-040e-11e7-9c5c-e8f466df5581.png)\r\n5. **Click on expand icon** to shrink the graph.\r\n\r\nThe graph is now overflowing: ![tensorboard - mozilla firefox_030](https://cloud.githubusercontent.com/assets/252960/23708531/18f031a2-040e-11e7-9e22-8c76415ddb14.png)\r\n\r\n### What other attempted solutions have you tried?\r\nTried to reproduce in Chromium 55.0.2883.87 but unable to.\r\n\r\n\r\n", "comments": ["Migrated this to the TensorBoard repo."]}, {"number": 8198, "title": "OOM although very small network", "body": "Operating System:\r\nUbuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\nCUDA 8.0\r\nCuDNN 5.1\r\n\r\nTensorFlow v0.12.1\r\n\r\nHi,\r\nI'm getting an OOM message on a very small network, and running on 2 GTX 1080.\r\nIt's a 2 layer network, first 2 layers of VGG, conv1_1, conv1_2.\r\nThe input image is 400x400 and I am trying to run a batch of size 16.\r\n\r\nMy training is taking a feature vector from 4 spatial positions and training with some loss on it.\r\nSo for example, after 2 VGG conv layers I will have a feature vector of size 64 at each pixel, or to be exact, a tensor of size [16,400,400,64].\r\nI want to take these vectors from 4 locations, meaning I will have 4 vectors of length 64, then calculating some loss function on them.\r\n\r\nSo this is my inference function:\r\n\r\n```\r\ndef inference(images, x1, y, x2, z, train=False):\r\n  # conv1_1\r\n  with tf.variable_scope('conv1_1') as scope:\r\n    kernel = _variable_with_weight_decay('weights', shape=[3, 3, 3, 64], wd=0.000, layer_name=scope.name)\r\n    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\r\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0), layer_name=scope.name)\r\n    conv1_1 = tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope.name)\r\n\r\n  # conv1_2\r\n  with tf.variable_scope('conv1_2') as scope:\r\n    kernel = _variable_with_weight_decay('weights', shape=[3, 3, 64, 64], wd=0.000, layer_name=scope.name)\r\n    conv = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\r\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0), layer_name=scope.name)\r\n    conv1_2 = tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope.name)\r\n\r\n  in1=tf.reshape(conv1_2[0, x1[0][0], x1[0][1], :],[1,64])\r\n  in2=tf.reshape(conv1_2[0, y[0][0], y[0][1], :],[1,64])\r\n  in3=tf.reshape(conv1_2[0, x2[0][0], x2[0][1], :],[1,64])\r\n  in4=tf.reshape(conv1_2[0, z[0][0], z[0][1], :],[1,64])\r\n\r\n  for i in range (1, FLAGS.batch_size):\r\n      in1 = tf.concat(0,[in1,tf.reshape(conv1_2[i, x1[i][0], x1[i][1], :],[1,64])])\r\n      in2 = tf.concat(0,[in2,tf.reshape(conv1_2[i, y[i][0], y[i][1], :],[1,64])])\r\n      in3 = tf.concat(0,[in3,tf.reshape(conv1_2[i, x2[i][0], x2[i][1], :],[1,64])])\r\n      in4 = tf.concat(0,[in4,tf.reshape(conv1_2[i, z[i][0], z[i][1], :],[1,64])])\r\n```\r\n\r\nNow, each in1,in2,in3,in4 is of size [16,64]\r\nFrom here on I calculate some loss with that.\r\nFor some reason I am getting an OOM message, although this is a very small network. **I guess that the way I am taking these feature vectors makes the tool allocate way bigger memory than needed.**\r\n\r\n> ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,400,400,64]\r\n> \t [[Node: gradients/strided_slice_84_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, begin_mask=8, ellipsis_mask=0, end_mask=8, new_axis_mask=0, shrink_axis_mask=7, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/strided_slice_84_grad/Shape, strided_slice_84/stack, strided_slice_84/stack_1, strided_slice_84/stack_2, gradients/Reshape_16_grad/Reshape)]]\r\n> \t [[Node: gradients/conv1_1/BiasAdd_grad/tuple/control_dependency_1/_99 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2359_gradients/conv1_1/BiasAdd_grad/tuple/control_dependency_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n> \r\n> \r\n\r\nThanks in advance for the help!", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8197, "title": "import tensorflow Segmentation fault (core dumped)", "body": "(gdb) bt\r\n#0  0x00007f8ef604478b in init_one_static_tls (map=0x0) at allocatestack.c:1171\r\n#1  __pthread_init_static_tls (map=0x0) at allocatestack.c:1196\r\n#2  0x00007f8ef667e6eb in _dl_close_worker () from /lib64/ld-linux-x86-64.so.2\r\n#3  0x00007f8ef667c613 in _dl_open () from /lib64/ld-linux-x86-64.so.2\r\n#4  0x00007f8ef5e3af66 in dlopen_doit (a=0x7ffc4dda53f0) at dlopen.c:67\r\n#5  0x00007f8ef6678266 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2\r\n#6  0x00007f8ef5e3b2dc in _dlerror_run (operate=0x7f8ef5e3af00 <dlopen_doit>, args=0x7ffc4dda53f0) at dlerror.c:164\r\n#7  0x00007f8ef5e3aee1 in __dlopen (file=<value optimized out>, mode=<value optimized out>) at dlopen.c:88\r\n#8  0x00007f8ef63918ce in _PyImport_GetDynLoadFunc (fqname=<value optimized out>, shortname=<value optimized out>, pathname=0x7f8edc36fa54 \"/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\", \r\n    fp=0x1d80f90) at Python/dynload_shlib.c:130\r\n#9  0x00007f8ef6376128 in _PyImport_LoadDynamicModule (name=0x7f8edc36524c \"_pywrap_tensorflow\", pathname=0x7f8edc36fa54 \"/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\", fp=0x1d80f90)\r\n    at ./Python/importdl.c:42\r\n#10 0x00007f8ef6375ca0 in imp_load_module (self=<value optimized out>, args=<value optimized out>) at Python/import.c:3207\r\n#11 0x00007f8ef635d969 in call_function (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:4352\r\n#12 PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2989\r\n#13 0x00007f8ef635eadf in fast_function (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:4437\r\n#14 call_function (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:4372\r\n#15 PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2989\r\n#16 0x00007f8ef635f3ce in PyEval_EvalCodeEx (co=0x7f8edc36f830, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)\r\n    at Python/ceval.c:3584\r\n#17 0x00007f8ef635f4e2 in PyEval_EvalCode (co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>) at Python/ceval.c:669\r\n#18 0x00007f8ef6371292 in PyImport_ExecCodeModuleEx (name=0x1d7a990 \"tensorflow.python.pywrap_tensorflow\", co=0x7f8edc36f830, pathname=0x1d7c9b0 \"/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.pyc\")\r\n    at Python/import.c:731\r\n#19 0x00007f8ef63739de in load_source_module (name=0x1d7a990 \"tensorflow.python.pywrap_tensorflow\", pathname=0x1d7c9b0 \"/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.pyc\", fp=<value optimized out>)\r\n    at Python/import.c:1121\r\n#20 0x00007f8ef6374791 in import_submodule (mod=0x7f8ef4cd09b8, subname=0x7f8edc3651dc \"pywrap_tensorflow\", fullname=0x1d7a990 \"tensorflow.python.pywrap_tensorflow\") at Python/import.c:2725\r\n#21 0x00007f8ef6374ccc in ensure_fromlist (mod=0x7f8ef4cd09b8, fromlist=0x7f8ef38c6fd0, buf=0x1d7a990 \"tensorflow.python.pywrap_tensorflow\", buflen=17, recursive=0) at Python/import.c:2631\r\n#22 0x00007f8ef63751b4 in import_module_level (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef38c6fd0, level=<value optimized out>) at Python/import.c:2293\r\n#23 PyImport_ImportModuleLevel (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef38c6fd0, level=<value optimized out>) at Python/import.c:2312\r\n#24 0x00007f8ef6354a4f in builtin___import__ (self=<value optimized out>, args=<value optimized out>, kwds=<value optimized out>) at Python/bltinmodule.c:49\r\n#25 0x00007f8ef62aac13 in PyObject_Call (func=0x7f8ef6767fc8, arg=<value optimized out>, kw=<value optimized out>) at Objects/abstract.c:2547\r\n#26 0x00007f8ef6354f33 in PyEval_CallObjectWithKeywords (func=0x7f8ef6767fc8, arg=0x7f8edc33ef50, kw=<value optimized out>) at Python/ceval.c:4221\r\n#27 0x00007f8ef6359bc6 in PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2624\r\n#28 0x00007f8ef635f3ce in PyEval_EvalCodeEx (co=0x7f8edc3545b0, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)\r\n    at Python/ceval.c:3584\r\n#29 0x00007f8ef635f4e2 in PyEval_EvalCode (co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>) at Python/ceval.c:669\r\n#30 0x00007f8ef6371292 in PyImport_ExecCodeModuleEx (name=0x1d918e0 \"tensorflow.python.framework.versions\", co=0x7f8edc3545b0, \r\n    pathname=0x1a4c2b0 \"/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/framework/versions.pyc\") at Python/import.c:731\r\n#31 0x00007f8ef63739de in load_source_module (name=0x1d918e0 \"tensorflow.python.framework.versions\", pathname=0x1a4c2b0 \"/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/framework/versions.pyc\", \r\n    fp=<value optimized out>) at Python/import.c:1121\r\n#32 0x00007f8ef6374791 in import_submodule (mod=0x7f8ef39046e0, subname=0x7f8ef39a1744 \"versions\", fullname=0x1d918e0 \"tensorflow.python.framework.versions\") at Python/import.c:2725\r\n#33 0x00007f8ef6374ccc in ensure_fromlist (mod=0x7f8ef39046e0, fromlist=0x7f8ef390e710, buf=0x1d918e0 \"tensorflow.python.framework.versions\", buflen=27, recursive=0) at Python/import.c:2631\r\n#34 0x00007f8ef63751b4 in import_module_level (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef390e710, level=<value optimized out>) at Python/import.c:2293\r\n#35 PyImport_ImportModuleLevel (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef390e710, level=<value optimized out>) at Python/import.c:2312\r\n#36 0x00007f8ef6354a4f in builtin___import__ (self=<value optimized out>, args=<value optimized out>, kwds=<value optimized out>) at Python/bltinmodule.c:49\r\n#37 0x00007f8ef62aac13 in PyObject_Call (func=0x7f8ef6767fc8, arg=<value optimized out>, kw=<value optimized out>) at Objects/abstract.c:2547\r\n#38 0x00007f8ef6354f33 in PyEval_CallObjectWithKeywords (func=0x7f8ef6767fc8, arg=0x7f8ef3922a70, kw=<value optimized out>) at Python/ceval.c:4221\r\n#39 0x00007f8ef6359bc6 in PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2624\r\n#40 0x00007f8ef635f3ce in PyEval_EvalCodeEx (co=0x7f8ef392e2b0, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)\r\n    at Python/ceval.c:3584\r\n#41 0x00007f8ef635f4e2 in PyEval_EvalCode (co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>) at Python/ceval.c:669\r\n#42 0x00007f8ef6371292 in PyImport_ExecCodeModuleEx (name=0x1a49280 \"tensorflow.python.framework.ops\", co=0x7f8ef392e2b0, pathname=0x1a4b2a0 \"/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\")\r\n    at Python/import.c:731\r\n#43 0x00007f8ef63739de in load_source_module (name=0x1a49280 \"tensorflow.python.framework.ops\", pathname=0x1a4b2a0 \"/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\", fp=<value optimized out>)\r\n    at Python/import.c:1121\r\n#44 0x00007f8ef6374791 in import_submodule (mod=0x7f8ef39046e0, subname=0x1a4929c \"ops\", fullname=0x1a49280 \"tensorflow.python.framework.ops\") at Python/import.c:2725\r\n#45 0x00007f8ef6374a04 in load_next (mod=0x7f8ef39046e0, altmod=0x7f8ef39046e0, p_name=<value optimized out>, buf=0x1a49280 \"tensorflow.python.framework.ops\", p_buflen=0x7ffc4dda68e0) at Python/import.c:2539\r\n#46 0x00007f8ef6375098 in import_module_level (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef3969c50, level=<value optimized out>) at Python/import.c:2256\r\n#47 PyImport_ImportModuleLevel (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef3969c50, level=<value optimized out>) at Python/import.c:2312\r\n#48 0x00007f8ef6354a4f in builtin___import__ (self=<value optimized out>, args=<value optimized out>, kwds=<value optimized out>) at Python/bltinmodule.c:49\r\n#49 0x00007f8ef62aac13 in PyObject_Call (func=0x7f8ef6767fc8, arg=<value optimized out>, kw=<value optimized out>) at Objects/abstract.c:2547\r\n#50 0x00007f8ef6354f33 in PyEval_CallObjectWithKeywords (func=0x7f8ef6767fc8, arg=0x7f8ef39666b0, kw=<value optimized out>) at Python/ceval.c:4221\r\n#51 0x00007f8ef6359bc6 in PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2624\r\n#52 0x00007f8ef635f3ce in PyEval_EvalCodeEx (co=0x7f8ef39e55b0, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)\r\n    at Python/ceval.c:3584\r\n#53 0x00007f8ef635f4e2 in PyEval_EvalCode (co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>) at Python/ceval.c:669\r\n---Type <return> to continue, or q <return> to quit---\r\n#54 0x00007f8ef6371292 in PyImport_ExecCodeModuleEx (name=0x1897ce0 \"tensorflow.python.framework.framework_lib\", co=0x7f8ef39e55b0, \r\n    pathname=0x1a48270 \"/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/framework/framework_lib.pyc\") at Python/import.c:731\r\n#55 0x00007f8ef63739de in load_source_module (name=0x1897ce0 \"tensorflow.python.framework.framework_lib\", pathname=0x1a48270 \"/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/framework/framework_lib.pyc\", \r\n    fp=<value optimized out>) at Python/import.c:1121\r\n#56 0x00007f8ef6374791 in import_submodule (mod=0x7f8ef39046e0, subname=0x1897cfc \"framework_lib\", fullname=0x1897ce0 \"tensorflow.python.framework.framework_lib\") at Python/import.c:2725\r\n#57 0x00007f8ef6374a04 in load_next (mod=0x7f8ef39046e0, altmod=0x7f8ef39046e0, p_name=<value optimized out>, buf=0x1897ce0 \"tensorflow.python.framework.framework_lib\", p_buflen=0x7ffc4dda6e80) at Python/import.c:2539\r\n#58 0x00007f8ef6375098 in import_module_level (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef4ccb790, level=<value optimized out>) at Python/import.c:2256\r\n#59 PyImport_ImportModuleLevel (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef4ccb790, level=<value optimized out>) at Python/import.c:2312\r\n#60 0x00007f8ef6354a4f in builtin___import__ (self=<value optimized out>, args=<value optimized out>, kwds=<value optimized out>) at Python/bltinmodule.c:49\r\n#61 0x00007f8ef62aac13 in PyObject_Call (func=0x7f8ef6767fc8, arg=<value optimized out>, kw=<value optimized out>) at Objects/abstract.c:2547\r\n#62 0x00007f8ef6354f33 in PyEval_CallObjectWithKeywords (func=0x7f8ef6767fc8, arg=0x7f8ef4f09310, kw=<value optimized out>) at Python/ceval.c:4221\r\n#63 0x00007f8ef6359bc6 in PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2624\r\n#64 0x00007f8ef635f3ce in PyEval_EvalCodeEx (co=0x7f8ef4cbcc30, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)\r\n    at Python/ceval.c:3584\r\n#65 0x00007f8ef635f4e2 in PyEval_EvalCode (co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>) at Python/ceval.c:669\r\n#66 0x00007f8ef6371292 in PyImport_ExecCodeModuleEx (name=0x1860290 \"tensorflow.python\", co=0x7f8ef4cbcc30, pathname=0x1896cd0 \"/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/__init__.pyc\") at Python/import.c:731\r\n#67 0x00007f8ef63739de in load_source_module (name=0x1860290 \"tensorflow.python\", pathname=0x1896cd0 \"/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/__init__.pyc\", fp=<value optimized out>) at Python/import.c:1121\r\n#68 0x00007f8ef637423a in load_package (name=0x1860290 \"tensorflow.python\", pathname=<value optimized out>) at Python/import.c:1188\r\n#69 0x00007f8ef6374791 in import_submodule (mod=0x7f8ef4cd0600, subname=0x186029b \"python\", fullname=0x1860290 \"tensorflow.python\") at Python/import.c:2725\r\n#70 0x00007f8ef6374a04 in load_next (mod=0x7f8ef4cd0600, altmod=0x7f8ef4cd0600, p_name=<value optimized out>, buf=0x1860290 \"tensorflow.python\", p_buflen=0x7ffc4dda7470) at Python/import.c:2539\r\n#71 0x00007f8ef6375098 in import_module_level (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef4ca4e50, level=<value optimized out>) at Python/import.c:2256\r\n#72 PyImport_ImportModuleLevel (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef4ca4e50, level=<value optimized out>) at Python/import.c:2312\r\n#73 0x00007f8ef6354a4f in builtin___import__ (self=<value optimized out>, args=<value optimized out>, kwds=<value optimized out>) at Python/bltinmodule.c:49\r\n#74 0x00007f8ef62aac13 in PyObject_Call (func=0x7f8ef6767fc8, arg=<value optimized out>, kw=<value optimized out>) at Objects/abstract.c:2547\r\n#75 0x00007f8ef6354f33 in PyEval_CallObjectWithKeywords (func=0x7f8ef6767fc8, arg=0x7f8ef4ca8230, kw=<value optimized out>) at Python/ceval.c:4221\r\n#76 0x00007f8ef6359bc6 in PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2624\r\n#77 0x00007f8ef635f3ce in PyEval_EvalCodeEx (co=0x7f8ef4f19cb0, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)\r\n    at Python/ceval.c:3584\r\n#78 0x00007f8ef635f4e2 in PyEval_EvalCode (co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>) at Python/ceval.c:669\r\n#79 0x00007f8ef6371292 in PyImport_ExecCodeModuleEx (name=0x188c4c0 \"tensorflow\", co=0x7f8ef4f19cb0, pathname=0x185f280 \"/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/__init__.pyc\") at Python/import.c:731\r\n#80 0x00007f8ef63739de in load_source_module (name=0x188c4c0 \"tensorflow\", pathname=0x185f280 \"/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/__init__.pyc\", fp=<value optimized out>) at Python/import.c:1121\r\n#81 0x00007f8ef637423a in load_package (name=0x188c4c0 \"tensorflow\", pathname=<value optimized out>) at Python/import.c:1188\r\n#82 0x00007f8ef6374791 in import_submodule (mod=0x7f8ef661e730, subname=0x188c4c0 \"tensorflow\", fullname=0x188c4c0 \"tensorflow\") at Python/import.c:2725\r\n#83 0x00007f8ef6374a04 in load_next (mod=0x7f8ef661e730, altmod=0x7f8ef661e730, p_name=<value optimized out>, buf=0x188c4c0 \"tensorflow\", p_buflen=0x7ffc4dda7a60) at Python/import.c:2539\r\n#84 0x00007f8ef6375044 in import_module_level (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef661e730, level=<value optimized out>) at Python/import.c:2247\r\n#85 PyImport_ImportModuleLevel (name=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>, fromlist=0x7f8ef661e730, level=<value optimized out>) at Python/import.c:2312\r\n#86 0x00007f8ef6354a4f in builtin___import__ (self=<value optimized out>, args=<value optimized out>, kwds=<value optimized out>) at Python/bltinmodule.c:49\r\n#87 0x00007f8ef62aac13 in PyObject_Call (func=0x7f8ef6767fc8, arg=<value optimized out>, kw=<value optimized out>) at Objects/abstract.c:2547\r\n#88 0x00007f8ef6354f33 in PyEval_CallObjectWithKeywords (func=0x7f8ef6767fc8, arg=0x7f8ef4f092b8, kw=<value optimized out>) at Python/ceval.c:4221\r\n#89 0x00007f8ef6359bc6 in PyEval_EvalFrameEx (f=<value optimized out>, throwflag=<value optimized out>) at Python/ceval.c:2624\r\n#90 0x00007f8ef635f3ce in PyEval_EvalCodeEx (co=0x7f8ef4f08db0, globals=<value optimized out>, locals=<value optimized out>, args=<value optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)\r\n    at Python/ceval.c:3584\r\n#91 0x00007f8ef635f4e2 in PyEval_EvalCode (co=<value optimized out>, globals=<value optimized out>, locals=<value optimized out>) at Python/ceval.c:669\r\n#92 0x00007f8ef637f785 in run_mod (fp=<value optimized out>, filename=0x7f8ef63d206f \"<stdin>\", flags=0x7ffc4dda7f80) at Python/pythonrun.c:1376\r\n#93 PyRun_InteractiveOneFlags (fp=<value optimized out>, filename=0x7f8ef63d206f \"<stdin>\", flags=0x7ffc4dda7f80) at Python/pythonrun.c:857\r\n#94 0x00007f8ef637f99e in PyRun_InteractiveLoopFlags (fp=0x7f8ef52de6c0, filename=0x7f8ef63d206f \"<stdin>\", flags=0x7ffc4dda7f80) at Python/pythonrun.c:777\r\n#95 0x00007f8ef63800dc in PyRun_AnyFileExFlags (fp=0x7f8ef52de6c0, filename=0x7f8ef63d206f \"<stdin>\", closeit=0, flags=0x7ffc4dda7f80) at Python/pythonrun.c:746\r\n#96 0x00007f8ef6395694 in Py_Main (argc=<value optimized out>, argv=<value optimized out>) at Modules/main.c:640\r\n#97 0x00007f8ef4f770bd in __libc_start_main (main=0x400730 <main>, argc=1, ubp_av=0x7ffc4dda80a8, init=<value optimized out>, fini=<value optimized out>, rtld_fini=<value optimized out>, stack_end=0x7ffc4dda8098) at libc-start.c:226\r\n#98 0x0000000000400669 in _start ()\r\n\r\n------------------------------------------------------------------------\r\n**I installed glibc 2.14 in another env and set LD_LIBRARY_PATH\r\nexport LD_LIBRARY_PATH=/usr/local/python2.7/lib:/opt/glibc-2.14/lib:$LD_LIBRARY_PATH\r\nbut when import tensorflow then core dumped, \r\nI need for help, thanks a lot**", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "OS\uff1aCentOS Linux release 6.2 (Final)  x86_64 x86_64 x86_64 GNU/Linux\r\nTensorFlow version\uff1ahttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl", "I've solved the problem by update glic to GLIBC_2.14, gcc to gcc-4.8.1, thanks"]}, {"number": 8196, "title": "tf.py_func treating result different on Windows and Ubuntu in Tensorflow 1.0", "body": "### Description\r\n\r\nI'm using tf.py_func in my data fetching pipeline. The applied function basically calculates an int from an int. When I run the tensorflow code, it works on my Windows 10 development laptop, but fails on the Ubuntu server with an error that the python function would return an int64 instead of the expected int32. It is the same behavior for both, the CPU and the GPU backend. \r\n\r\nI would expect tensorflow to behave the same across different platforms. Am I doing something wrong or is this a bug?\r\n\r\n### Environment info\r\nOperating Systems:\r\n\r\n* Windows 10: Python 3.5.2; Tensorflow 1.0.1 (I tried both, CPU and GPU) installed via pip; CUDA 8.0, cudnn 5.1\r\n\r\n   https://gist.github.com/andreas-eberle/76dfaeb8467dd3b520aa8390bd2b5d33\r\n\r\n* Ubuntu 14.04: Python 3.4.3 (I cannot update it because the server is managed); Tensorflow 1.0.0 (there seems to be no 1.0.1 for Ubuntu) (I tried both, CPU and GPU) installed via pip (https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp34-cp34m-linux_x86_64.whl); CUDA 8.0, cudnn 5.1\r\n\r\n   https://gist.github.com/andreas-eberle/fbba8fdb73bff433d89ece9a1946f269\r\n\r\n### Minimal reproducible example\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef multiply_by_2(value):\r\n    return value * 2\r\n\r\n\r\nsess = tf.InteractiveSession()\r\n\r\ninput_pl = tf.placeholder(tf.int32, [])\r\n\r\nresult_tensor = tf.py_func(multiply_by_2, [input_pl], [tf.int32])[0]\r\n\r\nresult_value = sess.run([result_tensor], feed_dict={input_pl: 6})\r\n\r\nprint(result_value)\r\n```\r\n\r\n#### Output on Windows (using tensorflow-cpu)\r\n```\r\nD:\\programs\\python3.5\\python.exe D:/development/private/masters/pReId-mentor/pipeline/BugTest.py\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\n[12]\r\n```\r\n\r\n#### Output on Ubuntu server (using tensorflow-cpu)\r\n```\r\naeberle@i14s35:~/development/pReId-mentor$ python3 pipeline/BugTest.py\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nTraceback (most recent call last):\r\n  File \"/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/lib/python3.4/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 0-th value returned by pyfunc_0 is int64, but expects int32\r\n         [[Node: PyFunc = PyFunc[Tin=[DT_INT32], Tout=[DT_INT32], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_Placeholder_0)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"pipeline/BugTest.py\", line 14, in <module>\r\n    result_value = sess.run([result_tensor], feed_dict={input_pl: 6})\r\n  File \"/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 0-th value returned by pyfunc_0 is int64, but expects int32\r\n         [[Node: PyFunc = PyFunc[Tin=[DT_INT32], Tout=[DT_INT32], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_Placeholder_0)]]\r\n\r\nCaused by op 'PyFunc', defined at:\r\n  File \"pipeline/BugTest.py\", line 12, in <module>\r\n    result_tensor = tf.py_func(multiply_by_2, [input_pl], [tf.int32])[0]\r\n  File \"/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/ops/script_ops.py\", line 192, in py_func\r\n    input=inp, token=token, Tout=Tout, name=name)\r\n  File \"/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/ops/gen_script_ops.py\", line 40, in _py_func\r\n    name=name)\r\n  File \"/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/aeberle/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): 0-th value returned by pyfunc_0 is int64, but expects int32\r\n         [[Node: PyFunc = PyFunc[Tin=[DT_INT32], Tout=[DT_INT32], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_Placeholder_0)]]\r\n```\r\n", "comments": ["@zffchen78 Could you please take a look and see if something is platform specific here?", "I suspect it's python difference. You can debug this:\r\n\r\ndef multiply_by_2(value):\r\n    ret = value * 2\r\n    print('ret.dtype = ', ret.dtype)\r\n    return ret\r\n\r\nRun your program on two platforms and see whether they produce the same. \r\n\r\nTo mitigate the platform difference, I expect you can simply do\r\ndef multiply_by_2(value):\r\n  return (value * 2).astype(np.int32)\r\n"]}, {"number": 8195, "title": "Changes for visual studio 2017", "body": "This change is for visual studio 2017", "comments": ["Can one of the admins verify this patch?", "Oh, I just found it is duplicated with #8008", "@snnn shall we close it as duplicate?", "Let's close this as the other PR is almost ready to merge."]}, {"number": 8194, "title": " NaN loss during training in GMM", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nGithub issue :- [7784 ](https://github.com/tensorflow/tensorflow/issues/7784)\r\nStackoverflow question :- [here](http://stackoverflow.com/questions/42551421/tensorflow-gmm-errortensorflowmodel-diverged-with-loss-nan) and [here](http://stackoverflow.com/questions/42505293/input-is-not-invertible-node-matrixinverse-2-matrixinverset-dt-float-adjoi)\r\n### Environment info\r\nOperating System: Windows 10 / Ubuntu 16.04\r\nInstalled version of CUDA and cuDNN:  NO\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: \r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 1.0 and from [here ](https://github.com/mahatosourav91/tensorflow/tree/pseudo_matrix_inverse)\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nIf using 1.0 \r\n```\r\nfrom tensorflow.contrib.factorization.python.ops import gmm as gmm_lib \r\nimport random \r\nimport numpy as np \r\nx = np.array([[random.random() for i in range(100)] for j in range(1000)] , dtype=np.float32) \r\ngmm = gmm_lib.GMM(128,random_seed=0) \r\ngmm.fit(x)\r\n```\r\n\r\nIf using master branch\r\n```\r\nfrom tensorflow.contrib.factorization.python.ops import gmm as gmm_lib\r\nfrom tensorflow.python.framework import constant_op\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import random_ops\r\nimport random\r\nimport numpy as np\r\n\r\n\r\ndef input_fn(points, batch_size=None):\r\n    num_points = points.shape[0]\r\n    batch_size = batch_size or num_points\r\n    def _fn():\r\n        x = constant_op.constant(points)\r\n        if batch_size == num_points:\r\n            return x, None\r\n        indices = random_ops.random_uniform(constant_op.constant([batch_size]),\r\n                                            minval=0, maxval=num_points - 1,\r\n                                            dtype=dtypes.int32,\r\n                                            seed=10)\r\n        return array_ops.gather(x, indices), None\r\n\r\n    return _fn\r\n\r\nx = np.array([[random.random() for i in range(100)] for j in range(1000)], dtype=np.float32)\r\ngmm = gmm_lib.GMM(50, random_seed=0)\r\ngmm.fit(input_fn=input_fn(x), max_steps=1)\r\ny = list(gmm.predict_assignments(input_fn=input_fn(x)))\r\n```\r\n### What other attempted solutions have you tried?\r\nAs per the discusion happened in [here](https://github.com/tensorflow/tensorflow/issues/7784#issuecomment-283115625). I tried to fix the issue of matrix inversion \r\n```\r\nInvalidArgumentError (see above for traceback): Input is not invertible.\r\n\t [[Node: MatrixInverse_2 = MatrixInverse[T=DT_FLOAT, adjoint=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add_138)]]\r\n```\r\n I have modified[ gmm_ops.py ](https://github.com/mahatosourav91/tensorflow/blob/pseudo_matrix_inverse/tensorflow/contrib/factorization/python/ops/gmm_ops.py) and [linalg_ops.py](https://github.com/mahatosourav91/tensorflow/blob/pseudo_matrix_inverse/tensorflow/python/ops/linalg_ops.py) here\r\nBut even after calculating pseudo matrix inverse I am getting a new error mentioned [here](https://github.com/tensorflow/tensorflow/issues/7784#issuecomment-283346222)\r\n\r\n### Logs or other output that would be helpful\r\n```\r\nERROR:tensorflow:Model diverged with loss = NaN.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-7be5ab638a15>\", line 15, in <module>\r\n    gmm.fit(input_fn=input_fn(x), max_steps=300)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 281, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 418, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 968, in _train_model\r\n    return loss\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3623, in get_controller\r\n    yield default\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 968, in _train_model\r\n    return loss\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3049, in device\r\n    yield\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 966, in _train_model\r\n    _, loss = mon_sess.run([model_fn_ops.train_op, model_fn_ops.loss])\r\n  File \"C:\\Users\\gidnri6\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 483, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 818, in run\r\n    run_metadata=run_metadata)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 775, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 936, in run\r\n    run_metadata=run_metadata))\r\n  File \"C:\\Users\\#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\basic_session_run_hooks.py\", line 481, in after_run\r\n    raise NanLossDuringTrainingError\r\ntensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.\r\n```\r\n\r\n", "comments": ["@agarwal-ashish You are listed as owner for this contrib package  - could you please take a look and see if this is expected behavior?", "I believe this should be fixed by commit e6126230200e2ce9c96da5c9e4dc7f104c645d11 that fixes some numerical stability issue. Please reopen if you continue to see some issues."]}, {"number": 8193, "title": "malfunction of tf.device() on Windows", "body": "tf.device('/gpu:0') fail to run the model on only gpu0, other gpus will be occupied as well. As a result, all the memory of all gpu cards on my server are filled. By setting 'config.gpu_options.visible_device_list = \"0\"', the model will run on gpu0 only.\r\n\r\nBTW, only the memory of other gpus are filled, no actual computation is performed on them, all the computation is performed on gpu0.\r\n\r\nAnyone who can tell me what's going on is sincerely appreciated.\r\n\r\n\r\n### Environment info\r\nOperating System: Windows Server 2012 R2\r\nCUDA: 8.0\r\nNO cuDNN\r\n\r\n1. A link to the pip package you installed:\r\nusing 'pip install tensorflow-gpu' to install\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n1.0.1\r\n\r\n### What other attempted solutions have you tried?\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.visible_device_list = '0'\r\n\r\nusing 'config' to open a session, the graph will only run on '/gpu:0';", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8192, "title": "No OpKernel was registered to support Op 'DecodeJpeg' with these attrs on ios", "body": "hey,everyone!\r\nAt first,i can run the demo in /ios_examples/camera successfully.then I use retrain.py to retrain with my own dataset,and get a output_graph.pb flie and a output_labels.txt file.after that, I replace the files in  /ios_examples/camera/data with them,and run the demo again,but it failed ,error description  are as follow:\r\n####error#########\r\n:Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]];\r\n\r\n:Couldn't load model: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]\r\n\r\nCan someone tell me how to fix it?", "comments": ["Closing as duplicate of #2883  (@petewarden - I'm assuming that workaround is still necessary?)"]}, {"number": 8191, "title": "ValueError: Attempt to reuse RNNCell with a different variable scope than its first use.", "body": "I am not sure if I am the first who met the following error:\r\n\r\nValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x10210d5c0> with a different variable scope than its first use.  First use of cell was with scope 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell', this attempt is with scope 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)\r\n\r\nwith the code fragment:\r\n\r\n      import tensorflow as tf\r\n      from tensorflow.contrib import rnn\r\n\r\n      hidden_size = 100\r\n      batch_size  = 100\r\n      num_steps   = 100\r\n      num_layers  = 100\r\n      is_training = True\r\n      keep_prob   = 0.4\r\n\r\n      input_data = tf.placeholder(tf.float32, [batch_size, num_steps])\r\n      lstm_cell = rnn.BasicLSTMCell(hidden_size, forget_bias=0.0, state_is_tuple=True)\r\n\r\n      if is_training and keep_prob < 1:\r\n          lstm_cell = rnn.DropoutWrapper(lstm_cell)\r\n      cell = rnn.MultiRNNCell([lstm_cell for _ in range(num_layers)], state_is_tuple=True)\r\n\r\n      _initial_state = cell.zero_state(batch_size, tf.float32)\r\n\r\n      iw = tf.get_variable(\"input_w\", [1, hidden_size])\r\n      ib = tf.get_variable(\"input_b\", [hidden_size])\r\n      inputs = [tf.nn.xw_plus_b(i_, iw, ib) for i_ in tf.split(input_data, num_steps, 1)]\r\n\r\n      if is_training and keep_prob < 1:\r\n          inputs = [tf.nn.dropout(input_, keep_prob) for input_ in inputs]\r\n    \r\n      outputs, states = rnn.static_rnn(cell, inputs, initial_state=_initial_state)\r\n\r\nI had googled around with no luck, can anyone show me a way out?", "comments": ["I am getting the same error when trying to run the translate example (even when doing the small self test) which can be found here: https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate", "I met the same issue. If you are all using compiled version on master branch, I believe that we are the same issue caused by the [recent commit](https://github.com/tensorflow/tensorflow/commit/54d50ffec8df4f748694632dbe5ebde9971e2c9e). As the commit message says:\r\n\r\n> Make all RNNCells in tf.contrib.rnn act like tf.layers Layers, but with stricter semantics for no\r\nw:\r\n>\r\n>    1. Upon first use of __call__, the used scope is stored in the cell. The RNNCell tries to create weights in that scope but if some are already set, an error is raised unless the RNNCell was constructed with argument reuse=True.\r\n>\r\n>    2. A subsequent use of __call__ of the same cell instance must be in the same scope.\r\n>       If it is not, an error is raised.\r\n\r\nFrom my case, which is running the [ptb tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb), the solution is just to add a parameter named with `reuse` like this at line 112:\r\n\r\n    def lstm_cell():\r\n      return tf.contrib.rnn.BasicLSTMCell(\r\n          size, forget_bias=0.0, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\r\n\r\nThen it works.", "@ebrevdo Could you please take a look at this?", "The issue replicates for me when using the Windows/GPU build 105 on the [Shakespeare RNN Repo](https://github.com/martin-gorner/tensorflow-rnn-shakespeare).\r\n\r\nWhen running the code with the Win 1.0.0/GPU Release, there is no issue.", "That repo looks like it's targeted at tf 1.0, not intermediate releases.\n\nOn Mar 8, 2017 3:56 PM, \"Tom Wanzek\" <notifications@github.com> wrote:\n\n> The issue replicates for me when using the Windows/GPU build 105 on the Shakespeare\n> RNN Repo <https://github.com/martin-gorner/tensorflow-rnn-shakespeare>.\n>\n> When running the code with the Win 1.0.0/GPU Release, there is no issue.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-285209555>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5ansaL1KN51T4nCaqLnqw2QHN4Wks5rj0BBgaJpZM4MWl4f>\n> .\n>\n", "@tongda , I am using the Release Version of Tensorflow 1.0, working on MacOS in cpu mode. I will switch to the master branch to see if it work by adding the \"reuse\" parameter, thanks.", "doncat99: if you do, please ensure your code queries the tensorflow version\nand raises a flag if the version is lower than the master branch version.\n you may need to check against:\n\nfrom tensorflow.core import versions\nversions.GIT_VERSION\n\nOn Wed, Mar 8, 2017 at 6:58 PM, doncat99 <notifications@github.com> wrote:\n\n> @tongda <https://github.com/tongda> , I am using the Release Version of\n> Tensorflow 1.0, working on MacOS in cpu mode. I will switch to the master\n> branch to see if it work by adding the \"reuse\" parameter, thanks.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-285240438>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim66cU9e16lgD-2D0QLGcQCiHbV0zks5rj2rbgaJpZM4MWl4f>\n> .\n>\n", "@ebrevdo So what would be the suggested changes to the Shakepeare RNN to allow it to work with the intermediate stable release?\r\n\r\nHere is the key architectural section of the code, which now fails with build#105:\r\n```python\r\n#\r\n# the model (see FAQ in README.md)\r\n#\r\nlr = tf.placeholder(tf.float32, name='lr')  # learning rate\r\npkeep = tf.placeholder(tf.float32, name='pkeep')  # dropout parameter\r\nbatchsize = tf.placeholder(tf.int32, name='batchsize')\r\n\r\n# inputs\r\nX = tf.placeholder(tf.uint8, [None, None], name='X')    # [ BATCHSIZE, SEQLEN ]\r\nXo = tf.one_hot(X, ALPHASIZE, 1.0, 0.0)                 # [ BATCHSIZE, SEQLEN, ALPHASIZE ]\r\n# expected outputs = same sequence shifted by 1 since we are trying to predict the next character\r\nY_ = tf.placeholder(tf.uint8, [None, None], name='Y_')  # [ BATCHSIZE, SEQLEN ]\r\nYo_ = tf.one_hot(Y_, ALPHASIZE, 1.0, 0.0)               # [ BATCHSIZE, SEQLEN, ALPHASIZE ]\r\n# input state\r\nHin = tf.placeholder(tf.float32, [None, INTERNALSIZE*NLAYERS], name='Hin')  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\r\n\r\n# using a NLAYERS=3 layers of GRU cells, unrolled SEQLEN=30 times\r\n# dynamic_rnn infers SEQLEN from the size of the inputs Xo\r\n\r\nonecell = rnn.GRUCell(INTERNALSIZE)\r\ndropcell = rnn.DropoutWrapper(onecell, input_keep_prob=pkeep)\r\nmulticell = rnn.MultiRNNCell([dropcell for _ in range(NLAYERS)], state_is_tuple=False)\r\nmulticell = rnn.DropoutWrapper(multicell, output_keep_prob=pkeep)\r\nYr, H = tf.nn.dynamic_rnn(multicell, Xo, dtype=tf.float32, initial_state=Hin)\r\n# Yr: [ BATCHSIZE, SEQLEN, INTERNALSIZE ]\r\n# H:  [ BATCHSIZE, INTERNALSIZE*NLAYERS ] # this is the last state in the sequence\r\n```\r\nI do not seem to find any documentation regarding a `reuse` flag?\r\n\r\nThanks in advance.", "Use:\n\nmulticell = rnn.MultiRNNCell([rnn.DropoutWrapper(rnn.GRUCell(INTERNALSIZE),\ninput_keep_prob=pkeep) for _ in range(NLAYERS)], state_is_tuple=False)\n\n\nWhich creates a separate grucell object for each layer.\n\n\nOn Mar 10, 2017 7:44 AM, \"Tom Wanzek\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> So what would be the suggested\n> changes to the Shakepeare RNN to allow it to work with the intermediate\n> stable release?\n>\n> Here is the key architectural section of the code, which now fails with\n> build#105:\n>\n> ## the model (see FAQ in README.md)#\n> lr = tf.placeholder(tf.float32, name='lr')  # learning rate\n> pkeep = tf.placeholder(tf.float32, name='pkeep')  # dropout parameter\n> batchsize = tf.placeholder(tf.int32, name='batchsize')\n> # inputs\n> X = tf.placeholder(tf.uint8, [None, None], name='X')    # [ BATCHSIZE, SEQLEN ]\n> Xo = tf.one_hot(X, ALPHASIZE, 1.0, 0.0)                 # [ BATCHSIZE, SEQLEN, ALPHASIZE ]# expected outputs = same sequence shifted by 1 since we are trying to predict the next character\n> Y_ = tf.placeholder(tf.uint8, [None, None], name='Y_')  # [ BATCHSIZE, SEQLEN ]\n> Yo_ = tf.one_hot(Y_, ALPHASIZE, 1.0, 0.0)               # [ BATCHSIZE, SEQLEN, ALPHASIZE ]# input state\n> Hin = tf.placeholder(tf.float32, [None, INTERNALSIZE*NLAYERS], name='Hin')  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n> # using a NLAYERS=3 layers of GRU cells, unrolled SEQLEN=30 times# dynamic_rnn infers SEQLEN from the size of the inputs Xo\n>\n> onecell = rnn.GRUCell(INTERNALSIZE)\n> dropcell = rnn.DropoutWrapper(onecell, input_keep_prob=pkeep)\n> multicell = rnn.MultiRNNCell([dropcell for _ in range(NLAYERS)], state_is_tuple=False)\n> multicell = rnn.DropoutWrapper(multicell, output_keep_prob=pkeep)\n> Yr, H = tf.nn.dynamic_rnn(multicell, Xo, dtype=tf.float32, initial_state=Hin)# Yr: [ BATCHSIZE, SEQLEN, INTERNALSIZE ]# H:  [ BATCHSIZE, INTERNALSIZE*NLAYERS ] # this is the last state in the sequence\n>\n> I do not seem to find any documentation regarding a reuse flag?\n>\n> Thanks in advance.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-285702372>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6MOOCbx3RJEJe8PQBDXGVIXTGPmks5rkW_jgaJpZM4MWl4f>\n> .\n>\n", "I don't understand why I am getting this error with the [seq2seq tutorial model](https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py):\r\n```python\r\ncell = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\r\n```\r\n[Source](https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py#L129)\r\n\r\nwhere the cell is created with\r\n```python\r\ndef single_cell():\r\n    return tf.contrib.rnn.GRUCell(size)\r\n```\r\n\r\n", "@ebrevdo Thanks for getting back to this issue. Unfortunately, the suggested change leaves matters as they are, with the aforementioned error. Given the above comment regarding the **seq2seq tutorial**, I suspect we are all in the same boat?", "Are you sure it's the exact same error?  Please copy and paste it here.", "My bad, I just went through the change process to the relevant code again (from scratch) and re-ran it as proposed. The error has indeed been removed and the Old Bard is hallucinating just fine now \ud83d\udc4d \r\n\r\nSo, thx, not sure where I went wrong yesterday, but it was clearly on me.", "I met the same problem when using the Release Version of Tensorflow 1.0 and working on MacOS in cpu mode.Even if add the \"reuse\" parameter\r\n```\r\ndef cell():\r\n    return tf.contrib.rnn.BasicLSTMCell(rnn_size,state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\r\n\r\nmuticell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple=True)\r\n```", "your multicell looks wrong... you should be using \"cell() for _ in\nrange(...)\"\n\nOn Thu, Mar 16, 2017 at 8:29 PM, cuiming <notifications@github.com> wrote:\n\n> I met the same problem when using the Release Version of Tensorflow 1.0\n> and working on MacOS in cpu mode.Even if add the \"reuse\" parameter\n>\n> def cell():\n>     return tf.contrib.rnn.BasicLSTMCell(rnn_size,state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\n>\n> muticell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple=True)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-287257629>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3A6JQr8ptRKrdiDW_kgNRIFkHGlks5rmf4WgaJpZM4MWl4f>\n> .\n>\n", "I was trying to run the translate example: python2.7 translate.py --data_dir data/ --train_dir train/ --size=256 --num_layers=2 --steps_per_checkpoint=50\r\n\r\nIt seems the way to use MultiRNNCell is correct:\r\ncell = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\r\n\r\nBut I got the same error:\r\nValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.GRUCell object at 0x7fba0683de90> with a different variable scope than its first use.  First use of cell was with scope 'embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/multi_rnn_cell/cell_0/gru_cell', this attempt is with scope 'embedding_attention_seq2seq/rnn/multi_rnn_cell/cell_0/gru_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([GRUCell(...)] * num_layers), change to: MultiRNNCell([GRUCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)", "@bowu - did you have any luck with this? if you haven't tried it yet, reinstall tensorflow from the latest source. there were some changes to some of the core_rnn files, among a few others. works for me now.", "@robmsylvester I reinstall tensorflow from the latest source, still the same error. I was on branch master and the latest commit is `commit 2a4811054a9e6b83e1f5a2705a92aab50e151b13`. What's the latest commit when you build your repo?", "Hi, I am using Tensorflow r1.0 using GPU built using source. I am trying to follow the unmodified Seq2Seq translation tutorial, but I'm getting the same error. i.e.\r\n\r\n> ValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.GRUCell object at 0x7f0fb51ebb00> with a different variable scope than its first use.  First use of cell was with scope 'embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/multi_rnn_cell/cell_0/gru_cell', this attempt is with scope 'embedding_attention_seq2seq/rnn/multi_rnn_cell/cell_0/gru_cell'.....\r\n\r\nThe relevant portion of the code in my seq2seq_model.py is:\r\n```\r\n # Create the internal multi-layer cell for our RNN.\r\n    def single_cell():\r\n      return tf.contrib.rnn.GRUCell(size)\r\n    if use_lstm:\r\n      def single_cell():\r\n        return tf.contrib.rnn.BasicLSTMCell(size)\r\n    cell = single_cell()\r\n    if num_layers > 1:\r\n      cell = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\r\n```\r\n\r\nWhat can I do to solve the problem?\r\n\r\nadding \"reuse=tf.get_variable_scope().reuse\" to the call where the GRUCell is created doesn't help.\r\n\r\nThanks a ton!", "@prashantserai - see what happens if you remove the MultiRNNCell line from above, effectively making your network just one layer. Does it work then? It might be a bug somewhere in MultiRNNCell. I've read about that somewhere recently, probably on stack overflow.\r\n\r\nIf you implement the stacked lstm/gru yourself, you don't get this error, and you can implement the same functionality (actually more, because you're free to do whatever you want with bidirectional architectures, weird residual and skip connections, etc.)", "@robmsylvester The same error persisted even when I tried with num_layers=1 which should effectively skip that line. Any other ideas? Thanks for the input.", "Hmmm. One thing that stands out to me is in the referenced legacy seq2seq file:\r\n\r\n`encoder_cell = copy.deepcopy(cell)`\r\n\r\nThis line appears to be used because the same architecture is used on both the encoder and decoder side. They make a copy of the cell, then pass the cell argument along to the attention decoder embedding function, then to the attention decoder itself. \r\n\r\nWhat happens if you explicitly create the encoder cell AND the decoder cell in your seq2seq model file and pass both along to the legacy library file, making the small adjustments to the functions and their arguments?\r\n", "@robmsylvester  shouldn't making changes in the scopes of the cells work? It's working for the other two examples as well. In my opinion, this would be a very ugly workaround; a cleaner solution must exist; maybe we are missing something? ( I got the same error on the seq2seq tutorial as well, tried all of the above solutions).", "@iamgroot42 - Yeah, that 'solution' is admittedly very ugly, but more so just trying to hunt down where an issue might be. I'll play with it in a few hours and see if I can track something down.", "In fact, the copy.deepcopy is there because these are legacy functions and\nwe don't have the resources to maintain/update them.  If you'd like to\nintroduce a backwards-compatible change that allows the user to provide a\nsecond cell for the decoding step, and if it's None then to fallback on the\ndeepcopy, then I would be happy to review the PR.  Keep in mind it would\nhave to be a backwards compatible change.\n\nOn Tue, Apr 4, 2017 at 11:38 AM, Rob Sylvester <notifications@github.com>\nwrote:\n\n> @iamgroot42 <https://github.com/iamgroot42> - Yeah, that 'solution' is\n> admittedly very ugly, but more so just trying to hunt down where an issue\n> might be. I'll play with it in a few hours and see if I can track something\n> down.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-291593289>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1QHTDhOC_zT6cKtmUFPOit5Yjn7ks5rso5CgaJpZM4MWl4f>\n> .\n>\n", "@ebrevdo - I'll think about it. I do have a translator that works pretty similar to this one but creates cells through a separate class that allows for inserting bidirectional layers where you want, residuals where you want, merging inputs with concat vs. sum, and a few other things. I think I could migrate my class over to this tutorial pretty easily by using static RNN's. I'll let you know.", "@ebrevdo i am running Tensorflow r1.0 (tensorflow-1.0.1-cp36-cp36m-linux_x86_64) on Red Hat and have the latest version of the translation tutorial from Github.. is there a way you know to make this work currently?", "It's unfortunate that the translation tutorial does not work with TF 1.0.  We should fix that.  @lukaszkaiser can you take a look?  We're working on a new tutorial but it's still a few weeks off and will require a nightly version of TensorFlow (or TF 1.1 or 1.2) to work.", "(lukasz; it's hard for me to identify from the various comments which part of the tutorial is faulty in TF 1.0.  any chance you could identify the line and i can help get it working?)", "@ebrevdo  It's [this ](https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate) tutorial. The error is in [this](https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py#L122) cluster of lines. The cells passed here are used for both the backward and forward phase of the legacy seq2seq model, which throws an error because of same cells being used with different scopes.", "@iamgroot42 do you want to make a PR with the needed changes? That would be great, I currently don't have the cycles to do that myself. Thanks!", "I noticed that the TF 1.0  works fine with the newest version of translation tutorial if compiled from the source on branch remotes/origin/r1.0\r\n```\r\n$ git clone https://github.com/tensorflow/tensorflow\r\n$ cd tensorflow\r\n$ git checkout remotes/origin/r1.0\r\n```\r\nthen build and install TensorFlow, it works fine.\r\n\r\nOn branch remotes/origin/r1.1 it has the \"different variable scope\" error. \r\nI modified the code as @robmsylvester suggested\r\n> What happens if you explicitly create the encoder cell AND the decoder cell in your seq2seq model file and pass both along to the legacy library file, making the small adjustments to the functions and their arguments?\r\n\r\nand it works for me now.", "@oxwsds the Tensorflow I'm using is 1.0.1 so maybe that's having an error..\r\n\r\nI had tried what @robmsylvester suggested then actually.. and the training had begun (2 days 13 hours done now).. it fails during decoding though with the error:\r\n```\r\n  File \"/homes/3/serai/.conda/envs/tensorflow_r1.0_gpu/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 883, in embedding_attention_seq2seq\r\n    initial_state_attention=initial_state_attention)\r\n  File \"/homes/3/serai/.conda/envs/tensorflow_r1.0_gpu/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 787, in embedding_attention_decoder\r\n    initial_state_attention=initial_state_attention)\r\n  File \"/homes/3/serai/.conda/envs/tensorflow_r1.0_gpu/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 686, in attention_decoder\r\n    cell_output, state = cell(x, state)\r\n  File \"/homes/3/serai/.conda/envs/tensorflow_r1.0_gpu/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 796, in __call__\r\n    % (len(self.state_size), state))\r\nValueError: Expected state to be a tuple of length 3, but received: Tensor(\"model_with_buckets/embedding_attention_seq2seq/rnn/gru_cell_4/add:0\", shape=(?, 1024), dtype=float32)\r\n\r\n```\r\nDid you try decoding?", "@prashantserai Don't exactly know, but what you met seems to be another issue. ", "@prashantserai If it fails only when you decode, perhaps it has something to do with using a batch size of one? Does the model still train if you lower the batch size to one during training?", "@bowu Same error here. Mac OX Sierra, TensorFlow 1.1.0-rc1, Python 2.7.10 & Python 3.6.1.", "@robmsylvester it did train successfully with a batch size of one too, but failed during decoding in the same way or similar way.. here's a full traceback.. the reason I was thinking of this as a connected error was because of the reference to seq2seq_f (which was one of the modified functions) (the #prashant comment from my code to signify a modified line is part of the trace)\r\n\r\n```\r\n2017-04-10 11:32:27.447042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: GeForce GTX 780 Ti\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928\r\npciBusID 0000:42:00.0\r\nTotal memory: 2.95GiB\r\nFree memory: 2.88GiB\r\n2017-04-10 11:32:27.447094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \r\n2017-04-10 11:32:27.447102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \r\n2017-04-10 11:32:27.447118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 780 Ti, pci bus id: 0000:42:00.0)\r\nTraceback (most recent call last):\r\n  File \"translate.py\", line 322, in <module>\r\n    tf.app.run()\r\n  File \"/homes/3/serai/.conda/envs/tensorflow_r1.0_gpu/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"translate.py\", line 317, in main\r\n    decode()\r\n  File \"translate.py\", line 248, in decode\r\n    model = create_model(sess, True)\r\n  File \"translate.py\", line 136, in create_model\r\n    dtype=dtype)\r\n  File \"/data/data6/scratch/serai/models/tutorials/rnn/translate/seq2seq_model.py\", line 168, in __init__\r\n    softmax_loss_function=softmax_loss_function)\r\n  File \"/homes/3/serai/.conda/envs/tensorflow_r1.0_gpu/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 1203, in model_with_buckets\r\n    decoder_inputs[:bucket[1]])\r\n  File \"/data/data6/scratch/serai/models/tutorials/rnn/translate/seq2seq_model.py\", line 167, in <lambda>\r\n    self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\r\n  File \"/data/data6/scratch/serai/models/tutorials/rnn/translate/seq2seq_model.py\", line 144, in seq2seq_f\r\n    dtype=dtype) #prashant\r\n  File \"/homes/3/serai/.conda/envs/tensorflow_r1.0_gpu/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 883, in embedding_attention_seq2seq\r\n    initial_state_attention=initial_state_attention)\r\n  File \"/homes/3/serai/.conda/envs/tensorflow_r1.0_gpu/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 787, in embedding_attention_decoder\r\n    initial_state_attention=initial_state_attention)\r\n  File \"/homes/3/serai/.conda/envs/tensorflow_r1.0_gpu/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 686, in attention_decoder\r\n    cell_output, state = cell(x, state)\r\n  File \"/homes/3/serai/.conda/envs/tensorflow_r1.0_gpu/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 796, in __call__\r\n    % (len(self.state_size), state))\r\nValueError: Expected state to be a tuple of length 3, but received: Tensor(\"model_with_buckets/embedding_attention_seq2seq/rnn/gru_cell_4/add:0\", shape=(?, 1024), dtype=float32)\r\n```\r\n\r\n@oxwsds does your opinion change on the basis of the full trace above?", "@prashantserai I tried decoding and it works fine. I just simply add a `encoder_cell` arg to function `tf.contrib.legacy_seq2seq.embedding_attention_seq2seq` and in `translate/seq2seq_model.py` create the cell and pass it to the function, which were called in function `seq2seq_f`. How did you change your code?", "@oxwsds @robmsylvester @ebrevdo \r\nI finally have something that's working now (I mean, results for my single layer 256 unit network are kind of appalling, but that's probably just because the network is ultra light weight and I didn't tune params AT ALL)\r\nThank you so much everyone...!!!!!\r\n\r\n_Here's my thoughts at the end of this:_\r\n\r\n@oxwsds comment **that the tutorial (in it's current form) works without any need for modification when Tensorflow is compiled from the branch remotes/origin/r1.0 was TRUE**. Although, the sad bit was that the version of Tensorflow I had for which modifications within Tensorflow code were needed, and the version in remotes/origin/r1.0 were both identically labelled.\r\n\r\n@robmsylvester 's fix in the comment (copied below) DID WORK for my version of Tensorflow where the Tutorial didn't work out of the box (and should work for TF 1.1 too I guess). It is slightly messy to implement, but I could do it, which is saying something :-P\r\nThe error in my last two comments before this was due to my mistake. Like a dummy, I was specifying the layers and hidden units parameters only during training, I was leaving the code to use defaults during decoding. **(this portion of the tutorial could be slightly more dummy proof: https://www.tensorflow.org/tutorials/seq2seq#lets_run_it )**\r\n\r\n> Hmmm. One thing that stands out to me is in the referenced legacy seq2seq file:\r\n> \r\n> encoder_cell = copy.deepcopy(cell)\r\n> \r\n> This line appears to be used because the same architecture is used on both the encoder and decoder side. They make a copy of the cell, then pass the cell argument along to the attention decoder embedding function, then to the attention decoder itself.\r\n> \r\n> What happens if you explicitly create the encoder cell AND the decoder cell in your seq2seq model file and pass both along to the legacy library file, making the small adjustments to the functions and their arguments?", "Thanks for the feedback!  Seems there's something different between the TF\non pypi and at that tag? Gunhan, is that possible?\n\nOn Mon, Apr 10, 2017 at 9:05 PM, prashantserai <notifications@github.com>\nwrote:\n\n> @oxwsds <https://github.com/oxwsds> @robmsylvester\n> <https://github.com/robmsylvester> @ebrevdo <https://github.com/ebrevdo>\n> I finally have something that's working now (I mean, results for my single\n> layer 256 unit network are kind of appalling, but that's probably just\n> because the network is ultra light weight and I didn't tune params AT ALL)\n>\n> Here's my bottomline:\n>\n> @oxwsds <https://github.com/oxwsds> comment *that the tutorial (in it's\n> current form) works without any need for modification when Tensorflow is\n> compiled from the branch remotes/origin/r1.0 was TRUE*. The sad bit\n> although being that the version of Tensorflow I had for which modifications\n> within Tensorflow code were needed, and the version in remotes/origin/r1.0\n> were both identically labelled.\n>\n> @robmsylvester <https://github.com/robmsylvester> 's fix in the comment\n> (copied below) DID WORK for my version of Tensorflow where the Tutorial\n> didn't work out of the box (and should work for TF 1.1 too I guess). It is\n> slightly messy to implement, but I could do it, which is saying something\n> :-P\n> The error in my last two comments before this was due to my mistake. Like\n> a dummy, I was specifying the layers and hidden units parameters only\n> during training, I was leaving the code to use defaults during decoding. *(this\n> portion of the tutorial is could be slightly more dummy proof:\n> https://www.tensorflow.org/tutorials/seq2seq#lets_run_it\n> <https://www.tensorflow.org/tutorials/seq2seq#lets_run_it> )*\n>\n> Hmmm. One thing that stands out to me is in the referenced legacy seq2seq\n> file:\n>\n> encoder_cell = copy.deepcopy(cell)\n>\n> This line appears to be used because the same architecture is used on both\n> the encoder and decoder side. They make a copy of the cell, then pass the\n> cell argument along to the attention decoder embedding function, then to\n> the attention decoder itself.\n>\n> What happens if you explicitly create the encoder cell AND the decoder\n> cell in your seq2seq model file and pass both along to the legacy library\n> file, making the small adjustments to the functions and their arguments?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-293143828>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxvcfFnbWbpj7aUs3BUjwGEFj6p5ks5ruvvygaJpZM4MWl4f>\n> .\n>\n", "For information I had this issue while trying to stack LSTM cells:\r\nMy orginial code was:\r\n```\r\n    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0, state_is_tuple=True)\r\n    if is_training and keep_prob < 1:\r\n      lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\r\n          lstm_cell, output_keep_prob=keep_prob)\r\n    cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_layers, state_is_tuple=True)\r\n```\r\n\r\nThen, with the following code, creating the model was ok, but I couldn't share the variable with another model. (for instance if you create a train_model and a valid_model supposed to share tensors, it will fail)\r\n```\r\n    lstm_creator = lambda: tf.contrib.rnn.BasicLSTMCell(\r\n                                        hidden_size, \r\n                                        forget_bias=0.0, state_is_tuple=True)\r\n    if is_training and keep_prob < 1:\r\n      cell_creator = lambda:tf.contrib.rnn.DropoutWrapper(\r\n          lstm_creator(), output_keep_prob=keep_prob)\r\n    else:\r\n      cell_creator = lstm_creator\r\n\r\n    cell = tf.contrib.rnn.MultiRNNCell([cell_creator() for _ in range(num_layers)], state_is_tuple=True)\r\n\r\n```\r\n\r\nSo finally I used `lstm_creator` to be the function like `lstm_cell` in [tensorflow/models/tutorials/rnn/ptb/ptb_word_lm.py#L112](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L112). I now have:\r\n```\r\ndef lstm_cell():\r\n      # With the latest TensorFlow source code (as of Mar 27, 2017),\r\n      # the BasicLSTMCell will need a reuse parameter which is unfortunately not\r\n      # defined in TensorFlow 1.0. To maintain backwards compatibility, we add\r\n      # an argument check here:\r\n      if 'reuse' in inspect.getargspec(\r\n          tf.contrib.rnn.BasicLSTMCell.__init__).args:\r\n        return tf.contrib.rnn.BasicLSTMCell(\r\n            size, forget_bias=0.0, state_is_tuple=True,\r\n            reuse=tf.get_variable_scope().reuse)\r\n      else:\r\n        return tf.contrib.rnn.BasicLSTMCell(\r\n            size, forget_bias=0.0, state_is_tuple=True)\r\n    attn_cell = lstm_cell\r\n    \r\n    lstm_creator = lstm_cell\r\n    if is_training and keep_prob < 1:\r\n      cell_creator = lambda:tf.contrib.rnn.DropoutWrapper(\r\n          lstm_creator(), output_keep_prob=keep_prob)\r\n    else:\r\n      cell_creator = lstm_creator\r\n\r\n    cell = tf.contrib.rnn.MultiRNNCell([cell_creator() for _ in range(num_layers)], state_is_tuple=True)\r\n```\r\n\r\nIt is now fully working", "trying to get this thing running, which results in the same error: \r\n\r\nhttps://gist.github.com/danijar/c7ec9a30052127c7a1ad169eeb83f159#file-blog_tensorflow_sequence_classification-py-L38\r\n\r\n\r\n@pltrdy 's solution didn't do it for me oddly. I'm getting\r\n\r\n```\r\nValueError: Variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n```\r\n\r\n\r\n", "@aep did you use the function of https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L112 I mention at the end of my post (now edited to be more clear)", "    cells=[]\r\n    for _ in range(15):\r\n        cell = create_lstm_cell(config)\r\n        cells.append(cell)\r\n    lsmt_layers = rnn.MultiRNNCell(cells)\r\nit solved my problem", "Managed to fix this issue by installing older version of Tensorflow:\r\n`pip install -Iv tensorflow==1.0`\r\n\r\nI was receiving the error when executing the seq2seq tutorial\r\n", "In regards to what @ebrevdo said, I think the solution is not to fix the legacy seq2seq code, but to update the tutorial to use the `contrib.seq2seq` package instead, which is actively maintained.  It is quite demoralizing when the first tensorflow program you ever run spits out a bunch of errors.  If I have some time this week, I'll submit a PR.\r\n", "We're working on a new seq2seq tutorial. We had hoped to release by end of\nlast month but are getting delayed.  It will use the new API.\n\nOn May 1, 2017 8:07 AM, \"Kyle Teague\" <notifications@github.com> wrote:\n\n> In regards to what @ebrevdo <https://github.com/ebrevdo> said, I think\n> the solution is not to fix the legacy seq2seq code, but to update the\n> tutorial to use the contrib.seq2seq package instead, which is actively\n> maintained. It is quite demoralizing when the first tensorflow program you\n> ever run spits out a bunch of errors. If I have some time this week, I'll\n> submit a PR.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-298350307>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim587xZx9Gi4-yXmwccSum8_Trc1oks5r1fUogaJpZM4MWl4f>\n> .\n>\n", "@ebrevdo I meet the same error when running the sequence_to_sequence model on the tensorflow1.1 website. And I have try to use 'reuse' parameter but failed. Could you tell me when the new seq2seq tutorial will be released?", "Looks like at the same time as tf 1.2, since we will rely on some new\nfeatures of that release.\n\nOn May 4, 2017 9:16 PM, \"njuzrs\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> I meet the same error when running\n> the sequence_to_sequence model on the tensorflow1.1 website. And I have try\n> to use 'reuse' parameter but failed. Could you tell me when the new seq2seq\n> tutorial will be released?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-299366774>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8_kFTM7-SsXQAA-Ar0dfhHMGT0Zks5r2qKngaJpZM4MWl4f>\n> .\n>\n", "@ebrevdo I am as well facing the same issue and unable to progress with seq2seq. It will be really helpful if you could let us/me know what is a probable date for a new tutorial.\r\nThanks a lot for your help.\r\n\r\n", "Installing using `pip install tensorflow==1.0` (Tensorflow 1.0) is working for me (translate tutorial).", "I have version 1.1.0-rc2.", "TF1.2 will solve this problem? Please help me how to continue training the model. TF 1.0 works but doesn't have devicewrapper api for multiple GPUs.", "Having the same problem with tensor flow 1.1. Still working on a solution ", "I tried several things, at the end I was able to use tensorflow 1.1 but had to make these changes: (based on Tshzzz above)\r\n\r\nRemove this:\r\n`multicell = rnn.MultiRNNCell([dropcell]*NLAYERS, state_is_tuple=False)`\r\n\r\nAnd add this:\r\ncells=[]\r\nfor _ in range(NLAYERS):\r\n    cell = rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(INTERNALSIZE), input_keep_prob=pkeep)\r\n    cells.append(cell)\r\nmulticell = rnn.MultiRNNCell(cells, state_is_tuple=False)", "@ebrevdo Congratulations, TF 1.2 just got released - was the new tutorial also released somewhere or is it being released anytime soon?\r\n\r\nThanks", "We'll plan to have an announcement when it's released. Working on it.\n\nOn May 19, 2017 7:02 PM, \"prashantserai\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Congratulations, TF 1.2 just got\n> released - was the new tutorial also released somewhere or is it being\n> released anytime soon?\n>\n> Thanks\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-302844002>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0RWDzNCXk-bIjKSyHLvgFxUvq2lks5r7km7gaJpZM4MWl4f>\n> .\n>\n", "For anyone using tensorflow-gpu==1.1.0  and getting this error, switching to 1.0.0 via pip install tensorflow-gpu==1.0.0 is not going to fix the problem, at least didn't work for me.\r\n\r\nI ran into this issue on both mac and ubuntu and compiling from source worked both times. So:\r\npip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp34-cp34m-linux_x86_64.whl", "@ajaanbaahu Still waiting for tf1.2 new seq2seq tutorial.", "It worked for me using `pip install tensorflow==1.0`.", "For tf r1.2, got deepcopy error. As listed in [sequence to sequence model error #1050](https://github.com/tensorflow/models/issues/1050)", "As the rookie, I raise some of my opinion.\r\nThe following code will make this similar mistake occure:\r\n(Piece of my code)\r\n```python\r\nlstm_cell = self.LSTMCell(self.num_hidden)\r\nlstm_entity = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=0.5)\r\nlayer = tf.contrib.rnn.MultiRNNCell([lstm_entity] * self.num_layer)\r\n__, _ = tf.nn.dynamic_rnn(layer, self.data, dtype=tf.float64)\r\n```\r\nThe error dump as the following:\r\n```\r\nTraceback (most recent call last):\r\n  File \"IntentNet.py\", line 71, in <module>\r\n    net = Net(data, target, 5, 1)\r\n  File \"IntentNet.py\", line 45, in __init__\r\n    __, _ = tf.nn.dynamic_rnn(layer, self.data, dtype=tf.float64)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 553, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 720, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 705, in _time_step\r\n    (output, new_state) = call_cell()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 691, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 953, in __call__\r\n    cur_inp, new_state = cell(cur_inp, cur_state)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 713, in __call__\r\n    output, new_state = self._cell(inputs, state, scope)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 235, in __call__\r\n    with _checked_scope(self, scope or \"basic_lstm_cell\", reuse=self._reuse):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 77, in _checked_scope\r\n    type(cell).__name__))\r\nValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x7fe4fc7bd150> with a different variable scope than its first use.  First use of cell was with scope 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell', this attempt is with scope 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)\r\n```\r\n\r\nBut after I do the revision, It can work.\r\n```python\r\n\"\"\"\r\nlstm_cell = self.LSTMCell(self.num_hidden)\r\nlstm_entity = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=0.5)\r\nlayer = tf.contrib.rnn.MultiRNNCell([lstm_entity] * self.num_layer)\r\n\"\"\"\r\nlayer = []\r\nfor i in range(self.num_layer):\r\n    lstm_cell = self.LSTMCell(self.num_hidden)\r\n    lstm_entity = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=0.5)\r\n    layer.append(lstm_entity)\r\nlayer = tf.contrib.rnn.MultiRNNCell(layer)\r\n__, _ = tf.nn.dynamic_rnn(layer, self.data, dtype=tf.float64)\r\n```", "None of those workarounds worked for me with Tensorflow 1.1\r\n\r\nI'm using `seq2seq` model with `MultiRNNCell` cells.\r\n\r\nI had to reverse back to 1.0.1: `pip3 install tensorflow==1.0`", "Anyone have these issues when working with legacy_seq2seq.rnn_decoder()? ", "@oxwsds As you said, I change input args cell of tf.contrib.legacy_seq2seq.embedding_attention_seq2seq to two different cell {encoder_cells, decoder_cells}. Finally, I get seq2seq model worked.  After 73200 setps, I get perplexity 5.54.\r\nThen I run decode part, \r\n >> Who is the president of the United States?\r\nQui est le pr\u00e9sident des \u00c9tats-Unis ?\r\n\r\nProblem solved.  Thanks.", "@doncat99 \r\nIt seems that `copy.deepcopy(cell)`  in `seq2seq.py`  doesn't make effect.\r\nSo I change the related part in `seq2seq_model.py` to \r\n\r\n```\r\nif num_layers > 1:\r\n      cell_enc = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\r\n      cell_dec = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\r\n\r\n    # The seq2seq function: we use embedding for the input and attention.\r\n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\r\n      return seq2seq.embedding_attention_seq2seq(\r\n          encoder_inputs,\r\n          decoder_inputs,\r\n          cell_enc,\r\n          cell_dec,\r\n          num_encoder_symbols=source_vocab_size,\r\n          num_decoder_symbols=target_vocab_size,\r\n          embedding_size=size,\r\n          output_projection=output_projection,\r\n          feed_previous=do_decode,\r\n          dtype=dtype)\r\n```", "@supermeatboy82 , Could you share your code?\r\n\r\n", "Upgrading to Tensorflow 1.2.0 and generating the cells in a loop instead of list multiplication fixed this for me.", "Got the error with TF1.2 when running translate.py, details:\r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.582\r\npciBusID 0000:02:00.0\r\nTotal memory: 10.91GiB\r\nFree memory: 10.76GiB\r\n2017-06-22 09:15:04.485252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-06-22 09:15:04.485256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-06-22 09:15:04.485265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0)\r\nCreating 3 layers of 1024 units.\r\nTraceback (most recent call last):\r\n  File \"translate.py\", line 322, in <module>\r\n    tf.app.run()\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"translate.py\", line 319, in main\r\n    train()\r\n  File \"translate.py\", line 178, in train\r\n    model = create_model(sess, False)\r\n  File \"translate.py\", line 136, in create_model\r\n    dtype=dtype)\r\n  File \"/data/research/github/dl/tensorflow/tensorflow/models/tutorials/rnn/translate/seq2seq_model.py\", line 179, in __init__\r\n    softmax_loss_function=softmax_loss_function)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 1206, in model_with_buckets\r\n    decoder_inputs[:bucket[1]])\r\n  File \"/data/research/github/dl/tensorflow/tensorflow/models/tutorials/rnn/translate/seq2seq_model.py\", line 178, in <lambda>\r\n    lambda x, y: seq2seq_f(x, y, False),\r\n  File \"/data/research/github/dl/tensorflow/tensorflow/models/tutorials/rnn/translate/seq2seq_model.py\", line 142, in seq2seq_f\r\n    dtype=dtype)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 848, in embedding_attention_seq2seq\r\n    encoder_cell = copy.deepcopy(cell)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 174, in deepcopy\r\n    y = copier(memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 476, in __deepcopy__\r\n    setattr(result, k, copy.deepcopy(v, memo))\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 230, in _deepcopy_list\r\n    y.append(deepcopy(a, memo))\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 230, in _deepcopy_list\r\n    y.append(deepcopy(a, memo))\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 230, in _deepcopy_list\r\n    y.append(deepcopy(a, memo))\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 237, in _deepcopy_tuple\r\n    y.append(deepcopy(a, memo))\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/lscm/opt/anaconda2/lib/python2.7/copy.py\", line 343, in _reconstruct\r\n    y.__dict__.update(state)\r\nAttributeError: 'NoneType' object has no attribute 'update'\r\n", "I also met the error caused by `copy.deepcopy(cell)` in `embedding_attention_seq2seq()` when running `self_test()` in the translate model in tutorial. \r\nI tried to change the codes in `seq2seq_f()` in `Seq2SeqModel` as follows:\r\n\r\n```\r\n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode=False):\r\n        tmp_cell = copy.deepcopy(cell) #new\r\n        return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\r\n            encoder_inputs,\r\n            decoder_inputs,\r\n            tmp_cell, #new\r\n            num_encoder_symbols=source_vocab_size,\r\n            num_decoder_symbols=target_vocab_size,\r\n            embedding_size=size,\r\n            output_projection=output_projection,\r\n            feed_previous=do_decode,\r\n            dtype=dtype)\r\n```\r\nThen there is no error now.\r\nBUT as a rookie I don't know whether the codes here work as before and it seems the changes make the model run slower.\r\n", "I would like to update everyone that I downgraded the tensorflow to 1.0.0 (tensorflow-GPU) and it is working for me. The models are performing as expected. I assume that the CPU version of 1.0.0 should function as expected? Or?.\r\nThanks :)\r\n", "Hi guys, I don't know if you're still interested on it, but I found that the problem is related to the operation of copying the cell passed as params to the `embedding_attention_seq2seq` function. This is because the same cell definition is used both for encoder and decoder. I think the tutorial is deprecated since it uses a seq2seq model with bucketing in contrast to a dynamic seq2seq. But, I'm pasting a modified function that works. The function is updated in the file `tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py`.\r\n\r\nthanks,\r\nFabio\r\n\r\n```!python\r\ndef embedding_attention_seq2seq(encoder_inputs,\r\n                                decoder_inputs,\r\n                                enc_cell,\r\n                                dec_cell,\r\n                                num_encoder_symbols,\r\n                                num_decoder_symbols,\r\n                                embedding_size,\r\n                                num_heads=1,\r\n                                output_projection=None,\r\n                                feed_previous=False,\r\n                                dtype=None,\r\n                                scope=None,\r\n                                initial_state_attention=False):\r\n  \"\"\"Embedding sequence-to-sequence model with attention.\r\n\r\n  This model first embeds encoder_inputs by a newly created embedding (of shape\r\n  [num_encoder_symbols x input_size]). Then it runs an RNN to encode\r\n  embedded encoder_inputs into a state vector. It keeps the outputs of this\r\n  RNN at every step to use for attention later. Next, it embeds decoder_inputs\r\n  by another newly created embedding (of shape [num_decoder_symbols x\r\n  input_size]). Then it runs attention decoder, initialized with the last\r\n  encoder state, on embedded decoder_inputs and attending to encoder outputs.\r\n\r\n  Warning: when output_projection is None, the size of the attention vectors\r\n  and variables will be made proportional to num_decoder_symbols, can be large.\r\n\r\n  Args:\r\n    encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\r\n    decoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\r\n    cell: tf.nn.rnn_cell.RNNCell defining the cell function and size.\r\n    num_encoder_symbols: Integer; number of symbols on the encoder side.\r\n    num_decoder_symbols: Integer; number of symbols on the decoder side.\r\n    embedding_size: Integer, the length of the embedding vector for each symbol.\r\n    num_heads: Number of attention heads that read from attention_states.\r\n    output_projection: None or a pair (W, B) of output projection weights and\r\n      biases; W has shape [output_size x num_decoder_symbols] and B has\r\n      shape [num_decoder_symbols]; if provided and feed_previous=True, each\r\n      fed previous output will first be multiplied by W and added B.\r\n    feed_previous: Boolean or scalar Boolean Tensor; if True, only the first\r\n      of decoder_inputs will be used (the \"GO\" symbol), and all other decoder\r\n      inputs will be taken from previous outputs (as in embedding_rnn_decoder).\r\n      If False, decoder_inputs are used as given (the standard decoder case).\r\n    dtype: The dtype of the initial RNN state (default: tf.float32).\r\n    scope: VariableScope for the created subgraph; defaults to\r\n      \"embedding_attention_seq2seq\".\r\n    initial_state_attention: If False (default), initial attentions are zero.\r\n      If True, initialize the attentions from the initial state and attention\r\n      states.\r\n\r\n  Returns:\r\n    A tuple of the form (outputs, state), where:\r\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\r\n        shape [batch_size x num_decoder_symbols] containing the generated\r\n        outputs.\r\n      state: The state of each decoder cell at the final time-step.\r\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\r\n  \"\"\"\r\n  with variable_scope.variable_scope(\r\n      scope or \"embedding_attention_seq2seq\", dtype=dtype) as scope:\r\n    dtype = scope.dtype\r\n    # Encoder.\r\n\r\n    encoder_cell = enc_cell\r\n\r\n    encoder_cell = core_rnn_cell.EmbeddingWrapper(\r\n        encoder_cell,\r\n        embedding_classes=num_encoder_symbols,\r\n        embedding_size=embedding_size)\r\n    encoder_outputs, encoder_state = rnn.static_rnn(\r\n        encoder_cell, encoder_inputs, dtype=dtype)\r\n\r\n    # First calculate a concatenation of encoder outputs to put attention on.\r\n    top_states = [\r\n        array_ops.reshape(e, [-1, 1, encoder_cell.output_size]) for e in encoder_outputs\r\n    ]\r\n    attention_states = array_ops.concat(top_states, 1)\r\n\r\n    # Decoder.\r\n    output_size = None\r\n    if output_projection is None:\r\n      dec_cell = core_rnn_cell.OutputProjectionWrapper(dec_cell, num_decoder_symbols)\r\n      output_size = num_decoder_symbols\r\n\r\n    if isinstance(feed_previous, bool):\r\n      return embedding_attention_decoder(\r\n          decoder_inputs,\r\n          encoder_state,\r\n          attention_states,\r\n          dec_cell,\r\n          num_decoder_symbols,\r\n          embedding_size,\r\n          num_heads=num_heads,\r\n          output_size=output_size,\r\n          output_projection=output_projection,\r\n          feed_previous=feed_previous,\r\n          initial_state_attention=initial_state_attention)\r\n\r\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\r\n    def decoder(feed_previous_bool):\r\n      reuse = None if feed_previous_bool else True\r\n      with variable_scope.variable_scope(\r\n          variable_scope.get_variable_scope(), reuse=reuse):\r\n        outputs, state = embedding_attention_decoder(\r\n            decoder_inputs,\r\n            encoder_state,\r\n            attention_states,\r\n            dec_cell,\r\n            num_decoder_symbols,\r\n            embedding_size,\r\n            num_heads=num_heads,\r\n            output_size=output_size,\r\n            output_projection=output_projection,\r\n            feed_previous=feed_previous_bool,\r\n            update_embedding_for_previous=False,\r\n            initial_state_attention=initial_state_attention)\r\n        state_list = [state]\r\n        if nest.is_sequence(state):\r\n          state_list = nest.flatten(state)\r\n        return outputs + state_list\r\n\r\n    outputs_and_state = control_flow_ops.cond(feed_previous,\r\n                                              lambda: decoder(True),\r\n                                              lambda: decoder(False))\r\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\r\n    state_list = outputs_and_state[outputs_len:]\r\n    state = state_list[0]\r\n    if nest.is_sequence(encoder_state):\r\n      state = nest.pack_sequence_as(\r\n          structure=encoder_state, flat_sequence=state_list)\r\n    return outputs_and_state[:outputs_len], state\r\n```", "@fabiofumarola Thank you for the function. Looks really helpful. I also saw that the tutorial is deprecated. I am still waiting for an official tutorial release. Looks like you have used the new api. Do you have any code that can be looked up to start coding on the new api?\r\nAny help is well appreciated. Thank you once again :)", "@syw2014 Did you fix your issue?", "@w268wang not yet, still waiting for other solutions, but  comments  of @Miopas may  have a try, and I am trying the solution of @fabiofumarola ", "it says `TypeError: embedding_attention_seq2seq() missing 1 required positional argument: 'dec_cell' `\r\nafter using the update that @fabiofumarola posted. Can you guys please help me?\r\n", "Yes because the update I have proposed require you to change the\nembedding_attention_seq2seq Function. If you go to the source file in you\ntensorflow release you can change the method definition you re self.\n\nOn Sun, 2 Jul 2017 at 18:15, sachinh35 <notifications@github.com> trote\n\n> it says TypeError: embedding_attention_seq2seq() missing 1 required\n> positional argument: 'dec_cell'\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-312500996>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABepUEc3W8m5CVDQGnCLu4dcJVFwwLDZks5sJ8IOgaJpZM4MWl4f>\n> .\n>\n-- \nSent from Gmail Mobile\n", "Yes i did the same thing. I changed the function in seq2seq.py file in the tensorflow release. Still i am getting the same error. Is there one more argument to the function?", "Yes, now in you code you need to specify to rnn_cells. One for the encoder\nand another for the decoder.\n\nOn Sun, 2 Jul 2017 at 20:54, fabio fumarola <fabiofumarola@gmail.com> wrote:\n\n> Yes\n>\n> On Sun, 2 Jul 2017 at 18:50, sachinh35 <notifications@github.com> wrote:\n>\n>> Yes i did the same thing. I changed the function in seq2seq.py file in\n>> the tensorflow release. Still i am getting the same error. Is there one\n>> more argument to the function?\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-312503106>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABepUOXTQC_mzLuhcwW0iZRVkLmmr8yIks5sJ8pugaJpZM4MWl4f>\n>> .\n>>\n> --\nSent from Gmail Mobile\n", "I am totally new to this. Maybe this a pretty basic question but could you tell what argument to be passed as the decoder cell in this code? I am trying to develop the seq2seq as shown in the tensorflow tutorial using own dataset. \r\n\r\n`\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport random\r\n\r\nimport numpy as np\r\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\r\nimport tensorflow as tf\r\n\r\nimport data_utils\r\n\r\n\r\nclass Seq2SeqModel(object):\r\n  def __init__(self,\r\n               source_vocab_size,\r\n               target_vocab_size,\r\n               buckets,\r\n               size,\r\n               num_layers,\r\n               max_gradient_norm,\r\n               batch_size,\r\n               learning_rate,\r\n               learning_rate_decay_factor,\r\n               use_lstm=False,\r\n               num_samples=512,\r\n               forward_only=False,\r\n               dtype=tf.float32):\r\n   \r\n    self.source_vocab_size = source_vocab_size\r\n    self.target_vocab_size = target_vocab_size\r\n    self.buckets = buckets\r\n    self.batch_size = batch_size\r\n    self.learning_rate = tf.Variable(\r\n        float(learning_rate), trainable=False, dtype=dtype)\r\n    self.learning_rate_decay_op = self.learning_rate.assign(\r\n        self.learning_rate * learning_rate_decay_factor)\r\n    self.global_step = tf.Variable(0, trainable=False)\r\n\r\n    \r\n    output_projection = None\r\n    softmax_loss_function = None\r\n    \r\n    if num_samples > 0 and num_samples < self.target_vocab_size:\r\n      w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\r\n      w = tf.transpose(w_t)\r\n      b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\r\n      output_projection = (w, b)\r\n\r\n      def sampled_loss(labels, inputs):\r\n        labels = tf.reshape(labels, [-1, 1])\r\n        \r\n        local_w_t = tf.cast(w_t, tf.float32)\r\n        local_b = tf.cast(b, tf.float32)\r\n        local_inputs = tf.cast(inputs, tf.float32)\r\n        return tf.cast(\r\n            tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\r\n                                       num_samples, self.target_vocab_size),\r\n            dtype)\r\n      softmax_loss_function = sampled_loss\r\n\r\n    \r\n    def single_cell():\r\n      return tf.nn.rnn_cell.GRUCell(size)\r\n    if use_lstm:\r\n      def single_cell():\r\n        return tf.nn.rnn_cell.BasicLSTMCell(size)\r\n    cell = single_cell()\r\n    if num_layers > 1:\r\n      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(num_layers)])\r\n\r\n   \r\n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\r\n      return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\r\n          encoder_inputs,\r\n          decoder_inputs,\r\n          cell,\r\n          num_encoder_symbols=source_vocab_size,\r\n          num_decoder_symbols=target_vocab_size,\r\n          embedding_size=size,\r\n          output_projection=output_projection,\r\n          feed_previous=do_decode,\r\n          dtype=dtype)\r\n\r\n    \r\n    self.encoder_inputs = []\r\n    self.decoder_inputs = []\r\n    self.target_weights = []\r\n    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\r\n      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\r\n                                                name=\"encoder{0}\".format(i)))\r\n    for i in xrange(buckets[-1][1] + 1):\r\n      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\r\n                                                name=\"decoder{0}\".format(i)))\r\n      self.target_weights.append(tf.placeholder(dtype, shape=[None],\r\n                                                name=\"weight{0}\".format(i)))\r\n\r\n    # Our targets are decoder inputs shifted by one.\r\n    targets = [self.decoder_inputs[i + 1]\r\n               for i in xrange(len(self.decoder_inputs) - 1)]\r\n\r\n    # Training outputs and losses.\r\n    if forward_only:\r\n      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\r\n          self.encoder_inputs, self.decoder_inputs, targets,\r\n          self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\r\n          softmax_loss_function=softmax_loss_function)\r\n      # If we use output projection, we need to project outputs for decoding.\r\n      if output_projection is not None:\r\n        for b in xrange(len(buckets)):\r\n          self.outputs[b] = [\r\n              tf.matmul(output, output_projection[0]) + output_projection[1]\r\n              for output in self.outputs[b]\r\n          ]\r\n    else:\r\n      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\r\n          self.encoder_inputs, self.decoder_inputs, targets,\r\n          self.target_weights, buckets,\r\n          lambda x, y: seq2seq_f(x, y, False),\r\n          softmax_loss_function=softmax_loss_function)\r\n\r\n    # Gradients and SGD update operation for training the model.\r\n    params = tf.trainable_variables()\r\n    if not forward_only:\r\n      self.gradient_norms = []\r\n      self.updates = []\r\n      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\r\n      for b in xrange(len(buckets)):\r\n        gradients = tf.gradients(self.losses[b], params)\r\n        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\r\n                                                         max_gradient_norm)\r\n        self.gradient_norms.append(norm)\r\n        self.updates.append(opt.apply_gradients(\r\n            zip(clipped_gradients, params), global_step=self.global_step))\r\n\r\n    self.saver = tf.train.Saver(tf.global_variables())\r\n\r\n  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\r\n           bucket_id, forward_only):\r\n   \r\n    # Check if the sizes match.\r\n    encoder_size, decoder_size = self.buckets[bucket_id]\r\n    if len(encoder_inputs) != encoder_size:\r\n      raise ValueError(\"Encoder length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(encoder_inputs), encoder_size))\r\n    if len(decoder_inputs) != decoder_size:\r\n      raise ValueError(\"Decoder length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\r\n    if len(target_weights) != decoder_size:\r\n      raise ValueError(\"Weights length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(target_weights), decoder_size))\r\n\r\n    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\r\n    input_feed = {}\r\n    for l in xrange(encoder_size):\r\n      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\r\n    for l in xrange(decoder_size):\r\n      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\r\n      input_feed[self.target_weights[l].name] = target_weights[l]\r\n\r\n    # Since our targets are decoder inputs shifted by one, we need one more.\r\n    last_target = self.decoder_inputs[decoder_size].name\r\n    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\r\n\r\n    # Output feed: depends on whether we do a backward step or not.\r\n    if not forward_only:\r\n      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\r\n                     self.gradient_norms[bucket_id],  # Gradient norm.\r\n                     self.losses[bucket_id]]  # Loss for this batch.\r\n    else:\r\n      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\r\n      for l in xrange(decoder_size):  # Output logits.\r\n        output_feed.append(self.outputs[bucket_id][l])\r\n\r\n    outputs = session.run(output_feed, input_feed)\r\n    if not forward_only:\r\n      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\r\n    else:\r\n      return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\r\n\r\n  def get_batch(self, data, bucket_id):\r\n   \r\n    encoder_size, decoder_size = self.buckets[bucket_id]\r\n    encoder_inputs, decoder_inputs = [], []\r\n\r\n    # Get a random batch of encoder and decoder inputs from data,\r\n    # pad them if needed, reverse encoder inputs and add GO to decoder.\r\n    for _ in xrange(self.batch_size):\r\n      encoder_input, decoder_input = random.choice(data[bucket_id])\r\n\r\n      # Encoder inputs are padded and then reversed.\r\n      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\r\n      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\r\n\r\n      # Decoder inputs get an extra \"GO\" symbol, and are padded then.\r\n      decoder_pad_size = decoder_size - len(decoder_input) - 1\r\n      decoder_inputs.append([data_utils.GO_ID] + decoder_input +\r\n                            [data_utils.PAD_ID] * decoder_pad_size)\r\n\r\n    # Now we create batch-major vectors from the data selected above.\r\n    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\r\n\r\n    # Batch encoder inputs are just re-indexed encoder_inputs.\r\n    for length_idx in xrange(encoder_size):\r\n      batch_encoder_inputs.append(\r\n          np.array([encoder_inputs[batch_idx][length_idx]\r\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\r\n\r\n    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\r\n    for length_idx in xrange(decoder_size):\r\n      batch_decoder_inputs.append(\r\n          np.array([decoder_inputs[batch_idx][length_idx]\r\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\r\n\r\n      # Create target_weights to be 0 for targets that are padding.\r\n      batch_weight = np.ones(self.batch_size, dtype=np.float32)\r\n      for batch_idx in xrange(self.batch_size):\r\n        # We set weight to 0 if the corresponding target is a PAD symbol.\r\n        # The corresponding target is decoder_input shifted by 1 forward.\r\n        if length_idx < decoder_size - 1:\r\n          target = decoder_inputs[batch_idx][length_idx + 1]\r\n        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\r\n          batch_weight[batch_idx] = 0.0\r\n      batch_weights.append(batch_weight)\r\n    return batch_encoder_inputs, batch_decoder_inputs, batch_weights`", "This is a good question for stack overflow.\n\nOn Jul 3, 2017 8:46 AM, \"sachinh35\" <notifications@github.com> wrote:\n\n> I am totally new to this. Maybe this a pretty basic question but could you\n> tell what argument to be passed as the decoder cell in this code? I am\n> trying to develop the seq2seq as shown in the tensorflow tutorial using own\n> dataset.\n> `# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n> Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n> not use this file except in compliance with the License. You may obtain a\n> copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless\n> required by applicable law or agreed to in writing, software distributed\n> under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES\n> OR CONDITIONS OF ANY KIND, either express or implied. See the License for\n> the specific language governing permissions and limitations under the\n> License. ============================================================\n> ==================\n>\n> \"\"\"Sequence-to-sequence model with an attention mechanism.\"\"\"\n>\n> from *future* import absolute_import\n> from *future* import division\n> from *future* import print_function\n>\n> import random\n>\n> import numpy as np\n> from six.moves import xrange # pylint: disable=redefined-builtin\n> import tensorflow as tf\n>\n> import data_utils\n>\n> class Seq2SeqModel(object):\n> \"\"\"Sequence-to-sequence model with attention and for multiple buckets.\n>\n> This class implements a multi-layer recurrent neural network as encoder,\n> and an attention-based decoder. This is the same as the model described in\n> this paper: http://arxiv.org/abs/1412.7449 - please look there for\n> details,\n> or into the seq2seq library for complete model implementation.\n> This class also allows to use GRU cells in addition to LSTM cells, and\n> sampled softmax to handle large output vocabulary size. A single-layer\n> version of this model, but with bi-directional encoder, was presented in\n> http://arxiv.org/abs/1409.0473\n> and sampled softmax is described in Section 3 of the following paper.\n> http://arxiv.org/abs/1412.2007\n> \"\"\"\n>\n> def *init*(self,\n> source_vocab_size,\n> target_vocab_size,\n> buckets,\n> size,\n> num_layers,\n> max_gradient_norm,\n> batch_size,\n> learning_rate,\n> learning_rate_decay_factor,\n> use_lstm=False,\n> num_samples=512,\n> forward_only=False,\n> dtype=tf.float32):\n> \"\"\"Create the model.\n>\n> Args:\n>   source_vocab_size: size of the source vocabulary.\n>   target_vocab_size: size of the target vocabulary.\n>   buckets: a list of pairs (I, O), where I specifies maximum input length\n>     that will be processed in that bucket, and O specifies maximum output\n>     length. Training instances that have inputs longer than I or outputs\n>     longer than O will be pushed to the next bucket and padded accordingly.\n>     We assume that the list is sorted, e.g., [(2, 4), (8, 16)].\n>   size: number of units in each layer of the model.\n>   num_layers: number of layers in the model.\n>   max_gradient_norm: gradients will be clipped to maximally this norm.\n>   batch_size: the size of the batches used during training;\n>     the model construction is independent of batch_size, so it can be\n>     changed after initialization if this is convenient, e.g., for decoding.\n>   learning_rate: learning rate to start with.\n>   learning_rate_decay_factor: decay learning rate by this much when needed.\n>   use_lstm: if true, we use LSTM cells instead of GRU cells.\n>   num_samples: number of samples for sampled softmax.\n>   forward_only: if set, we do not construct the backward pass in the model.\n>   dtype: the data type to use to store internal variables.\n> \"\"\"\n> self.source_vocab_size = source_vocab_size\n> self.target_vocab_size = target_vocab_size\n> self.buckets = buckets\n> self.batch_size = batch_size\n> self.learning_rate = tf.Variable(\n>     float(learning_rate), trainable=False, dtype=dtype)\n> self.learning_rate_decay_op = self.learning_rate.assign(\n>     self.learning_rate * learning_rate_decay_factor)\n> self.global_step = tf.Variable(0, trainable=False)\n>\n> # If we use sampled softmax, we need an output projection.\n> output_projection = None\n> softmax_loss_function = None\n> # Sampled softmax only makes sense if we sample less than vocabulary size.\n> if num_samples > 0 and num_samples < self.target_vocab_size:\n>   w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\n>   w = tf.transpose(w_t)\n>   b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\n>   output_projection = (w, b)\n>\n>   def sampled_loss(labels, inputs):\n>     labels = tf.reshape(labels, [-1, 1])\n>     # We need to compute the sampled_softmax_loss using 32bit floats to\n>     # avoid numerical instabilities.\n>     local_w_t = tf.cast(w_t, tf.float32)\n>     local_b = tf.cast(b, tf.float32)\n>     local_inputs = tf.cast(inputs, tf.float32)\n>     return tf.cast(\n>         tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\n>                                    num_samples, self.target_vocab_size),\n>         dtype)\n>   softmax_loss_function = sampled_loss\n>\n> # Create the internal multi-layer cell for our RNN.\n> def single_cell():\n>   return tf.nn.rnn_cell.GRUCell(size)\n> if use_lstm:\n>   def single_cell():\n>     return tf.nn.rnn_cell.BasicLSTMCell(size)\n> cell = single_cell()\n> if num_layers > 1:\n>   cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(num_layers)])\n>\n> # The seq2seq function: we use embedding for the input and attention.\n> def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n>   return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n>       encoder_inputs,\n>       decoder_inputs,\n>       cell,\n>       num_encoder_symbols=source_vocab_size,\n>       num_decoder_symbols=target_vocab_size,\n>       embedding_size=size,\n>       output_projection=output_projection,\n>       feed_previous=do_decode,\n>       dtype=dtype)\n>\n> # Feeds for inputs.\n> self.encoder_inputs = []\n> self.decoder_inputs = []\n> self.target_weights = []\n> for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n>   self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n>                                             name=\"encoder{0}\".format(i)))\n> for i in xrange(buckets[-1][1] + 1):\n>   self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n>                                             name=\"decoder{0}\".format(i)))\n>   self.target_weights.append(tf.placeholder(dtype, shape=[None],\n>                                             name=\"weight{0}\".format(i)))\n>\n> # Our targets are decoder inputs shifted by one.\n> targets = [self.decoder_inputs[i + 1]\n>            for i in xrange(len(self.decoder_inputs) - 1)]\n>\n> # Training outputs and losses.\n> if forward_only:\n>   self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n>       self.encoder_inputs, self.decoder_inputs, targets,\n>       self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n>       softmax_loss_function=softmax_loss_function)\n>   # If we use output projection, we need to project outputs for decoding.\n>   if output_projection is not None:\n>     for b in xrange(len(buckets)):\n>       self.outputs[b] = [\n>           tf.matmul(output, output_projection[0]) + output_projection[1]\n>           for output in self.outputs[b]\n>       ]\n> else:\n>   self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n>       self.encoder_inputs, self.decoder_inputs, targets,\n>       self.target_weights, buckets,\n>       lambda x, y: seq2seq_f(x, y, False),\n>       softmax_loss_function=softmax_loss_function)\n>\n> # Gradients and SGD update operation for training the model.\n> params = tf.trainable_variables()\n> if not forward_only:\n>   self.gradient_norms = []\n>   self.updates = []\n>   opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n>   for b in xrange(len(buckets)):\n>     gradients = tf.gradients(self.losses[b], params)\n>     clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n>                                                      max_gradient_norm)\n>     self.gradient_norms.append(norm)\n>     self.updates.append(opt.apply_gradients(\n>         zip(clipped_gradients, params), global_step=self.global_step))\n>\n> self.saver = tf.train.Saver(tf.global_variables())\n>\n> def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n> bucket_id, forward_only):\n> \"\"\"Run a step of the model feeding the given inputs.\n>\n> Args:\n>   session: tensorflow session to use.\n>   encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n>   decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n>   target_weights: list of numpy float vectors to feed as target weights.\n>   bucket_id: which bucket of the model to use.\n>   forward_only: whether to do the backward step or only forward.\n>\n> Returns:\n>   A triple consisting of gradient norm (or None if we did not do backward),\n>   average perplexity, and the outputs.\n>\n> Raises:\n>   ValueError: if length of encoder_inputs, decoder_inputs, or\n>     target_weights disagrees with bucket size for the specified bucket_id.\n> \"\"\"\n> # Check if the sizes match.\n> encoder_size, decoder_size = self.buckets[bucket_id]\n> if len(encoder_inputs) != encoder_size:\n>   raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n>                    \" %d != %d.\" % (len(encoder_inputs), encoder_size))\n> if len(decoder_inputs) != decoder_size:\n>   raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n>                    \" %d != %d.\" % (len(decoder_inputs), decoder_size))\n> if len(target_weights) != decoder_size:\n>   raise ValueError(\"Weights length must be equal to the one in bucket,\"\n>                    \" %d != %d.\" % (len(target_weights), decoder_size))\n>\n> # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n> input_feed = {}\n> for l in xrange(encoder_size):\n>   input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n> for l in xrange(decoder_size):\n>   input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n>   input_feed[self.target_weights[l].name] = target_weights[l]\n>\n> # Since our targets are decoder inputs shifted by one, we need one more.\n> last_target = self.decoder_inputs[decoder_size].name\n> input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n>\n> # Output feed: depends on whether we do a backward step or not.\n> if not forward_only:\n>   output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n>                  self.gradient_norms[bucket_id],  # Gradient norm.\n>                  self.losses[bucket_id]]  # Loss for this batch.\n> else:\n>   output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n>   for l in xrange(decoder_size):  # Output logits.\n>     output_feed.append(self.outputs[bucket_id][l])\n>\n> outputs = session.run(output_feed, input_feed)\n> if not forward_only:\n>   return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n> else:\n>   return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n>\n> def get_batch(self, data, bucket_id):\n> \"\"\"Get a random batch of data from the specified bucket, prepare for step.\n>\n> To feed data in step(..) it must be a list of batch-major vectors, while\n> data here contains single length-major cases. So the main logic of this\n> function is to re-index data cases to be in the proper format for feeding.\n>\n> Args:\n>   data: a tuple of size len(self.buckets) in which each element contains\n>     lists of pairs of input and output data that we use to create a batch.\n>   bucket_id: integer, which bucket to get the batch for.\n>\n> Returns:\n>   The triple (encoder_inputs, decoder_inputs, target_weights) for\n>   the constructed batch that has the proper format to call step(...) later.\n> \"\"\"\n> encoder_size, decoder_size = self.buckets[bucket_id]\n> encoder_inputs, decoder_inputs = [], []\n>\n> # Get a random batch of encoder and decoder inputs from data,\n> # pad them if needed, reverse encoder inputs and add GO to decoder.\n> for _ in xrange(self.batch_size):\n>   encoder_input, decoder_input = random.choice(data[bucket_id])\n>\n>   # Encoder inputs are padded and then reversed.\n>   encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n>   encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n>\n>   # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n>   decoder_pad_size = decoder_size - len(decoder_input) - 1\n>   decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n>                         [data_utils.PAD_ID] * decoder_pad_size)\n>\n> # Now we create batch-major vectors from the data selected above.\n> batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n>\n> # Batch encoder inputs are just re-indexed encoder_inputs.\n> for length_idx in xrange(encoder_size):\n>   batch_encoder_inputs.append(\n>       np.array([encoder_inputs[batch_idx][length_idx]\n>                 for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n>\n> # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n> for length_idx in xrange(decoder_size):\n>   batch_decoder_inputs.append(\n>       np.array([decoder_inputs[batch_idx][length_idx]\n>                 for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n>\n>   # Create target_weights to be 0 for targets that are padding.\n>   batch_weight = np.ones(self.batch_size, dtype=np.float32)\n>   for batch_idx in xrange(self.batch_size):\n>     # We set weight to 0 if the corresponding target is a PAD symbol.\n>     # The corresponding target is decoder_input shifted by 1 forward.\n>     if length_idx < decoder_size - 1:\n>       target = decoder_inputs[batch_idx][length_idx + 1]\n>     if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n>       batch_weight[batch_idx] = 0.0\n>   batch_weights.append(batch_weight)\n> return batch_encoder_inputs, batch_decoder_inputs, batch_weights`\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-312679587>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0l5UMHHtbL1sz7meXserV8NVS7cks5sKQzXgaJpZM4MWl4f>\n> .\n>\n", "Okay! thanks though! :)", "@ebrevdo  is there any update on when the new tutorial of seq2seq using new api will come out?\r\nThank you. Amazing work!.\r\n", "yeah waiting for the new tutorial... would be great to know if it's planned to be released anytime soon.. @ebrevdo \r\n\r\ntried to take code in the kernel tests and retrofit the beam search with the legacy seq2seq, but it was challenging...", "We're hoping for this coming week!\n\nOn Jul 3, 2017 10:16 AM, \"prashantserai\" <notifications@github.com> wrote:\n\n> yeah waiting for the new tutorial... would be great to know if it's\n> planned to be released anytime soon.. @ebrevdo\n> <https://github.com/ebrevdo>\n>\n> tried to take code in the kernel tests and retrofit the beam search with\n> the legacy seq2seq, but it seemed challenging...\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-312697274>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim45-HTuQrIRDhphqqHjqkKOKTe53ks5sKSHYgaJpZM4MWl4f>\n> .\n>\n", "Hi guys,\r\n\r\nAny update to this issue, I'm experiencing the same on tensorflow 1.1-gpu for mac os x", "@tshi1983 \r\nI got the same problem with tensorflow 1.1-gpu for ubuntu. \r\nI upgrade to tf 1.2. It still doesn't work. \r\nThen I change the function embedding_attention_seq2seq in file \r\ntensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\r\nto the one as @fabiofumarola suggested above. \r\nNow it starts training. I haven't tested decoding yet. \r\n", "Move the code on cell definition into seq2seq_f:\r\n\r\n```\r\ndef seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\r\n      def single_cell():\r\n        return tf.contrib.rnn.GRUCell(size)\r\n      if use_lstm:\r\n        def single_cell():\r\n          return tf.contrib.rnn.BasicLSTMCell(size)\r\n      cell = single_cell()\r\n      if num_layers > 1:\r\n        cell = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\r\n      return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\r\n\t  ...\r\n\t  )\r\n```\r\nThen \"python translate.py --data_dir data/ --train_dir checkpoint/ --size=256 --num_layers=2 --steps_per_checkpoint=50\" can work.\r\n\r\n", "@huxuanlai it works! At least it's training now, thx!", "@huxuanlai Works for me as well.  ", "I am receiving the same `AttributeError: 'NoneType' object has no attribute 'update'` but with `tf.contrib.legacy_seq2seq.model_with_buckets`. I am running tf 1.2.1 (GPU)  on ubuntu 16.04 lts.\r\n\r\nThis only seems to occur when I have more than 1 bucket.\r\n\r\nfull traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"chatbot.py\", line 262, in <module>\r\n    main()\r\n  File \"chatbot.py\", line 257, in main\r\n    train()\r\n  File \"chatbot.py\", line 138, in train\r\n    model.build_graph()\r\n  File \"/home/jkarimi91/Projects/cs20/code/hw/a3/model.py\", line 134, in build_graph\r\n    self._create_loss()\r\n  File \"/home/jkarimi91/Projects/cs20/code/hw/a3/model.py\", line 102, in _create_loss\r\n    softmax_loss_function=self.softmax_loss_function)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 1206, in model_with_buckets\r\n    decoder_inputs[:bucket[1]])\r\n  File \"/home/jkarimi91/Projects/cs20/code/hw/a3/model.py\", line 101, in <lambda>\r\n    lambda x, y: _seq2seq_f(x, y, False),\r\n  File \"/home/jkarimi91/Projects/cs20/code/hw/a3/model.py\", line 76, in _seq2seq_f\r\n    feed_previous=do_decode)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 848, in embedding_attention_seq2seq\r\n    encoder_cell = copy.deepcopy(cell)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 174, in deepcopy\r\n    y = copier(memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 476, in __deepcopy__\r\n    setattr(result, k, copy.deepcopy(v, memo))\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 230, in _deepcopy_list\r\n    y.append(deepcopy(a, memo))\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 230, in _deepcopy_list\r\n    y.append(deepcopy(a, memo))\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 230, in _deepcopy_list\r\n    y.append(deepcopy(a, memo))\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 237, in _deepcopy_tuple\r\n    y.append(deepcopy(a, memo))\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 334, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 163, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 190, in deepcopy\r\n    y = _reconstruct(x, rv, 1, memo)\r\n  File \"/home/jkarimi91/Apps/anaconda2/envs/tf/lib/python2.7/copy.py\", line 343, in _reconstruct\r\n    y.__dict__.update(state)\r\nAttributeError: 'NoneType' object has no attribute 'update'\r\n```", "@Tshzzz @jtubert \r\nthx, your solution worked for me. My tf verstion is 1.1.0.\r\n\r\nI changed from:\r\n```python\r\n    lstm_cell = tf.contrib.rnn.BasicLSTMCell(HIDDEN_SIZE, state_is_tuple=True)\r\n    cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(NUM_LAYERS)])\r\n    output, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)  \r\n```\r\nto:\r\n```python\r\n    cells=[]\r\n    for _ in range(NUM_LAYERS):\r\n        cell = tf.contrib.rnn.BasicLSTMCell(HIDDEN_SIZE, state_is_tuple=True)\r\n        cells.append(cell)\r\n    multicell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\r\n    output, _ = tf.nn.dynamic_rnn(multicell, X, dtype=tf.float32)\r\n```\r\n", "This is still not fixed , tried all possible solutions , ones mentioned in this thread and stackoverflow , it doesn't work with tensorflow 1.3 or 1.2 or 1.1 ", "I'm facing this error:\r\n```TypeError: embedding_attention_seq2seq() missing 1 required positional argument: 'dec_cell' ```\r\n\r\nThe error points to this function in seq2seq_model.py which is line 142 in seq2seq_model.py:\r\n\r\n  ```def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\r\n      return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\r\n          encoder_inputs,\r\n          decoder_inputs,\r\n          cell,\r\n          num_encoder_symbols=source_vocab_size,\r\n          num_decoder_symbols=target_vocab_size,\r\n          embedding_size=size,\r\n          output_projection=output_projection,\r\n          feed_previous=do_decode,\r\n          dtype=dtype) \r\n```\r\n\r\n\r\nAnyone who came across with this error and managed  to solve this, please help me correct this issue.", "ValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.GRUCell object at 0x11d32cbd0> with a different variable scope than its first use. First use of cell was with scope 'rnn/multi_rnn_cell/cell_0/gru_cell', this attempt is with scope 'rnn/multi_rnn_cell/cell_1/gru_cell'. Please create a new instance of the cell if you would like it to use a different set of weights. If before you were using: MultiRNNCell([GRUCell(...)] * num_layers), change to: MultiRNNCell([GRUCell(...) for _ in range(num_layers)]). If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse). In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)\r\n\r\nthe origin code:\r\nfrom tensorflow.contrib import rnn\r\ninputs = tf.placeholder(dtype=tf.int32, shape=[None, None], name=\"inputs\")\r\nkeep_prob = tf.placeholder(dtype=tf.float32, name=\"keep_prob\")\r\ncell = rnn.GRUCell(10)\r\ncell = rnn.DropoutWrapper(cell=cell, input_keep_prob=keep_prob)\r\ncell = rnn.MultiRNNCell([cell for _ in range(5)], state_is_tuple=True)\r\n\r\n  outs, states = tf.nn.dynamic_rnn(cell=cell, inputs=look_up, dtype=tf.float32)\r\nsolution:\r\ninputs = tf.placeholder(dtype=tf.int32, shape=[None, None], name=\"inputs\")\r\nkeep_prob = tf.placeholder(dtype=tf.float32, name=\"keep_prob\")\r\ncell = rnn.MultiRNNCell([rnn.DropoutWrapper(rnn.GRUCell(10), input_keep_prob=keep_prob) for _ in range(5)] , state_is_tuple=True)", "Do you have this issue with the tf nightlies?\n\nOn Oct 1, 2017 8:34 AM, \"Baohua Zhou\" <notifications@github.com> wrote:\n\n> I have the same issue when using tensorflow 1.1 on cpu with ios.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-333384725>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwOv7vf5vvFXBllbZryjCFwmJcU6ks5sn7DxgaJpZM4MWl4f>\n> .\n>\n", "AttributeError: 'NoneType' object has no attribute 'update'\r\n \r\nin tf=1.3  ", "   ValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.GRUCell object at 0x117f7cbd0> with a different variable scope than its first use.  First use of cell was with scope 'embedding_attention_seq2seq/rnn/multi_rnn_cell/cell_0/gru_cell', this attempt is with scope 'embedding_attention_seq2seq/rnn/multi_rnn_cell/cell_1/gru_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([GRUCell(...)] * num_layers), change to: MultiRNNCell([GRUCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)\r\n", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "The solution is to move to a newer version of TF.  This thread has drastically diverged from its original issue.  Closing.", "If you want instant solution you can try what i tried : \r\n\r\n`pip install tensorflow==1.0\r\n`\r\nThe issue is with tenorflow 1.1 version , it worked for me.\r\n\r\n"]}, {"number": 8190, "title": "A bug with slim.create_global_step", "body": "When I use **slim.create_global_step** in distributed training, the  **slim.create_global_step** can make the workers freeze just after session has been created.\r\n\r\nThis can be reproduced by just replacing the `global_step = tf.Variable(0, name=\"global_step\", trainable=False)` with `global_step = slim.create_global_step()` in the **mnist_replica.py** in the [dist_test](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/dist_test/python)", "comments": ["This bug has been fixed?", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "@prb12 The details includes:\r\n\r\n1. OS: Ubuntu Server 16.04\r\n\r\n2. Hardware: Dell R730 with Tesla K80\r\n\r\n3. TensorFlow Version: 1.0 \r\n\r\nBesides, the in-graph multi-gpu usage seems to be correct. This is quite strange.", "@linrio I just found this bug yesterday, so probably not fixed yet", "@prb12 \r\nOne thing I forgot to mention is that all my experiments run in nvidia-docker containers. For the distributed training experiment, I create two ps and two workers, so total four containers. \r\n  ", "@sguada \r\nI have run more tests and I think the problem occurs when the **dtype**  of **global_step** is **tf.int64** when **distributed training** is running in **nvidia-docker containers**. ", "Try slim.get_or_create_global_step() instead\nOtherwise they are creating different global_step\n\nOn Mar 9, 2017 5:50 PM, \"lsy643\" <notifications@github.com> wrote:\n\n> @sguada <https://github.com/sguada>\n> I have run more tests and I think the problem occurs when the *dtype* of\n> *global_step* is *tf.int64* when *distributed training* is running in *nvidia-docker\n> containers*.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8190#issuecomment-285549210>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABr0fPleAFCZJfy6PgDd_y-D5mZEbFStks5rkKxbgaJpZM4MWhqp>\n> .\n>\n", "@sguada \r\nI have tried slim.get_or_create_global_step(). Still the same problem, the **global_step/sec** just becomes 0 and training freezes just after session created. Since the global_step is created just at the beginning of the graph, it is probably not because different global_step is created", "[TensorFlow 1.0.1](https://github.com/tensorflow/tensorflow/releases/tag/v1.0.1) still has this issue.", "@wangyum \r\nI think this may be caused by that the distributed training part can not handle 8-bytes long variables. So just  create a global_step with dtype=tf.int32", "@lsy643  Do you mean change [here](https://github.com/tensorflow/tensorflow/blob/0ea7b2bcc2f553b06859b7cbf6962dfc340c868d/tensorflow/python/training/training_util.py#L116) to `dtype=dtypes.int32`?\r\n", "@wangyum \r\nYes, but instead of changing the source code of slim, I just create the global_step myself\r\n```python\r\nglobal_step` = tf.get_variable(\r\n        'global_step', [], tf.int32,\r\n        initializer=tf.zeros_initializer(), trainable=False)`\r\n```", "@lsy643 DataLossError: \r\n```\r\n  File \"/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nDataLossError (see above for traceback): Invalid size in bundle entry: key global_step; stored size 8; expected size 4\r\n         [[Node: save/RestoreV2_780 = RestoreV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:0/cpu:0\"](_recv_save/Const_0_S1, save/RestoreV2_780/tensor_names, save/RestoreV2_780/shape_and_slices)]]\r\n\r\n\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n        at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n        at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:317)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n        at java.lang.Thread.run(Thread.java:745)\r\n```", "@wangyum \r\nAccording to you log, `Invalid size in bundle entry: key global_step; stored size 8; expected size 4`\r\nI think there is a dtype mismatch for the global_step variable", "Re-open if needed", "I am using TF 1.1 on Ubuntu Xenial 16.04\r\nand I am experiencing the same problem with the new tf.train.get_or_create_global_step() function\r\nWhen I use it to create my global step variable I get on the following error on executing my MonitoredSession\r\n`tensorflow.python.framework.errors_impl.DataLossError: Invalid size in bundle entry: key global_step/global_step; stored size 4; expected size 8\r\n\t [[Node: save_1/RestoreV2 = RestoreV2[dtypes=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]\r\n\t [[Node: save_1/RestoreV2_4/_3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_20_save_1/RestoreV2_4\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n`\r\nWhen I manually change the dtype in the source of tf.train.get_or_create_global_step() to int32 the error does not show. But this obviously not how it is intended. \r\n\r\nSo for now I will go back to creating the global step manually. \r\n\r\nFull error trace: \r\n`line 107, in main\r\n    step_counter_hook]) as sess:\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 627, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 456, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 800, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 805, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 517, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 393, in create_session\r\n    init_fn=self._scaffold.init_fn)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py\", line 256, in prepare_session\r\n    config=config)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py\", line 188, in _restore_checkpoint\r\n    saver.restore(sess, ckpt.model_checkpoint_path)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1457, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.DataLossError: Invalid size in bundle entry: key global_step/global_step; stored size 4; expected size 8`", "I'm also running into this bug: If I use `int64` for global_step my training hangs, but if I use `int32` the training runs and the `evaluation_loop` fails when loading the checkpoints because of `Invalid size in bundle entry: key global_step/global_step; stored size 4; expected size 8`", "Is there a workaround besides editing the source files in Tensorflow? I'm using the `tensorflow-gpu` package and would prefer not to build from source.", "okay, current workaround for me is just to create the `global_step` variable in the evaluation file separately instead of loading it in: `global_step = tf.Variable(0, dtype=tf.int32, name='global_step', trainable=False)`", "Also running into `tf.int32` vs. `tf.int64` issues when using `tf.train.get_or_create_global_step` (this issue seems most closely related). Is there a particular reason why the resulting variable is [hardcoded](https://github.com/tensorflow/tensorflow/blob/0ea7b2bcc2f553b06859b7cbf6962dfc340c868d/tensorflow/python/training/training_util.py#L116) as `tf.int64`? More generally, can the related set of methods be updated to support passing in `dtype` as a keyword argument?", "@j-wilson Try Tensorflow 1.2.0."]}, {"number": 8189, "title": "tf.image.crop_and_resize not returning proper values of the cropped data", "body": "I am using **tf.image.crop_and_resize**  to obtain cropped data of image but the function is returning all zeroes instead of pixel values.\r\n\r\nMy code is like this \r\n```\r\nori_image = Image.open('/home/sumith/imagepyramids/2.jpg')\r\nimg_data = np.expand_dims(np.asarray(ori_image).astype(np.float32), axis=0)\r\nwith tf.Session() as sess:\r\n   sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\r\n   cropped_list = sess.run(tf.image.crop_and_resize(image=img_data, \r\n      boxes=[[0, 0, 50, 50]], crop_size=[40, 36],   box_ind[0]))\r\n   print(np.asarray(ori_image.crop([0, 0, 50, 50]).resize((40, 36)).astype(np.float32))\r\n   print(cropped_list[0])\r\n```\r\nthe first print statement prints the proper cropped data of image but the second print gives the array in proper shape as given in **crop_size** but array full of zeros.\r\n\r\nthe first print statement's output\r\n```\r\n[[[  60.   46.   42.]\r\n  [  60.   46.   40.]\r\n  [  63.   45.   38.]\r\n  ..., \r\n  [  71.   68.   85.]\r\n  [  73.   71.   86.]\r\n  [  74.   73.   87.]]\r\n  ..., \r\n  [  69.   70.   93.]\r\n  [  75.   74.   94.]\r\n  [  78.   74.   93.]]]\r\n```\r\n\r\nthe second print statement's output\r\n```\r\n[[[ 59.  45.  42.]\r\n  [  0.   0.   0.]\r\n  [  0.   0.   0.]\r\n  ..., \r\n  [  0.   0.   0.]\r\n  [  0.   0.   0.]\r\n  [  0.   0.   0.]]\r\n ....,\r\n  [  0.   0.   0.]\r\n  [  0.   0.   0.]\r\n  [  0.   0.   0.]]]\r\n```\r\nIt would be of great help if I get answer for this.", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Hi , \r\nBelow are the platform details\r\nOperating System : Ubuntu 14.04 \r\nProcessor : Intel\u00ae Core\u2122 i7-2820QM CPU @ 2.30GHz \u00d7 8 \r\ntensorflow version : 0.12.0-rc0\r\npython version : 3.4", "You example has numerous syntax errors, but I think the underlying problem may be that you are passing incorrect values for the `boxes` arg, which should be _relative_ coordinates in the range [0,1]\r\n\r\nPlease see the following documentation\r\nhttps://www.tensorflow.org/versions/r0.12/api_docs/python/image/cropping\r\nhttps://www.tensorflow.org/api_docs/python/tf/image/crop_and_resize\r\n\r\nPlease let me know if this fixes your problem?", "Hi ,\r\nI went through the tensoflow docs but didn't get much. As you told it as a relative co ordinate in the range [0, 1], It would be helpful if you give a code snippet on that.\r\nI didn't find any syntax errors in the code snippet I gave above. I have missed the imports. ", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8188, "title": "Feature request: Tensorboard: Set defaults (Split on underscores, tooltip sorting, ...)", "body": "It would be awesome to be able to set preferred defaults for TensorBoard somewhere, in particular:\r\n\r\n- Split on underscores\r\n- Tooltip sorting method\r\n\r\nBest,\r\nPhilip", "comments": ["How would you feel about saving into browser localStorage?", "Closing since we didn't hear a response. @haeusser feel free to re-open at https://github.com/tensorflow/tensorboard/issues"]}, {"number": 8187, "title": "Update protobuf.cmake to 3.2.0?", "body": "Hello,\r\n\r\nThe [protobuf.cmake](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/external/protobuf.cmake) is building an old version of protobuf, one which specifically does not include the parsing limit bump from 64MB to 2GB (referenced in #582 and resolved by #7338). It is currently building a fork which contains a specific MSVC [fix](https://github.com/mrry/protobuf/commit/1d2c7b6c7376f396c8c7dd9b6afd2d4f83f3cb05); this fix has been [merged](https://github.com/google/protobuf/pull/2203) into the main protobuf repository and is included in 3.2.0.\r\n\r\nIs it possible to align the repository and tree used by protobuf.cmake to that used by workspace.bzl? I'm happy to submit a PR.\r\n\r\nCheers.", "comments": ["@mrry @benoitsteiner Could one of you please comment on this suggestion?", "It sounds like a good suggestion.", "No objection to using the same protobuf version from cmake and bazel. I'd recommend we update the cmake files to extract the protobuf version from workspace.bzl instead of hardcoding a value in protobuf.cmake", "Hi @benoitsteiner, such a change may make it difficult if workspace.bzl were to be updated to a version of protobuf which introduced another platform quirk. Is it potentially more robust to manage them independently, given that cmake can target a boarder range of platforms?", "However, on reflection, the same difficulty would arise if bazel were consolidated on for all platforms.\r\n\r\nIn the short-term: would you accept a PR which simply updates the repository and tree?\r\nIn the long-term: I can either create another ticket to track the work, or leave this one open.\r\n\r\nCheers."]}, {"number": 8186, "title": " module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'", "body": "my OS is win10\uff0c and the version of tensorflow is 1.0. but  errors occured : No module named 'tensorflow.models',the code snippets as follow:\r\nfrom tensorflow.models.rnn.translate import data_utils\r\nfrom tensorflow.models.rnn.translate import seq2seq_model\r\n\r\nI am new to tensorflow ,thank you!", "comments": ["@yydxlv tensorflow.models has been moved to a separate [github repository](https://github.com/tensorflow/models) \r\n\r\nYou can clone this locally,and move the models folder into your tensorflow folder (found in python site packages ). After that you will have tensorflow.models in your import working correctly. Also if you still have problems with getting data_utils and seq2seq_model not importing from rnn.translate, you can move those files out and rearrange a bit of code in the translate.py to make everthing work. I also had the same problem when starting out, and this was the workaround I used.\r\n\r\nHope it helps!", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8185, "title": "Error with name of 'op'?", "body": "```\r\nTraceback (most recent call last):\r\n  File \"AI_control_4layer.py\", line 218, in <module>\r\n    writer)\r\n  File \"/home/longfei/Repository/drl_navigation/saved_neural_qlearning.py\", line 61, in __init__\r\n    self.create_variables()\r\n  File \"/home/longfei/Repository/drl_navigation/saved_neural_qlearning.py\", line 148, in create_variables\r\n    self.train_op = self.optimizer.apply_gradients(gradients)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 412, in apply_gradients\r\n    self._create_slots(var_list)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py\", line 101, in _create_slots\r\n    self._get_or_make_slot(v, val_rms, \"rms\", self._name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 639, in _get_or_make_slot\r\n    named_slots[var] = slot_creator.create_slot(var, val, op_name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.py\", line 101, in create_slot\r\n    return _create_slot_var(primary, val, '')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.py\", line 55, in _create_slot_var\r\n    slot = variable_scope.get_variable(scope, initializer=val, trainable=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 988, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 890, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 348, in get_variable\r\n    validate_shape=validate_shape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 333, in _true_getter\r\n    caching_device=caching_device, validate_shape=validate_shape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 684, in _get_single_variable\r\n    validate_shape=validate_shape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 226, in __init__\r\n    expected_shape=expected_shape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 322, in _init_from_args\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py\", line 178, in variable_op_v2\r\n    shared_name=shared_name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 708, in _variable_v2\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2398, in create_op\r\n    self._add_op(ret)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2099, in _add_op\r\n    \"is already used\" % op.name)\r\nValueError: cannot add op with name q_network/W_conv1/RMSProp as that name is already used\r\n```\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version and the code/steps necessary to reproduce your problem. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "I think this problem is caused by duplicated name of Variables.", "@xiaop1987 Looks very likely, but hard to tell from the scant information in the original problem report.  It does seem unlikely to be a bug.", "I've just encountered the same erro ,  so I was wondering did you fixed the bug? Thx!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8184, "title": "Mkl kernels", "body": "Adding MKL support for conv_ops", "comments": ["Can one of the admins verify this patch?", "@petewarden / @josh11b could one of you take a look?", "Looks good, but please address my comments above before committing.", "@tensorflow-jenkins test this please", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Can you address the build failure: \r\n\r\n```\r\nERROR: /workspace/tensorflow/core/kernels/BUILD:2374:1: in deps attribute of cc_test rule //tensorflow/core/kernels:immutable_constant_op_test: target '//tensorflow/core/kernels:mkl_ops' does not exist. Since this rule was created by the macro 'tf_cc_test', the error might have been caused by the macro implementation in /workspace/tensorflow/tensorflow.bzl:346:23.\r\nERROR: Analysis of target '//tensorflow/core/kernels:immutable_constant_op_test' failed; build aborted.\r\n```", "Code review comments should be addressed now, and the immutable op test failure should be fixed.", "@tensorflow-jenkins test this please", "\r\nThe failure here seems unrelated to our work (likely a setup issue):\r\n\r\n```15:48:16   7/241 Test  #12: C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/debug/cli/readline_ui_test.py ........................................***Failed   20.17 sec\r\n15:48:16 Traceback (most recent call last):\r\n15:48:16   File \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/debug/cli/readline_ui_test.py\", line 24, in <module>\r\n15:48:16     from tensorflow.python.debug.cli import readline_ui\r\n15:48:16   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\debug\\cli\\readline_ui.py\", line 20, in <module>\r\n15:48:16     import readline\r\n15:48:16 ImportError: No module named 'readline'\r\n\r\n```", "@cais do you recognize this failure?\n", "@martinwicke I believe you pinged the wrong person ...", "Ah, sorry, I did!\r\n\r\n@caisq, do you recognize this failure?", "@martinwicke Yes, this is probably because pyreadline (the Windows equivalent of readline), which is a dependency of tfdbg on Windows, is not installed on the particular Windows machine (win1-slave). I'll install it. This error shouldn't block this PR.", "Thank you!"]}, {"number": 8183, "title": "Correct shape->dense_shape in SparseTensor", "body": "Just a typo correction.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8182, "title": "bucket_by_sequence_length does not dequeue all items given to it from another queue", "body": "I've got some SequenceExamples gzipped in a file that I read with TFRecordReader.  I then enqueue those serialized SequenceExamples onto a RandomShuffleQueue.  Then, I dequeue the RandomShuffleQueue, get a number I need from the serialized SequenceExample, and pass both the number and the SequenceExample into bucket_by_sequence_length.\r\n\r\nSince there are 100 items stored in the file, I expected to get 20 batches of 5.  Instead, I'm getting 17 batches of 5, with many of the smallest input_length items missing.\r\n\r\nMy guess is that the top queue on batch_by_sequence_length is looking at its bucket queues, finding that only very small and very large inputs remain, and so refuses to put them together for the final batch(es).  I was hoping that by setting `allow_smaller_final_batch` to `True`, I could avoid that problem, but I guess not.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nGitHub issue #5609 (the discussion at the very bottom of this thread seems to be the only place on the web [as of 7 Mar 2017] with a discussion on how to actually use bucket_by_sequence_length)\r\n\r\n### Environment info\r\nOperating System:  Fedora 24\r\n\r\nInstalled version of CUDA and cuDNN:  8.0.44, 5.1.5\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n-rw-r--r-- 1 root root   558720 Nov 24 09:02 libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Nov 24 09:02 libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Nov 24 09:02 libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Nov 24 09:02 libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Nov 24 09:02 libcudart_static.a\r\n-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so\r\n-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so.5\r\n-rwxr-xr-x 1 root root 79337624 Nov 24 09:05 libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Nov 24 09:05 libcudnn_static.a\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:  https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl <- installed on 3 Mar 2017\r\n2. The output from `python3 -c \"import tensorflow; print(tensorflow.__version__)\"`.  1.0.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nSave the following code as `test_bucket.py`, download [other.gz](https://github.com/tensorflow/tensorflow/files/826517/other.gz) into the same directory, then run \r\n```python3 test_bucket.py --intput_file other.gz --run_type batches```\r\n\r\n\r\n\r\n```Python\r\nimport argparse\r\nimport tensorflow as tf\r\n\r\n\r\nLAST_RUNNER = 'batch_dequeue'\r\nCONTEXT_FEATURES = {\r\n    'aas_length': tf.FixedLenFeature([], dtype=tf.int64),\r\n    'funcs_length': tf.FixedLenFeature([], dtype=tf.int64)}\r\nSEQUENCE_FEATURES = {\r\n    'aas': tf.FixedLenSequenceFeature([], dtype=tf.int64),\r\n    'funcs': tf.FixedLenSequenceFeature([], dtype=tf.int64)}\r\n\r\n\r\ndef read_aas_length(serialized):\r\n    \"\"\"Read aas_length\"\"\"\r\n    context_parsed, _ = tf.parse_single_sequence_example(\r\n        serialized=serialized,\r\n        context_features=CONTEXT_FEATURES,\r\n        sequence_features=SEQUENCE_FEATURES)\r\n    return context_parsed['aas_length']\r\n\r\n\r\ndef main(args):\r\n    with tf.Session() as sess:\r\n        # filename_queue is FIFOQueue\r\n        filename_queue = tf.train.string_input_producer(\r\n            [args.input_file],\r\n            num_epochs=1,\r\n            name='tfrecord_filename_queue')\r\n        reader = tf.TFRecordReader(\r\n            name='tfrecord_reader',\r\n            options=tf.python_io.TFRecordOptions(\r\n                tf.python_io.TFRecordCompressionType.GZIP))\r\n\r\n        _, next_raw = reader.read(\r\n            filename_queue,\r\n            name='read_records')\r\n        random_raws = tf.RandomShuffleQueue(\r\n            capacity=50,\r\n            min_after_dequeue=0,\r\n            dtypes=tf.string,\r\n            # http://stackoverflow.com/questions/42119238/tensorflow-fifoqueues-dequeuemany-and-dequeueupto-require-the-components-to-ha\r\n            shapes=[()],\r\n            name='randomize_records')\r\n        enqueue_random_raws = random_raws.enqueue(next_raw)\r\n\r\n        serialized_example_dq = random_raws.dequeue()\r\n        aas_length = read_aas_length(serialized_example_dq)\r\n        if args.run_type == 'batches':\r\n            batch_max_lens, batches = \\\r\n                tf.contrib.training.bucket_by_sequence_length(\r\n                    tf.to_int32(aas_length),\r\n                    [serialized_example_dq],\r\n                    5,\r\n                    [100, 200, 300, 400, 500, 1000],\r\n                    allow_smaller_final_batch=True,\r\n                    name=LAST_RUNNER)\r\n\r\n        init = tf.global_variables_initializer()\r\n        sess.run(init)\r\n        # necessary when num_epochs in string_input_producer is not None\r\n        sess.run(tf.local_variables_initializer())\r\n\r\n        random_records_runner = tf.train.QueueRunner(\r\n            random_raws,\r\n            [enqueue_random_raws] * 4)\r\n        coord = tf.train.Coordinator()\r\n        tf.train.add_queue_runner(random_records_runner)\r\n        threads = tf.train.start_queue_runners(coord=coord)\r\n        working = True\r\n        i = 0\r\n\r\n        if args.run_type == 'batches':\r\n            fetch = {\r\n                'batch_max_lens': batch_max_lens,\r\n                'batches': batches}\r\n\r\n        while working:\r\n            try:\r\n                i += 1\r\n                if args.run_type == 'batches':\r\n                    fetched = sess.run(fetch)\r\n                    print(i, fetched['batch_max_lens'])\r\n                else:\r\n                    print(i, sess.run(aas_length))\r\n            except tf.errors.OutOfRangeError as err:\r\n                coord.request_stop()\r\n                working = False\r\n\r\n        coord.join(threads)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n        '--input_file',\r\n        type=str,\r\n        required=True,\r\n        help='Serialized file path... ?')\r\n    parser.add_argument(\r\n        '--run_type',\r\n        type=str,\r\n        default='',\r\n        help='Choose \"batches\" to run in batches')\r\n    args = parser.parse_args()\r\n    main(args)\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nI've tried collecting relevant data in the while loop and then feeding them into a placeholder for batch_by_sequence_length.  That yielded an error message about expecting a string but getting a _Message, or something to that effect.\r\n\r\nI've also looked at the node_def.name of the OutOfRangeError.  It only ever came up with the top queue created by bucket_by_sequence_length.\r\n\r\nSomewhat related, calling bucket_by_sequence_length but not fetching its outputs (so taking the function call out of the if block and running the code without the `--run_type` flag) still caused the list of input lengths to stop at 99 (not 100, as I would expect).\r\n\r\nI've also observed that the missing smaller numbers will show up if `num_epochs` on the TFRecordReader is set to 2, although they only appear once.\r\n\r\nThere were other failed attempts, but I don't recall their details.\r\n\r\n\r\n### Logs or other output that would be helpful\r\n[output_batches.txt](https://github.com/tensorflow/tensorflow/files/826529/output_batches.txt):  Example output of not getting back enough batches.\r\n[output_list.txt](https://github.com/tensorflow/tensorflow/files/826530/output_list.txt):  Example output of printing out all lengths (note that some lengths are less than 100, but there are no lengths in the batches less than 100).\r\n", "comments": ["@joel-shor @ebrevdo  Could one of you please comment on whether this behavior is expected?", "@nOkuda would it be possible for you to shorten your example to a minimal amount of code? That would make it easier for me to investigate this issue. If the problem is with `training.bucket_by_sequence_length` not batching short and long examples together, you should be able to create a relatively small example demonstrating this.\r\n\r\nI'll still investigate either way, but it might take longer for me to be helpful.", "@joel-shor Sorry about not making a better bit of code.  I was originally thinking that the problem was arising from reading tensorflow records or chaining together multiple queues.  But the new minimal working example definitely supports the hypothesis you chose to point out.\r\n\r\nUpdated minimal working example:\r\n```Python\r\nimport argparse\r\nimport tensorflow as tf\r\n\r\n\r\nLAST_RUNNER = 'batch_dequeue'\r\n\r\n\r\ndef main():\r\n    with tf.Session() as sess:\r\n        examples_queue = tf.train.string_input_producer(\r\n            ['1500', '1700', '10', '5', '1'],\r\n            num_epochs=1,\r\n            name='examples_queue')\r\n        example_dq = examples_queue.dequeue()\r\n\r\n        batch_max_lens, batches = \\\r\n            tf.contrib.training.bucket_by_sequence_length(\r\n                tf.string_to_number(example_dq, out_type=tf.int32),\r\n                [example_dq],\r\n                5,\r\n                [100, 200, 300, 400, 500, 1000],\r\n                allow_smaller_final_batch=True,\r\n                name=LAST_RUNNER)\r\n\r\n        init = tf.global_variables_initializer()\r\n        sess.run(init)\r\n        sess.run(tf.local_variables_initializer())\r\n\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(coord=coord)\r\n        working = True\r\n        i = 0\r\n\r\n        fetch = {\r\n            'batch_max_lens': batch_max_lens,\r\n            'batches': batches}\r\n\r\n        while working:\r\n            try:\r\n                i += 1\r\n                fetched = sess.run(fetch)\r\n                print(i, fetched['batch_max_lens'])\r\n            except tf.errors.OutOfRangeError as err:\r\n                coord.request_stop()\r\n                working = False\r\n\r\n        coord.join(threads)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```", "What bug are you trying to demonstrate with this code?\r\n\r\nI see two issues with your example:\r\n1) You need to call `coord.request_stop()`\r\n2) With `string_input_producer` only having one epoch, you will always get an `OutOfRange` exception before any of your `bucket_by_sequence_length` queues are full with 5 examples\r\n\r\nWould you mind re-posting a minimal working example with the bug you are trying to highlight? Are you positive that `bucket_by_sequence_length` behaves the way you think it does?", "I'm expecting `bucket_by_sequence_length` to dequeue all buckets once it's reached the end of the queue that's feeding into it.  Instead, it does not.\r\n\r\nIf what I'm expecting is not intended behavior, how do I dequeue the buckets once the feeding queue is empty?\r\n\r\nTo address your two points:  I am calling `coord.request_stop()` when I catch the `OutOfRangeError` exception.  Also, the `OutOfRangeError` exception is coming from the top queue built by `bucket_by_sequence_length`, not from the `string_input_producer`.  To see the `OutOfRangeError` behavior I have described, add a `print(err.node_def.name)` in the `except` block.\r\n\r\nI accept that the top queue of `bucket_by_sequence_length` may be passing up the `OutOfRangeError` from `string_input_producer`; but if that is the case, then I would expect `err.node_def.name` to be related to the dequeue from the `string_input_producer`.  Instead, the output name is `batch_dequeue/bucket/dequeue_top`.\r\n\r\nI'm also recalling reading that `OutOfRangeError`s do not come from queues dequeueing into another queue.  I can't remember where I read that, though.  Something about threads sleeping when there's nothing left to dequeue.  Maybe it was in the [Threading and Queueing](https://www.tensorflow.org/programmers_guide/threading_and_queues) instructions?  If doesn't work like that, I have no idea how `bucket_by_sequence_length` would work in the lazy manner it purports to in the [documentation](https://www.tensorflow.org/versions/master/api_docs/python/contrib.training/bucketing#bucket_by_sequence_length).", "@joel-shor could you take another look? Thanks!", "@nOkuda @yifeif Did you find a solution? I ran into the same problem and I have some idea why it happens but it won't be trivial to fix.", "@bdaskalov With respect to the project I was using this feature in, my solution was simply to run so many epochs that I wouldn't possibly run into the bug (i.e., training took so long that I never reached the end---this was fine, since I was saving out the information I needed after each epoch; when it was time to wrap up the project, I simply took the latest information and used that).\r\n\r\nAs for the bug I reported in this issue, I have not found a solution.", "I think fixing the problem is fairly easy, although since `QueueRunner` isn't really designed for this, it's slightly messy, but better than the current situation. The problem right now is that the `top_queue` is closed as soon as the input to `bucket_sequence_by_length` runs dry, and then nobody ever closes the `bucket_queues`, but it wouldn't even matter, since by then, the `top_queue` is already closed.\r\n\r\nThis seems to fix the problem:\r\n```\r\n    queue_runner.add_queue_runner(\r\n      queue_runner.QueueRunner(\r\n          bucket_queues[0], enqueues_to_top,\r\n          close_op=top_queue.close(),\r\n          queue_closed_exception_types=(errors.OutOfRangeError,\r\n                                        errors.CancelledError)))\r\n    queue_runner.add_queue_runner(\r\n        queue_runner.QueueRunner(\r\n            top_queue,\r\n            bucket_enqueue_ops,\r\n            close_op=control_flow_ops.group(*[q.close() for q in bucket_queues]),\r\n            queue_closed_exception_types=(errors.OutOfRangeError,\r\n                                          errors.CancelledError)))\r\n```\r\n\r\nBasically, instead of closing `top_queue` when the overall input runs dry, this closes all of the bucket queues, so they then start feeding their last batches. It also uses only one `QueueRunner` for the `enqueues_to_top`, so that after all of them individually run dry, the `top_queue` is finally closed.", "The PR above should fix this.", "I like it!  Will review this week.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "The corresponding PR has been merged internally and should be pushed out soon. Thanks @mschulkind!", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 8181, "title": "remove_training_nodes (Python Lib) Removes Identity Node Even When is Output", "body": "[`remove_training_nodes`](https://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/python/framework/graph_util_impl.py#L246) removes `Identity` nodes even if that node is the specified graph output. \r\n\r\nFor example if the `Identity` node was used to name and make more easily locatable the output of the graph.", "comments": ["@drpngx I believe you wrote this code?  Could you please comment.", "@petewarden who contributed the original code.\r\n\r\n@cancan101 Could you send a simple, compact repro case?\r\n\r\nNot sure if it's the same thing, but we have another case internally where replacing the [if clause](https://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/python/framework/graph_util_impl.py#L310) fixed a problem.\r\n```\r\n      while input_name in names_to_splice:\r\n        full_input_name = names_to_splice[input_name]\r\n        input_name = re.sub(r\"^\\^\", \"\", full_input_name)\r\n      new_node.input.append(full_input_name)\r\n```\r\n\r\n", "Fix submitted, but not quite sure when/how this will propagate here.", "Thanks! We try to push every day, hopefully it'll be there tomorrow. If you included `Fixes #8181` in your CL description, then it'll close this issue automatically when the fix is merged.", "Looks like the fix is in, but I don't seem to be able to close this myself...", "Can you link to PR?", "I think that would be https://github.com/tensorflow/tensorflow/pull/8402.", "In particular: https://github.com/tensorflow/tensorflow/pull/8402/commits/221cfa1e31be026a6f459410f0ed930befd8371d\r\n\r\nDid that work for you @cancan101 ?", "Not sure, the issue I was having was that the last node (the output node) was of type Identity and that was getting pruned itself.", "Can you try with the updated code?\n\nOn Mar 15, 2017 8:26 AM, \"Alex Rothberg\" <notifications@github.com> wrote:\n\n> Not sure, the issue I was having was that the last node (the output node)\n> was of type Identity and that was getting pruned itself.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8181#issuecomment-286776834>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbSABKLY6MGZHs_qO2zrHX_RHeMAwks5rmAMAgaJpZM4MWQgu>\n> .\n>\n", "Oh, then this may not fix your problem...\r\n\r\n![no see that solution is for a different problem than the one i have](http://4.bp.blogspot.com/-KzOtz-8coJU/Uga2fN7SNmI/AAAAAAAAKI4/6QXtU2oXcJ4/s1600/ADTWO21.png)", "Here is a short example demonstrating the issue:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.graph_util_impl import remove_training_nodes\r\n\r\nwith tf.Graph().as_default() as g:\r\n    input = tf.placeholder(tf.float32)\r\n    output = tf.identity(input, name='output')\r\n    graph_def = g.as_graph_def()\r\n    print(\"Before:\")\r\n    print([n.name for n in graph_def.node])\r\n    print(\"After:\")    \r\n    print([n.name for n in remove_training_nodes(graph_def).node])\r\n```\r\n\r\nwhich produces:\r\n```\r\nBefore:\r\n[u'Placeholder', u'output']\r\nAfter:\r\n[u'Placeholder']\r\n```\r\nIn this case the signature of `remove_training_nodes` might need to be changed to list the set of nodes that should not be removed. \r\n\r\nThoughts?", "@drpngx What's the status?", "We changed the `remove_training_nodes` to include a new parameter `protected_nodes`, which are kept unconditionally. @cancan101 would that work for you?", "That should work. ", "Great!", "Maybe I have the same problem. TensorFlow VERSION  1.1.0\r\nHas it been fixed in 1.1.0?\r\n\r\nI specifed [\"input\", \"output\"] as the input and output nodes of my model  by tf.identity.\r\nAfter frozen,  I can found \"input\", \"output\" in graph_def.node.\r\nAfter optimized, \"input\" is still there, but \"output\" is lost.\r\n\r\n# train.py\r\n```python\r\ndef inference(images):\r\n  images = tf.identity(images, 'input')\r\n... ...\r\n  softmax_linear = tf.identity(softmax_linear, 'output')\r\n  return softmax_linear\r\n```\r\n\r\n# package.py -- Generate model file\r\n```python\r\n# -*- coding: utf-8 -*-\r\n# Preparing a TF model for usage in Android\r\n# pylint: disable=missing-docstring\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nimport sys\r\nimport tensorflow as tf\r\nfrom tensorflow.python.tools import freeze_graph\r\nfrom tensorflow.python.tools import optimize_for_inference_lib\r\n\r\nMODEL_NAME = 'carc34'\r\npath='.'\r\ninput_graph_path = '%s/graph.pbtxt' % path\r\ncheckpoint_path = '%s/model.ckpt-100000' % path\r\ninput_saver_def_path = \"\"\r\ninput_binary = False\r\ninput_node_names = \"input\"\r\noutput_node_names = \"output,softmax_linear/softmax_linear\"\r\nrestore_op_name = \"save/restore_all\"\r\nfilename_tensor_name = \"save/Const:0\"\r\noutput_frozen_graph_name = 'frozen_'+MODEL_NAME+'.pb'\r\noutput_optimized_graph_name = 'optimized_'+MODEL_NAME+'.pb'\r\nclear_devices = True\r\n\r\n# Freeze the graph\r\nfreeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\r\n                          input_binary, checkpoint_path, output_node_names,\r\n                          restore_op_name, filename_tensor_name,\r\n                          output_frozen_graph_name, clear_devices, \"\")\r\n\r\n# Optimize for inference\r\ninput_graph_def = tf.GraphDef()\r\nwith tf.gfile.Open(output_frozen_graph_name, \"rb\") as f:\r\n    data = f.read()\r\n    input_graph_def.ParseFromString(data)\r\n    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\r\n                input_graph_def,\r\n                input_node_names.split(\",\"),  # an array of the input node(s)\r\n                output_node_names.split(\",\"), # an array of the output nodes\r\n                tf.float32.as_datatype_enum)\r\n\r\n# Save the optimized graph\r\nf = tf.gfile.FastGFile(output_optimized_graph_name, \"w\")\r\nf.write(output_graph_def.SerializeToString())\r\n##tf.train.write_graph(output_graph_def, './', output_optimized_graph_name)   #\r\n\r\n```\r\n\r\n# show_nodes.py -- Print all nodes in frozen and optimized\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nimport os.path\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import graph_util\r\nfrom tensorflow.python.framework import tensor_shape\r\nfrom tensorflow.python.platform import gfile\r\nfrom tensorflow.python.util import compat\r\n\r\nMODEL_OUTPUT_TENSOR_NAME = 'output'\r\nwith tf.Graph().as_default() as graph:\r\n    model_filename = os.path.join('.', 'frozen_carc34.pb')\r\n    with gfile.FastGFile(model_filename, 'rb') as f:\r\n      graph_def = tf.GraphDef()\r\n      graph_def.ParseFromString(f.read())\r\n      ##for n in graph_def.node:\r\n      ##  print (n.name)\r\n      output_tensor = tf.import_graph_def(graph_def, name='', return_elements=[MODEL_OUTPUT_TENSOR_NAME])\r\n      print (\"Found in frozen\")\r\n\r\n    model_filename = os.path.join('.', 'optimized_carc34.pb')\r\n    with gfile.FastGFile(model_filename, 'rb') as f:\r\n      graph_def = tf.GraphDef()\r\n      graph_def.ParseFromString(f.read())\r\n      ##for n in graph_def.node:\r\n      ##  print (n.name)\r\n      output_tensor = tf.import_graph_def(graph_def, name='', return_elements=[MODEL_OUTPUT_TENSOR_NAME])\r\n      print (\"Found in optimized\")\r\n```\r\n", "Hi Xiusir\r\nI need you help, I am facing the same issue while running the optimize_for_inference.py script.\r\nMy arguments are as below:\r\n\r\npython optimize_for_inference.py --input=res101_faster_rcnn_iter_5600.pb --output=optimized_graph.pb --input_names=\"input\" --output_names=\"final_result\"\r\n\r\nMy pb file is based on Resnet-101. I am sharing my res101_faster_rcnn_iter_5600.ckpt.custom file below to see it's contents:\r\n\r\n`node {\r\nname: \"Placeholder\"\r\nop: \"Placeholder\"\r\nattr {\r\nkey: \"dtype\"\r\nvalue {\r\ntype: DT_FLOAT\r\n}\r\n}\r\nattr {\r\nkey: \"shape\"\r\nvalue {\r\nshape {\r\ndim {\r\nsize: 1\r\n}\r\ndim {\r\nsize: -1\r\n}\r\ndim {\r\nsize: -1\r\n}\r\ndim {\r\nsize: 3\r\n}\r\n}\r\n}\r\n}\r\n}\r\nnode {\r\nname: \"Placeholder_1\"\r\nop: \"Placeholder\"\r\nattr {\r\nkey: \"dtype\"\r\nvalue {\r\ntype: DT_FLOAT\r\n}\r\n}\r\nattr {\r\nkey: \"shape\"\r\nvalue {\r\nshape {\r\ndim {\r\nsize: 1\r\n}\r\ndim {\r\nsize: 3\r\n}\r\n}\r\n}\r\n}\r\n}\r\nnode {\r\nname: \"Placeholder_2\"\r\nop: \"Placeholder\"\r\nattr {\r\nkey: \"dtype\"\r\nvalue {\r\ntype: DT_FLOAT\r\n}\r\n}\r\nattr {\r\nkey: \"shape\"\r\nvalue {\r\nshape {\r\ndim {\r\nsize: -1\r\n}\r\ndim {\r\nsize: 5\r\n}\r\n}\r\n}\r\n}\r\n}\r\nnode {\r\nname: \"resnet_v1_101/Pad/paddings\"\r\nop: \"Const\"\r\nattr {\r\nkey: \"dtype\"\r\nvalue {\r\ntype: DT_INT32\r\n}\r\n}\r\nattr {\r\nkey: \"value\"\r\nvalue {\r\ntensor {\r\ndtype: DT_INT32\r\ntensor_shape {\r\ndim {\r\nsize: 4\r\n}\r\ndim {\r\nsize: 2\r\n}\r\n}\r\ntensor_content: \"\\000\\000\\000\\000\\000\\000\\000\\000\\003\\000\\000\\000\\003\\000\\000\\000\\003\\000\\000\\000\\003\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\"\r\n}\r\n}\r\n}\r\n}\r\nnode {\r\nname: \"resnet_v1_101/Pad\"\r\nop: \"Pad\"\r\ninput: \"Placeholder\"\r\ninput: \"resnet_v1_101/Pad/paddings\"\r\nattr {\r\nkey: \"T\"\r\nvalue {\r\ntype: DT_FLOAT\r\n}\r\n}\r\nattr {\r\nkey: \"Tpaddings\"\r\nvalue {\r\ntype: DT_INT32\r\n}\r\n}\r\n}\r\nnode {\r\nname: \"resnet_v1_101/conv1/weights/Initializer/truncated_normal/shape\"\r\nop: \"Const\"\r\nattr {\r\nkey: \"_class\"\r\nvalue {\r\nlist {\r\ns: \"loc:@resnet_v1_101/conv1/weights\"\r\n}\r\n}\r\n}\r\nattr {\r\nkey: \"dtype\"\r\nvalue {\r\ntype: DT_INT32\r\n}\r\n}\r\nattr {\r\nkey: \"value\"\r\nvalue {\r\ntensor {\r\ndtype: DT_INT32\r\ntensor_shape {\r\ndim {\r\nsize: 4\r\n}\r\n}\r\ntensor_content: \"\\007\\000\\000\\000\\007\\000\\000\\000\\003\\000\\000\\000@\\000\\000\\000\"\r\n}\r\n}\r\n}\r\n}\r\nnode {\r\nname: \"resnet_v1_101/conv1/weights/Initializer/truncated_normal/mean\"\r\nop: \"Const\"\r\nattr {\r\nkey: \"_class\"\r\nvalue {\r\nlist {\r\ns: \"loc:@resnet_v1_101/conv1/weights\"\r\n}\r\n}\r\n}\r\n\r\nI am getting tyhe below error :\r\n\r\n__result\" /home/arun/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6 return f(*args, **kwds) /home/arun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype fromfloattonp.floatingis deprecated. In future, it will be treated asnp.float64 == np.dtype(float).type. from ._conv import register_converters as _register_converters Traceback (most recent call last): File \"optimize_for_inference.py\", line 146, in <module> app.run(main=main, argv=[sys.argv[0]] + unparsed) File \"/home/arun/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run _sys.exit(main(_sys.argv[:1] + flags_passthrough)) File \"optimize_for_inference.py\", line 90, in main FLAGS.output_names.split(\",\"), FLAGS.placeholder_type_enum) File \"/home/arun/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py\", line 109, in optimize_for_inference placeholder_type_enum) File \"/home/arun/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/strip_unused_lib.py\", line 83, in strip_unused raise KeyError(\"The following input nodes were not found: %s\\n\" % not_found) KeyError: \"The following input nodes were not found: {'input'}\\n\"_\r\nPlease help me identifying what arguments you passed to run the script.\r\n\r\npython touch.py -m res101_faster_rcnn_iter_5600.ckpt.meta -o res101_faster_rcnn_iter_5600.pb -b\r\n\r\nand my touch.py has following contents, which I took from some repository:\r\n\r\n```\r\n#! /usr/bin/python3\r\n#generate a pb file !\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\nimport argparse\r\nimport os\r\n\r\nif __name__ == '__main__':\r\n\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('-m','--model',required=True, type=str, metavar='' ,help=\"model meta graph AKA .ckpt.meta file to start with\")\r\n    parser.add_argument('-o', '--output_file', type=str, required=True, metavar='',help=\"Where to save the output file \")\r\n    parser.add_argument('-b','--bin',dest='binary_file',action='store_true', help=\"saves file as binary instead of default text file\")\r\n    parser.add_argument('-v','--verbose',dest='verbosity',action='store_true',help=\"trigger the verbose output (show tensorflow debug info)\")\r\n    args = parser.parse_args()\r\n    #get output dir from output file name\r\n    output_dir = os.path.dirname(args.output_file)\r\n    if output_dir == \"\":\r\n        output_dir=\".\"\r\n    output_file_name = os.path.basename(args.output_file)\r\n    #setting verbosity    \r\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n    if args.verbosity:\r\n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\r\n    print (\"Welcome to tensorflow graph saver\")\r\n    print (\"loading \",os.path.basename(args.model))\r\n    #opening a new session\r\n    with tf.Session() as sess:\r\n        new_saver = tf.train.import_meta_graph(args.model)\r\n        new_saver.restore(sess,args.model.replace(\".meta\",\"\") )\r\n        print (\"model\",os.path.basename(args.model),\"loaded correctly... converting it to graph\")\r\n        tf.train.write_graph(sess.graph, output_dir ,output_file_name, as_text=not args.binary_file)\r\n        print(\"conversion done, output file saved as\",args.output_file)\r\n```"]}, {"number": 8180, "title": "Double requirement given: tensorflow==1.0.0 when installing from binary url on Mac", "body": "### Environment info\r\nOperating System: MacOS 10.12.1\r\n\r\nInstalled version of CUDA and cuDNN: \r\n$ ls -l /usr/local/cuda/lib/libcud*\r\nlrwxr-xr-x  1 root  wheel     33 Jan 31 21:18 /usr/local/cuda/lib/libcuda.1.dylib -> /usr/local/cuda/lib/libcuda.dylib\r\n-rwxr-xr-x  1 root  wheel  13504 Sep 26 14:59 /usr/local/cuda/lib/libcuda.dylib\r\nlrwxr-xr-x  1 root  wheel     45 Sep 26 15:00 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a\r\nlrwxr-xr-x  1 root  wheel     50 Sep 26 15:00 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\nlrwxr-xr-x  1 root  wheel     46 Sep 26 15:00 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib\r\nlrwxr-xr-x  1 root  wheel     49 Sep 26 15:00 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a\r\nlrwxr-xr-x  1 root  wheel     47 Oct 24 21:11 /usr/local/cuda/lib/libcudnn.5.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5.dylib\r\nlrwxr-xr-x  1 root  wheel     45 Oct 24 21:11 /usr/local/cuda/lib/libcudnn.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.dylib\r\nlrwxr-xr-x  1 root  wheel     48 Oct 24 21:11 /usr/local/cuda/lib/libcudnn_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn_static.a\r\n\r\nI attempted to install via `pip install tensorflow --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl`\r\n\r\nThis is the error I got:\r\n```\r\nDouble requirement given: tensorflow==1.0.0 from https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl (already in tensorflow, name='tensorflow')\r\nStoring debug log for failure in /Users/Keven/.pip/pip.log\r\n```\r\n\r\nPrior to this, I did `sudo pip uninstall tensorflow`\r\n\r\n\r\nError log:\r\n```\r\n$ cat /Users/Keven/.pip/pip.log\r\n------------------------------------------------------------\r\n/usr/local/bin/pip run on Tue Mar  7 15:51:21 2017\r\nDownloading/unpacking https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl\r\n  Downloading from URL https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl\r\nCleaning up...\r\nDouble requirement given: tensorflow==1.0.0 from https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl (already in tensorflow, name='tensorflow')\r\nException information:\r\nTraceback (most recent call last):\r\n  File \"/Library/Python/2.7/site-packages/pip-1.5.6-py2.7.egg/pip/basecommand.py\", line 122, in main\r\n    status = self.run(options, args)\r\n  File \"/Library/Python/2.7/site-packages/pip-1.5.6-py2.7.egg/pip/commands/install.py\", line 278, in run\r\n    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)\r\n  File \"/Library/Python/2.7/site-packages/pip-1.5.6-py2.7.egg/pip/req.py\", line 1262, in prepare_files\r\n    self.add_requirement(req_to_install)\r\n  File \"/Library/Python/2.7/site-packages/pip-1.5.6-py2.7.egg/pip/req.py\", line 988, in add_requirement\r\n    % (install_req, self.get_requirement(name), name))\r\nInstallationError: Double requirement given: tensorflow==1.0.0 from https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl (already in tensorflow, name='tensorflow')\r\n```", "comments": ["The `pip` command you typed asks for a `tensorflow` package **and** a second specific package via a download URL.\r\n\r\nPlease double check your command against the ones in the instructions here: https://www.tensorflow.org/install/install_mac\r\n\r\ni.e. use either \r\n`pip install --upgrade tensorflow`\r\nor\r\n`pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl`\r\n\r\nPlease reopen if this doesn't solve your problem", "My bad. Thanks for the quick answer!"]}]