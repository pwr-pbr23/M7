[{"number": 19636, "title": "docker: update cuDNN to 7.1.4.18", "body": "Signed-off-by: Felix Abecassis <fabecassis@nvidia.com>\r\n\r\n@gunan lightly tested on my side, so it will have to pass CI.", "comments": ["Can we get a do-over for the MacOS CI?", "The failure is actually unrelated. Merging."]}, {"number": 19635, "title": "Fix mismatch of shape restriction in DrawBoundingBoxes", "body": "In the kernel of DrawBoundingBoxes, the shape of the input\r\nimages should be 4-D. Though in the shape function,\r\nat the end `UnchangedShapeWithRankAtLeast(c, 3)` was used instead\r\n(at the beginning of the shape function the validation is\r\n`WithRank(c->input(0), 4, &images)` which is correct).\r\n\r\nThis fix address the discrepancy by changing to `UnchangedShape`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19634, "title": "Fix redundancy in RELEASE.md", "body": "There is a semi-duplicate entry for 1.4.0.", "comments": ["Thanks! Probably a merge gone wrong."]}, {"number": 19633, "title": "Branch 198434814", "body": "", "comments": ["@aselle I'm getting error from CreateWrapperCPPFromBuffer that it doesn't like the unicode string input https://source.cloud.google.com/results/invocations/7ecc1d8e-2bea-420e-86ae-34e69da3f6bc/log \r\nWould wrapping model_content with str() at https://github.com/yifeif/tensorflow/blob/30ecb94df8f8e0b6c553a77e5c50ec99afc81824/tensorflow/contrib/lite/python/interpreter.py#L57 be the right fix here?", "cc @ankurtaly "]}, {"number": 19632, "title": "Disable tensors with unknown rank", "body": "I am a Tensorflow novice. I found that a lot of headache was caused by tensors of unknown rank. This sometimes caused runtime errors and sometimes errors at graph building time. I now understand why all of this happened but at the time it caused a lot of grief. I have seen the Tensorflow developers speak about tensors of unknown rank as something to be avoided.\r\n\r\nI propose adding a feature to Tensorflow to disable tensors with unknown rank. Often, when that happens it signals a coding error. That way I would immediately notice when this happens with some intermediate computation.\r\n\r\nThis feature could be either to immediately throw when this happens or it could be a linter that can be run on a graph to warn about suspicious patterns.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Experiences from custom code**\r\n- **Windows TF 1.8**\r\n- **Python 3.5**\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "These fields do not appear relevant to my feature request so I left them out. The TF version was included in my post as 1.8.", "There are many highly dynamic custom code that requires tensors with unknown rank.\r\nFor example you can create one with tf.placeholder(tf.float32).  \r\n\r\nPlease update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I was proposing to enable a sanitizing mode that rejects tensors with unknown ranks. Of course the framework must continue to allow dynamic use. This is important for some use cases and for compatibility. This should be opt-in. Of course I'm fine with closing this issue if you think such a feature would not be of merit. But I'm not sure you noticed that this was meant as an opt-in mode (e.g. `myGraph.setAllowDynamicRank(False)`)."]}, {"number": 19631, "title": "Just another missing module error", "body": "After seeing some YT videos about AI programming in Python using TensorFlow, I thought that this shouldn't be too hard. Although I got an error. I don't really understand it.\r\nThere's told everywhere that I should install some Nvidia-related DLLs, but I neither have Nvidia GPU nor tensorflow-gpu.\r\nThe error is listed [here](https://pastebin.com/Ft1qDyHD).\r\nPlease, explain me the error and preferably give the step by step solution for this issue.\r\nThanks for any help given.\r\nEDIT\r\nAlso, here's more info:\r\nHave I written custom code: No\r\nOS Platform and Distribution: Windows 10 Home\r\nTensorFlow installed from: tensorflow.org\r\nTensorFlow version: I don't know\r\nBazel version: I don't know\r\nCUDA/cuDNN version: I don't know, probably don't have it\r\nGPU model and memory: AMD Mobility Radeon 5000 Series, 1 GB VRAM, 4 GB RAM\r\nExact command to reproduce: import tensorflow as tf\r\n\r\nAlso, after PC restart it even can't find module named \"tensorflow\". What's going on? Help please", "comments": ["@Sparkie2 have you tried uninstalling and reinstalling from scratch? \r\n\r\nAnd also, do any of the installation techniques described on https://www.tensorflow.org/install/install_windows work for you?", "It has been 20 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 35 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19630, "title": "Possible Error in \"TensorFlow For Poets\" Guide", "body": "Small, possible error found in https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#4:\r\n\"The first figure shows accuracy (x-axis) as a function of training progress (y-axis):\"\r\n\r\nShould be corrected to:\r\n\"The first figure shows accuracy (y-axis) as a function of training progress (x-axis):\"\r\n\r\nOr you can change the \"accuracy_1\" graph below it.", "comments": ["Thanks, assigning to Wolff who added the codelab.\r\n", "      \n  \n\n np, thank you for the confirmation email\n  \n\n  \n  \n\n  \n  \n>   \n> On May 29, 2018 at 12:49 PM,  <Ayush Dubey (mailto:notifications@github.com)>  wrote:\n>   \n>   \n>   \n>\n> Thanks, assigning to Wolff who added the codelab.\n>\n>   \n>\n> \u2014\n>  You are receiving this because you authored the thread.\n>  Reply to this email directly,   view it on GitHub (https://github.com/tensorflow/tensorflow/issues/19630#issuecomment-392871729), or   mute the thread (https://github.com/notifications/unsubscribe-auth/AY6WJX57Ki7C2kaa-aBhitxtXwWC5ggnks5t3YorgaJpZM4UR46T).\n>\n>       \n>   \n  \n  \n     ", "Fixed.Thanks. ", "      \n  \n\n Cool, np. There are also some minor spelling errors on   https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#5   (in the yellow box):\n  \n\n  \n\"WIf you chose a mobilenet that takes a smaller input size, then be shure to set the   --input_size   flag using the shell variable you set earlier.\"\n  \n  \n  \nshould probably be:\n  \n\n  \n\"**If**   you chose a mobilenet that takes a smaller input size, then be to **sure** set the   --input_size   flag using the shell variable you set earlier.\"\n  \n\n  \nBut people can probably understand it regardless. Thank you.\n  \n  \n\n  \n>   \n> On May 29, 2018 at 3:49 PM,  <Mark Daoust (mailto:notifications@github.com)>  wrote:\n>   \n>   \n>   \n>\n> Fixed.Thanks.\n>\n>   \n>\n> \u2014\n>  You are receiving this because you authored the thread.\n>  Reply to this email directly,   view it on GitHub (https://github.com/tensorflow/tensorflow/issues/19630#issuecomment-392937949), or   mute the thread (https://github.com/notifications/unsubscribe-auth/AY6WJVtA_-32Fx-SnERBKVCi4oGr8dkUks5t3bQ8gaJpZM4UR46T).\n>\n>       \n>   \n  \n  \n     ", "Right. Fixed. Thanks Again."]}, {"number": 19629, "title": "Error in \"TensorFlow for Poets", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 19628, "title": "Feature Request: Add normalizer_fn to sequence_numeric_column.", "body": "### System information\r\n- **Have I written custom code**: no\r\n- **OS Platform and Distribution**: macOS Sierra 10.12.6\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version**: N/A\r\n- **GCC/Compiler version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nCurrently, `sequence_numeric_column` does not support a `normalizer_fn`, wondering if this could be added in like in `numeric_column`.", "comments": ["Added a PR #19649 for the support."]}, {"number": 19627, "title": "tf.image.resize_bilinear vs cv2.resize", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nNot to compare with opencv, but ... when the scale factor is larger, the unchanged part at right-bottom is larger. I found this a little bothersome. Set `align_corners=True` is not always reasonable because the four corners are not always supposed to be fixed in the corner.\r\n\r\nSo is there anyway to make it a little more \"symmetry\"?\r\n\r\nCode to reproduce:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport cv2\r\nnp.set_printoptions(precision=3)\r\nresize_shape = (10, 10)\r\n\r\na = np.ones((1, 2, 2, 1), dtype=np.float32)\r\na[0, 0, 0, 0] = 5.0\r\na[0, 1, 1, 0] = 5.0\r\n\r\nb = tf.constant(a, dtype=tf.float32)\r\nc = tf.image.resize_bilinear(b, resize_shape)\r\n\r\nwith tf.Session() as sess:\r\n    np_c = sess.run(c)\r\n    print np_c[0, :, :, 0]\r\n\r\nprint cv2.resize(a[0], resize_shape, interpolation=cv2.INTER_LINEAR)\r\n```\r\nObtained results:\r\n```\r\n# tf.image.resize_bilinear\r\n[[ 5.    4.2   3.4   2.6   1.8   1.    1.    1.    1.    1.  ]\r\n [ 4.2   3.72  3.24  2.76  2.28  1.8   1.8   1.8   1.8   1.8 ]\r\n [ 3.4   3.24  3.08  2.92  2.76  2.6   2.6   2.6   2.6   2.6 ]\r\n [ 2.6   2.76  2.92  3.08  3.24  3.4   3.4   3.4   3.4   3.4 ]\r\n [ 1.8   2.28  2.76  3.24  3.72  4.2   4.2   4.2   4.2   4.2 ]\r\n [ 1.    1.8   2.6   3.4   4.2   5.    5.    5.    5.    5.  ]\r\n [ 1.    1.8   2.6   3.4   4.2   5.    5.    5.    5.    5.  ]\r\n [ 1.    1.8   2.6   3.4   4.2   5.    5.    5.    5.    5.  ]\r\n [ 1.    1.8   2.6   3.4   4.2   5.    5.    5.    5.    5.  ]\r\n [ 1.    1.8   2.6   3.4   4.2   5.    5.    5.    5.    5.  ]]\r\n# cv2.resize\r\n[[ 5.    5.    5.    4.2   3.4   2.6   1.8   1.    1.    1.  ]\r\n [ 5.    5.    5.    4.2   3.4   2.6   1.8   1.    1.    1.  ]\r\n [ 5.    5.    5.    4.2   3.4   2.6   1.8   1.    1.    1.  ]\r\n [ 4.2   4.2   4.2   3.72  3.24  2.76  2.28  1.8   1.8   1.8 ]\r\n [ 3.4   3.4   3.4   3.24  3.08  2.92  2.76  2.6   2.6   2.6 ]\r\n [ 2.6   2.6   2.6   2.76  2.92  3.08  3.24  3.4   3.4   3.4 ]\r\n [ 1.8   1.8   1.8   2.28  2.76  3.24  3.72  4.2   4.2   4.2 ]\r\n [ 1.    1.    1.    1.8   2.6   3.4   4.2   5.    5.    5.  ]\r\n [ 1.    1.    1.    1.8   2.6   3.4   4.2   5.    5.    5.  ]\r\n [ 1.    1.    1.    1.8   2.6   3.4   4.2   5.    5.    5.  ]]\r\n```\r\n\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "I think this should be reopened. The sampling logic of this kernel's implementation is simply incorrect which is certainly a bug when the name of the operation is called 'Resize Bilinear'. \r\n\r\nAlso, at a cursory glance there seems to be little reason that FP16 isn't supported - the templated input type is hard-casted to and written out as a float. Couldn't this templated type be propagated to the output? This disconnect incites an anomalous perf issue when converting models from FP32 to FP16.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bilinear_op_gpu.cu.cc"]}, {"number": 19626, "title": "FailedPreconditionError: Table already initialized when use Feature Column in Eager mode", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Latest Mac OS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-2-443ffe416ebf> in <module>()\r\n      7 \r\n      8 features = df.to_dict('series')\r\n----> 9 input_layer = tf.feature_column.input_layer(features, columns)\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py in input_layer(features, feature_columns, weight_collections, trainable, cols_to_vars)\r\n    275   \"\"\"\r\n    276   return _internal_input_layer(features, feature_columns, weight_collections,\r\n--> 277                                trainable, cols_to_vars)\r\n    278 \r\n    279 \r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py in _internal_input_layer(features, feature_columns, weight_collections, trainable, cols_to_vars, scope)\r\n    200             builder,\r\n    201             weight_collections=weight_collections,\r\n--> 202             trainable=trainable)\r\n    203         num_elements = column._variable_shape.num_elements()  # pylint: disable=protected-access\r\n    204         batch_size = array_ops.shape(tensor)[0]\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py in _get_dense_tensor(***failed resolving arguments***)\r\n   3300     # Feature has been already transformed. Return the intermediate\r\n   3301     # representation created by _transform_feature.\r\n-> 3302     return inputs.get(self)\r\n   3303 \r\n   3304   def _get_sequence_dense_tensor(\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py in get(self, key)\r\n   2098     column = key\r\n   2099     logging.debug('Transforming feature_column %s.', column)\r\n-> 2100     transformed = column._transform_feature(self)  # pylint: disable=protected-access\r\n   2101     if transformed is None:\r\n   2102       raise ValueError('Column {} is not supported.'.format(column.name))\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py in _transform_feature(self, inputs)\r\n   3229       ValueError: if input rank is not known at graph building time.\r\n   3230     \"\"\"\r\n-> 3231     id_weight_pair = self.categorical_column._get_sparse_tensors(inputs)  # pylint: disable=protected-access\r\n   3232     id_tensor = id_weight_pair.id_tensor\r\n   3233     weight_tensor = id_weight_pair.weight_tensor\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py in _get_sparse_tensors(self, inputs, weight_collections, trainable)\r\n   2872   def _get_sparse_tensors(\r\n   2873       self, inputs, weight_collections=None, trainable=None):\r\n-> 2874     return _CategoricalColumn.IdWeightPair(inputs.get(self), None)\r\n   2875 \r\n   2876 \r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py in get(self, key)\r\n   2098     column = key\r\n   2099     logging.debug('Transforming feature_column %s.', column)\r\n-> 2100     transformed = column._transform_feature(self)  # pylint: disable=protected-access\r\n   2101     if transformed is None:\r\n   2102       raise ValueError('Column {} is not supported.'.format(column.name))\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py in _transform_feature(self, inputs)\r\n   2863         num_oov_buckets=self.num_oov_buckets,\r\n   2864         dtype=key_dtype,\r\n-> 2865         name='{}_lookup'.format(self.key)).lookup(input_tensor)\r\n   2866 \r\n   2867   @property\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py in index_table_from_tensor(vocabulary_list, num_oov_buckets, default_value, hasher_spec, dtype, name)\r\n   1097           name=\"table_init\")\r\n   1098       table = HashTable(\r\n-> 1099           init, default_value, shared_name=shared_name, name=hash_table_scope)\r\n   1100     if num_oov_buckets:\r\n   1101       table = IdTableWithHashBuckets(\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py in __init__(self, initializer, default_value, shared_name, name)\r\n    277           name=scope)\r\n    278 \r\n--> 279       super(HashTable, self).__init__(table_ref, default_value, initializer)\r\n    280 \r\n    281 \r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py in __init__(self, table_ref, default_value, initializer)\r\n    169         default_value, dtype=self._value_dtype)\r\n    170     self._default_value.get_shape().merge_with(tensor_shape.scalar())\r\n--> 171     self._init = initializer.initialize(self)\r\n    172 \r\n    173   @property\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py in initialize(self, table)\r\n    348                             self._values)) as scope:\r\n    349       init_op = gen_lookup_ops.initialize_table_v2(\r\n--> 350           table.table_ref, self._keys, self._values, name=scope)\r\n    351     ops.add_to_collection(ops.GraphKeys.TABLE_INITIALIZERS, init_op)\r\n    352     return init_op\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_lookup_ops.py in initialize_table_v2(table_handle, keys, values, name)\r\n    404       else:\r\n    405         message = e.message\r\n--> 406       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n    407 \r\n    408 \r\n\r\n/usr/local/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nFailedPreconditionError: Table already initialized. [Op:InitializeTableV2] name: input_layer/Col2_indicator/Col2_lookup/hash_table/table_init/\r\n```\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\n\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\ndf = pd.DataFrame([['A1', 'A2'], ['B1', 'B2'], ['C1', 'C2']], columns=['Col1', 'Col2'])\r\n\r\ncolumns = [\r\n    tf.feature_column.indicator_column(\r\n        tf.feature_column.categorical_column_with_vocabulary_list(col, df[col].unique())) for col in df.columns\r\n]\r\n\r\nfeatures = df.to_dict('list')\r\ninput_layer = tf.feature_column.input_layer(features, columns)\r\n```", "comments": ["Maybe @alextp has some idea on how to avoid re-initialization of lookup tables in eager mode.", "Seems `embedding_column` is not supported in eager mode 68ffc85450d328cf9e1323dd0021c6671110c5fb.\r\n\r\nHow about `indicator_column` and `categorical_column_*`? Are they supported in eager mode?", "@rohan100jain you were working on this?>", "This is also the case when using multiple `tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(keys, values), -1,)` calls in Eager mode, even when assigning different names in each method.", "I'm working on a solution to this problem. I'd estimate a couple of weeks before it gets out.", "@rohan100jain Thanks!", "I see the error when using a simple\r\nvocab_table.lookup() operation. Is there any workaround this to run it in eager mode?", "@rohan100jain Any updates from your side?", "Closing for now as I do not have time to reproduce it."]}, {"number": 19625, "title": "Raw pointer member variables of tflite::Interpreter are not initialized or released properly.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: 0.13.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)\r\n- **CUDA/cuDNN version**: 9/7\r\n- **GPU model and memory**: K80\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nbazel build -c opt --copt=\"-D TFLITE_PROFILING_ENABLED\" tensorflow/contrib/lite/tools:benchmark_model\r\n./bazel-bin/tensorflow/contrib/lite/tools/benchmark_model --graph=model.tflite\r\n```\r\noutput:\r\n```\r\nGraph: [model.tflite]\r\nNum runs: [50]\r\nInter-run delay (seconds): [-1.0]\r\nNum threads: [1]\r\nWarmup runs: [1]\r\nUse nnapi : [0]\r\nInitialized session in 0.002312s\r\nRunning benchmark for 1 ddditerations: \r\nSegmentation fault (core dumped)\r\n```\r\n\r\nThere are two raw pointer member variables within tflite::Interpreter.\r\n- [`profiling::Profiler* profiler_`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/interpreter.h#L576) is not initialized. \r\n[It may cause segmentation fault](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/profiling/profiler.h#L134) if `profiler_` is initialized with a nonzero value and users don't set it by `tflite::Interpreter::SetProfiler`.\r\n\r\n- [`ErrorReporter* error_reporter_`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/interpreter.h#L545)\r\nThe default error reporter pointed by `error_reporter_` will not be released.\r\n", "comments": ["Nagging Assignees @petewarden, @aselle, @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @aselle, @andrehentz: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @aselle, @andrehentz: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The raw pointer member variables are not owned by the Interpreter, but good point about the the profiler_ pointer, I am going to send a patch.", "Fix is already merged, closing."]}, {"number": 19624, "title": "Build issue Linking of rule '//tensorflow/cc:ops/nn_ops_gen_cc' failed (Exit 1)", "body": "Hi, \r\n\r\nI've encountered an issue occurred during build tf v1.6.0, the error message is like follows:\r\n\r\n    /path/to/tensorflow/tensorflow/cc/BUILD:422:1: Linking of rule '//tensorflow/cc:ops/nn_ops_gen_cc' failed (Exit 1)\r\n\r\nAny suggestion would be appreciated! Many thanks! \r\n\r\n### System information\r\n- **OS Platform**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from source**: yes\r\n- **TensorFlow version**: v1.6.0\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: 7.0.5\r\n- **GPU model and memory**: P100\r\n- **Exact command to reproduce**: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --action_env=\"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\"`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 19623, "title": "No module named '_pywrap_tensorflow_internal", "body": ">>>import keras\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"D:\\Download\\python3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 985, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 968, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 957, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 938, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm 2017.3.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 20, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"D:\\Download\\python3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm 2017.3.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 20, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm 2017.3.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 20, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm 2017.3.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 20, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm 2017.3.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 20, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\keras\\backend\\__init__.py\", line 84, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm 2017.3.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 20, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 6, in <module>\r\n    from tensorflow.python.training import moving_averages\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm 2017.3.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 20, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm 2017.3.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 20, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"D:\\Download\\python3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 985, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 968, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 957, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 938, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Program Files\\JetBrains\\PyCharm 2017.3.3\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 20, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Snow\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"D:\\Download\\python3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": [">>>import keras \r\nthere are many errors.  How?  Please help me.  ", "I also have this issue. Can somebody explain this?", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Also, does #9469 help?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "BIG ISSUE\r\n![Capture](https://user-images.githubusercontent.com/45896078/61205654-21cb6480-a70a-11e9-9652-08a3bcd4825c.PNG)\r\n"]}, {"number": 19622, "title": "SIGFPE in Conv2d when kernel size is 0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: via pip\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:N/A\r\n\r\n### Describe the problem\r\nWhen tf.nn.conv2d recives kernel with spatial dims 0, 0 or tf.layers.conv2d kernel_size is 0 tensorflow will silently crash leaving only `Process finished with exit code 136 (interrupted by signal 8: SIGFPE)`\r\nThat is super frustrating and prevents searching for errors in model construction code.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Can anyone take a look at this?", "@Luonic Would you mind to share your minimal code sample that triggers the crash?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I'm closing for now from lack of activity but please reopen if you can provide a repro thanks!"]}, {"number": 19621, "title": "[Bug] AdamOptimizer: No Exception on invalid input", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: 4G/Quadro M1000M\r\n- **Exact command to reproduce**: -\r\n\r\n\r\n### Describe the problem\r\nThe graph can be executed with AdamOptimizer, thus the graph input is invalid. In the example an out-of-bound embedding  index is passed. Other tested optimizer (RMSP,Ada) yield: InvalidArgumentError: indices[0,0] = 10 is not in [0, 10)\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef embed_helper(inputs, size, dim, name=None):\r\n    std = np.square(2. / dim)\r\n    test_emb = tf.Variable(tf.random_uniform([size, dim], -std, std), name=name)\r\n    return tf.nn.embedding_lookup(test_emb, inputs)\r\n\r\nnum_factors = 16\r\nnum_embed = 10\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    x = tf.placeholder(tf.int32, shape=(None, 1))\r\n    x_emb = embed_helper(x, num_embed, num_factors, name=None)\r\n    very_complicated_net = tf.square(tf.subtract(x_emb, tf.constant(3.)))\r\n\r\n    # RMSP and the other tested optimizer yield InvalidArgumentError - AdamOptimizer does not\r\n    # -> InvalidArgumentError (see above for traceback): indices[0,0] = 10 is not in [0, 10)\r\n    #opt = tf.train.RMSPropOptimizer(learning_rate=0.01)\r\n    opt = tf.train.AdamOptimizer(learning_rate=0.01)\r\n\r\n    graph_step = opt.minimize(very_complicated_net)\r\n    init = tf.global_variables_initializer()\r\n\r\nsession = tf.Session(config=None, graph=graph)\r\nsession.run(init)\r\n\r\n# 10 is not in [0,1,..,9]\r\nindex_batch_out_of_bound = np.array([10]).reshape(-1, 1)\r\n\r\nx_feed_dict = {\r\n    x: index_batch_out_of_bound\r\n}\r\n\r\nsession.run(graph_step, x_feed_dict)\r\n```\r\n", "comments": ["Consistency would be good. Feel free to send a fix. Thanks!", "I would like to work on this. Could you please guide me?", "I am working on this bug, found it happend after adding RMS optimizer: \"RMSProp/update_Variable/SparseApplyRMSProp/xx\"\r\nin the graph, the embedding operator then was bound to CPU though it has been bound to GPU.\r\nbefore add rms:\r\nembedding_lookup: (Gather): /job:localhost/replica:0/task:0/device:GPU:0\r\nafter add rms:\r\nembedding_lookup: (Gather): /job:localhost/replica:0/task:0/device:CPU:0\r\nAccording to the official [document](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup), it will **definitely** raise out of boundary error when you run it on CPU rather than GPU.\r\nIt is a weird problem, but compulsive bind the operation into GPU may solve the problem. Any idea how to do this? @drpngx \r\nCode place :python/framework/ops.py line 3210:\"self._add_op(op)\"\r\n\r\n", "[relevant](https://github.com/tensorflow/tensorflow/issues/3797)\r\n[relevant2](https://github.com/tensorflow/tensorflow/issues/2314)", "So there is code to colocate gradients and variables, is that causing the problem?\r\n\r\nAnyway, the way I would do this is to try and compare the working optimizer vs the non-working ones and see what the difference is. I'm not sure right now what exactly that is. Even if it's bound on another device it shouldn't return different results.", "It is the main reason that SparseApplyXXX can not run on GPU which compel 'embedding_lookup' to run on CPU and cause the problem.\r\nMaybe we can alter the embedding_lookup CPU implementation to conform with its GPU version. Or we can rewrite 'SparseApplyAdagrad' and 'SparseApplyRMSProp' so that they can run on GPU.\r\n@drpngx ", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Could you please upgrade your Tensorflow version to 2.x and your code to version compatibility and check for the error message again. \r\nRefer [tf.keras.optimizers.Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) for more details.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 19620, "title": "Tensorflow Lite NNAPI doesn't work", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android armeabi-v7a\r\n- **TensorFlow installed from (source or binary)**: no\r\n- **TensorFlow version (use command below)**: no\r\n- **Python version**:  no\r\n- **Bazel version (if compiling from source)**: no\r\n- **GCC/Compiler version (if compiling from source)**: no\r\n- **CUDA/cuDNN version**: no\r\n- **GPU model and memory**: no\r\n- **Exact command to reproduce**: no\r\n\r\n### Describe the problem\r\nI build a graph with simple matmul and reshap ops, and compile android apk with nigthly-build `org.tensorflow:tensorflow-lite:0.1.7'`. Then run the app in Android 8.1 devices.\r\nIf I `setUseNNAPI(false)`, its' result is rigth. However, when I `setUseNNAPI(true)`, it excute very fast and its result is wrong (it outputs whatever I feed inputs).\r\n\r\nCan anyone help me ?", "comments": ["When it runs, the device GPU utilization is 0", "check your device indeed support NNAPI.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@efeiefei : The device probably doesn't support NNAPI, also even if you use NNAPI it could be the case that NNAPI is using CPU fallback so there is no speed up.\r\nRegarding results being wrong, we recently fixed a few bugs in NNAPI delegation, make sure you are using the latest build.", "Can you add some Toast message (for Tensorflow Lite samples) that will tell if device supports NNAPI?", "I have the same problem with: `org.tensorflow:tensorflow-lite:1.10.0` ...", "For me it just crashes on Xiaomi 8.1 Redmi Note 5. Mb device doesn't support NNAPI - suitable hardware needed for that.. I guess. But why does it crash? Why not just use CPU fallback?\r\n\r\nAlso would be great to have some function which will tell us if NNAPI is supported by device. \r\n\r\n```\r\n2018-10-08 17:30:36.273 21069-21493/com.tes E/tflite: Custom operations are not supported when using NNAPI.\r\n2018-10-08 17:30:36.273 21069-21493/com.tes E/tflite: Returning error since TFLite returned failure nnapi_delegate.cc:745.\r\n2018-10-08 17:30:36.273 21069-21493/com.tes E/tflite: Failed to build graph for NNAPI\r\n2018-10-08 17:30:36.274 21069-21493/com.tes E/AndroidRuntime: FATAL EXCEPTION: Thread-4\r\n    Process: com.tes, PID: 21069\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:130)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:216)\r\n        at com.tes.Lite.TFLiteObjectDetection.recognizeImage(TFLiteObjectDetection.java:226)\r\n        at com.tes.Lite.NeuralNet.detect(NeuralNet.java:82)\r\n        at com.tes.camera.JavaCameraView$CameraWorker.run(JavaCameraView.java:395)\r\n        at java.lang.Thread.run(Thread.java:764)\r\n\r\n```\r\n\r\np.s. I use the latest nightly version (as I understand): `implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'`", "@anonym24 - Where you able to fix it? I face the same error\r\n", "@anonym24 no, I just don't use NNAPI (GPU acceleration), seems it works only with Google Devices (Nexus, Pixel), useless"]}, {"number": 19619, "title": "TensorRT: Large memory consumption on SSD-like graphs [Feature Request/Discussion]", "body": "Hi,\r\n\r\n`tf.contrib.tensorrt` currently automatically segments a given graph into subgraphs which can be fused into TensorRT nodes. This approach works well with networks having linear topology, e.g., CNN classifiers like VGG or ResNet-N: a single node is created, taking an image batch as input and producing a single tensor with predictions.\r\n\r\nThe situation with SSD-like networks is less favorable. Such a network has a topology of the form a->b->c->d->e->f, a->a_1, a->a_2, b->b_1, b->b_2, ..., f->f_1, f->f2, where a->b->c->d->e->f is a feature extractor and a_1, a_2, b_1, b_2, ..., f_1, f_2 are branches stemming from the feature extractor and predicting, e.g., classes and exact locations of the objects in the predefined anchor boxes. The `tf.contrib.tensorrt`'s segmentation algorithm on such a graph selects a subgraph consisting of all the feature extractor's nodes, plus maybe parts of the branches (e.g., convolutions computing the logits, but not the argmaxes computing the class ids; TensorRT as of version 3 does not support argmax). As a result, we get a huge operation with lots of outputs (e.g., all logits and all raw location amendments, before argmaxes, reshapes, or NCHW->NHWC transpositions), which all must simultaneously fit in GPU memory at the moment of TensorRT's op's completion.\r\n\r\nWhen the same graph is computed on TensorFlow, this high peak in memory usage can be (and seems to be) avoided: TensorFlow can compute the feature extractor up to a next level, compute the branches, copy the results computed in the branches into host RAM, go to the next level, and so on.\r\n\r\nThis means, `tf.contrib.tensorrt`-optimized graph can use (and seems to actually use in my experiments) significantly more GPU memory than the original graph, which can lead (and seems to lead in my experiments) to out-of-memory errors.\r\n\r\nOne workaround that I tried was to add to `tf.contrib.tensorrt.create_inference_graph` a parameter for specifying a list of subgraphs which should be independently segmented into subsubgraphs for fusion into TensorRT nodes. I passed individual levels of the feature extractor plus the branches at this level as such subgraphs. This reduced the memory use by TensorRT by a factor of around two and fixed the out-of-memory errors in my case.\r\n\r\nShould such a parameter then maybe added to the mainline `tf.contrib.tensorrt`?\r\nMaybe you have a better idea of avoiding a high GPU memory consumption in SSD-like graphs?\r\nComments, ideas are welcome.\r\n\r\nI guess, I should invite @drpngx, @samikama, @jjsjann123 to the discussion.\r\n\r\nIn case it matters, my experience comes from the experiments with TensorFlow 1.8, TensorRT-3.0.4 running on Ubuntu 16.04 (AMD64) with GTX 1080 Ti.\r\n\r\nThanks!", "comments": ["/CC @tfboyd @zheng-xq ", "@yegord, Thanks for the investigation. Would it be possible to share the graph so that we can investigate and improve our segmenter? Most common OOM issue is due to reservation for TensorRT which will be reduced with TensorRT4.0 GA since we start to share the TensorFlow's allocator and only workspace parameter needs to be tuned now. We are also restructuring the TFTRT workflow which should improve things. In upcoming updates we will add an option to specify placement of ops in to the TRT segments and users will be able to selectively keep certain nodes outside segments.\r\n\r\nThanks,\r\nSami\r\n ", "Please find a minimal example here: https://yadi.sk/d/797LMYGT3Wi2yy\r\n`./minimal_example.py --mode tf` gives me `5487MiB / 11172MiB` GPU memory usage on 1080 Ti, according to nvidia-smi.\r\n`./minimal_example.py --mode trt` gives me `9987MiB / 11172MiB`, which is 80% larger than with plain TensorFlow, is over the max_workspace_size_bytes (which is around 6G, not sure if all of that is used by TensorRT, though) and is close to the physical memory limit.\r\n\r\nSpecifying placements of ops into TRT segments sounds like what I proposed here.\r\nSharing of the allocator also sounds great.\r\nI guess, I should wait for the TensorRT 4 based release and repeat the experiment.\r\nCurrently, almost twofold memory usage is a blocker for the use of `tf.contrib.tensorrt` for me.", "Nagging Assignees @samikama, @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @samikama: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@yegord Could you please try with TRT4.0? You should not need to set gpu allocation fraction when using TRT4.0 and workspace will directly be allocated from TF memory. It should improve the things for you", "@samikama Thanks for the update! Which TensorFlow version should I test TRT 4 with? Master (98b9a4e45a559217bc89960b889c130af95c1d1a) crashes with\r\n```\r\n$ ./minimal_example.py --mode trt\r\n2018-07-14 01:17:30.655903: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1\r\nTraceback (most recent call last):\r\n  File \"./minimal_example.py\", line 81, in <module>\r\n    main()\r\n  File \"./minimal_example.py\", line 44, in main\r\n    minimum_segment_size=3\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 152, in create_inference_graph\r\n    int(msg[0]))\r\ntensorflow.python.framework.errors_impl.NotFoundError: No attr named 'shape' in NodeDef:\r\n         [[Node: placeholders/image_0 = Placeholder[dtype=DT_UINT8]()]] for 'placeholders/image_0' (op: 'Placeholder') with input shapes: .\r\n```\r\n(This is the minimal example mentioned in https://github.com/tensorflow/tensorflow/issues/19619#issuecomment-393221941.)\r\n", "@yegord, it looks like an issue with grapplers layout optimizer \r\n```\r\n2018-07-13 20:50:26.902178: I tensorflow/core/grappler/optimizers/layout_optimizer.cc:2187] Infer shape return status: Not found: No attr named 'shape' in NodeDef:\r\n\t [[Node: placeholders/image_0 = Placeholder[dtype=DT_UINT8]()]] for 'placeholders/image_0' (op: 'Placeholder') with input shapes: .\r\n2018-07-13 20:50:26.938333: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:318] layout: Not found: No attr named 'shape' in NodeDef:\r\n\t [[Node: placeholders/image_0 = Placeholder[dtype=DT_UINT8]()]] for 'placeholders/image_0' (op: 'Placeholder') with input shapes: .\r\n2018-07-13 20:50:27.010038: I tensorflow/core/grappler/costs/graph_properties.cc:1250] Propagating 2002 new shapes through 0 loops and 0 resources\r\n```\r\nWe are returning that error", "@yegord, please wait for #20794 and try \r\n```python\r\n#!/usr/bin/env python\r\n\r\nimport argparse\r\nimport subprocess\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport six\r\n\r\nfrom tensorflow.core.protobuf import config_pb2 as cpb2\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2 as rwpb2\r\n\r\nfrom tensorflow.contrib import tensorrt as trt\r\n\r\nNUM_IMAGES = 3\r\n\r\nINPUT_TENSORS = [u'placeholders/image_0', u'placeholders/image_1', u'placeholders/image_2', u'placeholders/num_images', u'placeholders/blend_coeff']\r\n\r\nOUTPUT_TENSORS = [u'detections/center_x', u'detections/center_y', u'detections/width', u'detections/height', u'detections/width_3d', u'detections/height_3d', u'detections/depth_3d', u'detections/class_id', u'detections/probability', u'detections/yaw', u'detections/properties/tl_rotation/class_id', u'detections/properties/tl_type/class_id', u'detections/properties/tl_road_tl_state/class_id', u'detections/properties/tl_bicycle_tl_state/class_id', u'detections/properties/tl_pedestrian_tl_state/class_id', u'detections/properties/tl_other_tl_state/class_id', u'detections/properties/tl_left_section/class_id', u'detections/properties/tl_left_section_state/class_id', u'detections/properties/tl_right_section/class_id', u'detections/properties/tl_right_section_state/class_id', u'segmentations/sdc1/0', u'segmentations/sdc1/1', u'segmentations/sdc1/2', u'visualizations/sdc1/0', u'visualizations/sdc1/1', u'visualizations/sdc1/2']\r\n\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser()\r\n\r\n    parser.add_argument(\r\n        '--mode',\r\n        choices=['tf', 'trt'],\r\n        required=True,\r\n        help='Evaluator to use.')\r\n\r\n    args = parser.parse_args()\r\n\r\n    graph_def = tf.GraphDef()\r\n    with tf.gfile.GFile('ssd-tensorflow.pb', 'rb') as f:\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    with tf.Graph().as_default():\r\n        tf.import_graph_def(graph_def, name='')\r\n        run_graph(mode=args.mode)\r\n\r\ndef run_graph(ntimes=100, mode='trt'):\r\n    image = np.zeros((1024, 768, 3), dtype=np.uint8)\r\n    feed_dict = {\r\n        tensor_name: image\r\n        for tensor_name in INPUT_TENSORS[:NUM_IMAGES]\r\n    }\r\n    feed_dict[u'placeholders/num_images'] = NUM_IMAGES\r\n    feed_dict[u'placeholders/blend_coeff'] = 0.5\r\n\r\n    feed_dict = {\r\n        tf.get_default_graph().get_operation_by_name(key).outputs[0]: value\r\n        for key, value in six.iteritems(feed_dict)\r\n    }\r\n\r\n    output_tensors = [\r\n        tf.get_default_graph().get_operation_by_name(tensor_name).outputs[0]\r\n        for tensor_name in OUTPUT_TENSORS\r\n    ]\r\n    opt_config = rwpb2.RewriterConfig()\r\n    opt_config.meta_optimizer_iterations = opt_config.ONE\r\n    opt_config.optimizers.extend([\"constfold\", \"layout\"])\r\n    custom_op = opt_config.custom_optimizers.add()\r\n    custom_op.name = \"TensorRTOptimizer\"\r\n    custom_op.parameter_map[\"minimum_segment_size\"].i = 10\r\n    custom_op.parameter_map[\"precision_mode\"].s = \"FP32\"\r\n    custom_op.parameter_map[\"max_batch_size\"].i =NUM_IMAGES\r\n    custom_op.parameter_map[\"is_dynamic_op\"].b = True\r\n    custom_op.parameter_map[\"max_workspace_size_bytes\"].i = 1 << 25\r\n    \r\n    graph_options = cpb2.GraphOptions(rewrite_options=opt_config)\r\n    if mode == 'trt':\r\n        config = tf.ConfigProto(graph_options=graph_options)\r\n    else:\r\n        config = tf.ConfigProto()\r\n\r\n    with tf.Session(config=config) as session:\r\n        for i in six.moves.xrange(ntimes):\r\n            session.run(output_tensors, feed_dict)\r\n        subprocess.check_call(['nvidia-smi'], close_fds=True)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```", "do not send this email for me.\n\nAt 2018-07-14 12:30:52, \"Sami Kama\" <notifications@github.com> wrote:\n\n\n@yegord, please wait for #20794 and try\n\n#!/usr/bin/env pythonimport argparse\nimport subprocess\nimport tensorflow as tf\nimport numpy as np\nimport six\n\nfrom tensorflow.core.protobuf import config_pb2 as cpb2\nfrom tensorflow.core.protobuf import rewriter_config_pb2 as rwpb2\n\nfrom tensorflow.contrib import tensorrt as trt\n\nNUM_IMAGES=3INPUT_TENSORS= [u'placeholders/image_0', u'placeholders/image_1', u'placeholders/image_2', u'placeholders/num_images', u'placeholders/blend_coeff']\n\nOUTPUT_TENSORS= [u'detections/center_x', u'detections/center_y', u'detections/width', u'detections/height', u'detections/width_3d', u'detections/height_3d', u'detections/depth_3d', u'detections/class_id', u'detections/probability', u'detections/yaw', u'detections/properties/tl_rotation/class_id', u'detections/properties/tl_type/class_id', u'detections/properties/tl_road_tl_state/class_id', u'detections/properties/tl_bicycle_tl_state/class_id', u'detections/properties/tl_pedestrian_tl_state/class_id', u'detections/properties/tl_other_tl_state/class_id', u'detections/properties/tl_left_section/class_id', u'detections/properties/tl_left_section_state/class_id', u'detections/properties/tl_right_section/class_id', u'detections/properties/tl_right_section_state/class_id', u'segmentations/sdc1/0', u'segmentations/sdc1/1', u'segmentations/sdc1/2', u'visualizations/sdc1/0', u'visualizations/sdc1/1', u'visualizations/sdc1/2']\n\n\ndefmain():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        '--mode',\n        choices=['tf', 'trt'],\n        required=True,\n        help='Evaluator to use.')\n\n    args = parser.parse_args()\n\n    graph_def = tf.GraphDef()\n    with tf.gfile.GFile('ssd-tensorflow.pb', 'rb') as f:\n        graph_def.ParseFromString(f.read())\n\n    with tf.Graph().as_default():\n        tf.import_graph_def(graph_def, name='')\n        run_graph(mode=args.mode)\n\ndefrun_graph(ntimes=100, mode='trt'):\n    image = np.zeros((1024, 768, 3), dtype=np.uint8)\n    feed_dict = {\n        tensor_name: image\n        for tensor_name inINPUT_TENSORS[:NUM_IMAGES]\n    }\n    feed_dict[u'placeholders/num_images'] =NUM_IMAGES\n    feed_dict[u'placeholders/blend_coeff'] =0.5\n\n    feed_dict = {\n        tf.get_default_graph().get_operation_by_name(key).outputs[0]: value\n        for key, value in six.iteritems(feed_dict)\n    }\n\n    output_tensors = [\n        tf.get_default_graph().get_operation_by_name(tensor_name).outputs[0]\n        for tensor_name inOUTPUT_TENSORS\n    ]\n    opt_config = rwpb2.RewriterConfig()\n    opt_config.meta_optimizer_iterations = opt_config.ONE\n    opt_config.optimizers.extend([\"constfold\", \"layout\"])\n    custom_op = opt_config.custom_optimizers.add()\n    custom_op.name =\"TensorRTOptimizer\"\n    custom_op.parameter_map[\"minimum_segment_size\"].i =10\n    custom_op.parameter_map[\"precision_mode\"].s =\"FP32\"\n    custom_op.parameter_map[\"max_batch_size\"].i =NUM_IMAGES\n    custom_op.parameter_map[\"is_dynamic_op\"].b =True\n    custom_op.parameter_map[\"max_workspace_size_bytes\"].i =1<<25\n    \n    graph_options = cpb2.GraphOptions(rewrite_options=opt_config)\n    if mode =='trt':\n        config = tf.ConfigProto(graph_options=graph_options)\n    else:\n        config = tf.ConfigProto()\n\n    with tf.Session(config=config) as session:\n        for i in six.moves.xrange(ntimes):\n            session.run(output_tensors, feed_dict)\n        subprocess.check_call(['nvidia-smi'], close_fds=True)\n\n\nif__name__=='__main__':\n    main()\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.", "As of master (d51aff27ac2123fd7093b685be8b0e5200f95029):\r\nThe vanilla graph uses 5499MiB.\r\nTRT-optimized graph uses 1753MiB.\r\n\r\nAll numbers are according to nvidia-smi, the test script is the one from https://github.com/tensorflow/tensorflow/issues/19619#issuecomment-404997677 with `config.gpu_options.allow_growth = True` added.\r\nThe result is definitely very positive, assuming there is no error in the measurements and given the optimized graph computes the same things as the original one (I did not check this yet).\r\n\r\nThanks a lot for your help!\r\nFeel free to close this task if you do not need it anymore.\r\nI will come back if I have any further problems on my way.", "Is the old way of creating an optimized graph with\r\n```\r\n    graph_def = trt.create_inference_graph(\r\n        input_graph_def=graph_def,\r\n        outputs=output_node_names,\r\n        max_batch_size=num_cameras,\r\n        max_workspace_size_bytes=1 << 25, \r\n        precision_mode=precision_mode,\r\n        minimum_segment_size=10,  # minimum number of nodes in an engine,\r\n    )   \r\n```\r\nstill supposed to work?\r\n\r\nI still get\r\n```\r\nTraceback (most recent call last):\r\n  File \"ros/src/perception/detection/tools/deploy.py\", line 294, in <module>\r\n    main()\r\n  File \"ros/src/perception/detection/tools/deploy.py\", line 40, in main\r\n    graph_def = _make_inference_graph(model, args.num_cameras)\r\n  File \"ros/src/perception/detection/tools/deploy.py\", line 270, in _make_inference_graph\r\n    minimum_segment_size=10,  # minimum number of nodes in an engine,\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 153, in create_inference_graph\r\n    int(msg[0]))\r\ntensorflow.python.framework.errors_impl.NotFoundError: No attr named 'shape' in NodeDef:\r\n         [[Node: placeholders/image_0 = Placeholder[dtype=DT_UINT8]()]] for 'placeholders/image_0' (op: 'Placeholder') with input shapes: .\r\n```\r\nwhen I try to call create_inference_graph, even after the fix https://github.com/tensorflow/tensorflow/pull/20794, on master (2791d58c2bbbce833a3de73503816d881a06cfe3).\r\n\r\nDo I need to run a couple of optimization passes before I call create_inference_graph (like here: https://github.com/tensorflow/tensorflow/issues/19619#issuecomment-405003062)? If yes, how do I do it outside session.run()? Maybe, there is a public manual on grappler and graph rewritings that you could recommend to read?\r\n\r\nThank you.", "@samikama Ping.\r\n\r\nIn general, can anybody here share the roadmap for TensorRT support in TensorFlow: on what time horizon is tf.contrib.tensorrt going to get a documented, relatively stable, and well-tested interface (like most other TensorFlow contrib modules have)?\r\n", "@pooyadavoodi, would you please help to take a look at this issue?", "Hi @yegord,\r\n\r\nIt looks like the problem you encountered about `No attr named 'shape' in NodeDef` is fixed. I patched the repro in https://github.com/aaroey/tensorflow/commit/f2bdf760c8a90df609ae1947c2a72843911d72eb and the script works well when building from master. Would you please help to double check?\r\n\r\nRegarding the roadmap, we're working on making the integration more stable and adding more test coverages. We planned to ship TF r1.11 with trt 4.0, which we believe will be a relatively stable version to try. But please always let us know if you encounter any problems, and that could help us to improve and make the integration better.\r\n\r\nThanks.", "> I patched the repro in aaroey@f2bdf76 and the script works well when building from master.\r\n\r\nThe script at the commit that you refer to did work on my side. What did not work was calling `create_inference_graph()` (as in the original minimal example script).\r\n\r\n> Would you please help to double check?\r\n\r\nThe original minimal example script seems to work again with 3b061fce8b9a1c867f2798d51b5375ea3a03b385. Apparently, something was fixed between 2791d58c2bbbce833a3de73503816d881a06cfe3 and 3b061fce8b9a1c867f2798d51b5375ea3a03b385 that made it work. I guess, I will make another attempt to integrate TensorRT in the near future then.\r\n\r\nThank you for the information!\r\n", "I'm closing this issue since the mentioned problem (large memory consumption) is fixed. Please feel free to let me know if there are any other questions."]}, {"number": 19618, "title": "Tensorflow custom Ops on AWS P3 instances exit with CUDA_ERROR_ILLEGAL_ADDRESS", "body": "I am trying to build my own Tensorflow Op on a AWS P3 instance (using NVIDIA's GPU Cloud and nvidia-docker) and I get the following error message:\r\n\r\n```\r\n2018-05-29 10:47:38.377439: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-05-29 10:47:38.377513: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n```\r\n\r\nTo build the Op, I had to create a coda_config.h, under /usr/local/cuda/, with the following content\r\n```\r\n#ifndef CUDA_CUDA_CONFIG_H_\r\n#define CUDA_CUDA_CONFIG_H_\r\n\r\n#define TF_CUDA_CAPABILITIES CudaVersion(\"3.0\")\r\n\r\n#define TF_CUDA_VERSION \"9.0\"\r\n#define TF_CUDNN_VERSION \"7.0.5\"\r\n\r\n#define TF_CUDA_TOOLKIT_PATH \"/usr/local/cuda-9.0\"\r\n\r\n#endif  // CUDA_CUDA_CONFIG_H_\r\n```\r\nI also had to make these changes to /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/platform/default/mutex.h\r\n\r\n```\r\n#include \"/usr/local/lib/python2.7/dist-packages/tensorflow/include/external/nsync/public/nsync_cv.h\" // <- \"nsync_cv.h\"\r\n#include \"/usr/local/lib/python2.7/dist-packages/tensorflow/include/external/nsync/public/nsync_mu.h\" // <- \"nsync_mu.h\"\r\n```\r\nI compile my source code files as follows\r\n```\r\nnvcc -std=c++11 -c -o my_op_gpu.cu.o my_op_gpu.cu.cc -I $TF_INC -I /usr/local/ -I /usr/local/lib/python2.7/dist-packages/tensorflow/include/external/nsync/public -I /usr/local/cuda/include/ -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC --expt-relaxed-constexpr\r\n\r\ng++ -std=c++11 -shared -D_GLIBCXX_USE_CXX_ABI=0 -o my_op.so my_op.cc my_op_gpu.cu.o -I $TF_INC -I /usr/local/lib/python2.7/dist-packages/tensorflow/include/external/nsync/public -D GOOGLE_CUDA=1 -L$TF_LIB -ltensorflow_framework -fPIC\r\n```\r\n\r\nwhere\r\n```\r\nTF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')\r\nTF_LIB=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')\r\n```\r\n\r\nPlease help.", "comments": ["CUDA_ERROR_ILLEGAL_ADDRESS indicates that some GPU kernel issued an out-of-bounds memory access.  You can run TF through `cuda-memcheck --binary-patching no` to determine which kernel it was.", "Thanks for the help, @cliffwoolley!\r\n\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19617, "title": "Model_Average_optimizer Use Problems in got multiple values for keyword argument 'dtype'", "body": "I used `Model_Average_optimizer` and some problems occured:<br>\r\nthe code is used more likely as `model_average_optimizer_test.py`.<br>\r\nI find this error is occured in `init ModelAverageOptimizer` to create local_step. And this problem is more likely some problems with device_setter.<br>\r\nBut I don't know why. Could someone help me?<br>\r\n\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Linux n10-044-067 4.4.0-33.bm.1-amd64 #1 SMP Thu, 22 Jun 2017 11:19:55 +0800 x86_64 GNU/Linux\r\nTensorFlow installed from: Anaconda2.5, pip install tensorflow_gpu\r\nTensorFlow version: 1.8.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: CUDA-9.0, cuDNN-7.1.2(installed in anaconda2.5 also)\r\nGPU model and memory: GeForce GTX 1080\r\nExact command to reproduce:\r\n\r\n    ~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=worker --task_id=0\r\n    ~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=worker --task_id=1\r\n    ~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=server --task_id=0\r\n\r\nYou may notice the errors below is occured when one worker init class `ModelAverageOptimizer`\r\n\r\nTraceback (most recent call last):\r\n\r\n    File \"/data00/home/wupeihao/ma_test/src/rnnlm.py\", line 112, in tf.app.run()\r\n    File \"/data00/home/wupeihao/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n    File \"/data00/home/wupeihao/ma_test/src/rnnlm.py\", line 109, in main\r\n    test()\r\n    File \"/data00/home/wupeihao/ma_test/src/rnnlm.py\", line 84, in test\r\n    interval_steps=3)\r\n    File \"/data00/home/wupeihao/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/opt/python/training/model_average_optimizer.py\", line 139, in init name=\"local_step\")\r\n    File \"/data00/home/wupeihao/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1317, in get_variable\r\n    constraint=constraint)\r\n    File \"/data00/home/wupeihao/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1079, in get_variable\r\n    constraint=constraint)\r\n    File \"/data00/home/wupeihao/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 417, in get_variable\r\n    return custom_getter(**custom_getter_kwargs)\r\n    File \"/data00/home/wupeihao/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/opt/python/training/model_average_optimizer.py\", line 92, in call\r\n    return getter(name, trainable, collections, *args, **kwargs)\r\n    TypeError: _true_getter() got multiple values for keyword argument 'dtype'\r\n\r\nThe code is below:\r\n    \r\n    import os\r\n    import time\r\n    import json\r\n    import copy\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    from tensorflow.python.framework import ops\r\n    from tensorflow.python.framework import constant_op\r\n    from tensorflow.contrib.opt.python.training import model_average_optimizer\r\n     \r\n    flags = tf.flags\r\n    flags.DEFINE_string(\"server_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\n    flags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\n    flags.DEFINE_string(\"job_name\", \"\", \"Either 'server' of 'worker'\")\r\n    flags.DEFINE_integer(\"task_id\", 0, \"Task Id for Each workers\")\r\n     \r\n    FLAGS = flags.FLAGS\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n     \r\n    def workers_ps_creator(args):\r\n        ps_hosts = args.server_hosts.split(\",\")\r\n        worker_hosts = args.worker_hosts.split(\",\")\r\n        num_workers = len(worker_hosts)\r\n     \r\n        cluster = tf.train.ClusterSpec({\"ps\": ps_hosts,\"worker\": worker_hosts})\r\n        gpu_options = tf.GPUOptions(allocator_type='BFC', allow_growth=True)\r\n        if args.job_name == \"server\":\r\n            server_def = tf.train.ServerDef(cluster=cluster.as_cluster_def(),\r\n                job_name='ps',\r\n                task_index=args.task_id,\r\n                default_session_config=tf.ConfigProto(gpu_options=gpu_options, device_count={\"GPU\":0}),\r\n                protocol=\"grpc\")\r\n        elif args.job_name == \"worker\":\r\n            server_def = tf.train.ServerDef(cluster=cluster.as_cluster_def(),\r\n                    job_name=\"worker\",\r\n                    task_index=args.task_id,\r\n                    default_session_config = tf.ConfigProto(gpu_options=gpu_options),\r\n                    protocol=\"grpc\")\r\n        server = tf.train.Server(server_def)\r\n        return server, cluster, num_workers, gpu_options\r\n     \r\n    def Model(opt):\r\n        if FLAGS.task_id == 0:\r\n            var_0 = tf.get_variable(initializer = 0.0, name = 'v0')\r\n            var_1 = tf.get_variable(initializer = 1.0, name = 'v1')\r\n            grads_0 = constant_op.constant(-1.0)\r\n            grads_1 = constant_op.constant(-1.0)\r\n        else:\r\n            var_0 = tf.get_variable(initializer = 7.0, name = 'v0')\r\n            var_1 = tf.get_variable(initializer = 8.0, name = 'v1')\r\n            grads_0 = constant_op.constant(-2.0)\r\n            grads_1 = constant_op.constant(-2.0)\r\n        train_op = opt.apply_gradients([[grads_0, var_0], [grads_1, var_1]],\r\n                global_step = tf.train.get_or_create_global_step())\r\n        return train_op\r\n     \r\n    def test():\r\n        server, cluster, num_workers, gpu_options = workers_ps_creator(FLAGS)\r\n        if FLAGS.job_name == \"server\":\r\n            server.join()\r\n        elif FLAGS.job_name == \"worker\":\r\n            is_chief = (FLAGS.task_id == 0)\r\n            #Between-graph replication\r\n            worker_device = \"/job:worker/task:%d\" % (FLAGS.task_id)\r\n            ma_custom = model_average_optimizer.ModelAverageCustomGetter(worker_device=worker_device)\r\n            from tensorflow.python.training import device_setter\r\n            with tf.device(\r\n                device_setter.replica_device_setter(\r\n                    cluster=cluster,\r\n                    worker_device=worker_device,\r\n                    ps_device=\"/job:ps\")), \\\r\n                tf.variable_scope(\"\", custom_getter=ma_custom):\r\n                #create model\r\n                lr = tf.Variable(1, trainable=False)\r\n                opt = tf.train.GradientDescentOptimizer(lr)\r\n                sync_opt = model_average_optimizer.ModelAverageOptimizer(\r\n                            opt=opt,\r\n                            num_worker=num_workers,\r\n                            ma_custom_getter=ma_custom,\r\n                            is_chief=is_chief,\r\n                            interval_steps=3)\r\n                tf.logging.info('model start')\r\n                train_model = Model(sync_opt)\r\n                tf.logging.info('model end')\r\n                ma_hook = sync_opt.make_session_run_hook()\r\n            sess_config = tf.ConfigProto(gpu_options=gpu_options)\r\n            sess_config.log_device_placement = False\r\n            sess_config.allow_soft_placement = True\r\n      \r\n            all_hooks = [ma_hook]\r\n     \r\n            tf.logging.info('Start Sess')\r\n            with tf.train.MonitoredTrainingSession(master=server.target,\r\n                    is_chief=is_chief,\r\n                    hooks=all_hooks) as sess:\r\n                tf.logging.info(\"is chief: %s, len: %s\", is_chief, num_workers)\r\n                for i in range(4):\r\n                    sess.run(train_op)\r\n                    pp1 = sess.run(tf.get_default_graph().get_tensor_by_name('v0:0'))\r\n                    pp2 = sess.run(tf.get_default_graph().get_tensor_by_name('v1:0'))\r\n                    tf.logging.info(\"%d %.2f %.2f\" % (FLAGS.task_id, pp1, pp2))\r\n            sv.stop()\r\n            tf.logging.info(\"done\")", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Sorry for delay, details are updated.", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I've updated this questions yesterday.\r\nI've no idea if there are any problems for solving this problem.", "Hi @yunyin, I am the author of ModelAverageOptimizer. Sorry for the bug. Did you solve the problem? There are two ways to solve it: \r\n1. place opt out of the ma_custom scope\r\n2.  modify model_average_optimizer.py line 91 like this:\r\n```\r\n    else:\r\n      kwargs['trainable'] = trainable\r\n      kwargs['collections'] = collections\r\n      return getter(name, *args, **kwargs)\r\n```\r\nI will make a pull request to fix this. Thanks for trying this opt!", "Thanks for responding @jinxin0924 - marking as \"Contributions welcome\" since it seems you have PR out for this.", "@jinxin0924 Thank you for your reply. I solve this problems with your second suggestion.\r\nIt's a nice work. However I still have some problems about `local_step` in `model_average_optimizer.py`. \r\nI found that it seems different workers share the same `local_step`.\r\n\r\ndetail logs are shown below,\r\nIt seems that model average is triggered in `(0,0),(4,2),(6,4),(9,9)`, it doesn't make any sense.\r\n\r\nworker0 log:\r\n\r\n\tINFO:tensorflow:is chief: True, len: 2\r\n\tINFO:tensorflow:0 0 8.00 9.00\r\n\tINFO:tensorflow:0 1 9.00 10.00\r\n\tINFO:tensorflow:0 2 10.00 11.00\r\n\tINFO:tensorflow:0 3 11.00 12.00\r\n\tINFO:tensorflow:0 4 18.00 19.00\r\n\tINFO:tensorflow:0 5 19.00 20.00\r\n\tINFO:tensorflow:0 6 27.00 28.00\r\n\tINFO:tensorflow:0 7 28.00 29.00\r\n\tINFO:tensorflow:0 8 29.00 30.00\r\n\tINFO:tensorflow:0 9 48.50 49.50\r\n\r\nworker1 log:\r\n\r\n\tINFO:tensorflow:is chief: False, len: 2\r\n\tINFO:tensorflow:1 0 8.00 9.00\r\n\tINFO:tensorflow:1 1 16.00 17.00\r\n\tINFO:tensorflow:1 2 18.00 19.00\r\n    INFO:tensorflow:1 3 26.00 27.00\r\n    INFO:tensorflow:1 4 27.00 28.00\r\n    INFO:tensorflow:1 5 35.00 36.00\r\n    INFO:tensorflow:1 6 43.00 44.00\r\n    INFO:tensorflow:1 7 51.00 52.00\r\n    INFO:tensorflow:1 8 59.00 60.00\r\n    INFO:tensorflow:1 9 48.50 49.50\r\n", "@yunyin, yes, different workers share the same local_step. It is a bug. You can modify model_average_optimizer.py line 91 like this:\r\n```\r\n    else:\r\n      kwargs['trainable'] = trainable\r\n      kwargs['collections'] = collections\r\n      if ops.GraphKeys.LOCAL_VARIABLES in collections:\r\n        with ops.device(self._worker_device):\r\n          return getter(name, *args, **kwargs)\r\n      else:\r\n        return getter(name, *args, **kwargs)\r\n```\r\nIn my experiments, I always create model average optimizer out of the ma_custom scope, so I did not find these bugs. Sorry for that. Again, thanks a lot for tying this opt! ", "@jinxin0924 Thank you for reply soon. However, I still have the same problems after following all your suggests", "@yunyin , clould you debug at model_average_optimizer.py line 94 `if ops.GraphKeys.LOCAL_VARIABLES in collections:` and take a look what will happen when the name is local_step?", "@jinxin0924 I've solved this problem.\r\nYou should put opt in ma_custom scope, so that local_step could be LOCAL_VARIABLES.\r\nThanks again for your work and support."]}, {"number": 19615, "title": "fix typo in function comment of name_scope", "body": "tiny commit for bewbie", "comments": []}, {"number": 19614, "title": "Update python_configure.bzl", "body": "Add BAZEL_SH environment variable as a dependency of python_configure repository rule", "comments": ["@gunan A follow up on 694892d4c0fba6bd4322e943f8b1483b36f1ae99"]}, {"number": 19613, "title": "Cannot create initializer for non-floating point type", "body": "I m using tensorflow.contrib.slim for creating my model and google Colaboratory for running my code.\r\nMy code was working like a charm with tensorflow 1.7 but when the new version 1.8 was installed in the cloud, I got the following error:\r\n\r\n`TypeError                                 Traceback (most recent call last)\r\n/content/drive/Colab/baby_training_model_estimators/train_model.py in <module>()\r\n    100     import logging\r\n    101     logging.getLogger('tensorflow').propagate = False\r\n--> 102     run_train()\r\n\r\n/content/drive/Colab/baby_training_model_estimators/train_model.py in run_train(args)\r\n     94                                       run_config=run_config,\r\n     95                                       schedule=\"train_and_evaluate\",\r\n---> 96                                       hparams=params)\r\n     97 \r\n     98 if __name__ == '__main__':\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    248               'in a future version' if date is None else ('after %s' % date),\r\n    249               instructions)\r\n--> 250       return func(*args, **kwargs)\r\n    251     return tf_decorator.make_decorator(\r\n    252         func, new_func, 'deprecated',\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py in run(experiment_fn, output_dir, schedule, run_config, hparams)\r\n    223   schedule = schedule or _get_default_schedule(run_config)\r\n    224 \r\n--> 225   return _execute_schedule(experiment, schedule)\r\n    226 \r\n    227 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py in _execute_schedule(experiment, schedule)\r\n     50     logging.error('Allowed values for this experiment are: %s', valid_tasks)\r\n     51     raise TypeError('Schedule references non-callable member %s' % schedule)\r\n---> 52   return task()\r\n     53 \r\n     54 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py in train_and_evaluate(self)\r\n    664                   hooks=self._eval_hooks)\r\n    665           ]\r\n--> 666       self.train(delay_secs=0)\r\n    667 \r\n    668     # If the checkpoint_and_export flag and appropriate estimator configuration\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py in train(self, delay_secs)\r\n    387         max_steps=self._train_steps,\r\n    388         hooks=self._train_monitors + extra_hooks,\r\n--> 389         saving_listeners=self._saving_listeners)\r\n    390 \r\n    391   def evaluate(self, delay_secs=None, name=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py in _call_train(self, _sentinel, input_fn, steps, hooks, max_steps, saving_listeners)\r\n    874           max_steps=max_steps,\r\n    875           hooks=hooks,\r\n--> 876           saving_listeners=saving_listeners)\r\n    877     else:\r\n    878       return self._estimator.fit(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    361 \r\n    362     saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 363     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    364     logging.info('Loss for final step: %s.', loss)\r\n    365     return self\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n    841       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n    842     else:\r\n--> 843       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n    844 \r\n    845   def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)\r\n    854       worker_hooks.extend(input_hooks)\r\n    855       estimator_spec = self._call_model_fn(\r\n--> 856           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n    857       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n    858                                              hooks, global_step_tensor,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n    829 \r\n    830     logging.info('Calling model_fn.')\r\n--> 831     model_fn_results = self._model_fn(features=features, **kwargs)\r\n    832     logging.info('Done calling model_fn.')\r\n    833 \r\n\r\n/content/drive/Colab/baby_training_model_estimators/tf_autoencoder/estimator.py in _model_fn(features, labels, mode)\r\n     92                                                  dropout=dropout,\r\n     93                                                  weight_decay=weight_decay,\r\n---> 94                                                  mode=mode)\r\n     95             return _create_estimator_spec_from_logits(labels=labels,\r\n     96                                                       logits=logits,\r\n\r\n/content/drive/Colab/baby_training_model_estimators/tf_autoencoder/layers.py in fully_connected_autoencoder(network_type, model_id, sparse_coding, inputs, hidden_units, activation_fn, dropout, weight_decay, mode)\r\n     90         with tf.variable_scope(scope, 'FCAutoencoder', [inputs]):\r\n     91                 with slim.arg_scope(autoencoder_arg_scope(activation_fn, dropout, weight_decay, None, mode)):\r\n---> 92                         net = fc_encoder(inputs, hidden_units, network_type, model_id, dropout, sparse_coding)\r\n     93                         n_features = inputs.shape[1].value\r\n     94                         decoder_units = hidden_units[:-1][::-1] + [n_features]\r\n\r\n/content/drive/Colab/baby_training_model_estimators/tf_autoencoder/layers.py in fc_encoder(inputs, hidden_units, network_type, model_id, dropout, sparse_coding)\r\n     15                         with tf.variable_scope('layer_{}'.format(layer_id), values=(net,)) as layer_scope:\r\n     16                                 print(num_hidden_units)\r\n---> 17                                 net = slim.fully_connected(net, num_outputs= num_hidden_units, scope=layer_scope)\r\n     18                                 if sparse_coding is not None:\r\n     19                                         tf.losses.mean_squared_error(net, tf.zeros_like(net), weights=sparse_coding, scope=layer_scope)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)\r\n    181       current_args = current_scope[key_func].copy()\r\n    182       current_args.update(kwargs)\r\n--> 183     return func(*args, **current_args)\r\n    184 \r\n    185   _add_op(func)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py in fully_connected(inputs, num_outputs, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\r\n   1714         _scope=sc,\r\n   1715         _reuse=reuse)\r\n-> 1716     outputs = layer.apply(inputs)\r\n   1717 \r\n   1718     # Add variables to collections.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in apply(self, inputs, *args, **kwargs)\r\n    826       Output tensor(s).\r\n    827     \"\"\"\r\n--> 828     return self.__call__(inputs, *args, **kwargs)\r\n    829 \r\n    830   def _add_inbound_node(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    697           if all(hasattr(x, 'get_shape') for x in input_list):\r\n    698             input_shapes = nest.map_structure(lambda x: x.get_shape(), inputs)\r\n--> 699           self.build(input_shapes)\r\n    700         try:\r\n    701           # Note: not all sub-classes of Layer call Layer.__init__ (especially\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/core.py in build(self, input_shape)\r\n    136                                     constraint=self.kernel_constraint,\r\n    137                                     dtype=self.dtype,\r\n--> 138                                     trainable=True)\r\n    139     if self.use_bias:\r\n    140       self.bias = self.add_variable('bias',\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in add_variable(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner)\r\n    544             constraint=constraint,\r\n    545             trainable=trainable and self.trainable,\r\n--> 546             partitioner=partitioner)\r\n    547 \r\n    548         if init_graph is not None:  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\r\n    434     new_variable = getter(\r\n    435         name=name, shape=shape, dtype=dtype, initializer=initializer,\r\n--> 436         **kwargs_for_getter)\r\n    437 \r\n    438     # If we set an initializer and the variable processed it, tracking will not\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\r\n   1315       partitioner=partitioner, validate_shape=validate_shape,\r\n   1316       use_resource=use_resource, custom_getter=custom_getter,\r\n-> 1317       constraint=constraint)\r\n   1318 get_variable_or_local_docstring = (\r\n   1319     \"\"\"%s\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\r\n   1077           partitioner=partitioner, validate_shape=validate_shape,\r\n   1078           use_resource=use_resource, custom_getter=custom_getter,\r\n-> 1079           constraint=constraint)\r\n   1080 \r\n   1081   def _get_partitioned_variable(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\r\n    415       if \"constraint\" in estimator_util.fn_args(custom_getter):\r\n    416         custom_getter_kwargs[\"constraint\"] = constraint\r\n--> 417       return custom_getter(**custom_getter_kwargs)\r\n    418     else:\r\n    419       return _true_getter(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py in layer_variable_getter(getter, *args, **kwargs)\r\n   1609   def layer_variable_getter(getter, *args, **kwargs):\r\n   1610     kwargs['rename'] = rename\r\n-> 1611     return _model_variable_getter(getter, *args, **kwargs)\r\n   1612 \r\n   1613   return layer_variable_getter\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py in _model_variable_getter(getter, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, rename, use_resource, **_)\r\n   1600       partitioner=partitioner,\r\n   1601       custom_getter=getter,\r\n-> 1602       use_resource=use_resource)\r\n   1603 \r\n   1604 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)\r\n    181       current_args = current_scope[key_func].copy()\r\n    182       current_args.update(kwargs)\r\n--> 183     return func(*args, **current_args)\r\n    184 \r\n    185   _add_op(func)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/variables.py in model_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter, use_resource)\r\n    289                  caching_device=caching_device, device=device,\r\n    290                  partitioner=partitioner, custom_getter=custom_getter,\r\n--> 291                  use_resource=use_resource)\r\n    292   return var\r\n    293 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)\r\n    181       current_args = current_scope[key_func].copy()\r\n    182       current_args.update(kwargs)\r\n--> 183     return func(*args, **current_args)\r\n    184 \r\n    185   _add_op(func)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/variables.py in variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter, use_resource)\r\n    244                   caching_device=caching_device,\r\n    245                   partitioner=partitioner,\r\n--> 246                   use_resource=use_resource)\r\n    247 \r\n    248 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\r\n    392           trainable=trainable, collections=collections,\r\n    393           caching_device=caching_device, validate_shape=validate_shape,\r\n--> 394           use_resource=use_resource, constraint=constraint)\r\n    395 \r\n    396     if custom_getter is not None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\r\n    784         validate_shape=validate_shape,\r\n    785         constraint=constraint,\r\n--> 786         use_resource=use_resource)\r\n    787     if not context.executing_eagerly() or self._store_eager_variables:\r\n    788       # In eager mode we do not want to keep default references to Variable\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in variable(initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint, use_resource)\r\n   2218                          name=name, dtype=dtype,\r\n   2219                          constraint=constraint,\r\n-> 2220                          use_resource=use_resource)\r\n   2221 \r\n   2222 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in <lambda>(**kwargs)\r\n   2208              constraint=None,\r\n   2209              use_resource=None):\r\n-> 2210   previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n   2211   for getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\r\n   2212     previous_getter = _make_getter(getter, previous_getter)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)\r\n   2191         collections=collections, validate_shape=validate_shape,\r\n   2192         caching_device=caching_device, name=name, dtype=dtype,\r\n-> 2193         constraint=constraint)\r\n   2194 \r\n   2195 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint)\r\n    233           dtype=dtype,\r\n    234           expected_shape=expected_shape,\r\n--> 235           constraint=constraint)\r\n    236 \r\n    237   def __repr__(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint)\r\n    341             with ops.name_scope(\"Initializer\"), ops.device(None):\r\n    342               self._initial_value = ops.convert_to_tensor(\r\n--> 343                   initial_value(), name=\"initial_value\", dtype=dtype)\r\n    344               shape = (self._initial_value.get_shape()\r\n    345                        if validate_shape else tensor_shape.unknown_shape())\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in <lambda>()\r\n    768           initializer = initializer(dtype=dtype)\r\n    769         init_val = lambda: initializer(  # pylint: disable=g-long-lambda\r\n--> 770             shape.as_list(), dtype=dtype, partition_info=partition_info)\r\n    771         variable_dtype = dtype.base_dtype\r\n    772 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/initializers.py in _initializer(shape, dtype, partition_info)\r\n    118     \"\"\"Initializer function.\"\"\"\r\n    119     if not dtype.is_floating:\r\n--> 120       raise TypeError('Cannot create initializer for non-floating point type.')\r\n    121     # Estimating fan_in and fan_out is not possible to do perfectly, but we try.\r\n    122     # This is the right thing for matrix multiply and convolutions.\r\n\r\nTypeError: Cannot create initializer for non-floating point type.\r\n\r\n`\r\n\r\nI think it is related to a previous bug: [#6342](https://github.com/tensorflow/tensorflow/issues/6342)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Sorry, it seems that I have deleted a line that converts the integer tensors into floating tensors.\r\nSo there is no bug.\r\n", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 19612, "title": "import _pywrap_tensorflow ModuleNotFoundError: No module named '_pywrap_tensorflow'", "body": "Hello,  I know already a few had a similar problem. But I just don't get it fixed.\r\n\r\nThis is my error:\r\n\r\nImport tensorflow\r\n\r\n`Traceback (most recent call last):\r\n  File \"C:\\Users\\danyb\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\danyb\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\danyb\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\danyb\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\danyb\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\danyb\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\danyb\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\danyb\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\danyb\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\danyb\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\danyb\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\danyb\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.`\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 31 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19611, "title": "performance bug in Conv3d_transpose causes it to be a factor >100 slower than equivalent computations", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: This only concerns a nativ tensorflow function\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Tested on Arch Linux, OS X and Windows 8\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5, 1.7 & 1.8 (cpu)\r\n- **Python version**: 2.7.14 and 3.6.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CPU-only issue\r\n- **GPU model and memory**: CPU-only issue\r\n- **Exact command to reproduce**: tf.conv3d_transpose\r\n\r\n### Describe the problem\r\ntf.conv3d_transpose is a factor >100 slower than other operations doing the same or an equivalent computation when working with tensorflow on cpu. This was previously reported in Issue #10535 and #7610 , but none have provided a minimal example and both were closed due to inactivity.\r\n\r\n### Source code / logs\r\nMinimal example that shows how much slower conv3d_transposed is compared to einsum doing the same computation and to the conv3d forward computation:\r\n```\r\nimport tensorflow as tf\r\nimport time\r\n\r\nbatch_size = 200\r\ninp = tf.ones((batch_size,28,28,1,1))\r\nfilter_conv = tf.ones((28,28,1,1,50))\r\nfilter_fc = tf.ones((28,28,1,1,50))\r\nlat = tf.ones((batch_size,1,1,1,50))\r\n\r\nsess = tf.InteractiveSession()\r\nt0 = time.time()\r\n# takes 47sec on my machine\r\nsess.run(tf.nn.conv3d_transpose(lat, filter_conv, inp.shape,[1,1,1,1,1],padding='VALID'))\r\ndelta_t = time.time() - t0\r\nprint(\"time it takes for conv3d_transpose:\", delta_t)\r\nt0 = time.time()\r\n# takes .04sec on my machine\r\nsess.run(tf.einsum(\"ijclm,abclm->iabcj\",lat,filter_fc)) \r\ndelta_t = time.time() - t0\r\nprint(\"time it takes with einsum to do the same computation:\", delta_t)\r\nt0 = time.time()\r\n# takes .01sec on my machine\r\nsess.run(tf.nn.conv3d(inp, filter_conv, [1,1,1,1,1],padding='VALID'))\r\ndelta_t = time.time() - t0\r\nprint(\"time it takes to apply conv3d:\", delta_t)\r\n```\r\n", "comments": ["I'm seeing extreme slowdown for 3D convolutions on CPU as well. I'm going through Keras `Conv3D`, but I suspect it's the same issue. Is there any workaround identified for this for the time being?", "> I'm seeing extreme slowdown for 3D convolutions on CPU as well. I'm going through Keras Conv3D, but I suspect it's the same issue. Is there any workaround identified for this for the time being?\r\n\r\n\r\nYes it might be the same issue. It should only be present in the gradients though, because these use the adjoint of conv3d.\r\nI am unfortunately not aware of any workaround.", "@ckdotca @boeseMilch  I've submitted couple of changes that should make Conv3D performance much better on CPU (I'd expect something like ~5x, maybe ~10x in some cases). One of them is custom kernels for backprop input and filter: https://github.com/tensorflow/tensorflow/commit/e183b8d0328d7398cb6ffc530d1ae8fdbd2111c0\r\n\r\nNew kernels allocate quite large temporary buffers, so you might see increased peak memory usage. It's possible to fallback on original Eigen kernels using `kernel_label_map` (see https://github.com/tensorflow/tensorflow/blob/8cb0558da924e891aa1bb5d79a6c0c846301e4eb/tensorflow/python/framework/ops.py#L3311), old kernels registered with a \"eigen_tensor\" label.\r\n\r\nI'd be super interested to know how much this helps in your specific case.", "Hi @karmel & @ezhulenev , please advise if we can consider this issue resolved and hence close it. Thanks.", "I think it's a solved issue.", "Thank you @ezhulenev "]}, {"number": 19610, "title": "tflite AllocateTensors error after ResizeInputTensor", "body": "OS Platform and Distribution: Android Native C, Android API 26\r\nTensorFlow installed from: Compiled from the official github master branch\r\nTensorFlow version: Master\r\nBazel versio: 0.13.0-homebrew\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\nCode below: \r\n```\r\nint input = interpreter->inputs()[0];\r\n//  TfLiteStatus status = interpreter->ResizeInputTensor(0, sizes);\r\n  TfLiteStatus status = interpreter->ResizeInputTensor(input, sizes);\r\n\r\n  if (interpreter->ResizeInputTensor(input, sizes) != kTfLiteOk) {\r\n        exit(-1);\r\n   }\r\n\r\n  if (interpreter->AllocateTensors() != kTfLiteOk) {\r\n      exit(-1);\r\n  }\r\n```\r\nAllocateTensors errors after resize the InputTensor, any suggestions? I am using mobilenet_ssd.tflite model.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Re-edit the  question.", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Please include the error message (see stderr or stdout). Also, you might try the nightly or master of Tensorflow lite (not 1.9), because some bugs have recently been repaired in AllocateTensors()", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19609, "title": "Add a note that stop_gradient in moments does not change the gradient", "body": "In https://github.com/tensorflow/tensorflow/commit/eccd162119675d0bf5bc6f8e6a93dcda7ab6db4a#diff-ef8609a43751227afcaacc838670a96f @sguada added support for sample mean in `moments`. He added a `stop_gradient` to the mean in the variance calculation. Since it took me some time to figure out that this `stop_gradient` has no effect (except a reduced computation time), I think a note to the `stop_gradient` could be usefull.\r\n\r\nPS: This PR is a suggestion for such a note.", "comments": []}, {"number": 19608, "title": "Value error in variable scope", "body": "\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 653, in _get_single_variable\r\n    name, \"\".join(traceback.format_list(tb))))\r\n\r\nValueError: Variable proj_w already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\r\n\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n\r\n\r\nThis is the error i am getting with the sequence to sequence model i am using to build a chatbot using the cornell movie conversation dataset. the console throws a error that variable proj_w already exists, disallowed. Did you mean to set reuse=True in VarScope? How to rectify this problem?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "i had solved the issue\n\nthanks\n\n\nOn Thu, Jun 14, 2018 at 12:34 AM Alfred Sorten Wolf <\nnotifications@github.com> wrote:\n\n> It has been 15 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19608#issuecomment-397049420>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APvMMNO7LyIfLfNQG2WUTm-WtNY5bHg3ks5t8WI9gaJpZM4UQ3Sc>\n> .\n>\n\n\n-- \n<https://about.me/jaiaravind?promo=email_sig&utm_source=product&utm_medium=email_sig&utm_campaign=gmail_api&utm_content=thumb>\nJai Aravind\nabout.me/jaiaravind\n<https://about.me/jaiaravind?promo=email_sig&utm_source=product&utm_medium=email_sig&utm_campaign=gmail_api&utm_content=thumb>\n"]}, {"number": 19607, "title": "Android tensorflow lite kernel_util.cc:34 input_product_scale < output_scale", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:1.8\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n1.I retrained and quantized the openpose model(add tf.contrib.quantize.create_training_graph() in my code\uff09\r\n\r\n2.Generate .pb (add tf.contrib.quantize.create_eval_graph() in my code) \r\n\r\n3.Then, I converted my own pb model to lite.\r\nIt was succeed,the command:\r\n\r\n./bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=models/freeze/frozen_q_256.pb \\\r\n  --output_file=tf_files/lite/frozen_q_256.lite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_array=image \\\r\n  --input_shape=1,256,256,3\\\r\n  --output_array=Openpose/concat_stage7 \\\r\n  --std_value=127.5 \\\r\n  --mean_value=127.5 \\\r\n\r\n4.But when I put it on android, I got an error.\r\n\r\n### Source code / logs\r\nava.lang.NullPointerException: Can not allocate memory for the given inputs: tensorflow/contrib/lite/kernels/kernel_util.cc:34 input_product_scale < output_scale was not true.\r\n                                                                                  at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n                                                                                  at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)\r\n                                                                                  at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:112)\r\n                                                                                  at org.tensorflow.lite.Interpreter.run(Interpreter.java:93)\r\n                                                                                  at com.asus.android.poseestimator.ImageClassifier.classifyFrame(ImageClassifier.java:167)\r\n                                                                                  at com.asus.android.poseestimator.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:740)\r\n                                                                                  at com.asus.android.poseestimator.Camera2BasicFragment.access$1000(Camera2BasicFragment.java:74)\r\n                                                                                  at com.asus.android.poseestimator.Camera2BasicFragment$5.run(Camera2BasicFragment.java:621)\r\n                                                                                  at android.os.Handler.handleCallback(Handler.java:836)\r\n                                                                                  at android.os.Handler.dispatchMessage(Handler.java:103)\r\n                                                                                  at android.os.Looper.loop(Looper.java:208)\r\n                                                                                  at android.os.HandlerThread.run(HandlerThread.java:61)\r\n\r\nmy tensorflow lite version is 0.1.1. Does this version support qauntized lite??\r\nHow can I solve this problem?\r\n\r\n", "comments": ["this may be resolved in the latest version of TensorFlow Lite, but to be sure, could you provide me the your graph.pbtxt. In particular I would like to figure out what the culprit TFLite operation is. Thanks.", "Thanks for your reply.  I changed version to tensorflow 0.1.7, but a new error occurred.\r\n\r\nLogs:\r\n java.lang.RuntimeException: Unable to start activity ComponentInfo{com.asus.android.poseestimator/com.asus.android.poseestimator.CameraActivity}: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter\r\n\r\n\r\n", "it looks like same issue as\r\nhttps://github.com/tensorflow/tensorflow/issues/19372\r\n", "@suharshs The issue should be caused by the operator CONCATENATION when quantization:\r\n\r\n`\r\n\r\n127 | [340, 370, 13] | [433] | {'axis': 3, 'fused_activation_function': 'NONE'} | CONCATENATION (opcode=1)\r\n\r\n340 | Openpose/MConv_Stage5_L1_5_pointwise/add_fold | UINT8 | [1, 32, 32, 38] | 347 | {'min': [-1.05201], 'max': [6.00753], 'scale': [0.027684], 'zero_point': [38]}\r\n371 | Openpose/MConv_Stage5_L2_5_pointwise/weights_quant/FakeQuantWithMinMaxVars | UINT8 | [19, 1, 1, 64] | 139 | {'min': [-9.128201], 'max': [9.394188], 'scale': [0.072637], 'zero_point': [126]}\r\n13 | MobilenetV1/Conv2d_11_pointwise/Relu6 | UINT8 | [1, 32, 32, 256] | 240 | {'min': [-1.05201], 'max': [6.00753], 'scale': [0.027684], 'zero_point': [38]}\r\n\r\nthe warning message when do Toco convert:\r\n2018-05-29 12:09:56.228183: W tensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:105] Tweaking the MinMax of array Openpose/MConv_Stage1_L2_5_pointwise/add_fold, which is an input to {Concatenation operator with output Openpose/MConv_Stage2_concat}, because we want all inputs and outputs of a Concatenation operator to have the same MinMax so that it can be implemented as a pure byte-copy, no arithmetic.\r\n\r\nany suggestions?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n`\r\n\r\n", "Thanks for the extra info. This may be a recent issue we found and fixed. The issue has to do with TOCO trying to guarantee that the input and output scales of Concatentation operations match. This may be resolved by providing `--change_concat_input_ranges=false` to your commandline TOCO invocation. \r\n\r\nPlease let me know if that doesn't help. Thanks!", "Still not work. It still fails when load tflite file.\r\nThe Interpreter fail to execute AllocateTensors\r\n Caused by: java.lang.NullPointerException: Can not allocate memory for the interpreter\r\n                      at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n                      at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n                      at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:51)\r\n                      at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:90)\r\n                      at com.asus.android.poseestimator.ImageClassifier.<init>(ImageClassifier.java:117)\r\n\r\nI try to skip the Concatentation when call tf.contrib.quantize.create_eval_graph() and it can load successfully.\r\n\r\nAny other solution?\r\n\r\nThanks.", "waiting for solve this issue : )", "I have met the same problem when I load openpose quantized model to Android.\r\nCaused by: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/pooling.cc:104 input->params.scale != output->params.scale (3 != 3)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:73)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:52)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:114)\r\nHow can I solve it?\r\nThanks.", "Can you explain the issue more clearly and provide an example to reproduce the problem?\r\nThanks,", "Can you try and see if this issue persists with the most recent release? If so can you provide a openpose quantized model for me to reproduce? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Nagging Assignee @liyunlu0618: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19606, "title": "Android tensorflow lite kernel_util.cc:34 input_product_scale < output_scale", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Duplicate of #19607"]}]