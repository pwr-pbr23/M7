[{"number": 35301, "title": "query replaced with value", "body": " Fixed Issue [#34283](https://github.com/tensorflow/tensorflow/issues/34283) and #34696 ", "comments": ["Please open against `master` branch, not `r2.0`. The release process for TensorFlow is as follows:\r\n\r\n1. We cut the release branch for the next version. Say `r2.2` for TF2.2\r\n2. We get cherry-picks from `master` into `r2.2` to fix issues reported via testing the RC candidates\r\n3. We release the final version, `tensorflow==2.2.0`\r\n\r\nThat's it. Observe that we never merge back release branches into master and we only pick commits from `master` only if they fix tests for the release candidates.\r\n\r\nAfter time passes we might need to do a patch release for security issues. However, even in that case we only do cherry-picks from `master` and only for the commits which are relevant to the fixed issue.\r\n\r\nThus, in all cases, a change that comes to a release branch will get lost and we won't be able to take it.\r\n\r\nTL;DR: please rebase/reopen agains `master`. I'm going to switch base manually but if that doesn't work I will close the PR and ask you to reopen, rebased against `master`", "Manually changing base didn't work. As such, closing but please reopen against `master`"]}, {"number": 35300, "title": "Graphics Card Recommendation", "body": "I am running video-detection application using Yolo and FRCNN model( using Tensorflow backend).\r\nI have to run around 50 parallel video detection simultaneously. Please advice which Graphics card or Combination of Graphics should be able to handle the load of Graphics.\r\n\r\nMy options are : -\r\ni) 2 x RTX-TITAN\r\nii) 4 x RTX 2080ti\r\niii) 8 x GTX 1080ti\r\n", "comments": ["Will you be training a model or running a model? If you are training a model, then go with 2 x Titan RTX. If you are running the model then any of these combinations will suffice.\r\nPS: I've never seen any computer with 8 GTX 1080ti except for mining rigs. Be careful before you buy it.", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 35299, "title": "About bias quantization of hexagon delegate", "body": "As far as I know, Tensorflow Lite uses input_scale * weight_scale as bias scale to quantize bias tensor to int32. However, hexagon use int32_min/in32_max as min/max to quantize bias tensor.\r\nSo how do you handle this difference to hexagon delegation? Below is the conv_2d builder of the hexagon delegation. I don't see where the transformation is.\r\nhttps://github.com/tensorflow/tensorflow/blob/539b7642a928c7fbfb4d896f650f7e6d79c2a5e0/tensorflow/lite/experimental/delegates/hexagon/builders/conv_2d_builder.cc#L234", "comments": ["Hi @lee-bin \r\n\r\nThe delegate doesn't use int32_min/max as min max for quantization, it computes them from the scale/zero point data in the tensor\r\nhttps://github.com/tensorflow/tensorflow/blob/539b7642a928c7fbfb4d896f650f7e6d79c2a5e0/tensorflow/lite/experimental/delegates/hexagon/builders/op_builder.h#L133\r\n\r\nThe quantization params in the tensor should guarantee the input_scale*weight_scale you mentioned.\r\n\r\nThanks", "Thanks. The bias scale, this way, is the same as in lite and hexagon which guarantees the restored bias. It really helped."]}, {"number": 35298, "title": "OneDeviceStrategy allocates memory on two GPUs", "body": "**System information**\r\n- OS Platform and Distribution: Arch Linux, 5.4.2-arch1-1-ARCH\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.1.0rc1\r\n- Keras version: 2.2.4-tf\r\n- Python version: 3.8\r\n- GPU model and memory: 2x GTX 1080 Ti 11GB\"`\r\n\r\n**Describe the current behavior**\r\nOneDeviceStrategy is set to use GPU0 but allocates full memory at GPU0 and GPU1 too.\r\nOn both GPUs 11GB are allocated.\r\nUsing not Strategy (simple plain TF code) - the memory is allocated only at GPU0.\r\n\r\n**Describe the expected behavior**\r\nOneDeviceStrategy is set to use GPU0 allocates memory only at GPU0 as the code does if OneDeviceStrategy is not used.", "comments": ["@olk, Could you post the standalone code to analyze the issue. Thanks!", "```\r\nimport tensorflow as tf\r\n  import tensorflow_datasets as tfds\r\n  import time\r\n  \r\n  from tensorflow.keras.optimizers import Adam\r\n  \r\n  def build_model():\r\n      filters = 48\r\n      units = 24\r\n      kernel_size = 7\r\n      learning_rate = 1e-4\r\n      model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(units, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n      ])\r\n      model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\r\n      return model\r\n  \r\n  datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n  mnist_train, mnist_test = datasets['train'], datasets['test']\r\n  \r\n  num_train_examples = info.splits['train'].num_examples\r\n  num_test_examples = info.splits['test'].num_examples\r\n  \r\n  strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\r\n  #strategy = tf.distribute.MirroredStrategy()\r\n  print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n  \r\n  BUFFER_SIZE = 10000\r\n  BATCH_SIZE = 32\r\n  \r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n  \r\n  train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n  eval_dataset = mnist_test.map(scale).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n  \r\n  with strategy.scope():\r\n    model = build_model()\r\n  \r\n  epochs=5\r\n  start = time.perf_counter()\r\n  model.fit(\r\n          train_dataset,\r\n          validation_data=eval_dataset,\r\n          steps_per_epoch=num_train_examples/epochs,\r\n          validation_steps=num_test_examples/epochs,\r\n          epochs=epochs)\r\n  elapsed = time.perf_counter() - start\r\n  print('elapsed: {:0.3f}'.format(elapsed))\r\n```", "olk@ TensorFlow by default maps all the GPU memory on all available GPUs for a given process. To limit the number of GPUs you want to use you can use the `tf.config.experimental.set_visible_devices` option. There is also an option for memory growth. More details can be found [here](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth).\r\nYou should be seeing the same behavior without using OneDeviceStrategy as well.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35298\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35298\">No</a>\n", "@ anj-s c TF allocates 100% memory on **both GPUs** if I use **OneDeviceStrategy()** !", "olk@ what is the behavior when you don't use OneDeviceStrategy? Also how do you determine memory allocation? Did using `tf.config.experimental.set_visible_devices` help?", "@anj-s Using OneDeviceStrategy(device='/gpu:1') allocates memory 11GB on gpu:0 and gpu:1. The same happens if I don't use the code from above without strategy. The resource utilization is shown via nvtop and/or nvidia_smi.", "Just to clarify the behavior is the same with and without strategy. Did using tf.config.experimental.set_visible_devices help?", "The same with and without strategy - I would expect that TF allocates memory only at the gpu:0 as it is specified by **OneDeviceStrategy(device='/gpu:0'))**. Otherwise I would get problems if I run a second TF app that uses the other second GPU via **OneDeviceStrategy(device='/gpu:1'))** (and both apps try to allocate memory on both GPUs).", "@olk As mentioned in the above [comment](https://github.com/tensorflow/tensorflow/issues/35298#issuecomment-569151712) this is expected. Please reopen the issue if you are unable to use `tf.config.experimental.set_visible_devices` successfully. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35298\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35298\">No</a>\n", "@anj-s I would consider it as  a bug - OneDeviceStrategy allocates (100%!) memory on other GPUs than specified in OneDeviceStrategy. I'd like to ask what's the benefit of OneDeviceStrategy compered to code that doesn't use a Strategy at all.\r\nAt least the naming '_OneDeviceStrategy_' is misleading.", "On a three GPU machine, I have used tf.config.experimental.set_visible_devices() to limit things to gpu:1.  However, passing \"/gpu:1\" to OneDeviceStrategy fails saying it cannot find that GPU, and setting it to \"/gpu:0\" just allocates space on GPU 0 (physical).\r\nGranted, one can forgo using OneDeviceStrategy altogether, but that renders OneDeviceStrategy completely moot.  The problem is that I'm writing code to switch between that and MirroredStrategy depending on how many GPUs are requested for a given code run, so it's a real shame.", "FYI - I solved the issue by using the following\r\n`import os`\r\n`os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'`"]}, {"number": 35297, "title": "[INTEL MKL] Create a partial key for output_scale.", "body": "MKL Conv2d's primitive key creation has significant overhead, especially for quantized conv2d which has an extraordinary long `output_scale` post ops (for re-quantization). Instead of using actual values of `output_scale` vector, we are using the pointers of min/max_filter_vector as a partial key, and this works well since the filter vector is always a constant in `MklQuantizedConv2DOp`.\r\n\r\nThis feature could improve the performance for models with small workloads. For example, mobilenet will improve ~30%, and ssd-mobilenet will improve ~10%.\r\n", "comments": ["Any updates? @penpornk ", "@penpornk Thanks for your review. Changes applied"]}, {"number": 35296, "title": "Missing UnPack Op added", "body": "This serves to update the test case with the UnPack Op struct.", "comments": ["@jdduke @aselle Please review, I'd like to know if anything more should be done. Thanks :)", "Hi @gbaned , is there anything I should be doing here?", "> Hi @gbaned , is there anything I should be doing here?\r\n\r\nHi @kyscg, nothing at this time. We will let you know if anything required. Thank you."]}, {"number": 35295, "title": "TF Lite GPU delegate gives fuse_auto_input failed error when running TF Lite model that uses only supported GPU operations.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN.A.\r\n\r\n- TensorFlow installed from (source or binary):\r\nSource, 9a5b203\r\n\r\n- TensorFlow version (use command below):\r\n2\r\n\r\n- Python version:\r\n3.6.8\r\n\r\n- Bazel version (if compiling from source):\r\n1.0.0\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n7.4.0\r\n\r\n- CUDA/cuDNN version:\r\n10.0.130/7.4.2\r\n\r\n- GPU model and memory:\r\nRTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nTF Lite GPU delegate gives `fuse_auto_input failed` error when running TF Lite model that only uses supported operations listed in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/gpu.\r\n\r\nAt first, I suspect that TF Lite GPU delegate expects a model that has no fused operators. Therefore, I tried to prevent TFLiteConverter from fusing operators by commenting the following lines in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/toco_tooling.cc:\r\n```\r\nif (SupportsFusedActivationFunction(output_format)) {\r\n  transformations.Add(new FuseActivationFunctions);\r\n} else {\r\n  transformations.Add(new UnfuseActivationFunctions);\r\n}\r\n```\r\n\r\nHowever, it still gives the same error.\r\n\r\nIt only works when I add `compile_options.auto_input_fusion = false` to this file (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/gl_delegate.cc), which prevents TF Lite GPU delegate from trying to fuse operators. \r\n\r\n**Describe the expected behavior**\r\nTF Lite GPU delegate should be able to run TF Lite model without needing to modify TensorFlow source code.\r\n\r\n**Code to reproduce the issue**\r\n\r\n1. git pull https://github.com/google/mediapipe.git\r\n2. Switch this model (https://drive.google.com/file/d/1LwNKYcf_sYDDWNWML-mNjxp8_j_2ofRe/view?usp=sharing) with the `hair_segmentation.tflite` model in MediaPipe\r\n3. Run the hair segmentation pipeline by \r\n```\r\nbazel build -c opt --copt -DMESA_EGL_NO_X11_HEADERS mediapipe/examples/desktop/hair_segmentation:hair_segmentation_gpu\r\nGLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/hair_segmentation/hair_segmentation_gpu     --calculator_graph_config_file=mediapipe/graphs/hair_segmentation/hair_segmentation_mobile_gpu.pbtxt\r\n```\r\n\r\n**Other info / logs**\r\n> INFO: Created TensorFlow Lite delegate for GPU.\r\n> ERROR: TfLiteGpuDelegate Prepare: fuse_auto_input failed\r\n> ERROR: Node number 8 (TfLiteGpuDelegate) failed to prepare.\r\n> \r\n> ERROR: Restored previous execution plan after delegate application failure.\r\n> E1220 10:32:56.170152  5452 demo_run_graph_main_gpu.cc:186] Failed to run the graph: Graph has errors: \r\n> Calculator::Open() for node \"[TfLiteInferenceCalculator, TfLiteInferenceCalculator with output stream: segmentation_tensor]\" failed: ; (interpreter_->ModifyGraphWithDelegate(delegate_))==(kTfLiteOk)e_calculator.cc:593) \r\n\r\n\r\n", "comments": ["@yxchng \r\n\r\n`hair_segmentation.tflite` is a model that we know runs on mobile GPUs.\r\n\r\n> Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN.A.\r\n\r\nwith this + mesa, you're entering a domain we don't support =/\r\n\r\nNote that `fuse_auto_input` doesn't mean the same thing as in what TOCO calls fusion, so you shouldn't worry about fixing TOCO; it's fusion for GPU kernels only.  And the fusion logic is not even the GPU land, but purely on the CPU land, so you might be able to pinpoint what's causing the failure.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35295\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35295\">No</a>\n"]}, {"number": 35294, "title": "lite/micro: Add tensor allocation tests for #35121", "body": "Add a second operation to the MockModel sharing inputs with the first\r\noperation to catch #35121 (Tensor lifetime incorrectly calculated on\r\nmultiple use) and verify the fix.\r\n\r\nBoth micro_allocator_test and micro_interpreter_test will fail with this change until  #35123 is merged.", "comments": ["@suphoff Can you please resolve conflicts? Thanks!", "Merged to #35123 as requested by @petewarden "]}, {"number": 35293, "title": "Added usage example for tf.image.rgb_to_yiq", "body": "", "comments": ["@alextp hi, I've made the requested changes.", "No you haven't. I still don't see the results of the operations in the doctest and I still don't see a seed.", "@alextp Sorry, I've added the results and added `tf.random.set_seed(1)`", "#35388 also touches `tf.image.random_hue`", "Ah, I didn't see that lol. Okay I will remove the random_hue example and only add the rgb_to_yiq() example", "No need to provide a print, just remove the \"y = \" and the doctest will\nwork.\n\nOn Fri, Dec 27, 2019 at 9:38 AM HotPotatoC <notifications@github.com> wrote:\n\n> *@HotPotatoC* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/ops/image_ops_impl.py\n> <https://github.com/tensorflow/tensorflow/pull/35293#discussion_r361712205>\n> :\n>\n> > @@ -2931,13 +2931,21 @@ def rgb_to_yiq(images):\n>    Outputs a tensor of the same shape as the `images` tensor, containing the YIQ\n>    value of the pixels.\n>    The output is only well defined if the value in images are in [0,1].\n> +\n> +  Usage Example:\n> +    ```python\n> +    >>> x = tf.constant([[[1.0, 2.0, 3.0]]])\n> +    >>> y = tf.image.rgb_to_yiq(x)\n> +    [[[ 1.815      -0.9...  0.09...]]]\n>\n> should I provide a print(y.numpy()) ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/35293?email_source=notifications&email_token=AAABHRPX54BEJG7UFWTSZ73Q2Y4RXA5CNFSM4J5VZERKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCQJ2A4A#discussion_r361712205>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRMOT6A7YUZYGD6LT3DQ2Y4RXANCNFSM4J5VZERA>\n> .\n>\n\n\n-- \n - Alex\n", "The doctest seems to have failed\r\n\r\n```\r\nExpected:\r\n    <tf.Tensor: shape=(1, 1, 3), dtype=float32, numpy=array([[[ 1.815     , -0.9...,  0.09...]]], dtype=float32)>\r\n    ```\r\nGot:\r\n    <tf.Tensor: shape=(1, 1, 3), dtype=float32, numpy=array([[[ 1.815     , -0.91724455,  0.09962624]]], dtype=float32)>\r\n```\r\n\r\nWhy is the '...' not working?", "Maybe the number of spaces is different? +Yash Katariya\n<yashkatariya@google.com> do you know?\n\nOn Fri, Dec 27, 2019 at 10:25 AM HotPotatoC <notifications@github.com>\nwrote:\n\n> The doctest seems to have failed\n>\n> Expected:\n>     <tf.Tensor: shape=(1, 1, 3), dtype=float32, numpy=array([[[ 1.815     , -0.9...,  0.09...]]], dtype=float32)>\n>     ```\n> Got:\n>     <tf.Tensor: shape=(1, 1, 3), dtype=float32, numpy=array([[[ 1.815     , -0.91724455,  0.09962624]]], dtype=float32)>\n>\n> Why is the '...' not working?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/35293?email_source=notifications&email_token=AAABHRLFNMUGMBMATQIWCNTQ2ZCA7A5CNFSM4J5VZERKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHXSOIA#issuecomment-569321248>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPZ7BVEBGIUXFOIBS3Q2ZCA7ANCNFSM4J5VZERA>\n> .\n>\n\n\n-- \n - Alex\n", "It is also matching the ` ``` ` from the next line. You should remove ` ```python` and the ` ``` ` at the end", "Happy to contribute :)! And happy new year 2020 from Indonesia"]}, {"number": 35291, "title": "Fix compilation crash of replay_computation", "body": "ParseFromStringPiece isn't defined.\r\nCrash introduced in f62a214c2202573fb13b0b601f721c6d757fad68\r\n\r\n@thomasjoerg @sanjoy ", "comments": []}, {"number": 35290, "title": "Upgrade Edge Board Support Package in TFLu (micro)", "body": "This transitions the ```micro_speech```, ```magic_wand```, and ```person_detection``` examples for the SparkFun Edge board to use the more polished board support package (BSP).\r\n\r\nLED control is standardized to use the Ambiq led device interface like the apollo3_evb.\r\n\r\nIt also cleans out a few unused downloads from the Apollo3 targets. \r\n\r\n**dependencies**\r\n[Update TFLu (micro) to use AmbiqSuite SDK Release 2.2.0 for Apollo3](https://github.com/tensorflow/tensorflow/pull/35236)", "comments": []}, {"number": 35289, "title": "For python2, pin scipy to 1.2.2 (latest released).", "body": "This means py2 won't get the fix in scipy/scipy#11237\r\n\r\nPiperOrigin-RevId: 286456504\r\nChange-Id: Ic94ee7e57dd6ea590d79aa643e5de4192709ff17", "comments": []}, {"number": 35288, "title": "Fix deprecation warnings caused by math_ops.div", "body": "This PR is a follow up to #34807, fixing all remaining warnings\r\ncaused by usages of deprecated math_ops.div.\r\n\r\nAll non-test files are cleared of the math_ops.div warning now.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 35287, "title": "NFC - minor spelling tweaks", "body": "This PR addresses minor spelling tweaks. follow-on of #34958", "comments": ["@jaingaurav Sure, done"]}, {"number": 35286, "title": "NFC - minor spelling tweaks under lite directory", "body": "This PR addresses minor spelling tweaks under `tensorflow/lite` directory.\r\nfollow-on of #34958", "comments": ["cc @jaingaurav ", "@renjie-liu Thank you for pointing them out. I addressed both of them.", "@renjie-liu Thank you. Addressed your three comments.", "@kiszk Can you please address Ubuntu Sanity errors? Thanks!", "Thank you for pinging me. I overlooked pylint error. I have just pushed the fixes.", "@kiszk Can you please resolve conflicts? Thanks!", "@gbaned Sure, done", "@kiszk Can you please resolve conflicts? Thanks!", "@gbaned sure, done", "@kiszk Still, conflicts appearing. Could you please resolve those? Thanks!", "@gbaned sure, resolved.", "@gbaned Fesolved conflict again.", "@gbaned Resolved conflict again.", "@kiszk Can you please resolve conflicts? Thanks!", "@gbaned Thank you for pinging me again. Just resolved the conflicts.", "@gbaned @mihaimaruseac Resolved a conflict again.\r\n", "Manually imported the change and synced to head again. If this still fails to merge I'll suggest splitting it as per previous comment", "Turns out this fails to merge properly. I'm starting a new run to eliminate transient errors but I think it would be better to split it (and resync on master). Apologies for the extra work you'll have to do.", "@kiszk Can you please check mihaimaruseac's comments and keep us posted? Thanks!", "@gbaned thank you for pinging me. I overlooked the comments. I will split this PR into multiple PRs within next few days\r\n```\r\ntensorflow/lite/\r\ntensorflow/lite/c\r\ntensorflow/lite/delegates\r\ntensorflow/lite/experimental\r\ntensorflow/lite/g3doc\r\ntensorflow/lite/kernels\r\ntensorflow/lite/lib_package\r\ntensorflow/lite/micro\r\ntensorflow/lite/python\r\ntensorflow/lite/testing\r\ntensorflow/lite/toco\r\ntensorflow/lite/tools\r\n```\r\n ", "@kiszk  Sure, Thank you very much for the update. ", "You can ping me/assign to me all of the subsequent PRs. Thank you", "Once all subdirectories are fixed, we can sync this back on master to get the files that are left out. Or, we can just close it now and get the other PRs as needed.\r\n\r\nThank you for all the fixes.", "I created all of sub-PRs. When they are closed, I think that it would be good to close this PR, too.", "Sounds good. Thank you", "Everything seems solved. Let's rebase this on master if there is still work left to do or close otherwise.\r\n\r\nThank you", "@mihaimaruseac Thank you very much. It is the time to close this.", "Thank you for all the work and for the patience to carry on these PRs over 4 months"]}, {"number": 35285, "title": "NFC - minor spelling tweaks under core directory", "body": "This PR addresses minor spelling tweaks under `tensorflow/core` directory.\r\nfollow-on of #34958", "comments": ["@kiszk Can you please resolve conflicts? Thanks!", "@gbaned Thank you for pinging me, too. I have just resolved the conflict.", "@gbaned I have just resolved the conflict.", "@kiszk  Conflicts appearing. Could you please resolve those? Thanks!", "@gbaned sure, resolved.", "@kiszk There are new conflicts, can you please resolve those? Thanks!", "@gbaned Thank you for pinging me, just resolved.", "@gbaned @jaingaurav Again, resolved conflicts.", "@gbaned @jaingaurav Again, resolved conflicts.", "@kiszk Conflicts appearing. Could you please resolve those? Thanks!", "@gbaned Thank you for pinging me again. Just resolved it.", "@kiszk Can you please check mihaimaruseac's comments and keep us posted? Thanks!", "@gbaned Thank you for pinging me. I overlooked the comment. I have just addressed the comment.", "@mihaimaruseac @gbaned Resolved a conflict again.", "@mihaimaruseac @gbaned Thank you very much"]}, {"number": 35284, "title": "Revert \"<release 2.1>-<rc1> cherry-pick request: update tflite op versions\"", "body": "Reverts tensorflow/tensorflow#34662", "comments": []}, {"number": 35282, "title": "2.1 cherry-pick request: Fix doc formatting in dataset_ops.py", "body": "The effect can be seen in the Args section of https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#shuffle\r\n\r\nPiperOrigin-RevId: 286304265\r\nChange-Id: I318caf0b33a92d881ad42065d0e4a7a603d91fc0", "comments": []}, {"number": 35281, "title": "Fix images and tests for Cuda 10.1 and TensorRT 6", "body": "This change makes the dockerfile build tests pass. They have been broken\never since CUDA 10.1, and I fixed them by removing dependency on the old\nci_build configure scripts and by updating TensorRT to version 6.\n\nUnfortunately, TensorRT builds seem to be broken at head (see\nhttps://github.com/tensorflow/tensorflow/issues/35115), so I've had to\ndisable TensorRT for the build tests. However, this should make the\ndevel images able to build regularly again.", "comments": []}, {"number": 35280, "title": "tensorflow error issue python3.8.1", "body": "```\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\mark scorp lezeret\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\mark scorp lezeret\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\mark scorp lezeret\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\mark scorp lezeret\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 114\r\n    def TFE_ContextOptionsSetAsync(arg1, async):\r\n                                         ^\r\nSyntaxError: invalid syntax\r\n```", "comments": ["Please fill in template. Note that we don't yet release python 3.8 pip packages.", "Dont work on anny visjoner of python anny tips", "We also don't release pips for 32 bits systems.\r\n\r\nWithout a proper issue template being filled in, we can't know how to progress.", "@MRScorpcore,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 35279, "title": "Better warning log on allocation failures.", "body": "Allocation failure is based on a threshold of available ram and not the total ram of the system.  Enhanced the log message to make that more obvious.", "comments": ["Perhaps \"available\"?", "> Perhaps \"available\"?\r\n\r\nActually \"available\" may be misinterpreted, wondering if users will be confused between total available memory vs free memory?  \r\n'Free' explicitly states it is the 'free' memory that we are talking about.", "OK free seems indeed better, since that's what sysinfo doc is saying (http://man7.org/linux/man-pages/man2/sysinfo.2.html)", "Thank you!\r\n\r\n"]}, {"number": 35278, "title": "Pin scipy to 1.4.1.", "body": "Fixes segfaults caused by scipy/scipy#11237 before 1.4.1 (observed at scipy==1.4.0 and any version of TF and scipy==1.1.0 and TF==2.1.0rc1 on a specific VM setup)\r\n\r\nPiperOrigin-RevId: 286416747\r\nChange-Id: I9f66f9145517d3b9279883a9292ae050b0dfa555", "comments": []}, {"number": 35277, "title": "TF-Lite Micro: Selectively omit data types at compile-time", "body": "My application only requires kTfLiteInt8 kernels.\r\nHowever, kTfLiteFloat32 and kTfLiteUInt8 kernels are also built into the application.\r\n\r\nIt would save a considerable amount of code space if there was a way to disable building in the unused data types, e.g.:\r\n```\r\n  switch (input->type) {  // Already know in/out types are same.\r\n#ifndef TFLITE_FLOAT32_DISABLED\r\n    case kTfLiteFloat32:\r\n      return EvalFloat(context, node, params, &data, input, filter, bias,\r\n                       nullptr, nullptr, output);\r\n      break;\r\n#endif \r\n#infdef TFLITE_INT8_DISABLED\r\n    case kTfLiteInt8:\r\n      return EvalQuantizedPerChannel(context, node, params, &data, input,\r\n                                     filter, bias, output, nullptr);\r\n      break; \r\n#endif \r\n#ifndef TFLITE_UINT8_DISABLED\r\n    case kTfLiteUInt8:\r\n      return EvalQuantized(context, node, params, &data, input, filter, bias,\r\n                           nullptr, nullptr, output);\r\n      break;\r\n#endif\r\n    default:\r\n      context->ReportError(context, \"Type %s (%d) not supported.\",\r\n                           TfLiteTypeGetName(input->type), input->type);\r\n      return kTfLiteError\r\n```\r\n\r\nOr something more elegant ;) \r\n", "comments": ["@advaitjain ping :) ", "Hello @driedler , I'm facing exactly the same issue as you, were you able to select only int8 type ?", "Closing this issue in favor of https://github.com/tensorflow/tflite-micro/issues/720", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35277\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35277\">No</a>\n"]}, {"number": 35276, "title": "Update math_ops.py", "body": "Had updated the tf.multiply module and taken out additional comments", "comments": ["You can use the other PR (#35229), no need to create two."]}, {"number": 35275, "title": "TensorFlow building error: debug_ops_gpu.cu.cc(47): error: more than one instance of overloaded function \"isinf\" matches the argument list", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 16.04.6`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `N/A`\r\n- TensorFlow installed from (source or binary): `source`\r\n- TensorFlow version: `2.0.0 (master)`\r\n- Python version: `Python 3.5.2`\r\n- Installed using virtualenv? pip? conda?: `apt`\r\n- Bazel version (if compiling from source): `1.1.0`\r\n- GCC/Compiler version (if compiling from source): `gcc 5.4.0`\r\n- CUDA/cuDNN version: `CUDA 10.2 / cuDNN 7.6.5.32-1+cuda10.2`\r\n- GPU model and memory: `NVIDIA Tesla V100`\r\n\r\n\r\n\r\n**Describe the problem**\r\nObserved after 316bd31e02b78a071d2f7f5a87898dd5f125f371 commit.\r\nTensorFlow building fails with the error:\r\n```\r\n[2019-12-19T02:07:55.208Z] tensorflow/core/kernels/debug_ops_gpu.cu.cc(47): error: more than one instance of overloaded function \"isinf\" matches the argument list:\r\n[2019-12-19T02:07:55.208Z]             function \"isinf(float)\"\r\n[2019-12-19T02:07:55.208Z]             function \"isinf(double)\"\r\n[2019-12-19T02:07:55.208Z]             function \"isinf(long double)\"\r\n[2019-12-19T02:07:55.209Z]             argument types are: (const tensorflow::int16)\r\n[2019-12-19T02:07:55.209Z]           detected during:\r\n[2019-12-19T02:07:55.209Z]             instantiation of \"void tensorflow::<unnamed>::CurtHealthKernel(const Tin *, int, Tout *) [with Tin=tensorflow::int16, Tout=float]\" \r\n[2019-12-19T02:07:55.209Z] (163): here\r\n[2019-12-19T02:07:55.209Z]             instantiation of \"void tensorflow::CurtHealthLaunch<Tin, Tout>::Run(const tensorflow::<unnamed>::GPUDevice &, const Tin *, int, Tout *) [with Tin=tensorflow::int16, Tout=float]\" \r\n[2019-12-19T02:07:55.209Z] (171): here\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n.tf_configure.bazelrc:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python3.5/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_CUDA_VERSION=\"10.2\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_NCCL_VERSION=\"2.6.0\"\r\nbuild --action_env TF_CUDA_PATHS=\"/hpc/local/oss/cuda10.2/cuda-toolkit,/usr,/usr/local/cuda\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/hpc/local/oss/cuda10.2/cuda-toolkit\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr\"\r\nbuild --action_env NCCL_INSTALL_PATH=\"<cut>/nccl/stable\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"7.0\"\r\nbuild --action_env LD_LIBRARY_PATH=\"<cut>/nccl/stable/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/nccl_rdma_sharp_plugin/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/ucx/lib/ucx:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/ucx/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/sharp/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/hcoll/lib:<cut>/ci_tools_do_not_remove/hpcx-v2.6.pre-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-ubuntu16.04-x86_64/ompi/lib:/hpc/local/oss/cuda10.2/cuda-toolkit/lib64:/hpc/local/oss/cuda10.2/cuda-toolkit/lib64/stubs:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc-5\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_env=LD_LIBRARY_PATH\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nCC: @caisq\r\n", "comments": ["@itemko A fix is on its way. It should land in GitHub sometime today.", "@caisq something similar I encountered too but on Windows 10 with cuda (GTX 1060 6GB 6.1) capable machine\r\n\r\n```\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt\\corecrt_math.h(415): error: more than one instance of overloaded function \"fpclassify\" matches the argument list:\r\n            function \"fpclassify(float)\"\r\n            function \"fpclassify(double)\"\r\n            function \"fpclassify(long double)\"\r\n            argument types are: (tensorflow::int32)\r\n          detected during:\r\n            instantiation of \"__nv_bool isnan(_Ty) [with _Ty=tensorflow::int32]\"\r\ntensorflow/core/kernels/debug_ops_gpu.cu.cc(47): here\r\n            instantiation of \"void tensorflow::<unnamed>::CurtHealthKernel(const Tin *, int, Tout *) [with Tin=tensorflow::int32, Tout=float]\"\r\ntensorflow/core/kernels/debug_ops_gpu.cu.cc(163): here\r\n            instantiation of \"void tensorflow::CurtHealthLaunch<Tin, Tout>::Run(const tensorflow::<unnamed>::GPUDevice &, const Tin *, int, Tout *) [with Tin=tensorflow::int32, Tout=float]\"\r\ntensorflow/core/kernels/debug_ops_gpu.cu.cc(172): here\r\n\r\n4 errors detected in the compilation of \"C:/Users/Bha~/AppData/Local/Temp/nvcc_inter_files_tmp_dir/tmp2ajahmt5/debug_ops_gpu.cu.cpp1.ii\"\r\n\r\n```", "It seems the issue is fixed - our TensorFlow building scenario has passed.", "@itemko thanks for the confirmation."]}, {"number": 35274, "title": "[ROCm] Minor updates to ROCm CI scripts and Dockerfile(s)", "body": "Also removing `no_rocm` tag from tests that are now passing on the rocm platform\r\n\r\n---------------------------\r\n\r\n/cc @whchung @chsigg ", "comments": []}, {"number": 35273, "title": "Update version numbers for TensorFlow 2.1.0-rc2", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 1 -> 1\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.1.0-rc1\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.1.0rc1\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 35272, "title": "While using tf.import_graph_def: ValueError: Input 1 of node StatefulPartitionedCall", "body": "I am trying to freeze and obfuscate my model but when running:\r\ntf.import_graph_def(graph_def, name='')\r\n\r\nI am getting the error:\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"a.py\", line 129, in <module>\r\n    optimize_model('/home/xxxx/Desktop/Pervasive/Projects/Ofuscar/1576096897/','/home/xxxx/Desktop/Pervasive/Projects/Ofuscar/out/')\r\n  File \"a.py\", line 123, in optimize_model\r\n    convert_graph_def_to_saved_model(output_model_dir, input_nodes, output_nodes, graph_filepath)\r\n  File \"a.py\", line 99, in convert_graph_def_to_saved_model\r\n    tf.import_graph_def(graph_def, name='')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/importer.py\", line 405, in import_graph_def\r\n    producer_op_list=producer_op_list)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/importer.py\", line 505, in _import_graph_def_internal\r\n    raise ValueError(str(e))\r\nValueError: Input 1 of node StatefulPartitionedCall was passed float from xxxx/conv0/weights:0 incompatible with expected resource.\r\n\r\nThe graph_def was loaded with the function:\r\ndef get_graph_def_from_file(graph_filepath):\r\n    with tf.Graph().as_default():\r\n        with tf.gfile.GFile(graph_filepath, 'rb') as f:\r\n            graph_def = tf.GraphDef()\r\n            graph_def.ParseFromString(f.read())\r\n            return graph_def ", "comments": ["@caibadal5, Please provide the complete standalone code and Tensorflow version that your using. Thanks! ", "@caibadal5, Any update on code!", "The main part of the code is the one I showed. I have used tf 1.15.0 both for the saved model and for this code. Thank you for your patience!", "@caibadal5, Looks like code is incomplete please provide the complete code and necessary files to replicate the issue. Thanks!", "It's a model that we are selling to a company, I don't feel comfortable publishing it on git :) Thanks for your interest anyway", "@caibadal5, Can you provide minimal code snippet which replicates the reported issue to analyze. ", "Closing the issue. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Were you able to resolve this issue? I am having something similar when importing a graph with tf1.15", "Were you able to resolve this issue? I am having something similar when importing a graph with tf 2.4"]}, {"number": 35271, "title": "Fixing an issue where scipy and tensorflow cannot be imported at the \u2026", "body": "\u2026same time.\r\n\r\nPiperOrigin-RevId: 286313739\r\nChange-Id: I9d4dfb66a8e82c56249bc7f7b70c71d228912e9f", "comments": []}, {"number": 35270, "title": "Revert \"Revert \"[r2.1 cherry-pick] Fix pip package API generation\"\"", "body": "Reverts tensorflow/tensorflow#35234\r\n\r\nThis was not the culprit. The segfaults were caused by scipy.", "comments": []}]