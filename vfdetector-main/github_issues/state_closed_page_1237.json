[{"number": 16049, "title": "Windows: Batch script for building the CUDA-enabled C library.", "body": "Follow up to PR #15878 to help resolve #11602\r\n\r\n", "comments": []}, {"number": 16048, "title": "Cannot compile with Visual Studio 15", "body": "If I build a version of the current tensorflow version 1.5.0RC on Windows with CMAKE and Visual Studio 15 following error is occurred:\r\n\r\n1>sparse_column_iterable.cc\r\n1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): error C2678: binary '*': no operator found which takes a left-hand operand of type 'const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion)\r\n1>D:\\....\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc(54): note: could be 'const __int64 &tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator::operator *(void)'\r\n1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): note: while trying to match the argument list '(const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator)'\r\n1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2439): note: see reference to function template instantiation '_FwdIt std::_Lower_bound_unchecked<_Iter,_Ty,_Fn>(_FwdIt,_FwdIt,const _Ty &,_Pr)' being compiled\r\n1>        with\r\n1>        [\r\n1>            _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n1>            _Iter=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n1>            _Ty=tensorflow::int64,\r\n1>            _Fn=std::less<void>,\r\n1>            _Pr=std::less<void>\r\n1>        ]\r\n1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2447): note: see reference to function template instantiation '_FwdIt std::lower_bound<_FwdIt,_Ty,std::less<void>>(_FwdIt,_FwdIt,const _Ty &,_Pr)' being compiled\r\n1>        with\r\n1>        [\r\n1>            _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n1>            _Ty=tensorflow::int64,\r\n1>            _Pr=std::less<void>\r\n1>        ]\r\n1>D:\\....\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc(119): note: see reference to function template instantiation '_FwdIt std::lower_bound<tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,tensorflow::int64>(_FwdIt,_FwdIt,const _Ty &)' being compiled\r\n1>        with\r\n1>        [\r\n1>            _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n1>            _Ty=tensorflow::int64\r\n1>        ]\r\n1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): error C2100: illegal indirection\r\n1>Done building project \"tf_core_kernels.vcxproj\" -- FAILED.\r\n\r\nThis problem is also discussed in #12000 after closing this issue.\r\n\r\n###System information\r\n tensorflow 1.5.0RC\r\n Windows 10\r\n VisualStudio Prof. 2017\r\n CMake 3.10.1", "comments": ["For me  Visual Studio 2015 Update 3 is working. I am currently testing for Visual Studio 2017. Will update you on how it goes.", "I'm very confused with the versions numbers of Visual Studio. I use 15.5.3: that is Visual Studio 2017!!", "You are right and not seem to be confused. I have build tensorflow using cmake and Visual Studio 2015 update 3 release with version 14.x.x. I think you can try to build it using Visual Studio 2015 Update 3 Community Edition.", "I can confirm that Visual Studio 2015 Version 14.1 works!", "Here is Step by Step instruction to build tensorflow gpu version 1.5.0 on Windows OS. Hope this helps for windows users. Let me know if the tutorial worked. \r\nVisit my blog here. http://www.python36.com/install-tensorflow-gpu-windows", "in #15925 is a solution\r\nThanks to boblw for the info!", "gpu version of tensorflow cannot compiled with Visual Studio 2017 and toolset v141. The example of nvcc you must compile with toolset 14.0!"]}, {"number": 16047, "title": "Branch 181629980", "body": "", "comments": ["@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please."]}, {"number": 16046, "title": "Feature Request: clarify supported environments for official binaries.", "body": "As it stands now, binary release of TensorFlow 1.5 is set to drop compatibility with Ubuntu 14.04 ( https://github.com/tensorflow/tensorflow/issues/15777), and compatibility with Debian Linux distros, such as Amazon Linux AMI (`ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found`).\r\n\r\nTo avoid surprise, TensorFlow should either:\r\n1. Follow other open-source projects like Ray/PyTorch and provide official binaries for these systems\r\nor\r\n2. Document that support is dropped, to encourage other players (ie, AWS) to take over the job of providing these binaries\r\n\r\n@martinwicke", "comments": ["@gunan at the very least we must update the support matrix, which is public. \r\n\r\nHowever, there is a valid question as to whether we can continue to have our binaries work on 14.04 simply by building against\u00a0an older GLIBC. ", "@yaroslavvb Debian stretch uses `glibc 2.24`. This is new than Ubuntu 16.04's `glibc 2.23`. I think Debian stretch might still work (haven't tried yet).", "PS: Amazon Linux uses glibc 2.18", "I'm surprised to hear that about Amazon. It seems that a few weeks ago [Amazon Linux 2](https://aws.amazon.com/amazon-linux-2/release-notes/) was released which has glibc 2.25.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@jart I guess it's still in the \"candidate\" stage, the images I launched recently still have previous version", "IE, Deep Learning AMI's are still using Amazon Linux 1 -- https://aws.amazon.com/marketplace/pp/B01M0AXXQB", "I was playing with AWA Linux 2 and noticed there is a small build failure when compiling TensorFlow (missing `#include <functional>`). Added a PR #16470 for that fix.", "Closing since it seems that the PR was merged. Feel free to reopen if there's a remaining issue.", "it's still not clear if Ubuntu 14.04 binaries will be supported. Someone mentioned Google possibly providing 14.04 binaries to allow compatibility with GCP dataflow containers which are on 14.04", "The 1.5 final binaries should work on 14.04. If they don't we made some mistake and will patch. "]}, {"number": 16045, "title": "XLA: Won't converge and doesn't respect visible devices.", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (writing a copy I can share)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Nvidia Optimized Container 17.12\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: IntelPython 3.5\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9.0.176 with CUBLAS Basic Accelerated Linear Algebra 9.0.234 / 7\r\n- **GPU model and memory**: Pascal Titan 12Gb\r\n- **Exact command to reproduce**: Coming\r\n\r\n### Describe the problem\r\nI've seen 2 problems with XLA:\r\n1) XLA doesn't respect visible devices (neither CUDA Visible devices or tf config options)\r\n2) XLA doesn't converge\r\n\r\n_1_\r\nXLA doesn't respect visible devices, I have seen this in both Horovod and when running on a single GPU (on a Multi-GPU desktop), `N` XLA instances are created despite only one GPU being used.\r\n\r\n_2_ \r\nWhen I run my script with and without XLA, the XLA compiled version plateaus and doesn't converge, whilst the none-XLA version converges. Where XLA has a loss of a magnitude higher.\r\nThe benchmark is a VGG network (with BatchNorm) and achieves a 3x speed up with XLA, it just doesn't converge.\r\n\r\nIs this a known issue? Or is it just a possibility of occurring?\r\n\r\n", "comments": ["@jlebar do you know about these problems with XLA?", "(For future reference, it's a lot better to file two separate issues if you have two problems.  Otherwise when the bug becomes long, it's hard to track the problems.)\r\n\r\nI use CUDA_VISIBLE_DEVICES with XLA all the time and it works fine.  I'm pretty surprised to hear that it's not working for you, because it takes effect in an nvidia library that's much lower-level than TF or XLA.  That is, because neither TF nor XLA is responsible for respecting this env var, I suspect the environment variable is getting stripped out somewhere in your setup.  Without steps to reproduce, I can't say more.\r\n\r\nI'm not a TF person, so I can't speak to the TF config options not working, but again, I think a TF person would need additional information, probably concrete steps to reproduce, in order to assist.\r\n\r\nWith respect to your model not converging, that's very concerning.  But again I'd need concrete steps to reproduce in order to debug this.", "Having debugged _1_ a bit more, it seems that Tensorflow (or underlying libraries) does not totally respect the `config.gpu_options.visible_device_list`.\r\nThis results in a minor `10Mb` being taken when not using XLA and `500Mb` when using XLA on GPUs which are not in use in that process. I.e. Having an 8 Card system and wanting to run 4 jobs results in memory issues.\r\n\r\nI'm still debugging the _2nd_ issue on my end, as our internal benchmarks are tuned towards our private data - I'm trying to replicate in tf_benchmarks to see if it is a container problem/benchmark code issue/XLA code issue. ", "One possibility for (2) that I just thought of: ptxas in CUDA 9 causes miscompiles with XLA until version 9.0.276.  I checked in a change this week (not sure if it's made its way to open-source yet) that prints a warning if you're using a bad version.\r\n\r\nWe're still on cuda 8 + cudnn 6 at Google.  I can't rule out the possibility of additional bugs in cuda 9 / cudnn 7 that we haven't yet discovered.\r\n\r\nLooking online, it appears that one can only download cuda 9.1 from nvidia these days?  That's unfortunate, we have done no testing with that at all.  :(  I have no idea if the public build of 9.1 has the ptxas bug fix, nor what other new and exciting bugs it contains.", "Thank you, I'll try running with those suggestions!", "Did you mean CUDA 9.0.176? - In which case we have that version and still get the result.\r\n\r\nI'll try CUDA 8 + cudnn 6, but I'll have to run that on a desktop as it doesn't support the new voltas (the build fails due to compute capability).", "9.0.176 is known bad, confirmed on our end.  nvidia tells us that 9.0.276 is the first 9.0.x version with the fix, but that the first public version with the fix is 9.0.282.", "Thanks! I\u2019ll chase them up as I can\u2019t find it on their site.\r\n\r\nHowever, I also confirmed I still have an issue on TF 1.5rc and Cuda/Cudnn 8/6. \r\n\r\nWould a debug report like #11564 help? - I\u2019ll hopefully have a script ready next week to recreate the issue.", "> Would a debug report like #11564 help?\r\n\r\nYes, it would.", "Here are the logs with the following flags: with \"--tf_xla_parallel_checking=true --parallel_check_failfast=false\"\r\n\r\n[vgg_parallel_exec_tests.txt](https://github.com/tensorflow/tensorflow/files/1631436/vgg_parallel_exec_tests.txt)\r\n\r\nI've also updated the information at the top. Working on a script I can share to reproduce.\r\n", "Additional info:\r\n- Ran with `tf.data` api, NCCL replicated (tried all parameter servers) and staging areas on both the CPU and GPU.\r\n- The model is a fully convolutional network with skip connections, like resnet.", "Would all of the *.dot files help? - Nearly finished code I can share (Other things to run in parallel.)", "The dot files wouldn't help a lot, but if you can dump a SessionModule proto (--xla_dump_computations_to=/tmp/foo, possibly passing the flag via the TF_XLA_FLAGS envvar), that is probably enough for us to reproduce.  You'd want to dump this from TF built at or near to HEAD, as that format changes, and our tools can't necessarily go back.", "Just tried it, I don't think it's new enough, sadly.", "CUDA 9.1.85 is, I am told, the first public version to contain the fix to the ptxas problems we've been having.\r\n\r\nUnfortunately I cannot vouch for CUDA 9.1 at all.", "Sorry, nvidia gave me incorrect information.  9.1.85 is known-bad, and there does not appear to be a fix publicly available at the moment.", "Thanks, I seemingly still had the problem with CUDA 8 and cudnn 6 also. \r\n\r\nCurrently building a master container with 9/7 to get those computations and then will build an 8/6 one to follow it up (just to ensure it is/isn't ptxas). ", "[xlacomputations.zip](https://github.com/tensorflow/tensorflow/files/1643208/xlacomputations.zip)\r\n\r\nHere are the computations ran with TF Master, CUDA 8 and Cudnn 6, same proplem with the loss not going down.", "These SessionModules work, thanks.\r\n\r\nIt looks like I'm going to need to build some tools in order to figure out what's going wrong here.  Stay tuned, I guess...", "Thanks for keeping me in the loop!", "Spent a while debugging this today.  I've found a number of XLA computations which generate different values than we get with the reference backend, but this isn't necessarily a sign of a bug.  In particular, cudnn convolution isn't necessarily deterministic or even particularly accurate, depending on the algorithm we choose.\r\n\r\nIn theory XLA and TF use pretty much the same method for choosing cudnn algorithms, so I'd be surprised if this was the source of your problem.  But it's making it hard for me to tell what, if anything, is going wrong here...\r\n\r\nI'm going to try to spend another day poking around at the XLA level to see if I can see what's going wrong here, but we may need that e2e testcase after all.\r\n\r\ncc @hawkinsp, who may have ideas himself.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "I haven't been able to find anything suspicious in the SessionModules.  :(\r\n\r\nThat's not to say that something isn't broken -- I think something probably is.  But just that whatever is broken isn't obvious when I shove random data through these XLA parts.\r\n\r\nA TF reproducer would very likely help.", "Marking as \"awaiting response\", since it appears a TF reproducer is needed to debug this.\r\n\r\nAs for the issue \"XLA doesn't respect visible devices (neither CUDA Visible devices or tf config options)\", @joeyearsley, can you file a separate issue?", "Hi @reedwm, I'll try and get the issue up once I've confirmed it on the latest release.\r\n", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "> CUDA 9.1.85 is, I am told, the first public version to contain the fix to the ptxas problems we've been having.\r\n\r\nCUDA 9.1.85.2 has just been released, and it fixes this problem.", "@rapatel0 is looking into this on our end now, he'll get back hopefully by the end of the day (UK/GMT)", "Tested with 9.1.85 and its broken but I didn't see that the patches were not applied in nvidia's default container. Will patch and get back to you \r\n", "I added the cuda patches to my existing docker container and ran the test with the XLA streamexecutor running on CPU+GPU, GPU only, and a model without XLA as a baseline. We set the global seed in all cases and all image sizes and batch sizes are the same.\r\n \r\n- After 70,000 iterations the noXLA model reaches a loss of 0.19  and is still trending downward.\r\n- After 120,000 iterations both XLA models oscillate between losses of 0.3-0.4 and continue to oscillate until a limit of 200,000 iterations. \r\n\r\nI still need to try a test with patches applied before compiling tensorflow. Not sure if this will make a difference. ", "Nagging Assignee @jlebar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jlebar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Still getting bad shape problems on Cuda 9.1 where the filter sizes/strides/shapes are garbage w/ ptxas 9.1.121, Cudnn 7.1.2, Tensorflow built from source running on V100s:\r\n\r\nCouldn't allocate space for input/filter/output of convolution %custom-call.24 = (f32[20,64,80,64]{3,2,1,0}, u8[0]{0}) custom-call(f32[20,2078,3238,64]{3,2,1,0} %pad.3, f32[125,157,64,64]{3,2,1,0} %multiply.62, s64[] %constant.161), window={size=125x157 stride=31x39}, dim_labels=b01f_01io->b01f, custom_call_target=\"__cudnn$convForward\". Falling back to default algorithm.\r\n\r\nSegmentation fault (core dumped)\r\n\r\nDo you know if ptxas still having issues or is this a stable version and the problem is somewhere else in the XLA/JIT pipeline?", "@TylerBalsam , this error seems quite different from the other etiologies discussed in this bug.\r\n\r\n\"Couldn't allocate space\" error is coming from cudnn_algorithm_picker.  It's trying to choose the fastest cudnn algorithm to use for the given convolution.  Which is not so surprising -- the input to the conv has size `20*2078*3238*64 * 4 bytes = 34gb`, which is not going to fit on a V100.\r\n\r\nThis probably is a failure to pattern-match a backwards-input or backwards-filter conv to the correct cudnn call.  If you can give us steps to reproduce this (probably in a separate bug?), we can probably fix that.\r\n\r\nI'm not sure what the segfault is about.  We shouldn't segfault on GPU OOM, but it's entirely possible the CPU is OOM'ing somewhere or something.  Not really enough info here to say.", "I'm not aware of any bugs in ptxas 9.1.121 that affect TF or XLA.", "I'm going to close this bug due to inactivity on the initially reported problem, but please feel free to reopen if that persists, and feel free to cc me on other XLA issues."]}, {"number": 16044, "title": "Feature Request: tf.multi_one_hot that is one-hot encoding multiple columns of a Tensor", "body": "Hi there,\r\n\r\nI just wrote a function that creates multiple one-hot-encodings for a tensor and concatenates them. I was curious whether this might serve some others and contribute this feature.\r\n\r\n\r\n```\r\ndef multiple_one_hot(cat_tensor, depth_list):\r\n    \"\"\"Creates one-hot-encodings for multiple categorical attributes and\r\n    concatenates the resulting encodings\r\n\r\n    Args:\r\n        cat_tensor (tf.Tensor): tensor with mutiple columns containing categorical features\r\n        depth_list (list): list of the no. of values (depth) for each categorical\r\n\r\n    Returns:\r\n        one_hot_enc_tensor (tf.Tensor): concatenated one-hot-encodings of cat_tensor\r\n    \"\"\"\r\n    one_hot_enc_tensor = tf.one_hot(cat_int_tensor[:,0], depth_list[0], axis=1)\r\n    for col in range(1, len(depth_list)):\r\n        add = tf.one_hot(cat_int_tensor[:,col], depth_list[col], axis=1)\r\n        one_hot_enc_tensor = tf.concat([one_hot_enc_tensor, add], axis=1)\r\n\r\n    return one_hot_enc_tensor\r\n\r\n```\r\nI am happy for your feedback. Tell me if you think others might profit and I would enjoy to create a pull request ;)", "comments": ["@squall-1002 thanks for volunteering! I will mark \"contributions welcome\". ", "@squall-1002 If you are not working on it can I make a PR?", "@divyanshj16 Thanks, I just pushed it and created the PR, but feel free to comment ;)", "@squall-1002 Could you give some practical example of this method?\r\nI checked the tests but I am not sure of a use case where I would benefit from this, hence reverting to you for more details (maybe you also want to add the example in the documentation of the method for others like me :)", "@wileeam Of course: Recommender Systems ;)\r\nIn RecSys I am dealing with lots of features of varying modalities, among them many categorical features. Assume we work on vehicle recommendations for an online market. I have colors, fuel types, maker IDs, model IDs, etc. The way we treat those features is to one-hot encode them. But, where colors or fuel types may have tens of possible values, we may have thousands of model IDs which is why I can't include this as a test as this would be way too much.\r\nIn this case and I assume in others where people deal with multiple categorical features, one could profit from a higher abstraction of **tf.one_hot** which is why I propose **tf.multi_one_hot**.\r\nHope that helps you to understand the practical use of it.", "hi, @squall-1002 , in recommender system, anyway we need to do some data process to merge the ID features (such as maker IDs) into a list.\r\n\r\nDo you think the code below is working as you expected?\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# some sparse features, from raw maker_id to index\r\nmaker_id_list = [1, 3, 9, 14, 2]\r\none_hot_enc = tf.one_hot(indices=maker_id_list, depth=16)\r\n\r\nwith tf.Session() as sess:\r\n    feature_list = tf.reduce_sum(one_hot_enc, reduction_indices=0)\r\n    print(sess.run(feature_list))\r\n```\r\n\r\nThe output is\r\n```\r\n[0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\r\n```", "(API review)\r\n\r\nI just closed #16300, which contained an implementation. We believe this function is too specialized and too hard to understand and use effectively to be of general use. I'll close this feature request.\r\n\r\nSorry about the delay in making this decision.", "Amazing function !!! Very useful for ML models ! However it seems like a part is missing cat_int_tensors vs cat_tensors ", "\r\nFor people who would like to use `tf.one_hot()` function for a multi-label classification problem (e.g. multi-label text classification):\r\n\r\nModified version of the code made by @lenjoy  :\r\n```\r\nimport tensorflow as tf\r\n\r\nindices = tf.ragged.constant([[1, 2], [1], [3, 2]])\r\n\r\none_hot = tf.one_hot(indices, depth=4)\r\none_hot_multi = tf.reduce_max(one_hot, axis=1)\r\n```\r\n\r\nOutputs is in this case: \r\n```\r\n<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\r\narray([[0., 1., 1., 0.],\r\n       [0., 1., 0., 0.],\r\n       [0., 0., 1., 1.]], dtype=float32)>\r\n```\r\n\r\n@martinwicke, I don't particularly agree that the function is too specialized. Multi-label classification problems are quite common nowadays and `tf.one_hot()` is currently not sufficient to make it work. Maybe instead of creating another function, `tf.one_hot()` could be modified to provide this functionality?", ">@martinwicke, I don't particularly agree that the function is too specialized. Multi-label classification problems are quite common nowadays and tf.one_hot() is currently not sufficient to make it work. Maybe instead of creating another function, tf.one_hot() could be modified to provide this functionality?\r\n\r\nI agree with this. The usage of multi-hot labels isn't limited to just recommender systems, but also multi-output networks and multi-task learning. \r\n\r\n", "Any updates on this issue? What's the current recommended way to achieve multi_hot encoding for a set of string labels in TF-2.4? ", "@mhorlacher you can use the following TF2.x snippet (modified from above) to turn integers to multi-hot:\r\n\r\n```\r\nmaker_id_list = [1, 3, 9, 14, 2]\r\none_hot_enc = tf.one_hot(indices=maker_id_list, depth=16)\r\n\r\nfeature_list = tf.reduce_sum(one_hot_enc, axis=0)\r\n```\r\n\r\nTo use this, you have to cast your string labels to integers. I hope this can help you (or others) as a starting point.\r\n\r\nEdit:\r\nUse this to cast a list of string labels to a list of integer labels:\r\n```\r\nlist(map(int, ['1', '5', '9']))\r\n```", "Hi @Yannik1337 - sorry for the late response. Yes this was the option I was going for in the end. Thanks!"]}, {"number": 16043, "title": "fix C2678 error on VS2017 15.5", "body": "fix build fail on Visual Studio 2017 15.5\r\n\r\nerror log\r\n\r\n    1>------ \u5df2\u958b\u59cb\u5efa\u7f6e: \u5c08\u6848: tf_core_kernels, \u7d44\u614b: Debug x64 ------\r\n    1>sparse_column_iterable.cc\r\n    1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): error C2678: \u4e8c\u5143\u904b\u7b97\u5b50 '*': \u627e\u4e0d\u5230\u4f7f\u7528\u5de6\u65b9\u904b\u7b97\u5143\u985e\u578b 'const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' \u7684\u904b\u7b97\u5b50 (\u6216\u662f\u6c92\u6709\u53ef\u63a5\u53d7\u7684\u8f49\u63db)\r\n    1>C:\\Users\\User\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc(54): note: \u53ef\u80fd\u662f 'const __int64 &tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator::operator *(void)'\r\n    1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): note: \u7576\u5617\u8a66\u7b26\u5408\u5f15\u6578\u6e05\u55ae '(const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator)' \u6642\r\n    1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2439): note: \u8acb\u53c3\u95b1\u6240\u8981\u7de8\u8b6f\u7684\u51fd\u5f0f \u6a23\u677f \u5177\u73fe\u5316 '_FwdIt std::_Lower_bound_unchecked<_Iter,_Ty,_Fn>(_FwdIt,_FwdIt,const _Ty &,_Pr)' \u4e4b\u53c3\u8003\r\n    1>        with\r\n    1>        [\r\n    1>            _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n    1>            _Iter=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n    1>            _Ty=tensorflow::int64,\r\n    1>            _Fn=std::less<void>,\r\n    1>            _Pr=std::less<void>\r\n    1>        ]\r\n    1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2447): note: \u8acb\u53c3\u95b1\u6240\u8981\u7de8\u8b6f\u7684\u51fd\u5f0f \u6a23\u677f \u5177\u73fe\u5316 '_FwdIt std::lower_bound<_FwdIt,_Ty,std::less<void>>(_FwdIt,_FwdIt,const _Ty &,_Pr)' \u4e4b\u53c3\u8003\r\n    1>        with\r\n    1>        [\r\n    1>            _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n    1>            _Ty=tensorflow::int64,\r\n    1>            _Pr=std::less<void>\r\n    1>        ]\r\n    1>C:\\Users\\User\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc(119): note: \u8acb\u53c3\u95b1\u6240\u8981\u7de8\u8b6f\u7684\u51fd\u5f0f \u6a23\u677f \u5177\u73fe\u5316 '_FwdIt std::lower_bound<tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,tensorflow::int64>(_FwdIt,_FwdIt,const _Ty &)' \u4e4b\u53c3\u8003\r\n    1>        with\r\n    1>        [\r\n    1>            _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n    1>            _Ty=tensorflow::int64\r\n    1>        ]\r\n    1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): error C2100: \u4e0d\u5408\u6cd5\u7684\u9593\u63a5\u53d6\u503c\r\n    1>\u5c08\u6848 \"tf_core_kernels.vcxproj\" \u5efa\u7f6e\u5b8c\u6210 -- \u5931\u6557\u3002\r\n    ========== \u5efa\u7f6e: 0 \u6210\u529f\u30011 \u5931\u6557\u30010 \u6700\u65b0\u30010 \u7565\u904e ==========", "comments": ["Can one of the admins verify this patch?"]}, {"number": 16042, "title": "tf.contrib.cudnn_rnn.CudnnGRU does not work with input_mode='skip_input' in TF1.5", "body": "### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: `pip install tensorflow-gpu==1.5.0rc0`\r\n- **TensorFlow version**: v1.3.0-rc1-6090-g622487f 1.5.0-rc0\r\n- **Python version**:  3.5\r\n- **CUDA/cuDNN version**: CUDA 9.0, CuDNN 7.0.5\r\n- **GPU model and memory**: GTX 1080 8GB\r\n\r\n### Describe the problem\r\nWe are running tf.contrib.cudnn_rnn.CudnnGRU in our speech recognition setup with input_mode='skip_input' and it crashes the whole process. Here is the assertion error that we are getting:\r\n\r\n`Check failed: size == params_input[i].NumElements() Params size mismatch. Expected 0, got 10000`\r\n\r\nLooks like it's crashing here: [tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc:440](https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc#L440). It does not crash if input_mode is 'linear_input'.\r\n\r\nHere is the minimal example to reproduce:\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\nlayer = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=100,\r\n                             input_mode='skip_input', direction='bidirectional')\r\n# (time, batch_size, num_inputs)\r\nx = tf.random_normal((100, 16, 100))\r\ny = layer(x)\r\n\r\nwith tf.Session() as sess:\r\n\tsess.run(tf.global_variables_initializer())\r\n\tprint(sess.run(y))\r\n\r\n```\r\n\r\nLogs:\r\n\r\n```\r\n2018-01-11 17:10:44.858413: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-01-11 17:10:45.067854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8095\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\r\n2018-01-11 17:10:45.274222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 1 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8095\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\r\n2018-01-11 17:10:45.274776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device peer to peer matrix\r\n2018-01-11 17:10:45.274804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126] DMA: 0 1 \r\n2018-01-11 17:10:45.274812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 0:   Y Y \r\n2018-01-11 17:10:45.274817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 1:   Y Y \r\n2018-01-11 17:10:45.274826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2018-01-11 17:10:45.274832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\n2018-01-11 17:10:46.240862: F tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc:440] Check failed: size == params_input[i].NumElements() Params size mismatch. Expected 0, got 10000\r\nAborted (core dumped)\r\n```\r\n\r\n### Thoughts\r\n\r\nI think the problem is in python wrapper code located at [contrib/cudnn_rnn/python/layers/cudnn_rnn.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py).\r\n\r\nBoth [`_canonical_weight_shape(self, layer)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py#L417) and [`_canonical_bias_shape(self, layer)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py#L443) don't handle the case when `input_mode='skip_input'`. The wrapper code doesn't even check if `input_size == num_units` when `input_mode='skip_input'` as it should! The issue seems to be easy to fix, but I may be mistaken.\r\n\r\nHere is an example that shows that both 'skip_input' and 'linear_input' get the same set of canonical parameter shapes:\r\n```\r\nCudnnGRU = lambda **kwargs: tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=7, direction='bidirectional', **kwargs)\r\n\r\nlayer = CudnnGRU(input_mode='skip_input')\r\nlayer.build((200, 16, 5))\r\nprint(\"skip_input weights\", layer.canonical_weight_shapes)\r\nprint(\"skip_input biases\", layer.canonical_bias_shapes)\r\n\r\nlayer = CudnnGRU(input_mode='linear_input')\r\nlayer.build((200, 16, 5))\r\nprint(\"linear_input_weights\", layer.canonical_weight_shapes)\r\nprint(\"linear_input_biases\", layer.canonical_bias_shapes)\r\n```\r\n\r\nExample output:\r\n```\r\nskip_input weights [(7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7), (7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7)]\r\nskip_input biases [[7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7]]\r\nlinear_input_weights [(7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7), (7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7)]\r\nlinear_input_biases [[7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7]]\r\n```\r\n\r\n### Workaround\r\n\r\nIn our setup we decided that we will keep allocating opaque_params buffer manually with first querying its size with \r\n```\r\n# Note: this code may require tf==1.4 to run.\r\nlayer = tf.contrib.cudnn_rnn.CudnnGRU(1, num_units=800, input_size=800,\r\n                                      input_mode='linear_input',\r\n                                      direction='bidirectional')\r\nwith tf.Session() as sess:\r\n    OPAQUE_BUFFER_SIZE = sess.run(layer.params_size())\r\n    print(OPAQUE_BUFFER_SIZE)\r\n```\r\n\r\nBut this is really troublesome because it requires running the intermediate graph to get OPAQUE_BUFFER_SIZE before building the main graph.", "comments": ["Hi standy66@, thanks for reporting the issue. Yes only linear_input input_mode is handled for the saveable. The reason is we intend to make sure cudnn-trained checkpoint can be used by regular TF RNNs. Skipping the input isn't what regular TF RNNs do.\r\n\r\nI wonder if it would be a problem if we remove the input_mode option from the cudnn_rnn layers API? If one uses cudnn_rnn_ops, they will still be functional, only that users will need to manage saving/restoring variables in canonical format themselves.", "Hello, @protoget, thanks for quick reply.\r\n\r\nFirst, let me introduce a strong case where skip_input is useful. We are following [DeepSpeech 2](https://arxiv.org/pdf/1512.02595.pdf) paper from Baidu Research. They are using batch normalization for recurrent neural networks similarly to [Laurent et al., 2015](https://arxiv.org/pdf/1510.01378.pdf). Here is an excerpt from the paper:\r\n\r\n\r\n![](http://oi66.tinypic.com/sw8h9i.jpg)\r\n\r\n\r\nAnd in GRUs and LSTMs, they suggest replacing `Wx + b` parts in equiations with `BN(Wx)`. One of the authors of original DeepSpeech 2 paper confirmed that on [Reddit](https://www.reddit.com/r/MachineLearning/comments/4082yn/rnn_batch_normalization_for_grulstm/cyw4ezu/). This is easily achievable by stacking dense layer, batch normalization and recurrent layer with `input_mode=skip_input`.\r\n\r\n\r\nAs for the answer for your question, yes, I think removing input_mode from cudnn_rnn layers API is ok, but then `cudnn_rnn_ops` should be accessibe in documentation (as far as I understand now it's not the case with TF 1.5), maybe with this particular case as an example of where `skip_input` is needed.", "Any update on this issue? This problem is also present in 1.6 and `skip_input` would be very helpful. - Thx", "TF is moving out of contrib module. Try using ```tf.keras.layers.CuDNNGRU``` instead.\r\nClosing this issue due to staleness. Please check with the latest version of TensorFlow. Feel free to reopen if issue still persists. Thanks!", "`tf.keras.layers.CuDNNGRU` does not support `skip_input`. It's a shame tensorflow stepped down from providing low-level CuDNN-like interface."]}, {"number": 16041, "title": "Code documentation for `confusion_matrix.py` misleading", "body": "### Describe the problem\r\n\r\nThe documentation for `confusion_matrix.py` says:\r\n\r\n```\r\n  Args:\r\n    labels: 1-D `Tensor` of real labels for the classification task.\r\n    predictions: 1-D `Tensor` of predictions for a given classification.\r\n```\r\n\r\n, however I found that those two arguments are simply python arrays and not Tensors. The following trial test code demonstrates this. As a TF/Python newbie, I'm wondering if this is actually a real issue, and if so I'll create a PR to correct it to prevent confusion to future programmers.\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ny_ = [0, 2, 2, 2]\r\ny = [2, 1, 2, 2]\r\n\r\nwith tf.Session() as sess:\r\n    confusion_matrix = tf.confusion_matrix(labels=y_, predictions=y, num_classes=4)\r\n    confusion_matrix_to_Print = sess.run(confusion_matrix)\r\n    print(confusion_matrix_to_Print)\r\n\r\n```", "comments": ["IIUC, labels and predictions can accept `Tensor` objects, numpy arrays, Python lists, and Python scalars. @mrry do you know if there is any reason behind this or this is something which we need to fix.", "This is covered in the documentation [here](https://www.tensorflow.org/programmers_guide/graphs#tensor-like_objects). @shivaniag is correct to say that other \"tensor-like\" types are accepted (including in this case Python lists and 1-D NumPy arrays) because they are implicitly convertible to `tf.Tensor` objects. I'd only consider this misleading if `tf.confusion_matrix()` **didn't** accept `tf.Tensor` objects, but as far as I can tell from the code it does."]}, {"number": 16040, "title": "Bug in boolean input tensors for ops ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes. \r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\npip3 install --user tensorflow-gpu \r\n\r\n- **TensorFlow version (use command below)**:\r\n1.4.1 \r\n\r\n- **Python version**: \r\n3.5.2 \r\n\r\n- **Bazel version (if compiling from source)**:\r\nNA\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\n\r\n- **CUDA/cuDNN version**:\r\ncuda-8.0.61-cudnn-v6\r\n\r\n- **GPU model and memory**:\r\nany\r\n\r\n- **Exact command to reproduce**:\r\nsee description below\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nBoolean tensors seem to have a bug. \r\n\r\nIn C++ I define an op with a boolean input tensor like so: \r\n    REGISTER_OP(\"SpatialAugmentationPossible\")\r\n        .Input(\"input: float32\")\r\n        .Output(\"output: bool\")\r\n        .Input(\"mirror: bool\")\r\n        .Input(\"angle: float32\")\r\n        .Input(\"dx: float32\")\r\n        .Input(\"dy: float32\")\r\n        .Input(\"sx: float32\")\r\n        .Input(\"sy: float32\")\r\n        .Attr(\"crop_width: int\")\r\n        .Attr(\"crop_height: int\")\r\n    ...\r\nNote that mirror is a boolean tensor. The op compiles fine. When calling like this: \r\n\r\n                    op = ops.spatial_augmentation_possible(\r\n                        input=test_input,\r\n                        mirror=tf.cast(mirror, dtype=tf.bool),\r\n                        angle=tf.convert_to_tensor(angle, dtype=tf.float32),\r\n                        dx=tf.convert_to_tensor(dx, dtype=tf.float32),\r\n                        dy=tf.convert_to_tensor(dy, dtype=tf.float32),\r\n                        sx=tf.convert_to_tensor(sx, dtype=tf.float32),\r\n                        sy=tf.convert_to_tensor(sy, dtype=tf.float32),\r\n                        crop_width = 500,\r\n                        crop_height = 300\r\n                    )\r\n\r\n                    possible_tensor = sess.run(op)\r\n\r\nI get:\r\n\r\n2018-01-11 14:57:45.722505: F tensorflow/core/framework/tensor.cc:586] Check failed: dtype() == expected_dtype (10 vs. 1)\r\n\r\nI checked the proto, 10 corresponds to boolean and 1 to float32. If I change the line with the mirror argument like this:                     \r\n\r\n                    mirror=tf.cast(mirror, dtype=tf.float32),\r\n\r\nI get: \r\n\r\nValueError: Tensor conversion requested dtype bool for Tensor with dtype float32: 'Tensor(\"Cast:0\", shape=(1,), dtype=float32, device=/device:GPU:0)'\r\n\r\n\r\nThat means now it is complaining that I don't input a bool. But the message above is saying that I shouldn't input a bool. For this reason there seems to be a bug. \r\n\r\nBest, \r\n\r\n\r\nEddy\r\n                    \r\n\r\n", "comments": ["I think I found the problem: in the compute function there was this line of code:\r\n\r\nconst_cast<float*>(context->input(1).flat<float>().data()) \r\n\r\nIf I change float to bool there it works. However, the execution of the program before never even got to this statement so it seems to be something that is set during compile time! (which I cannot read from this code) "]}, {"number": 16039, "title": "How TF-Detect draw a rectangular?", "body": "How TF-Detect draw a rectangular?\r\nI can't find the corresponding code?\r\nIs it calling OpenGL to draw a rectangular?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16038, "title": "Link gives 404", "body": "https://github.com/tensorflow/models/tree/master/syntaxnet#installation gives   404", "comments": ["@unsalatasoy where does the link come from?", "@unsalatasoy it looks like it's back up again. I'll close, please reopen if you're still having problems.", "https://github.com/tensorflow/models/tree/master/syntaxnet#installation \r\nthis link still gives 404. I got this link from the following link. \r\nwhere \"The [tutorial] shows you how to:\"\r\nhttps://www.tensorflow.org/versions/r0.12/tutorials/syntaxnet/\r\n\r\nI think the right link must be following. Could you please confirm me\r\nhttps://github.com/tensorflow/models/tree/master/research/syntaxnet"]}, {"number": 16037, "title": "Windows: Remove -j option when zip libtensorflow package", "body": "Fix http://ci.tensorflow.org/view/Nightly/job/nightly-libtensorflow-windows/329/console", "comments": []}, {"number": 16036, "title": "raise PiCameraMMALError(status, prefix) picamera.exc.PiCameraMMALError: Failed to enable connection: Out of resources", "body": "", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Solved the issue, sir, thanks for your reply.", "@prabhundps Hello, I am having the same issue. Could you please explain how you solved it?", "@nnguyen1602, Will you please send me your code?", "Same error help please"]}, {"number": 16035, "title": "FasterRCNN error", "body": "Hi friends. \r\nWhile  i am trying to execute tensor flow based faster RCNN, i got the following error. \r\nplease help me how to solve this.\r\n\r\ntensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 2815, status: invalid device function\r\n", "comments": ["This is the complete error while , executing faster RCNN. please help me to resolve it. Thanks in advance.\r\n\r\n\r\n018-01-11 06:05:49.066479: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices. temp_storage_bytes: 2815, status: invalid device function\r\n2018-01-11 06:05:49.066923: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices. temp_storage_bytes: 2815, status: invalid device function\r\n[[Node: LOSS_default/Where = Where_device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\r\n2018-01-11 06:05:49.094692: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices. temp_storage_bytes: 2815, status: invalid device function\r\n[[Node: LOSS_default/Where = Where_device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\r\n2018-01-11 06:05:49.094692: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices. temp_storage_bytes: 2815, status: invalid device function\r\n[[Node: LOSS_default/Where = Where_device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\r\n2018-01-11 06:05:49.094700: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices. temp_storage_bytes: 2815, status: invalid device function\r\n[[Node: LOSS_default/Where = Where_device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\r\n2018-01-11 06:05:49.094705: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices. temp_storage_bytes: 2815, status: invalid device function\r\n[[Node: LOSS_default/Where = Where_device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\r\n2018-01-11 06:05:49.094712: W tensorflow/core/framework/op_kernel.cc:1192] Internal: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices. temp_storage_bytes: 2815, status: invalid device function\r\n[[Node: LOSS_default/Where = Where_device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\r\nTraceback (most recent call last):\r\nFile \"/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\r\nreturn fn(*args)\r\nFile \"/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1306, in _run_fn\r\nstatus, run_metadata)\r\nFile \"/home/anaconda3/lib/python3.6/contextlib.py\", line 89, in exit\r\nnext(self.gen)\r\nFile \"/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\npywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices. temp_storage_bytes: 2815, status: invalid device function\r\n[[Node: LOSS_default/Where = Where_device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"./tools/trainval_net.py\", line 140, in\r\nmax_iters=args.max_iters)\r\nFile \"/home/FasterRCNN/tf-faster-rcnn-master/tools/../lib/model/train_val.py\", line 400, in train_net\r\nsw.train_model(sess, max_iters)\r\nFile \"/home/FasterRCNN/tf-faster-rcnn-master/tools/../lib/model/train_val.py\", line 311, in train_model\r\nself.net.train_step(sess, blobs, train_op)\r\nFile \"/home/FasterRCNN/tf-faster-rcnn-master/tools/../lib/nets/network.py\", line 465, in train_step\r\nfeed_dict=feed_dict)\r\nFile \"/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\nrun_metadata_ptr)\r\nFile \"/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\nfeed_dict_tensor, options, run_metadata)\r\nFile \"/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\noptions, run_metadata)\r\nFile \"/home/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\nraise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices. temp_storage_bytes: 2815, status: invalid device function\r\n[[Node: LOSS_default/Where = Where[_device=\"/job:localhost/replica:0/task:0/gpu:0\"](LOSS_default/Not", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The problem is coming when i set GPU NMS flag. But if  disable GPU NMS flag,  the no error is showing up.\r\nLinux OS. TF latest version", "Thanks for suggestion", "Is this still an issue? If so, please provide us with instructions for reproducing the error.", " If i make 'use_GPU_nms flag' as False in cfg file, code is executing without any error."]}, {"number": 16034, "title": "Feature request: tf.nn.dropout noise_shape should support unspecified dimensions", "body": "It would be nice if the noise_shape in [tf.nn.dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) would support unspecified dimensions, and just use the shape of the input tensor, e.g. `-1` or `None`. This way it could be specified as `noise_shape = [-1, 1, 1, -1]` instead of `noise_shape = [k, 1, 1, n]`.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Added a PR #16085 for `-1` support. ", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 16033, "title": "NNAPI/libneuralnetwors.so does not work for other models than default mobilenet_quant_v1_224.tflite", "body": "I have made changes to tensorflowlite to force it use nnapi on my android 8.1 phone, and the nnapi only worked normally for the default default mobilenet_quant_v1_224.tflite.\r\n\r\nThe nnapi/libneuralnetworks.so did not work for the tensorflow_inception_graph.tflite inside the attached asserts.zip, which was built by toco, from tensorflow_inception_graph.pb of https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android\r\n\r\nMy steps,\r\n1. generated tensorflow_inception_graph.tflite from tensorflow_inception_graph.pb by toco\r\n2. add 7 extra lines to labels.txt \"a\\nb\\nc\\nd\\ne\\ng\\ng\"\r\n3. copy new labels.txt and tensorflow_inception_graph.tflite to my bazel cache\r\n4. apply the attached patch a.txt to tensorflow\r\n5.  bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n6. adb install -r <apk_file>\r\n\r\nIf the nnapi is not forced to use, then the apk run normally, though the results are wrong. But if I forced it to use nnapi, then the apk crashed.\r\n\r\nI guess the tensorflow_inception_graph.tflite contained some operators which were not supported by nnapi/libneuralnetwors.so. How can I confirm it?\r\n\r\nThank you.\r\n\r\n[a.txt](https://github.com/tensorflow/tensorflow/files/1622153/a.txt)\r\n[assets.zip](https://github.com/tensorflow/tensorflow/files/1622067/assets.zip)\r\n", "comments": ["Could you inspect the output of 'adb logcat' for errors from the demo app?", "The log\r\n\r\n01-12 02:45:42.465  7030  7030 D TfLiteCameraDemo: Created a Tensorflow Lite Image Classifier.\r\n01-12 02:45:42.763  7030  7047 D TfLiteCameraDemo: Timecost to put values into ByteBuffer: 20\r\n01-12 02:45:42.764  7030  7047 I TfLiteCameraDemo: load function ANeuralNetworksModel_create successfully\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: load function ANeuralNetworksModel_addOperand successfully\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: load function ANeuralNetworksModel_setOperandValue successfully\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 142 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 146 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 155 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 164 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 176 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 184 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 206 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 211 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 216 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 234 3********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 236 3********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 259 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 343 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: load function ANeuralNetworksModel_addOperation successfully\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 349 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 142 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 146 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 155 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 164 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 176 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 184 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 206 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 211 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 216 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 234 17********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 236 17********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 249 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 343 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 349 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 142 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 146 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 155 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 164 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 176 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 184 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 206 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 211 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 216 ********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 234 13********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: ********** 236 13********\r\n01-12 02:45:42.765  7030  7047 I TfLiteCameraDemo: Op code 13 is currently not delegated to NNAPI\r\n", "I saw the reason, because BuiltinOperator_LOCAL_RESPONSE_NORMALIZATION can not be delegated to NNAPI.\r\n\r\nThank you."]}, {"number": 16032, "title": "When will those operations such as \"BuiltinOperator_L2_NORMALIZATION\" be delegated to NNAPI? ", "body": "System information\r\nHave I written custom code: yes\r\nOS Platform and Distribution: Ubuntu 14.04\r\nTensorFlow installed from: pip\r\nTensorFlow version (use command below): 1.4.1\r\nPython version: 2.7.5\r\n\r\nMy problem:\r\n      My android application wants to load the \"facenet\" model through TF lite with NNAPI enabled to do face recognition. But it always crashed. After I debugged,  I found it was caused by that the \"L2_NORMALIZATION\"  was not delegated to NNAPI. I wonder when will those operations be supported by nnapi_delegate, as my application has to use the hardware acceleration. Or is there any workaround to make the application complete normally with \"Use_NNAPI\" enabled ?  ", "comments": ["@andrehentz @aselle @petewarden ", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 118 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 134 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 149 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 164 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 179 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Fixed by https://github.com/tensorflow/tensorflow/commit/32fe0302b0cf02d6cc3ae6cf67b233ad65c74bfe"]}, {"number": 16031, "title": "tf.data.Dataset.padded_batch() doesn't work with dataset.map using tf.py_func", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution**: CentOS Linux release 7.2.1511\r\n- **TensorFlow installed from**: pip\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 2.7.5\r\n\r\n### Describe the problem\r\nIt's quite common for NLP tasks to read variable-length sentences from text files, to map them and to padd them. But Dataset.padded_batch() doesn't work with tf.dataset which uses map (tf.py_func)\r\n\r\n### Source code\r\n```\r\nimport tensorflow as tf                                                                           \r\nimport numpy as np                                                                                \r\n                                                                                                  \r\ndef convert(line):                                                                                \r\n    tokens = line.split()                                                                 \r\n    return np.array(tokens, dtype=np.int32)\r\n\r\n# Simulating reading variable-length sentences from a file. Using TextLineDataset will have the same problem\r\ndataset = tf.data.Dataset.from_tensor_slices([\"1 2 3\", \"4 5\"])         \r\n                           \r\n# Tokenize each sentence and convert it to list of int\r\ndataset = dataset.map(lambda line: tf.py_func(convert, [line], [tf.int32]))     \r\n                  \r\ndataset = dataset.padded_batch(1, [None]) # This line doesn't work whatever the batch_size is\r\n# dataset = dataset.batch(1) # This line works well                                              \r\n \r\niterator = dataset.make_one_shot_iterator()                                                       \r\nbatch_data = iterator.get_next()                                                                  \r\n                                                                                                  \r\nwith tf.Session() as sess:                                                                        \r\n    print sess.run(batch_data)                                                                                                 \r\n```\r\n\r\n### Log\r\n```\r\n    dataset = dataset.padded_batch(1, [None])\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 695, in padded_batch\r\n    return PaddedBatchDataset(self, batch_size, padded_shapes, padding_values)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1292, in __init__\r\n    input_dataset.output_shapes, _partial_shape_to_tensor, padded_shapes)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/data/util/nest.py\", line 512, in map_structure_up_to\r\n    assert_shallow_structure(shallow_tree, input_tree)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/data/util/nest.py\", line 356, in assert_shallow_structure\r\n    \"Input has type: %s.\" % type(input_tree))\r\nTypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: <type 'list'>.\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "The structure of the element and the padding specification doesn't match. Replace these lines:\r\n\r\n```python\r\ndataset = dataset.map(lambda line: tf.py_func(convert, [line], [tf.int32]))     \r\n                  \r\ndataset = dataset.padded_batch(1, [None]) # This line doesn't work whatever the batch_size is\r\n```\r\n\r\nwith either:\r\n\r\n```python\r\ndataset = dataset.map(lambda line: tf.py_func(convert, [line], tf.int32))     \r\n                  \r\ndataset = dataset.padded_batch(1, [None])\r\n```\r\n\r\n...or:\r\n\r\n```python\r\ndataset = dataset.map(lambda line: tf.py_func(convert, [line], [tf.int32]))     \r\n                  \r\ndataset = dataset.padded_batch(1, ([None],))\r\n```", "@mrry Many thanks for your support! That works\r\n"]}, {"number": 16030, "title": "Use https for `https://github.com/tensorflow/serving`", "body": "It looks like there are a couple of links in saved_model.md using http to point to serving, and the rest are using https to point to serving, This fix updates to `https` for consistency and security.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 16029, "title": "Update embedding.md", "body": "When I reference embedding projector, I found the string of https:// was removed. So I added it to help other people get a correct reference about How to Use t-SNE Effectively from Embedding of Programmer's guides.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 16027, "title": "py2tf: add py2tf_internal BUILD rule to pip package", "body": "* to make pip tests pass", "comments": ["cc @mdanatg "]}, {"number": 16026, "title": "can't use mpi_allreduce op when tensors run on gpus", "body": "I compiled and installed new mpi_collective feature with openmpi3.0 on tensorflow 1.5.  When I use allreduce function to aggregate loss, I got segmentation fault errors. However, When I force code to run on CPU device, it's fine. \r\n\r\n**## here is my runtime environment.**\r\n== cat /etc/issue ===============================================\r\nLinux user-ubuntu 3.19.0-25-generic #26~14.04.1-Ubuntu SMP Fri Jul 24 21:16:20 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"14.04.5 LTS, Trusty Tahr\"\r\nVERSION_ID=\"14.04\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\r\nCopyright (C) 2013 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux user-ubuntu 3.19.0-25-generic #26~14.04.1-Ubuntu SMP Fri Jul 24 21:16:20 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.5.1)\r\ntensorflow (1.5.0rc0)\r\ntensorflow-tensorboard (0.4.0rc2)\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.5.0-rc0\r\ntf.GIT_VERSION = unknown\r\ntf.COMPILER_VERSION = unknown\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Jan 10 21:56:08 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P100-PCIE...  Off  | 0000:06:00.0     Off |                    0 |\r\n| N/A   27C    P0    27W / 250W |      0MiB / 16276MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla P100-PCIE...  Off  | 0000:84:00.0     Off |                    0 |\r\n| N/A   25C    P0    27W / 250W |      0MiB / 16276MiB |      3%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n\r\n**##here is error.**\r\n\r\ntensor@user-ubuntu:~/tensorflow-r1.4/tensorflow/contrib/mpi_collectives$ mpirun -n 2 python mpi_simple_nn.py \r\n\r\n2018-01-10 21:50:44.579981: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-01-10 21:50:44.580748: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-01-10 21:50:47.127209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 0 with properties: \r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:06:00.0\r\ntotalMemory: 15.89GiB freeMemory: 15.34GiB\r\n2018-01-10 21:50:47.127952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 0 with properties: \r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:06:00.0\r\ntotalMemory: 15.89GiB freeMemory: 15.34GiB\r\n2018-01-10 21:50:48.027960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 1 with properties: \r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:84:00.0\r\ntotalMemory: 15.89GiB freeMemory: 15.34GiB\r\n2018-01-10 21:50:48.028762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 1 with properties: \r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:84:00.0\r\ntotalMemory: 15.89GiB freeMemory: 15.34GiB\r\n2018-01-10 21:50:48.028835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1221] Device peer to peer matrix\r\n2018-01-10 21:50:48.028932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1221] Device peer to peer matrix\r\n2018-01-10 21:50:48.028886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1227] DMA: 0 1 \r\n2018-01-10 21:50:48.028910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1237] 0:   Y N \r\n2018-01-10 21:50:48.028926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1237] 1:   N Y \r\n2018-01-10 21:50:48.028933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 0\r\n2018-01-10 21:50:48.028938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 1\r\n2018-01-10 21:50:48.028966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1227] DMA: 0 1 \r\n2018-01-10 21:50:48.028974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1237] 0:   Y N \r\n2018-01-10 21:50:48.028979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1237] 1:   N Y \r\n2018-01-10 21:50:48.028994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 0\r\n2018-01-10 21:50:48.029000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 1\r\n2018-01-10 21:50:48.568865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14824 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:06:00.0, compute capability: 6.0)\r\n2018-01-10 21:50:48.668728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 553 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:06:00.0, compute capability: 6.0)\r\n2018-01-10 21:50:48.676343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14824 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)\r\n2018-01-10 21:50:48.708625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 553 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)\r\nmy_rank  1\r\nmy_rank  0\r\n[user-ubuntu:49967] *** Process received signal ***\r\n[user-ubuntu:49967] Signal: Segmentation fault (11)\r\n[user-ubuntu:49967] Signal code: Invalid permissions (2)\r\n[user-ubuntu:49967] Failing at address: 0x1042c005000\r\n[user-ubuntu:49967] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x10330)[0x7f2122752330]\r\n[user-ubuntu:49967] [ 1] /lib/x86_64-linux-gnu/libc.so.6(+0x9ac36)[0x7f2122413c36]\r\n[user-ubuntu:49967] [ 2] /usr/local/openmpi3/lib/openmpi/mca_btl_vader.so(mca_btl_vader_sendi+0x332)[0x7f1fd5d9b832]\r\n[user-ubuntu:49967] [ 3] /usr/local/openmpi3/lib/openmpi/mca_pml_ob1.so(+0xb6eb)[0x7f1fd63c86eb]\r\n[user-ubuntu:49967] [ 4] /usr/local/openmpi3/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_send+0x690)[0x7f1fd63ca130]\r\n[user-ubuntu:49967] [ 5] /usr/local/openmpi3/lib/libmpi.so.40(PMPI_Send+0xf2)[0x7f1ffc59d062]\r\n[user-ubuntu:49967] [ 6] /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/mpi_collectives/python/ops/_mpi_ops.so(_ZN10tensorflow7contrib15mpi_collectives13RingAllreduceIN5Eigen9GpuDeviceEfEENS_6StatusEPNS_15OpKernelContextEPKNS_6TensorEPS8_SB_+0x14f)[0x7f21082b50bf]\r\n[user-ubuntu:49967] [ 7] /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/mpi_collectives/python/ops/_mpi_ops.so(+0x1cab9)[0x7f21082aaab9]\r\n[user-ubuntu:49967] [ 8] /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/mpi_collectives/python/ops/_mpi_ops.so(+0x23760)[0x7f21082b1760]\r\n[user-ubuntu:49967] [ 9] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb1a60)[0x7f1ff5c48a60]\r\n[user-ubuntu:49967] [10] /lib/x86_64-linux-gnu/libpthread.so.0(+0x8184)[0x7f212274a184]\r\n[user-ubuntu:49967] [11] /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7f2122476ffd]\r\n[user-ubuntu:49967] *** End of error message ***\r\n-------------------------------------------------------\r\nPrimary job  terminated normally, but 1 process returned\r\na non-zero exit code. Per user-direction, the job has been aborted.\r\n-------------------------------------------------------\r\n--------------------------------------------------------------------------\r\nmpirun noticed that process rank 0 with PID 0 on node user-ubuntu exited on signal 11 (Segmentation fault)", "comments": ["There currently isn't a CODEOWNER for contrib/mpi. Please try [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) noting that @jbedorf was the original contributor of this functionality, and might be able to provide assistance. We'd also gladly accept contributions improving this support.", "Note that this is the `mpi_collectives` operation for which I'm not the author, but the problem in this case is most likely that you did not build your MPI environment with GPU/CUDA support. Without that support the MPI calls that get GPU pointers as argument will fail. \r\nSee: https://www.open-mpi.org/faq/?category=building#build-cuda"]}, {"number": 16025, "title": "Checkpoints continue to grow after the first restore", "body": "### System information\r\n- No\r\n\r\n- **OS Platform and Distribution**: Mac OS X, v 10.13.2\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: v1.3.0-rc1-5211-gab0fcac 1.5.0-dev20171126\r\n- **Python version**: Python 3.5.0\r\n- **Bazel version**: N/A\r\n- **GCC/Compiler version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: Intel Iris Pro 1536 MB\r\n- **Exact command to reproduce**: N/A\r\n- **Have I written custom code**: N/A\r\n\r\n### Describe the problem\r\n\r\nThe parameter `max_to_keep` of the `Saver` class does not seem to have effect once a model and its training variables are restored. In other words, the first time I train my model, the saver is keeping only `max_to_keep` checkpoints. Then I interrupt the training. Later, when I resume it, the number of checkpoints keeps going without any apparent limit.\r\n\r\n### Related issues\r\n\r\n- https://github.com/tensorflow/tensorflow/issues/5929\r\n- https://github.com/tensorflow/tensorflow/issues/6326\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code", "I've just noticed that the file `checkpoint`, which keeps track of the names of the last checkpoints, only contains the last `max_to_keep` filenames.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "I can reproduce with\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ngs = tf.Variable(0, name='global_step')\r\ninc_gs = gs.assign_add(1)\r\nsaver = tf.train.Saver([gs], max_to_keep=2)\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  for _ in range(3):\r\n    saver.save(sess, '/tmp/ckpts/model.ckpt', gs)\r\n    sess.run(inc_gs)\r\n\r\nsaver = tf.train.Saver([gs], max_to_keep=2)\r\nwith tf.Session() as sess:\r\n  saver.restore(sess, tf.train.latest_checkpoint('/tmp/ckpts'))\r\n  for _ in range(3):\r\n    saver.save(sess, '/tmp/ckpts/model.ckpt', gs)\r\n    sess.run(inc_gs)\r\n```\r\n\r\n@panyx0718, do you know if this is intended?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 100 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 116 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 131 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is it still an issue for the latest TF?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@bignamehyp \r\n\r\n> Please update the issue when new information becomes available, and we will reopen the issue\r\n\r\nUpdate the issue? Why should I need to update the issue? TF developers have to solve this issue. There's nothing to update. So I don't get why you're closing this issue with this message."]}, {"number": 16024, "title": "R1.4", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "R1.4 has already been merged back into master."]}, {"number": 16023, "title": " can't feed  the multi-channel to contrib_audio.audio_spectrogram", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.0-cp35-cp35m-linux_x86_64.whl\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**:  3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 8.0/cuDNN6.0\r\n- **GPU model and memory**: Geforce 1080 TI\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\ncan't  feed the multi-channel to audio_spectrogram()\r\nthe error is \r\nSpectrogram size calculation failed:Expected height 98 but got 100\r\n\r\nif use [16000, 1] at line 29 in my below code, the result is ok. no error. It means only accept one channel.\r\nI read the code at:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/spectrogram_op.cc\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/spectrogram.cc\r\n\r\nI think the error is :\r\nsamples_to_next_step_ and  input_queue_ is modified when calculation for each channel.\r\nBut Spectrogram don't initialized these two variable at each channel, \r\n\r\n### Source code / logs\r\n![image](https://user-images.githubusercontent.com/32910309/34800518-fff6eaee-f618-11e7-8563-3cac1b8637c6.png)\r\n\r\n![image](https://user-images.githubusercontent.com/32910309/34800232-c10414f2-f617-11e7-94a5-2c0a7ec3738d.png)\r\n\r\n", "comments": ["@petewarden should the spectrogram code work for multiple channels?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 117 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 133 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 148 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "multi-channel spectrograms (and mel-cepstrums) would be useful!", "the workaround is:\r\n\r\n```\r\nspectrograms = []\r\nfor ichannel in range(10):\r\n  spectrograms.append(contrib_audio.audio_spectrogram(\r\n      tf.slice(feature_in, [0, ichannel], [-1, 1]),\r\n      window_size=400,\r\n      stride=160,\r\n      magnitude_squared=True))\r\nspectrogram_ = tf.stack(spectrograms, -1)\r\n```"]}, {"number": 16022, "title": "Update version strings and revert pom.xml changes", "body": "The maven numbers will be updated manually later.", "comments": []}, {"number": 16021, "title": "Update version strings.", "body": null, "comments": ["Gah, my local branch wasn't synced correctly."]}, {"number": 16020, "title": "MKL: Fix LRN tensor shape when Eigen path is taken.", "body": "On Eigen path, the workspace tensor should only have one dimension of size 0.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 16019, "title": "tf-nightly and master - cannot import tensorflow", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: tf-nightly-gpu-1.6.0.dev20180110\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0/7\r\n- **GPU model and memory**: V100 16GB\r\n- **Exact command to reproduce**: ```import tensorflow```\r\n\r\n### Describe the problem\r\nOn the current tf-nightly-gpu I cannot import tensorflow, the following error is produced. I am also seeing the same behavior on tf-nightly and also a build from source of master (SHA: 82b1e8eee8847730026379e3a5762c0e09d6fd36):\r\n``` python\r\nIn [1]: import tensorflow\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\n/home/ubuntu/tensorflow/tensorflow/__init__.py in <module>()\r\n     22\r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26\r\n\r\n/home/ubuntu/tensorflow/tensorflow/python/__init__.py in <module>()\r\n     47 import numpy as np\r\n     48\r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50\r\n     51 # Protocol buffers\r\n\r\n/home/ubuntu/tensorflow/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     23 import traceback\r\n     24\r\n---> 25 from tensorflow.python.platform import self_check\r\n     26\r\n     27\r\n\r\nImportError: No module named platform\r\n```\r\n\r\n### Source code / logs\r\nN/A", "comments": ["Double check and make sure you aren't starting jupyter-notebook or the python interpreter from the tensorflow source directory. That seems to be a common cause for this particular error.", "Interesting, that was indeed the issue. Interestingly, when importing under Python 2.7 it gives the error: ```ImportError: No module named platform```\r\n\r\nWhile under python 3.5 it gives the error: ```ImportError: Could not import tensorflow. Do not import tensorflow from its source directory; change directory to outside the TensorFlow source tree, and relaunch your Python interpreter from there.```\r\n\r\nObviously the latter is much more useful. Any chance this could be updated to give the same error message under 2.7?", "Looks like this is fixed on master, closing issue."]}]