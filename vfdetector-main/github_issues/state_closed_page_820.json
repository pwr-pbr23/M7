[{"number": 28932, "title": " java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find custom op for name 'TFLite_Detection_PostProcess' with version 1  Registration failed.", "body": "\r\nwindows\r\nandroid 8.0.0\r\nhuawei mate9  \r\n\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n\r\nCameraActivity: Exception!\r\n    java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find custom op for name 'TFLite_Detection_PostProcess' with version 1\r\n    Registration failed.\r\n    \r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:125)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:102)\r\n        at org.tensorflow.lite.examples.detection.CameraActivity.onPreviewFrame(CameraActivity.java:199)\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1137)\r\n        at android.os.Handler.dispatchMessage(Handler.java:108)\r\n        at android.os.Looper.loop(Looper.java:166)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7406)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:245)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:926)\r\n     Caused by: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find custom op for name 'TFLite_Detection_PostProcess' with version 1\r\n    Registration failed.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:73)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:52)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:114)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:123)\r\n        \t... 9 more\r\n\r\n\r\njava.lang.NullPointerException: Attempt to invoke virtual method 'void org.tensorflow.lite.NativeInterpreterWrapper.close()' on a null object reference\r\n        at org.tensorflow.lite.Interpreter.close(Interpreter.java:252)\r\n        at org.tensorflow.lite.Interpreter.finalize(Interpreter.java:259)\r\n        at java.lang.Daemons$FinalizerDaemon.doFinalize(Daemons.java:254)\r\n        at java.lang.Daemons$FinalizerDaemon.runInternal(Daemons.java:241)\r\n        at java.lang.Daemons$Daemon.run(Daemons.java:104)\r\n        at java.lang.Thread.run(Thread.java:784)", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28931, "title": "RuntimeError when using distribution strategy (Distributed training in TensorFlow: Keras Tutorial throws error)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Cent OS 7\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.0 / 7.3.1 (conda installed cudnn)\r\n- GPU model and memory: 8x GeForce RTX 2080 / 7951MiB each\r\n\r\n**Describe the current behavior**\r\nModel fails to train, raising a `RuntimeError: Replica-local variables may only be assigned in a replica context.` I was able to reproduce this issue just by using the official tutorial, so that's the code given below rather than mine.\r\n\r\n**Describe the expected behavior**\r\nThis code should correctly utilize my GPUs.\r\n\r\n**Code to reproduce the issue**\r\nThis code is taken straight from [the tutorial](https://www.tensorflow.org/tutorials/distribute/keras)\r\n```python\r\n#!/usr/bin/env python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nimport os\r\ndatasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n\r\nmnist_train, mnist_test = datasets['train'], datasets['test']\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nnum_train_examples = info.splits['train'].num_examples\r\nnum_test_examples = info.splits['test'].num_examples\r\n\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE_PER_REPLICA = 64\r\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\ndef scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n\r\ntrain_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\neval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\r\n\r\nwith strategy.scope():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n\r\n    model.compile(loss='sparse_categorical_crossentropy',\r\n                  optimizer=tf.keras.optimizers.Adam(),\r\n                  metrics=['accuracy'])\r\n# Define the checkpoint directory to store the checkpoints\r\ncheckpoint_dir = './training_checkpoints'\r\n# Name of the checkpoint files\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n\r\ncallbacks = [\r\n    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\r\n    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\r\n                                    save_weights_only=True),\r\n    tf.keras.callbacks.LearningRateScheduler(decay),\r\n]\r\n\r\nmodel.fit(train_dataset, epochs=10, callbacks=callbacks)\r\n```\r\n\r\n**Other info / logs**\r\nFull output including traceback:\r\n```sh\r\n$ CUDA_VISIBLE_DEVICES=2,3,4,5,6,7 python -m mnist_tf_check\r\n\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nWARNING:tensorflow:From /home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0522 08:53:17.781122 140396075140928 deprecation.py:323] From /home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-05-22 08:53:17.916369: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2019-05-22 08:53:17.931970: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200015000 Hz\r\n2019-05-22 08:53:17.934971: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55cb744b5090 executing computations on platform Host. Devices:\r\n2019-05-22 08:53:17.935018: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-05-22 08:53:20.146420: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55cb74533ed0 executing computations on platform CUDA. Devices:\r\n2019-05-22 08:53:20.146509: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5\r\n2019-05-22 08:53:20.146535: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (1): GeForce RTX 2080, Compute Capability 7.5\r\n2019-05-22 08:53:20.146556: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (2): GeForce RTX 2080, Compute Capability 7.5\r\n2019-05-22 08:53:20.146591: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (3): GeForce RTX 2080, Compute Capability 7.5\r\n2019-05-22 08:53:20.146613: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (4): GeForce RTX 2080, Compute Capability 7.5\r\n2019-05-22 08:53:20.146634: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (5): GeForce RTX 2080, Compute Capability 7.5\r\n2019-05-22 08:53:20.148829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:\r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.71\r\npciBusID: 0000:08:00.0\r\ntotalMemory: 7.77GiB freeMemory: 7.62GiB\r\n2019-05-22 08:53:20.149286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 1 with properties:\r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.71\r\npciBusID: 0000:09:00.0\r\ntotalMemory: 7.77GiB freeMemory: 7.62GiB\r\n2019-05-22 08:53:20.149717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 2 with properties:\r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.71\r\npciBusID: 0000:84:00.0\r\ntotalMemory: 7.77GiB freeMemory: 7.62GiB\r\n2019-05-22 08:53:20.150156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 3 with properties:\r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.71\r\npciBusID: 0000:85:00.0\r\ntotalMemory: 7.77GiB freeMemory: 7.62GiB\r\n2019-05-22 08:53:20.150586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 4 with properties:\r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.71\r\npciBusID: 0000:88:00.0\r\ntotalMemory: 7.77GiB freeMemory: 7.62GiB\r\n2019-05-22 08:53:20.151019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 5 with properties:\r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.71\r\npciBusID: 0000:89:00.0\r\ntotalMemory: 7.77GiB freeMemory: 7.62GiB\r\n2019-05-22 08:53:20.152662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1, 2, 3, 4, 5\r\n2019-05-22 08:53:20.169207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-22 08:53:20.169256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1 2 3 4 5\r\n2019-05-22 08:53:20.169281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N N N N N N\r\n2019-05-22 08:53:20.169326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   N N N N N N\r\n2019-05-22 08:53:20.169345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 2:   N N N N N N\r\n2019-05-22 08:53:20.169379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 3:   N N N N N N\r\n2019-05-22 08:53:20.169398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 4:   N N N N N N\r\n2019-05-22 08:53:20.169416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 5:   N N N N N N\r\n2019-05-22 08:53:20.171379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7416 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:08:00.0, compute capability: 7.5)\r\n2019-05-22 08:53:20.171960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7416 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080, pci bus id: 0000:09:00.0, compute capability: 7.5)\r\n2019-05-22 08:53:20.172517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 7416 MB memory) -> physical GPU (device: 2, name: GeForce RTX 2080, pci bus id: 0000:84:00.0, compute capability: 7.5)\r\n2019-05-22 08:53:20.173011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 7416 MB memory) -> physical GPU (device: 3, name: GeForce RTX 2080, pci bus id: 0000:85:00.0, compute capability: 7.5)\r\n2019-05-22 08:53:20.173526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 7416 MB memory) -> physical GPU (device: 4, name: GeForce RTX 2080, pci bus id: 0000:88:00.0, compute capability: 7.5)\r\n2019-05-22 08:53:20.174043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 7416 MB memory) -> physical GPU (device: 5, name: GeForce RTX 2080, pci bus id: 0000:89:00.0, compute capability: 7.5)\r\n2019-05-22 08:53:20.184131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1, 2, 3, 4, 5\r\n2019-05-22 08:53:20.185576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-22 08:53:20.185617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1 2 3 4 5\r\n2019-05-22 08:53:20.185638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N N N N N N\r\n2019-05-22 08:53:20.185656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   N N N N N N\r\n2019-05-22 08:53:20.185673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 2:   N N N N N N\r\n2019-05-22 08:53:20.185692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 3:   N N N N N N\r\n2019-05-22 08:53:20.185709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 4:   N N N N N N\r\n2019-05-22 08:53:20.185749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 5:   N N N N N N\r\n2019-05-22 08:53:20.187543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 7416 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:08:00.0, compute capability: 7.5)\r\n2019-05-22 08:53:20.188006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:1 with 7416 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080, pci bus id: 0000:09:00.0, compute capability: 7.5)\r\n2019-05-22 08:53:20.188405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:2 with 7416 MB memory) -> physical GPU (device: 2, name: GeForce RTX 2080, pci bus id: 0000:84:00.0, compute capability: 7.5)\r\n2019-05-22 08:53:20.188747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:3 with 7416 MB memory) -> physical GPU (device: 3, name: GeForce RTX 2080, pci bus id: 0000:85:00.0, compute capability: 7.5)\r\n2019-05-22 08:53:20.189115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:4 with 7416 MB memory) -> physical GPU (device: 4, name: GeForce RTX 2080, pci bus id: 0000:88:00.0, compute capability: 7.5)\r\n2019-05-22 08:53:20.189446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:5 with 7416 MB memory) -> physical GPU (device: 5, name: GeForce RTX 2080, pci bus id: 0000:89:00.0, compute capability: 7.5)\r\nTraceback (most recent call last):\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/stilljm/tensorflow_is_terrible/mnist_tf_check.py\", line 38, in <module>\r\n    metrics=['accuracy'])\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/base.py\", line 442, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 499, in compile\r\n    sample_weights=self.sample_weights)\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1844, in _handle_metrics\r\n    return_stateful_result=return_stateful_result))\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1800, in _handle_per_output_metrics\r\n    stateful_metric_result = _call_stateful_fn(stateful_fn)\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1773, in _call_stateful_fn\r\n    fn, y_true, y_pred, weights=weights, mask=mask)\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 852, in call_metric_function\r\n    return metric_fn(y_true, y_pred, sample_weight=weights)\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py\", line 438, in __call__\r\n    update_op = self.update_state(*args, **kwargs)\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py\", line 98, in decorated\r\n    update_op = update_state_fn(*args, **kwargs)\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py\", line 651, in update_state\r\n    matches, sample_weight=sample_weight)\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py\", line 604, in update_state\r\n    update_total_op = state_ops.assign_add(self.total, values)\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/ops/state_ops.py\", line 191, in assign_add\r\n    return ref.assign_add(value)\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\", line 911, in assign_add\r\n    _assert_replica_context()\r\n  File \"/home/stilljm/.conda/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\", line 894, in _assert_replica_context\r\n    \"Replica-local variables may only be assigned in a replica context.\")\r\nRuntimeError: Replica-local variables may only be assigned in a replica context.\r\n```\r\n", "comments": ["I was able to reproduce the issue reported here with Tensorflow 1.13.1. I got the error RuntimeError: Replica-local variables may only be assigned in a replica context.", "Hi, this tutorial is supposed to work only with TF 1.x nightly, and the upcoming 1.14 release. It is expected to fail with 1.13. \r\n@yashk2810 - would you mind updating the tutorial to install tf-nightly at the top so that it's clear what it is supposed to work with? by default, the external colab currently uses 1.13 so this fails. \r\nWould be good to check this for all the other tutorials as well. \r\n\r\n", "The notebook should be updated by tomorrow. Also, I would recommend looking at tensorflow.org/alpha. See the distributed keras notebook here (https://www.tensorflow.org/alpha/tutorials/distribute/keras)", "I want to know where can be find the tutorial about tf1.13 at low level api ?", "Closing this issue since the tutorial mentioned was expected to fail with 1.13.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28931\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28931\">No</a>\n", "I am using 1.14 and when I start my eval session I got the same termination error\r\n```\r\neval_sess = tf.compat.v1.Session(graph=eval_graph)\r\n\r\n  File \"/home/adamduong26111996/env/anaconda3/envs/cuda-10/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 825, in __init__\r\n    self.build()\r\n  File \"/home/adamduong26111996/env/anaconda3/envs/cuda-10/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 837, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/home/adamduong26111996/env/anaconda3/envs/cuda-10/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 875, in _build\r\n    build_restore=build_restore)\r\n  File \"/home/adamduong26111996/env/anaconda3/envs/cuda-10/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 508, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"/home/adamduong26111996/env/anaconda3/envs/cuda-10/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 350, in _AddRestoreOps\r\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\r\n  File \"/home/adamduong26111996/env/anaconda3/envs/cuda-10/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\", line 1371, in restore\r\n    return self._sync_on_read_variable.assign(tensor)\r\n  File \"/home/adamduong26111996/env/anaconda3/envs/cuda-10/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\", line 1412, in assign\r\n    _assert_replica_context(self._distribute_strategy)\r\n  File \"/home/adamduong26111996/env/anaconda3/envs/cuda-10/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\", line 1381, in _assert_replica_context\r\n    \"Replica-local variables may only be assigned in a replica context.\")\r\n\r\nRuntimeError: Replica-local variables may only be assigned in a replica context.\r\n```"]}, {"number": 28930, "title": "BUG: tf.function cannot handle exception raised by custom op", "body": "Fix https://github.com/tensorflow/addons/issues/260\r\n\r\nHi, would you mind telling me where to add unit test?", "comments": ["Thank you for the fix!\r\n\r\nI just realized that the integration test we used for this piece is not open sourced yet. So for now we should add a unit test in this same directory, similar to how other files do it - manually building an ErrorMetadataBase object with a suitable fake translated_stack should do the trick.\r\n\r\n@bananabowl we should pull this into the 1.14 RC as well.", "@mdanatg Thank you, Dan. I have added an unit test for the fix.", "Awesome, thank you! I left a few style comments, encouraged but optional.", "@bananabowl this can be pulled into 1.14 as soon as tests are green.", "Thank you, I think all comments have been solved."]}, {"number": 28929, "title": "TF2.0 Problem making prediction from a checkpoint if model has an embedding column", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): NA\r\n- TensorFlow version (use command below):  v1.12.0-9492-g2c319fb415 2.0.0-alpha0\r\n- Python version: 3.6.7 (default, Oct 22 2018, 11:32:17) \\n[GCC 8.2.0]\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI have a keras model that has an embedding column as a  feature.\r\n\r\nI can train and save the model's weights just fine. Making predictions immediately after training works too.\r\n\r\nThe problem is when I recreate the model and reload the weights. Making a prediction from that model gives a shape error.\r\n\r\n**Describe the expected behavior**\r\nSame output as making predictions immediately after training.\r\n\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nPlease refer to this [gist](https://gist.github.com/hsm207/305d068c982edf9e1df7db77446df50c).\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I was able to reproduce the mentioned output on Colab with TensorFlow version 2.0.0-alpha0.", "@hsm207 I could not reproduce the issue as it was resolved in `tf-nightly-2.0-preview`. I can see exactly same results with model_1  (original) and model_2(restored model). Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/a6a1ca212893e68b4b2448bca31bfa05/tf2_bug_embedding_column.ipynb).\r\n\r\nI am closing the issue as it was resolved in tf-nightly-2.0-preview. Please feel free to open it if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28929\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28929\">No</a>\n"]}, {"number": 28928, "title": "[TF2.0] steps_per_epoch parameter not working when input data passed as dictionary", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nIf using a dictionary to pass input to a `keras.Model` with named inputs, the `steps_per_epoch` argument appears to have no effect. Instead as many steps are performed as needed for a full epoch.\r\n\r\n**Describe the expected behavior**\r\nI would expect only as many iterations as specified by `steps_per_epoch` in each iteration. While it's possible I have misunderstood something, this is the behaviour in TensorFlow 1 when I run analogous code.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = np.random.random((1000, 2))\r\ny = 2 * x[:, 0] + 3 * x[:, 1] + np.random.normal(size=(1000,))\r\n\r\ninput_ = tf.keras.Input((2,), name=\"input\")\r\noutput = tf.keras.layers.Dense(1)(input_)\r\nmodel = tf.keras.Model(inputs=[input_], outputs=[output])\r\n\r\nmodel.compile(tf.optimizers.Adam(), loss=tf.losses.mse)\r\n```\r\n\r\nwith this setup, I observe\r\n\r\n```python\r\nmodel.fit(\r\n    x, y, steps_per_epoch=2, batch_size=10,\r\n)\r\n```\r\n```\r\n2/2 [==============================] - 0s 38ms/step - loss: 1.5998\r\n```\r\n\r\nbut on the other hand, if I pass the input as a dictionary I observe\r\n\r\n```python\r\nmodel.fit(\r\n    {\"input\": x}, y, steps_per_epoch=2, batch_size=10,\r\n)\r\n```\r\n```\r\n1000/1000 [==============================] - 5s 5ms/sample - loss: 1.5324\r\n```\r\n\r\nStrangely, if I then run the first `.fit` command again, it also performs a full pass over the data, rather than doing the specified number of steps as originally.\r\n\r\nIn TensorFlow 1, with the same code as above, but a TensorFlow 1 friendly model compilation step\r\n\r\n```python\r\nmodel.compile(\r\n    tf.train.AdamOptimizer(),\r\n    loss=tf.losses.mean_squared_error,\r\n)\r\n```\r\n\r\nI do not see the same issue, the number of steps is as specified, whether data is passed in a dictionary or not.\r\n\r\n\r\n**Other info / logs**\r\nThis could be related to #28710? Though the author there was using TensorFlow 1 and reported that data input as NumPy arrays did not cause problems, so this seemed different enough for its own issue. Happy for them to be consolidated if they are too similar.\r\n", "comments": ["@tcbegley I tried reproducing the issue through colab with Tf 2.0.0-alpha0 but the code executed as expected. Can you try once again and let us know if that still gives weird output. Thanks!", "@gadagashwini thanks for getting back to me.\r\n\r\nI was able to replicate my issue in Google Colab, see [this notebook](https://colab.research.google.com/drive/1pjpBakrkgd0pQKhGhnQW2_PNUUXKI28G).\r\n\r\nAs in the screenshot below, which is taken from that notebook, the first fit argument performs two steps of training, the second cell appears to do a full pass over the data, ignoring the `steps_per_epoch` argument. As I stated in my original post this is different to behaviour in TensorFlow 1, and also different to my interpretation of the docs, so I think it's a bug.\r\n\r\n![image](https://user-images.githubusercontent.com/15220906/58250963-72ea5800-7d5a-11e9-852c-abb6916d9686.png)\r\n", "I was able to reproduce the mentioned output. ", "@tcbegley The other possibly related issue is fixed with latest tf nightly build. Is this still an issue with tf 2.0 nightly build? (pip install tf-nightly-2.0-preview)", "@ymodak I tried out the nightly build, and everything worked correctly, so it seems likely this issue has also been resolved.", "Great. Thanks for confirming. I will close this issue now. Feel free to reopen if still have problems. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28928\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28928\">No</a>\n"]}, {"number": 28927, "title": "Two more fixes", "body": "", "comments": []}, {"number": 28926, "title": "Build tensorflow lite for aarch64: Error in script download_dependencies.sh and build_aarch64_lib.sh", "body": "**System information**\r\n- OS Platform and Distribution: Google coral board aarch64\r\n- TensorFlow lite installed from source\r\n- TensorFlow version: master branch\r\n\r\nIf I run the script ./tensorflow/lite/tools/make/download_dependencies.sh  from the root directory of the repo I am getting following error:\r\n\r\n**downloading http://mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/a0d250e79c79.tar.gz**\r\n\r\n**gzip: stdin: not in gzip format**\r\n**tar: Child returned status 1**\r\n**tar: Error is not recoverable: exiting now**\r\n\r\n2 days ago I had not the problem to run the script download_dependencies.sh.\r\nThere was some clean up of dependencies for tensorflow lite at least if I check the commit messages of the master branch.\r\nCould it have something to do with that?\r\n\r\nThanks in advance!\r\n", "comments": ["See: \r\n https://github.com/tensorflow/tensorflow/commit/69aa57b4158da0f09ca732ee302ef453fa6b3209#diff-455a4c7f8e22d7c514e8c2caa27506c5\r\n\r\nThere was an update to the artifact it tries to download.  Tried to download both with wget and  the old artifact is available but the new one can't be downloaded.", "Could fix that problem with following change:\r\nIn tensorflow/lite/tools/make/download_dependencies.sh script I changed the EIGEN_URL definition in line 32 to use https instead of http\r\n`EIGEN_URL=\"$(grep -o 'https.*bitbucket.org/eigen/eigen/get/.*tar\\.gz' \"${BZL_FILE_PATH}\" | grep -v mirror.bazel | head -n1)\"`\r\n\r\n\r\nBut now I have a problem in compiling tflite.\r\nFirst I got the error described in [here](https://stackoverflow.com/questions/56055359/tensorflow-lite-arm64-error-cannot-convert-const-int8x8-t). After using the suggested changes (disabling NNAPI, adding flag -flax-vector-conversions for CXXFLAGS and add  \"-lrt\" to tensorflow/lite/tools/make/targets/aarch64_makefile.inc) I get the following error:\r\n\r\n`In file included from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h:21,\r\n                 from tensorflow/lite/kernels/depthwise_conv.cc:28:\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h: In static member function 'static void tflite::optimized_ops::depthwise_conv::KernelMacroBlock<(tflite::DepthwiseConvImplementation)3, (tflite::DepthwiseConvDepthMultiplication)0, 2>::Run(const int8*, const int8*, const int32*, uint8*, const tflite::optimized_ops::depthwise_conv::DepthwiseConvDotProdParams*)':\r\n**./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:8255:3: error: x29 cannot be used in asm here\r\n   }\r\n   ^\r\n./tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h:8255:3: error: x29 cannot be used in asm here**\r\nIn file included from ./tensorflow/lite/kernels/cpu_backend_gemm.h:22:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:42,\r\n                 from tensorflow/lite/kernels/dequantize.cc:24:\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h: In static member function 'static void tflite::cpu_backend_gemm::detail::CustomGemvImpl<LhsScalar, RhsScalar, int, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<int, DstScalar, quantization_flavor>&, int, int)':\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:485:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:488:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:491:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:494:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:497:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:500:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\ntensorflow/lite/tools/make/Makefile:241: recipe for target '/usr/src/app/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/kernels/depthwise_conv.o' failed\r\nmake: *** [/usr/src/app/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/kernels/depthwise_conv.o] Error 1\r\nmake: *** Waiting for unfinished jobs....`\r\n\r\nTried to google what the error message **\"error: x29 cannot be used in asm here\"** means and how to resolve it but could not find anything.\r\nNeed to mention that I am trying to build tflite in a docker container running on my google coral dev board.", "> error: x29 cannot be used in asm here\r\n\r\nExperiencing the same issue while compiling TFLite natively on ARMv8-A (after trying [this](https://stackoverflow.com/questions/56055359/tensorflow-lite-arm64-error-cannot-convert-const-int8x8-t) solution as well).", "Same issue compiling for ARM6", "same issue for me. My platform is nvidia jetson tx2. I changed the eigen url as this [https://github.com/tensorflow/tensorflow/commit/69aa57b4158da0f09ca732ee302ef453fa6b3209#diff-455a4c7f8e22d7c514e8c2caa27506c5](url)", "Same issue. After doing the steps OP talked about I get the same \"**error: x29 cannot be used in asm here**\". Using latest tensorflow on ubuntu 16.04.\r\n\r\nedit: I got it to work by using bazel instead (note you have point to python3 when doing `./configure`): \r\n\r\n`bazel build //tensorflow/lite:libtensorflowLite.so --config=android_arm --cpu=arm64-v8a  --cxxopt=\"-std=c++11\"`", "Same issue on Raspberry Pi3 B+", "Same issue on Raspberry Pi 3B", "download_dependencies.sh completed successfully.\r\n", "> download_dependencies.sh completed successfully.\r\n\r\nBut can you build it afterwards for arm?", "@vizero1 no, there is an error which is X 29 or something like that can not be used \r\n", "any updates on this issue?", "Hi, I am having the same error, I found that cURL command is having\r\n\r\n`<?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Details>No such object: mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/a0d250e79c79.tar.gz</Details></Error>` because http://mirror.tensorflow.org/ is not having `a0d250e79c79.tar.gz`. \r\n\r\nJust for information that tensorflow repo was set to commit f5ce1c00d4397875ff3d706881bd46430f4a9667 (HEAD -> master, tag: v1.14.0-rc0)", "Oh..there is a PR https://github.com/tensorflow/tensorflow/pull/29017 to fix this issue. https://github.com/tensorflow/tensorflow/issues/29030 is also the same as this one.", "@aakbar5 the more significant problem is building tensorflow lite for aarch64. If you run the script you get the following error \"error: x29 cannot be used in asm here\"\r\nWe are waiting for a fix for that problem.", "@vizero1 Here is a possible solution and I tried it on my jetson tx2 board. It worked. In the arm architecture, it seems the x29 register is used as the frame pointer register by default. By adding the '-fomit-frame-pointer' to the CXXFLAGS in the Makefile, we can tell the compiler the x29 register can be used without worrying about impacting the frame pointer. ", "Don't know if this is related.. trying to build `v1.13.1` on `ubuntu` `raspberry pi 2` with\r\n```\r\nCI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.6\" tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n```\r\n\r\nbut getting this error\r\n```\r\n...\r\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\r\nReading package lists...\r\nBuilding dependency tree...\r\nReading state information...\r\nPackage ffmpeg is not available, but is referred to by another package.\r\nThis may mean that the package is missing, has been obsoleted, or\r\nis only available from another source\r\n\r\nE: Unable to locate package clang-format-3.8\r\nE: Couldn't find any package by regex 'clang-format-3.8'\r\nE: Package 'ffmpeg' has no installation candidate\r\nThe command '/bin/sh -c /install/install_deb_packages.sh' returned a non-zero code: 100\r\nERROR: docker build failed. Dockerfile is at /home/ubuntu/apps/tensorflow/tensorflow/tools/ci_build/Dockerfile.pi-python3\r\n```", "> @vizero1 Here is a possible solution and I tried it on my jetson tx2 board. It worked. In the arm architecture, it seems the x29 register is used as the frame pointer register by default. By adding the '-fomit-frame-pointer' to the CXXFLAGS in the Makefile, we can tell the compiler the x29 register can be used without worrying about impacting the frame pointer.\r\n\r\n@dongxiao92 Yes that resolves the \"x29 register\" error but now I am getting the following error: \r\n\r\n```\r\n\r\n> /usr/src/app/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(while.o): In function `tflite::ops::custom::while_kernel::Init(TfLiteContext*, char const*, unsigned long)':\r\n> while.cc:(.text+0x1d88): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n> while.cc:(.text+0x1d94): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n> while.cc:(.text+0x1df8): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n> while.cc:(.text+0x1e08): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n> /usr/src/app/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(audio_spectrogram.o): In function `tflite::ops::custom::audio_spectrogram::Init(TfLiteContext*, char const*, unsigned long)':\r\n> audio_spectrogram.cc:(.text+0x1070): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n> /usr/src/app/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(audio_spectrogram.o):audio_spectrogram.cc:(.text+0x107c): more undefined references to `flatbuffers::ClassicLocale::instance_' follow\r\n> collect2: error: ld returned 1 exit status\r\n\r\n```\r\nSomeone has a solution for that? Found following issues  #26731 and #28863 related to that.", "Hi @dongxiao92 \r\nDid you solve the flatbuffers issue.\r\nI am also getting similar error, Please let me know once you resolve the issue.\r\nThanks inadvance.\r\n\r\n", "@dongxiao92, @thotaram, To solve the flatbuffer, I did the following\r\n`git clone https://github.com/google/flatbuffers.git`\r\n`cd flatbuffers`\r\n`cmake -G \"Unix Makefiles\"`\r\n`make`\r\n`make install`\r\n \r\nOnce flatbuffers is installed, Append `-lflatbuffers` to the `LIBS` in `tensorflow/lite/tools/make/Makefile`.", "@aakbar5 \r\nHi, I have followed your inputs still am getting the similar issue.\r\n\r\nwhile.cc:(.text+0x1d04): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nwhile.cc:(.text+0x1d18): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nwhile.cc:(.text+0x1d6c): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\nwhile.cc:(.text+0x1d78): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n\r\nHave you resolved Flatbuffers issue using above commands.\r\nPlease help me still am missing anything.", "@thotaram,\r\nCould you confirm your `LDOPTS` variable? Mine looks like `LDOPTS := -L/usr/local/lib -L/usr/lib`.", "@aakbar5 \r\nHi,\r\nThank your for your quick reply.\r\nFor me it is like\r\nLDOPTS := -L/usr/lib\r\nI changed LDOPTS as per your suggestions.\r\nLDOPTS := -L/usr/local/lib -L/usr/lib\r\n\r\nStill am getting the error.\r\nMy Makefile changes are \r\nTARGET := aarch64\r\nTARGET_ARCH := armv8-a\r\nCXXFLAGS := -O3 -DNDEBUG -fPIC -flax-vector-conversions -fomit-frame-pointer\r\nLIBS := \\\r\n-lstdc++ \\\r\n-lflatbuffers \\\r\n\r\nCan you please share your Makefile to resolve this problem.\r\n\r\n", "@aakbar5 \r\nIn Makefile i could not find LDOPTS where it is using.\r\nOnly LDOPTS option is there but it is not using in Makefile.\r\nCan you please share where LDOPTS is used in Makefile.", "> In Makefile i could not find LDOPTS where it is using.\r\n> Only LDOPTS option is there but it is not using in Makefile.\r\n> Can you please share where LDOPTS is used in Makefile.\r\n\r\nIf I recall correctly, I was using HEAD revision of the Tensorflow master branch. Moreover I hope you are doing native compilation not cross compilation.\r\n", "I am using tensorflow version 1.13.1\r\nWhile compiling the sources i can see the below command(aarch64-linux-gnu-g++)\r\n\r\naarch64-linux-gnu-g++ O3 -DNDEBUG -fPIC -flax-vector-conversions -fomit-frame-pointer  -std=c++11 -march=armv8-a -funsafe-math-optimizations -ftree-vectorize -fPIC \r\nBelow link i have followed to cross compile tensorflow lite.\r\nhttps://www.tensorflow.org/lite/guide/build_arm64\r\n\r\nPlease give your inputs whether am doing right procedure or not?\r\n", "@aakbar5 Compiled flatbuffers and added it to the libs in Makefile.\r\nChanged LDOPTS to -L/usr/local/lib -L/usr/lib .\r\n```\r\naarch64-linux-gnu-g++ -O3 -DNDEBUG -fPIC -flax-vector-conversions  --std=c++11 -march=armv8-a -funsafe-math-optimizations -ftree-vectorize -flax-vector-conversions -fomit-frame-pointer -fPIC -I. -I/usr/src/app/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/usr/src/app/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/usr/src/app/tensorflow/tensorflow/lite/tools/make/downloads/ -I/usr/src/app/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/usr/src/app/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/usr/src/app/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/usr/src/app/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/usr/src/app/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/usr/src/app/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include \\\r\n-o /usr/src/app/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal /usr/src/app/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/examples/minimal/minimal.o \\\r\n /usr/src/app/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lrt -lstdc++ -lpthread -lm -ldl -lrt\r\n```\r\nThat is the command it uses for compilation. Still have the flatbuffers error :/\r\nUsing head  of tensorflow master branch.", "I could not understand the below your point.\r\nStill have the flatbuffers error :/\r\nYou are also facing the faltbuffers issue, or issue is resolved by changing Makefile.\r\nCan you please send the tensorflow master branch link.\r\n", "@aakbar5 \r\n/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lrt -lstdc++ -lpthread -lm -ldl \r\n\r\nIn the above statement i could not see the \"-lflatbuffers\" library. Although it is there in LIBS in Makefile.", "@thotaram \r\nNow it is running for me. Changed in the aarch64_makefile.inc the LIBS part and added -lflatbuffers.\r\n\r\n```\r\n  LIBS := \\\r\n    -lflatbuffers \\\r\n    -lstdc++ \\\r\n    -lpthread \\\r\n    -lm \\\r\n    -ldl \\\r\n    -lrt\r\n```", "@aakbar5 For some reason the linker is not able to use the flatbuffer library. It gives the following error:\r\n\r\n /usr/lib/gcc-cross/aarch64-linux-gnu/7/../../../../aarch64-linux-gnu/bin/ld: skipping incompatible //usr/local/lib/libflatbuffers.a when searching for -lflatbuffers\r\n/usr/lib/gcc-cross/aarch64-linux-gnu/7/../../../../aarch64-linux-gnu/bin/ld: cannot find -lflatbuffers\r\n\r\nWould it make a difference if you were trying to cross compile instead of natively compiling the tflite for ARM?", "@vizero1 \r\nIncluded -lfaltbuffers in aarch64_makefile.inc file LIBS part.\r\nStill am getting the below error.\r\ntensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lrt -lflatbuffers -lstdc++ -lpthread -lm -ldl -lrt\r\n/usr/lib/gcc-cross/aarch64-linux-gnu/5/../../../../aarch64-linux-gnu/bin/ld: skipping incompatible //usr/local/lib/libflatbuffers.a when searching for -lflatbuffers\r\n/usr/lib/gcc-cross/aarch64-linux-gnu/5/../../../../aarch64-linux-gnu/bin/ld: cannot find -lflatbuffers\r\ncollect2: error: ld returned 1 exit status", "> @aakbar5\r\n> /tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lrt -lstdc++ -lpthread -lm -ldl\r\n> \r\n> In the above statement i could not see the \"-lflatbuffers\" library. Although it is there in LIBS in Makefile.\r\n\r\nHave you tried your luck by adding -lflatbuffers in the above command?", "> @aakbar5 For some reason the linker is not able to use the flatbuffer library. It gives the following error:\r\n> \r\n> /usr/lib/gcc-cross/aarch64-linux-gnu/7/../../../../aarch64-linux-gnu/bin/ld: skipping incompatible //usr/local/lib/libflatbuffers.a when searching for -lflatbuffers\r\n> /usr/lib/gcc-cross/aarch64-linux-gnu/7/../../../../aarch64-linux-gnu/bin/ld: cannot find -lflatbuffers\r\n> \r\n> Would it make a difference if you were trying to cross compile instead of natively compiling the tflite for ARM?\r\n\r\nYes, it's matter a lot as you need to make sure that you are not exposing libs and headers of your host machine for cross-compilation. For example there are a lot of variables in the build system which are just assuming that /usr/lib or /usr/include is having correct libs/headers which won't be case for cross-compilation.", "@aakbar5 \r\nHave you tried your luck by adding -lflatbuffers in the above command?\r\n\r\nI included -lfaltbuffers in aarch64_makefile.inc file. Now it showing as below\r\n\r\ntensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/benchmark-lib.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lrt -lflatbuffers -lstdc++ -lpthread -lm -ldl -lrt\r\n\r\nStill getting the error after adding this -lfaltbuffers also. Please see the below error.\r\n\r\n/usr/lib/gcc-cross/aarch64-linux-gnu/5/../../../../aarch64-linux-gnu/bin/ld: skipping incompatible //usr/local/lib/libflatbuffers.a when searching for -lflatbuffers\r\n/usr/lib/gcc-cross/aarch64-linux-gnu/5/../../../../aarch64-linux-gnu/bin/ld: cannot find -lflatbuffers\r\n", "> @aakbar5\r\n> Have you tried your luck by adding -lflatbuffers in the above command?\r\n> \r\n> I included -lfaltbuffers in aarch64_makefile.inc file. Now it showing as below\r\n> \r\n> tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/benchmark-lib.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lrt -lflatbuffers -lstdc++ -lpthread -lm -ldl -lrt\r\n> \r\n> Still getting the error after adding this -lfaltbuffers also. Please see the below error.\r\n> \r\n> /usr/lib/gcc-cross/aarch64-linux-gnu/5/../../../../aarch64-linux-gnu/bin/ld: skipping incompatible //usr/local/lib/libflatbuffers.a when searching for -lflatbuffers\r\n> /usr/lib/gcc-cross/aarch64-linux-gnu/5/../../../../aarch64-linux-gnu/bin/ld: cannot find -lflatbuffers\r\n\r\n@thotaram, Are you sure that flatbuffers is in /usr/local/lib/ because that's where ld is looking for. However make sure that your flatbuffers lib is built with the correct toolchain. \r\n\r\nAre you sure that you are not mixing libraries of different toolchains? I am thinking about this due to `skipping incompatible` message.", "Below is my flatbuffer library.\r\n-rwxr-xr-x 1 root root 4897132 Jun  6 10:34 /usr/local/lib/libflatbuffers.a\r\nAs per your inputs i compiled the flatbuffers source code.\r\ncmake -G \"Unix Makefiles\"\r\nmake\r\nmake install\r\nPlease let me know still am missing anything.\r\n", "@aakbar5 \r\nHi, \r\nSkipping incompatible issue is not coming now. Getting the below error.\r\n\r\nopt/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/bin/../lib/gcc/aarch64-linux-gnu/7.4.1/../../../../aarch64-linux-gnu/bin/ld: cannot find -lflatbuffers\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:269: recipe for target '/home/aiiec/ARMNN/TF_lite_latest/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal' failed\r\nmake: *** [/home/xxxx/ARMNN/TF_lite_latest/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\naarch64-linux-gnu-ar: creating /home/xxxx/ARMNN/TF_lite_latest/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/benchmark-lib.a\r\n\r\nCan you please suggest how to resolve it.", "@aakbar5 \r\nHi,\r\nCan you please share cmake command to cross compile the Flatbuffers source code for ARM64\r\nThanks andvance.", "Hi ..!\r\nI git-cloned tensorflow, and on the latest branch i executed `download_dependencies.sh` and `build_aarch64_lib.sh` \r\n( after disabling the NNAPI and adding the above mentioned compiler flags )\r\n\r\nI was able to generate the static library `libtensorflow-lite.a`, but the compilation terminated with error on compiling `<TF root>/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal`\r\n\r\nI'm not sure whether the static library compilation is complete.\r\nCan anyone please help me in pointing out a resource / example for using this static library for building a simple working application ..?!", "Hi All,\r\n\r\nI am still facing issue with eigen URL, even I use http or https\r\nwget https://mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/a0d250e79c79.tar.gz --no-check-certificate\r\n--2019-06-27 11:47:15--  https://mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/a0d250e79c79.tar.gz\r\nResolving mirror.tensorflow.org (mirror.tensorflow.org)... 172.217.163.176, 2404:6800:4007:80f::2010\r\nConnecting to mirror.tensorflow.org (mirror.tensorflow.org)|172.217.163.176|:443... connected.\r\nWARNING: no certificate subject alternative name matches\r\n        requested host name \u2018mirror.tensorflow.org\u2019.\r\nHTTP request sent, awaiting response... 404 Not Found\r\n2019-06-27 11:47:16 ERROR 404: Not Found.\r\n\r\nI changed to http://www.bitbucket.org/eigen/eigen/get/a0d250e79c79.tar.gz and it is compiling for me but can we get some fix, **I don't want to change URL for each and every release** ?\r\n\r\nPlease note, I am not facing compilation issue because I wrote my own CMake for tflite just for compiling libtensorflow-lite.a taking reference from Makefile, I didn't need to compile flatbuffer library but has to add one util.cpp from it. I am not compiling Benchmark app so can't say this will work for you. If I will compile benchmark will let you know.", "Hi All,\r\nTensorflow lite build is successful, able to run examples.\r\n\r\nlabel_image.cc example ran successfully with opencv 3.2.0 version but getting errors with opencv 3.4 and opencv 4.1.0 versions.\r\n\r\nbelow code is changed in the Makefile\r\n$(LABEL_IMAGE_BINARY): $(LABEL_IMAGE_OBJS) $(LIB_PATH)\r\n\t@mkdir -p $(dir $@)\r\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) \\\r\n\t-o $(LABEL_IMAGE_BINARY) $(LABEL_IMAGE_OBJS) \\\r\n\t$(LIBFLAGS) $(LIB_PATH) $(LDFLAGS) $(LIBS) -L/home/xxx/opencvAarch64/opencv/build/install/lib -lopencv_core -lopencv_imgcodecs -lopencv_imgproc -lopencv_highgui\r\n\r\nWith opencv 3.2 is working fine, getting errors when includes 3.4 or 4.1 libraries.\r\nERROR:\r\n /home/xxx/TFlite/TensorFlow_Lite/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lstdc++ -lpthread -lm -ldl -lrt /home/xxx/TFlite/TensorFlow_Lite/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/benchmark-lib.a -L/home/xxx/opencvAarch64/opencv_4.1.0/opencv-4.1.0/build/install/lib -lopencv_core -lopencv_imgcodecs -lopencv_imgproc -lopencv_highgui -lopencv_features2d -lopencv_calib3d -lopencv_flann\r\n/home/xxx/TFlite/TensorFlow_Lite/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/examples/custom_sv_psd_image/psd/src_psd/psdl_label.o: In function `tflite::label_image::RunInference(tflite::label_image::Settings*)':\r\nlabel.cc:(.text+0x2214): **undefined reference to `cv::imread(cv::String const&, int)'**\r\nlabel.cc:(.text+0x22c4): undefined reference to `cv::String::deallocate()'\r\nlabel.cc:(.text+0x2f88): undefined reference to `cv::String::allocate(unsigned long)'\r\nlabel.cc:(.text+0x3678): undefined reference to `cv::String::deallocate()'\r\n\r\n\r\nPlease suggest how to resolve this problem.\r\n\r\nThanks,\r\nRam.\r\n", "I've tested download_dependencies.sh and build_aarch64_lib.sh today.\r\nIt works well so let me close this issue.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28926\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28926\">No</a>\n"]}, {"number": 28925, "title": "ValueError: Attempt to convert a value (<6x6 sparse matrix of type '<class 'numpy.float64'>' \twith 1 stored elements in COOrdinate format>) with an unsupported type (<class 'scipy.sparse.coo.coo_matrix'>) to a Tensor.", "body": "\r\nI am building a recommendation model using the WALS algorithm and implementing it in Tersorflow. I have processed the data and when I try to initialize the object I get the error below. \r\n\r\nValueError: Attempt to convert a value (<6x6 sparse matrix of type '<class 'numpy.float64'>'\r\n\twith 1 stored elements in COOrdinate format>) with an unsupported type (<class 'scipy.sparse.coo.coo_matrix'>) to a Tensor.\r\n\r\n`input_tensor = tf.SparseTensor(indices=list(zip(tr_sparse.row, tr_sparse.col)),\r\n                                values=(tr_sparse).astype(np.float64),\r\n                                dense_shape=tr_sparse.shape)`", "comments": ["@AmandaCSantos Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28924, "title": "tensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Custom code\r\n- Ubuntu 18\r\n\r\n- TensorFlow installed from source:\r\n- TensorFlow version :2.0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version:10.0/\r\n- GPU model and memory: 1080Ti/11Go\r\n\r\nContrary to the docs, tf.print do not handle printing tensors in a session run (for example when building a keras model with the model API)\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/print\r\n\r\n\r\n**Describe the expected behavior**\r\nI expect the example shown in the docs to run smoothly\r\n\r\n\r\n\r\n**Code to reproduce the issue**\r\n```pytho\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import BatchNormalization\r\nimport numpy as np\r\ninput = np.random.uniform(size=(6, 40, 40, 1)).astype(np.float32)\r\n\r\nds = tf.data.Dataset.from_tensor_slices(input).batch(2)\r\n\r\niterator = iter(ds)\r\ninp = tf.keras.Input((None, None, 1))\r\nprint_op = tf.print(inp)\r\nwith tf.control_dependencies([print_op]):\r\n    out = tf.keras.layers.Conv2D(5, 3)(inp)\r\nmodel = tf.keras.Model(inputs=inp, outputs=out)\r\nmodel(next(iterator))\r\n```\r\n\r\nWhich raises:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_string_ops.py\", line 807, in string_format\r\n    summarize)\r\ntensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"log_test.py\", line 13, in <module>\r\n    print_op = tf.print(inp)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/logging_ops.py\", line 339, in print_v2\r\n    name=format_name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/string_ops.py\", line 192, in string_format\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_string_ops.py\", line 813, in string_format\r\n    summarize=summarize, name=name, ctx=_ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_string_ops.py\", line 867, in string_format_eager_fallback\r\n    _attr_T, inputs = _execute.convert_to_mixed_eager_tensors(inputs, _ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 210, in convert_to_mixed_eager_tensors\r\n    types = [t._datatype_enum() for t in v]  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 210, in <listcomp>\r\n    types = [t._datatype_enum() for t in v]  # pylint: disable=protected-access\r\nAttributeError: 'Tensor' object has no attribute '_datatype_enum'\r\n```\r\n\r\n", "comments": ["Thanks for finding the issue. I could reproduce the issue with TF2.0. ", "Any updates on this? \r\n", "Any solution here?", "I am getting the same error when mixing an LSTM with some additional preprocessing formulas (that have their own parameters to be fitted)", "Any updates?", "I\u00b4m having this issue as well. Could someone help with why is happening? Thanks.", "I'm having the same issue while converting code from Pytorch to tensorflow2.0, did anyone find a solution? ", "I have the same issue when I updated TensorFlow version 1.13 to 1.14. ", "I had this issue with Tensorflow 2.0.0. Downgrading to 1.13.2 resolved the issue, which for me was an acceptable solution.", "I\u00b4m having this issue as well", "Is there any update on this issue with Tensorflow v2.0?", "Any update with TF2.0 ? ", "Same issue here", "Same issue on version 2", "Thanks for the issue!\r\n\r\n`control_dependencies` should not be used in TF2.x.\r\n\r\nThe way to do this is:\r\n\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import BatchNormalization\r\nimport numpy as np\r\ninput = np.random.uniform(size=(6, 40, 40, 1)).astype(np.float32)\r\n\r\nds = tf.data.Dataset.from_tensor_slices(input).batch(2)\r\n\r\ndef _print(t):\r\n  tf.print(t)\r\n  return t\r\n\r\niterator = iter(ds)\r\ninp = tf.keras.Input((None, None, 1))\r\nout = tf.keras.layers.Lambda(_print)(inp)\r\nout = tf.keras.layers.Conv2D(5, 3)(out)\r\nmodel = tf.keras.Model(inputs=inp, outputs=out)\r\nmodel(next(iterator))\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28924\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28924\">No</a>\n", "I am still getting this error and I am not using control_dependencies", "I am still getting this error also. Tensorflow 2.1.0", "Same error here: Tensorflow 2.2.0", "Found an easy solution for this problem in Tensorfolw v 2.xx.xx:\r\n\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_eager_execution()", "Had the same problem when using a generator. Turns out that on_epoch_end and on_epoch_begins where not called. I added \r\nLambdaCallback(on_epoch_end=train_gen.on_epoch_end,on_epoch_begin=train_gen.on_epoch_begin) to callbacks and this solved the issue."]}, {"number": 28923, "title": "TF 2.0 save/restore DenseFeatures", "body": "** Code **\r\nFollwing this guide i created a model to classify structured data https://www.tensorflow.org/alpha/tutorials/keras/feature_columns :\r\n```\r\nfeature_columns = []\r\nheaders = dataframe.columns.tolist()\r\n\r\n# feature cols\r\nfor header in headers:\r\n    temp = feature_column.numeric_column(header)\r\n    feature_columns.append(feature_column.bucketized_column(temp, boundaries=[-89, -70, -65, -60, -55, -50, -40, -30, -20]))\r\n    #feature_columns.append(temp) #old code used without buckets. only numeric columns\r\n\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n\r\nmodel = tf.keras.Sequential([\r\n  feature_layer,\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(3, activation='sigmoid') #cambiare con il numero di zone\r\n])\r\n```\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- TensorFlow version (use command below): 2.0.0 alpha\r\n- Python version: 3.6\r\n\r\n**Describe the behavior**\r\n\r\nNow when i try to save with the usual \r\n`model.save('path_to_my_model.h5')`\r\ni cannot restore the model since i get an exception:\r\n\r\n> Unknown layer: DenseFeatures\r\n\r\nlooking at some other [https://github.com/tensorflow/tensorflow/issues/26835](url) posts  I tried with \r\n`tf.keras.experimental.export_saved_model(model, 'experimental.h5')`\r\nbut here I get another exception:\r\n> __init__() missing 1 required positional argument: 'feature_columns'\r\n\r\nTried to restore using \r\n```\r\nfor header in headers:\r\n    temp = feature_column.numeric_column(header)\r\n    feature_columns.append(feature_column.bucketized_column(temp, boundaries=[-89, -70, -65, -60, -55, -50, -40, -30, -20]))\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n\r\nnew_model = tf.keras.models.load_model('guesser.h5',custom_objects={'DenseFeatures':feature_layer})\r\n\r\nnew_model.summary()\r\n```\r\nbut it always give me the feature_columns error\r\n", "comments": ["Tried with 2.0.0-alpha on Colab and was able to get the mentioned output.", "I tried with tensorflow-2.0.0beta1, the issue is still there.", "I spent time debugging issue in load_model(). Here is my theory:\r\n- As for the error message \"Unknown layer: DenseFeatures\",  the problem happens with this line of code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/serialization.py#L82.\r\nDenseFeatures class is not part of the dict returned by the call to globals().\r\n\r\nThis problem can be solved (not sure if it is a correct way) by adding the following import to serialization.py:\r\nfrom tensorflow.python.feature_column.feature_column_v2 import DenseFeatures\r\n\r\n- After that, load_model() will complain about \"init() missing 1 required positional argument: 'feature_columns'\". This happens in the __init__ of DenseFeatures class. I think the root cause is that feature columns information is not serialized in the saved model file or is not read from the file correctly. From the deserialized model_config dict, I don't see feature columns.\r\n\r\nCould anyone from the team take a look?  Thank you.\r\n", "Digging more on this. based on this commit: https://github.com/tensorflow/tensorflow/commit/cd09510f9218220b872803c9ee70ba1dcd8c0f45#diff-99593c5667932dcaa61fb3e1b4921cc1, it seems like @karmel was fixing this issue and the fix hasn't made it to tf2.0 beta1 yet?\r\n\r\n", "Can you try using the TensorFlow format for saving and restoring?\r\n\r\n```\r\n# feature cols\r\nfor header in headers:\r\n    temp = feature_column.numeric_column(header)\r\n    feature_columns.append(feature_column.bucketized_column(temp, boundaries=[-89, -70, -65, -60, -55, -50, -40, -30, -20]))\r\n    #feature_columns.append(temp) #old code used without buckets. only numeric columns\r\n\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n\r\nmodel = tf.keras.Sequential([\r\n  feature_layer,\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(3, activation='sigmoid') #cambiare con il numero di zone\r\n])\r\n\r\nmodel.save('output_path', format='tf')\r\n```\r\n\r\nThat should work with the beta, though the change in question may have missed the cut. If it doesn't work, try with the nightly 2.0 release: https://pypi.org/project/tf-nightly-2.0-preview/\r\n\r\n", "With tf.version 2.0.0-dev20190724, I was not able to save a model using `model.save('output_path', save_format='tf')`. I used the keras functional api to build the model.\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-16-070fe449db81> in <module>\r\n      1 # doesn't open successfully\r\n----> 2 model.save('wd-model', save_format='tf')\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format)\r\n   1158     ```\r\n   1159     \"\"\"\r\n-> 1160     saving.save_model(self, filepath, overwrite, include_optimizer, save_format)\r\n   1161 \r\n   1162   def save_weights(self, filepath, overwrite=True, save_format=None):\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format)\r\n    105         model, filepath, overwrite, include_optimizer)\r\n    106   else:\r\n--> 107     saved_model_save.save(model, filepath, overwrite, include_optimizer)\r\n    108 \r\n    109 \r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer)\r\n     83     model.optimizer = None\r\n     84 \r\n---> 85   save_lib.save(model, filepath)\r\n     86 \r\n     87   if not include_optimizer:\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py in save(obj, export_dir, signatures)\r\n    838   object_saver = util.TrackableSaver(checkpoint_graph_view)\r\n    839   asset_info, exported_graph = _fill_meta_graph_def(\r\n--> 840       meta_graph_def, saveable_view, signatures)\r\n    841   saved_model.saved_model_schema_version = (\r\n    842       constants.SAVED_MODEL_SCHEMA_VERSION)\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py in _fill_meta_graph_def(meta_graph_def, saveable_view, signature_functions)\r\n    568 \r\n    569   with exported_graph.as_default():\r\n--> 570     signatures = _generate_signatures(signature_functions, resource_map)\r\n    571     for concrete_function in saveable_view.concrete_functions:\r\n    572       concrete_function.add_to_graph()\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py in _generate_signatures(signature_functions, resource_map)\r\n    442             argument_inputs, signature_key, function.name))\r\n    443     outputs = _call_function_with_mapped_captures(\r\n--> 444         function, mapped_inputs, resource_map)\r\n    445     signatures[signature_key] = signature_def_utils.build_signature_def(\r\n    446         _tensor_dict_to_tensorinfo(exterior_argument_placeholders),\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py in _call_function_with_mapped_captures(function, args, resource_map)\r\n    393   \"\"\"Calls `function` in the exported graph, using mapped resource captures.\"\"\"\r\n    394   export_captures = _map_captures_to_created_tensors(\r\n--> 395       function.graph.captures, resource_map)\r\n    396   mapped_inputs = args + export_captures\r\n    397   # Calls the function quite directly, since we have new captured resource\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py in _map_captures_to_created_tensors(original_captures, resource_map)\r\n    315            \"be tracked by assigning them to an attribute of a tracked object \"\r\n    316            \"or assigned to an attribute of the main object directly.\")\r\n--> 317           .format(interior))\r\n    318     export_captures.append(mapped_resource)\r\n    319   return export_captures\r\n\r\nAssertionError: Tried to export a function which references untracked object Tensor(\"StatefulPartitionedCall/args_55:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.```", "@cysmnl Can you post a standalone code to reproduce the issue? Thanks!", "I am **very** confused about which API I should use: see [this simple Colab example](https://colab.research.google.com/drive/1CXESHGbE8pPZqtBPcsS5UV2tj_MYrxaH) with different approaches of *saving* and *restoring* a `tf.keras.Sequential` model with `feature_columns / DenseFeatures`.\r\n\r\n---\r\n**Save**:\r\n```\r\n\ud83e\uddd0 Saved with keras APIs.\r\n\ud83e\uddd0 Saved with keras APIs in SavedModel format.\r\n\ud83d\ude35 Could **not** save with export_saved_model: __init__() missing 1 required positional argument: 'feature_columns'\r\n\ud83e\uddd0 Saved with tf.saved_model.save.\r\n```\r\n**Restore**:\r\n```\r\n\ud83d\ude35 Couldn't load model via tf.keras.models.load_model (without custom objects): Unknown layer: DenseFeatures\r\n\ud83d\ude35 Couldn't load model via tf.keras.models.load_model (with custom objects): __init__() missing 1 required positional argument: 'feature_columns'\r\n\ud83d\ude35 Couldn't load model via tf.keras.experimental.load_from_saved_model: keras_model_tf/assets/saved_model.json; No such file or directory\r\n\ud83d\ude35 Couldn't load model via tf.keras.models.load_model: Passing a dictionary input to a Sequential Model which doesn't have FeatureLayer as the first layer is an error.\r\n\ud83e\uddd0 Model restored via `tf.saved.model.load('keras_model_tf')`\r\n   Accuracy: 95.00%\r\n\ud83e\uddd0 Model restored via `tf.saved.model.load('tf_saved_model/1/')`\r\n   Accuracy: 95.00%\r\n```\r\n---\r\nI am now quite unsure about where the issue is and what are the best practices for exporting and restoring in this case.", "I am currently submitting a fix for this issue, which will allow DenseFeatures to be saved and loaded using Keras APIs (`model.save` and `tf.keras.models.load_model`). \r\n\r\nThe best practices can be summed up to:\r\n If you are planning to load the model back into Keras, use the Keras APIs for saving and loading. If you're not planning on using the Keras APIs to further train the model, then `tf.saved_model.save` and `tf.saved_model.load` is sufficient. \r\n\r\nI believe the docs need to be updated, as the experimental function `tf.keras.experimental.export_saved_model` is now deprecated.", "@batikus Is this resolved? Please take a look at the [`feature_column`]((https://www.tensorflow.org/beta/tutorials/keras/feature_columns)) tutorial. I could save the model in \"tf\" and \".h5\" formats without any issue.  Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/b6d63d35fd05058a3eb204891ca891c0/feature_columns.ipynb). If you still have an issue, please provide a simple standalone code to reproduce the issue. Thanks!", "I am closing the issue as it was resolved in `tf-nightly-2.0-preview`. Please feel free to open it if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28923\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28923\">No</a>\n", "UPD: after investigation found that for SequenceFeatures should be open another issue, because this fix does not cover SequenceFeatures behavior. The new issue is #32385\r\n\r\n> I am currently submitting a fix for this issue, which will allow DenseFeatures to be saved and loaded using Keras APIs (`model.save` and `tf.keras.models.load_model`).\r\n\r\n@k-w-w , does this fix cover `tf.keras.experimental.SequenceFeatures` as well?\r\n\r\nI save model by `tf.keras.models.model.save('MODEL_PATH.h5')`. Then while loading model by `tf.keras.models.load_model(MODEL_PATH.h5)`, I experience the similar error message as described in this issue: \r\n\r\n> Unknown layer: SequenceFeatures\r\n\r\nI have not found separete issue for save\\load problems concerning `SequenceFeatures` but my problem is reproduced in tensorflow _2.0.0-rc0_ and _2.0.0b1_  and **do not** reproduced in **tf-nightly-2.0-preview**. It is not clear now, open new issue for `SequenceFeatures` or not.\r\n\r\n"]}, {"number": 28922, "title": "newest dockerfile did not include nccl", "body": "\r\n", "comments": ["Looks like a spam. Closing this issue. If you are stuck, please open a separate issue with all information filled as per the [template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!"]}, {"number": 28921, "title": "AWS S3 implementation can't handle weird path containing an \"=\" sign", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n\r\n**Describe the current behaviour**\r\nthe AWS S3 implementation can't handle weird path\r\n\r\n**Describe the expected behavior**\r\nthe AWS S3 implementation should be able to handle weird path\r\n\r\n**Code to reproduce the issue**\r\n```bash\r\nexport AWS_ACCESS_KEY_ID=XXX\r\nexport AWS_SECRET_ACCESS_KEY=XXX\r\nexport AWS_REGION=eu-west-1\r\nexport S3_ENDPOINT=XXX\r\nexport S3_USE_HTTPS=0\r\nexport S3_VERIFY_SSL=0\r\n\r\nexport S3_BUCKET=accesstest\r\nexport NORMAL_FILENAME=normal.txt\r\nexport WEIRD_FILENAME=weird=true.txt # Contains an \"=\"\r\n\r\n\r\n# Make bucket\r\nif ! aws --endpoint-url http://$S3_ENDPOINT s3api head-bucket --bucket \"$S3_BUCKET\" 2>/dev/null\r\nthen\r\n  echo \"Creating bucket $S3_BUCKET\"\r\n  aws --endpoint-url http://$S3_ENDPOINT s3 mb s3://$S3_BUCKET\r\nfi\r\n\r\necho \"Creating file\"\r\necho \"test\" > file.txt\r\n\r\n\r\necho \"Upload a normal file\"\r\naws --endpoint-url http://$S3_ENDPOINT s3 cp file.txt s3://$S3_BUCKET/$NORMAL_FILENAME\r\necho \"\"\r\n\r\necho \"Upload a weird name file\"\r\naws --endpoint-url http://$S3_ENDPOINT s3 cp file.txt s3://$S3_BUCKET/$WEIRD_FILENAME\r\necho \"\"\r\n\r\necho \"Check that files exist\"\r\naws --endpoint-url http://$S3_ENDPOINT s3 ls --human-readable s3://$S3_BUCKET/\r\necho \"\"\r\n\r\n\r\necho \"Check TF access\"\r\npython bug.py\r\n\r\n```\r\n```python\r\nimport os\r\nfrom tensorflow.python.lib.io import file_io\r\n\r\nassert \"AWS_ACCESS_KEY_ID\" in os.environ\r\nassert \"AWS_SECRET_ACCESS_KEY\" in os.environ\r\nassert \"S3_ENDPOINT\" in os.environ\r\nassert \"S3_USE_HTTPS\" in os.environ\r\nassert \"S3_VERIFY_SSL\" in os.environ\r\n\r\nassert \"S3_BUCKET\" in os.environ\r\nassert \"NORMAL_FILENAME\" in os.environ\r\nassert \"WEIRD_FILENAME\" in os.environ\r\n\r\nprint(\"Checking access to normal file {} \\nShould return '<tensorflow.python.pywrap_tensorflow_internal.FileStatistics; proxy of <Swig Object of type 'tensorflow::FileStatistics *' at 0x10c2171b0> >'\\n\".format(os.environ[\"NORMAL_FILENAME\"]))\r\nprint(file_io.stat('s3://{}/{}'.format(os.environ[\"S3_BUCKET\"], os.environ[\"NORMAL_FILENAME\"])))\r\n\r\nprint(\"\\n\")\r\nprint(\"Checking access to weird file {} \\nShould return '<tensorflow.python.pywrap_tensorflow_internal.FileStatistics; proxy of <Swig Object of type 'tensorflow::FileStatistics *' at 0x10c2171b0> >'\\n\".format(os.environ[\"WEIRD_FILENAME\"]))\r\nprint(file_io.stat('s3://{}/{}'.format(os.environ[\"S3_BUCKET\"], os.environ[\"WEIRD_FILENAME\"])))\r\n```\r\n\r\n**Other info / logs**\r\nResults of the scripts:\r\n```\r\nUpload a normal file\r\nupload: ./file.txt to s3://accesstest/normal.txt               \r\n\r\nUpload a weird name file \r\nupload: ./file.txt to s3://accesstest/weird=true.txt          \r\n\r\nCheck that files exist\r\n2019-05-22 10:04:32    5 Bytes normal.txt\r\n2019-05-22 10:04:33    5 Bytes weird=true.txt\r\n\r\nCheck TF access\r\nChecking access to normal file normal.txt \r\nShould return '<tensorflow.python.pywrap_tensorflow_internal.FileStatistics; proxy of <Swig Object of type 'tensorflow::FileStatistics *' at 0x10c2171b0> >'\r\n\r\n2019-05-22 10:04:35.615026: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /Users/morgangiraud//.aws/config and using profilePrefix = 1\r\n2019-05-22 10:04:35.615056: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /Users/morgangiraud//.aws/credentials and using profilePrefix = 0\r\n2019-05-22 10:04:35.615074: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /Users/morgangiraud//.aws/credentials for credentials file and /Users/morgangiraud//.aws/config for the config file , for use with profile default\r\n2019-05-22 10:04:35.615101: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections2 and scheme http\r\n2019-05-22 10:04:35.615134: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2\r\n2019-05-22 10:04:35.615155: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000\r\n2019-05-22 10:04:35.615178: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-05-22 10:04:35.615462: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25\r\n2019-05-22 10:04:35.615566: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-05-22 10:04:35.615716: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2\r\n2019-05-22 10:04:35.615726: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-05-22 10:04:35.758012: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-05-22 10:04:35.758177: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n<tensorflow.python.pywrap_tensorflow_internal.FileStatistics; proxy of <Swig Object of type 'tensorflow::FileStatistics *' at 0x122576990> >\r\n\r\n\r\nChecking access to weird file weird=true.txt \r\nShould return '<tensorflow.python.pywrap_tensorflow_internal.FileStatistics; proxy of <Swig Object of type 'tensorflow::FileStatistics *' at 0x10c2171b0> >'\r\n\r\n2019-05-22 10:04:35.827145: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-05-22 10:04:35.827307: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-05-22 10:04:35.891635: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 403\r\n2019-05-22 10:04:35.891774: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\r\n2019-05-22 10:04:35.892084: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-05-22 10:04:35.892314: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 19, in <module>\r\n    print(file_io.stat('s3://{}/{}'.format(os.environ[\"S3_BUCKET\"], os.environ[\"WEIRD_FILENAME\"])))\r\n  File \"/Users/morgangiraud/miniconda3/envs/clever-tensorboard/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 735, in stat\r\n    return stat_v2(filename)\r\n  File \"/Users/morgangiraud/miniconda3/envs/clever-tensorboard/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 754, in stat_v2\r\n    return file_statistics\r\n  File \"/Users/morgangiraud/miniconda3/envs/clever-tensorboard/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Object s3://accesstest/weird=true.txt does not exist\r\n```\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Thanks for your answer.\r\n\r\nI'm well aware of that fact but since the AWS API & CLI works fine, the only layer that looked suspicious to me was the TF layer and so it would introduce at least a limitation if not a bug.\r\nI'm not familiar enough with this part of TF code to tell for sure though.\r\n\r\nMy bad if the bug comes from another 3rd-party.", "@morgangiraud On Linux with nightly build, I don't see an issue with file_io:\r\n```\r\nwith file_io.FileIO('s3://test-xxx/weird=test.txt', \"rb\") as f:\r\n    print(f.read())\r\n```", "Hi Yong Tang,\r\n\r\nThanks for your feedback, I've just made more tests and it looks, it is not coming from TensorFlow:\r\n- it works in docker nightly with official AWS S3 endpoint.\r\n- it does not work in docker nightly with my 3rparty endpoint.\r\n\r\nSorry for this wrong report, have a nice day."]}, {"number": 28920, "title": "Limit of Classes (label_map.txt or label_map.pbtxt)", "body": "Is there any limit in label map classes used in the training config file?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there and provide faster resolution to such issues. Thanks!\r\n"]}, {"number": 28919, "title": "resize_image_with_pad output dimensions must be positive", "body": "**System information**\r\n- Custom code\r\n- OS Platform and Distribution Windows 10:\r\n- TensorFlow installed using pip\r\n- TensorFlow versions: tested 1.12, 1.13:\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9\r\n- GPU model and memory:  GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7715\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.63GiB\r\n\r\n**Describe the current behavior**\r\nI have a custom estimator which i trained and i use predictor.from_saved_model(model_dir) to load an exported model. At the preprocessing stage i resize the image with tf.image.resize_image_with_pad with the bilinear method to keep the aspectratio. This works well for a seemingly arbitrary amount of images but sometimes the program crashes with this error:\r\n\r\nInvalidArgumentError (see above for traceback): output dimensions must be positive\r\n\r\nI read the file with PIL and convert it to a numpy array. Afterwards i check\r\n1. If the image has 3 dimensions\r\n2. Has at least 5 rows and cols \r\n3. Has 3 channels\r\n4. The image is not all zeros\r\n5. The image values lie between 0 and 255\r\nSo i guess it should be a valid input array.\r\n\r\nI do provide correct input parameters for the resize fn, and try to resize the images to 128x128.\r\n\r\n**Other info / logs**\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: output dimensions must be positive\r\n\t [[{{node map/while/resize_image_with_pad/resize_images/ResizeBilinear}} = ResizeBilinear[T=DT_FLOAT, align_corners=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](map/while/resize_image_with_pad/control_dependency, map/while/resize_image_with_pad/resize_images/size)]]\r\n\t [[{{node add_19/_165}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_561_add_19\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/bab/projects/vat_image_classification/run_inference.py\", line 31, in <module>\r\n    main(sys.argv)\r\n  File \"C:/bab/projects/vat_image_classification/run_inference.py\", line 27, in main\r\n    inf.inference_multi(model_dir, config)\r\n  File \"C:\\bab\\projects\\vat_image_classification\\inference\\multi_predict_from_sql_roi.py\", line 163, in inference_multi\r\n    predict_to_sql(result, predict_fn, config, engine)\r\n  File \"C:\\bab\\projects\\vat_image_classification\\inference\\multi_predict_from_sql_roi.py\", line 122, in predict_to_sql\r\n    prediction.append(predict_fn({'images_ul': [image]})['probabilities'][0][1])\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\contrib\\predictor\\predictor.py\", line 77, in __call__\r\n    return self._session.run(fetches=self.fetch_tensors, feed_dict=feed_dict)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: output dimensions must be positive\r\n\t [[node map/while/resize_image_with_pad/resize_images/ResizeBilinear (defined at C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\contrib\\predictor\\saved_model_predictor.py:153)  = ResizeBilinear[T=DT_FLOAT, align_corners=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](map/while/resize_image_with_pad/control_dependency, map/while/resize_image_with_pad/resize_images/size)]]\r\n\t [[{{node add_19/_165}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_561_add_19\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 'map/while/resize_image_with_pad/resize_images/ResizeBilinear', defined at:\r\n  File \"C:/bab/projects/vat_image_classification/run_inference.py\", line 31, in <module>\r\n    main(sys.argv)\r\n  File \"C:/bab/projects/vat_image_classification/run_inference.py\", line 27, in main\r\n    inf.inference_multi(model_dir, config)\r\n  File \"C:\\bab\\projects\\vat_image_classification\\inference\\multi_predict_from_sql_roi.py\", line 136, in inference_multi\r\n    predict_fn = predictor.from_saved_model(model_dir)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\contrib\\predictor\\predictor_factories.py\", line 153, in from_saved_model\r\n    config=config)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\contrib\\predictor\\saved_model_predictor.py\", line 153, in __init__\r\n    loader.load(self._session, tags.split(','), export_dir)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 197, in load\r\n    return loader.load(sess, tags, import_scope, **saver_kwargs)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 350, in load\r\n    **saver_kwargs)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 278, in load_graph\r\n    meta_graph_def, import_scope=import_scope, **saver_kwargs)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1696, in _import_meta_graph_with_return_elements\r\n    **kwargs))\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\r\n    return_elements=return_elements)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 442, in import_graph_def\r\n    _ProcessNewOps(graph)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 234, in _ProcessNewOps\r\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3440, in _add_new_tf_operations\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3440, in <listcomp>\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3299, in _create_op_from_tf_operation\r\n    ret = Operation(c_op, self)\r\n  File \"C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): output dimensions must be positive\r\n\t [[node map/while/resize_image_with_pad/resize_images/ResizeBilinear (defined at C:\\Users\\r.beckmann\\AppData\\Local\\conda\\conda\\envs\\babenv\\lib\\site-packages\\tensorflow\\contrib\\predictor\\saved_model_predictor.py:153)  = ResizeBilinear[T=DT_FLOAT, align_corners=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](map/while/resize_image_with_pad/control_dependency, map/while/resize_image_with_pad/resize_images/size)]]\r\n\t [[{{node add_19/_165}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_561_add_19\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\n", "comments": ["Can you please help us to get the minimal code snippet to reproduce the issue. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Do you solve the problem ? cause I met the same error ."]}, {"number": 28918, "title": "fix huber loss function for when delta = inf", "body": "Convert huber loss = l2 loss when delta = inf.\r\n\r\nHuber loss should be defined as l2 loss when delta = inf. This clears up poorly defined behavior.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28918) for more info**.\n\n<!-- need_sender_cla -->", "I've signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28918) for more info**.\n\n<!-- ok -->", "@cfifty  Could you please check reviewer comments and keep us posted. Thanks!", "> @cfifty Could you please check reviewer comments and keep us posted. Thanks!\r\n\r\nApologies for the delay, ICML's been keeping me busy. I'll update the PR pursuant to Alex's comment this weekend.", "Apologies, it's taking me a while to get bazel setup on my local for testing.", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28917, "title": "libtensorflowlite.so crash when load model, just crash on SessionOption destructor", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:HUAWEI armv8 mobile\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): r1.13\r\nPython version:2.7\r\nBazel version (if compiling from source):19.0.2\r\nGCC/Compiler version (if compiling from source):4.9\r\nCUDA/cuDNN version:no\r\nGPU model and memory:no\r\n\r\nDescribe the current behavior\r\nI compile tensorflow lite arm64-v8a and armeabi-v7a, both success build, when i use them on my android device with my c++ code and own model, armeabi-v7a and get correct result, but arm64-v8a crash when destruct SessionOptions.\r\n\r\nCode to reproduce the issue, I want to load my own model but failed, so I just using this line, and the function crash.\r\nstd::unique_ptr<tensorflow::Session> ModelLoader::load_pb(const std::string& graph_file_name) {\t\r\n\ttensorflow::SessionOptions options;\r\n\treturn nullptr;\r\n}\r\n\r\nOther info / logs\r\nI try to rebuild the tensorflow using ndk-r10e, ndk-r11c,ndk-r12b, ndk-r14b, and ndk-r15c, both build success and cannot work.\r\n\r\nmy bazel command: bazel build //tensorflow/lite:libtensorflowLite.so --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\"\r\n\r\nI add  lines  into tensorflow/lite BUILD File:\r\ncc_binary( \r\n    name = \"libtensorflowLite.so\",\r\n    linkopts=[\r\n        \"-shared\", \r\n        \"-Wl,-soname=libtensorflowLite.so\",\r\n    ],\r\n    linkshared = 1,\r\n    copts = tflite_copts(),\r\n    deps = [\r\n        \":framework\",\r\n        \"//tensorflow/lite/kernels:builtin_ops\",\r\n        \"//tensorflow//lite/delegates/flex:delegate\",\r\n    ],\r\n)\r\n\r\nI thick the so is ok because armeabi-v7a can work successfully.  Thank you very much to look this issue.", "Can you try adding the `--config=monolithic` flag to your `bazel build` command?", "I use .pb file in android,  should i change it to .tflite file to using my own model? I will try to rebuild as you recommended.", "TensorFlow Lite requires use of .tflite models, generated with the TensorFlow Lite converter. It cannot read or use frozen graphs or graph defs (or saved models).", "Hi @kunkun007 \r\nI am new to tensorflow. I am building tensorflow for arm64.\r\nI hope you cross compiled tensorflow source code.\r\nCan you please help how to build tensorflow for arm64.", "Hi, @jdduke  I build tensorflow lite as you have recommanded, but it still not work, so i change my model to .tflite, it can load the model successfully. Unfortunately, I use mtcnn, pnet must have fiexed input size, otherwise the tf_converter failed, I don't know how to inference the mtcnn then. Should I open a new issue for this problem? Thank you for your reply.", "Hi, @thotaram 1You should clone the tensorflow source code use git clone command  \r\n2./configure to set android sdk and ndk \r\n3modify tensorflow/lite BUILD File:\r\ncc_binary(\r\nname = \"libtensorflowLite.so\",\r\nlinkopts=[\r\n\"-shared\",\r\n\"-Wl,-soname=libtensorflowLite.so\",\r\n],\r\nlinkshared = 1,\r\ncopts = tflite_copts(),\r\ndeps = [\r\n\":framework\",\r\n\"//tensorflow/lite/kernels:builtin_ops\",\r\n\"//tensorflow//lite/delegates/flex:delegate\",\r\n],\r\n)\r\nmaybe in low tensorflow version, you should change the lite file /tensorflow/contrib/lite and no such line:\"//tensorflow//lite/delegates/flex:delegate\".\r\n4 use the bazal build command to build: my bazel command: bazel build //tensorflow/lite:libtensorflowLite.so --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\"\r\n\r\nYou should install bazel first and install a property version. ", "Hi @kunkun007 \r\nThank you for your inputs.\r\nI used the below link to cross build the tensorflow.\r\nhttps://github.com/xifengcun/tensorflow-aarch64-crossbuild\r\nConfiguration is done successfully, but while running the bazel command getting the below error.\r\n\r\nERROR: /home/aiiec/ARMNN/tensorflow_v1.13.1/tensorflow/tools/aarch64_compiler/BUILD:15:1: //tools/aarch64_compiler:gcc-linux-aarch64: no such attribute 'dynamic_runtime_libs' in 'cc_toolchain' rule\r\nERROR: /home/aiiec/ARMNN/tensorflow_v1.13.1/tensorflow/tools/aarch64_compiler/BUILD:15:1: //tools/aarch64_compiler:gcc-linux-aarch64: no such attribute 'static_runtime_libs' in 'cc_toolchain' rule\r\n\r\nI would like to know you have followed any web link to build tensorflow lite successfully.\r\nThanks inadvance.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28917\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28917\">No</a>\n"]}, {"number": 28916, "title": "Fix a segfault in StreamExecutorMemoryAllocator in a multi-GPU case", "body": "PiperOrigin-RevId: 249335436", "comments": []}, {"number": 28915, "title": "No module named 'tensorflow_hub'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.4\r\n- Installed using virtualenv? pip? conda?: pip3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nIt's on a Google Cloud Platform machine. \r\n\r\n**Describe the problem**\r\nIn jupyter-notebook python3 \r\nGot the following error\r\nModuleNotFoundError: No module named 'tensorflow_hub'\r\n\r\nWhen executed import tensorflow_hub as hub\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nMany commands that I can't remember, but basically I installed tensorflow_hub using pip3 install tensorflow-hub\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["It works for me\r\n`Collecting tensorflow-hub\r\n  Downloading https://files.pythonhosted.org/packages/10/5c/6f3698513cf1cd730a5ea66aec665d213adf9de59b34f362f270e0bd126f/tensorflow_hub-0.4.0-py2.py3-none-any.whl (75kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 162kB/s\r\nRequirement already satisfied: six>=1.10.0 in c:\\users\\win 10\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow-hub) (1.10.0)\r\nRequirement already satisfied: protobuf>=3.4.0 in c:\\users\\win 10\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow-hub) (3.6.1)\r\nRequirement already satisfied: numpy>=1.12.0 in c:\\users\\win 10\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow-hub) (1.16.3)\r\nRequirement already satisfied: setuptools in c:\\users\\win 10\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from protobuf>=3.4.0->tensorflow-hub) (39.0.1)\r\nInstalling collected packages: tensorflow-hub\r\nSuccessfully installed tensorflow-hub-0.4.0`\r\nHave you installed numpy ? What about protobuf ? ", "@pinnareet Can you please follow [these](https://www.tensorflow.org/hub/installation) steps to install tensorflow-hub. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I'm really sorry for the late reply. numpy and protobuf are installed. I also attached screenshots showing tensorflow-hub installed but failed to load in python3. \r\n<img width=\"567\" alt=\"Screenshot 2019-05-28 15 56 55\" src=\"https://user-images.githubusercontent.com/4829500/58517821-96017700-8161-11e9-9cde-3f09bec887f2.png\">\r\n<img width=\"1420\" alt=\"Screenshot 2019-05-28 15 57 07\" src=\"https://user-images.githubusercontent.com/4829500/58517823-96017700-8161-11e9-90cd-27db6abd228c.png\">\r\n<img width=\"1241\" alt=\"Screenshot 2019-05-28 15 58 41\" src=\"https://user-images.githubusercontent.com/4829500/58517824-96017700-8161-11e9-8171-0286e3d505f1.png\">\r\n\r\n\r\n", "@pinnareet This question better asked on [Tensorflow hub](https://github.com/tensorflow/hub/issues) repo. Thanks!", "> pip install https://files.pythonhosted.org/packages/10/5c/6f3698513cf1cd730a5ea66aec665d213adf9de59b34f362f270e0bd126f/tensorflow_hub-0.4.0-py2.py3-none-any.whl\r\n\r\n\r\nIt worked for me . Please try these"]}, {"number": 28914, "title": "Move the optimizer name scope from model.training to optimizer", "body": "PiperOrigin-RevId: 249328139", "comments": []}, {"number": 28913, "title": "Remove data dependencies so that these two tests can be built", "body": "PiperOrigin-RevId: 249313553", "comments": []}, {"number": 28912, "title": "Unexpected behaviour of tf.keras.Model.predict using tf.data.Dataset as input", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: _Yes_\r\n- OS Platform and Distribution: _MacOS X, Ubuntu 16.04_\r\n- Mobile device: _None_\r\n- TensorFlow installed from (source or binary): _from source_\r\n- TensorFlow version (use command below): _v1.13.1-0-g6612da8951_\r\n- Python version: _3.7.3 (MacOS) and 3.6.7 (Ubuntu)_\r\n- Bazel version: _0.20.0-homebrew (MacOS) and 0.20.0 (Ubuntu)_\r\n- GCC/Compiler version: _clang-1000.11.45.5 (MacOS) and 7.3.0 (Ubuntu)_\r\n- CUDA/cuDNN version: _None (MacOS) and 10.0 (Ubuntu)_\r\n- GPU model and memory: _None (MacOS) and GeForce GTX 1080 Ti (Ubuntu) 11178 MiB memory_\r\n\r\n**Describe the current behavior**\r\nCalling `model.predict(x, steps=N)` on a `tf.keras.Model` instance with two inputs fails with many errors:\r\n1. If the model is not compiled, I get a RuntimeError as follows:\r\n```\r\nRuntimeError: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.\r\n```\r\n2. If I do compile the model (even though this is not necessary, since `predict` should not compute any losses), I get a ValueError (see `ds1` below):\r\n```\r\nValueError: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [<tf.Tensor 'IteratorGetNext_2:0' shape=(?, 28, 28, 1) dtype=float32>]...\r\n```\r\n3. If I further provide a target in the dataset (see `ds2` below), the predict method works and produces an output of the correct shape.\r\n \r\n**Describe the expected behavior**\r\nCalling `model.predict(x, steps=N)` on both an uncompiled (1) and compiled (2)`tf.keras.Model` should produce a list of numpy arrays corresponding to the outputs of the model without any errors.\r\n\r\n**Code to reproduce the issue**\r\nThis test case is modified from #23702 to include multiple model inputs. The issue described there seems to be related, but not identical to this one. In particular, the fixes introduced in `tf-nightly-1.13.0-dev20190213`, that were mentioned, do not resolve the issue for this test case.\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nx_test = x_test.reshape((10000, 28, 28, 1)).astype(np.float32)\r\ny_test = tf.keras.utils.to_categorical(y=y_test)\r\n\r\nx1 = tf.keras.layers.Input((28, 28, 1), dtype=tf.float32)\r\nconv1 = tf.keras.layers.Conv2D(8, kernel_size=3, activation='relu')\r\nflat1 = tf.keras.layers.Flatten()\r\n\r\nx2 = tf.keras.layers.Input((28, 28, 1), dtype=tf.float32)\r\nconv2 = tf.keras.layers.Conv2D(8, kernel_size=3, activation='relu')\r\nflat2 = tf.keras.layers.Flatten()\r\n\r\nconcat = tf.keras.layers.Concatenate()\r\nproj = tf.keras.layers.Dense(10, activation='softmax')\r\n\r\nm = tf.keras.models.Model([x1, x2], proj(concat([flat1(conv1(x1)), flat2(conv2(x2))])))\r\nm.compile('adam', 'mse')\r\n\r\nds = tf.data.Dataset.from_tensor_slices(x_test)\r\nds1 = tf.data.Dataset.zip((ds, ds))\r\nds2 = tf.data.Dataset.zip((ds1, tf.data.Dataset.from_tensor_slices(y_test)))\r\noutput = m.predict(x=ds1.batch(batch_size=10), steps=1000, verbose=True)  # fails with ValueError\r\noutput = m.predict(x=ds2.batch(batch_size=10), steps=1000, verbose=True)  # works\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-e5c6dc2477e5> in <module>\r\n     23 ds = ds.batch(batch_size=10)\r\n     24 data = ds.make_one_shot_iterator()\r\n---> 25 output = m.predict(x=data, steps=1000, verbose=True)\r\n\r\n~/.virtualenvs/python3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\r\n   1094       # batch size.\r\n   1095       x, _, _ = self._standardize_user_data(\r\n-> 1096           x, check_steps=True, steps_name='steps', steps=steps)\r\n   1097\r\n   1098     if (self.run_eagerly or (isinstance(x, iterator_ops.EagerIterator) and\r\n\r\n~/.virtualenvs/python3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\r\n   2380         feed_input_shapes,\r\n   2381         check_batch_axis=False,  # Don't enforce the batch size.\r\n-> 2382         exception_prefix='input')\r\n   2383\r\n   2384     if y is not None:\r\n\r\n~/.virtualenvs/python3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n    321                        'Expected to see ' + str(len(names)) + ' array(s), '\r\n    322                        'but instead got the following list of ' +\r\n--> 323                        str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\r\n    324     elif len(names) > 1:\r\n    325       raise ValueError(\r\n\r\nValueError: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [<tf.Tensor 'IteratorGetNext:0' shape=(?, 28, 28, 1) dtype=float32>]...```", "comments": ["I am able to reproduce the issue reported here. ", "@jvishnuvardhan please assign this to someone familiar with Keras, thank you", "@atomberg This is intended behavior. `model m` has two inputs and 10 outputs. \r\nIn `ds1 = tf.data.Dataset.zip((ds, ds))`, dataset understands that first `ds` is `x_train` and second `ds` is `y_train`. However, your `model.predict` requires two elements as input (x_train) whereas you are providing a single element. In case of `ds2` you are correctly proving two elements as `x_train` and hence there is no error. Please [check the updated code](https://colab.sandbox.google.com/gist/jvishnuvardhan/720b7c6ae0ce81da1af47a18a8922faf/tf_28912_keras_dataset.ipynb) that runs without any issue.\r\n\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nx_test = x_test.reshape((10000, 28, 28, 1)).astype(np.float32)\r\ny_test = tf.keras.utils.to_categorical(y=y_test)\r\n\r\nx1 = tf.keras.layers.Input((28, 28, 1), dtype=tf.float32)\r\nconv1 = tf.keras.layers.Conv2D(8, kernel_size=3, activation='relu')\r\nflat1 = tf.keras.layers.Flatten()\r\n\r\nx2 = tf.keras.layers.Input((28, 28, 1), dtype=tf.float32)\r\nconv2 = tf.keras.layers.Conv2D(8, kernel_size=3, activation='relu')\r\nflat2 = tf.keras.layers.Flatten()\r\n\r\nconcat = tf.keras.layers.Concatenate()\r\nproj = tf.keras.layers.Dense(10, activation='softmax')\r\n\r\nm = tf.keras.models.Model([x1, x2], proj(concat([flat1(conv1(x1)), flat2(conv2(x2))])))\r\nm.compile('adam', 'mse')\r\n\r\nm2 = tf.keras.models.Model([x1], proj(concat([flat1(conv1(x1)), flat2(conv2(x1))])))\r\nm2.compile('adam', 'mse')\r\n\r\nds = tf.data.Dataset.from_tensor_slices(x_test)\r\nds1 = tf.data.Dataset.zip((ds, ds))\r\nds2 = tf.data.Dataset.zip((ds1, tf.data.Dataset.from_tensor_slices(y_test)))\r\noutput = m2.predict(x=ds1.batch(batch_size=10), steps=1000, verbose=True)  # also works\r\noutput = m.predict(x=ds2.batch(batch_size=10), steps=1000, verbose=True)  # works\r\n```\r\n\r\nI am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28912\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28912\">No</a>\n", "@jvishnuvardhan Could you confirm that the following two behaviours are indeed intended:\r\n1. An uncompiled model **cannot** be used in predict mode with a dataset as input.\r\n1. A tensor for the y_true placeholder **must** be provided in the dataset used in the predict function, even though its value is never read (like the `ds2` in the example above).\r\n\r\nIf the answer to any one of the above questions is no, please reopen the issue. Thanks!"]}, {"number": 28911, "title": "Update BUILD.bazel", "body": "Add a definition for linux_aarch64", "comments": ["It says \"Ubuntu Python3 PIP \u2014 Internal CI build failed\". Why is there such an error?", "An error not caused by your change, afaict. We'll retry, nothing that you have to do at the moment."]}, {"number": 28910, "title": "[Docs] Fix typo in Cropping2D example", "body": "", "comments": []}, {"number": 28909, "title": "Fix typo in Cropping2D example", "body": "", "comments": []}, {"number": 28908, "title": "3 cherrrypicks", "body": "", "comments": []}, {"number": 28907, "title": "3 more cherrypicks", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28907) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 28906, "title": "Set the forward-compat date to the future.", "body": "This way we pick up all changes that were only being delayed for\r\nforward-compatibility reasons.", "comments": []}, {"number": 28905, "title": "Update png_archive version to 1.6.37", "body": "Cherry-pick from 4be5655", "comments": []}, {"number": 28904, "title": "Update png_archive version to 1.6.37", "body": "Cherry-pick from 4be5655bd64ec0c0f7c0bab6d451563043c87e40", "comments": []}, {"number": 28903, "title": "Receiving InvalidArgument error when feeding inputs of uneven length", "body": "**System information**\r\n- I have written custom, albeit straightforward code using keras.Sequential.\r\n- Platform: Google Colab, CPU runtime, Python 3\r\n- TensorFlow installed using _pip install -q tensorflow==2.0.0-alpha0_\r\n\r\n**Current behavior**\r\nInvalidArgument error occurs.\r\n\r\n**Expected behavior**\r\nI should be able to train the network using inputs of uneven length.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n!pip install tensorflow==2.0.0-alpha0 \r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.optimizers import Adam\r\nimport numpy as np\r\nimport pickle\r\n\r\nmodel = keras.Sequential([\r\n    layers.Embedding(input_dim=48191, output_dim=300),\r\n    layers.LSTM(300),\r\n    layers.Dense(1, activation='sigmoid')\r\n])\r\n\r\nmodel.compile(\r\n    loss='binary_crossentropy', \r\n    optimizer=Adam(), \r\n    metrics=['accuracy']\r\n    )\r\n\r\ndataset = pickle.load(open(\"dataset.pkl\", \"rb\"))\r\ndata_generator = ((np.asarray([x]), np.asarray([y])) for x, y in dataset)\r\nmodel.fit_generator(data_generator, steps_per_epoch=len(dataset))\r\n```\r\n**Error message**\r\n```\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Tried to stack elements of an empty list with non-fully-defined element_shape: [?,300]\r\n\t [[{{node unified_lstm_2/TensorArrayV2Stack/TensorListStack}}]] [Op:__inference_keras_scratch_graph_6215]\r\n```\r\n\r\n[The dataset is required to reproduce the issue.](https://github.com/tensorflow/tensorflow/files/3203395/dataset.zip)", "comments": ["I could reproduce the issue reported here. ", "@drunkinlove The `dataset` has an empty list at index=63. \r\nIndex=63, dataset=([], 1.0)\r\nindex=62 , dataset=([16564,\r\n  12869,\r\n  48036,\r\n  23694,\r\n  41333,\r\n  18458,\r\n  9110,\r\n  41136,\r\n  15388,\r\n  44860,\r\n  18979,\r\n  8556,\r\n  16411,\r\n  10861,\r\n  13137,\r\n  46889,\r\n  19663,\r\n  8556,\r\n  46100,\r\n  25021,\r\n  16411,\r\n  7388,\r\n  12736,\r\n  6535,\r\n  7394,\r\n  19571,\r\n  3623,\r\n  20091,\r\n  44915,\r\n  24392,\r\n  9080,\r\n  41136,\r\n  6535,\r\n  44136,\r\n  23273,\r\n  32950],\r\n 1.0). \r\n\r\nas there is an empty list, it is failing when index=63. Thanks!", "I think it was resolved. Closing due to lack of recent activity. Please open new ticket if you see similar issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28903\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28903\">No</a>\n"]}]