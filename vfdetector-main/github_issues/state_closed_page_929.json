[{"number": 25578, "title": "CancelledError: Loop execution was cancelled", "body": "I was doing some regression analysis using google colabs and until 2 days back everything was working fine but suddenly I'm getting following error. I din't know why the training is getting stopped in the middle. On stackoverflow I came to know that tensorflow was updated from 1.12 to 1.13.0rc0. The issue started the moment the tensorflow was updated. \r\n\r\nHere is the link to my [notebook](https://colab.research.google.com/drive/13LuWmVIjza-lfxuOyfWAEf_pJjtvAMEh) on colabs.\r\n\r\nFollowing is the output from training:\r\n\r\n```\r\n SA1 \r\n\r\n\r\nFold:  1\r\n\r\n\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From <ipython-input-8-001ee577e90b>:225: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\r\nWARNING:tensorflow:From <ipython-input-9-1bd488b4dd9a>:239: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nWARNING:tensorflow:From <ipython-input-9-1bd488b4dd9a>:348: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.dense instead.\r\nWARNING:tensorflow:From <ipython-input-9-1bd488b4dd9a>:161: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nINFO:tensorflow:Restoring parameters from data/vars/SA1/feeder.cpt\r\nTraining the model with He ...\r\n0 MAPE: 11442.98324584961\r\n100 MAPE: 1032.3982238769531\r\n200 MAPE: 460.38122177124023\r\n300 MAPE: 190.61466455459595\r\n400 MAPE: 96.63236141204834\r\n500 MAPE: 59.168386459350586\r\n600 MAPE: 41.031333804130554\r\n700 MAPE: 29.591849446296692\r\n800 MAPE: 22.207358479499817\r\n900 MAPE: 19.93158310651779\r\n1000 MAPE: 17.7777960896492\r\n1100 MAPE: 16.45900011062622\r\n1200 MAPE: 16.250891983509064\r\n1300 MAPE: 14.98890072107315\r\n1400 MAPE: 14.976367354393005\r\n1500 MAPE: 14.184540510177612\r\n1600 MAPE: 13.81748765707016\r\n1700 MAPE: 14.370763301849365\r\n1800 MAPE: 13.738515973091125\r\n1900 MAPE: 13.814045488834381\r\n2000 MAPE: 13.300719857215881\r\n2100 MAPE: 13.28342854976654\r\n2200 MAPE: 13.087129592895508\r\n2300 MAPE: 13.045386970043182\r\n2400 MAPE: 12.72062063217163\r\n2500 MAPE: 12.522627413272858\r\n2600 MAPE: 12.2012197971344\r\n2700 MAPE: 12.064149230718613\r\n2800 MAPE: 11.975772678852081\r\n2900 MAPE: 11.860555410385132\r\n3000 MAPE: 11.887001246213913\r\n3100 MAPE: 11.541987955570221\r\n3200 MAPE: 11.460774391889572\r\n3300 MAPE: 11.266357451677322\r\n3400 MAPE: 11.597257852554321\r\n3500 MAPE: 11.223434656858444\r\n3600 MAPE: 11.532413214445114\r\n3700 MAPE: 10.872676968574524\r\n3800 MAPE: 11.064480245113373\r\n3900 MAPE: 11.555147916078568\r\n4000 MAPE: 11.46700382232666\r\n4100 MAPE: 10.984917730093002\r\n4200 MAPE: 10.663384199142456\r\n4300 MAPE: 11.119940131902695\r\n4400 MAPE: 10.703090578317642\r\n4500 MAPE: 10.289034992456436\r\n4600 MAPE: 9.894699603319168\r\n\r\n---------------------------------------------------------------------------\r\n\r\nCancelledError                            Traceback (most recent call last)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1333     try:\r\n-> 1334       return fn(*args)\r\n   1335     except errors.OpError as e:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1318       return self._call_tf_sessionrun(\r\n-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1320 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1406         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1407         run_metadata)\r\n   1408 \r\n\r\nCancelledError: Loop execution was cancelled.\r\n\t [[{{node while/LoopCond}}]]\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nCancelledError                            Traceback (most recent call last)\r\n\r\n<ipython-input-11-86884b56c50d> in <module>()\r\n     65                 while True:\r\n     66                   try:\r\n---> 67                     _, error = sess.run([train_model.train_op, train_model.mape])\r\n     68                   except tf.errors.OutOfRangeError:\r\n     69                     break\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    927     try:\r\n    928       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 929                          run_metadata_ptr)\r\n    930       if run_metadata:\r\n    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1151       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1152                              feed_dict_tensor, options, run_metadata)\r\n   1153     else:\r\n   1154       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1326     if handle is None:\r\n   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1328                            run_metadata)\r\n   1329     else:\r\n   1330       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1346           pass\r\n   1347       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1348       raise type(e)(node_def, op, message)\r\n   1349 \r\n   1350   def _extend_graph(self):\r\n\r\nCancelledError: Loop execution was cancelled.\r\n\t [[node while/LoopCond (defined at <ipython-input-9-1bd488b4dd9a>:383) ]]\r\n\r\nCaused by op 'while/LoopCond', defined at:\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-11-86884b56c50d>\", line 59, in <module>\r\n    asgd_decay=None)\r\n  File \"<ipython-input-9-1bd488b4dd9a>\", line 70, in __init__\r\n    inp.y_feature, inp.x_feature[:, -1, 0], init)\r\n  File \"<ipython-input-9-1bd488b4dd9a>\", line 383, in decoder\r\n    _, _, _, targets_ta, outputs_ta = tf.while_loop(cond_fn, loop_fn, loop_init)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3556, in while_loop\r\n    return_same_structure)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3087, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3008, in _BuildLoop\r\n    self._pivot = loop_cond(c, name=\"LoopCond\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_control_flow_ops.py\", line 339, in loop_cond\r\n    \"LoopCond\", input=input, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nCancelledError (see above for traceback): Loop execution was cancelled.\r\n\t [[node while/LoopCond (defined at <ipython-input-9-1bd488b4dd9a>:383) ]]\r\n\r\n\r\n```", "comments": ["@skye this looks like a control flow problem, it's on 1.13 -- we should check whether this is at head as well.\r\n\r\n@aselle FYI. Potential blocker for 1.13.", "@mrry @jsimsa do you know if anything went in between 1.12 and now that would affect cancellation? Or who would know better about this? (http://go/gh/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/control_flow_ops.cc#L613 is newly triggering apparently)", "The cancellation check itself was added after 1.12: https://github.com/tensorflow/tensorflow/commit/fc5392dedec126f988788a597edb55021fb07b60\r\n\r\n", "Ok that's probably why it started showing up :) Is it correct for this to be an error?", "Ah, according to the discussion at https://github.com/tensorflow/tensorflow/pull/23811, the error behavior is correct. I'm confused why this is firing in your colab though, as I don't see any timeouts set.\r\n\r\nI think this is the code snippet where the error is occurring:\r\n```python\r\nwith tf.Session() as sess:\r\n      os.makedirs(saver_path)\r\n      inp = VarFeeder.read_vars(data_dir)\r\n      pipe_train = InputPipe(inp, mode=ModelMode.TRAIN, k=k, batch_size=batch_size, \r\n                       n_epoch=hparams.epochs, verbose=False, \r\n                       train_window=train_window, predict_window=predict_window,\r\n                       rand_seed=hparams.seed)\r\n      train_model = Model(pipe_train, hparams, is_train=True, seed=hparams.seed, init=init, graph_prefix=None, \r\n                      asgd_decay=None)\r\n      sess.run(tf.global_variables_initializer())\r\n      inp.restore(sess)\r\n      print(\"Training the model with\",init_name,\"...\")\r\n      for ep in range(hparams.epochs):\r\n        train_model.inp.init_iterator(sess)\r\n        while True:\r\n          try:\r\n            _, error = sess.run([train_model.train_op, train_model.mape])\r\n          except tf.errors.OutOfRangeError:\r\n            break\r\n        if ep%100 == 0:\r\n          print(ep, \"MAPE:\", error*100)\r\n```\r\n\r\n@mrry do you know what besides a timeout might be triggering this cancellation? @anshkumar do you happen to know where in your code you're creating a tf.while_loop? (It might be in internal library code, I didn't look through the whole colab.)", "@skye the `tf.while_loop` is right here: https://colab.research.google.com/drive/13LuWmVIjza-lfxuOyfWAEf_pJjtvAMEh#scrollTo=TKDG28iWVtmr&line=386&uniqifier=1\r\n\r\nI had done a cursory search for `timeout` in that notebook and didn't see it; is it possibly related to execution restoring from saved checkpoints? (This is a wild guess, could be completely off.)", "Oh duh, thanks :)\r\nAre you able to share \"./drive/My Drive/data.zip\" so I can try to copy your colab and run it myself? You can email me if that's easiest.", "@skye It might not be a timeout. An asynchronous `Session.close()` will set the cancelled bit. Also, any op failure will trigger this code to set the cancelled bit: https://github.com/tensorflow/tensorflow/blob/5c909541ee86bb1130f11a5c3b6a51ee8c7e12c9/tensorflow/core/common_runtime/executor.cc#L2227 (but that seems to be happening after the executor's [status is set](https://github.com/tensorflow/tensorflow/blob/5c909541ee86bb1130f11a5c3b6a51ee8c7e12c9/tensorflow/core/common_runtime/executor.cc#L2215), so I wouldn't expect to see a `CancelledError` bubble up to the user).", "@skye [data.zip](https://drive.google.com/open?id=1WeloJjk-9nwEIrtxCcAkJi-QIiilH8Kq)\r\n[temperature data](https://drive.google.com/open?id=1ixIwfLEP0UXrhxXi3edBlJDH3CzbIXX6)", "What does this line even mean? Especially with NOTHING after the colon in \"Instructions for updating:\"?\r\n I'm getting it too in one of the standard notebooks that I'm trying in Colab.\r\n<pre>\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n</pre>", "@alxfed tf.colocate_with is going away in TF 2.0, which is why it prints the deprecation warning. It's trying to say that you don't need to worry about updating because it should be handled automatically, but I agree this could be more clear. I don't believe that's related to this issue however.\r\n\r\n@anshkumar as an update, I was able to repro the problem in a copy of your notebook, but unfortunately have been unable to repro the problem outside of the colab environment, which is difficult to debug. I'm still working on figuring out what's going on though.", "We've reverted the change adding the \"Loop execution was cancelled\" error in 1.13: https://github.com/tensorflow/tensorflow/commit/7f404e98923a78365aef63e14b8e2bc6f077c3c3\r\n\r\nI've verified internally that @anshkumar colab runs successfully again. This should mean there are no regressions in 1.13, although we'll still have the bug that error check was attempting to fix. I'll continue trying to debug this so we can commit a real fix on master.", "@skye are the changes available in tensorflow 1.13.0-rc1 ?", "@gunan have we released an RC with 7f404e9?", "rc2 is released and should have this change.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25578\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25578\">No</a>\n"]}, {"number": 25577, "title": "Running code with Tensorflow GPU", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): 1.12.0\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: cuda=10.0/cudnn=7.4.2\r\n- GPU model and memory: RTX 2080\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI installed the everything and ran the tests for cuda and cudnn that were said from and they both passed normally. Running small tests with tensorflow also worked, i.e. a linear regression model. However, I was trying to run a file using keras and doing a more complicated [model](https://github.com/tensorflow/models/tree/master/samples/outreach/blogs/segmentation_blogpost) and it threw errors when it started running an individual epoch. I also just ran the code with only a small portion of the images.\r\n\r\n```\r\nI tensorflow/stream_executor/platform/default/dso_loader.cc:161] successfully opened CUDA library libcudnn.so.7 locally\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:482] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:482] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n\r\n...\r\n...\r\n\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d/Conv2D}}]]\r\n\t [[conv2d_transpose_2/Shape/_1171]]\r\n```\r\n\r\nRunning the code normally with tensorflow works fine, only the gpu version is failing. I tried both building it from source and using tf-nightly-gpu.", "comments": ["duplicate #24828. Please take a look at the [workaround](https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-457425190) used by another user.", "Already tried that. Cudnn was installed using the instructions [here](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#install-linux). \r\n\r\nUsing conda list cudnn did not list any packages. The cudnn version was also the latest one available, compatible with cuda 10.0. I did try to use both methods since I first tried the debian file, which gave the same error. Later did the unzip to cuda location, still no difference and mnistCUDNN still passed, is there a possibility of conflict?", "According to these [instructions](https://www.tensorflow.org/install/gpu) tensorflow is compatible with cuda 9.0 and, if i'm not mistaken, you have installed version 10.0. Maybe you should try to downgrade, i had a similar issue too", "I tried installing cuda 9.0 first, but it also crashed with a different error, saying some compatibility issues. I then read from [here](https://devtalk.nvidia.com/default/topic/1042338/does-it-work-rtx-2080-cuda-9-0-ubuntu-16-04-/) that it doesn't support turing architecture.\r\n\r\nOnly the default tensorflow on pip supports cuda 9.0, but the nightly build supports cuda 10.0. So it's to either build it from source or use the unstable nightly.", "@HaoboZ Can you please try installing TF 1.13.0-rc0? It supports cuda 10.0\r\nAlso are you able to execute your code in Google Colab successfully using GPU?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@HaoboZ Did you solve this problem?", "Nope, changed the OS to Windows and it worked perfectly fine. That means it's probably not a problem with the hardware. ", "Solution: Restart the Kernel of Jupyter Notebook. Worked for me every time."]}, {"number": 25576, "title": "error while loading shared libraries: libprotobuf.so.17: cannot open shared object file: No such file or directory", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): [source](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile)\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): Not used Bazel. Built static library\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No GPU. RAM-8GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed the exact steps [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile)\r\n\r\nUpon running benchmark file,\r\n`tensorflow/contrib/makefile/gen/bin/benchmark \\\r\n --graph=$HOME/graphs/inception/tensorflow_inception_graph.pb`\r\n\r\nI get the following error,\r\n> error while loading shared libraries: libprotobuf.so.17: cannot open shared object file: No such file or directory\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["somehow managed to work by copying `tensorflow/tensorflow/contrib/makefile/gen/protobuf/lib/libprotobuf.so.17` to `/usr/local/lib` and running `sudo ldconfig`\r\nright/recommended approach?", "\"tensorflow/tensorflow/contrib/makefile/gen/protobuf/lib/libprotobuf.so.17 to /usr/local/lib and running sudo ldconfig\"\r\n\r\nSorry to say, but this approach is also not working ", "@varun046 Which TF version you have installed. If it is not TF1.12, then open a new issue for your environment and fill issue template accordingly. Thanks!", "makefile is not a supported build strategy.\r\nThis was caused by incomplete configuration of the makefiles, as they were there, kept surviving with little to no support.\r\nClosing this issue as they are removed now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25576\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25576\">No</a>\n"]}, {"number": 25575, "title": "TFLite Converter not able to convert tf.keras model in TensorFlow 2.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): 2.0.0-dev20190206\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWith TF 2.0 preview, the converter is not able to convert a tf.keras model to tflite either through python or command line\r\n\r\n**Describe the expected behavior**\r\nThe TFlite converter converts the tf.keras model to tflite model.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nPlease use this code for your testing:\r\nhttps://github.com/margaretmz/test-tensorflow-2.0/blob/master/test-tflite-converter/test_tf2.0_keras_to_tflite.py\r\nIt contains code that you can use for testing - the conversion works for tf 1.11.0 and tf.12.0 but not for tf 2.0\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["In tf2.0, there is no direct function as from_keras_model, you have to create a concrete function. \r\nIt works, but on conversion to tflite, other issues are actually coming like some key error for batch normalization as it is falling back to v1 for batch normalization, and the resource nodes for all the operators are getting added up along with this.. I am using TF2.0 alpha release", "See https://www.tensorflow.org/lite/r2/convert/concrete_function to know more.\r\nFeel free to reopen issue if running into issues. Thanks!"]}, {"number": 25574, "title": "Update docstrings in upgrade_schema.py", "body": "Update the missed RuntimeError and ValueError in upgrade_schema.py", "comments": ["Adding gargn@ for review.", "Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@jianlijianli @gargn please help to review this documentation changes"]}, {"number": 25573, "title": "Docstring updated for missed 'Raises\"", "body": "", "comments": []}, {"number": 25572, "title": "docstring updated", "body": "`Raises:\r\n      ValueError: In case of invalid arguments.`\r\nThis is already available in line 205", "comments": []}, {"number": 25571, "title": "TF Ops document updated(unreachable link removed)", "body": "Document fix", "comments": []}, {"number": 25570, "title": "Feature Request: tf.keras.layers.Conv2D supports channels_first when there are no GPU", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.0-rc0,  b'v1.13.0-rc0-0-ga8e5c41c5b'\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAlthough the API document explicitly says that data_format can be either \"channels_first\" and \"channel_last\", the implementation of tf.keras.layers.Conv2D only supports data_format=\"channels_last\" currently. If I choose \"channel_first\", it raises \"InvalidArgumentError: Conv2DCustomBackpropFilterOp only supports NHWC.\"\r\n\r\nCf: The API doc (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) says:\r\n`data_format: A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width). It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be \"channels_last\".\r\n`\r\nExample Code:\r\n```\r\nimport numpy as np\r\nimport tensorflow.keras as keras\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Conv2D(32, 8, \r\n        input_shape=(3, 128, 128), data_format=\"channels_first\"))\r\nmodel.add(keras.layers.Flatten())\r\nmodel.add(keras.layers.Dense(1))\r\nmodel.compile(optimizer='adam', loss='mse')\r\nx = np.ones((64, 3, 128, 128), dtype=np.float16)\r\ny = np.ones((64, 1), dtype=np.float16)\r\nmodel.fit(x, y)\r\n```\r\nCurrent output of the code:\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-3-580fd1a8364b> in <module>()\r\n      9 x = np.ones((64, 3, 128, 128), dtype=np.float16)\r\n     10 y = np.ones((64, 1), dtype=np.float16)\r\n---> 11 model.fit(x, y)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    878           initial_epoch=initial_epoch,\r\n    879           steps_per_epoch=steps_per_epoch,\r\n--> 880           validation_steps=validation_steps)\r\n    881 \r\n    882   def evaluate(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\r\n    327 \r\n    328         # Get outputs.\r\n--> 329         batch_outs = f(ins_batch)\r\n    330         if not isinstance(batch_outs, list):\r\n    331           batch_outs = [batch_outs]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)\r\n   3071         feed_symbols != self._feed_symbols or self.fetches != self._fetches or\r\n   3072         session != self._session):\r\n-> 3073       self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)\r\n   3074 \r\n   3075     fetched = self._callable_fn(*array_vals,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in _make_callable(self, feed_arrays, feed_symbols, symbol_vals, session)\r\n   3017       callable_opts.run_options.CopyFrom(self.run_options)\r\n   3018     # Create callable.\r\n-> 3019     callable_fn = session._make_callable_from_options(callable_opts)\r\n   3020     # Cache parameters corresponding to the generated callable, so that\r\n   3021     # we can detect future mismatches and refresh the callable.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _make_callable_from_options(self, callable_options)\r\n   1469     \"\"\"\r\n   1470     self._extend_graph()\r\n-> 1471     return BaseSession._Callable(self, callable_options)\r\n   1472 \r\n   1473 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in __init__(self, session, callable_options)\r\n   1423         with errors.raise_exception_on_not_ok_status() as status:\r\n   1424           self._handle = tf_session.TF_SessionMakeCallable(\r\n-> 1425               session._session, options_ptr, status)\r\n   1426       finally:\r\n   1427         tf_session.TF_DeleteBuffer(options_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    526             None, None,\r\n    527             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 528             c_api.TF_GetCode(self.status.status))\r\n    529     # Delete the underlying status object from memory otherwise it stays alive\r\n    530     # as there is a reference to status from this from the traceback due to\r\n\r\nInvalidArgumentError: Conv2DCustomBackpropFilterOp only supports NHWC.\r\n\t [[{{node training_2/Adam/gradients/conv2d_2/Conv2D_grad/Conv2DBackpropFilter}}]]\r\n```\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\ndata_format=\"channel_first\" is already widely used in lots of existing keras programs. This feature is somehow promised by Keras.\r\n\r\n\r\n**Any Other info.**\r\n", "comments": ["Could you take a try with MKL? https://www.tensorflow.org/guide/performance/overview#tuning_mkl_for_the_best_performance", "@facaiy This link is useful. Thanks!", "Closing this issue since workaround is available. Feel free to reopen if have any follow up questions. Thanks!"]}, {"number": 25569, "title": "When using MirroredStrategy in keras, custom callbacks receive an incorrect model instance ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nA keras custom callback typically has an access to the \"model\" object. There are many use cases where the access to model in a callback is required. \r\n\r\nWhen MirroredStrategy is used there is still access to \"model\" object however its type is not `tf.keras.models.Model` rather it is `DistributedModel`\r\n\r\nWhen training such a model tensorflow generates following warnings -\r\n\r\n```\r\nWARNING:tensorflow:Your input callback is not one of the predefined Callbacks that supports DistributionStrategy. You might encounter an error if you access one of the model's attributes as part of the callback since these attributes are not set. You can access each of the individual distributed models using the `_grouped_model` attribute of your original model.\r\n```\r\n\r\nInside the custom callback in this scenario self.model.layers is set to [] i.e. length is zero.\r\n\r\n**Describe the expected behavior**\r\n\r\nThere are couple of issues with the existing behavior -:\r\n\r\na) Ideally one should not be required to change the code in the callbacks. But that is an ideal situation; it is understandable to some extent that in cases such as MirroredStrategy a custom callback may require extra conditions.\r\n\r\nb) However, what is strange or rather would be considered  ill advised is the warning generated by tensorflow i.e. \"You can access each of the individual distributed models using the `_grouped_model` attribute of your original model\"\r\n\r\nThe warning seems to suggest that one should try to access an `undocumented` and `private` property of an object. \r\n\r\nAlso there is no `_grouped_model` attribute on self.model anyways; the one that I see is `_original_model`\r\n\r\nThat said, the expected behavior is to document if a custom callback is to be designed considering some of the caveats when using MirroredStrategy and provide a public attribute and/or method to have access to the original model.\r\n\r\n\r\n**Code to reproduce the issue**\r\n\r\nHere is an example created to reproduce this behavior (with the workaround) -\r\n\r\nhttps://github.com/ksachdeva/tensorflow-bugs/tree/mirroredstrategy-model-access-in-callback\r\n", "comments": ["Same question.", "Same Question\r\n", "ksachdeva@ we have a new scope API that you can use that might help with this issue. You instantiate a strategy and open a scope. Within this scope you define and compile your Keras model. Here is an example of its usage: https://www.tensorflow.org/tutorials/distribute/keras#keras_api\r\n\r\nPlease reopen if you still run into issues with a custom callback model. Thanks!"]}, {"number": 25568, "title": "Bug in CrossShardOptimizer for Windows running with TPU", "body": "**System information**\r\n\r\n* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows server 2012\r\n* Tensorflow installed from (source or binary): pip\r\n* Tensorflow version (use command below): tensorflow-gpu 1.12.0 \r\n* Python version: 3.6.6\r\n* CUDA/cuDNN version: 9.0  V9.0.176\r\n* GPU model and memory: Tesla Volta V100\r\n\r\nUsing tensorflow 1.12.0 (also tried with 1.10.0 on a windows OS running a Tesla V100 TPU, I was not able to optimize using the CrossShardOptimizer. I expected to define the optimization function so I could optimize a network (same as using an adam optimizer). However, when I wrap an adam optimizer in a crossshardoptimizer, I get an error when I call minimize or apply gradients.\r\n\r\nThe following is very simple code that illustrates the bug.\r\n\r\nx = tf.placeholder(tf.float32, shape=(None, 32))\r\ny = tf.layers.dense(x, 5)\r\nyhat = tf.ones((20, 5))\r\nloss = tf.reduce_mean(tf.square(y - yhat))\r\n\r\nopt = tf.contrib.tpu.CrossShardOptimizer(tf.train.AdamOptimizer()).minimize(loss)\r\n\r\nBellow is the error message ( I tried to debug it myself simulating the Linux code, and that explanation is below).\r\n\r\nAttributeError Traceback (most recent call last)\r\nin \r\n4 loss = tf.reduce_mean(tf.square(y - yhat))\r\n5\r\n----> 6 opt = tf.contrib.tpu.CrossShardOptimizer(tf.train.AdamOptimizer()).minimize(loss)\r\n\r\nD:\\Anaconda3\\envs\\gq\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n408\r\n409 return self.apply_gradients(grads_and_vars, global_step=global_step,\r\n--> 410 name=name)\r\n411\r\n412 def compute_gradients(self, loss, var_list=None,\r\n\r\nD:\\Anaconda3\\envs\\gq\\lib\\site-packages\\tensorflow\\contrib\\tpu\\python\\tpu\\tpu_optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)\r\n167 else:\r\n168 with ops.colocate_with(grad):\r\n--> 169 summed_grads_and_vars.append((tpu_ops.cross_replica_sum(\r\n170 grad, self._group_assignment), var))\r\n171 return self._opt.apply_gradients(summed_grads_and_vars, global_step, name)\r\n\r\nAttributeError: module 'tensorflow.contrib.tpu.python.ops.tpu_ops' has no attribute 'cross_replica_sum'\r\n\r\nTrying to build a workaround, within tpu_ops.py I found that most of the code was only defined for linux OS, so I tried to just define the functions I needed and run the imports I needed, which I believed to be:\r\n\r\nfrom tensorflow.contrib.tpu.ops import gen_tpu_ops\r\nfrom tensorflow.contrib.tpu.ops.gen_tpu_ops import *\r\n\r\n```\r\nfrom tensorflow.contrib.util import loader\r\nfrom tensorflow.python.platform import resource_loader\r\n\r\ndef _create_default_group_assignment():\r\n    num_shards = tpu_function.get_tpu_context().number_of_shards\r\n    if num_shards is None:\r\n        logging.warning(\r\n            \"cross_replica_sum should be used within a tpu_shard_context, but \"\r\n            \"got unset number_of_shards. Assuming 1.\")\r\n        num_shards = 1\r\n    group_assignment = [list(range(num_shards))]\r\n    return group_assignment\r\n\r\ndef cross_replica_sum(x, group_assignment=None, name=None):\r\n    \"\"\"Sum the input tensor across replicas according to group_assignment.\r\n\r\n    Args:\r\n    x: The local tensor to the sum.\r\n    group_assignment: Optional 2d int32 lists with shape [num_groups,\r\n        num_replicas_per_group]. `group_assignment[i]` represents the replica\r\n        ids in the ith subgroup.\r\n    name: Optional op name.\r\n\r\n    Returns:\r\n    A `Tensor` which is summed across replicas.\r\n    \"\"\"\r\n    if group_assignment is None:\r\n        group_assignment = _create_default_group_assignment()\r\n\r\n    return gen_tpu_ops.cross_replica_sum(x, group_assignment, name=name)\r\n```\r\nHowever, I still received an error message that made me believe the windows version was not built correctly in the binary files:\r\n\r\n> > > x = tf.placeholder(tf.float32, shape=(None, 32))\r\n> > > y = tf.layers.dense(x, 5)\r\n> > > yhat = tf.ones((20, 5))\r\n> > > loss = tf.reduce_mean(tf.square(y - yhat))\r\n> > > opt = tf.contrib.tpu.CrossShardOptimizer(tf.train.AdamOptimizer()).minimize(loss)\r\n> > > WARNING:tensorflow:CrossShardOptimizer should be used within a tpu_shard_context, but got unset number_of_shards. Assuming 1.\r\n> > > WARNING:tensorflow:cross_replica_sum should be used within a tpu_shard_context, but got unset number_of_shards. Assuming 1.\r\n> > > Traceback (most recent call last):\r\n> > > File \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1628, in _create_c_op\r\n> > > c_op = c_api.TF_FinishOperation(op_desc)\r\n> > > tensorflow.python.framework.errors_impl.InvalidArgumentError: Op type not registered 'CrossReplicaSum' in binary running on . Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'CrossReplicaSum'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"\", line 1, in \r\nFile \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 410, in minimize\r\nname=name)\r\nFile \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\contrib\\tpu\\python\\tpu\\tpu_optimizer.py\", line 170, in apply_gradients\r\ngrad, self._group_assignment), var))\r\nFile \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\contrib\\tpu\\python\\ops\\tpu_ops.py\", line 417, in cross_replica_sum\r\nreturn gen_tpu_ops.cross_replica_sum(x, group_assignment, name=name)\r\nFile \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\contrib\\tpu\\ops\\gen_tpu_ops.py\", line 322, in cross_replica_sum\r\nname=name)\r\nFile \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\nop_def=op_def)\r\nFile \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\r\nreturn func(*args, **kwargs)\r\nFile \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\r\nop_def=op_def)\r\nFile \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1792, in **init**\r\ncontrol_input_ops)\r\nFile \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1631, in _create_c_op\r\nraise ValueError(str(e))\r\nValueError: Op type not registered 'CrossReplicaSum' in binary running on . Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'CrossReplicaSum'\r\n\r\n> > >\r\n\r\nAlso to note, rather than calling minimize, I also tried compute gradients, apply gradients, and that also crashed similarly.\r\n\r\nI hope that the tensorflow team can recommend a work around and/or work this fix into an upcoming version.\r\nThanks", "comments": ["Hi @leedtan We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25568\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25568\">No</a>\n"]}, {"number": 25567, "title": "Bug in CrossShardOptimizer for Windows running with TPU", "body": "Using tensorflow 1.12.0 (also tried with 1.10.0 on a windows 7 OS running a Tesla V100 TPU, I was not able to optimize using the CrossShardOptimizer.\r\n\r\nThe following is very simple code that illustrates the bug. \r\n\r\nx = tf.placeholder(tf.float32, shape=(None, 32))\r\ny = tf.layers.dense(x, 5)\r\nyhat = tf.ones((20, 5))\r\nloss = tf.reduce_mean(tf.square(y - yhat))\r\n\r\nopt = tf.contrib.tpu.CrossShardOptimizer(tf.train.AdamOptimizer()).minimize(loss)\r\n\r\n\r\nBellow is the error message ( I tried to debug it myself simulating the Linux code, and that explanation is below).\r\n\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-18-c8e9584279b0> in <module>\r\n      4 loss = tf.reduce_mean(tf.square(y - yhat))\r\n      5 \r\n----> 6 opt = tf.contrib.tpu.CrossShardOptimizer(tf.train.AdamOptimizer()).minimize(loss)\r\n\r\nD:\\Anaconda3\\envs\\gq\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n    408 \r\n    409     return self.apply_gradients(grads_and_vars, global_step=global_step,\r\n--> 410                                 name=name)\r\n    411 \r\n    412   def compute_gradients(self, loss, var_list=None,\r\n\r\nD:\\Anaconda3\\envs\\gq\\lib\\site-packages\\tensorflow\\contrib\\tpu\\python\\tpu\\tpu_optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)\r\n    167       else:\r\n    168         with ops.colocate_with(grad):\r\n--> 169           summed_grads_and_vars.append((tpu_ops.cross_replica_sum(\r\n    170               grad, self._group_assignment), var))\r\n    171     return self._opt.apply_gradients(summed_grads_and_vars, global_step, name)\r\n\r\nAttributeError: module 'tensorflow.contrib.tpu.python.ops.tpu_ops' has no attribute 'cross_replica_sum'\r\n\r\n\r\n\r\n\r\nTrying to build a workaround, within tpu_ops.py I found that most of the code was only defined for linux OS, so I tried to just define the functions I needed and run the imports I needed, which I believed to be:\r\n\r\n\r\nfrom tensorflow.contrib.tpu.ops import gen_tpu_ops\r\n    from tensorflow.contrib.tpu.ops.gen_tpu_ops import *\r\n\r\n    from tensorflow.contrib.util import loader\r\n    from tensorflow.python.platform import resource_loader\r\n\r\n    def _create_default_group_assignment():\r\n        num_shards = tpu_function.get_tpu_context().number_of_shards\r\n        if num_shards is None:\r\n            logging.warning(\r\n                \"cross_replica_sum should be used within a tpu_shard_context, but \"\r\n                \"got unset number_of_shards. Assuming 1.\")\r\n            num_shards = 1\r\n        group_assignment = [list(range(num_shards))]\r\n        return group_assignment\r\n\r\n    def cross_replica_sum(x, group_assignment=None, name=None):\r\n        \"\"\"Sum the input tensor across replicas according to group_assignment.\r\n\r\n        Args:\r\n        x: The local tensor to the sum.\r\n        group_assignment: Optional 2d int32 lists with shape [num_groups,\r\n            num_replicas_per_group]. `group_assignment[i]` represents the replica\r\n            ids in the ith subgroup.\r\n        name: Optional op name.\r\n\r\n        Returns:\r\n        A `Tensor` which is summed across replicas.\r\n        \"\"\"\r\n        if group_assignment is None:\r\n            group_assignment = _create_default_group_assignment()\r\n\r\n        return gen_tpu_ops.cross_replica_sum(x, group_assignment, name=name)\r\n\r\n\r\nHowever, I still received an error message that made me believe the windows version was not built correctly in the binary files:\r\n\r\n>>> x = tf.placeholder(tf.float32, shape=(None, 32))\r\n>>> y = tf.layers.dense(x, 5)\r\n>>> yhat = tf.ones((20, 5))\r\n>>> loss = tf.reduce_mean(tf.square(y - yhat))\r\n>>>\r\n>>> opt = tf.contrib.tpu.CrossShardOptimizer(tf.train.AdamOptimizer()).minimize(loss)\r\nWARNING:tensorflow:CrossShardOptimizer should be used within a tpu_shard_context, but got unset number_of_shards. Assuming 1.\r\nWARNING:tensorflow:cross_replica_sum should be used within a tpu_shard_context, but got unset number_of_shards. Assuming 1.\r\nTraceback (most recent call last):\r\n  File \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1628, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Op type not registered 'CrossReplicaSum' in binary running on <ServerName>. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'CrossReplicaSum'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 410, in minimize\r\n    name=name)\r\n  File \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\contrib\\tpu\\python\\tpu\\tpu_optimizer.py\", line 170, in apply_gradients\r\n    grad, self._group_assignment), var))\r\n  File \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\contrib\\tpu\\python\\ops\\tpu_ops.py\", line 417, in cross_replica_sum\r\n    return gen_tpu_ops.cross_replica_sum(x, group_assignment, name=name)\r\n  File \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\contrib\\tpu\\ops\\gen_tpu_ops.py\", line 322, in cross_replica_sum\r\n    name=name)\r\n  File \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1792, in __init__\r\n    control_input_ops)\r\n  File \"D:\\Anaconda3\\envs\\lt\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1631, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Op type not registered 'CrossReplicaSum' in binary running on <ServerName>. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'CrossReplicaSum'\r\n>>>\r\n\r\nAlso to note, rather than calling minimize, I also tried compute gradients, apply gradients, and that also crashed similarly.\r\n\r\nI hope that the tensorflow team can recommend a work around and/or work this fix into an upcoming version.\r\nThanks", "comments": []}, {"number": 25566, "title": "Added mobilenetV2 in keras application test", "body": "", "comments": []}, {"number": 25565, "title": "TFTRT: Support GatherV2 op", "body": "* Add support for GatherV2 Op. \r\n* Add unit tests\r\n* Add CheckAxis helper method (need to use in other functions, will issue separate PR for this)\r\n\r\nhttps://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/gather-v2\r\n\r\nThe \"scalar indices\" case is currently unsupported because we don't allow scalar tensors, but they are now supported in TRT 5.0+ or maybe earlier, need to double check (TRT calls these 0-D tensors). We will have to update our code to allow these tensors in order to allow this case to convert.\r\n\r\nWill add support for constant (weight) indices with automatic weight -> tensor helper soon.", "comments": ["Hi @smit-hinsu, would you please help to take a look? Thanks.", "Thanks @smit-hinsu, must have missed those.\r\n\r\nI am a little concerned about the usability of the `GetSpan()` method (previously `ToVector`). It seems like a  `Span<const T>` cannot be implicitly convered to an `std::vector<T>`? This kinda defeats the point of the helper function because it would require an extra step to get an std::vector. ", "What kind of use-cases are you envisioning that would require std::vector?\r\n\r\nabsl::Span provides read-only view to the Tensor as long as the tensor is not destructed. Even if we convert it to std::vector, we won't be able to update content of the original Tensor and I expect absl::Span to be usable in places where std::vector would have been used.", "It seems that some of the tests started failing after PR but I couldn't figure out the reason.\r\n\r\ntensorflow/python/compiler/tensorrt:gpu_base_test and\r\ntensorflow/python/compiler/tensorrt:gpu_conv2d_test\r\ntensorflow/python/compiler/tensorrt:gpu_vgg_block_test\r\n\r\n Trevor, could you take a look?", "@smit-hinsu Thanks Smit, I will look into it.", "@smit-hinsu That was a tricky one. I forgot a comma when adding \"GatherV2\" to the `candidate_ops` list, which caused it to merge with the string below, which was \"Identity\" making the op that we claimed to support called \"GatherV2Identity\". ", "The commit that added the comma did not show up internally for some reason and therefore I had to manually fix it. That worked but ended up making me author of the commit and did not give credit to you. Sorry about that.\r\nhttps://github.com/tensorflow/tensorflow/commit/368674dfe253eef39537fcabbb675f12be47ab05", "I tried to reopen and merge, but there is no diff."]}, {"number": 25564, "title": "tf.keras.initializers.glorot_normal is not consistent with its documentation", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): Anaconda\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5.0\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nreference: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal\r\n\r\nAccording to reference, it draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.\r\n\r\nI generated them with fan_in = 100, fan_out = 100 and ploted their histogram. I printed out the smallest and largest from the sampe and I got -0.22728574 0.22735965. These should be 2!\r\n\r\n**Describe the expected behavior**\r\n\r\nI generated them with fan_in = 100, fan_out = 100 and ploted their histogram. I printed out the smallest and largest from the sampe and I got -0.22728574 0.22735965. These should be 2!\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\n\r\nfan_in = 100\r\nfan_out = 100\r\n\r\ntf.random.set_random_seed(337)\r\n\r\nW = tf.get_variable(\"W\", \\\r\n                    shape=(fan_in, fan_out), \\\r\n                    initializer=tf.keras.initializers.glorot_normal)\r\n\r\nsigma = np.sqrt(2 / (fan_in + fan_out))\r\n\r\nwith tf.Session() as sess:\r\n    tf.global_variables_initializer().run()\r\n    \r\n    W_now = sess.run(W)\r\n    \r\n    plt.hist(W_now.reshape((-1,)), bins=100)\r\n    plt.show()\r\n    \r\n    print(2*sigma)\r\n    print(np.min(W_now.reshape((-1,))), np.max(W_now.reshape((-1,))))\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi, I am confused about what do you mean \"2\". If fan_in = 100, fan_out = 100, then stddev = sqrt(2 / (fan_in + fan_out)) = 0.1. So the samples are drawn from N(0.0, 0.1).", "If fan_in = 100, fan_out = 100, then stddev = sqrt(2 / (fan_in + fan_out)) = 0.1. So the samples are drawn from truncated_N(0.0, 0.1), not N(0.0, 0.1). See below ref2. If samples are from truncated_N(0.0, sigma) with sigma=0.1, then they are in [-2*sigma,2*sigma], i.e., [-0.2,0.2]. Oh, I mean 0.2, not 2. Sorry about the confusion.\r\n\r\nCorrection:\r\nI generated them with fan_in = 100, fan_out = 100 and ploted their histogram. I printed out the smallest and largest from the sampe and I got -0.22728574 0.22735965. These two numbers should be approximately -0.2 and 0.2!\r\n\r\n-----------------\r\nref1: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal\r\nIt draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.\r\n-------------------\r\n--------------------\r\nref2: https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal\r\nThe generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.\r\n---------------------", "The documentation is correct: it is a truncated normal distribution with that standard deviation. If it were what you expected, the standard deviation of the distribution would be lower (because of the truncated part). Eg. you can run:\r\n```\r\n>>> from scipy import stats\r\n>>> stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\r\n0.8796256610342398\r\n```\r\nAs you can see, the stddev of a truncated normal distribution (where the non-truncated normal distribution is a N(0, 1)) is lower than 1.0. So if you want a truncated normal distribution with stddev = 1, you need to use the corresponding non-truncated N(0, 1 / 0.8796256610342398). So you can get values higher than -2 and 2.", "Thank @mgaido91 for your detailed explanation. I'd like to add some details below:\r\n\r\nglorot_normal is inherited from [VarianceScaling](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/VarianceScaling). Its API explains:\r\n\r\n> With distribution=\"truncated_normal\" or \"untruncated_normal\", samples are drawn from a truncated/untruncated normal distribution with a mean of zero and **a standard deviation (after truncation, if used)** stddev = sqrt(scale / n) where n is: - number of input units in the weight tensor, if mode = \"fan_in\" - number of output units, if mode = \"fan_out\" - average of the numbers of input and output units, if mode = \"fan_avg\"\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/ops/init_ops.py#L474-L478\r\n\r\nHence in your example, the real stddev is corrected as 0.1 / 0.87962566103423978 = 0.11368472343385565, so the samples are expected in 2 * [-stddev, stddev] = [-0.2273694468677113, 0.2273694468677113], rather than [-0.2, 0.2].\r\n\r\nPerhaps we need to also emphasize that stddev is used after truncation in the glorot_normal API ?", "@facaiy despite I think that a note to clarify that would be good, I haven't submitted a PR because it would mean that we have to make the same clarifications also in all the other initializers using a truncated normal distribution (eg. `lecun_normal`) and I am not sure this approach is easy to maintain; moreover the description is accurate (despite the misunderstanding is easy since truncated normal distributions are not so widespread).\r\n\r\n", "Mant thanks @mgaido91 and @facaiy. I understand what happens. However, documentations are still inconsistent or at least misleading. I think documentations should be updated. I have a simple suggestion below.\r\n\r\nref1: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal\r\nIt draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor. ---- This part is not consistent or at least misleading with definition of truncated normal in ref2. According to ref2 the key word stddev of tf.truncated_normal_initializer is refering the stddev of the original normal distribution, not the standard deviation of the truncated normal distribution. A simple fix I can think of: a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out))  ------> a truncated normal distribution centered on 0 whose standard deviation, not stddev of underling normal, is sqrt(2 / (fan_in + fan_out)) \r\n\r\nref2: https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal\r\nThe generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked. ---- This part is ok with my simple simulation code using tf.truncated_normal_initializer:\r\n\r\nI generate many samples from tf.truncated_normal_initializer(stddev=sigma) where sigma=0.1. In the code, print(-2*sigma, 2*sigma) produces -0.2 0.2 and print(np.min(W_now.reshape((-1,))), np.max(W_now.reshape((-1,)))) produces -0.19992638 0.19999139 and these two numbers are very close to -0.2 and 0.2. So, as ref2 said the generated values are from a normal distribution with specified mean and stddev, not stddev / .87962566103423978, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\n\r\nfan_in = 100\r\nfan_out = 100\r\nsigma = np.sqrt(2 / (fan_in + fan_out))\r\n\r\ntf.random.set_random_seed(337)\r\n\r\nW = tf.get_variable(\"W\", \\\r\n                    shape=(fan_in, fan_out), \\\r\n                    initializer=tf.truncated_normal_initializer(stddev=sigma))\r\n\r\nwith tf.Session() as sess:\r\n    tf.global_variables_initializer().run()\r\n    \r\n    W_now = sess.run(W)\r\n    \r\n    plt.hist(W_now.reshape((-1,)), bins=100)\r\n    plt.show()\r\n    \r\n    print(-2*sigma, 2*sigma)\r\n    print(np.min(W_now.reshape((-1,))), np.max(W_now.reshape((-1,))))", "@SungchulLee I still think that the documentation is correct and not inconsistent.\r\nref1 says: \"It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) ...\" which is true.\r\n\r\nref2 says: \"The generated values follow a normal distribution with specified mean and standard deviation...\" which is true again.\r\n\r\nYou are saying that the documentation is inconsistent because you are saying that these 2 methods are equivalent, but this is a connection you are making and it is not done by the documentation.\r\n\r\nWhat I think could be done, which may help avoiding misunderstandings like this, IMHO, is to add a sentence in ref2 like: \"The returned values, hence, follow a truncated normal distribution centered in `mean` with a stddev = `stddev / 0.8....`.\". So pointing out that the stddev of a truncated normal distribution is different from the stddev of the original normal distribution it is taken from.\r\n\r\nDo you think this may help?", "@fchollet @ppwwyyxx can anyone shed some light on the discussion? ", "@mgaido91 My point is that: \r\n\r\nThe key word stddev in the definition of tf.truncated_normal_initializer in ref2 is the standard deviation of the underlying normal distribution, not  the standard deviation of the truncated normal distribution itself (I did a simple experiment with above python code and ckecked this fact. @mgaido91 misunderstood this part) whereas \r\n\r\nthe key word stddev of tf.truncated_normal_initializer in the definition of tf.keras.initializers.glorot_normal of ref1 is, as @mgaido91 pointed out, the standard deviation of the truncated normal distribution itself. \r\n\r\nThat is why I was misunderstood initially.", "@SungchulLee I understood what you mean and why you got confused. The point is that in ref1 the documentation is talking about the stddev of the truncated normal distribution (I think it is quite clear from the snippet I copy-pasted above); while in ref2 it is referring to the stddev of the underlying normal distribution (again, I think it is clear from the snippet copied from there). I think the documentation states this, while it could be emphasized more in order to avoid confusions.\r\n\r\nMaybe you mean that in the second case we should rename the parameter to `underlying_stddev` or something similar? That seems an overkill to me.", "The documentation looks correct to me as well. To make sure that such a confusion does not arise again, maybe we can edit the **Returns** part of ref2 to say. \"A tensor of the specified shape filled with random values from a truncated normal distribution with `trunc_mean = mean` and `trunc_stddev = stddev * 0.87962566103423978`\".", "How about changing ref1 as follows:\r\n\r\nref1: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal\r\nIt draws samples from a truncated normal distribution centered on 0 with ```standard deviation``` sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.\r\n\r\nThis change is ok with me as long as stddev is not used since the stddev of tf.truncated_normal_initializer in ref2 or random_ops.truncated_normal below means the standard deviation of the underlying normal distribution, not the standard deviation of the truncated normal distribution itself. \r\n\r\n```\r\n if self.distribution == \"normal\" or self.distribution == \"truncated_normal\": \r\n   # constant taken from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.) \r\n   stddev = math.sqrt(scale) / .87962566103423978 \r\n   return random_ops.truncated_normal( \r\n       shape, 0.0, stddev, dtype, seed=self.seed)\r\n```", "@SungchulLee, let's see what others think and we can change it the way you say, if more people agree to it. However, we have to keep in mind that if we are making any changes to ref1, then we should also make similar changes in the documentation of other initializers using a truncated normal distribution as well. On the other hand, changing the **Returns** part of ref2 would allow us to skip this extra work and would serve our purpose as well.", "@ Sudeepam97 I think you misunderstood the situation in two ways:\r\n\r\n1. tf.random.truncated_normal is more fundamental than tf.keras.initializers.glorot_normal. tf.keras.initializers.glorot_normal depends on tf.random.truncated_normal.\r\n\r\n2. In tf.random.truncated_normal, stddev means the standard deviation of the underlying normal distribution, not the standard deviation of the truncated normal distribution itself.", "@SungchulLee, I'm confused. I'm not sure why you think that I misunderstand.\r\n\r\nI agree with these two points and this is what I had been thinking. Am I wrong? If so, then can you please explain the actual situation to me? If I am not wrong then can you please point out why you thought that I think otherwise? I sense some miscommunication.", "In ref1, **stddev** refers to the standard deviation of truncated normal. There are other initializers as well (like `lecun_normal`) where **stddev** refers to the standard deviation of truncated normal. Going by your way, we should edit **stddev** to **standard deviation** for all of these initializers.\r\n\r\nNow, all of these initializers depend on `tf.random.truncated_normal` and in it, **stddev** refers to the standard deviation of underlying normal, and this is why, standard deviation of truncated normal is divided by 0.8... for adjustment.\r\n\r\nNow, If we mention, [right here](https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal#returns), in `tf.random.truncated_normal`, that it...\r\n\"returns a tensor of the specified shape filled with random values from a truncated normal distribution with `trunc_mean = mean` and `trunc_stddev = stddev * 0.87962566103423978`\".\r\n\r\nWe essentially convey that an initializer like `tf.keras.initializers.glorot_normal` draws samples from a truncated normal distribution centred on 0 with truncated normal standard deviation = underlying normal standard deviation * 0.87962566103423978. This is what I meant.", "@Sudeepam97 I think we are on the same page. Good. \r\n\r\nI don't think it is wise to change any key word. What I propose is to change documentation instead.\r\n\r\nOriginal:\r\nref1: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal\r\nIt draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.\r\n\r\nProposal:\r\nref1: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal\r\nIt draws samples from a truncated normal distribution centered on 0 with standard deviation sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.\r\n\r\nWith proposed documentation I would figure out what's going on without unnecessary confusing.", "> @Sudeepam97 I think we are on the same page. Good.\r\n> \r\n> I don't think it is wise to change any key word. What I propose is to change documentation instead.\r\n\r\nCan you clear me up on what exactly do you mean by 'key word'? Even I propose to change the documentation, but I suppose that we should change [this](https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal#returns) section of ref2 to say, (I'll re-phrase) \"returns a tensor of the specified shape, filled with random values from a truncated normal distribution with the mean equal to `mean` and standard deviation equal to `stddev * 0.87962566103423978`. \r\n\r\nBut we can go by your method as well, which is to change ref1 as follows...\r\n\r\n> \r\n> Original:\r\n> ref1: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal\r\n> It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.\r\n> \r\n> Proposal:\r\n> ref1: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal\r\n> It draws samples from a truncated normal distribution centered on 0 with standard deviation sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.\r\n> \r\n> With proposed documentation I would figure out what's going on without unnecessary confusing.\r\n\r\nNow that a few solutions have been discussed, let's see what the others think. A PR can be created accordingly.", "@Sudeepam97 I am new to github (this is my first writing in github), python (2 years in a computer language), and tensorflow (so, you can guess) and so my wording may be incorrect. Even, I don't know what is PR.  \r\n\r\nAccording to my shallow understanding of python language \r\n\r\n```\r\ntf.random.truncated_normal(\r\n    shape, # <----- positional argument\r\n    mean=0.0, # <----- keyword argument = default argument\r\n    stddev=1.0, # <----- keyword argument = default argument\r\n    dtype=tf.float32, # <----- keyword argument = default argument\r\n    seed=None, # <----- keyword argument = default argument\r\n    name=None  # <----- keyword argument = default argument\r\n)\r\n```", "Don't worry, PR is short for pull request :-) \r\n\r\nThank everyone for your useful suggestion! Let's make it simple and keep consistent with VarianceScaling, I'm fine with: \r\n\r\nIt draws samples from a truncated normal distribution centered on 0 with **standard deviation (after truncation)**  stddev = sqrt(2 / (fan_in + fan_out)) \r\n\r\nWelcome to create a PR if anyone is interested in it. Please ping me,then I'll review and approve. ", "@facaiy  I am fine with your proposal.", "@facaiy I'll be happy to create a PR. I'll submit one by today. Should I edit the documentation for all the initializers that depend on `tf.random.truncated_normal` or should I edit `glorot_normal` only?", "Thank you, Sudeepam. How about only glorot_normal at first?", "Alright, I'll start right away.", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25564)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25564)\r\n"]}, {"number": 25563, "title": "Can LSTM be fully quantized for inference?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (or github SHA if from source): 1.13.0-rc0\r\n\r\n**Problem Summary and Errors Encountered**\r\n\r\nI am trying to have the LSTM version from TFLite ([here](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/lite/experimental/examples/lstm/tflite_lstm.py)) quantized for inference through TFLite. \r\n\r\nI use `tf.contrib.quantize.create_training_graph()` for training and have added fake quant node after the input. But I think the use of `tf.unstack()` creates a problem when using the 'static_rnn()`\r\n\r\nThe model is as below:\r\n\r\n```\r\ninput_frequency_size = model_settings['dct_coefficient_count']\r\ninput_time_size = model_settings['spectrogram_length']\r\nfingerprint_4d = tf.reshape(fingerprint_input, \r\n                              [-1, input_time_size, input_frequency_size])\r\nnum_classes = model_settings['label_count']\r\nprojection_units = model_size_info[0]\r\nLSTM_units = model_size_info[1]\r\nwith tf.name_scope(\"LSTM-Layer\"):\r\n  with tf.variable_scope(\"lstm\"):\r\n    lstm_cell = TFLiteLSTMCell(num_units=LSTM_units,\r\n                                 use_peepholes=True,\r\n                                 num_proj=projection_units)\r\n    lstm_input = tf.unstack(fingerprint_4d, input_time_size, 1)\r\n    # lstm_input = tf.fake_quant_with_min_max_args(lstm_input)\r\n    _, last = tf.nn.static_rnn(cell=lstm_cell, inputs=lstm_input,\r\n                                   dtype=tf.float32)\r\n    # tf.quantization.fake_quant_with_min_max_args(last)\r\n    flow = last[-1]\r\n\r\nwith tf.name_scope(\"Output-Layer\"):\r\n  W_o = tf.get_variable('W_o', shape=[projection_units, num_classes],\r\n                          initializer=tf.contrib.layers.xavier_initializer())\r\n  b_o = tf.get_variable('b_o', shape=[num_classes],\r\n                          initializer=tf.constant_initializer(0.01))\r\n  logits = tf.add(tf.matmul(flow, W_o), b_o)\r\n```\r\n\r\nAfter training, I freeze the graph and use the following script to convert to TF lite:\r\n\r\n```\r\ngraph_def_file = \"frozen_graph.pb\"  # This is the .pb file.\r\ninput_arrays = [\"mfcc_data\"]            # This is the name of the input node\r\noutput_arrays = [\"labels_softmax\"]  # This is the name of the output node\r\n\r\n# This is the main code to call tflite converter (i.e. toco)\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\n# Since fake quantization used during training, we can quantize the graph during converting\r\nconverter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\nconverter.reorder_across_fake_quant = True\r\ninput_arrays = converter.get_input_arrays()\r\nconverter.quantized_input_stats = {input_arrays[0] : (227., 1.)}  # mean, std_dev\r\ntflite_model = converter.convert()\r\nopen(\"lstm-tflite.tflite\", \"wb\").write(tflite_model)   # The resulting .tflite file.\r\n```\r\n\r\nBut this then gives error as below:\r\n```\r\nConverterError: TOCO failed. See console for info.\r\n2019-02-06 11:57:30.492529: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1098 operators, 1429 arrays (0 quantized)\r\n2019-02-06 11:57:30.560084: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1098 operators, 1429 arrays (0 quantized)\r\n2019-02-06 11:57:30.646273: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 573 operators, 853 arrays (1 quantized)\r\n2019-02-06 11:57:30.675022: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 573 operators, 853 arrays (1 quantized)\r\n2019-02-06 11:57:30.686976: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 522 operators, 802 arrays (1 quantized)\r\n2019-02-06 11:57:30.703210: F tensorflow/lite/toco/tooling_util.cc:1702] Array LSTM-Layer/lstm/unstack, which is an input to the Concatenation operator producing the output array LSTM-Layer/lstm/rnn/tf_lite_lstm_cell/concat, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\n```\r\n\r\nI try to add the fake quant nodes manually after the unstack as you can see in the commented code above, but this will give an error from `static_rnn` because it will not remain a sequence anymore resulting in the following error while training:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 324, in <module>\r\n    tf.app.run(main=main)\r\n  File \"C:\\Users\\bhargav\\AppData\\Local\\Continuum\\miniconda3\\envs\\tf113cpu\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 140, in main\r\n    logits, dropout_prob = models.create_model(\r\n  File \"C:\\Users\\bhargav\\Documents\\audio\\keyword-spotting\\models.py\", line 58, in create_model\r\n    model_size_info, is_training)\r\n  File \"C:\\Users\\bhargav\\Documents\\audio\\keyword-spotting\\models.py\", line 198, in create_lstm_tflite_model\r\n    dtype=tf.float32)\r\n  File \"C:\\Users\\bhargav\\AppData\\Local\\Continuum\\miniconda3\\envs\\tf113cpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\bhargav\\AppData\\Local\\Continuum\\miniconda3\\envs\\tf113cpu\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 1290, in static_rnn\r\n    raise TypeError(\"inputs must be a sequence\")\r\nTypeError: inputs must be a sequence\r\n```\r\n\r\nI want to know if the full quantization of LSTM will be supported any time in near future. If GraphDef is needed, let me know.", "comments": ["Is there any update on this? Please let us know.", "@aselle ptal?", "Hi all,\r\nSince it's April, I'm wondering if someone has idea on this one? I got the same issue, and it looks like I'm not alone here. \r\n@ebrevdo @aselle ", "Same issue here. Any progress on quantization-aware training for LSTM/RNN?", "Hi bkanaki, xddkdx, TF-deve, thanks for reaching out. We are actively working on fully quantized LSTM in the following areas:\r\n1) quantization-aware training for LSTM and post-training quantization for LSTM; each of the two options can be used to produce fully quantized LSTM model, depending on one's use case.\r\n2) fully quantized LSTM op that runs the model. TFLite already has a basic fully quantized LSTM (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/reference/reference_ops.h#L1887) and it is being expanded to include more features such as peephole, cifg, projection etc.\r\n\r\nBoth parts are needed to fully support quantized LSTM and we are making progress. Please stay tuned.", "> Hi bkanaki, xddkdx, TF-deve, thanks for reaching out. We are actively working on fully quantized LSTM in the following areas:\r\n> \r\n> 1. quantization-aware training for LSTM and post-training quantization for LSTM; each of the two options can be used to produce fully quantized LSTM model, depending on one's use case.\r\n> 2. fully quantized LSTM op that runs the model. TFLite already has a basic fully quantized LSTM (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/reference/reference_ops.h#L1887) and it is being expanded to include more features such as peephole, cifg, projection etc.\r\n> \r\n> Both parts are needed to fully support quantized LSTM and we are making progress. Please stay tuned.\r\n\r\nHi @jianlijianli , thanks for your reply! In point 1, you mentioned that post-training quantization also produces fully quantized LSTM? I thought post-training quantization still uses float32 kernels during inference (please correct me if I'm wrong). So I guess I'm not very clear about the definition of \"fully quantization\"? Thanks.", "Hi TF-Deve, apologies for the delay. You are right that at this moment post-training quantization is using float32 activation (and uint8 weight and does quantization on the fly inside the kernel). The \"fully quantized\" I mentioned means a scheme that everything is integer and this is what we are currently working on. Stay tuned:)", "@jianlijianli Area you finished \"fully quantized\" LSTM? I'm trying to coverter Tensorflow-Model to TfLite-Model(static or dynamic), but I can't. So there's noway to use Tflite-LSTM \"fully quantized\" operator? Thanks.", "Hello @jianlijianli , is this still \"work in progress\"? Because it's been 1 and a half years since this is work in progress and no updates. Is there any timeline that we can follow? ", "Hi @jianlijianli, do you have any information on the progress of this work? ", "TFLite has a well documented post-training quantization path documented\nhere, new since I last checked this bug.\n\nhttps://www.tensorflow.org/lite/performance/post_training_quantization\n\nHave you tried that?\n\n\n\nOn Sun, Feb 21, 2021, 8:42 PM Tom Becnel <notifications@github.com> wrote:\n\n> Hi @jianlijianli <https://github.com/jianlijianli>, do you have any\n> information on the progress of this work?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25563#issuecomment-783075074>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG7DHRHV3ZKUALDBUJ3TAHOEVANCNFSM4GUZL2RA>\n> .\n>\n", "Hi @ebrevdo, thanks for responding. I have used post-training quantization and am unable to convert an LSTM using full integer quantization, although I've had no issues with dynamic range quantization of LSTM models. ", "hey @ebrevdo , I'm also having issues when trying to fully quantize a (keras) LSTM to int8. I'm running code like the following:\r\n```\r\n    representative_dataset_generator = build_representative_dataset_generator(\r\n        [s.shape.as_list() for s in model.inputs],\r\n        params,\r\n    )\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = representative_dataset_generator\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.int8\r\n    converter.inference_output_type = tf.int8\r\n    tflitemodel = converter.convert()\r\n```\r\nand seeing this in response:\r\n```\r\n  File \"/Users/jax/code/www/tools/model_release/convert_to_tflite.py\", line 137, in convert_tf2_to_tflite\r\n    return converter.convert()\r\n  File \"/Users/jax/code/www/venv/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 874, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"/Users/jax/code/www/venv/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 632, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"/Users/jax/code/www/venv/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 449, in _calibrate_quantize_model\r\n    calibrate_quantize = _calibrator.Calibrator(result)\r\n  File \"/Users/jax/code/www/venv/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 60, in __init__\r\n    raise ValueError(\"Failed to parse the model: %s.\" % e)\r\nValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.\r\n```\r\nis this a known issue? fwiw, if i comment out the lines relevant to quantization, the convert call is successful.", "@liufengdb is our TFLite post-training-quantization expert so I've asked him to answer these questions.", "Hi Jackson, just want to confirm that you are using the latest tf version 2.4. (Your stack shows you are still using old converter). ", "thanks @ebrevdo !\r\n\r\nyes, @liufengdb , i tried both `2.4.0` and `2.4.1`. is there an easy to force my local package to use the new converter?", "@Xhark @liufengdb @ebrevdo:We are using LSTM op in our model and could verify accuracy using TF, TFLite float32 models.\r\nPlease see below Colab for more information.\r\nhttps://colab.research.google.com/drive/1LIqNXYszOB4eUHFhqnbGLESdZs4Ipu_n?usp=sharing\r\nHowever when I generate TFLite int8 model using post training quantization and int8 model doesn't work with my test data.\r\nNote: If I use uint8 post training quantization it works fine.\r\nFor some reason int8 quantization is not working with LSTM op.\r\n", "@nyadla-sys You have to use quantized input/output as follow this doc: https://www.tensorflow.org/lite/performance/post_training_quantization\r\n\r\nFor your case, validation code for int8 model should be:\r\n\r\n    input_scale, input_zero_point = input_details[0][\"quantization\"]\r\n    spectrogram = np.array(spectrogram)\r\n    spectrogram = spectrogram / input_scale + input_zero_point\r\n    q_spectrogram = np.array(spectrogram, dtype=np.int8).reshape(1,49, 257)\r\n    tflite_interpreter_quant_int8.set_tensor(input_details[0]['index'], q_spectrogram )\r\n    tflite_interpreter_quant_int8.invoke()\r\n    tflite_q_model_predictions = tflite_interpreter_quant_int8.get_tensor(output_details[0]['index'])\r\n    print(tflite_q_model_predictions)\r\n    # print(\"\\nPrediction results shape:\", tflite_q_model_predictions.shape)\r\n    # print(\"\\nPrediction results shape:\",np.array(tflite_q_model_predictions, dtype=np.int8))\r\n    output_scale, output_zero_point = output_details[0][\"quantization\"]\r\n    tflite_model_predictions = (np.array(tflite_q_model_predictions, dtype=np.float32) - output_zero_point) * output_scale\r\n    tflite_pred_dataframe = pd.DataFrame(tflite_model_predictions)\r\n    tflite_pred_dataframe.columns = commands\r\n    print(tflite_pred_dataframe)\r\n\r\n\r\nIt seems better than before, but I'm not so sure why uint8 and int8 result is different even if use validation code above.\r\nuint8 and int8 TFLite both uses int8 LSTM op, and I checked all tensors related to LSTM op (weights and input, by called get_tensor on python interpreter) are same but has different output. I guess TFLite kernel (unidirectional_sequence_lstm) may have some issue?", "@nyadla-sys If you use \r\n    spectrogram = np.array(spectrogram)\r\n    spectrogram = np.clip(np.floor(spectrogram / input_scale + input_zero_point), 0, 255) # for uint8 validation\r\n    spectrogram = np.clip(np.floor(spectrogram / input_scale + input_zero_point), -128, 127) # for int8 validation\r\n\r\nthen int8 and uint8 model output seems almost same. Interestingly with floor and without floor makes huge difference.\r\nWould you please check it works?\r\n", "@Xhark Thanks for your time and will try as mentioned above \r\nHowever I tried below for int8 model and it works for me.\r\n    spectrogram_t = np.array(spectrogram, dtype=np.uint8).reshape(1,49, 257)\r\n    spectrogram_t = np.array(spectrogram_t-128, dtype=np.int8)\r\n    tflite_interpreter_quant.set_tensor(input_details[0]['index'],spectrogram_t )\r\n", "@Xhark It works and thanks\r\n", "@bkanaki Could you please let us know if this issue still persists in TF v2.5 ? Thank you!", "@sushreebarsa ,Yes, please see the below links. \r\nhttps://github.com/nyadla-sys/tflite-micro/tree/micro_speech_lstm_with_training/tensorflow/lite/micro/examples/micro_speech_lstm/train\r\n\r\nhttps://colab.research.google.com/github/nyadla-sys/tflite-micro/blob/upload_lstm/tensorflow/lite/micro/kernels/xtensa/examples/micro_speech_lstm/train/micro_speech_with_lstm_op.ipynb\r\n\r\n", "Closing this as I have moved on to a different project a long time ago, the links above seem to be working well.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25563\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25563\">No</a>\n", "Do quantization aware training supports LSTM/GRU layers now?"]}, {"number": 25562, "title": "Update RELEASE.md", "body": "Add TensorRT 5.0 support in release notes.", "comments": []}, {"number": 25561, "title": "fixed the reuse argument in Dense layer's documentation", "body": "PR for #25393.  `reuse` is no longer a valid argument, and should be replaced with `_reuse` instead. Passing reuse to Dense will cause error.", "comments": []}, {"number": 25560, "title": "[XLA] Add support to specify boundary nodes in interactive_grahviz tool.", "body": "The enhanced command is `<instr> [<width>] [/ <boundary_instr1> <boundary_instr2> ...]`.\r\n\r\nThe boundary nodes are optional. This is useful in cases where one wants to further prune\r\na graph when using a large width.", "comments": ["@rthadur Is there anything else I need to do? The CI seems to be failed. Maybe it requires a restart. Thanks.\r\n", "I have no idea what's going on here. The changes in this pull request did land in TF's master branch already, but the pull request itself seems to be stuck in some weird state. ", "I think our system wants a 1:1 correspondence between PRs and CLs that eventually get submitted.  Because we've already submitted a CL here, I'd recommend closing this PR and opening a new one for any followups.", "Please let me know what I need to do on my side.", "It looks like there was a race -- viz_improve branch got updated while I've already been in the process of submitting the change. Once my copy landed, there was a minor conflict on this pull request that I've resolved, but there's still one tiny bit of difference left. I suspect internal review tracker ping-ponging on abandoning review was triggered by the pull request updates.\r\n\r\nOverall I think we're good now and we can close this pull request as the changes are already in TF, despite the 'open' status. "]}, {"number": 25559, "title": "TensorRT version in  TensorFlow 1.13.0-rc0 ?", "body": "What version of TensorRT does TensorFlow 1.13.0-rc0 support?  I don't see any mention about TensorRT in the release notes [link](https://github.com/tensorflow/tensorflow/releases/tag/v1.13.0-rc0)\r\n\r\nThanks", "comments": ["@samikama Can you PTAL? Thanks!", "The GPU builds of 1.13 are built against TRT5.0, but TRT4 is also supported.", "Closing this issue since it was answered by @pooyadavoodi . Feel free to reopen if have any follow up questions. Thanks!", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25559)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25559)\r\n", "Thanks"]}, {"number": 25558, "title": "ImportError: No module named 'tensorflow' in official Docker image", "body": "**System information**\r\n- Windows 10 x64:\r\n- Official tensorflow-jupiter Docker container (without GPU support), latest 06 Feb 2019\r\n- TensorFlow version: unable to know\r\n- Python version: sys.version_info(major=3, minor=5, micro=2, releaselevel='final', serial=0)\r\n\r\n**Describe the problem**\r\n`import tensorflow as tf ` command causing this error:\r\n\r\n> ImportError                               Traceback (most recent call last)\r\n> <ipython-input-7-7f6e5ebe327b> in <module>()\r\n> ----> 1 import tensorflow as tf\r\n> ImportError: No module named 'tensorflow'\r\n> \r\n\r\n", "comments": ["UPD: 6 Feb 2019 11:06 PM GMT docker image updated, but it not helps.", "There was a bug in our Docker CI scripts that pushed broken images. Now that they've been fixed, I verified that this works on Linux:\r\n\r\n```\r\ndocker run -it --rm tensorflow/tensorflow:latest-jupyter python -c 'import tensorflow as tf'\r\n```\r\n\r\nSorry about the confusion! This should be resolved now, but please comment again if not.", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25558)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25558)\r\n", "I found the same error with \r\n`docker run -it --rm tensorflow/tensorflow:devel-gpu  python -c 'import tensorflow as tf'`\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named tensorflow", "@dearboll The `devel` tags are for developers to make changes *to* TensorFlow. They do not have TensorFlow installed by default.", "@angersson Got it. Thanks a lot. ", "@angersson I'm new to this and I had the same issue - `ImportError: No module named tensorflow`... how can install tensorflow in the devel images? I would like to test the changes I've made which are only changing ` ENV TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.0,6.1,7.0 ` to include 5.0", "@angerson but how to chnage the gpu version under docker run -it --rm tensorflow/tensorflow:latest-jupyter. I would like to work on tensorflow 2.2.0, but it is providing only tf 2.4.1"]}, {"number": 25557, "title": "Bazel test failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v1.12\r\n- Python version: 3.6.4\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: none \r\n- GPU model and memory: none\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nAfter ./configure, the .tf_configure.bazelrc file is shown as:\r\n\r\n```build --action_env PYTHON_BIN_PATH=\"/home/jon/anaconda3/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/jon/anaconda3/lib/python3.6/site-packages\"\r\nbuild --python_path=\"/home/jon/anaconda3/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --config=xla\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"0\"\r\nbuild --action_env TF_DOWNLOAD_CLANG=\"0\"\r\nbuild:None --define with_mpi_support=true\r\nbuild --config=None\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\n```\r\nThen i ran \r\n\r\n`bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...` \r\n\r\nand got following error:\r\n\r\n```\r\nERROR: /home/jon/local_build/tensorflow/tensorflow/contrib/BUILD:166:12: in deps attribute of cc_library rule //tensorflow/contrib:contrib_kernels: py_library rule '//tensorflow/contrib/mpi_collectives:mpi_collectives_py' is misplaced here (expected cc_library, objc_library, cc_proto_library or cc_import) and '//tensorflow/contrib/mpi_collectives:mpi_collectives_py' does not have mandatory providers: 'CcInfo'\r\nERROR: Analysis of target '//tensorflow/contrib:contrib_kernels' failed; build aborted: Analysis of target '//tensorflow/contrib:contrib_kernels' failed; build aborted\r\nINFO: Elapsed time: 5.272s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n```\r\n\r\nI'm not sure if this is due to the incompatibility of bazel. If so, can anyone suggest a bazel version and a gcc version to build tensorflow v1.12.0 or other release versions?\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nThe full log is\r\n```\r\njon@jon-OptiPlex-3050:~/local_build/tensorflow$ bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...\r\nINFO: Invocation ID: af9321c5-5726-4ff2-a7ce-5f3d449cdb29\r\nDEBUG: /home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:libtensorflow_jni: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:common_deps: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:eager_cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clib: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clicenses: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/python/ops/distributions/BUILD:9:1: target '//tensorflow/python/ops/distributions:distributions' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:86:1: target '//tensorflow/contrib/session_bundle:constants' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:94:1: target '//tensorflow/contrib/session_bundle:exporter' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:138:1: target '//tensorflow/contrib/session_bundle:gc' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:181:1: target '//tensorflow/contrib/session_bundle:session_bundle' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:200:1: target '//tensorflow/contrib/session_bundle:session_bundle_lite' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:244:1: target '//tensorflow/contrib/session_bundle:session_bundle_py' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:312:1: target '//tensorflow/contrib/session_bundle:signature' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:293:1: target '//tensorflow/contrib/session_bundle:signature_lite' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:349:1: target '//tensorflow/contrib/session_bundle:test_util' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/distributions/BUILD:16:1: target '//tensorflow/contrib/distributions:bijectors_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/distributions/BUILD:48:1: target '//tensorflow/contrib/distributions:distributions_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nERROR: /home/jon/local_build/tensorflow/tensorflow/contrib/BUILD:166:12: in deps attribute of cc_library rule //tensorflow/contrib:contrib_kernels: py_library rule '//tensorflow/contrib/mpi_collectives:mpi_collectives_py' is misplaced here (expected cc_library, objc_library, cc_proto_library or cc_import) and '//tensorflow/contrib/mpi_collectives:mpi_collectives_py' does not have mandatory providers: 'CcInfo'\r\nERROR: Analysis of target '//tensorflow/contrib:contrib_kernels' failed; build aborted: Analysis of target '//tensorflow/contrib:contrib_kernels' failed; build aborted\r\nINFO: Elapsed time: 5.272s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n```\r\n", "comments": ["@jw447 [Here](https://www.tensorflow.org/install/source#linux) is the link to tested build configuration for Ubuntu and TF 1.12. Currently supporting GCC 4.8 | Bazel 0.15.0. try downgrading bazel to 0.15.0 and check whether you get the error. Here is another [resource](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions) that might help you partially. Please let me know how it progresses. Thanks!\r\n\r\n\r\n", "Thank you @jvishnuvardhan for the help. The build on ubuntu went through. May I follow up with another question, I'm also trying Tensor-Flow on a centos server which gives me more computation resources. I'm getting the error of \r\n\r\n```\r\nERROR: /home/j/jw447/local_build/tensorflow/tensorflow/contrib/BUILD:138:12: in deps attribute of cc_library rule //tensorflow/contrib:contrib_kernels: py_library rule '//tensorflow/contrib/mpi_collectives:mpi_collectives_py' is misplaced here (expected cc_library, objc_library, cc_proto_library or cc_import)\r\nERROR: Analysis of target '//tensorflow/contrib:contrib_kernels' failed; build aborted: Analysis of target '//tensorflow/contrib:contrib_kernels' failed; build aborted\r\nINFO: Elapsed time: 7.671s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n\r\nCould you please help with this one as well?\r\n", "@jw447 Good to hear that the resource worked. There are several posts on CentOS in GitHub and Stackoverflow. Here is [one](https://github.com/tensorflow/tensorflow/issues/23673#issuecomment-441880448) in GitHub. It might help you. Please let me know how it progresses. Try searching GitHub with filter \"is:issue centOS is:closed centOS\", it shows up open and closed issues of centOS. Check the issues that were closed, then you have greater chance of finding a solution. I will close this issue. Please open new issue for centOS so that it is easy to search by other. Thanks! ", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 25556, "title": "Export `local_conv1d`/`local_conv2d` in `tf.keras.backend`", "body": "This fix tries to address the issue raised in #25546 where\r\nthere is a mismatch between `tf.keras.backend` and `keras.backend`\r\nfor `local_conv1d`/`local_conv2d`.\r\n\r\nThis fix exports `local_conv1d`/`local_conv2d` in `tf.keras.backend`,\r\nso that `tf.keras.backend` is compatible with `keras.backend`\r\n(https://keras.io/backend/).\r\n\r\nThis fix fixes #25546.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Ping @rthadur Any update on this one? Really would like to see the api consistency between `keras` and `tf.keras`.", "> Ping @rthadur Any update on this one? Really would like to see the api consistency between `keras` and `tf.keras`.\r\n\r\nThank you @yongtang ,working on it , waiting for status to be reported ", "@yongtang could you please fix build errors ", "Thanks @rthadur for the help. The PR has been updated and all tests passed now."]}, {"number": 25555, "title": "check for platform prior to resusing sources for android", "body": "", "comments": ["Looks like this is an android/lite target.\r\nCould you redirect to a reviewer from that team?", "> Looks like this is an android/lite target.\r\n> Could you redirect to a reviewer from that team?\r\n\r\nsure @gunan ", "@jdduke @gunan Additional info -  \r\n```\r\nbazel test //tensorflow/core/...  \r\n```\r\nfails without this fix if Android build is not enabled. Runs into the following build error\r\n```\r\n//tensorflow/core:emscripten_proto_config_lite_runtime: missing input file '//tensorflow/core:android_proto_config.asciipb'\r\n```", "Thanks @agramesh1 but seems like we no longer need this."]}, {"number": 25554, "title": "Raspberry PI - Golang - Tensorflow - std::bad_alloc Issue", "body": "@poxvoculi @penpornk @nhasabni @reedwm @shashishekhar @claynerobison @andydavis1 \r\nHi guys, I have checked some previous issues on ```bad_allocation``` and checked all the discussions and I could not reach to a conclusion how to resolve my ```bad_allocation``` issue. Below is my details for my issue with log. If I could get some insights, it will be very helpful.\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nWritten custom code for face image classification. Model accepts 224x224x3 tensor. The model has 4,253,864 parameters and model depth is 88.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nRaspberry Pi, CPU: 1.4Ghz, 980 Mb RAM.\r\n- Programming Language\r\nGolang \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) \r\nNo\r\n- TensorFlow installed from (source or binary):\r\nBuild from source\r\n- TensorFlow version (use command below):\r\nv1.12.0\r\n- Python version:\r\nNO\r\n- Bazel version (if compiling from source):\r\nv0.15.2\r\n- GCC/Compiler version (if compiling from source):\r\n6.3\r\n- CUDA/cuDNN version:\r\nNO\r\n- GPU model and memory:\r\nNO\r\n\r\n**Describe the current behavior**\r\nI am using GoLang enviroment to load Tensorflow model and run. My Tensorflow model is MobileNet.\r\nIt does face classification. I faced issue in memory allocation.\r\n**Describe the expected behavior**\r\nModel loading is fine and runs perfectly for 15-20 min and after that, it throws ```bad_allocation``` error and exits.\r\n**Code to reproduce the issue**\r\n1. Generate libtensorflow.so and libtensorflow_framework.so artifact for raspberry pi. \r\n2. Load tensorflow model into Golang Env. \r\n3. While model is running and after 15-20 min this happens.\r\n\r\n**Other info / logs**\r\n2019-02-06 10:41:44.057443: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\nE0206 10:41:44.128595    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n2019-02-06 10:41:44.313010: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\nE0206 10:41:44.314077    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n2019-02-06 10:41:44.409438: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\nE0206 10:41:44.411047    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n2019-02-06 10:41:44.504644: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\nE0206 10:41:44.505833    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n2019-02-06 10:41:44.601160: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\nE0206 10:41:44.602079    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\nI0206 10:41:48.258835    2719 camera_input.go:69] Captured Image, Len Queue 0\r\nI0206 10:41:48.898151    2719 runner.go:85] Detection time 638.594898ms\r\n2019-02-06 10:41:48.997640: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\nE0206 10:41:48.999425    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n2019-02-06 10:41:49.100121: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\nE0206 10:41:49.101226    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n2019-02-06 10:41:49.197890: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\nE0206 10:41:49.199211    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\nAborted\r\n\r\n", "comments": ["```\r\nruntime/cgo: pthread_create failed: Resource temporarily unavailable\r\nSIGABRT: abort\r\n```", "It doesn't look like I'll get any time to investigate this unfortunately, so unassigning and adding to the contributions welcome hotlist.", "@anilknayak We see that you are using old version of tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.Please open a new issue in case you face any errors, we will get you the right help .Hence moving this to closed status.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25554\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25554\">No</a>\n"]}, {"number": 25553, "title": "Update docstring parser -> parses", "body": "", "comments": ["Is there somewhere I can see the status of these checks? I feel like they aren't supposed to take 3+ days."]}, {"number": 25552, "title": "RTX 2070 8GB tensorflow 1.13 build from source", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13/1.12\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip/conda\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): Unsure\r\n- CUDA/cuDNN version: 10.0/7.4\r\n- GPU model and memory: MSI Armor RTX 2070 8GB \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nUnable to build 1.13 from source using bazel. bazel is properly installed using chocolatey. I get to the end of configuring my build but I cannot build. TF version 1.12 does not work properly on CUDA 10.0. on TF 1.12, training data loads into the GPU's VRAM but uses the CPU to compute.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nOn first pass, I was able to get to configure.py in tensorflow folder and go through the configuration process. Upon finishing, I was not able to build. After a few days, I am now attempting to configure the build and I am getting an output 'Cannot find bazel. Please install bazel. I installed bazel through chocolately. Attached are the logs showing bazel install through chocolately. I also have some logs during the first attempt at building. I forget where I located the tensorflow building logs. If you can direct me, I'll happily provide those. I'm also curious as to when the 1.13 release becomes official. Thanks. \r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[chocolatey.log](https://github.com/tensorflow/tensorflow/files/2836941/chocolatey.log)\r\n[choco.summary.log](https://github.com/tensorflow/tensorflow/files/2836942/choco.summary.log)", "comments": ["@timyee90 Please check [tested build configurations](https://www.tensorflow.org/install/source_windows#gpu). Also, there is a common issue with WIndows10 where long paths are creating an issue due to default limit on length of a path. Check the solutions provided [here](https://github.com/tensorflow/tensorflow/issues/24705) and [here](https://github.com/tensorflow/tensorflow/issues/24886). Please let me know how it progresses. Thanks!", "I also have problems compiling Tensorflow 1.13 with cuda 10.0/cuddnn 7 on RTX 2070 GPU. \r\n\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n* TensorFlow installed from (source or binary): source\r\n* TensorFlow version: 1.13\r\n* Python version: 3.6\r\n* Installed using virtualenv? pip? conda?: pip\r\n* Bazel version (if compiling from source): 0.21.0\r\n* GCC/Compiler version (if compiling from source): Visual Studio 2015 Update 3.\r\n* CUDA/cuDNN version: 10.0/7.4.2\r\n* GPU model and memory: RTX 2070 8GB\r\n\r\nIt fails after almost finishing all the compilation.  The error message looks like bellow:\r\n\r\n```\r\nERROR: C:/codes/tensorflow/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/daido/_bazel_daido/arhjsgmd/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin\r\n    SET PYTHON_BIN_PATH=C:/Program Files/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Program Files/Python36/lib/site-packages\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  C:/msys64/usr/bin/bash.exe bazel-out/x64_windows-opt/genfiles/tensorflow/tf_python_api_gen_v1.genrule_script.sh\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\daido\\AppData\\Local\\Temp\\Bazel.runfiles__c2da756\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\daido\\AppData\\Local\\Temp\\Bazel.runfiles__c2da756\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\daido\\AppData\\Local\\Temp\\Bazel.runfiles__c2da756\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n```\r\n", "@daidong Could you implement the solutions mentioned above to remove the default limit on length of a path in Windows10. Then, install TFlow again. Please let me know how it progresses. Thanks!", "@jvishnuvardhan thanks for your comment.  The path length in Windows 10 was already removed before I had these error messages. ", "@daidong I have same problems when I build tensorflow1.13 from source with cuda10.0 and cuda9.2,how did you solve it?", "No, the issue is still there.", "Could you share full build logs using pastebin?\r\nThe error means dll cannot be found, but I think there was another issue building the DLL.\r\n\r\nCould you also use pastebin to share the logs when you run configure?", "I was able to run configure.py and get bazel to build, however, I am still not able to build successfully. I'm guessing it is related to what I type into \"Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]: avx2\". Ideally I want CPU & GPU support in the same build which includes SSE3, SSE4.1, SSE4.2, AVX, AVX2, FMA, MKL, etc... I have an intel core i7 8700K, so I'm not sure what is preventing me from building. Please let me know what the exact build command is - to include the above CPU instruction set if possible. Thanks.\r\n\r\nedit: i attempted building two more times without AVX2, and added build options, respectively. Both getting the dll error referenced by the gentleman above.\r\n\r\n[build.txt](https://github.com/tensorflow/tensorflow/files/2851363/build.txt)\r\n[configure.txt](https://github.com/tensorflow/tensorflow/files/2851364/configure.txt)\r\n[build_1.txt](https://github.com/tensorflow/tensorflow/files/2851894/build_1.txt)\r\n[build_2.txt](https://github.com/tensorflow/tensorflow/files/2851895/build_2.txt)\r\n\r\n\r\n", "> \r\n> \r\n> Could you share full build logs using pastebin?\r\n> The error means dll cannot be found, but I think there was another issue building the DLL.\r\n> \r\n> Could you also use pastebin to share the logs when you run configure?\r\n\r\nplease check the configuration logs at: https://pastebin.com/LGK64Yhk\r\nand build logs at: https://pastebin.com/TiVAsvVb\r\n\r\nMany warning generated during the building. But I think the last error stopped it. \r\n\r\n", "not sure what is going on here, CC @meteorcloudy may be able to help.", "This might be related to https://github.com/bazelbuild/bazel/issues/7026\r\nCan you try build with `--incompatible_strict_action_env=false`? Or just upgrade Bazel to 0.22.0 in which the  flag flip is reverted.", "I tried to use tf1.12 with cuda 10.0 and the same problem happened, and when I build tensorflow1.13rc1 from source with cuda10, the error is also \"dll not found\", but when I get tf1.13rc1.wheel from pypi, this problem has been solved. When I installed tf1.13rc1.wheel with pip, some other libraries was installed automatically, maybe these libraries is the cause of \"dll not found error\".  ", "> This might be related to [bazelbuild/bazel#7026](https://github.com/bazelbuild/bazel/issues/7026)\r\n> Can you try build with `--incompatible_strict_action_env=false`? Or just upgrade Bazel to 0.22.0 in which the flag flip is reverted.\r\n\r\nI tried Bazel 0.22 before. But it complains and says please use Bazel 0.21.0 instead. I got the same configuration and version compiled well on Ubuntu.  I guess it should be just missing libraries in Windows now. ", "@gunan the problem here is the system path is completely gone and replaced with \r\n\r\n> SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin\r\n\r\nThe _pywrap... library fails to load because it can't load almost any of its dependencies (not even python or cuda dlls) because they're not there and they're not in PATH. I've run the script (tf_python_api_gen_v1.genrule_script.sh) manually with my system path and it works, but not under Bazel with its crippled path. What can we do to set a proper path, at least for //tensorflow:tf_python_api_gen_v1 ?", "If you're using bazel 0.21 you can build with --incompatible_strict_action_env=false to fix the path issue.\r\n(see https://github.com/bazelbuild/bazel/issues/7026 and https://github.com/bazelbuild/bazel/issues/6648 ) ", "I install tensorflow 1.13.1 from source with bazel 0.21 successfully, with cuda 10.0, cuDNN 7.5, RTX 2080, win10 x64, python 3.6.6, by commands:\r\n```\r\nset BAZEL_VS=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\r\n\r\nbazel build --config=opt --config=cuda --incompatible_strict_action_env=false  --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package --local_resources 6144,.5,1.0\r\n\r\nbazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package C:/tmp/tensorflow_pkg\r\n\r\npip3 install C:/tmp/tensorflow_pkg/tensorflow-version-cp35-cp35m-win_amd64.whl\r\n```", "@asa008 \r\nCould you share your .whl file? \r\n![image](https://user-images.githubusercontent.com/29853829/59459075-82872900-8e4e-11e9-857d-37e8bd0082e4.png)\r\nMine is Windows 10 RTX2070 CUDA 10.1.168 CuDNN 7.6.0.64 building tensorflow branch r1.14 (i thought the software has not caught up to the hardware so newer is better), i followed https://www.tensorflow.org/install/source_windows#gpu and finally got build failed because could not find cudart64_.dll. I see there is a cudart64_101.dll in my CUDA >v10.1 folder. Why is the prompt URL (https://developer.nvidia.com/cuda-90-download-archive) leading me to CUDA 9.0 (Can't work with RTX2070) when it already can pick up that i have CUDA10.1 when running configure.py?  \r\n\r\nEDIT: I finally have a successful import tensorflow by pip install the wheel from https://github.com/gitgithan/tensorflow-windows-wheel/tree/master/1.13.1/py37/GPU/cuda101cudnn75avx2 and adding \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include\" to PATH (it was still giving errors about dll not found until i added)", "I can confirm that command **--incompatible_strict_action_env=false** helped me to get rid of _\"dll not found\"_ error. My configuration:\r\nRTX 2080\r\nWindows 10\r\nTensorflow 1.13.1\r\nCUDA 10.1\r\nCuDNN 7.6\r\n", "> I also have problems compiling Tensorflow 1.13 with cuda 10.0/cuddnn 7 on RTX 2070 GPU.\r\n> \r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n> * TensorFlow installed from (source or binary): source\r\n> * TensorFlow version: 1.13\r\n> * Python version: 3.6\r\n> * Installed using virtualenv? pip? conda?: pip\r\n> * Bazel version (if compiling from source): 0.21.0\r\n> * GCC/Compiler version (if compiling from source): Visual Studio 2015 Update 3.\r\n> * CUDA/cuDNN version: 10.0/7.4.2\r\n> * GPU model and memory: RTX 2070 8GB\r\n> \r\n> It fails after almost finishing all the compilation. The error message looks like bellow:\r\n> \r\n> ```\r\n> ERROR: C:/codes/tensorflow/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash.exe failed: error executing command\r\n>   cd C:/users/daido/_bazel_daido/arhjsgmd/execroot/org_tensorflow\r\n>   SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n>     SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n>     SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin\r\n>     SET PYTHON_BIN_PATH=C:/Program Files/Python36/python.exe\r\n>     SET PYTHON_LIB_PATH=C:/Program Files/Python36/lib/site-packages\r\n>     SET TF_CUDA_CLANG=0\r\n>     SET TF_CUDA_COMPUTE_CAPABILITIES=5.0\r\n>     SET TF_CUDA_VERSION=10.0\r\n>     SET TF_CUDNN_VERSION=7\r\n>     SET TF_NEED_CUDA=1\r\n>     SET TF_NEED_OPENCL_SYCL=0\r\n>     SET TF_NEED_ROCM=0\r\n>   C:/msys64/usr/bin/bash.exe bazel-out/x64_windows-opt/genfiles/tensorflow/tf_python_api_gen_v1.genrule_script.sh\r\n> Execution platform: @bazel_tools//platforms:host_platform\r\n> Traceback (most recent call last):\r\n>   File \"\\\\?\\C:\\Users\\daido\\AppData\\Local\\Temp\\Bazel.runfiles__c2da756\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"\\\\?\\C:\\Users\\daido\\AppData\\Local\\Temp\\Bazel.runfiles__c2da756\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"\\\\?\\C:\\Users\\daido\\AppData\\Local\\Temp\\Bazel.runfiles__c2da756\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 243, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: The specified module could not be found.\r\n> ```\r\n\r\nI also have the same error with rtx 2070\r\nwindows 10 home\r\ntensorflow 1.13\r\ncuda 10.1\r\ncudnn 7.5\r\nbazel 0.21.0", "Can we close this?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25552\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25552\">No</a>\n"]}, {"number": 25551, "title": "No TF import library to link custom operators on Windows", "body": "I cannot reopen the issue here: https://github.com/tensorflow/tensorflow/issues/23740\r\n\r\nBut the issue persists, the file is still missing in the pip package as of today in TensorFlow 1.12.\r\n\r\nCould you please reintroduce the library _pywrap_tensorflow_internal.lib in the pip pickage ? It is required to build custom ops on Windows platform.\r\n\r\nThanks,\r\n\r\n", "comments": ["@meteorcloudy Can you PTAL? Thanks!", "@evgal This issue is fixed by https://github.com/tensorflow/tensorflow/commit/4beea30ec8a231a12b9183f631608a6af64b8172, but it will only be released in 1.13\r\n\r\nYou can also cherry-pick this commit to build your own TF pip package that contains the import library.", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25551)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25551)\r\n"]}, {"number": 25550, "title": "TF Keras merge_test missing test cases add", "body": "1- Subtract api test case\r\n2- compute_mask api test case\r\n3- Add, Subtract, Concatenate invalif test case", "comments": []}, {"number": 25549, "title": "input_shapes error converting model to tflite", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra\r\n- TensorFlow installed from (source or binary): over pip\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): 0.22.0\r\n\r\n\r\n\r\nI get an error when converting a modified mobilenet_v1 (posenet to be exact)  frozen graph (.pb) with `input_shapes={'image':[1,225,225,3]} `to tflite.\r\nWhen changing it to [1,224,224,3] it works.\r\nI am using the function `tf.contrib.lite.TocoConverter.from_frozen_graph()`\r\n\r\nThe error is:\r\n```\r\nI tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 213 operators, 318 arrays (0 quantized)\\n2019-02-06 13:28:17.720907: \r\nI tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 213 operators, 318 arrays (0 quantized)\\n2019-02-06 13:28:17.732215: \r\nF tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:991]  Check failed: height_with_paddings % block_height == 0 (1 vs. 0)\\n'\r\nNone\r\n```", "comments": ["See https://github.com/tensorflow/tensorflow/issues/23600 for a possible dupe.", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25549)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25549)\r\n", "Hi @gustavz, Were you able to solve this issue ? \r\nI am also getting this error. I can convert with 224 but the accuracy is greatly affected."]}]