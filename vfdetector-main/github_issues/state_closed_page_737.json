[{"number": 31461, "title": "Adding 3D Conv and Deconv TensorRT acceleration, with unit tests", "body": "I add 3D node conversion for Tensorflow-TensorRT (TF-TRT), so inference of 3D CNN's are accelerated using TensorRT. Supported nodes are Conv3D and Conv3DBackpropInputV2, which enables convolution and deconvolution respectively. Associated unit tests are included. ", "comments": ["@aaroey I've made the changes requested. Can you please review?"]}, {"number": 31460, "title": "[ROCm] Unit-test updates for the ROCm CSB build", "body": "This PR is a supplement to PR #31454. It contains updates to the unit-tests that will be run as part of the ROCm CSB build. \r\n\r\nThe commits mostly update the unit-test to either \r\n * add ROCm specific code so that theyrun correctly when in ROCm mode, or \r\n * disable (only in ROCm mode) subtests which test functionality which is not yet supported in ROCm mode.\r\n\r\n\r\n@gunan please review / approve this PR and #31454 \r\n\r\nthanks\r\n\r\n-----------------------------------------------------------------------------\r\n\r\n@tatianashp @whchung @parallelo @gunan @chsigg \r\n", "comments": ["re-based PR to fix the buildifer check errors...@whchung, please re-approve..thanks"]}, {"number": 31459, "title": "Removed duplicated registration of LessEqual with bfloat16", "body": "This is a follow up PR to #30479 where the duplicated registration of\r\nLess with bfloat16 has been removed, but not LessEqual\r\n\r\nThis fix fixed duplicate bfloat16 entry of LessEqual as well.\r\n\r\nThis fix is related to #30476.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 31458, "title": "tf.metrics reset_state() not called when executing in graph mode", "body": "`tf.keras.metrics.*` are reset using the method `.reset_states()`, however when executed in graph mode the method isn't called and the metric isn't reset.\r\n\r\nExample to reproduce:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\nmean = tf.metrics.Mean()\r\n\r\ndef eager_working():\r\n    \r\n    mean(1.)\r\n    mean.reset_states()\r\n    mean(2.)\r\n    \r\n    tf.print(mean.result())\r\n\r\n@tf.function\r\ndef graph_not_working():\r\n    \r\n    mean(1.)\r\n    mean.reset_states()\r\n    mean(2.)\r\n    \r\n    tf.print(mean.result())\r\n\r\n@tf.function\r\ndef graph_working():\r\n    mean(1.)\r\n    mean.total.assign(0)\r\n    mean.count.assign(0)\r\n    mean(2.)\r\n    \r\n    tf.print(mean.result())\r\n\r\n\r\neager_working()\r\nmean.reset_states()\r\ngraph_not_working()\r\nmean.reset_states()\r\ngraph_working()\r\n```\r\n\r\nOutput:\r\n```\r\n2.0.0-beta1\r\n2\r\n1.5\r\n2\r\n```", "comments": ["Issue replicating with TF version 2.0beta, kindly find the [gist](https://colab.sandbox.google.com/drive/1qB1JOy9v5IW-zbgRCiMS795e2Dmj92lc#scrollTo=fuCUdASHo_t7) of colab.", "I can confirm the same bug in TF version 2.1. It behaves rather strangely - the first two functions work but the last does not.\r\n```\r\n@tf.function\r\ndef working1():\r\n    mean.reset_states()\r\n\r\n@tf.function\r\ndef working2():\r\n    mean(1)\r\n\r\n@tf.function\r\ndef not_working():\r\n    mean(1)\r\n    mean.reset_states()\r\n```", "This seems to be fixed in tf-nightly-2.2.0a20200221", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31458\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31458\">No</a>\n"]}, {"number": 31457, "title": "let jni BUILD defs be publicly visible", "body": "The tensorflow jni BUILD is not publicly visible, so external projects trying to BUILD custom tflite clients that require a jni are not able to use it.\r\n\r\nFor example, with this change, [the smartreply demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/models/smartreply) would be able to move to [tensorflow/examples](https://github.com/tensorflow/examples), which I think is a more fitting location for it.", "comments": ["Hi Richard, we don't need to make the whole package public, only the necessary part public should meet your requirement?", "@renjie-liu thank you for the suggestion. I have opened a new PR (#31466) changing it so that only the cc_library def has its visibility changed."]}, {"number": 31456, "title": "Use \"correct\" op in while_v2 gradient", "body": "PiperOrigin-RevId: 262251730", "comments": []}, {"number": 31455, "title": "python for loops in eager model yield expected results for  keras model.predict but not for saved model with multiple outputs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): NA\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): na\r\n- GCC/Compiler version (if compiling from source): na\r\n- CUDA/cuDNN version: na\r\n- GPU model and memory: na\r\n\r\n**Describe the current behavior**\r\ntf.keras Model's predict method prediction differs from tf.saved_model's prediction\r\n**Describe the expected behavior**\r\n`tf.saved_model.save()` should either fail to serialize, or should yield correct predictions on reload.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nprint(tf.__version__)\r\nfrom tensorflow.keras.layers import Input, Dense, Lambda\r\nfrom tensorflow.keras.models import Model\r\nimport numpy as np\r\n\r\nin0 = Input(shape=(1,), dtype=\"float32\", name='my_input_0')\r\nin1 = Input(shape=(1,), dtype=\"float32\", name='my_input_1')\r\nconcatted = Lambda(lambda inputs: tf.concat(inputs, axis=-1))([in0,in1])\r\n\r\noutputs = Dense(3)(concatted)\r\n\r\n#------------- way 1 (does not work)-----------------\r\n\r\n# outs = [Lambda(lambda outputs: outputs[...,i], name=f'output_{i}')(outputs) for i in range(3)]\r\n\r\n#------------- way 2 (does not work)-----------------\r\n\r\nouts = []\r\nfor i in range(3):\r\n  outs.append(Lambda(lambda outputs: outputs[...,i], name=f'output_{i}')(outputs))\r\n\r\n#------------- way 3 (does work)-----------------\r\n\r\n# out0 = Lambda(lambda outputs: outputs[...,0], name='my_output_0')(post_process)\r\n# out1 = Lambda(lambda outputs: outputs[...,1], name='my_output_1')(post_process)\r\n# out2 = Lambda(lambda outputs: outputs[...,2], name='my_output_2')(post_process)\r\n# outs=[out0, out1, out2]\r\n\r\n#----------------------------\r\n\r\nmy_model = Model(inputs=[in0,in1], outputs=outs)\r\ntf.keras.backend.learning_phase = 0\r\n\r\nmy_model.predict([np.array([[.5],[.3]]), np.array([[-.1],[.2]])])\r\n```\r\n\r\nyields\r\n```\r\n1.14.0\r\n[array([0.13002533, 0.03461001], dtype=float32),\r\n array([-0.45988005, -0.3892737 ], dtype=float32),\r\n array([-0.28218567, -0.14224575], dtype=float32)]\r\n```\r\nOn the other hand,\r\n```python\r\ntf.saved_model.save(my_model, './mymodel')\r\nreloaded = tf.saved_model.load_v2('./mymodel')\r\nsig = reloaded.signatures['serving_default']\r\nsig(my_input_0=tf.constant(np.array([[.5],[.3]]), dtype=tf.float32), my_input_1=tf.constant(np.array([[-.1],[.2]]), dtype=tf.float32))\r\n```\r\nyields\r\n```\r\n{'output_0': <tf.Tensor: id=316, shape=(2,), dtype=float32, numpy=array([-0.28218567, -0.14224575], dtype=float32)>,\r\n 'output_1': <tf.Tensor: id=317, shape=(2,), dtype=float32, numpy=array([-0.28218567, -0.14224575], dtype=float32)>,\r\n 'output_2': <tf.Tensor: id=318, shape=(2,), dtype=float32, numpy=array([-0.28218567, -0.14224575], dtype=float32)>}\r\n```\r\ntry it for yourself here:\r\nhttps://colab.research.google.com/drive/1x1eDTl-nMuhZTQYWSi_I9F_SguFZRP2i\r\n\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Can you try on a nightly? I think there was a similar issue fixed not too long ago with named multi-output models.", "It appears to still be broken in 1.15.0-dev20190815\r\n\r\nhttps://colab.research.google.com/drive/1x1eDTl-nMuhZTQYWSi_I9F_SguFZRP2i", "I see, the difference is just that saving using `Model.__call__`, which has the behavior you describe. So it's not a saving-specific issue; you can reproduce the behavior using `print(my_model([np.array([[.5],[.3]]), np.array([[-.1],[.2]])]))` before saving. Why that's different than predict() I don't know; I'll reassign to someone who works on Keras.", "Tom, please see last comment. Could you take a look or redirect?", "@mckeown12 \r\nIs this still an issue ", "Yes, it is still an issue.\r\n\r\nIn 1.x that behaves pretty much as the OP.\r\nhttps://colab.research.google.com/drive/1x1eDTl-nMuhZTQYWSi_I9F_SguFZRP2i\r\n\r\nAs of 2.2 it appears that `model.predict` and `model.__call__` have the same behavior, but way 2 from the OP still gives unexpected results which are different from way 3.\r\nhttps://colab.research.google.com/drive/1m9xYRcOmfi2XHxL89ySYAtTtGCc5eth3\r\n\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31455\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31455\">No</a>\n"]}, {"number": 31454, "title": "[ROCm] Script updates for the ROCm CSB build", "body": "This PR contains commits that add / update scripts required to setup the ROCm Community Supported Build (CSB).\r\n\r\n@gunan , please review the `tensorflow/tools/ci_build/linux/rocm/run_csb_tests.sh`. It is based on your recommendation of the bazel test command to use, and will be the script we use for our CSB.\r\n\r\n------------------------------------------------\r\n\r\n@tatianashp @whchung @parallelo @gunan \r\n\r\n\r\n", "comments": ["@gunan, gentle ping.   please approve PR. thanks", "The errors in \"Ubuntu Sanity\" CI run, do not seem to be related to any changes in this PR. \r\n\r\nLet me know if there is anything that needs to be done on my end to resolve them. \r\n\r\nthanks "]}, {"number": 31453, "title": "[ROCm] Adding RCCL support", "body": "This PR adds ROCm RCCL support.\r\n\r\nThis PR also adds a general performance improvement for NCCL/RCCL integration where the NcclManager records and waits on an Event as each Participant is added, rather than synchronizing with the tensor stream only after all Participants have been added.  Otherwise, most compute kernels are added to the compute stream prior to the NCCL/RCCL sync Event, delaying the start of the collective.\r\n\r\nThis PR also adds ROCm-specific performance improvements.  Specifically, this adds the nccl stream as a member of the StreamGroup rather than delaying the creation of the NCCL stream until NCCL is first used in the NcclManager.  This allows the compute and nccl stream to be force-initialized as immediate siblings.\r\n", "comments": ["Hi Jeff, I like your changes. Would it be possible without too much extra work to split them up though? Adding RCCL, creating the NCCL stream early, and increasing concurrency with the use of events seem like individual bits to me.\r\n\r\nWe have had our share of issues with NCCL recently (and its TF integration) and our testing coverage is not ideal, so I'm a little more cautious. Thanks for your understanding.", "@chsigg, as requested, here are the three new PRs:\r\n1. increasing concurrency: https://github.com/tensorflow/tensorflow/pull/31481\r\n2. create NCCL stream early: https://github.com/tensorflow/tensorflow/pull/31483\r\n3. add RCCL: https://github.com/tensorflow/tensorflow/pull/31485"]}, {"number": 31452, "title": "Changed example showing gradient processing", "body": "The original example was processing the gradients twice.\r\n1st: grads_and_vars = zip(processed_grads, var_list)\r\n2nd: capped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]\r\n\r\nThe 2nd line is especially weird because it unnecessarily zips the gradients with the var_list even though it is only processing the gradient part.\r\n\r\nRefactored the example to be clearer, now there is only a single line that processes the gradients.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31452) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31452) for more info**.\n\n<!-- ok -->"]}, {"number": 31451, "title": "Error when selecting 1 GPU + TensorBoard: \"(...) XLA_GPU_JIT device number 0\"", "body": "On TensorFlow 1.14 (OS Ubuntu 16.04), when I call fit() of a tf.Keras model using TensorBoard as one of the tf.Keras.callbacks, and selecting one GPU to use prior training, it fails with:\r\n\r\n```\r\n(...)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py \r\nin ensure_initialized(self)\r\n    410         if self._execution_mode == ASYNC:\r\n    411           pywrap_tensorflow.TFE_ContextOptionsSetAsync(opts, True)\r\n--> 412         self._context_handle = pywrap_tensorflow.TFE_NewContext(opts)\r\n    413       finally:\r\n    414         pywrap_tensorflow.TFE_DeleteContextOptions(opts)\r\n\r\nInvalidArgumentError: Invalid device ordinal value (1). Valid range is [0, 0].\r\n\twhile setting up XLA_GPU_JIT device number 1\r\n```\r\n\r\nIf I suppress the callbacks argument in fit(), the training works.  The code to reproduce follows:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.utils import to_categorical\r\nfrom tensorflow.keras.callbacks import TensorBoard\r\n\r\ngpu_id = 0\r\nsess_config = tf.compat.v1.ConfigProto()\r\nsess_config.gpu_options.allow_growth = True\r\nsess_config.gpu_options.visible_device_list = '{}'.format(gpu_id)\r\nsess = tf.compat.v1.Session(config=sess_config)\r\ntf.compat.v1.keras.backend.set_session(sess)\r\n\r\n(Xtr, Ytr), (Xva, Yva) = tf.keras.datasets.cifar10.load_data()\r\nXtr, Ytr, Xva, Yva, nc = Xtr[:1000], Ytr[:1000], Xva[:100], Yva[:100], 10\r\nXtr, Xva = Xtr.astype('float32') / 255, Xva.astype('float32') / 255\r\nYtr, Yva, ins = to_categorical(Ytr, nc), to_categorical(Yva, nc), Xtr.shape[1:]\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Conv2D(8, (3, 3), input_shape=ins, activation='relu'))\r\nmodel.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(tf.keras.layers.Dropout(0.25))\r\nmodel.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(tf.keras.layers.Dropout(0.25))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(32, activation='relu'))\r\nmodel.add(tf.keras.layers.Dropout(0.5))\r\nmodel.add(tf.keras.layers.Dense(nc, activation='softmax'))\r\nopt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])\r\n\r\nl_cb = [TensorBoard(log_dir='./tb_logs/cur', batch_size=32, write_graph=False)]\r\n\r\nmodel.fit(x=Xtr, y=Ytr, batch_size=32, epochs=100, callbacks=l_cb,\r\n          validation_data=(Xva, Yva), shuffle='batch')\r\n```", "comments": ["@andmax \r\n\r\nI tried executing the code on Colab with Tensorflow-gpu 1.14.0 but it executed without any error. Can you try once let us know is this still an issue. Thanks!\r\n", "Hi @ravikyram  I think the problem happens when you have more than 1 GPU available in the host system and you try to select one by set_session() ConfigProto().  This works fine without the TensorBoard callback, but when I include this callback it breaks with the GPU JIT error.", "Hey @ravikyram, any update on this?  This is causing issues for users who wish to run distributed training with Horovod using `tf.keras` in TensorFlow 1.14.\r\n\r\nI was able to repro by running with more than one GPU available and setting `visible_device_list` and adding the `TensorBoard` callback.", "Same issue for me here. Adding the TensorBoard callback to runner 0 when running with Horovod causes the issue described here. ", "Same issue. tf 1.14, horovod training and running tensorboard only on rank 0.\r\nWorkaround that worked for me without disabling tensorboard, was to disable profiling on tensorboard:\r\n\r\n`keras.callbacks.TensorBoard(log_dir=logdir, profile_batch=0)`\r\n", "Hi @shashankprasanna  help me understand this profile_batch=0 option: does it mean the tensorboard will not follow the training (when you say \"disable profiling\")?  Then why should we use the tensorboard callback in the first place?  For me it seems that using profile_batch=0 is the same as not using the tensorboard callback at all.", "@andmax - I didn't need the profiling capability of tensorboard, just the performance curves and for visualizing the graph which works with this workaround. Tensorboard also offers profiling: [TensorBoard Profile](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras), to profile runtime of every op (traceviewer screenshots in link above). I currently don't need this.", "Hi @shashankprasanna  thanks for explaining this!  Much appreciated, I also don't need this type of profiling.", "This issue has been fixed in TF 1.15 and 2.0.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31451\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31451\">No</a>\n"]}, {"number": 31450, "title": "(Resubmission) Improve Flatten to avoid using dynamic shapes when possible", "body": "Fixes issues @ebrevdo pointed out with #30380 \r\nI also added a unit test for the int32 overflow situation.\r\nSee original PR for explanation of overall changes.", "comments": ["Thanks for fixing!", "Thanks for the improvements @ebrevdo . Let me know if you have any ideas to shorten the code since it's gotten a bit long now.", "Hey @ebrevdo it looks like the PR was reverted again?", "yes :(  for some reason the shape inference code on the *python* side is\ntruncating in *windows* (this affects the unit test).  i'll follow up with\nour internal team on how we can replicate this in windows.  if you have a\nwindows box, let us know as you may be able to do some debugging by just\nrunning the unit test with your PR in.\n\nOn Mon, Aug 12, 2019 at 1:54 PM Trevor Morris <notifications@github.com>\nwrote:\n\n> Hey @ebrevdo <https://github.com/ebrevdo> it looks like the PR was\n> reverted again?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/31450?email_source=notifications&email_token=AANWFG3K7VB3YDX27WAEPUTQEHEZJA5CNFSM4IKMVIM2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4DZOLI#issuecomment-520591149>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AANWFG22UYA3EW6PVITPI33QEHEZJANCNFSM4IKMVIMQ>\n> .\n>\n", "My suggestion for now is to disable the unit test in windows; easiest way to check:\r\n\r\nimport platform\r\nis_windows = any(platform.win32_ver())\r\n\r\nand self.skipTest if is_windows.\r\n", "@mrry know who might have time to figure out why shape inference code (either the c++ or swig) is truncating int32 values in windows?  i tried debugging it but ran into too many issues with the tooling.", "@trevor-m sorry for the delay.  i'm hoping that final change will allow this PR to stick :)", "ebrevdo@- I'm not clear on what you are suggesting. Should we disable the unit test on windows? Or are you saying you've uncovered a different path to make the PR stick?", "disabling the unit test on windows is the best choice right now."]}, {"number": 31449, "title": "GPU race conditions from `tf.map_fn` ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): All the code that caused this issue uses Tensorflow/Keras operations.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0/7.6.2\r\n- GPU model and memory: RTX 6000x2, 48 GB\r\n\r\n**Describe the current behavior**\r\n\r\nI've created a custom layer called `ROI` in Keras that uses `tf.map_fn`, [precisely because it has unknown parameter that it needs to take as tensor object](https://github.com/keras-team/keras/issues/12139). \r\n\r\nThis layer works perfectly on CPU inference and training, it also works perfectly on GPU during inference. But during training with a powerful GPU, exception about GPU colocation of `ROI` layer occurs:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation ROI/map/while/Identity_1: Could not satisfy explicit device specification '' because the node node ROI/map/while/Identity_1 (defined at /path/to/custom/layer/custom.py:70) placed on device No device assignments were active during op 'ROI/map/while/Identity_1' creation.\r\n```\r\n\r\n```\r\n[[node ROI/map/while/Identity_1 (defined at /path/to/custom/layer/custom.py:70) Additional information about colocations: No node-device colocations were active during                                                                                                                                                              op 'ROI/map/while/Identity_1' creation.\r\nNo device assignments were active during op 'ROI/map/while/Identity_1' creation.`\r\n```\r\n\r\nManual colocation of `ROI` layer to CPU device with `tf.device` worked, but I want `ROI` to support GPU as well.\r\n\r\n**My hypothesis**\r\n\r\n`ROI` layer works on CPU because only single core at a time should handle the layer - even if multiprocessing is activated, there are few cores slowly balancing the task. \r\n\r\nBut whenever GPU is utilized, thousands of cores are working together in parallel and they are not waiting for each other to finish their tasks. Thus one of the processes tries to gather data from `TensorArray` that is still in while loop (using `tf.map_fn`), which causes the error.\r\n\r\n**Describe the expected behavior**\r\n\r\nTensorflow should be able to handle these race conditions by waiting for its own `tf.map_fn` to be finished instead of raising exceptions. \r\n\r\n**Code to reproduce the issue**\r\n\r\n[This is the code\r\n](https://gist.github.com/ShellRox/a1c11565ed53f219bc1033164947aa72) that instantly causes the mentioned issue on my local machine.\r\n\r\n**Other info / logs**\r\n\r\nFull Log:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/path/to/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/path/to/site-packages/tensorflow/python/client/session.py\", line 1339, in _run_fn\r\n    self._extend_graph()\r\n  File \"/path/to/site-packages/tensorflow/python/client/session.py\", line 1374, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation ROI/map/while/Identity_1: Could not satisfy explicit device specification '' because the node {{colocation_node ROI/map/while/Identity_1}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:GPU:0].\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nStridedSliceGrad: GPU CPU XLA_CPU XLA_GPU\r\nNextIteration: GPU CPU XLA_CPU XLA_GPU\r\nMul: GPU CPU XLA_CPU XLA_GPU\r\nEqual: GPU CPU XLA_CPU XLA_GPU\r\nDynamicStitch: GPU CPU XLA_CPU XLA_GPU\r\nFill: GPU CPU XLA_CPU XLA_GPU\r\nFloorMod: GPU CPU XLA_CPU XLA_GPU\r\nShape: GPU CPU XLA_CPU XLA_GPU\r\nReshape: GPU CPU XLA_CPU XLA_GPU\r\nTensorArrayReadV3: GPU CPU XLA_CPU XLA_GPU\r\nTensorArrayScatterV3: GPU CPU XLA_CPU XLA_GPU\r\nTensorArraySizeV3: GPU CPU XLA_CPU XLA_GPU\r\nConst: GPU CPU XLA_CPU XLA_GPU\r\nTensorArrayWriteV3: GPU CPU XLA_CPU XLA_GPU\r\nIdentity: GPU CPU XLA_CPU XLA_GPU\r\nGreaterEqual: GPU CPU XLA_CPU XLA_GPU\r\nExit: GPU CPU XLA_CPU XLA_GPU\r\nCast: GPU CPU XLA_CPU XLA_GPU\r\nControlTrigger: GPU CPU XLA_CPU XLA_GPU\r\nTensorArrayGradV3: GPU CPU XLA_CPU XLA_GPU\r\nPack: GPU CPU XLA_CPU XLA_GPU\r\nEnter: GPU CPU XLA_CPU XLA_GPU\r\nTensorArrayV3: GPU CPU XLA_CPU XLA_GPU\r\nMerge: GPU CPU XLA_CPU XLA_GPU\r\nStackV2: GPU CPU XLA_CPU XLA_GPU\r\nRange: GPU CPU XLA_CPU XLA_GPU\r\nTensorArrayGatherV3: GPU CPU XLA_CPU XLA_GPU\r\nStackPushV2: GPU CPU XLA_CPU XLA_GPU\r\nSwitch: GPU CPU XLA_CPU XLA_GPU\r\nRealDiv: GPU CPU XLA_CPU XLA_GPU\r\nAdd: GPU CPU XLA_CPU XLA_GPU\r\nStridedSlice: GPU CPU XLA_CPU XLA_GPU\r\nMax: GPU CPU XLA_CPU XLA_GPU\r\nLoopCond: GPU CPU XLA_CPU XLA_GPU\r\nSum: GPU CPU XLA_CPU XLA_GPU\r\nStackPopV2: GPU CPU XLA_CPU XLA_GPU\r\nSub: GPU CPU XLA_CPU XLA_GPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  ROI/map/TensorArray_2 (TensorArrayV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\r\n  ROI/map/while/Identity_1 (Identity)\r\n  ROI/map/while/map/TensorArray_1 (TensorArrayV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\r\n  ROI/map/while/map/while/Identity_1 (Identity)\r\n  ROI/map/while/map/while/strided_slice_4/stack (Pack)\r\n  ROI/map/while/map/while/strided_slice_4/stack_1 (Pack)\r\n  ROI/map/while/map/while/strided_slice_4 (StridedSlice)\r\n  ROI/map/while/map/while/Max (Max)\r\n  ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3/Enter (Enter)\r\n  ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3 (TensorArrayWriteV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\r\n  ROI/map/while/map/while/Exit_2 (Exit)\r\n  ROI/map/while/map/TensorArrayStack/TensorArraySizeV3 (TensorArraySizeV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\r\n  ROI/map/while/map/TensorArrayStack/range/start (Const)\r\n  ROI/map/while/map/TensorArrayStack/range/delta (Const)\r\n  ROI/map/while/map/TensorArrayStack/range (Range)\r\n  ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3 (TensorArrayGatherV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\r\n  ROI/map/while/TensorArrayWrite/TensorArrayWriteV3/Enter (Enter)\r\n  ROI/map/while/TensorArrayWrite/TensorArrayWriteV3 (TensorArrayWriteV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\r\n  ROI/map/TensorArrayStack/TensorArraySizeV3 (TensorArraySizeV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\r\n  ROI/map/TensorArrayStack/range/start (Const)\r\n  ROI/map/TensorArrayStack/range/delta (Const)\r\n  ROI/map/TensorArrayStack/range (Range)\r\n  ROI/map/TensorArrayStack/TensorArrayGatherV3 (TensorArrayGatherV3)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\r\n  training/MultiplierWrapper/gradients/f_count_3 (Const)\r\n  training/MultiplierWrapper/gradients/f_count_4 (Enter)\r\n  training/MultiplierWrapper/gradients/Merge_2 (Merge)\r\n  training/MultiplierWrapper/gradients/Switch_2 (Switch)\r\n  training/MultiplierWrapper/gradients/Add_1/y (Const)\r\n  training/MultiplierWrapper/gradients/Add_1 (Add)\r\n  training/MultiplierWrapper/gradients/f_count_5 (Exit)\r\n  training/MultiplierWrapper/gradients/Const (Const)\r\n  training/MultiplierWrapper/gradients/f_acc (StackV2)\r\n  training/MultiplierWrapper/gradients/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/StackPushV2 (StackPushV2)\r\n  training/MultiplierWrapper/gradients/StackPopV2/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/StackPopV2 (StackPopV2)\r\n  training/MultiplierWrapper/gradients/b_count_4 (Const)\r\n  training/MultiplierWrapper/gradients/b_count_5 (Enter)\r\n  training/MultiplierWrapper/gradients/Merge_3 (Merge)\r\n  training/MultiplierWrapper/gradients/GreaterEqual_1/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/GreaterEqual_1 (GreaterEqual)\r\n  training/MultiplierWrapper/gradients/b_count_6 (LoopCond)\r\n  training/MultiplierWrapper/gradients/Switch_3 (Switch)\r\n  training/MultiplierWrapper/gradients/Sub_1 (Sub)\r\n  training/MultiplierWrapper/gradients/b_count_7 (Exit)\r\n  training/MultiplierWrapper/gradients/ROI/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 (TensorArrayGradV3)\r\n  training/MultiplierWrapper/gradients/ROI/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/gradient_flow (Identity)\r\n  training/MultiplierWrapper/gradients/ROI/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3 (TensorArrayScatterV3)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3 (TensorArrayGradV3)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/gradient_flow (Identity)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/Const (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc (StackV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPushV2 (StackPushV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2 (StackPopV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3 (TensorArrayReadV3)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/Const (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/f_acc (StackV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPushV2 (StackPushV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2 (StackPopV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/Const_1 (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/f_acc_1 (StackV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPushV2_1 (StackPushV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2_1/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2_1 (StackPopV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 (TensorArrayGradV3)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/gradient_flow (Identity)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3/Const (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3/f_acc (StackV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3/StackPushV2 (StackPushV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3/StackPopV2/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3/StackPopV2 (StackPopV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3 (TensorArrayScatterV3)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Exit_2_grad/b_exit (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/Const (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/f_acc (StackV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPushV2 (StackPushV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2 (StackPopV2)\r\n  training/MultiplierWrapper/gradients/b_sync (ControlTrigger)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/TensorArrayGradV3 (TensorArrayGradV3)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayGrad/gradient_flow (Identity)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/Const (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc (StackV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/Enter_1 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPushV2 (StackPushV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2/Enter_1 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/StackPopV2 (StackPopV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3 (TensorArrayReadV3)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Shape (Shape)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Size (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/add/Const (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/add (Add)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/mod (FloorMod)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Shape_1 (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/range/start (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/range/delta (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/range (Range)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Fill/value (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Fill (Fill)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/Const (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/f_acc (StackV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/Enter_1 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/StackPushV2 (StackPushV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/StackPopV2/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/StackPopV2/Enter_1 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch/StackPopV2 (StackPopV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/DynamicStitch (DynamicStitch)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/Const (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/f_acc (StackV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/Enter_1 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/StackPushV2 (StackPushV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/StackPopV2/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/StackPopV2/Enter_1 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape/StackPopV2 (StackPopV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape (Reshape)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape_1 (Reshape)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/Const (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/f_acc (StackV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/Enter_1 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/StackPushV2 (StackPushV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/StackPopV2/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/StackPopV2/Enter_1 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal/StackPopV2 (StackPopV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Equal (Equal)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Cast (Cast)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Sum (Sum)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/Reshape_2 (Reshape)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/truediv (RealDiv)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/Max_grad/mul (Mul)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/Shape (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/Const (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/f_acc (StackV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/Enter_1 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPushV2 (StackPushV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPopV2/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPopV2/Enter_1 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPopV2 (StackPopV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/Const_1 (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/f_acc_1 (StackV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/Enter_2 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/Enter_3 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPushV2_1 (StackPushV2)\r\n  training/MultiplierWrapper/gradients/NextIteration_2 (NextIteration)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPopV2_1/Enter (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPopV2_1/Enter_1 (Enter)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/StackPopV2_1 (StackPopV2)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/b_sync (ControlTrigger)\r\n  training/MultiplierWrapper/gradients/NextIteration_3 (NextIteration)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad/Const_2 (Const)\r\n  training/MultiplierWrapper/gradients/ROI/map/while/map/while/strided_slice_4_grad/StridedSliceGrad (StridedSliceGrad)\r\n\r\n         [[{{node ROI/map/while/Identity_1}}]]\r\n```", "comments": ["Could not reproduce the issue with normal CPU and GPU run-time, also not able to reproduce issue for the GPU model mentioned.", "We are working on reproducing it through a Google Colab notebook to share with you. Not sure if the issue appears only with powerful multi GPUs or also with average GPUs like the default K80 from colab. We will share updates ASAP @oanush, @jvishnuvardhan ", "# Update\r\n\r\nTried to implement the slightly altered code to Google Colab and couldn't replicate the issue. Then tried the same slightly altered code on my machine and surprisingly the issue still couldn't be replicated. **Only after** I pre-loaded the weights before training - the issue immediately re-occurred  (on my local machine, and not colab).\r\n\r\nThus there are two prerequisites to replicate this issue:\r\n\r\n- Load **arbitrary** of `.h5` weights before compiling the network with `ROI` layer implemented within.\r\n- Execute the training script on a powerful GPU. K80 from Colab fails to replicate this issue for some reason, but I've tested it on RTX 6000x2, P5000, P4000 and GTX 960 and they successfully replicate the issue if condition in the first bullet point is satisfied (although GTX 960 might fail sometimes when it's overloaded).\r\n\r\nBy the way, I highlighted word **arbitrary** in the first bullet point because this happens for any weight. Thus you can simply train the network, save its weights and then try training again by pre-loading weights with `model.load_weights` before `model.compile`.\r\n\r\n---\r\n\r\nCurrently I'm minimizing the code as much as possible, and I believe I'll be able to send it tomorrow. I think it'll make it easier to replicate this issue (although it is important to consider the usage of powerful GPU). I'll be probably able to post minimized code by tomorrow.\r\n\r\nOtherwise, I'm confident that satisfying mentioned prerequisites will replicate this issue.\r\n\r\nThank you for the response! @oanush @jvishnuvardhan ", "> Could not reproduce the issue with normal CPU and GPU run-time, also not able to reproduce issue for the GPU model mentioned.\r\n\r\nI've been able to construct a minimal code that causes this issue instantly on my local machine, [you can see it here](https://gist.github.com/ShellRox/a1c11565ed53f219bc1033164947aa72) (I've also updated the issue with this link). Please make sure to execute this script **twice**, i.e `python GPU_ROI_issue.py;python GPU_ROI_issue.py` so that it saves the weights after training and then tries to train with loaded weights which causes the error mentioned in this issue. \r\n\r\nAlso as mentioned earlier, please make sure that this code is executed on efficient GPU, since from my experience it is only caused on them.\r\n\r\nThank you!", "# Update\r\n\r\nNo fix is yet found for training a model containing layer that uses `tf.map_fn` on GPU with pre-loaded weights. As soon as any weights of any layers are manually set (i.e with `layer.set_weights` or `model.load_weights`), which uses `batch_set_value` to pass them into the `feed_dict` to a symbolic callable function that Keras creates (but kernel/bias initializers of layers work perfectly though).\r\n\r\nPerhaps `channels_first` (NCHW) setting of layers might have something to do with it, considering that Tensorflow has a default channel order of NHWC and therefore is probably optimized for that exact format.\r\n\r\nThe only workaround with `tf.map_fn` is, by colocating `ROIPoolingLayer` on CPU device with `tf.device(\"CPU:0\")`, but this is obviously not an optimal solution (considering that GPU support is highly significant for efficiency).\r\n\r\nAnother obvious workaround is using vectorized operations only, but this might not be an easy task for such layer without hardcoding.", "@oanush @jvishnuvardhan were you able to reproduce the issue?\r\n\r\n> > Could not reproduce the issue with normal CPU and GPU run-time, also not able to reproduce issue for the GPU model mentioned.\r\n> \r\n> I've been able to construct a minimal code that causes this issue instantly on my local machine, [you can see it here](https://gist.github.com/ShellRox/a1c11565ed53f219bc1033164947aa72) (I've also updated the issue with this link). Please make sure to execute this script **twice**, i.e `python GPU_ROI_issue.py;python GPU_ROI_issue.py` so that it saves the weights after training and then tries to train with loaded weights which causes the error mentioned in this issue.\r\n> \r\n> Also as mentioned earlier, please make sure that this code is executed on efficient GPU, since from my experience it is only caused on them.\r\n> \r\n> Thank you!\r\n\r\n", "> # Update\r\n> No fix is yet found for training a model containing layer that uses `tf.map_fn` on GPU with pre-loaded weights. As soon as any weights of any layers are manually set (i.e with `layer.set_weights` or `model.load_weights`), which uses `batch_set_value` to pass them into the `feed_dict` to a symbolic callable function that Keras creates (but kernel/bias initializers of layers work perfectly though).\r\n> \r\n> Perhaps `channels_first` (NCHW) setting of layers might have something to do with it, considering that Tensorflow has a default channel order of NHWC and therefore is probably optimized for that exact format.\r\n> \r\n> The only workaround with `tf.map_fn` is, by colocating `ROIPoolingLayer` on CPU device with `tf.device(\"CPU:0\")`, but this is obviously not an optimal solution (considering that GPU support is highly significant for efficiency).\r\n> \r\n> Another obvious workaround is using vectorized operations only, but this might not be an easy task for such layer without hardcoding.\r\n\r\n@oanush @jvishnuvardhan Channel order is not a problem, I've set all layers to `channels_last` (NHWC) order and modified ROI pooling layer to support this change. I've also replaced every `keras` usage with `tf.keras` (to ensure this is not a compatibility issue) - and I can still reproduce this error.\r\n\r\n[This is the code using tf.keras and NHWC layer order that reproduces the issue mentioned in the original post](https://gist.github.com/ShellRox/91bead44f6b476a08b9a3c223629ce4c)", "I can reproduce it with a 1080 Ti.\r\n\r\nThe problem is not a race condition. Part of the constructed graph is not GPU-friendly and it has to be placed on the CPU. The placer manages to work out a good arrangement if the model has not been loaded any weights. Otherwise, something somewhere takes the wrong turn and the placer arrives to a pathological state. \r\n\r\nI tried to compare graph dumps with and without weight load, but they are exceedingly complicated and the reason for the mismatch is not apparent. Although it looks like the run with weight load is missing all the backprop-related nodes until a late point. For example, without weight load, placer_input_2.pbtxt is 325 kb and it has lots of \"training/...\" nodes. With weight load, placer_input_2.pbtxt is 116 kb and it has no \"training\" nodes at all. (They only appear in placer_input_4.pbtxt.) In placer_output_2.pbtxt, the node roi_layer/map/while/TensorArrayWrite/TensorArrayWriteV3 gets assigned to the CPU without weight load and to the GPU with weight load.", "> I can reproduce it with a 1080 Ti.\r\n> \r\n> The problem is not a race condition. Part of the constructed graph is not GPU-friendly and it has to be placed on the CPU. The placer manages to work out a good arrangement if the model has not been loaded any weights. Otherwise, something somewhere takes the wrong turn and the placer arrives to a pathological state.\r\n> \r\n> I tried to compare graph dumps with and without weight load, but they are exceedingly complicated and the reason for the mismatch is not apparent. Although it looks like the run with weight load is missing all the backprop-related nodes until a late point. For example, without weight load, placer_input_2.pbtxt is 325 kb and it has lots of \"training/...\" nodes. With weight load, placer_input_2.pbtxt is 116 kb and it has no \"training\" nodes at all. (They only appear in placer_input_4.pbtxt.) In placer_output_2.pbtxt, the node roi_layer/map/while/TensorArrayWrite/TensorArrayWriteV3 gets assigned to the CPU without weight load and to the GPU with weight load.\r\n\r\nHello, thank you for the attempt of reproduction and debugging info. I think race conditions or some GPU related issues might be precisely caused due to part of constructed graph not being GPU-friendly as you mentioned (I assume this implies `tf.map_fn` compatibility, or at least some part of its code), but I'm not aware of exact issue.\r\n\r\nManual colocation (`tf.device` wrapper) completely fixes this issue by colocating `ROI` graph to CPU - although I don't think this is as efficient as Tensorflow's automatic colocator. But I still find weight loading concept quite perplexing (and its relations with device colocations), thank you very much for the experiment though, it still has very interesting results.", "More information:\r\n\r\nDuring placement, in core/common_runtime/colocation_graph.cc, placer creates node 93:\r\n```\r\n{name:'roi_layer/map/while/map/TensorArray_1' \r\nid:93 \r\nop device:{} \r\ndef:{{{node roi_layer/map/while/map/TensorArray_1}} = TensorArrayV3[clear_after_read=\r\ntrue, dtype=DT_FLOAT, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name=\"\"](roi_layer/map/while/map/strided_slice)}}\r\n```\r\nIt then goes through a series of optimizations and merges, which I haven't quite traced, and, as a result of which, the corresponding Member object ends up with ```requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0'```.\r\n\r\nAt this point, all backprop ops are missing and they aren't added to the graph till much later.\r\n\r\nEventually, it assigns node 588, which is, I think, a gradient of op 93:\r\n```\r\n{name:'training/SGD/gradients/roi_layer/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPushV2' \r\nid:588 \r\nop device:{} \r\ndef:{{{node training/SGD/gradients/roi_layer/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPushV2}} = StackPushV2[T=DT_RESOURCE, _class=[\"loc:@roi_layer/map/while/map/TensorArray_1\"], swap_memory=false](training/SGD/gradients/roi_layer/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, roi_layer/map/while/map/TensorArray_1, ^training/SGD/gradients/Add)}}\r\n```\r\n\r\nBut that one can't be placed on the GPU, because the corresponding GPU kernel seems to be missing. But it has ```_class=[\"loc:@roi_layer/map/while/map/TensorArray_1\"]```, which seems to say that it either can be, or must  be, colocated with 93.\r\n\r\nA while later, placer does colocate 93 and 588.\r\nAnd now we end up with a Member object that was _requested_ to be on the GPU, but can only be _placed_ on the CPU. (Because colocation logic does not check for this possibility.)\r\n\r\nThen, quite a while later, the problem is noticed and reported in a singularly unhelpful error message.\r\n\r\nAt this point I'm not sure if the bug is in the placer or in the savefile loader. It looks like loading the savefile somehow messes up the model. The error seems to go away if I replace \r\n```K.batch_set_value(weight_value_tuples)```\r\nwith direct\r\n```\r\n    symbolic_weights[0].assign(weight_values[0])\r\n    symbolic_weights[1].assign(weight_values[1])\r\n```\r\n\r\nin keras/saving/hdf5_format.py. (Though I'm not sure if it cures the problem or just messes up the loading.)\r\n", "Okay, I think I see the big picture.\r\nWe have a graph containing an operation that can be placed on the GPU, but whose gradient can only be placed on the CPU.\r\n\r\nWhen constructed normally via model.compile/model.fit, it works because the graph is completely constructed in one go.\r\n\r\nWhen we load weights, the graph is partially constructed inside K.batch_set_value, using only a subset of the nodes. Since gradient ops are still missing, the placer has no problem placing everything on the GPU.\r\n\r\nWhen we then try to train, we add the remaining ops, but we retain the device designations made during the partial construction, which are incompatible with the full construction.\r\n\r\nOr something like that. :)\r\n\r\nP.S. And I found where exactly device designations are being retained. Right here:\r\nhttps://github.com/tensorflow/tensorflow/blob/ee16fcac960ae660e0e4496658a366e2f745e1f0/tensorflow/core/common_runtime/graph_execution_state.cc#L243\r\n\r\nAfter commenting out that line as well as these https://github.com/tensorflow/tensorflow/blob/ee16fcac960ae660e0e4496658a366e2f745e1f0/tensorflow/core/common_runtime/direct_session.cc#L1502-L1505 , and then rebuilding, the error goes away!\r\n( Of course, those lines are probably there for a reason and simply axing them in the live repository is probably not the optimal solution ... )", "> Okay, I think I see the big picture.\r\n> We have a graph containing an operation that can be placed on the GPU, but whose gradient can only be placed on the CPU.\r\n> \r\n> When constructed normally via model.compile/model.fit, it works because the graph is completely constructed in one go.\r\n> \r\n> When we load weights, the graph is partially constructed inside K.batch_set_value, using only a subset of the nodes. Since gradient ops are still missing, the placer has no problem placing everything on the GPU.\r\n> \r\n> When we then try to train, we add the remaining ops, but we retain the device designations made during the partial construction, which are incompatible with the full construction.\r\n> \r\n> Or something like that. :)\r\n> \r\n> P.S. And I found where exactly device designations are being retained. Right here:\r\n> https://github.com/tensorflow/tensorflow/blob/ee16fcac960ae660e0e4496658a366e2f745e1f0/tensorflow/core/common_runtime/graph_execution_state.cc#L243\r\n> \r\n> After commenting out that line as well as these\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/ee16fcac960ae660e0e4496658a366e2f745e1f0/tensorflow/core/common_runtime/direct_session.cc#L1502-L1505\r\n> \r\n> , and then rebuilding, the error goes away!\r\n> ( Of course, those lines are probably there for a reason and simply axing them in the live repository is probably not the optimal solution ... )\r\n\r\nThat's very interesting, so subset of regular operations that are temporarily held with `K.batch_set_value` during weight assignment have complete support for GPU - therefore it is assumed that the whole graph, including the complement set containing gradient operations should be colocated on GPU device (however, not all gradient operations in this complement set have GPU kernels). \r\n\r\nI still don't get why automatic weight initializers and `load_weights` use similar graph construction and colocation schemes though, wouldn't it be easier? (perhaps not, but not sure why).\r\n\r\nRegardless, thank you for your findings. I hope there is at least slightly simple general solution for this weird issue.", "> therefore it is assumed that the whole graph, including the complement set containing gradient operations should be colocated on GPU device \r\n\r\nI don't know if it's _assumed_, or simply that no one yet realized this scenario is possible. \r\n\r\n> I still don't get why automatic weight initializers and load_weights use similar graph construction and colocation schemes though, wouldn't it be easier? (perhaps not, but not sure why).\r\n\r\nAs far as I understand, when the graph is initially constructed, weight tensors are provided with initializers. They are told \"if you are asked to execute but you don't have any contents, go to this RNG and pull some values out of it to fill yourself.\" But nothing actually happens until you start the session run inside model.fit().\r\n\r\nWhen you load weights, you explicitly assign contents to the tensors, which requires a separate session run.\r\n\r\nWhat I'm not sure about is why you don't get the complete graph right out of model.compile(), since that would solve the problem too.\r\n\r\nFYI, on a tangential topic, in case you haven't realized. On your system, there are (probably) two separate keras Python source trees, one in \"keras\", and one in \"tensorflow.keras\". They aren't necessarily the same (they are very similar but may have different versions.)  If you pull the code from the tensorflow repository right now and follow the instructions to build it from source, you'll get the new binaries, but your code will still be using the same python code.\r\n\r\nTo use the \"correct\" keras, you need to add \"tensorflow.\" to all import directives.\r\n\r\n```\r\nimport tensorflow.keras as keras\r\nfrom tensorflow.keras import callbacks\r\nimport tensorflow.keras.layers as layers\r\nfrom tensorflow.keras.activations import linear\r\n```\r\netc\r\n\r\nFYI2, for a hacky workaround for this problem that does not involve manually holding the ROI graph to the CPU, you can run the training for 1 epoch, _then_ load weights, then train for however long you need.", "> > I still don't get why automatic weight initializers and load_weights use similar graph construction and colocation schemes though, wouldn't it be easier? (perhaps not, but not sure why).\r\n> \r\n> As far as I understand, when the graph is initially constructed, weight tensors are provided with initializers. They are told \"if you are asked to execute but you don't have any contents, go to this RNG and pull some values out of it to fill yourself.\" But nothing actually happens until you start the session run inside model.fit().\r\n> \r\n> When you load weights, you explicitly assign contents to the tensors, which requires a separate session run.\r\n> \r\n> What I'm not sure about is why you don't get the complete graph right out of model.compile(), since that would solve the problem too.\r\n\r\nYes, what I mentioned was that it could be more explicit if random initializers created separate session to fill the weight tensors just like manual methods. But I agree, whole graph construction during `model.compile` would fix this issue as well - I guess there's some drawback to that though.\r\n\r\n\r\n\r\n> FYI, on a tangential topic, in case you haven't realized. On your system, there are (probably) two separate keras Python source trees, one in \"keras\", and one in \"tensorflow.keras\". They aren't necessarily the same (they are very similar but may have different versions.) If you pull the code from the tensorflow repository right now and follow the instructions to build it from source, you'll get the new binaries, but your code will still be using the same python code.\r\n> \r\n> To use the \"correct\" keras, you need to add \"tensorflow.\" to all import directives.\r\n> \r\n> ```\r\n> import tensorflow.keras as keras\r\n> from tensorflow.keras import callbacks\r\n> import tensorflow.keras.layers as layers\r\n> from tensorflow.keras.activations import linear\r\n> ```\r\n> etc\r\n\r\nYes, I am aware of that. In fact I've linked [a new code](https://gist.github.com/ShellRox/91bead44f6b476a08b9a3c223629ce4c) in one of my replies for this issue, that uses `tf.keras` and NHWC format (I was hoping it would fix this problem but it didn't)\r\n\r\n\r\n> FYI2, for a hacky workaround for this problem that does not involve manually holding the ROI graph to the CPU, you can run the training for 1 epoch, _then_ load weights, then train for however long you need.\r\n\r\nI don't think I understand how can these steps be executed. Can I train the model for `1` epoch and then load weights without stopping this process? I guess I need to temporarily change things in source code?", "```\r\n    history = model.fit_generator(TrainSequence(data_set),\r\n                          epochs=1,\r\n                          use_multiprocessing=False,\r\n                          workers=1)\r\n     if os.path.exists(weights_file):\r\n        model.load_weights(\"new.h5\")\r\n     history = model.fit_generator(TrainSequence(data_set),\r\n                          epochs=10,\r\n                          use_multiprocessing=False,\r\n                          workers=1)\r\n```\r\nPretty sure it would work", "@reedwm could you help to take a look at this? ", "@ekuznetsov139: I got a similar problem and I confirmed your trick of fit - load_weights and then fit again works. Thank you very much!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31449\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31449\">No</a>\n"]}, {"number": 31448, "title": "Fix sanity build failure after #31425", "body": "Error that this is fixing is\r\n\r\n```\r\ntensorflow/python/framework/func_graph.py:32: [W0611(unused-import), ] Unused composite_tensor imported from tensorflow.python.framework\r\n```", "comments": []}, {"number": 31447, "title": "Adding speech output for tensorflowlite apps", "body": "Hello, I am currently building a image recognizer using tensorflow lite. I also wanted to add speech output with the prediction of the model. For example, if the model predicts that a given image is a daisy flower, it will say out loud \"daisy\".\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Tensorflow lite 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?** The api would be modified such that text to speech is involved.\r\n\r\n**Who will benefit with this feature?** Adding speech would greatly help people with visual impairments, and benefit other app users in general.\r\n\r\n**Any Other info.**\r\n", "comments": ["It seems to be solved by recording some audios and then just playing it? The audios can be generated from TTS first. Or combine an image recognizer model and a TTS model in your apps?", "@JiayiFu, that makes sense. I was thinking of that, but how exactly would I do that? Where do I add the TTS code in the tflite files? Is there a tflite documentation explaining the function of each file?", "@novice03,\r\nSorry for the delayed response. Can you please let us know if the [TF Lite Examples for Speech Commands](https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/android) help you? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 31446, "title": "Init operation is not added automatically to collections with tf.GraphKeys.INIT_OP", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): conda/pip\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nIn:\r\n```python\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n\r\n  with tf.variable_scope('signal_in'):\r\n    signal_in = tf.placeholder(tf.float32, shape=(10,40,2,1))\r\n\r\n  with tf.variable_scope('dascope1'):\r\n    conv_linear = tf.keras.layers.Conv2D( 8, (8,2), padding='valid', name='conv_linear', use_bias=True, kernel_initializer=tf.initializers.lecun_normal(seed=137), bias_initializer=tf.initializers.lecun_normal(seed=137) )(signal_in)\r\n  \r\n  with tf.variable_scope('softmax'):\r\n    logits = tf.contrib.layers.fully_connected(conv_linear, 2, activation_fn=None, normalizer_fn=None, normalizer_params=None, weights_initializer=tf.initializers.lecun_normal(seed=731), weights_regularizer=None, biases_initializer=tf.initializers.lecun_normal(seed=777), biases_regularizer=None, reuse=None, variables_collections=None, outputs_collections=None, trainable=True, scope='logit')\r\n    softmax = tf.nn.softmax(logits,axis=0)            \r\n    \r\n  with tf.variable_scope('loss'):\r\n    l_vec = tf.placeholder(tf.float32, shape=(10,2))\r\n    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0)(l_vec, softmax)         \r\n    minimize_op = tf.train.AdamOptimizer(learning_rate=0.05).minimize(loss)\r\n    tf.global_variables_initializer()\r\n\r\nprint(graph.get_collection_ref(tf.GraphKeys.INIT_OP))\r\n``` \r\nreturns [] to stdout\r\n**Describe the expected behavior**\r\nit must return the value of the collection keyed by tf.GraphKeys.INIT_OP, in this case it should be something like loss/init  type NoOp\r\n\r\n**Code to reproduce the issue**\r\nGiven above.\r\n\r\n**Other info / logs**\r\nthis must be a easy to circumvent bug, but for coherence, i think must be corrected.\r\n", "comments": ["@Uiuran ,\r\nWhen tried executing the given code i got output as per the screenshot, can you confirm if the same warnings are received.Thanks!\r\n![issue](https://user-images.githubusercontent.com/52397990/62768120-d3647800-bab3-11e9-9942-bfff8effdd16.png)\r\n\r\n", "yes, but i feel that's only a version issue, not related to the issue i reffered ", "There is no loss/init_op op. Variable initializers don't end up in INIT_OP unless you add it yourself. This is working as intended.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31446\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31446\">No</a>\n", "> There is no loss/init_op op. Variable initializers don't end up in INIT_OP unless you add it yourself. This is working as intended.\r\n\r\nIs INIT_OP  GraphKey added for something else ? just curious if iam missusing or Is it for it, but is faculted to the user as a suggestion ? what do you think ?", "If you're using Supervisor (deprecated) the\ntf.global_variables_initializer() op gets added to GraphKeys.INIT_OP. If\nyou're using MonitoredSession (not deprecated, implied by Estimator), if\nyou the user put something in INIT_OP the framework will run it once to\ninitialize things, but by default it remains unpopulated.\n\nOn Thu, Aug 15, 2019 at 4:33 PM Daniel Penalva <notifications@github.com>\nwrote:\n\n> There is no loss/init_op op. Variable initializers don't end up in INIT_OP\n> unless you add it yourself. This is working as intended.\n>\n> Is INIT_OP GraphKey added for something else ? just curious if iam\n> missusing or Is it for it, but is faculted to the user as a suggestion ?\n> what do you think ?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31446?email_source=notifications&email_token=AAABHRJZWKOS35LVAPXF4ZDQEXRVXA5CNFSM4IKKKP6KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4NIKFQ#issuecomment-521831702>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRN2TUWPYXFVY6XQNRDQEXRVXANCNFSM4IKKKP6A>\n> .\n>\n\n\n-- \n - Alex\n", "ok, nice, thank u\n\nOn Fri, Aug 16, 2019 at 12:31 PM Alexandre Passos <notifications@github.com>\nwrote:\n\n> If you're using Supervisor (deprecated) the\n> tf.global_variables_initializer() op gets added to GraphKeys.INIT_OP. If\n> you're using MonitoredSession (not deprecated, implied by Estimator), if\n> you the user put something in INIT_OP the framework will run it once to\n> initialize things, but by default it remains unpopulated.\n>\n> On Thu, Aug 15, 2019 at 4:33 PM Daniel Penalva <notifications@github.com>\n> wrote:\n>\n> > There is no loss/init_op op. Variable initializers don't end up in\n> INIT_OP\n> > unless you add it yourself. This is working as intended.\n> >\n> > Is INIT_OP GraphKey added for something else ? just curious if iam\n> > missusing or Is it for it, but is faculted to the user as a suggestion ?\n> > what do you think ?\n> >\n> > \u2014\n> > You are receiving this because you modified the open/close state.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/31446?email_source=notifications&email_token=AAABHRJZWKOS35LVAPXF4ZDQEXRVXA5CNFSM4IKKKP6KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4NIKFQ#issuecomment-521831702\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AAABHRN2TUWPYXFVY6XQNRDQEXRVXANCNFSM4IKKKP6A\n> >\n> > .\n> >\n>\n>\n> --\n> - Alex\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31446?email_source=notifications&email_token=AAOOCX2XJPCB4HAQRVTGE2TQE3B3TA5CNFSM4IKKKP6KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4O5H7Y#issuecomment-522048511>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAOOCX2UZBFVAOXZTN3GWXLQE3B3TANCNFSM4IKKKP6A>\n> .\n>\n"]}, {"number": 31445, "title": "bazel fully recompiles the codebase even though no files were changed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nn/a\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version:\r\n1.14\r\n- Python version:\r\n3.5\r\n- Installed using virtualenv? pip? conda?:\r\nvirtualenv\r\n- Bazel version (if compiling from source):\r\nBuild label: 0.26.1\r\n- GCC/Compiler version (if compiling from source):\r\ngcc version 5.4.0 20160609\r\n- CUDA/cuDNN version:\r\nn/a\r\n- GPU model and memory:\r\nn/a\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe problem is that bazel rebuilds the whole codebase during the sequential bazel runs even though literally no files changed. I am building two targets:\r\n`bazel run //tensorflow/lite/python:tflite_convert`\r\nand\r\n`bazel build //tensorflow/tools/pip_package:build_pip_package --host_force_python=PY3 PYTHON_BIN_PATH=/home/kk/tmp/tfclean/bin/python`\r\n\r\nThe thing is that if say I take the first command and run it 3 times, then the full compilation will only be performed for the first time, two remaining runs will result in no compilation steps. The same for the second command. \r\n\r\nHowever, if I run the first, then the second command, the first again - the full recompilation will happen again, even though no files were changed. \r\n\r\nWhy is it so? No files were changed, while does bazel recompile the whole tensorflow codebase for the second time? \r\n\r\n**Provide the exact sequence of commands/steps that you executed before running into the problem**\r\n\r\nThese three commands will results in the full tensorflow codebase recompilation, even though no source files were changed. \r\n\r\n`bazel build //tensorflow/tools/pip_package:build_pip_package --host_force_python=PY3 PYTHON_BIN_PATH=/home/kk/tmp/tfclean/bin/python`\r\n`bazel run //tensorflow/lite/python:tflite_convert`\r\n`bazel build //tensorflow/tools/pip_package:build_pip_package --host_force_python=PY3 PYTHON_BIN_PATH=/home/kk/tmp/tfclean/bin/python`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["The compile flags are changing in between the two compilations so Bazel cache is invalidated.\r\n\r\nThis is the expected behavior as changing those flags can result in files being compiled with different compiler, different compiler options, different ABI, etc.\r\n\r\nFrom the point of view of Tensorflow, this is working as intended. Please open an issue on Bazel repo if you want to request a feature where some flags don't invalidate the cache, as that is not a TensorFlow issue."]}, {"number": 31444, "title": "tensorflow c++ header missing ubuntu", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: \r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:PIP\r\n- Bazel version (if compiling from source):0.26.\r\n\r\n**Describe the problem**\r\nI am using tensor flow for c++ project, but i get some problem with install, No such file or directory #include \"tensorflow/core/lib/framework/graph.pb.h\".\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n g++ some_file.cpp   -ltensorflow -o hello_tf\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nfatal error: tensorflow/core/lib/framework/graph.pb.h: No such file or directory #include \"tensorflow/core/lib/framework/graph.pb.h\"", "comments": ["In the `g++` line there is no `-I<include_path>` to point to the include directory. If the header file is not in system paths then it won't be found."]}, {"number": 31443, "title": "Fixing wrong line break in documentation", "body": "The method description (`tf.unravel_index`) looks a bit broken: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/unravel_index", "comments": []}, {"number": 31442, "title": "python API debugging best practices", "body": "Hi guys,\r\n\r\nI work for ARM Ltd and we are looking into adding int16 quantization support for the TFLite format. For that, I would need to change the source code of some files, including the command-line interface implemented in /tensorflow/lite/python/tflite_convert.py. I was just wondering what is the most efficient way to debug my code and iterate quickly?\r\n\r\nI am currently using CLIon+Google Bazel Plugin. This combination allows quick and convenient code development process. Thanks to the tools, I can place breakpoints, see variables, etc(only for CPP though). What is the most efficient way to debug python APIs?\r\n\r\nI get, for example, I could rebuild the tflite_convert tool every time with the following command:\r\nbazel build //tensorflow/lite/python:tflite_convert.\r\n\r\nThis is fine, but I can\u2019t debug this executable, which is inconvenient and does not allow quick iterations.\r\nMay I ask you to share your best practices for tensorflow python APIs development?\r\nMany thanks,\r\nKonstantin", "comments": ["I prefer logging info  then setting breakpoints and debugging.\r\nI have tried someway to debug python with cpp library, it's soo complicated ", "Hi @konstantinARM @Mycaster \r\nwhich version of tensorflow and clion you used, can teach me the configuration process import tensorflow source code .    Thank you!", "It has already been long time ago this is issued. But I am having hard time to build tensorflow using Clion. I want to do the exactly same thing you(issue writer) did such as adding break point in c++ files and build and debug using them. If you can see it, Could you explain how you did it? Thank you.", "Hi, there is a document available on Tensorflow on same topic [Post-training integer quantization with int16 activations](https://www.tensorflow.org/lite/performance/post_training_integer_quant_16x8). You can try the same.\r\nFeel free to reopen the issue if the error still persists, Thank you.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31442\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31442\">No</a>\n"]}, {"number": 31441, "title": "Zero copy between CPU and GPU", "body": "<em>I made sure that this is a feature request. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): __tensorflow-gpu 1.13.1+nv19.5__\r\n- Are you willing to contribute it (Yes/No): __No__, not involved in project, could try\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI use some network which requires **a lot of CPU <-> GPU transfers**, namely NMSv2 on a hardware platform where CPU and GPU have **unified (common) memory**. Such a transfers cause a lot of copying between the same memory, while **zero-copy** technique could be used.\r\n\r\nNVIDIA team [says](https://devtalk.nvidia.com/default/topic/1059017/jetson-agx-xavier/zero-copy-with-tensorflow/post/5369698/#5369698) that this request should be filed here.\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Yes.** Probably requires adding some new option in config, which handles whether or not to copy between CPU & GPU. like\r\n```python\r\nconfig.gpu_options.zero_copy = True\r\n```\r\n\r\nMaybe some further investigation is needed.\r\n\r\n**Who will benefit with this feature?**\r\n\r\n- **Users** who use embedded devices with unified memory will get **performance increase** in cases with many CPU <-> GPU transfers.\r\n- **Tensorflow itself** could use this feature to persuade developers to choose it for embedded systems.\r\n\r\n**Any Other info.**\r\n\r\n- A [question](https://devtalk.nvidia.com/default/topic/1059017/jetson-agx-xavier/zero-copy-with-tensorflow/) about zero copy on NVIDIA DevTalk forum\r\n- A [post](http://arrayfire.com/zero-copy-on-tegra-k1/) about sources modification need to make zero-copy work", "comments": ["The original request seems related to Jetson platform. I'm not familiar with that. @nluehr any suggestions?", "@aaroey,\r\nLet me clarify the thing that the request on Nvidia forum is related to Jetson indeed, but the entire freq we're discussing aims not on Jetson only.\r\nAFAIK, tensorflow uses CUDA to allocate memory on GPU, and CUDA allows zero-coly memory allocation.", "@faerics, hmm I don't fully understand, what did you mean by `CUDA allows zero-coly memory allocation`?", "@aaroey, soory for the delay,\r\nI'm not a CUDA pro by myself, but according to the [post](http://arrayfire.com/zero-copy-on-tegra-k1/) I linked one needs to use different allocation functions (namely, `CudaHostAlloc` instead of `CudaMalloc` and then `cudaHostGetDevicePointer` instead of `CudaMemcpy`).\r\n\r\nThe post is from 2014, not sure that functions' names sre the same nowadays.", "Hello, commenting to bubble this up. @aaroey, let's continue the discussion!", "@chsigg what do you think about the proposal of adding pin memory allocation support?", "I'm fine with adding a pinned memory allocator (IIRC, we already have it for tests), if someone is willing to contribute. I don't expect it to be useful except for shared memory systems (like Jetson). ", "I've implemented a zero copy allocator on Jetson AGX Xavier. The issue is that `cuMemHostAlloc` actually disables _both_ the CPU and GPU cache. Therefore, I'm getting slower training speeds compared to the default `cuMemAlloc`.\r\n\r\n**Edit:** See [this](https://personal.utdallas.edu/~soroush/papers/RTAS2020.pdf) published in RTAS 2020.", "> I've implemented a zero copy allocator on Jetson AGX Xavier. The issue is that `cuMemHostAlloc` actually disables _both_ the CPU and GPU cache. Therefore, I'm getting slower training speeds compared to the default `cuMemAlloc`.\r\n> \r\n> **Edit:** See [this](https://personal.utdallas.edu/~soroush/papers/RTAS2020.pdf) published in RTAS 2020.\r\n\r\nCache-coherence really hurts...", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 31440, "title": "Better documentation for Dataset.from_tensors/from_tensor_slices", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices\r\n\r\n## Description of issue (what needs changing):\r\n\r\nWhile following Google's ML crash course, I found it very difficult to understand the difference between `Dataset.from_tensors/from_tensor_slices` and when to use each. One thing that confused me was that `from_tensors` only creates a single tensor, despite the name including the plural form \"tensors\".\r\n\r\nBeginners get introduced to these APIs very early, but the current documentation consists of one terse sentence about behaviour (plus a multi-line warning about memory usage):\r\n\r\n```\r\nFTS:\r\nCreates a Dataset whose elements are slices of the given tensors.\r\n\r\nFT:\r\nCreates a Dataset with a single element, comprising the given tensors.\r\n```\r\n\r\nI think this would benefit from some elaboration and a clearer description of how the two are related. Given that users enocunter this API very early, the behaviour should ideally be obvious. A small example would help communicate this, e.g:\r\n\r\n```\r\nmy_data = { \"my_feature\" : [ [1, 2, 3], [4, 5, 6] ] }\r\ntf.data.Dataset.from_tensors(my_data) # Models a single, 2x3 tensor.\r\n```\r\n\r\n```\r\nmy_data = { \"my_feature\" : [ [1, 2, 3], [4, 5, 6] ] }\r\ntf.data.Dataset.from_tensor_slices(my_data) # Splits on rows. Models two, 1x3 tensors.\r\n```\r\n\r\n### Clear description\r\n\r\n### Correct links\r\n\r\nFine AFAIK\r\n\r\n### Parameters defined\r\n\r\nFine AFAIK\r\n\r\n### Returns defined\r\n\r\nFine AFAIK\r\n\r\n### Raises listed and defined\r\n\r\nFine AFAIK\r\n\r\n### Usage example\r\n\r\n**There is currently no usage example, and I think the documentation would greatly benefit from one.**\r\n\r\n### Request visuals, if applicable\r\n\r\nThere are currently no visuals. They might possibly help, but I think a usage example is probably sufficient. \r\n\r\n### Submit a pull request?\r\n\r\nI'm not sure if I'll submit a PR to improve this. I'd like to, but I'm still quite new to TF and wouldn't like to introduce any inaccuracies.\r\n", "comments": ["@karwa,\r\nThe documentation for both [from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) and [from_tensors](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensors) has been updated with a brief description and an example.\r\n\r\nCould you please take a look at the given link. Thanks!", "> Could you please take a look at the given link. Thanks!\r\n\r\n@karwa,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 31439, "title": "TensorFlow 1.14 eager execution not working with tf.function", "body": "**System information**\r\n- OS Platform and Distribution: macOS Mojave\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: Python 3.7.3\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI use `tf.math.argmax(predictions).numpy()` to get the prediction in a function. But with the `tf.function` declared before the function, there will raise an AttributeError: 'Tensor' object has no attribute 'numpy'.\r\n\r\n**Code to reproduce the issue**\r\nSimilar to [this tutorial](https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention), but with `tf.math.argmax(predictions).numpy()` inside the `train_step()` to print the prediction.\r\n", "comments": ["Hi,\r\n\r\nThe whole point of `tf.function` is to disable Eager execution on a portion of your code in favour of building a re-usable computational graph (rather than building and discarding graphs as data comes by) ; it is therefore not a bug, but a feature, that you can no longer use the `numpy` method, which is exclusive to `EagerTensor` (a subclass of the more general `Tensor` type).\r\n\r\nIf you want to print something, you should use `tf.print` on your `Tensor` (this is also true in contexts when you are handling `EagerTensor` instances, since this is more general and less error-prone than using a combination of the built-in `print` and Eager functionalities). So, in your example, use `tf.print(tf.math.argmax(predictions))`, which will create a print-out node in the built computation graph."]}, {"number": 31438, "title": "Failed to load the native TensorFlow runtime.", "body": "\r\nC:\\Users\\safalabolo\\Desktop\\7-inversion>c:\\Python36\\python.exe apply_algo.py\r\nWARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\r\nc:\\Python36\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\r\n  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\r\nWARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\r\nWARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\r\nWARNING (theano.tensor.blas): Failed to import scipy.linalg.blas, and Theano flag blas.ldflags is empty. Falling back on slower implementations for dot(matrix, vector), dot(vector, matrix) and dot(vector, vector) (cannot import name 'NUMPY_MKL')\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"c:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 35, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 30, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Impossibile trovare il modulo specificato.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"apply_algo.py\", line 11, in <module>\r\n    loaded_model = pickle.load(open(modelname, 'rb'))\r\n  File \"c:\\Python36\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"c:\\Python36\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"c:\\Python36\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"c:\\Python36\\lib\\site-packages\\keras\\backend\\__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"c:\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"c:\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"c:\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"c:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 35, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 30, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Impossibile trovare il modulo specificato.\r\n\r\n", "comments": ["@safalabolo, Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 31437, "title": "Do not accumulate Const nodes created in forward pass in while_v2.", "body": "PiperOrigin-RevId: 261958798", "comments": []}, {"number": 31436, "title": "[TF 2.0 API Docs] tf.keras.backend.relu", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/backend/relu\r\n\r\n## Description of the issue (what needs changing):\r\n\r\nThe documentation GitHub symbol link on the official `API_Docs` redirects to another symbol than the expected symbol.\r\n\r\n### Correct links\r\n\r\nNo\r\n\r\n### Parameters defined\r\n\r\nNo\r\n\r\n### Returns defined\r\n\r\nNo\r\n\r\n### Raises listed and defined\r\nNo\r\n\r\n### Usage example\r\n\r\nNo\r\n\r\n### Request visuals, if applicable\r\n\r\nYes\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Pointing to the wrong place in the file. Should be here (or wherever it changes to): https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/backend.py#L4198", "Hi @mwinel, thanks for reporting this.\r\n\r\nIt looks like this is caused by slippage between the contents of the 2.0b1 release that we generated the docs from, and the current state fo the 2.0 branch.\r\n\r\nWe will regenerate all the docs when 2.0rc0 is up. That will fix this and any other similar issues.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31436\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31436\">No</a>\n"]}, {"number": 31435, "title": "Do not accumulate Const nodes created in forward pass in while_v2.", "body": "", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31435) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 31434, "title": "tensorflow 1.14 build failed ", "body": "\r\n**System information**\r\n- OS Platform and Distribution (Linux Ubuntu 18.04):\r\n- TensorFlow installed from (source):\r\n- TensorFlow version: r1.14\r\n- Python version: v3.6.8\r\n- Bazel version (tried v0.24.1, v0.26, v0.28):\r\n- GCC/Compiler version (gcc version 7.4):\r\n- CUDA/cuDNN version: 10.0/7.5\r\n- GPU model and memory: GTX1050\r\n\r\n\r\n\r\n**Describe the problem**\r\nI tried to use below command to build concat_op_test application.\r\n`bazel test -c opt  --strip=never  -s //tensorflow/core/kernels:concat_op_test --verbose_failures` \r\nThe compiler I select is clang and build with CUDA support.\r\nI got below errors when bazel version is 0.24.1 and 0.26\r\n```\r\nERROR: Skipping '//tensorflow/core/kernels:concat_op_test': error loading package 'tensorflow/core/kernels': Encountered error while reading extension file 'cu\r\nda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"/home/chengleiwang/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"/home/chengleiwang/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1199, in _create_local_cuda_repository\r\n                _tpl(repository_ctx, \"crosstool:BUILD\", c...)\r\n        File \"/home/chengleiwang/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 745, in _tpl\r\n                repository_ctx.template(out, Label((\"//third_party/gpus/%s...)), ...)\r\nclass com.google.devtools.build.lib.syntax.SkylarkList$MutableList cannot be cast to class java.lang.String (com.google.devtools.build.lib.syntax.SkylarkList$MutableList is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')\r\nERROR: error loading package 'tensorflow/core/kernels': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cu\r\nda//cuda': Traceback (most recent call last):\r\n        File \"/home/chengleiwang/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"/home/chengleiwang/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1199, in _create_local_cuda_repository\r\n                _tpl(repository_ctx, \"crosstool:BUILD\", c...)\r\n        File \"/home/chengleiwang/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 745, in _tpl\r\n                repository_ctx.template(out, Label((\"//third_party/gpus/%s...)), ...)\r\nclass com.google.devtools.build.lib.syntax.SkylarkList$MutableList cannot be cast to class java.lang.String (com.google.devtools.build.lib.syntax.SkylarkList$MutableList is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')\r\n``` \r\nI got below errors when bazel version is 0.28.0 (I changed the bazel version check in configure.py   `current_bazel_version = check_bazel_version('0.24.1', '0.28.2')` )\r\n```\r\nStarting local Bazel server and connecting to it...\r\nInternal error thrown during build. Printing stack trace: java.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIREC\r\nTORY:@local_config_cuda' (requested by nodes 'REPOSITORY:@local_config_cuda')\r\n        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:528)\r\n        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n        at java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.ClassCastException: class com.google.devtools.build.lib.syntax.SkylarkList$MutableList cannot be cast to class java.lang.String (com.google.devtools.build.lib.syntax.SkylarkList$MutableList is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')\r\n        at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryContext.createFileFromTemplate(SkylarkRepositoryContext.java:295)\r\n        at jdk.internal.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)\r\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n        at java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n        at com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:135)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:930)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:898)\r\n        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)\r\n        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)\r\n        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)\r\n        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:992)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:904)\r\n        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)\r\n        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)\r\n        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)\r\n        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:992)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:904)\r\n        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)\r\n        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)\r\n        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)\r\n        at com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:231)\r\n        at com.google.devtools.build.lib.syntax.Eval.execIf(Eval.java:144)\r\n        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:214)\r\n        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)\r\n        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)\r\n        at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:167)\r\n        at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.fetchRepository(RepositoryDelegatorFunction.java:288)\r\n        at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:215)\r\n        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:451)\r\n        ... 4 more\r\n\r\nINFO: Elapsed time: 2.858s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/core/kernels\r\n    Fetching @local_config_cuda; fetching\r\nInternal error thrown during build. Printing stack trace: java.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@local_config_cuda' (requested by nodes 'REPOSITORY:@local_config_cuda')\r\n        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:528)\r\n        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n        at java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.ClassCastException: class com.google.devtools.build.lib.syntax.SkylarkList$MutableList cannot be cast to class java.lang.String (com.google.devtools.build.lib.syntax.SkylarkList$MutableList is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')\r\n        at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryContext.createFileFromTemplate(SkylarkRepositoryContext.java:295)\r\n        at jdk.internal.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)\r\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n        at java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n        at com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:135)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:930)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:898)\r\n        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)\r\n        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)\r\n        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)\r\n        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:992)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:904)\r\n        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)\r\n        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)\r\n        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)\r\n        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:992)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:904)\r\n        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)\r\n        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)\r\n        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)\r\n        at com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:231)\r\n        at com.google.devtools.build.lib.syntax.Eval.execIf(Eval.java:144)\r\n        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:214)\r\n        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)\r\n        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)\r\n        at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:167)\r\n        at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.fetchRepository(RepositoryDelegatorFunction.java:288)\r\n        at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:215)\r\n        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:451)\r\n        ... 4 more\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@local_config_cuda' (requested by nodes 'REPOSITORY:@local_config_cuda')\r\n        at com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:528)\r\n        at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:399)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n        at java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.ClassCastException: class com.google.devtools.build.lib.syntax.SkylarkList$MutableList cannot be cast to class java.lang.String (com.google.devtools.build.lib.syntax.SkylarkList$MutableList is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')\r\n        at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryContext.createFileFromTemplate(SkylarkRepositoryContext.java:295)\r\n        at jdk.internal.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)\r\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n        at java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n        at com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:135)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:930)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:898)\r\n        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)\r\n        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)\r\n        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)\r\n        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:992)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:904)\r\n        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)\r\n        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)\r\n        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)\r\n        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.callFunction(FuncallExpression.java:992)\r\n        at com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:904)\r\n        at com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:75)\r\n        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:201)\r\n        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)\r\n        at com.google.devtools.build.lib.syntax.Eval.execStatements(Eval.java:231)\r\n        at com.google.devtools.build.lib.syntax.Eval.execIf(Eval.java:144)\r\n        at com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:214)\r\n        at com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:183)\r\n        at com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)\r\n        at com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)\r\n        at com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:167)\r\n        at com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.fetchRepository(RepositoryDelegatorFunction.java:288)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/core/kernels\r\n    Fetching @local_config_cuda; fetching\r\n```\r\n\r\n", "comments": ["This version could build pass:  c1de6bc140d22b2b646d8e0f925ed7dc0bf21a13\r\nAny regression?", "@oanush, I'm afraid I lack expertise with this part of the code. Can you find someone more appropriate to take a look?", "Can you try a new checkout of the branch? There was a recent cherry-pick regarding java.", "Try the newest checkout of r1.14 \uff0cstill same error...\r\n```\r\ncommit e19c354920c3b246dda6598229210a582caaa1a9 (HEAD -> r1.14, origin/r1.14)\r\nAuthor: Mihai Maruseac <mihaimaruseac@google.com>\r\nDate:   Thu Aug 8 08:33:18 2019 -0700\r\n\r\n    Fix sanity build failure after #31425\r\n    \r\n    Error that this is fixing is\r\n    \r\n    ```\r\n    tensorflow/python/framework/func_graph.py:32: [W0611(unused-import), ] Unused composite_tensor imported from tensorflow.python.framework\r\n```", "Unfortunately I don't have ideas on how to debug Java, adding @sjamesr ", "Ping @sjamesr , any suggestion? ", "I recommend to try gcc 6.5, you can use update-alternatives to switch between gcc 7.4 and gcc 6.5. The build should be successful. @chengleiwang ", "@mengdong, actually I am using clang as compiler.  So how could update-alternatives  gcc can build successful?  ", "ah, sorry I missed, I haven't built with clang before hence no comment on that. ", "Afaik update-alternatives can also work with clang. Though I don't know which version is the one that's supported best", "Confirmed same error on Ubuntu 18.04 with Bazel 0.25.2, clang 6.0.0-1ubuntu2, Python 3.6.8, NVIDIA Tesla K80, CUDA Version: 10.2, with v1.14.0 checked out (@ 87989f69597d6b2d60de8f112e1e3cea23be7298)", "Can you try `r1.15` branch instead? Turns out our 1.14 branch is in a terrible state, sorry for that.", "Confirmed `master` branch breaks also: https://github.com/tensorflow/tensorflow/issues/32323\r\n\r\nI can try with `r1.15` next", "Confirmed errors with `r1.15` (ea930781c3164c9646e26cf3716f86804aa65b63):\r\n```\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/7d3379297965407e4f621f04220127eb/external/nccl_archive/BUILD.bazel:53:1: output 'external/nccl_archive/_objs/device_lib/functions.cu.o' was not created\r\n[4,495 / 6,642] 4 actions running\r\n    Compiling external/nccl_archive/src/collectives/device/functions.cu.cc [for host]; 1s local\r\n    Compiling external/nccl_archive/src/collectives/device/max_f64_all_reduce.cu.cc [for host]; 0s local\r\n    Compiling external/nccl_archive/src/collectives/device/min_u8_all_reduce.cu.cc [for host]; 0s local\r\n    Compiling external/nccl_archive/src/collectives/device/sum_i8_all_gather.cu.cc [for host]; 0s local\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/7d3379297965407e4f621f04220127eb/external/nccl_archive/BUILD.bazel:53:1: not all outputs were created or valid\r\n[4,495 / 6,642] 4 actions running\r\n    Compiling external/nccl_archive/src/collectives/device/functions.cu.cc [for host]; 1s local\r\n    Compiling external/nccl_archive/src/collectives/device/max_f64_all_reduce.cu.cc [for host]; 0s local\r\n    Compiling external/nccl_archive/src/collectives/device/min_u8_all_reduce.cu.cc [for host]; 0s local\r\n    Compiling external/nccl_archive/src/collectives/device/sum_i8_all_gather.cu.cc [for host]; 0s local\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n[4,499 / 6,642] checking cached actions\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n[4,499 / 6,642] checking cached actions\r\nINFO: Elapsed time: 416.926s, Critical Path: 54.00s\r\n[4,499 / 6,642] checking cached actions\r\nINFO: 453 processes: 453 local.\r\n[4,499 / 6,642] checking cached actions\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nWith an earlier build failing on:\r\n```\r\n[3,141 / 3,324] 4 actions running\r\n    Compiling tensorflow/core/kernels/spacetobatch_functor_gpu.cu.cc [for host]; 13s local\r\n    Compiling tensorflow/core/kernels/reverse_sequence_op.cc [for host]; 3s local\r\n    Compiling tensorflow/core/kernels/one_hot_op_gpu.cu.cc [for host]; 2s local\r\n    Compiling tensorflow/core/kernels/reverse_op.cc [for host]; 0s local\r\nERROR: /home/ubuntu/repos/tensorflow-for-py3/tensorflow/core/kernels/BUILD:4651:1: output 'tensorflow/core/kernels/_objs/batch_space_ops_gpu/spacetobatch_functor_gpu.cu.pic.o' was not created\r\n[3,141 / 3,324] 4 actions running\r\n    Compiling tensorflow/core/kernels/spacetobatch_functor_gpu.cu.cc [for host]; 13s local\r\n    Compiling tensorflow/core/kernels/reverse_sequence_op.cc [for host]; 3s local\r\n    Compiling tensorflow/core/kernels/one_hot_op_gpu.cu.cc [for host]; 2s local\r\n    Compiling tensorflow/core/kernels/reverse_op.cc [for host]; 0s local\r\nERROR: /home/ubuntu/repos/tensorflow-for-py3/tensorflow/core/kernels/BUILD:4651:1: not all outputs were created or valid37.198.227] out: \r\n[3,141 / 3,324] 4 actions running\r\n    Compiling tensorflow/core/kernels/spacetobatch_functor_gpu.cu.cc [for host]; 13s local\r\n    Compiling tensorflow/core/kernels/reverse_sequence_op.cc [for host]; 3s local\r\n    Compiling tensorflow/core/kernels/one_hot_op_gpu.cu.cc [for host]; 2s local\r\n    Compiling tensorflow/core/kernels/reverse_op.cc [for host]; 0s local\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n[3,145 / 3,324] checking cached actions\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n[3,145 / 3,324] checking cached actions\r\nINFO: Elapsed time: 317.459s, Critical Path: 116.62s\r\n[3,145 / 3,324] checking cached actions\r\nINFO: 157 processes: 157 local.\r\n[3,145 / 3,324] checking cached actions\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```", "All I can think of for next step is to try building from scratch again (make sure on clean environment) and also pass in `--verbose_failures` flag when building.\r\n\r\nThen, please _attach_ a log of the entire compilation.", "Full output (with input and environment variables also shown): https://gist.githubusercontent.com/SamuelMarks/9dfd50d70e6946a83ee3bd29bf931c49/raw/7483f3d2babade9e44acad7a32e884e9d0c3c66a/tflog.txt\r\n\r\nRan this beforehand:\r\n```\r\nrm -rf ~/repos/tensorflow-for-py3 ~/.cache/bazel\r\n```", "So there are 36 lines of\r\n\r\n```\r\nout: ./tensorflow/core/util/gpu_device_functions.h(648): error: calling a constexpr __host__ function(\"real\") from a __device__ function(\"GpuAtomicAdd\") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.\r\n```", "Hmm:\r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures --expt-relaxed-constexpr\r\nout: Starting local Bazel server and connecting to it...\r\nout: ERROR: Unrecognized option: --expt-relaxed-constexpr\r\n```", "> Hmm:\r\n> \r\n> ```\r\n> bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures --expt-relaxed-constexpr\r\n> out: Starting local Bazel server and connecting to it...\r\n> out: ERROR: Unrecognized option: --expt-relaxed-constexpr\r\n> ```\r\n\r\nI also met this error before. I tried to add --expt-relaxed-constexpr in tensorflow.bzl _cuda_copts() function. but still with no lucky.\r\n\r\n```\r\ndef _cuda_copts(opts = []):\r\n    \"\"\"Gets the appropriate set of copts for (maybe) CUDA compilation.\r\n\r\n      If we're doing CUDA compilation, returns copts for our particular CUDA\r\n      compiler.  If we're not doing CUDA compilation, returns an empty list.\r\n\r\n      \"\"\"\r\n    return cuda_default_copts() + select({\r\n        \"//conditions:default\": [],\r\n        \"@local_config_cuda//cuda:using_nvcc\": ([\r\n            \"-nvcc_options=relaxed-constexpr\",\r\n            \"-nvcc_options=ftz=true\",\r\n        ]),\r\n        \"@local_config_cuda//cuda:using_clang\": ([\r\n            \"-fcuda-flush-denormals-to-zero\",\r\n            \"--expt-relaxed-constexpr\",\r\n        ]),\r\n    }) + if_cuda_is_configured_compat(opts)\r\n```\r\n\r\n", "I think you have to use the `copt` option in Bazel.", "Overnight I ran:\r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures --copt='--expt-relaxed-constexpr'\r\n```\r\n\r\nBut looking at the log, it's still littered with:\r\n```\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(121): warning: calling a constexpr __host__ function(\"imag\") from a __host__ __device__ function(\"operator()\") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.\r\n```\r\n\r\nFYI: Build it still failing, on r1.15 branch @ 2e29d3b35a4c5363258ad040a12b3a15affff3c2", "Hi @chengleiwang !Could you refer this [link ](https://www.tensorflow.org/install/source_windows) for suitable bazel and tensorflow versions.  Could you try with tensorflow_gpu 2.0 or 2.1  ? \r\nPlease open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31434\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31434\">No</a>\n"]}, {"number": 31433, "title": "[ROCm] Fix for the broken `--config=rocm` build", "body": "The following PR/commit breaks the --config=rocm build\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/ddd77ee043ac720793d8dfb887b0eab3cfcb0adb\r\n\r\nIt introduces references to se::cuda::RedzoneAllocator (which is only visible in the CUDA build) within code that is common to both the ROCm and CUDA builds. This \"fix\" moves those reference to code that is visible only in the CUDA build\r\n\r\nThis is essentially the same bug + fix as in PR #31393 (but in a different file)\r\n\r\n---------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg \r\n", "comments": []}, {"number": 31432, "title": "[Intel MKL] Enable 3d shapes in the MklSlice op", "body": "Support 3d shapes (NCDHW / NDHWC) in MklSliceOp.\r\n\r\nExisting MklSliceOp support only 2d shapes and crash all 3d-CNN models such as https://github.com/Youngseok0001/Intel_benchmark/tree/master/3D_CNN.", "comments": ["@wenxizhu Please check the `SliceTest.test3Dimension` failure in the `Ubuntu Python2` [test](https://source.cloud.google.com/results/invocations/ab246701-c3ee-4309-a10e-6be5c3182c01/log).", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31432) for more info**.\n\n<!-- need_author_consent -->", "> @wenxizhu Please check the `SliceTest.test3Dimension` failure in the `Ubuntu Python2` [test](https://source.cloud.google.com/results/invocations/ab246701-c3ee-4309-a10e-6be5c3182c01/log).\r\n\r\n@penpornk Fixed.", "@wenxizhu @trollyxia \r\nSince the last commit is not done by the pull request submitter, could each of you please reply with just `@googlebot I consent.` to show your consent?", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31432) for more info**.\n\n<!-- ok -->", "@penpornk I changed the author of the last commit. Is the CLA OK now?", "@wenxizhu Could you please help take a look at `SliceTest.test3Dimension` failure in the [Ubuntu CPU test](https://source.cloud.google.com/results/invocations/d7492126-2897-46e6-92a1-d514bba930e5/log)? Thank you!", "> @wenxizhu Could you please help take a look at `SliceTest.test3Dimension` failure in the [Ubuntu CPU test](https://source.cloud.google.com/results/invocations/d7492126-2897-46e6-92a1-d514bba930e5/log)? Thank you!\r\n\r\n@penpornk Could you give me some details of how to reproduce this failure? It passed in our local test environment.", "@wenxizhu I can't reproduce it either. I'm thinking it might have to do with how `random_ops.random_normal` is implemented (that somehow make seed=0 get different answer on different OS/architecture). Could you please generate the input as a numpy array first and create the solution with numpy slice instead? This way the solution isn't hard-coded and we don't have to worry about different initialization.", "> @wenxizhu I can't reproduce it either. I'm thinking it might have to do with how `random_ops.random_normal` is implemented (that somehow make seed=0 get different answer on different OS/architecture). Could you please generate the input as a numpy array first and create the solution with numpy slice instead? This way the solution isn't hard-coded and we don't have to worry about different initialization.\r\n\r\nI'm also skeptical about this, which `seed=0` is not portable across devices (most likely they have different `random()` implementations). I will commit a fix soon.\r\n", "@penpornk Let's see if it works now.", "Can one of the admins verify this patch?", "Other tests are still running, but `Ubuntu Sanity` is already finished and it's failing. Please fix the pylint error. ", "@wenxizhu Could you please resolve the conflicts? Thanks!", "@penpornk @gbaned I'm a little confused, where these conflicts come from? I didn't change any of these files.", "Sorry there is a misunderstanding. This PR has already been merged since yesterday (https://github.com/tensorflow/tensorflow/commit/8c1d5b95c7ed83e6b6c1b9ece06e310777565d27). For some reason it isn't marked so (probably because I made some additional changes internally). Closing the PR."]}]