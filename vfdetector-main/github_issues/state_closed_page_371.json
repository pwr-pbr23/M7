[{"number": 42913, "title": "Remove a redundant loop in edgeset.cc", "body": "redundant loop", "comments": ["@keveman  Is this an acceptable change? or any hidden bugs invoked by this?\r\nMany thanks!", "This isn't equivalent to what was happening before; previously it'd search the whole list for the value and return false (no new insert) if the value already existed. After this change it looks like it'd find the first of an empty slot or an equal value, which means it'd sometimes return true (new insert) even if the value already existed.\r\n\r\nLet me know if I've missed something."]}, {"number": 42912, "title": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXPAND_DIMS, FILL, FLOOR, GATHER, GATHER_ND, LEAKY_RELU, MAXIMUM, MEAN, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, RESIZE_BILINEAR, RESIZE_NEAREST_NEIGHBOR, SHAPE, STRIDED_SLICE, SUB, TRANSPOSE_CONV, UNPACK. Here is a list of operators for which you will need custom implementations: MaxPoolWithArgmax, Size.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXPAND_DIMS, FILL, FLOOR, GATHER, GATHER_ND, LEAKY_RELU, MAXIMUM, MEAN, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, RESIZE_BILINEAR, RESIZE_NEAREST_NEIGHBOR, SHAPE, STRIDED_SLICE, SUB, TRANSPOSE_CONV, UNPACK. Here is a list of operators for which you will need custom implementations: MaxPoolWithArgmax, Size.\r\n\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["You can convert your model with Select TF operator set. Please take a look:\r\nhttps://www.tensorflow.org/lite/guide/ops_select\r\nhttps://www.tensorflow.org/lite/guide/reduce_binary_size", "@rajaihimdiat \r\nPlease update the issue template with all details tf version, sample code or colab gist with issue reported.\r\nPlease refer to above comment as update.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42911, "title": "bug when using \"num_parallel_calls\" when mapping dataset to tfa function", "body": "As mentioned over the issue [here](https://github.com/tensorflow/addons/issues/2139) and advised from other contributors, i'm creating this issue cause using \"num_parallel_calls=tf.data.experimental.AUTOTUNE\" inside the .map call from my dataset, appeared to generate a deadlock.\r\n\r\nI've tested with tensorflow versions 2.2 and 2.3, and tensorflow addons 0.11.1 and 0.10.0\r\nOn Google Colab Pro gpu env, python3.8\r\n\r\nLink to example TFRecord: https://drive.google.com/drive/folders/1dc6ehBGL_mwGTuSy71VhUYVp0eMdHADP?usp=sharing\r\n\r\n### Code to Reproduce the issue:\r\n```python\r\ntest_dataset = tf.data.TFRecordDataset(num_parallel_reads=tf.data.experimental.AUTOTUNE,filenames=DRIVE_DIR+'/tf_issue/test_0.tfrecord').map(parsing_fn,num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\ntest_dataset = test_dataset.map(translate,num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE) \r\ntest_dataset = test_dataset.unbatch()\r\niterator = tf.compat.v1.data.make_one_shot_iterator(test_dataset)\r\nfor i in range(5):\r\n  image,label = iterator.get_next()\r\n```\r\n###  Auxiliary functions\r\n```python\r\n\r\ndef parsing_fn(serialized):\r\n    features = \\\r\n        {\r\n            'image': tf.io.FixedLenFeature([], tf.string),\r\n            'label': tf.io.FixedLenFeature([], tf.int64)            \r\n        }\r\n    parsed_example = tf.io.parse_single_example(serialized=serialized,\r\n                                             features=features)\r\n    image_raw = parsed_example['image']\r\n    image = tf.io.decode_jpeg(image_raw)    \r\n    image = tf.image.resize(image,size=[224,224])    \r\n    label = parsed_example['label']    \r\n    return image, label\r\n\r\ntranslate = lambda image,label: tf.py_function(func=translate_pipeline,inp=[image,label],Tout=[tf.float32,tf.int64])\r\ndef translate_pipeline(original_image,label):\r\n  print(1)\r\n  height = tf.shape(original_image)[0].numpy()    \r\n  width = tf.shape(original_image)[1].numpy()\r\n  y_fraction  = tf.convert_to_tensor(height * 0.2, dtype=tf.float32)\r\n  x_fraction = tf.convert_to_tensor(width * 0.2,dtype=tf.float32)\r\n  print(2)\r\n  batched_image = tf.tile(tf.expand_dims(original_image,axis=0),[4,1,1,1]) # Create 4 copied versions from the original image and add to a batch\r\n  translated_images = tfa.image.translate_ops.translate(images=batched_image,translations=[[x_fraction,-y_fraction],[-x_fraction,y_fraction],[-x_fraction,-y_fraction],[x_fraction,y_fraction]])\r\n  augmented_images = tf.concat([tf.expand_dims(original_image,axis=0),translated_images],axis=0)\r\n  print(3)\r\n  label = tf.reshape(label,[1,1])\r\n  labels = tf.tile(label,[5,1])\r\n  print(4)\r\n  return augmented_images, labels\r\n\r\n```\r\n### Output\r\n>1\r\n>1\r\n>2\r\n>2\r\n\r\n", "comments": ["@FalsoMoralista \r\n\r\nI am seeing the error message as below.\r\n`NotFoundError: /usr/local/lib/python3.6/dist-packages/tensorflow_addons/custom_ops/image/_image_ops.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl11string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS8_EE`.\r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps in localizing the issue faster. Thanks!", "@ravikyram Have you tried to `pip install --upgrade tensorflow-addons` in your colab gist?", "@bhack \r\n\r\nSorry, i did not notice that i have not installed TF addons. Thanks for your help\r\n\r\n@FalsoMoralista \r\nI tried in colab with TF version 2.3 and i am seeing below output. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/6d18dd03ea18e3b123d76dce75bc1793/untitled308.ipynb). Thanks!\r\n```\r\n1\r\n2\r\n1\r\n2\r\n```", "@ravikyram It was installed by default in colab cause if not you fail on import. \n\nThe problem was that It Is not updated.", "@ravikyram If you scope is just the reproducibility you can claim that it is reproducible and pass to the next level of the triaging protocol. As you can see It never print `3` and `4` in the `translate_pipeline`", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@ravikyram I think you need to add a label for the bot.", "How many cores are there on the machine?\r\n\r\nCan you recreate the issue when setting `num_parallel_calls` to the number of cores instead of `AUTOTUNE`? \r\n\r\nDoes the issue go away when you [configure](https://www.tensorflow.org/api_docs/python/tf/config/threading/set_inter_op_parallelism_threads) the inter-op parallelism threads to be greater than the number of available cores?\r\n\r\nI suspect that what's going on is that your machine has a small number of cores which is used as the default number of the inter-op thread threads and executing `tf.data.Dataset.map` call will need two threads in the inter-op threadpool -- one of the tf.data map function and one for the py_func. If there is only one thread left, this will deadlock.\r\n\r\nSetting the inter-op parallelism threads to a higher value should work around this problem. There is not much else that can be done to avoid this problem, short of reimplement chunks of TF runtime and py_func support, which is out of the scope of the tf.data team's charter.", "> How many cores are there on the machine?\r\n> \r\n> Can you recreate the issue when setting `num_parallel_calls` to the number of cores instead of `AUTOTUNE`?\r\n> \r\n> Does the issue go away when you [configure](https://www.tensorflow.org/api_docs/python/tf/config/threading/set_inter_op_parallelism_threads) the inter-op parallelism threads to be greater than the number of available cores?\r\n> \r\n> I suspect that what's going on is that your machine has a small number of cores which is used as the default number of the inter-op thread threads and executing `tf.data.Dataset.map` call will need two threads in the inter-op threadpool -- one of the tf.data map function and one for the py_func. If there is only one thread left, this will deadlock.\r\n> \r\n> Setting the inter-op parallelism threads to a higher value should work around this problem. There is not much else that can be done to avoid this problem, short of reimplement chunks of TF runtime and py_func support, which is out of the scope of the tf.data team's charter.\r\n\r\n```\r\nlscpu\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              4\r\nOn-line CPU(s) list: 0-3\r\nThread(s) per core:  2\r\nCore(s) per socket:  2\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               85\r\nModel name:          Intel(R) Xeon(R) CPU @ 2.00GHz\r\nStepping:            3\r\nCPU MHz:             2000.180\r\nBogoMIPS:            4000.36\r\nHypervisor vendor:   KVM\r\nVirtualization type: full\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            1024K\r\nL3 cache:            39424K\r\nNUMA node0 CPU(s):   0-3\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities ```\r\n", "@jsimsa It is not the `AUTOTUNE` cause it deadlocks also with `multiprocessing.cpu_count()` \r\n\r\nThe deadlock  is in the inter-op parallelism resources pool cause I've unlocked it with\r\n```\r\ntf.config.threading.set_inter_op_parallelism_threads(\r\n    multiprocessing.cpu_count()+1\r\n)\r\n```\r\n", "@jsimsa I don't know if a solution similar to https://github.com/tensorflow/tensorflow/commit/f5fcd1fdcf896f46aed03c7e61525b48b75d1acc could be valid ", "Was able to run your code in Tf Nightly 2.6, please confirm if your issue is fixed by checking the result in the gist [here](https://colab.research.google.com/gist/sachinprasadhs/2e6ef952359db914c2841bffbecd6bbb/42911.ipynb).", "> Was able to run your code in Tf Nightly 2.6, please confirm if your issue is fixed by checking the result in the gist [here](https://colab.research.google.com/gist/sachinprasadhs/2e6ef952359db914c2841bffbecd6bbb/42911.ipynb).\r\n\r\nFixed, thank you! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42911\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42911\">No</a>\n"]}, {"number": 42910, "title": "[INTEL MKL] Remove 'onednn' from Dockerfile names and image tags", "body": "The motivation for this PR is to remove `onednn` from Dockerfile(s) and Docker image tag(s) since the Dockerfile(s) reside in `onednn` folder anyway.\r\n\r\nLike before, to generate the OneDNN dockerfiles run the following commands:\r\n```\r\ncd tensorflow/tools/dockerfiles\r\n\r\nalias asm_dockerfiles=\"docker run --rm -u $(id -u):$(id -g) -v $(pwd):/tf tf-tools python3 assembler.py \"\r\n\r\nasm_dockerfiles --release dockerfiles --construct_dockerfiles\r\nsed -i '' -e 's/Copyright 2019 The TensorFlow Authors/Copyright 2020 The TensorFlow Authors/g' dockerfiles/onednn/ubuntu-*\r\n```\r\nto build the images run the following commands:\r\n```\r\nalias asm_images=\"docker run --rm -v $(pwd):/tf -v /var/run/docker.sock:/var/run/docker.sock tf-tools python3 assembler.py \"\r\n\r\nTF_VERSION=2.2.0 && asm_images --release onednn --repository intel/intel-optimized-tensorflow --arg BAZEL_VERSION=2.0.0 --arg TF_BRANCH=v${TF_VERSION} --arg TF_PACKAGE_VERSION=${TF_VERSION} --arg _TAG_PREFIX=${TF_VERSION}-ubuntu --build_images --only_tags_matching '.*ubuntu-.*'\r\n```\r\nOnce done you have the following 36 images:\r\n```\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04-devel\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04-devel-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04-devel-mpi-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04-devel-mpi-horovod-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04-devel-mpich-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04-devel-mpich-horovod-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04-mpi-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04-mpi-horovod-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04-mpich-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-16.04-mpich-horovod-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04-devel\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04-devel-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04-devel-mpi-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04-devel-mpi-horovod-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04-devel-mpich-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04-devel-mpich-horovod-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04-mpi-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04-mpi-horovod-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04-mpich-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-18.04-mpich-horovod-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04-devel\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04-devel-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04-devel-mpi-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04-devel-mpi-horovod-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04-devel-mpich-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04-devel-mpich-horovod-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04-mpi-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04-mpi-horovod-jupyter\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04-mpich-horovod\r\nintel/intel-optimized-tensorflow:2.2.0-ubuntu-20.04-mpich-horovod-jupyter\r\n```", "comments": ["/assign @angerson "]}, {"number": 42909, "title": "Deprecated function setdiff1d still used in the tf source code", "body": "**System information**\r\n- I have written custom code\r\n- Linux Ubuntu 20.04\r\n- TensorFlow 2.3 installed using pip\r\n- Python 3.8.2\r\n\r\n**Current (unexpected) behavior**\r\nCalculating the gradient of the `reduce_prod` function raises this warning.\r\n> WARNING:tensorflow:From /home/prasanth/.local/pythonuserbase/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py:297: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\r\nInstructions for updating:\r\nThis op will be removed after the deprecation date. Please switch to tf.sets.difference().\r\n\r\n**Standalone code to reproduce the issue**\r\n(The warning will only be displayed once in a session)\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.ones(5)\r\nwith tf.GradientTape() as g:\r\n    g.watch(x)\r\n    y = tf.math.reduce_prod(x)\r\n\r\ngrad = g.gradient(y, x)\r\n```", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/9d4cbcc58a6b57b56c107d0ea30419ca/42909-tf-nightly.ipynb). Thanks!", "Note sure if this is the best approach, but modifying [/tensorflow/tensorflow/python/ops/math_grad.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py) as follows could fix it.\r\n\r\n1. Adding the following import statements:\r\n    ```\r\n    from tensorflow.python.ops import sparse_ops\r\n    from tensorflow.python.ops import sets_impl\r\n    ```\r\n2. In the definition of `_ProdGrad(op, grad)`, replacing\r\n    ```\r\n      other, _ = array_ops.setdiff1d(idx, reduced)\r\n    ```\r\n    with\r\n    ```\r\n      other = sparse_ops.sparse_tensor_to_dense(sets_impl.set_difference([idx], [reduced]))[0]\r\n    ```", "The issue could be fixed by using the internal non-deprecated version of `gen_array_ops.list_diff` (instead of the deprecated `setdiff1d` API endpoint). Created a PR #42935 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42909\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42909\">No</a>\n"]}, {"number": 42908, "title": "[TFLM] Fix resize_nearest_neighbor test for debug build.", "body": "This fixes #42832 using the patch attached to that issue by @andrewstevens-infineon \r\n\r\nAdditionally, added the debug build to the continuous integration to prevent such issues in the future.\r\n", "comments": []}, {"number": 42907, "title": "Update contribution guidlines with developer workflow.", "body": "Making progress towards making the contribution guidelines have more detail (per\r\nissue #42569)\r\n\r\nSpecifically, this PR:\r\n\r\n * Adds some workflows before creating a PR and during the PR review process.\r\n\r\n * Add some tips that I have found useful as a PR reviewer to keep things moving\r\n   along (especially since not all the checks in Google's internal CI system have\r\n   a counterpart that is visible to external developers.\r\n\r\nI'm also trying out a semi-automated way to generate a table of content for our\r\nmarkdown files using [this tool](https://github.com/ekalinin/github-markdown-toc#auto-insert-and-update-toc).\r\n\r\n", "comments": []}, {"number": 42906, "title": "Please release TFLite wheels for Beaglebone", "body": "The [prebuilt wheels](https://www.tensorflow.org/lite/guide/python) of the Tensorflow Lite interpreter for armv7l do not run on the [Beaglebone](https://beagleboard.org/black/), triggering an \"invalid instruction\" error. They seem to have been built for the Raspberry Pi using the `-mfpu=neon-vfpv4` build option which is appropriate for that hardware, but not all armv7l devices.\r\n\r\nPull request #28724 added a `bbb` make target which produces a usable library on the Beaglebone, but when building the wheel, the code still assumes `rpi`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/build_pip_package.sh#L42-L45\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/build_pip_package.sh#L85-L87\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/setup.py#L54-L56\r\n\r\nSince it is rather complicated to build, it would be great if official releases could be made. And in the interim, it would be helpful to note on the releases page what hardware features the builds require, to avoid confusion and frustration over incompatible wheels. ", "comments": ["I don't have a Beaglebone device.\r\nCould you share the result of the following command?\r\n ```\r\n$ cat /proc/cpuinfo \r\n```", "This is from a Beaglebone Blue. I think it is the same CPU as the Beaglebone Black. Note that `vfpv4` is not supported by the processor.\r\n\r\n```\r\nroot@arm:~# cat /proc/cpuinfo\r\nprocessor\t: 0\r\nmodel name\t: ARMv7 Processor rev 2 (v7l)\r\nBogoMIPS\t: 597.19\r\nFeatures\t: half thumb fastmult vfp edsp thumbee neon vfpv3 tls vfpd32 \r\nCPU implementer\t: 0x41\r\nCPU architecture: 7\r\nCPU variant\t: 0x3\r\nCPU part\t: 0xc08\r\nCPU revision\t: 2\r\n\r\nHardware\t: Generic AM33XX (Flattened Device Tree)\r\nRevision\t: 0000\r\nSerial\t\t: 0000000000000000\r\n```", "I am trying to track this down as well. \r\n\r\nI'd like to get the associated wheels.\r\nhttps://www.tensorflow.org/lite/guide/python\r\nbuild for Beagle Bone Black. \r\n\r\nIllegal Instruction upon invoking models which invoke fine on an RPI using the same prebuild 3.5 wheel.\r\n\r\nI am trying to build a Pi Wheel for a Stretch Beagle Bone Black:\r\n\r\nMy steps are:\r\ngit clone \"tensorflow repo\"\r\ngit checkout tags/v2.2.0\r\n\r\n\r\nInside tensorflow\\tensorflow\\lite\\tools\\make\\targets\r\nChange the instances of mfpu=neon-vfpv4 to mfpu=neon inside the rpi_makefile.inc.\r\n\r\nThen go into the following directory and issue:\r\ntensorflow/tensorflow/lite/tools/pip_package\r\nmake BASE_IMAGE=debian:stretch PYTHON=python3 TENSORFLOW_TARGET=rpi docker-build\r\n\r\n\r\nI then installed the built pip package on a beagle bone black running stretch. \r\n\r\n\r\n**Works great.** Models that caused an Illegal Instruction before work fine. Results between x86 and arm7l are near identical. \r\n\r\nHere is a Beagle Bone Black cpuinfo output.\r\n\r\ndebian@beaglebone:~$ cat /proc/cpuinfo\r\nprocessor       : 0\r\nmodel name      : ARMv7 Processor rev 2 (v7l)\r\nBogoMIPS        : 995.32\r\nFeatures        : half thumb fastmult vfp edsp thumbee neon vfpv3 tls vfpd32\r\nCPU implementer : 0x41\r\nCPU architecture: 7\r\nCPU variant     : 0x3\r\nCPU part        : 0xc08\r\nCPU revision    : 2\r\n\r\nHardware        : Generic AM33XX (Flattened Device Tree)\r\nRevision        : 0000\r\nSerial          : 2916BBBK0A58", "Use the `bbb` target where available. The lines I linked to in the top post are where you need to hack the code to use `bbb` where it says `rpi`. Good luck. ", "Does anyone have a tflite wheel that they already compiled for the beaglebone black?", "Nevermind, I compiled using @bobharris instructions and got it working. [Here is the wheel](https://drive.google.com/file/d/1Pjf_ML35IzesaGlnz4lANW_bu37nQXYU/view?usp=sharing)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42906\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42906\">No</a>\n"]}, {"number": 42905, "title": "module 'tensorflow.keras.layers' has no attribute 'MultiHeadAttention'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary (pip install tensor flow)\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nwhen try to import official.nlp.bert.bert_models it shows error (try to reimplement the code in  https://www.tensorflow.org/official_models/fine_tuning_bert)\r\n\r\nmodule 'tensorflow.keras.layers' has no attribute 'MultiHeadAttention'\r\n\r\n**Describe the expected behavior**\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nIt's a little weird because when I check the current master branch of tensorflow, there is multiheadattention in keras.layers. However, there is no in version 2.3.0. I think I can proceed with the master branch. It seems that with `pip install tensorflow -U` I can only install 2.3.0. Is there any way to solve the problem? Maybe I should download the source code and compile by myself?\r\n", "comments": ["already solved. seems everything is Ok after `pip install tf-models-official==2.3.0`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42905\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42905\">No</a>\n"]}, {"number": 42904, "title": "[MLIR] Add generatedNames argument to LowerAddNOp", "body": "The generated names argument is used in the cost graph to determine the shortest path of patterns that legalizes an operation. I added this argument to LowerAddNOp so that when this same legalization will be used in the TF to TFL conversion (in this PR: https://github.com/tensorflow/tensorflow/pull/42525) it does not unintendly use the TF Addn -> TF Add -> TFL Add path instead of TF Addn -> TFL Addn", "comments": []}, {"number": 42902, "title": "Fix 1.15 build and run issues on iOS", "body": "- Fix `download_dependencies.sh` to use Google archive mirror for the Eigen library (The bitbucket archive link was dead because Eigen had moved to Gitlab)\r\n\r\n- Temporarily fix crash issue on iOS runs (#29627) caused by the fact that `CPU_utils.h` tries to access a [prohibited register on iOS](https://github.com/xianyi/OpenBLAS/issues/2281#issuecomment-541154490). (It may actually be the case that we just need a [register permission set](https://android.googlesource.com/kernel/bcm/+/android-wear-5.0.2_r0.5/arch/arm64/include/asm/arch_timer.h#110), but I will leave that to the experts).\r\n\r\n@mihaimaruseac ", "comments": ["Will merge as soon as we are ready to issue a patch release"]}, {"number": 42901, "title": "[WIP] Disable TFTRT conversion of ResizeBilinear Nodes when align_corners=false", "body": "ResizeBilinear align_corners = false is not supported as of TRT 7.1, and therefore should not be converted by TFTRT.  The goal of this PR is to disable that conversion.\r\n\r\n\r\n@bixia1 @tfeher @DEKHTIARJonathan for visibility", "comments": ["To clarify, this PR is needed for TensorRT versions >= 7.1", "@MattConley Can you please check @bixia1's comments and keep us posted ? Thanks!"]}, {"number": 42900, "title": "experimental_steps_per_execution option discarded when loading a model from checkpoint", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Google Colab, TPU\r\n- TensorFlow version: 2.3\r\n- Python version: 3\r\n\r\n**Describe the current behavior**\r\nI have created and trained a model compiled with the `experimental_steps_per_execution` option. \r\nWhen I load the model through `tf.keras.models.load_model` to resume the training the option `experimental_steps_per_execution' is not taken in consideration and keras perform `on_step...` actions after every step and not after n steps as specified in `experimental_steps_per_execution` when the model was first compiled.\r\n\r\n**Describe the expected behavior**\r\nThe model should load the `experimental_steps_per_execution' option or there should be an option to specify `experimental_steps_per_execution` when loading the model.\r\n\r\nIf necessary I can modify an MNIST example on Colab, but to reproduce this bug it is just needed to:\r\n\r\n- create a model\r\n- compile it with the `experimental_steps_per_execution` option enabled\r\n- train it for a few epoch to verify the correct behaviour of `experimental_steps_per_execution`\r\n- save the model\r\n- load the model through `tf.keras.models.load_model`\r\n- train the model for a few epoch to verify that `experimental_steps_per_execution` is being disregarded\r\n\r\n**Other info / logs** \r\nOne possible workaroud (which I am testing right now) is to set manually the steps per execution with `model._configure_steps_per_execution(experimental_steps_per_execution)` once it is loaded.\r\n\r\nThank you for your help!\r\n", "comments": ["@aurelio-amerio \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "Here is a notebook to reproduce the issue as well as the \"workaround\" implemented:\r\nhttps://colab.research.google.com/drive/1qeleit1tIzRY_1AmF0r_SQ4XtbaWoLNl?usp=sharing\r\n\r\nThank you", "I think it is in the ignored properties https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L204", "@aurelio-amerio \r\n\r\nI tried in colab with TF nightly version and i am not seeing any issue .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/2d0207d4eba4b8ce09f8f00ce8462f91/untitled315.ipynb).\r\n\r\nAlso, The argument `steps_per_execution` is no longer experimental. Pass `steps_per_execution` instead of `experimental_steps_per_execution`.\r\n\r\nPlease, verify once and close the issue. Thanks!", "Thank you for your time, everything seems to be working properly in the nightly version, I'm looking forward to the 2.4 release!\r\n\r\n@ravikyram Have a nice day!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42900\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42900\">No</a>\n"]}, {"number": 42899, "title": "How preprocess image from tf.io.decode_raw ?", "body": "I want to use tf.keras.preprocessing.image.random_rotation and other function to get more dataset , but these image from \r\n```\r\ntf.io.decode_raw(features['image'], out_type=tf.uint8)\r\n```\r\nso it's tensor image , tf.keras.preprocessing.image.random_rotation must be numpy array ,anyone know how can I do?", "comments": ["I think that you can use something like https://colab.research.google.com/gist/jvishnuvardhan/4698ae014a3960ce271a700de419200a/39237-tf-nightly.ipynb", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42898, "title": "How to reduce the size of the tflite model below 200mb while converting h5 to tflite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.2.0\r\n\r\n\r\n\r\n\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://gist.github.com/SaiBalaji22/e3815c47970416c37328e9f69a43c958\r\n\r\n`\r\n\r\n**Failure details**\r\n\r\nI have made an image classifier model to classifiy rock,paper,scissor images.But the problem is i was able to convert it to tflite model but the size of the model exceeds 200mb.I want to use it in android studio.When i add the model it give me error.How can i reduce the tflite model size while conversion.\r\n![image](https://user-images.githubusercontent.com/51410810/92009720-8ca08a80-ed66-11ea-8f3b-7a2affb455ba.png)\r\n\r\n\r\n\r\n", "comments": ["You could try post_training_quantiziation https://www.tensorflow.org/lite/performance/post_training_quantization", "Also see [weight quantization](https://www.tensorflow.org/model_optimization/guide/quantization/post_training#quantizing_weights)\r\nSetting `OPTIMIZE_FOR_SIZE` flag can help reduce the model weights. Thanks!", "Thanks it worked but accuracy has been reduced.", "For Better performance you could explore https://www.tensorflow.org/model_optimization/guide/quantization/training"]}, {"number": 42897, "title": "[ROCm] Fix for ROCm CSB Breakage - 200902 - 1", "body": "The following commit introduces regressions in two unit-tests, in the ROCm TF build\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/458d0906ebe75072ebd87cf5088772405d0cd03e\r\n\r\n```\r\n//tensorflow/python/kernel_tests:conv_ops_3d_test_gpu                    FAILED in 3 out of 3 in 212.4s\r\n//tensorflow/python/kernel_tests:conv_ops_test_gpu                       FAILED in 3 out of 3 in 591.0s\r\n```\r\n\r\nThe failures are within subtests that test padding for the float16 type.\r\n\r\nThe cause of failure is a strange one. In the ROCm TF build,\r\n* the call(s) to the `PadInput` functor are in the files (conv_*.cc) that are compiled by the \"CPU\" compiler, while the\r\n++ the GPUDevice specific template instantiations of the  `PadInput` functor are in the files that are compiled by the \"GPU\" compiler.\r\n\r\n\r\nFor T == Eigen::half, the value of the \"padding_value\" argument (prior to this commit, when it was pass-by-value) was getting corrupted, leading to regressions in the convolution unit tests.\r\n\r\nI do not understand the exact reason for the this, but based on similar past issues, it is likely due to a combination of\r\n* an ABI incompatibility between the \"old\" CPU compiler (gcc 5.4 for Ubuntu 16.04, gcc 7.5 for Ubuntu 18.04) and the \"new\" ROCm GPU compiler (hipclang which is based on latest clang), AND\r\n* Eigen::half having the same size but different internals on the CPU and GPU sides (unsigned short on CPU, union of {unsigned short, _Float16} on GPU\r\n\r\nChanging the \"padding value\" argument to be a const reference type seems to suppress the bug\r\n\r\nThis workaround can be removed once the \"CPU\" compiler is gcc8 or higher.\r\n\r\n--------------------------------------------\r\n\r\n/cc @chsigg @cheshire @nvining-work \r\n", "comments": ["This fix does not seem to address all cases....please ignore for now.  I will be pushing out a new commit to this PR in a day or two", "pushed out a commit with a different fix, that works for the failures mentioned in the description of this PR and another one I will be filing shortly", "@chsigg gentle ping", "@chsigg gentle ping", "@chsigg gentle ping", "@chsigg gentle ping", "@chsigg gentle ping", "@cheshire \r\n\r\nThe second compiler is needed to handle GPU specific code (i..e all the `*.cu.cc` files). That has code within it (GPU kernels) for which we need to generate GPU code objects (binaries) and that can only be done by GPU compilers.", "why we can't compile the whole thing with the GPU compiler then to avoid\nsuch issues?\n\nOn Mon, Sep 21, 2020 at 10:39 AM Deven Desai <notifications@github.com>\nwrote:\n\n> @cheshire <https://github.com/cheshire>\n>\n> The second compiler is needed to handle GPU specific code (i..e all the\n> *.cu.cc files). That has code within it (GPU kernels) for which we need\n> to generate GPU code objects (binaries) and that can only be done by GPU\n> compilers.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/42897#issuecomment-696264490>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH3MGYUGICKR25VQSIDSG6FVDANCNFSM4QTH5G2A>\n> .\n>\n", "> why we can't compile the whole thing with the GPU compiler then to avoid such issues?\r\n\r\nThe ROCm TF compilation is based along the same lines as the CUDA counterpart (which sends the `*.cu.cc` files to nvcc and the rest to the host compiler).  As to why not send all the files to hipcc/nvcc, think that is really a question for the TF team that setup the initial TF GPU build flow.\r\n", "@rthadur this PR seems to gotten stuck in the merge pipeline..please checkin on it...thanks"]}, {"number": 42896, "title": "Documentation is becoming too undecipherable for a meaningful practical work", "body": "The documentation is becoming too undecipherable for any practical applications using Tensorflow.\r\n\r\nLook at this one, https://www.tensorflow.org/guide/data\r\n\r\nHow would anyone learn anything for a streaming data from Disk ? Not many how-tos for data that don't fit into memory. And that page.. just like a man-page for API. We end up wasting many man hours without good documentation.", "comments": ["Is this a duplicate of https://github.com/tensorflow/tensorflow/issues/32938?", "Can you send PRs to fix documentation? This is an evolving process, we try to get documentation that is as useful as possible to as many people as possible.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42895, "title": "Does TensorFlow1.x support CUDA11? And how can i compile tensorflow 1.x with cuda11?", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["I don't think it will be officially supported for TF 1.x. Please ask at https://github.com/tensorflow/tensorflow/issues/42047", "No. Each CUDA support is tied with a version of TF and is impossible to backport to older releases.\r\n\r\nI would say community can create builds with newer CUDA and older TF, but it is likely that the compile process will fail, since we also change TF code to be in sync with CUDA specs. Hence, even backporting some patches to the release branches is impossible.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42894, "title": "Keras load_model breaks for hdf5 files on Google Cloud Storage bucket", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9 (TensorFlow:2.3 image on GCP) \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nCalling `tensorflow.keras.model.load_model` with hdf5 serialized models on GCS breaks\r\n```\r\nfrom tensorflow.keras.models import load_model\r\n\r\npath = 'gs://mybucket/foo.hdf5'  # anonymized\r\nload_model(path)\r\n```\r\ngives\r\n```\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-7-33783c76cb4e> in <module>\r\n      1 if config.model['build']['middle_ear'] is not None:\r\n----> 2     config.model['build']['middle_ear'] = load_model(config.model['build']['middle_ear']['path'])\r\n\r\n~/.venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile, options)\r\n    184     filepath = path_to_string(filepath)\r\n    185     if isinstance(filepath, six.string_types):\r\n--> 186       loader_impl.parse_saved_model(filepath)\r\n    187       return saved_model_load.load(filepath, compile, options)\r\n    188 \r\n\r\n~/.venv/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py in parse_saved_model(export_dir)\r\n    111                   (export_dir,\r\n    112                    constants.SAVED_MODEL_FILENAME_PBTXT,\r\n--> 113                    constants.SAVED_MODEL_FILENAME_PB, path_to_pbtxt, path_to_pb))\r\n    114 \r\n    115 \r\n\r\nOSError: SavedModel file does not exist at: gs://mybucket/foo.hdf5/{saved_model.pbtxt|saved_model.pb}.\r\n```\r\nwhile loading the same file locally works fine\r\n```\r\nload_model('/tmp/081.hdf5')\r\n<tensorflow.python.keras.engine.functional.Functional at 0x7f49f0276250>\r\n```\r\nI've tracked it down to the h5py check [here](https://github.com/tensorflow/tensorflow/blob/963e2693da908b0fa3d0174ac16c821d5bcc66fb/tensorflow/python/keras/saving/save.py#L182), which seems to always evaluate to false for `gs://` filepaths\r\n```\r\n>>> import h5py\r\n>>> h5py.is_hdf5('gs://mybucket/foo.hdf5')\r\nFalse\r\n>>> h5py.is_hdf5('/tmp/foo.hdf5')\r\nTrue\r\n```\r\nwhich is why it falsely ends up [here](https://github.com/tensorflow/tensorflow/blob/963e2693da908b0fa3d0174ac16c821d5bcc66fb/tensorflow/python/keras/saving/save.py#L188) in the case analysis.\r\n\r\n**Describe the expected behavior**\r\n\r\nhdf5 serialized models should be loadable from GCS buckets\r\n\r\n", "comments": ["We already have a tracker for this issue. See https://github.com/tensorflow/tensorflow/issues/36453\r\nClosing this issue since it's a duplicate. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42894\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42894\">No</a>\n"]}, {"number": 42893, "title": "\u201cNo dashboards are active for the current dataset\u201d despite the event file is created correctly", "body": "**System information**\r\n- OS Platform and Distribution: Win 10\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.9\r\n- tensorboard 2.2.1\r\n- jupyter notebook 6.1.6\r\n- same problem with spyder 4.1.4\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\nI trained a CNN and created log files for tensorboard successfully. Now I want to optimize the model with tensorboard. For that I called \"cmd\" in Windows 10, activated the correct environment and typed:\r\n`tensorboard --logdir=C:\\Users\\bauerch\\Documents\\Py_DL_Env\\DL_Sentdex\\logs`\r\n\r\nWhen I go to http://localhost:6006/ in my browser I get the following result:\r\n![grafik](https://user-images.githubusercontent.com/67313472/91977423-f39c5000-ed22-11ea-8696-0f6053a719de.png)\r\n\r\n**As you can see at the bottom of the picture there is written \"Data location: /tmp/MNIST_data\". But I created the event file in the directory named above.**\r\n\r\nWhat I tried so far without success:\r\n- https://github.com/tensorflow/tensorboard/blob/master/README.md#my-tensorboard-isnt-showing-any-data-whats-wrong.\r\n- https://stackoverflow.com/questions/47113472/tensorboard-error-no-dashboards-are-active-for-current-data-set\r\n- https://github.com/tensorflow/tensorflow/issues/7856\r\n\r\n**Describe the expected behavior**\r\nTensorBoard should be opened without a problem via http://localhost:6006/\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\r\nimport pickle\r\nimport numpy as np\r\nfrom tensorflow.keras.callbacks import TensorBoard \r\nimport time\r\n\r\n# Tensorboard\r\nNAME = \"Cats-vs-dog-cnn-64x2-{}\".format(int(time.time())) \r\ntensorboard = TensorBoard(log_dir=\"logs\\\\{}\".format(NAME))\r\n\r\n# Data import\r\nX = pickle.load(open(\"X.pickle\",\"rb\")) \r\ny = pickle.load(open(\"y.pickle\",\"rb\"))\r\n\r\n# Model\r\nmodel = Sequential()\r\n### 1.Layer\r\nmodel.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n### 2.Layer\r\nmodel.add(Conv2D(64, (3, 3)))\r\nmodel.add(Activation('relu')) \r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n### 3.Layer\r\nmodel.add(Flatten()) \r\nmodel.add(Dense(64))\r\nmodel.add(Activation('relu')) # Activation function muss man stets hinzuf\u00fcgen, da man sonst\r\n# einen sinnlosen linearen Output aus den Neuronen bekommt\r\n### 4.Output Layer\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n\r\n# Compile\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\n# Training\r\nmodel.fit(X, y, batch_size=32, epochs=2, validation_split=0.3, callbacks=[tensorboard])\r\n```\r\n", "comments": ["@chriuxenman1 \r\n\r\nCan you please go through [link1](https://github.com/tensorflow/tensorboard/issues/1306), [link2](https://stackoverflow.com/questions/47113472/tensorboard-error-no-dashboards-are-active-for-current-data-set?rq=1) and see if it helps you.\r\n\r\nThis issue is more suitable for Tensorboard repo. Please post it on tensorboard  repo from [here.](https://github.com/tensorflow/tensorboard/issues/new/choose) Thanks!", "Is your `log_dir` really `C:\\Users\\bauerch\\Documents\\Py_DL_Env\\DL_Sentdex\\logs`?", "@ravikyram I tried both links already but it did not work. I will post it on the tensorboard repo, thank you.\r\n\r\n@bhack Yes my `log_dir `is really `C:\\Users\\bauerch\\Documents\\Py_DL_Env\\DL_Sentdex\\logs`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42893\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42893\">No</a>\n", "@chriuxenman1 If you can see the logs in that folder have you tried double quoting this path when running the tensorboard?", "@bhack How do I double quote?", "Just to be sure:\ncd {your_logdir}\ndir (to verify that you have logs inside current dir\" \ntensorboard --logdir ./"]}, {"number": 42892, "title": "TFLite compiling error", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): 2.2.0 (downgraded from 2.3.0)\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nmodel_name = \"new_model\"\r\nimport_dir = \".\\exported-models\\\\\" + model_name + \"\\saved_model\"\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(import_dir)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n\r\n# Save the TF Lite model.\r\nwith tf.io.gfile.GFile(\"exported-models\\\\\" + model_name + \".tflite\", 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"custom_convert.py\", line 25, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Users\\Workstation\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 920, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"C:\\Users\\Workstation\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 751, in convert\r\n    return super(TFLiteFrozenGraphConverterV2,\r\n  File \"C:\\Users\\Workstation\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 494, in convert\r\n    result = _toco_convert_impl(\r\n  File \"C:\\Users\\Workstation\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 546, in toco_convert_impl\r\n    data = toco_convert_protos(\r\n  File \"C:\\Users\\Workstation\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 256, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-09-02 12:55:48.008380: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:307] Ignored output_format.\r\n2020-09-02 12:55:48.008562: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:310] Ignored drop_control_dependency.\r\nloc(\"Func/StatefulPartitionedCall/input/_1\"): error: requires all operands and results to have compatible element types\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\workstation\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"c:\\users\\workstation\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Workstation\\anaconda3\\envs\\tensorflow\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\workstation\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\users\\workstation\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\users\\workstation\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"c:\\users\\workstation\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\users\\workstation\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 50, in execute\r\n    output_str = _pywrap_toco_api.TocoConvert(\r\nException: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_1\"): requires all operands and results to have compatible element types\r\n<unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_1\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x?x?x3x!tf.quint8>) -> tensor<1x?x?x3xui8>\r\n```\r\n\r\n**When I uninstaled tf-nightly and reinstalled tensorflow 2.2.0 i get this output after trying to convert**\r\n```\r\nTraceback (most recent call last):\r\n  File \"custom_convert.py\", line 25, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Users\\Workstation\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 480, in convert\r\n    raise ValueError(\r\nValueError: None is only supported in the 1st dimension. Tensor 'input_tensor' has invalid shape '[1, None, None, 3]'.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://cloud.zbe.si/s/ADNzzjizn82WtHe\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\nError mid converting the file. I had to downgrade from 2.3.0 to 2.2.0 so I could load tflite model into my flutter app. Now i cant even convert the file without installing tf-nightly, which repeats the same error in flutter.\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Can you share your model definition? Thanks", "@pablo3x6,\r\nI was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/647d4f64a328b79b97da91120b3eca05/42892.ipynb). \r\n\r\nHowever, the error seems to be fixed with the latest TF-nightly. I was able to convert the model without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/077b48fad6a2139af7cc47e4ae365d73/42892-tf-nightly.ipynb). Thanks!", "@amahendrakar \r\nI would need to convert this in version 2.2.0 since if I use tf-nightly, I get `java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '5'` when I try to load it into my flutter app using the tflite package.\r\n\r\n@bhack \r\nI'm new to this, by model definition you mean my pre-trained models?", "Your model definition, compile, and save code to reproduce this", "For compiling im using the model_main_tf2.py file\r\n\r\ncommand: ```python model_main_tf2.py --model_dir=pre-trained-models\\ssd_resnet50_v1_fpn_640x640_coco17_tpu-8 --pipeline_config_path=pre-trained-models\\ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\\pipeline.config```\r\n\r\n```\r\n# Lint as: python3\r\n# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\r\nr\"\"\"Creates and runs TF2 object detection models.\r\n\r\nFor local training/evaluation run:\r\nPIPELINE_CONFIG_PATH=path/to/pipeline.config\r\nMODEL_DIR=/tmp/model_outputs\r\nNUM_TRAIN_STEPS=10000\r\nSAMPLE_1_OF_N_EVAL_EXAMPLES=1\r\npython model_main_tf2.py -- \\\r\n  --model_dir=$MODEL_DIR --num_train_steps=$NUM_TRAIN_STEPS \\\r\n  --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \\\r\n  --pipeline_config_path=$PIPELINE_CONFIG_PATH \\\r\n  --alsologtostderr\r\n\"\"\"\r\nfrom absl import flags\r\nimport tensorflow.compat.v2 as tf\r\nfrom object_detection import model_lib_v2\r\n\r\nflags.DEFINE_string('pipeline_config_path', None, 'Path to pipeline config '\r\n                    'file.')\r\nflags.DEFINE_integer('num_train_steps', None, 'Number of train steps.')\r\nflags.DEFINE_bool('eval_on_train_data', False, 'Enable evaluating on train '\r\n                  'data (only supported in distributed training).')\r\nflags.DEFINE_integer('sample_1_of_n_eval_examples', None, 'Will sample one of '\r\n                     'every n eval input examples, where n is provided.')\r\nflags.DEFINE_integer('sample_1_of_n_eval_on_train_examples', 5, 'Will sample '\r\n                     'one of every n train input examples for evaluation, '\r\n                     'where n is provided. This is only used if '\r\n                     '`eval_training_data` is True.')\r\nflags.DEFINE_string(\r\n    'model_dir', None, 'Path to output model directory '\r\n                       'where event and checkpoint files will be written.')\r\nflags.DEFINE_string(\r\n    'checkpoint_dir', None, 'Path to directory holding a checkpoint.  If '\r\n    '`checkpoint_dir` is provided, this binary operates in eval-only mode, '\r\n    'writing resulting metrics to `model_dir`.')\r\n\r\nflags.DEFINE_integer('eval_timeout', 3600, 'Number of seconds to wait for an'\r\n                     'evaluation checkpoint before exiting.')\r\n\r\nflags.DEFINE_bool('use_tpu', False, 'Whether the job is executing on a TPU.')\r\nflags.DEFINE_string(\r\n    'tpu_name',\r\n    default=None,\r\n    help='Name of the Cloud TPU for Cluster Resolvers.')\r\nflags.DEFINE_integer(\r\n    'num_workers', 1, 'When num_workers > 1, training uses '\r\n    'MultiWorkerMirroredStrategy. When num_workers = 1 it uses '\r\n    'MirroredStrategy.')\r\nflags.DEFINE_integer(\r\n    'checkpoint_every_n', 1000, 'Integer defining how often we checkpoint.')\r\nflags.DEFINE_boolean('record_summaries', True,\r\n                     ('Whether or not to record summaries during'\r\n                      ' training.'))\r\n\r\nFLAGS = flags.FLAGS\r\n\r\n\r\ndef main(unused_argv):\r\n  flags.mark_flag_as_required('model_dir')\r\n  flags.mark_flag_as_required('pipeline_config_path')\r\n  tf.config.set_soft_device_placement(True)\r\n\r\n  if FLAGS.checkpoint_dir:\r\n    model_lib_v2.eval_continuously(\r\n        pipeline_config_path=FLAGS.pipeline_config_path,\r\n        model_dir=FLAGS.model_dir,\r\n        train_steps=FLAGS.num_train_steps,\r\n        sample_1_of_n_eval_examples=FLAGS.sample_1_of_n_eval_examples,\r\n        sample_1_of_n_eval_on_train_examples=(\r\n            FLAGS.sample_1_of_n_eval_on_train_examples),\r\n        checkpoint_dir=FLAGS.checkpoint_dir,\r\n        wait_interval=300, timeout=FLAGS.eval_timeout)\r\n  else:\r\n    if FLAGS.use_tpu:\r\n      # TPU is automatically inferred if tpu_name is None and\r\n      # we are running under cloud ai-platform.\r\n      resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\r\n          FLAGS.tpu_name)\r\n      tf.config.experimental_connect_to_cluster(resolver)\r\n      tf.tpu.experimental.initialize_tpu_system(resolver)\r\n      strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n    elif FLAGS.num_workers > 1:\r\n      strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n    else:\r\n      strategy = tf.compat.v2.distribute.MirroredStrategy()\r\n\r\n    with strategy.scope():\r\n      model_lib_v2.train_loop(\r\n          pipeline_config_path=FLAGS.pipeline_config_path,\r\n          model_dir=FLAGS.model_dir,\r\n          train_steps=FLAGS.num_train_steps,\r\n          use_tpu=FLAGS.use_tpu,\r\n          checkpoint_every_n=FLAGS.checkpoint_every_n,\r\n          record_summaries=FLAGS.record_summaries)\r\n\r\nif __name__ == '__main__':\r\n  tf.compat.v1.app.run()\r\n```\r\n\r\nand then im exporting it with exporter_main_tf2.py\r\n\r\ncommand: ```python .\\exporter_main_v2.py --input_type image_tensor --pipeline_config_path .\\pre-trained-models\\ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\\pipeline.config --trained_checkpoint_dir .\\pre-trained-models\\ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\\ --output_directory .\\exported-models\\new_model```\r\n\r\n```\r\n# Lint as: python2, python3\r\n# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\r\nr\"\"\"Tool to export an object detection model for inference.\r\n\r\nPrepares an object detection tensorflow graph for inference using model\r\nconfiguration and a trained checkpoint. Outputs associated checkpoint files,\r\na SavedModel, and a copy of the model config.\r\n\r\nThe inference graph contains one of three input nodes depending on the user\r\nspecified option.\r\n  * `image_tensor`: Accepts a uint8 4-D tensor of shape [1, None, None, 3]\r\n  * `float_image_tensor`: Accepts a float32 4-D tensor of shape\r\n    [1, None, None, 3]\r\n  * `encoded_image_string_tensor`: Accepts a 1-D string tensor of shape [None]\r\n    containing encoded PNG or JPEG images. Image resolutions are expected to be\r\n    the same if more than 1 image is provided.\r\n  * `tf_example`: Accepts a 1-D string tensor of shape [None] containing\r\n    serialized TFExample protos. Image resolutions are expected to be the same\r\n    if more than 1 image is provided.\r\n\r\nand the following output nodes returned by the model.postprocess(..):\r\n  * `num_detections`: Outputs float32 tensors of the form [batch]\r\n      that specifies the number of valid boxes per image in the batch.\r\n  * `detection_boxes`: Outputs float32 tensors of the form\r\n      [batch, num_boxes, 4] containing detected boxes.\r\n  * `detection_scores`: Outputs float32 tensors of the form\r\n      [batch, num_boxes] containing class scores for the detections.\r\n  * `detection_classes`: Outputs float32 tensors of the form\r\n      [batch, num_boxes] containing classes for the detections.\r\n\r\n\r\nExample Usage:\r\n--------------\r\npython exporter_main_v2.py \\\r\n    --input_type image_tensor \\\r\n    --pipeline_config_path path/to/ssd_inception_v2.config \\\r\n    --trained_checkpoint_dir path/to/checkpoint \\\r\n    --output_directory path/to/exported_model_directory\r\n    --use_side_inputs True/False \\\r\n    --side_input_shapes dim_0,dim_1,...dim_a/.../dim_0,dim_1,...,dim_z \\\r\n    --side_input_names name_a,name_b,...,name_c \\\r\n    --side_input_types type_1,type_2\r\n\r\nThe expected output would be in the directory\r\npath/to/exported_model_directory (which is created if it does not exist)\r\nholding two subdirectories (corresponding to checkpoint and SavedModel,\r\nrespectively) and a copy of the pipeline config.\r\n\r\nConfig overrides (see the `config_override` flag) are text protobufs\r\n(also of type pipeline_pb2.TrainEvalPipelineConfig) which are used to override\r\ncertain fields in the provided pipeline_config_path.  These are useful for\r\nmaking small changes to the inference graph that differ from the training or\r\neval config.\r\n\r\nExample Usage (in which we change the second stage post-processing score\r\nthreshold to be 0.5):\r\n\r\npython exporter_main_v2.py \\\r\n    --input_type image_tensor \\\r\n    --pipeline_config_path path/to/ssd_inception_v2.config \\\r\n    --trained_checkpoint_dir path/to/checkpoint \\\r\n    --output_directory path/to/exported_model_directory \\\r\n    --config_override \" \\\r\n            model{ \\\r\n              faster_rcnn { \\\r\n                second_stage_post_processing { \\\r\n                  batch_non_max_suppression { \\\r\n                    score_threshold: 0.5 \\\r\n                  } \\\r\n                } \\\r\n              } \\\r\n            }\"\r\n\r\nIf side inputs are desired, the following arguments could be appended\r\n(the example below is for Context R-CNN).\r\n   --use_side_inputs True \\\r\n   --side_input_shapes 1,2000,2057/1 \\\r\n   --side_input_names context_features,valid_context_size \\\r\n   --side_input_types tf.float32,tf.int32\r\n\"\"\"\r\nfrom absl import app\r\nfrom absl import flags\r\n\r\nimport tensorflow.compat.v2 as tf\r\nfrom google.protobuf import text_format\r\nfrom object_detection import exporter_lib_v2\r\nfrom object_detection.protos import pipeline_pb2\r\n\r\ntf.enable_v2_behavior()\r\n\r\n\r\nFLAGS = flags.FLAGS\r\n\r\nflags.DEFINE_string('input_type', 'image_tensor', 'Type of input node. Can be '\r\n                    'one of [`image_tensor`, `encoded_image_string_tensor`, '\r\n                    '`tf_example`, `float_image_tensor`]')\r\nflags.DEFINE_string('pipeline_config_path', None,\r\n                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '\r\n                    'file.')\r\nflags.DEFINE_string('trained_checkpoint_dir', None,\r\n                    'Path to trained checkpoint directory')\r\nflags.DEFINE_string('output_directory', None, 'Path to write outputs.')\r\nflags.DEFINE_string('config_override', '',\r\n                    'pipeline_pb2.TrainEvalPipelineConfig '\r\n                    'text proto to override pipeline_config_path.')\r\nflags.DEFINE_boolean('use_side_inputs', False,\r\n                     'If True, uses side inputs as well as image inputs.')\r\nflags.DEFINE_string('side_input_shapes', '',\r\n                    'If use_side_inputs is True, this explicitly sets '\r\n                    'the shape of the side input tensors to a fixed size. The '\r\n                    'dimensions are to be provided as a comma-separated list '\r\n                    'of integers. A value of -1 can be used for unknown '\r\n                    'dimensions. A `/` denotes a break, starting the shape of '\r\n                    'the next side input tensor. This flag is required if '\r\n                    'using side inputs.')\r\nflags.DEFINE_string('side_input_types', '',\r\n                    'If use_side_inputs is True, this explicitly sets '\r\n                    'the type of the side input tensors. The '\r\n                    'dimensions are to be provided as a comma-separated list '\r\n                    'of types, each of `string`, `integer`, or `float`. '\r\n                    'This flag is required if using side inputs.')\r\nflags.DEFINE_string('side_input_names', '',\r\n                    'If use_side_inputs is True, this explicitly sets '\r\n                    'the names of the side input tensors required by the model '\r\n                    'assuming the names will be a comma-separated list of '\r\n                    'strings. This flag is required if using side inputs.')\r\n\r\nflags.mark_flag_as_required('pipeline_config_path')\r\nflags.mark_flag_as_required('trained_checkpoint_dir')\r\nflags.mark_flag_as_required('output_directory')\r\n\r\n\r\ndef main(_):\r\n  pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\r\n  with tf.io.gfile.GFile(FLAGS.pipeline_config_path, 'r') as f:\r\n    text_format.Merge(f.read(), pipeline_config)\r\n  text_format.Merge(FLAGS.config_override, pipeline_config)\r\n  exporter_lib_v2.export_inference_graph(\r\n      FLAGS.input_type, pipeline_config, FLAGS.trained_checkpoint_dir,\r\n      FLAGS.output_directory, FLAGS.use_side_inputs, FLAGS.side_input_shapes,\r\n      FLAGS.side_input_types, FLAGS.side_input_names)\r\n\r\n\r\nif __name__ == '__main__':\r\n  app.run(main)\r\n```\r\n\r\nAfter that I tried to convert it to tflite.", "The pretrained model and error seems the same as https://github.com/tensorflow/tensorflow/issues/42079. Can you try that workaround?", "See also https://github.com/tensorflow/models/issues/9033", "@pablo3x6,\r\nAny updates regarding this issue? Thanks!\r\n", "I have tried fixes from other threads, looks like I wont be able to build a tflite model in 2.2.0\r\n\r\nGuess I'll just wait till binaries get updated in the flutter package I'm using.", "@pablo3x6,\r\nThank you for the update. Marking the issue as closed since the error is fixed in TF-nightly. \r\n\r\nPlease feel free to reopen the issue if you still face the error with the next release. Thanks!"]}, {"number": 42891, "title": "Tensorflow 2.3.0 Java API on Windows - UnsatisfiedLinkError: Cannot find TensorFlow native library for OS", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1 Pro (64bit)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: N/A (using Tensorflow Java API for Windows)\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A (CPU-only)\r\n\r\n**Describe the current behavior**\r\nCurrently I've problems to run the `HelloTensorFlow.java` with Tensorflow Java 2.3.0 API on Windows. \r\n\r\nThe start of `HelloTensorFlow.java` fails with the following error message:\r\n```\r\norg.tensorflow.NativeLibrary: jniResourceName: org/tensorflow/native/windows-x86_64/tensorflow_jni.dll\r\norg.tensorflow.NativeLibrary: frameworkResourceName: org/tensorflow/native/windows-x86_64/tensorflow_framework.dll\r\nException in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.\r\n\tat org.tensorflow.NativeLibrary.load(NativeLibrary.java:79)\r\n\tat org.tensorflow.TensorFlow.init(TensorFlow.java:67)\r\n\tat org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:82)\r\n\tat org.tensorflow.Graph.<clinit>(Graph.java:479)\r\n\tat com.test.HelloTensorFlow.main(HelloTensorFlow.java:12)\r\n```\r\n\r\n\r\nI double-checked that _-Djava.library.path_ contains the correct path to _tensorflow_jni.dll_. _Visual C++2015 Redistributable_ is also installed on my Windows system.\r\n\r\nI've tested the same `HelloTensorFlow.java` on macOS (with the corresponding macOS TF 2.3.0 native libs). Here everything works like a charm.\r\n\r\n**Standalone code to reproduce the issue**\r\nI've used \r\n- `tensorflow_jni.dll` downloaded at https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-cpu-windows-x86_64-2.3.0.zip\r\n- `libtensorflow-2.3.0.jar` downloaded at https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-2.3.0.jar\r\n- `HelloTensorFlow.java` (copied from https://www.tensorflow.org/install/lang_java)\r\n\r\nI've also added the necessary VM argument: -Djava.library.path=<path-to-tensorflow_jni.dll>\r\n", "comments": ["@FreddyLab \r\nPlease refer to below linksand let us know if it helps: \r\n[link](https://stackoverflow.com/questions/46303270/exception-in-thread-main-java-lang-unsatisfiedlinkerror-cannot-find-tensorflo), [link1](https://xbuba.com/questions/46303270)", "@Saduf2019 \r\nThanks for the quick reply and the links. I've checked them but unfortunately these hints do not help to solve my problem. \r\nI think my configuration is correct because if I replace the Windows TF Java 2.3.0 libs with older TF Java 1.14.0 libs (tensorflow_jni.dll + libtensorflow-1.14.0.jar) I can run the `HelloTensorFlow.java` without any problem:\r\n```\r\norg.tensorflow.NativeLibrary: isLoaded: true\r\norg.tensorflow.NativeLibrary: isLoaded: true\r\n2020-09-03 09:39:59.861845: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nHello from 1.14.0\r\n```\r\n\r\nThe `UnsatisfiedLinkError` ONLY occurs when using current Java TF 2.3.0 API libs (Windows).", "@FreddyLab Is the error occuring with tensorflow 2.2.0 or tf-nightly?", "@gowthamkpr I've only tested with Windows TF Java 2.3.0 libs so far (downloaded at https://www.tensorflow.org/install/lang_java#tensorflow_with_the_jdk). \r\n\r\nI haven't tested with TF 2.2 or tf-nightly yet, because I haven't found any links to download these Libtensorflow JNI packages versions. At the URL https://storage.googleapis.com/libtensorflow-nightly I only found libs for Linux and MacOs but none for Windows. Do you know where I can download TF 2.2 or nightly libtensorflow versions for Windows?", "Please redirect to TF java owners. I have no experience with our java releases.\r\nFor libtensorflow, please reach out to @av8ramit \r\nFor java, I do not know who would be responsible at the moment.", "> I haven't tested with TF 2.2 or tf-nightly yet, because I haven't found any links to download these Libtensorflow JNI packages versions. At the URL https://storage.googleapis.com/libtensorflow-nightly I only found libs for Linux and MacOs but none for Windows. Do you know where I can download TF 2.2 or nightly libtensorflow versions for Windows?\r\n\r\nThat's odd I don't know why you cannot see it in the browser window, but they are there. Might be a listing issue. `gsutil` might list them better.\r\nEither way, here are the ones from 09/17.\r\n[CPU](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/cpu_libtensorflow/nightly/891/20200917-043607/github/tensorflow/lib_package/libtensorflow-cpu-windows-x86_64.zip)\r\n[CPU JNI](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/cpu_libtensorflow/nightly/891/20200917-043607/github/tensorflow/lib_package/libtensorflow_jni-cpu-windows-x86_64.zip)\r\n[GPU](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/gpu_libtensorflow/nightly/944/20200917-042911/github/tensorflow/lib_package/libtensorflow-gpu-windows-x86_64.zip)\r\n[GPU JNI](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/gpu_libtensorflow/nightly/944/20200917-042911/github/tensorflow/lib_package/libtensorflow_jni-gpu-windows-x86_64.zip)\r\n", "> Either way, here are the ones from 09/17.\r\n> [CPU](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/cpu_libtensorflow/nightly/891/20200917-043607/github/tensorflow/lib_package/libtensorflow-cpu-windows-x86_64.zip)\r\n> [CPU JNI](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/cpu_libtensorflow/nightly/891/20200917-043607/github/tensorflow/lib_package/libtensorflow_jni-cpu-windows-x86_64.zip)\r\n> [GPU](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/gpu_libtensorflow/nightly/944/20200917-042911/github/tensorflow/lib_package/libtensorflow-gpu-windows-x86_64.zip)\r\n> [GPU JNI](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/gpu_libtensorflow/nightly/944/20200917-042911/github/tensorflow/lib_package/libtensorflow_jni-gpu-windows-x86_64.zip)\r\n\r\nThanks for the links. I have tested both the nightly CPU JNI and GPU JNI version. Unfortunately I still have the same \"Can't find dependent libraries\" problem. After starting `HelloTensorflow.java` with the VM parameters `-Djava.library.path=jni -Dorg.tensorflow.NativeLibrary.DEBUG=1` I still get the error message:\r\n\r\n```\r\norg.tensorflow.NativeLibrary: tryLoadLibraryFailed: C:\\seu\\workspace\\Tensorflow2JavaAPITest\\jni\\tensorflow_jni.dll: Can't find dependent libraries\r\norg.tensorflow.NativeLibrary: jniResourceName: org/tensorflow/native/windows-x86_64/tensorflow_jni.dll\r\norg.tensorflow.NativeLibrary: frameworkResourceName: org/tensorflow/native/windows-x86_64/tensorflow_framework.dll\r\nException in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.\r\n\tat org.tensorflow.NativeLibrary.load(NativeLibrary.java:79)\r\n\tat org.tensorflow.TensorFlow.init(TensorFlow.java:67)\r\n\tat org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:82)\r\n\tat org.tensorflow.Graph.<clinit>(Graph.java:479)\r\n\tat com.HelloTensorFlow.main(HelloTensorFlow.java:12)\r\n```\r\n\r\nIf I interpret this error message correctly tensorflow_jni.dll is found but apparently there is another dependency which is missing. Do you have any ideas?\r\n\r\nThx for your help!\r\n\r\n\r\n\r\n", "@av8ramit \r\nIgnore my last comment. Finally I solved my problem. After I installed Microsoft Visual C++ Redistributable **2019** the problem disappeared. Before that I had only installed the 2015 version.\r\n\r\nThis means that the TF 2.x JNI libs just work perfectly.However, the documentation on https://www.tensorflow.org/install/lang_java?hl=en#download is not correct/up to date:\r\n> Note: On Windows, the native library (tensorflow_jni.dll) requires msvcp140.dll at runtime. See the Windows build from source guide to install the Visual C++ 2015 Redistributable.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42891\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42891\">No</a>\n", "@FreddyLab thank you for bringing that to our attention. I'll submit a fix for the documentation. Glad your issue is resolved now!"]}, {"number": 42890, "title": "load_model not properly working in TF 2.3.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom Code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- TensorFlow installed from (source or binary): Colab TF 2.3.0 vs TF 2.2.0\r\n- TensorFlow version (use command below): Colab TF 2.3.0 vs TF 2.2.0\r\n- Python version: Colab Python\r\n- GPU model and memory: Colab GPU\r\n\r\nWhen I used TF 2.2.0 in colab I was able to fit, save and load a model. So I run my code, fit my model:\r\n![grafik](https://user-images.githubusercontent.com/68148852/91966779-9bf5e880-ed12-11ea-8ec3-7e499a5024eb.png)\r\n\r\n and check again with model.evaluate:\r\n![grafik](https://user-images.githubusercontent.com/68148852/91966830-ac0dc800-ed12-11ea-9ab8-68c2b84cb218.png)\r\n\r\nEverything as expected. Then I save the model and load it back again and I check it again with model.evaluate and same accuracy is shown. Everything fine. However, the same code does not work in TF 2.3.0. See my code below. If this code is run by default TF 2.3.0 is used in colab and it does not work. The accuracy of the loaded model loadedmodel.evaluate(x_test_r,test_labels) is not the same:\r\n\r\n![grafik](https://user-images.githubusercontent.com/68148852/91966880-c051c500-ed12-11ea-9b72-3c57fffc5646.png)\r\n\r\nIt is completely wrong (approx .098... vs supposed 0.99... when I run it). When I add !pip install tensorflow==2.2.0 to my code and run the same code in my colab again (note that the runtime has to be restarted) then with the 2.2.0 version it does work.\r\n\r\n(When I save the model in TF 2.2.0 (where evaluate works and correct accuracy is shown) and then load it in TF 2.3.0, it does not work. model.evaluate shows a wrong accuracy.)\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n#!pip install tensorflow==2.2.0\r\nimport tensorflow as tf\r\nimport os\r\nimport PIL\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow_hub as hub\r\n\r\nfrom tensorflow.keras.models import Sequential\r\n\r\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\nimport keras_preprocessing\r\nfrom keras_preprocessing import image\r\n\r\nfrom tensorflow.python.keras.utils.version_utils import training\r\nfrom tensorflow.keras.optimizers import RMSprop\r\n\r\nprint(tf.__version__)\r\n\r\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\r\n\r\nx_trainf = train_images.astype('float32') / 255.0\r\nx_testf = test_images.astype('float32') / 255.0\r\n\r\nx_train_r = x_trainf.reshape(x_trainf.shape[0], 28, 28, 1)\r\nx_test_r = x_testf.reshape(x_testf.shape[0], 28, 28, 1)\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', input_shape=(28,28,1))) \r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=2))\r\nmodel.add(tf.keras.layers.Dropout(0.2))\r\nmodel.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=2))\r\nmodel.add(tf.keras.layers.Dropout(0.2))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\r\nmodel.add(tf.keras.layers.Dropout(0.2))\r\nmodel.add(tf.keras.layers.Dense(10))\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train_r, \r\n          train_labels, batch_size=32,  \r\n          epochs=10,\r\n          validation_data=(x_test_r,test_labels))\r\n\r\nmodel.evaluate(x_test_r,test_labels)\r\n\r\nmodel.save('/tmp/loaddatamnist.h5')\r\n\r\nloadedmodel=tf.keras.models.load_model('/tmp/loaddatamnist.h5')\r\nloadedmodel.evaluate(x_test_r,test_labels)\r\n```\r\n\r\n\r\n", "comments": ["@HaraBeron \r\n\r\nI have tried in colab with TF nightly version(2.4.0-dev20200902) and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/4cb33851aa5e54ab42b787f9734773f2/untitled297.ipynb).Please, verify once and close the issue. Thanks!", "Thanks for your fast response and having a look into this issue.\r\n\r\nI was not talking about TF nightly version 2.4.0-... I was talking about TF 2.2.0 and TF 2.3.0 and there the issue exists and this is what most users are using. They do not use nightly version.\r\n\r\nWhen you run your colab without the first line, so without !pip install tf-nightly. Then TF 2.3.0 is used and then the same issue occurs. I run your code without this line and the issue as described occurs.\r\n\r\nIn detail:\r\nWhen I run your code (with just 2 epochs) I get an val_accuracy of 0.9881 when I use model.evaluate it gives the same 0.988099. Then I save and load the model and run model.evaluate again. This should give the same value, however it doesn't. It gives a wrong value of 0.099.\r\n\r\nSo the issue does exist.", "Here is a screenshot of your colab when I run it (not with TF 2.4. nightly, but without any modifications, so usual import tf and then by default TF 2.3.0 is used):\r\n\r\n![grafik](https://user-images.githubusercontent.com/68148852/91978231-13804380-ed24-11ea-8718-c5863a101cec.png)\r\n\r\nThere you can see the issue (here in this case it was not 0.099, but 0.1, still a wrong value). It is without the tf nightly. It is TF 2.3.0.\r\n\r\nWhen you add !pip install tensorflow==2.2.0 instead of the tf nightly and then run it (you maybe have to restart the runtime), you can see, that it does work. Here is a screenshot:\r\n\r\n![grafik](https://user-images.githubusercontent.com/68148852/91978716-e54f3380-ed24-11ea-96e9-97ba9d3e5519.png)\r\n\r\nSo again: It works in TF 2.2.0, but not in TF 2.3.0.\r\n\r\n\r\n", "@HaraBeron \r\n\r\nI tried in TF 2.3 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/ea2d084ee5d54411a0dbb545b13ca7e6/untitled295.ipynb).", "I don't know if the compiled status it is recovered. Please try to recompile after reloading:\r\n\r\n```\r\nloadedmodel=tf.keras.models.load_model('/tmp/test.h5')\r\nloadedmodel.compile(optimizer='adam',\r\n              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\nloadedmodel.evaluate(x_test_r,test_labels)\r\n```", "@HaraBeron This is a know bug with `accuracy`. In earlier TF versions `accuracy` was correctly inferred from the `loss` function. In this case, `accuracy` need to be inferred as `sparse_categorical_accuracy` but in the current version it is not working as expected.\r\n\r\nEverything works as expected If you replace `accuracy` with `sparse_categorical_accuracy` in `model.compile` as shown below.\r\n\r\n```\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['sparse_categorical_accuracy'])\r\n```\r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/933388973689f871e75248e022e33800/untitled295.ipynb). \r\n\r\nWe will track the progress of this bug in [this issue](https://github.com/tensorflow/tensorflow/issues/42045).\r\n\r\nPlease verify once and close the issue. We will track the progress of this bug in the above mentioned issue. \r\n\r\nI will also comment when everything is working as intended. Thanks!\r\n", "@HaraBeron This is not completely fixed in the recent `tf-nightly`. Recompiling and loading as mentioned by @bhack works as expected. Please take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/f4e29da076c58ab8d0504142ee8b211a/untitled295.ipynb). Thanks!\r\n", "This is resolved in recent tf-nightly. Please feel free to reopen if you notice the issue.\r\nThis is available in stable TF2.4 in the near future. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42890\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42890\">No</a>\n"]}, {"number": 42889, "title": "nan gradient issue", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.8 not using gpu\r\n\r\n**Describe the current behavior**\r\ntraining on an easy example, tf sometimes got `nan` for gradient\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\n# on cpu nan at 1098 iter\r\n# Iter    0: loss 9.99180138e-01,  runtime:     0.11\r\n# Iter    1: loss 9.98347044e-01,  runtime:     0.11\r\n# Iter    2: loss 9.97434497e-01,  runtime:     0.12\r\n# on gpu nan at 2212 iter\r\n# Iter    0: loss 9.99180079e-01,  runtime:     2.46\r\n# Iter    1: loss 9.98346925e-01,  runtime:     2.47\r\n# Iter    2: loss 9.97434497e-01,  runtime:     2.48\r\n\r\nnp.random.seed(1)\r\ntf.keras.backend.set_floatx('float32')\r\n\r\nfunc = lambda x: x\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-1)\r\n\r\ndata = np.array(\r\n    [[0., 0.07179281], [0., 0.44064897], [0., 0.7666122], [0., -0.655319],\r\n     [0., -0.28546047], [0., 0.8460491], [0., 0.14823522], [0., -0.14381762],\r\n     [0., 0.7200559], [0., -0.92189044], [0., 0.37300184], [0., -0.525946],\r\n     [0., 0.07766213], [0., 0.370439], [0., 0.17311008], [0., 0.88918954],\r\n     [0., -0.5910955], [0., -0.947578], [0., -0.7192261], [0., 0.5109261],\r\n     [0., 0.85887444], [0., -0.75145805], [0., 0.89897853], [0., 0.23428982],\r\n     [0., 0.5785587], [0., 0.0541162], [0., 0.97772217], [0., 0.24339144],\r\n     [0., -0.72505057], [0., -0.39533487], [0., 0.6692513], [0., -0.7257285],\r\n     [0., 0.93652314], [0., 0.17861107], [0., 0.38464522], [0., 0.38880032],\r\n     [0., -0.73994285], [0., -0.9602397], [0., 0.07763347], [0., 0.6147826],\r\n     [0., 0.68406177], [0., 0.39951673], [0., -0.17188802], [0., -0.10017573],\r\n     [0., 0.7917724], [0., 0.35767105], [0., 0.7892133], [0., -0.62747955],\r\n     [0., 0.7562349], [0., -0.16161098], [0., -0.77050805], [0., 0.8068038],\r\n     [0., -0.37315163], [0., -0.3467102], [0., -0.70654285], [0., -0.8679997],\r\n     [0., 0.5002886], [0., -0.7214473], [0., 0.7718842], [0., -0.5767438],\r\n     [0., 0.8550172], [0., 0.4230495], [0., -0.7064882], [0., 0.11737966],\r\n     [0., 0.326883], [0., -0.439112], [0., -0.99425936], [0., -0.94338703],\r\n     [0., -0.8153228], [0., 0.8651909], [0., -0.96342343], [0., 0.9296801],\r\n     [0., -0.50757784], [0., 0.24734442], [0., 0.80675906], [0., 0.38375422],\r\n     [0., -0.7953311], [0., -0.4127717], [0., 0.39363632], [0., -0.30887854],\r\n     [0., -0.8299116], [0., -0.603797], [0., -0.9452248], [0., -0.80330634],\r\n     [0., 0.34093502], [0., -0.793548], [0., 0.6014891], [0., 0.7527783],\r\n     [0., 0.38179383], [0., -0.9000931], [0., 0.4963313], [0., 0.45199597],\r\n     [0., -0.9612661], [0., -0.30446827], [0., 0.9946457], [0., 0.14735897],\r\n     [0., 0.24672022], [0., -0.20646505], [0., -0.20464632], [0., -0.1837264],\r\n     [0., 0.8170703], [0., -0.15778475], [0., 0.5018849], [0., -0.8932749],\r\n     [0., 0.10564396], [0., 0.91577905], [0., -0.01685368], [0., -0.42444932],\r\n     [0., -0.30220333], [0., -0.46014422], [0., -0.99977124], [0., 0.06633057],\r\n     [0., 0.15677923], [0., -0.46890667], [0., -0.36896873], [0., -0.6692916],\r\n     [0., -0.17164145], [0., 0.756285], [0., -0.16595599], [0., 0.817191],\r\n     [0., 0.5016242], [0., 0.3275893], [0., 0.50775236], [0., 0.02977822],\r\n     [0., -0.10421295], [0., -0.9683575], [0., -0.6603392], [0., -0.1653904]],\r\n    dtype=np.float32)\r\ndata_x = data[:, 0:1]\r\ndata_y = data[:, 1:2]\r\n\r\n\r\ndef loss_func(x, y):\r\n    return tf.reduce_mean(tf.norm(func(x) - y, axis=1) / tf.norm(y, axis=1))\r\n\r\n\r\nclass MyNN(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.input_dims = [1, 1]\r\n        self.func = func\r\n        self.optimizer = optimizer\r\n\r\n        self.net1 = tf.keras.layers.Dense(\r\n            **{\"units\": 4, \"activation\": 'relu',\r\n               \"kernel_initializer\": {\r\n                   'class_name': 'glorot_uniform',\r\n                   'config': {'seed': 1}}}\r\n        )\r\n        self.net2 = tf.keras.layers.Dense(\r\n            **{\"units\": 1, \"activation\": None,\r\n               \"kernel_initializer\": {\r\n                   'class_name': 'glorot_uniform',\r\n                   'config': {'seed': 1}}}\r\n        )\r\n\r\n    def train_one_step(self, x, y):\r\n        with tf.GradientTape() as tape:\r\n            x_pred = self(x, y)\r\n            loss = loss_func(x_pred, y)\r\n        grads = tape.gradient(loss, self.trainable_variables)\r\n\r\n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\r\n        return loss\r\n\r\n    def train(self, start_time=time.time(), max_iter=3000):\r\n        for it in range(max_iter):\r\n            loss = self.train_one_step(data_x, data_y)\r\n\r\n            print(\"Iter %4d: loss %14.8e,  runtime: %8.2f\"\r\n                  % (it, loss.numpy(), time.time() - start_time))\r\n\r\n    def call(self, x, y):\r\n        r = y - self.func(x)\r\n        g = self.net2(self.net1(r)) * 2e-3\r\n        return x + g\r\n\r\n\r\nmodel = MyNN()\r\nmodel.train()\r\n\r\n```\r\n", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/4e1aeea723e430f025641758472e84aa/42889.ipynb). Thanks!", "@j7168908jx Can you try with this formulation:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\nnp.random.seed(1)\r\ntf.keras.backend.set_floatx('float32')\r\n\r\nfunc = lambda x: x\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-1)\r\n\r\ndata = np.array(\r\n    [[0., 0.07179281], [0., 0.44064897], [0., 0.7666122], [0., -0.655319],\r\n     [0., -0.28546047], [0., 0.8460491], [0., 0.14823522], [0., -0.14381762],\r\n     [0., 0.7200559], [0., -0.92189044], [0., 0.37300184], [0., -0.525946],\r\n     [0., 0.07766213], [0., 0.370439], [0., 0.17311008], [0., 0.88918954],\r\n     [0., -0.5910955], [0., -0.947578], [0., -0.7192261], [0., 0.5109261],\r\n     [0., 0.85887444], [0., -0.75145805], [0., 0.89897853], [0., 0.23428982],\r\n     [0., 0.5785587], [0., 0.0541162], [0., 0.97772217], [0., 0.24339144],\r\n     [0., -0.72505057], [0., -0.39533487], [0., 0.6692513], [0., -0.7257285],\r\n     [0., 0.93652314], [0., 0.17861107], [0., 0.38464522], [0., 0.38880032],\r\n     [0., -0.73994285], [0., -0.9602397], [0., 0.07763347], [0., 0.6147826],\r\n     [0., 0.68406177], [0., 0.39951673], [0., -0.17188802], [0., -0.10017573],\r\n     [0., 0.7917724], [0., 0.35767105], [0., 0.7892133], [0., -0.62747955],\r\n     [0., 0.7562349], [0., -0.16161098], [0., -0.77050805], [0., 0.8068038],\r\n     [0., -0.37315163], [0., -0.3467102], [0., -0.70654285], [0., -0.8679997],\r\n     [0., 0.5002886], [0., -0.7214473], [0., 0.7718842], [0., -0.5767438],\r\n     [0., 0.8550172], [0., 0.4230495], [0., -0.7064882], [0., 0.11737966],\r\n     [0., 0.326883], [0., -0.439112], [0., -0.99425936], [0., -0.94338703],\r\n     [0., -0.8153228], [0., 0.8651909], [0., -0.96342343], [0., 0.9296801],\r\n     [0., -0.50757784], [0., 0.24734442], [0., 0.80675906], [0., 0.38375422],\r\n     [0., -0.7953311], [0., -0.4127717], [0., 0.39363632], [0., -0.30887854],\r\n     [0., -0.8299116], [0., -0.603797], [0., -0.9452248], [0., -0.80330634],\r\n     [0., 0.34093502], [0., -0.793548], [0., 0.6014891], [0., 0.7527783],\r\n     [0., 0.38179383], [0., -0.9000931], [0., 0.4963313], [0., 0.45199597],\r\n     [0., -0.9612661], [0., -0.30446827], [0., 0.9946457], [0., 0.14735897],\r\n     [0., 0.24672022], [0., -0.20646505], [0., -0.20464632], [0., -0.1837264],\r\n     [0., 0.8170703], [0., -0.15778475], [0., 0.5018849], [0., -0.8932749],\r\n     [0., 0.10564396], [0., 0.91577905], [0., -0.01685368], [0., -0.42444932],\r\n     [0., -0.30220333], [0., -0.46014422], [0., -0.99977124], [0., 0.06633057],\r\n     [0., 0.15677923], [0., -0.46890667], [0., -0.36896873], [0., -0.6692916],\r\n     [0., -0.17164145], [0., 0.756285], [0., -0.16595599], [0., 0.817191],\r\n     [0., 0.5016242], [0., 0.3275893], [0., 0.50775236], [0., 0.02977822],\r\n     [0., -0.10421295], [0., -0.9683575], [0., -0.6603392], [0., -0.1653904]],\r\n    dtype=np.float32)\r\ndata_x = data[:, 0:1]\r\ndata_y = data[:, 1:2]\r\n\r\ndef loss(model, x, y, training):\r\n  # training=training is needed only if there are layers with different\r\n  # behavior during training versus inference (e.g. Dropout).\r\n  y_ = model(x,y, training=training)\r\n\r\n  return loss_func(y, y_)\r\n\r\n\r\ndef loss_func(x, y):\r\n    return tf.reduce_mean(tf.norm(func(x) - y, axis=1) / tf.norm(y, axis=1))\r\n\r\n\r\nclass MyNN(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.input_dims = [1, 1]\r\n        self.func = func\r\n        self.optimizer = optimizer\r\n\r\n        self.net1 = tf.keras.layers.Dense(\r\n            **{\"units\": 4, \"activation\": 'relu',\r\n               \"kernel_initializer\": {\r\n                   'class_name': 'glorot_uniform',\r\n                   'config': {'seed': 1}}}\r\n        )\r\n        self.net2 = tf.keras.layers.Dense(\r\n            **{\"units\": 1, \"activation\": None,\r\n               \"kernel_initializer\": {\r\n                   'class_name': 'glorot_uniform',\r\n                   'config': {'seed': 1}}}\r\n        )\r\n    def grad(self,inputs, targets):\r\n      with tf.GradientTape() as tape:\r\n        loss_value = loss(self, inputs, targets, training=True)\r\n      return loss_value, tape.gradient(loss_value, model.trainable_variables)\r\n\r\n    def train_one_step(self, x, y):\r\n      loss_value, grads = self.grad(x, y)\r\n      optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n      return loss_value\r\n\r\n\r\n    def train(self, start_time=time.time(), max_iter=3000):\r\n        for it in range(max_iter):\r\n            loss = self.train_one_step(data_x, data_y)\r\n            print(\"Iter %4d: loss %14.8e,  runtime: %8.2f\" % (it, loss.numpy(), time.time() - start_time))\r\n\r\n    def call(self, x, y):\r\n        r = y - self.func(x)\r\n        g = self.net2(self.net1(r)) * 2e-3\r\n        return x + g\r\n\r\n\r\nmodel = MyNN()\r\nmodel.train()\r\n```", "@bhack I tried this formulation, and found that the loss value is different from what i had. Also, the nan issue still remains\r\n\r\nIter    0: loss 4.19011816e+03,  runtime:     0.18\r\nIter    1: loss 8.31127930e+02,  runtime:     0.19\r\nIter    2: loss 5.12066895e+02,  runtime:     0.20\r\nIter    3: loss 3.87902283e+02,  runtime:     0.21\r\n\r\n\r\nIter 7848: loss 5.08754015e-01,  runtime:   107.28\r\nIter 7849: loss 5.08563876e-01,  runtime:   107.30\r\nIter 7850: loss            nan,  runtime:   107.31\r\nIter 7851: loss            nan,  runtime:   107.32", "@j7168908jx Other then small cosmetic refactoring as you can see I've just inverted predict and true order in `loss_func(y, y_)` just to check if there was a problem between the nominator and denominator in your loss.\r\nI've runnned that code code on colab with your original example `max_iter=3000` but now seems that you have expanded the number of iteration.\r\nI think that you can double check some behavior of the gradient in your loss as you are using `tf.norm` a https://github.com/tensorflow/tensorflow/issues/12071", "/cc @rmlarsen", "@bhack, the denominator in the loss is the norm of true $y$ values, which are all away from zero. Hence, the calculation of the denominator should be just a variable of batch sampling and is independent of any trainable variables. Unless something wired is happening in the back propagation, otherwise, the denominator should not be any place near zero. ", "Can you check the impact of the numerator `norm`?", "After removing the denominator, the nan appears even earlier. See [gist](https://colab.research.google.com/gist/YingzhouLi/46d48caea89f1aeaf2fd1a04a214a3ef/untitled0.ipynb).", "My previous comment was about numeratore.", "I have implement norm by tf functions and also remove the `sqrt` from the implementation. The `nan` issue is indeed caused by the `tf.sqrt` function. Likely, the function is nearly zero on a batch. (the last point is not verified) See [gist](https://colab.research.google.com/gist/YingzhouLi/b5e65dff5e703643fad867f5717943ed/untitled0.ipynb).\r\n\r\n|loss function | issue |\r\n|---------|--------------------------------------------------------------------------------------------------------------|\r\n| norm(.) | nan at 618 iter |\r\n| square(norm(.)) | no nan |\r\n| sqrt(sum(square(.))) | nan at 618 iter |\r\n|sum(square(.)) | no nan|", "Was able to reproduce your issue in Tensorflow 2.5 , please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/d0c8bfa484082c5a835db00d91176e29/35650.ipynb). Thanks!", "Hi @j7168908jx ! I was able to resolve this issue after  squaring the denominator in tf.reduce.mean function as suggested in this [comment ](https://github.com/tensorflow/tensorflow/issues/42889#issuecomment-688592166). Attaching [gist](https://colab.sandbox.google.com/gist/mohantym/46a9a49176c6008c257b5042e963aab6/42889.ipynb#scrollTo=zJaZgT-2ciok) for reference. Can we move this issue to closed status now?\r\n\r\n```\r\ndef loss_func(x, y):\r\n    return tf.reduce_mean(tf.norm(func(x) - y, axis=1) / tf.math.square(tf.norm(y, axis=1)))\r\n```\r\n", "Thanks for all comments above! I'd like to close it since squaring does resolve this problem for now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42889\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42889\">No</a>\n"]}, {"number": 42888, "title": "scaling issue by using multiworkerstrategy for CPU", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NA\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): conda \r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI have written code that need to scale to multiple host having 20 cpu cores each. I have used multiworkerstrategy. We are basically focusing on distributed training on CPU \"NOT ON GPU\" in tensorflow 2.3. I am able to successfully run the code on the cluster in all the node. \r\n\r\nNow the problem -\r\n\r\nI see scaling issue. I have run it on single host and 5 different host. I doesn't see any performance gain on increasing the number of host. The time it take for each epochs and whole training is same in 1 machine vs 2 , 4 or 5 machine. There is no performance gain.\r\n\r\nThan I have left my code and used the example in tensorflow docs with the same setup.\r\nhttps://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras  \r\nI see the same behavior, not scaling on the increasing host.\r\n\r\nCould someone please help me that \r\n1. Is multiworkerstrategy doesnot support scaling for distributed CPU in the different hosts?\r\n\r\n**Describe the expected behavior**\r\n\r\nExpected behaviour is that by using multiworkerstrategy it will scale by increasing the no of hosts.\r\n[workers.log](https://github.com/tensorflow/tensorflow/files/5161305/workers.log)\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nAttaching the logs of the worker. Each worker having the same logs as explained above time consumed for using 5 host or 1 host is same. So attaching 1 logs. \r\n", "comments": ["Hi @rajeev921, yes `MultiWorkerMirroredStrategy` can be used to scale training jobs across CPU only machines. If I understand correctly, when you trained the model from the Multi-worker training with Keras tutorial on one machine it took `14s/step` and then when you added more machines, it was still `14s/step` ? To help debug this performance issue, can you provide the exact code you used to train? I know you linked to the tutorial but would be great if you could provide the exact code you used in a colab, just to make I'm using the same batch size, steps_per_epoch, etc. I can run some tests on my end and want to make sure I'm running the exact same code you are.\r\n\r\nCan you also provide the full logs? The logs you've provided only show the results of the training steps, but it would be useful to see what was output prior to the training epochs. ", "\r\n[workers.log](https://github.com/tensorflow/tensorflow/files/5165737/workers.log)\r\nHi @nikitamaia. \r\n\r\nAttaching a full log. As I mentioned logs are the same across all the 5 hosts( Epoch time, per step).\r\n\r\nimport os\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport json\r\n\r\ndef mnist_dataset(batch_size):\r\n  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\r\n  x_train = x_train / np.float32(255)\r\n  y_train = y_train.astype(np.int64)\r\n  train_dataset = tf.data.Dataset.from_tensor_slices(\r\n      (x_train, y_train)).shuffle(60000).repeat().batch(batch_size)\r\n  return train_dataset\r\n\r\ndef build_and_compile_cnn_model():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.Input(shape=(28, 28)),\r\n      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu'),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(128, activation='relu'),\r\n      tf.keras.layers.Dense(256, activation='relu'),\r\n      tf.keras.layers.Dense(512, activation='relu'),\r\n      tf.keras.layers.Dense(1024, activation='relu'),\r\n      tf.keras.layers.Dense(2048, activation='relu'),\r\n      tf.keras.layers.Dense(128, activation='relu'),\r\n      tf.keras.layers.Dense(10)\r\n  ])\r\n  model.compile(\r\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n      metrics=['accuracy'])\r\n  return model\r\n\r\nwith open('./tf_config', 'r') as fp:\r\n    tf_config = json.load(fp)\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\nnum_workers = 5\r\nper_worker_batch_size = 64\r\n\r\nglobal_batch_size = per_worker_batch_size * num_workers\r\nmulti_worker_dataset = mnist_dataset(global_batch_size)\r\n\r\nwith strategy.scope():\r\n  multi_worker_model = build_and_compile_cnn_model()\r\n\r\nmulti_worker_model.fit(multi_worker_dataset, epochs=150, steps_per_epoch=70)\r\n\r\n'''\r\nJson file content for tfconfig \r\n{\r\n    \"cluster\": {\r\n        \"worker\": [\r\n            \"host1:12000\",\r\n            \"host2:12001\",\r\n            \"host3:12002\",\r\n            \"host4:12003\",\r\n            \"host5:12004\"\r\n        ]\r\n    },\r\n    \"task\": {\r\n        \"type\": \"worker\",\r\n        \"index\": 0\r\n    }\r\n\r\nI have just changed index value to 0, 1, 2, 3, 4 in all the different host, rest is same.\r\n'''", "@nikitamaia \r\n\r\nColab link - https://colab.research.google.com/drive/1cttMaBOJ7ZrveJH2Inek7jvIq0FLwye6#scrollTo=4h4bTT_FvuFH\r\n\r\n[workers.log](https://github.com/tensorflow/tensorflow/files/5168227/workers.log)\r\n\r\nI have attached the log of the mnist.\r\n\r\nPlease ignore the above comment logs. As that log is our private repo code. I cannot share private repo code. \r\nBut from the logs have the same behaviour as mnist. No scaling.\r\n", "I can't access the Colab, can you share it with me?", "@nikitamaia. \r\nPlease check now.\r\n \r\nhttps://colab.research.google.com/drive/1cttMaBOJ7ZrveJH2Inek7jvIq0FLwye6?usp=sharing", "I suspect the issue here is that you're scaling up the batch size by the number of workers, but you're always keeping your `steps_per_epoch` the same. In tf-nightly, you are not required to pass in `steps_per_epoch` to `model.fit`. Can you try running your code in tf-nightly? You'll want to remove the `.repeat()` operation from your dataset, and also remove the `steps_per_epoch` from `model.fit`. You should keep the `global_batch_size = batch_size * mirrored_strategy.num_replicas_in_sync`\r\n\r\nWhen you do this, you should see that the number of steps per epoch will decrease as you add more machines. Ideally this will mean that each epoch will take less time, however, for a small model and dataset like MNIST you might not see much benefit because the overhead of synchronizing the gradients from the two machines might not be worth it since MNIST trains pretty quickly on a single machine anyway (especially if it already has 20 cores). \r\n\r\nI'm assuming the model and dataset from your private repo code is larger than MNSIT, so try running that code in tf-nightly as well without passing in `steps_per_epoch` and let me know what you see. I would suggest starting by scaling to two machines first and making sure there's a performance gain before adding another machine.\r\n", "Hi @nikitamaia,\r\n\r\nAs you suggested I tried to run it on tf_nightly build.\r\nI have tried removing  steps_per_epoch,  .repeat(), and keeping global_batch_size = batch_size * mirrored_strategy.num_replicas_in_sync.\r\n\r\nI see the difference in behaviour in 1st Epoch it is running of 1/unknown number of steps per epoch. after that, all other epochs have 4 steps per epoch. And it is same for using 1 host to scaling up to 1 to 5 different hosts and also the runtime is the same.\r\n\r\n\r\nFor detail Please see the logs. I am attaching logs of 3 host only.\r\n[workers_3.log](https://github.com/tensorflow/tensorflow/files/5178796/workers_3.log)\r\n\r\n[workers_1.log](https://github.com/tensorflow/tensorflow/files/5178785/workers_1.log)\r\n[workers_2.log](https://github.com/tensorflow/tensorflow/files/5178786/workers_2.log)\r\n\r\n", "@nikitamaia \r\n\r\nDo you have any working examples that you have tested on nigtly_build that is scaling for multi cpus for multi hosts? \r\n\r\nIf you have please share, So I can try and see what is wrong in my situation.\r\n", "Hi @nikitamaia \r\n\r\nI thought to test the same setting on a bigger dataset. So did took a bigger dataset. apply local_batch_size of 512 then used\r\nlobal_batch_size = local_batch_size * mirrored_strategy.num_replicas_in_sync\r\n\r\nI removed .repeat and steps per epoch. and ran with a single host and 5 different hosts.\r\n \r\nOutcome - \r\n1. With 1 host I see steps per epochs as 45 and Epochs time 470 avg and 10s/step. Please check the log of worker_single_host.log\r\n\r\n2. I have scaled it to 2 and 5 different hosts and  I see multiple problems here.\r\n    a.  I see the same 45 steps_per_epochs.\r\n    b.  Time for Epochs and per step increased drastically and it is different in the different hosts ( for 5 host in some 683 15s/step and in some 745 17 sec/step) and (2-host 563 13s/step).\r\n    c. I see the same time different hosts executing different steps and different epochs. So I have captured the initial logs.\r\n    please wee worker 1 - 5 logs for 5 hosts .\r\n\r\nPlease help, now I am going out of my mind, without having any clue.\r\n\r\n[workers_single_host.log](https://github.com/tensorflow/tensorflow/files/5191144/workers_single_host.log)\r\n\r\n[workers_1.log](https://github.com/tensorflow/tensorflow/files/5191139/workers_1.log)\r\n[workers_2.log](https://github.com/tensorflow/tensorflow/files/5191140/workers_2.log)\r\n[workers_3.log](https://github.com/tensorflow/tensorflow/files/5191141/workers_3.log)\r\n[workers_4.log](https://github.com/tensorflow/tensorflow/files/5191142/workers_4.log)\r\n[workers_5.log](https://github.com/tensorflow/tensorflow/files/5191143/workers_5.log)\r\n\r\n\r\n", "This is difficult to debug since I can't see the actual code you're running, so let's try something simple as a sanity check.\r\nPlease see this [gist](https://colab.research.google.com/drive/1uimh_CcczehHmRkONHohPnmeAUprHoZ2?usp=sharing) here for running multi worker training with MNIST\r\n\r\nRun this code in nightly, first with one machine. Then with two. The only thing in the code you will need to change is the TF Config.\r\n\r\nWhen I run that code with one machine, I see 469 steps per epoch\r\nWhen I switch to two machines, I see 235 steps per epoch on each machine.\r\n\r\nPlease try this in tf-nightly and let me know what you see.", "I have run the gist and it is scaling as you have suggested, I am getting the same result.\r\n\"\"\"\r\nWhen I run that code with one machine, I see 469 steps per epoch\r\nWhen I switch to two machines, I see 235 steps per epoch on each machine.\r\n\"\"\"\r\nonly this error I get before execting epochs on both the host.\r\n2020-09-09 05:03:22.600791: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:591] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\r\nop: \"TensorSliceDataset\"\r\ninput: \"Placeholder/_0\"\r\ninput: \"Placeholder/_1\"\r\nattr {\r\n  key: \"Toutput_types\"\r\n  value {\r\n    list {\r\n      type: DT_FLOAT\r\n      type: DT_INT64\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: 28\r\n        }\r\n        dim {\r\n          size: 28\r\n        }\r\n      }\r\n      shape {\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n====================\r\nNow experiment on my code. \r\nI have custom layer defined like defined in the code.log.\r\n[code.log](https://github.com/tensorflow/tensorflow/files/5195156/code.log)\r\n\r\nThis code is working fine till tf2.3 but raising an error on tf_nightly build\r\n\r\nValueError: The name \"conv2d\" is used 3 times in the model. All layer names should be unique.\r\n\r\nSo, I have replaced my custom keras layer with \r\n\r\n```\r\ndef get_uncompiled_model():\r\n    temp_layer = img_inputs\r\n    for kernel_shape in process.layer_cfg.genKernShape():\r\n\t   temp_layer = tf.keras.layers.Conv2D(\r\n\t\tfilters=kernel_shape[-1],\r\n\t\tkernel_size=kernel_shape[0:2],\r\n\t\tpadding='valid',\r\n\t\tkernel_initializer='random_normal',\r\n\t\tbias_initializer='zeros',\r\n\t\tactivation='sigmoid'\r\n\t    )(temp_layer)\r\n\r\n    deform = Model(inputs=img_inputs, outputs=temp_layer)\r\n    return deform\r\n\r\n```\r\n\r\n \r\nNow I am able to run the code. \r\nfor  1 host with a smaller  dataset (4 steps per epochs) and a bigger dataset (45 steps_per_epochs), it runs successfully. \r\n\r\nFor 2 hosts with a smaller dataset(2 steps_per_epochs) and a bigger dataset(22 steps_per_epochs), it runs and then crashes with this error in logs - host_1.log and host_2.log.\r\n\r\n[host_1.log](https://github.com/tensorflow/tensorflow/files/5195228/host_1.log)\r\n[host_2.log](https://github.com/tensorflow/tensorflow/files/5195239/host_2.log)\r\n\r\n\r\nCode for reading the dataset \r\n`\r\n```\r\ndef get_train_valid_data(imgParser ):  \r\n    batch_size = 512\r\n    BATCH_SIZE = batch_size * mirrored_strategy.num_replicas_in_sync\r\n    \r\n    # Build tf.data.dataset for train\r\n    train_image_paths = glob.glob(f\"{args['train_data']}/*.tfrecords\")\r\n    training_data_ds = tf.data.TFRecordDataset(train_image_paths)\r\n    train_parsed_ds = training_data_ds.map(imgParser.parse_image_field)\r\n    train_input_ds = train_parsed_ds.map(imgParser.generate_target_field)\r\n\r\n    train_ds = train_input_ds \\\r\n        .batch(BATCH_SIZE)  \\\r\n        .prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\n     # Build dataset for validation\r\n    valid_image_paths = glob.glob(f\"{args['valid_data']}/*.tfrecords\")\r\n    valid_data_ds = tf.data.TFRecordDataset(valid_image_paths)\r\n    valid_parsed_ds = valid_data_ds.map(imgParser.parse_image_field)\r\n    valid_input_ds = valid_parsed_ds.map(imgParser.generate_target_field)\r\n\r\n    valid_ds = valid_input_ds\\\r\n            .batch(batch_size)\r\n\r\n    return train_ds, valid_ds\r\n```\r\n`\r\n \r\n", "The `Error: Found an unshardable source dataset: name: \"TensorSliceDataset/_2` can be ignored. It's slightly misleading but it's just a warning and not an error that will affect your code.\r\n\r\nFrom the logs from running our custom code, it looks like it's crashing on the validation step, as it appears to make it through the training step before the error message. You can try removing the validation step for now since the goal is first to make sure that the training code scales as you add more machines.", "I also thought the same, I have removed validation data but still, it's failing the same way. no changes.\r\n\r\nI am sure this issue is related to the below link.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/25254\r\nhttps://github.com/tensorflow/tensorflow/issues/33531\r\n\r\nAs the workaround was given that use .repeat() but if I use .repeat(), I have to specify steps_per_epochs(). So I believe this is still an issue that is not fixed.\r\n\r\nThe difference is I am using a model. fit() rather than a custom loop.\r\n\r\nFor clarification about the other error, Do I need to open a new issue for conv2d error while using custom class in tf_nightly?", "@nikitamaia \r\n\r\nAny update about how to solve the crashing issue ?", "Have you tried setting the `tf.data.experimental.AutoShardPolicy` to `DATA` ?\r\n\r\n[From the docs](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_dataset), the policy is by default `AUTO`:\r\n`AUTO: This is the default option which means an attempt will be made to shard by FILE. The attempt to shard by FILE fails if a file-based dataset is not detected. tf.distribute will then fall back to sharding by DATA. Note that if the input dataset is file-based but the number of files is less than the number of workers, an error will be raised.`\r\nIf you have more machines than number of files, then this might be causing the problem.\r\n\r\nFor the conv2d problem, yes I think opening a new issue is probably best.", "I have tried tf.data.experimental.AutoShardPolicy to DATA  and FILE both but still it is failing with the same error. \r\n\r\nI am now using a bigger dataset running for 1 host with 45 steps_per_epochs. \r\nBut fail for 2 hosts with 23 steps_per_epochs.\r\n\r\nBelow is the code I have used. I tried to remove prefetch also no change same error.\r\n\r\n```\r\n train_ds = train_input_ds \\\r\n        .batch(BATCH_SIZE)  \\\r\n        .prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\n    options = tf.data.Options()\r\n    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\n    train_ds = train_ds.with_options(options)\r\n\r\n```\r\n\r\nAttaching the error log.\r\n\r\n[error.log](https://github.com/tensorflow/tensorflow/files/5213540/error.log)\r\n", "It's difficult to debug the issue here without being able to see a minimal version of the code that reproduces the error. If you can provide that I'd be happy to take a deeper look, but otherwise I think I'm out of ideas unfortunately.\r\nAs a side note, removing prefetch won't make a difference, as using a distribution strategy with the Keras API will automatically leverage prefetch under the hood anyway.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I see the behaviour of the code follows on nightly-build.\r\n\r\nFor 1 machine it is working as expected.\r\nfor multiple machine with using .repeat() and steps_per_epochs() it is working as expected.\r\nAfter removing .repeat() and steps_per_epochs()  on multi machine, It is showing problem.\r\n[Adjust_GPUs.txt](https://github.com/tensorflow/tensorflow/files/5292717/Adjust_GPUs.txt)\r\n", "What problem are you referring to when you remove `.repeat` and `steps_per_epoch`?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@rajeev921 Did you find solution for the issue. If so, can you please share your final code. I am writing a similar code and any input will be really great.\r\n\r\nThanks,\r\nAshwin"]}, {"number": 42887, "title": "undefined reference to `aws_mutex_init'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Loongnix OS(based on Fedora) with kernel:Linux localhost.localdomain 3.10.84-22.fc21.loongson.7.mips64el\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:2.4.0\r\n- Python version:Python\uff1a2.7.9 Python3\uff1a3.7.9\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):3.4.1\r\n- GCC/Compiler version (if compiling from source):gcc version 7.3.1 20180303 (Red Hat 7.3.1-6) (GCC)\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nwhen I build the tensorflow 2.4.0 source code on loongson platform\uff08arch\uff1amips64\uff09use bazel 3.4.1.\uff0c \r\n**use ld\uff1ald.bfd,    not ld.gold**\r\n\r\n**the build failed log is as below:**\r\nERROR: /home/loongson/tensorflow/tensorflow-206/tensorflow/tensorflow/compiler/tf2xla/cc/BUILD:30:21: Linking of rule '//tensorflow/compiler/tf2xla/cc:ops/xla_jit_ops_gen_cc' failed (Exit 1): gcc failed: error executing command\r\n   (cd /home/loongson/.cache/bazel/_bazel_loongson/f0c20e6eab3cd4f95dcf1e27683765aa/execroot/org_tensorflow && \\\r\n   exec env - \\\r\n     LD_LIBRARY_PATH=/opt/protobuf-3.9.2/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib \\\r\n     PATH=/opt/protobuf-3.9.2/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/loongson/.local/bin:/home/loongson/bin \\\r\n     PWD=/proc/self/cwd \\\r\n   /opt/rh/devtoolset-7/root/usr/bin/gcc @bazel-out/mips64-opt-exec-50AE0418/bin/tensorflow/compiler/tf2xla/cc/ops/xla_jit_ops_gen_cc-2.params)\r\n Execution platform: @local_execution_config_platform//:platform\r\n bazel-out/mips64-opt-exec-50AE0418/bin/_solib_mips64/_U_S_Stensorflow_Scompiler_Stf2xla_Scc_Cops_Sxla_Ujit_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `aws_mutex_init'\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Then, I set  .bazelrc file:     build:xla --define=with_xla_support=true to false**\r\n\r\n**then the above failed log is disapear! !!**\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["**but the new build failed accured! build failed log as below:**\r\nERROR: /home/loongson/tensorflow/tensorflow-206/tensorflow/tensorflow/python/BUILD:2944:29: Linking of rule '//tensorflow/python:gen_data_flow_ops_py_wrappers_cc' failed (Exit 1): gcc failed: error executing command \r\n  (cd /home/loongson/.cache/bazel/_bazel_loongson/f0c20e6eab3cd4f95dcf1e27683765aa/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/opt/protobuf-3.9.2/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib \\\r\n    PATH=/opt/protobuf-3.9.2/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/loongson/.local/bin:/home/loongson/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  /opt/rh/devtoolset-7/root/usr/bin/gcc @bazel-out/mips64-opt-exec-50AE0418/bin/tensorflow/python/gen_data_flow_ops_py_wrappers_cc-2.params)\r\nExecution platform: @local_execution_config_platform//:platform\r\nbazel-out/mips64-opt-exec-50AE0418/bin/_solib_mips64/_U_S_Stensorflow_Spython_Cgen_Udata_Uflow_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `aws_mutex_init'\r\nbazel-out/mips64-opt-exec-50AE0418/bin/_solib_mips64/_U_S_Stensorflow_Spython_Cgen_Udata_Uflow_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `aws_thread_launch'\r\nbazel-out/mips64-opt-exec-50AE0418/bin/_solib_mips64/_U_S_Stensorflow_Spython_Cgen_Udata_Uflow_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `Aws::Time::LocalTime(tm*, long)'\r\nbazel-out/mips64-opt-exec-50AE0418/bin/_solib_mips64/_U_S_Stensorflow_Spython_Cgen_Udata_Uflow_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `Aws::FileSystem::CreateTempFilePath()'\r\nbazel-out/mips64-opt-exec-50AE0418/bin/_solib_mips64/_U_S_Stensorflow_Spython_Cgen_Udata_Uflow_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `aws_debug_break'\r\nbazel-out/mips64-opt-exec-50AE0418/bin/_solib_mips64/_U_S_Stensorflow_Spython_Cgen_Udata_Uflow_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `aws_gmtime'\r\nbazel-out/mips64-opt-exec-50AE0418/bin/_solib_mips64/_U_S_Stensorflow_Spython_Cgen_Udata_Uflow_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `aws_timegm'\r\n\r\n\r\n**why? please help me  for this issue! Thanks very much!**", "/cc @gunan Is this 2019 issue still valid https://github.com/tensorflow/tensorflow/issues/29666#issuecomment-551906604 ?", "Is this issue because aws is diabled\uff1fI see  .bazelrc file:  build:noaws --define=no_aws_support=true.\r\nHow can I enable aws?", "could anyone help me to resolve this build fail\uff1f Thank you\uff01", "@mihaimaruseac @samikama may know", "This is also for mips architecture. TF has no official support for this architecture.", "I have resolved this issue\uff01\r\nCan I push code  for supporting mips platform \uff1f", "The code to support mips landed. I think we can close this.\r\n\r\nPlease reopen if there's something else left to do", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42887\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42887\">No</a>\n", "> I have resolved this issue\uff01 Can I push code for supporting mips platform \uff1f\r\n\r\nhow you solve the issue, I am now building tensorflow on riscv64 platform, same Error I meet."]}, {"number": 42886, "title": "TFLite: C++/Java: experimental kernel ctc_beam_search_decoder returns always buffer length=length+1", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):  3.1.0\r\n- GCC/Compiler version (if compiling from source): 9.3.0 \r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n- NDK: android-ndk-r20\r\n\r\n**Describe the current behavior**\r\nAll returned Java `IntBuffer` (TFLite Android) from the concrete function ( `decoder.tflite`) have an extra added byte=0 in the end of the returned `dense_decoded`, e.g. [11,11,4,7,8,0].\r\n=> This happens only in TFLite with the tflite experimental kernel [ctc_beam_search_decoder.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/kernels/ctc_beam_search_decoder.cc) returned from Java.\r\n\r\n**Describe the expected behavior**\r\nThe returned `dense_decoded` from the concrete function ( `decoder.tflite`) should be e.g. [11,11,4,7,8].\r\n=> If I use the `concrete function` directly in python it works as expected\r\n=> If I use the `concrete function` exported as `decoder.tflite` and loaded directly in python it works as expected.\r\n\r\n**Standalone code to reproduce the issue**\r\n```bash\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout -b master 6e9d916229b5aefbdcfd33cbc4b34c9f48b5e6e1\r\nnano .tf_configure.bazelrc\r\n```\r\n\r\nBazel config .tf_configure.bazelrc:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --action_env ANDROID_NDK_HOME=\"CHANGE_TO_YOUR_ANDROID_NDK_HOME\"\r\nbuild --action_env ANDROID_NDK_API_LEVEL=\"21\"\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"28.0.0\"\r\nbuild --action_env ANDROID_SDK_API_LEVEL=\"23\"\r\nbuild --action_env ANDROID_SDK_HOME=\"CHANGE_TO_YOUR_ANDROID_SDK_HOME\"\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu\r\ntest --build_tag_filters=-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\nAnd compile with \r\n```bash\r\nbazel build --cxxopt='--std=c++14' -c opt --fat_apk_cpu=arm64-v8a,armeabi-v7a --config=monolithic \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  //tensorflow/lite/java:tensorflow-lite \\\r\n  //tensorflow/lite/java:tensorflow-lite-gpu \\\r\n  //tensorflow/lite/delegates/flex:delegate \\\r\n  //tensorflow/lite/experimental/kernels:ctc_beam_search_decoder_op \\\r\n  //tmp:tensorflow-lite-select-tf-ops\r\n```\r\n\r\nConcrete function exported to `decoder.tflite`:\r\n```python\r\n@tf.function\r\ndef decode(logits, top_paths=3, beam_width=3):\r\n    batch_size_current, timesteps, _ = tf.shape(input=logits)\r\n    seq_len = tf.fill([batch_size_current], timesteps)\r\n    logits = tf.transpose(a=logits, perm=(1, 0, 2))\r\n\r\n    decoded, log_probabilities = tf.nn.ctc_beam_search_decoder(inputs=logits, top_paths=top_paths, beam_width=beam_width, sequence_length=seq_len)\r\n\r\n    dense_decoded = tf.sparse.to_dense(decoded[0], default_value=-1)\r\n```", "comments": ["@renjie-liu could you take a look at this?", "Hi,\r\n\r\nThanks for filing the bug, wonder do you have the tflite model?\r\n\r\nAlso, wonder if this only occurs with java usage? Have you tried with python tflite api?", "Thank you for your help!\r\n\r\n- \"Have you tried with python tflite api?\": As described, yes and works\r\n=> If I use the concrete function directly in python it works as expected\r\n=> If I use the concrete function exported as decoder.tflite and loaded directly in python it works as expected.\r\n\r\n- \"Also, wonder if this only occurs with java usage?\":\r\nThis I can't say you for sure if the Tensor.java fills the bytes wrong, but I'm quite sure it is coming from the C++ experimental kernel, as from the return of my concrete function (above) `dense_decoded` I'll concat my probability in the end and the 0 zero value is then in between\r\n```python\r\n    ...\r\n    dense_decoded = tf.sparse.to_dense(decoded[0], default_value=-1)\r\n    ...\r\n    output = tf.concat([dense_decoded, probabilities_int, 1)\r\n```\r\nSo the output `dense_decoded ` is [11,11,4,7,8,0] with the added probability [11,11,4,7,8,0, 89] and the zero is before which comes from `tf.nn.ctc_beam_search_decoder` or `tf.sparse.to_dense`.\r\n\r\nIn python the direct model with the concrete function (e.g. training with inference) or loaded as exported tflite works as expected, otherwise I would have wrong accuracies.", "Mhhhh, maybe you are right, it could be from the logits too and then from java as I get the logits from encoder.tflite in java and pass them to the decoder.tflite, where I see the prescribed results", "Xunkai, seems it might relate the java api, can you help take a look?\r\nthanks!", "Sure, can you give me a hint?", "It would be great if we can have a minimal reproducible example (just have beam_search & sparse_to_dense)?\r\n\r\nThen we can compare the c++ inputs/outputs with java inputs outputs?\r\n\r\n+Xunkai, wdyt?", "So the `decoder.tflite` model is:\r\n```python \r\nclass Decode(tf.keras.Model):\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 24, 49], dtype=tf.float32), tf.TensorSpec(shape=[None,], dtype=tf.int32)]) \r\n    def decode(self, logits, batch_size, top_paths=3, beam_width=3):\r\n        batch_size_current, timesteps, _ = logits.get_shape()\r\n        batch_size = batch_size[0]\r\n        seq_len = tf.fill([batch_size], timesteps)\r\n        logits = tf.transpose(a=logits, perm=(1, 0, 2))\r\n\r\n        decoded, log_probabilities = tf.nn.ctc_beam_search_decoder(inputs=logits, top_paths=top_paths, beam_width=beam_width, sequence_length=seq_len)\r\n\r\n        dense_decoded = tf.sparse.to_dense(decoded[0], default_value=-1)\r\n\r\n        return dense_decoded\r\n```\r\n## Python\r\nThe tensor used in python (`shape=(1, 24, 49), dtype=float32`) against the `decoder.tflite` model:\r\n```python \r\ntf.Tensor(\r\n[[[ -1.1646128     -3.7951221     -1.0731119     -5.230322    -4.3759046     -1.1051129     -5.9621816     -2.1139586    -1.6347717     -3.0709565      0.25088528    -4.5464454     0.9719744     -2.6028876     -5.2125697     -4.693067    -3.4992895     -6.196344      -1.9408542     -1.8609589    -5.123465      -2.1778982     -0.3016574     -1.4162496    -0.05596298    -3.3783443     -4.0272593     -3.7445364    -0.0028205316  -0.8726251     -2.6476762     -2.7670584    -3.531002      -2.86586       -3.0021574      1.9833964    -6.5366592     -4.3795114     -9.089881      -2.9438267    -2.824938      -8.183831      -5.9174848    -15.839222    -8.734572     -10.726117      -9.316691       4.295628    18.199356    ]\r\n  [ -0.031096319   -3.871109      -1.1181607     -2.0011232    -4.7141924     -1.2818102     -4.978454       2.0263755    -0.21877344    -0.46911952     1.3215663     -2.7769063     3.4695244     -1.7167692     -4.468163      -3.00177    -3.4645154     -4.2073197     -5.397524      -0.36775938    -4.821177      -1.638662       0.23415852    -1.3846282    -2.6762388     -4.1896615     -1.411914      -2.8303812     1.251152       0.5686881     -0.6898544     -2.794163    -4.0943656     -2.028222      -4.0183196      2.4679868    -2.5596936     -1.7351736     -5.4891076     -1.3824191    -2.1094246     -2.8548317     -3.896057     -13.179549    -8.732274      -8.050248      -7.7822146      1.5458045    20.359257    ]\r\n  [  1.2163008     -2.6954525     -0.86427027     0.6024355    -6.1996074     -3.0337527     -3.3046398      2.9629385     3.3836906      0.6956072     -0.5831372      0.26549777     2.1988766      0.034438375   -1.2926706     -3.415465    -1.8027607     -4.706817      -2.7042725      1.7454389    -1.63372       -2.0432458      0.5463361     -2.4805515    -3.2112253     -3.8260605     -2.330295       0.48245263    -2.5352879     -1.9981833     -3.6866705     -4.316192    -4.7724094     -3.8186944     -6.9987373      0.83749133    -5.358316      -3.382599      -3.0282784     -1.8950666    -4.312699      -0.3736072     -5.263495     -11.096023   -10.782363      -8.377029      -9.81446       -0.5631078    18.654766    ]\r\n  [  1.6706834     -2.135751      -2.6569047      1.4258738    -3.101607      -7.347342      -6.336166      -0.92469734     3.8745248     -0.68873376    -2.5481994      3.0339756     0.019852579    2.4946487     -3.264964      -9.122855    -0.5293837     -3.387656      -6.479887      -0.41217116     2.1036355     -5.3812556     -4.8552337     -4.6979804    -6.1752295     -3.2327073     -3.25391       -1.0128698    -4.135406      -8.941207      -3.8629827     -5.694706    -7.9600415     -6.6381497     -7.7798233     -4.398782    -5.243107      -6.0830593      3.9232616     -4.3167377    -3.767523      -3.2243702     -5.2422233     -9.081962    -9.653151      -6.1841607     -6.414804       3.442686    16.106243    ]\r\n  [  2.5365763     -0.3145837      6.921036       5.1193705     1.4797157     -8.922007       2.5846255      1.3787552     3.09406    9.555295      -2.7645082     15.209727    -2.015432      -3.7025976      4.959807      -5.207284     0.76611304    -3.6603796      3.0620315     -6.1943145    18.713396      -1.9181551     -0.14312221    -3.73462    -5.586571       4.035087       6.6243644     -4.5170846    -2.1310537     -4.0330253      3.1677713     -0.8995264     0.41301596    -2.8552415     -1.0611678     -2.8348706    -7.8290887     -1.8352652      7.9356976     -6.084311    -6.470459      -0.59961814    -2.9941504     -5.9037833    -8.757953      -8.05204       -7.22634       -2.3976681     5.6136956   ]\r\n  [  0.86832124    -4.314832      -1.1376096     -1.2795423     0.89594346    -3.2714832     -4.813009      -3.6954434     6.292471       1.9540067     -1.269824       3.7818973     1.2630774      2.311032      -3.6972969     -8.757897    -1.1408868     -6.4515824     -5.1825223      0.3880705     2.5104828     -1.8647186      1.7398003     -0.53895247    -3.8964303     -4.1509137     -3.1393435     -2.174281    -6.1708093     -5.2546153     -2.1971817     -4.55695    -4.3245444     -1.6113325     -4.6183186     -2.4085505    -3.1237144     -8.3430395      1.2164363     -3.4950488    -2.4105551     -3.26954       -3.483475      -3.6448991    -6.45342       -4.3655534     -3.9982698      2.0058196    13.91262     ]\r\n  [  2.5283318     -1.4483672      0.9969271      0.7170124     4.1993585     -5.9225745     -1.3270315      1.2754217     0.6596287      1.3184649     -3.2381012     16.223343    -2.6598966     -1.6049436     -5.1619163     -5.2185836    -3.4646883      0.37716112    -1.4162335     -3.7551687     6.011435      -6.2358327     -1.0053082     -5.046207    -4.0837035      0.4770194      1.1101766     -0.33754197     1.6581774     -4.2621255      1.5940342      1.3807868    -2.2744715     -0.29614684    -2.8848875     -2.2694578    -8.568389     -10.925359      -0.16693757    -1.9699303    -5.057368      -3.8518627     -1.813983     -14.393661    -8.807828     -10.923024      -8.132841       6.037188     6.71938     ]\r\n  [ -1.0551867     -1.7578293      1.9037585      3.2873793     2.0240114     -2.354026       0.60487497    -0.18619181    -2.147204       0.6680657     -5.2943177     10.29514    -3.7262926     -2.830817      -2.7276952     -5.4776435    -2.9794304     -3.8625076     -1.3735079     -2.5151794     2.8896475     -4.4512296     -2.0915463     -1.2939028    -3.104559      -2.2537181     -4.120955      -4.342249    -1.8565747     -6.2458816     -2.3996937     -1.9764471    -6.0649962     -2.6825225     -5.5547967     -4.4913697    -5.119028      -4.1911855      2.1648426      0.5508409     5.076616       1.6188253     -3.582057      -7.621736    -1.5033997     -2.183586       1.0535034      9.071255    11.899856    ]\r\n  [ -5.2529254     -6.6716137     -3.1146243     -6.264843    -2.824476       5.0508304     -1.5491694     -1.2883891     0.74578935    -7.33851       -3.1347349     -2.9942334     0.5507162     -1.6148804     -4.938992       0.6466265    -2.2037792     -1.9882045     -6.203813       4.076163    -4.691016       1.2091874     -1.0557848     -4.588415    -1.2097976     -3.7823641     -4.6124034     -2.0866807    -3.623191      -5.36186       -4.4979167     -4.4606586    -5.770378      -1.2358248     -6.613919      -3.8514888    -6.235285      -5.905029      -1.0715307      2.8696413    -0.47416314     2.7852128     -4.025336      -4.1802793     0.25927126    -1.3094192      3.6837552     13.21021    11.347459    ]\r\n  [ -5.057721      -5.343788      -1.7458328     -6.3566365    -2.9424264      2.2776768     -3.4749358     -5.947929     3.3519404     -3.874953      -6.0880322     -6.619028    -0.37194625    -1.7473186     -5.295511      -3.2047918    -1.077995      -7.218947      -4.7407937      3.0634851    -6.251387       1.4197563      0.4252913     -5.691605    -2.564915      -6.166654      -5.7835813     -1.8982929    -4.4162564     -3.5899343     -4.082336      -4.0357556    -6.603014      -3.4210358     -7.8280125     -1.4068938    -5.7733307     -8.0943    -2.8861597     -2.887181    -3.1830013     -1.4008173     -2.458579      -2.2794507    -2.2364883     -2.1403112      2.0037267      9.625611    12.308739    ]\r\n  [ -4.736945     -10.628989      -4.470187      -5.965631    -4.763827      -0.7323       -12.857819       0.12708224     4.8935037     -6.2190022      0.14463969    -3.6041832     6.298675       7.4380403     -8.638773      -5.8449683    -2.175786      -2.8586502     -7.0307355      3.7856443    -7.0863433     -3.1873302      3.9701254      0.7573487     1.4138317     -9.364082     -10.078433      -5.241612    -5.444578      -6.0157566     -4.574983      -9.558782   -12.992415      -2.7820683     -9.652053      -2.7950761    -5.322698     -10.596647      -0.91065645    -5.9889627    -0.9559841     -6.9453692      4.51732       -6.399874    -8.571931      -5.1669536     -4.506823       3.0171888    22.158484    ]\r\n  [ -3.068115      -4.752106      -3.8195612     -9.741685    -8.325681       8.063115      -7.9095597     13.477923     3.2089014    -10.606659       3.0653007     -2.3270218    21.029633       4.2431703     -5.482428       7.7515645     5.5879655      2.1668143     -5.791489      -0.39452899    -9.12094    6.6312127     12.389896       5.121841    11.055828      -4.6672044     -3.212209       2.9474885    -1.384693      -1.3363131     -1.3095058     -2.4004366    -4.466605       4.201877      -2.345684       0.7684905    -1.7537786     -8.601547      -2.332861      -2.8643725    -5.8592925      4.338715      -3.1820097     -2.8485777    -7.5072083     -4.256681      -2.5743508      0.94620275     7.3088403   ]\r\n  [ -5.452369      -9.016264      -5.144456      -4.3855796    -7.7754283      1.135269      -6.0672054     -0.46107984    -1.5757593    -10.944435      -5.541817      -3.7400455     1.863794      -4.2366767     -6.463822      -1.712952    -0.47423482    -5.9268956    -11.652019      -1.4437933   -12.260867       0.07316127     0.79780954    -7.9282737     0.9615293     -5.531859      -8.971187      -6.121879    -5.8926487     -8.665367      -7.853302      -6.535997    -8.472007      -1.7106935     -9.621098      -3.748615    -9.184453     -11.467549      -1.8488725      2.7637093     0.13214706    12.264789      -1.925021      -4.1397038     0.675782       0.8341355      4.790847      15.223495    16.989325    ]\r\n  [ -3.9985054     -9.17712       -9.745225      -9.341779    -4.371359      -3.9573042     -5.219108     -10.031667    -0.4832547     -7.221607      -9.378301      -3.553962    -4.277628      -8.165889      -6.766566      -7.317974    -6.7054176     -8.275714      -8.473683      -2.6473777    -9.137348      -4.5135856     -7.3449397     -4.220116    -4.296596      -0.122100346   -8.692269       9.825402     0.2553134     -1.4249631      1.4904976     -3.87309    -6.7046905      4.780445      -8.814603      -3.5223646    -8.859568     -13.593255      -7.687883       1.6292764     2.5215695     10.300719      -0.8014014     -8.760519     0.9533107     -2.471599      -0.55962145    15.309047     8.776088    ]\r\n  [  2.1049914     -5.7175593     -0.6238275     -1.5772029    -2.3334796     -7.376515      -5.821173      -4.5206504     5.1160502      0.7101815     -2.4477665      1.0354464    -3.0778773     -5.6364365     -3.1852434     -6.0968738    -0.25753418    -4.420667      -5.6371856      2.2084491    -6.0431843     -1.6050267     -3.8200142     -0.9649778    -1.0719372      3.1703932      0.28604648    19.212442     5.1282506      1.6608907      3.9577866     -1.4500607    -7.6976585      6.7093034     -5.434231      -0.1969173    -1.6897439    -11.350258      -5.0953856     -2.5845845    -8.346431      -5.030808      -7.1316266    -10.417641    -9.598651     -12.076622     -15.840962      -2.767027     5.2507353   ]\r\n  [ -1.6916462     -6.835287      -1.2959373     -1.9050026    -6.942196      -3.4665506     -4.852651      -3.9581516     1.3320965     -3.2232249      0.0419402     -0.6517505    -3.0223713      0.40320182    -3.8511922     -5.849378    -3.6755798     -4.371869      -4.9620113     -1.1411376    -1.0673978     -3.7844536     -1.251507      -5.0279365    -2.329897      -5.9839554      2.9948866      1.7984604    -2.9251158     -2.347848      -0.07170879    -2.4061537    -1.4042406     -2.319747      -4.6162496     -1.4313077    -4.4996104     -6.91234       -0.17195721    -6.1135383    -4.90153       -7.101444      -5.3525157     -5.6080756    -8.066487      -7.533741      -8.561157      -0.61673695    11.969102    ]\r\n  [ -5.2910733     -4.0735116      3.8942814      4.9986653    -8.251605      -5.863548       0.52648324    -2.2165954    -6.2707744      4.5292373     -7.903667      -1.6777164    -7.690946      -3.0448997      9.52889       -3.925293     3.7865422     -6.1437564      1.8726383     -5.674963     3.302004      -3.169571      -5.190231      -7.159308    -5.56988       -6.0096536     14.196355      -2.219622    -3.208381       2.9780896      0.40442243    -1.306855    -1.3609582     -1.6217264      1.4096924      2.9067333   -10.75253       -1.7663544      3.9251118     -6.1464143   -13.524259      -3.199681       0.40105966   -12.946118   -11.353162     -12.651736     -14.381966      -2.9572937     6.5882106   ]\r\n  [ -1.9417683     -5.0573096     -2.8481205     -2.6008158    -3.7663803     -5.171085      -4.253186      -6.007723    -0.06816677     2.1923892     -4.1221375     -2.1063364    -4.822325      -1.5166817      0.07642153    -5.5134835     0.9086635     -5.2698345     -3.3004758     -1.0602385    -1.2910267     -1.3988553     -1.7253238     -3.6106098    -3.3936682     -3.8622196      3.0371997     -0.35850498    -5.599482      -3.3486063     -2.944565      -3.8021555    -3.807739      -1.49082       -2.3763907     -2.1279447    -5.170482      -5.828091      -0.5425779     -3.1264536    -4.5475307     -1.6777412     -4.526622      -5.372235    -7.2601147     -5.48196       -6.54913       -1.120478    12.390136    ]\r\n  [ -3.461581      -1.4558958      6.1289496      2.9735203    -1.2450281     -4.602448      -1.9891156     -6.117136    -5.5234065     -2.696553      -3.586568       2.8084233    -7.3573537     -4.507172       5.5286765     -2.4690719     2.523189      -2.8085434     -1.6559683     -1.8171147     2.1076992     -5.626428      -7.231596      -6.0143523    -6.9726806     -0.8886002     13.542512      -1.3580791     0.6832211      2.2281325      0.20517886     0.4207892     0.1542706      0.22497724     2.903372      -0.43212676    -9.586123      -6.539781       0.26695496    -8.207171   -15.066978      -5.713083      -2.879772     -16.250275   -12.022546     -15.688462     -14.957932      -2.622998     5.1257606   ]\r\n  [ -5.2359405     -6.4311886      3.7954795     -0.6747386    -3.9157393     -4.123837      -3.995203      -6.349108    -3.793074      -0.80095726    -1.330827      -2.0078313    -5.1554894     -2.8631523     -0.881418      -5.6142144    -1.8561249     -6.5457616     -4.5799966     -2.044014     0.47274694    -1.1704346     -3.5675278     -4.5367837    -5.979168      -5.4071426      6.3918777     -1.1991844    -2.0291529     -0.08346957    -1.2887247     -3.2507048    -1.9896507     -0.8941142     -0.2793278     -3.2223158    -5.5826225     -4.921161       0.6414426     -7.240561    -9.987512      -5.0585003     -6.553722      -7.946823    -8.550385      -7.669768      -9.521777      -3.718647    13.088218    ]\r\n  [ -6.0787616     -2.760232      -1.467815       2.1198342    -8.399127      -8.965743      -1.4192119     -4.3845778    -8.508703      -0.2962972     -7.1257405     -3.9597538    -7.555639      -3.2412634      4.911804      -7.2009654     1.1253124     -6.595501      -3.956956      -5.8060346     2.4504921     -3.4591324     -5.757239     -10.887218    -9.258279      -9.541945      12.070434      -1.5752869    -6.057066       2.3558514     -3.168074      -2.4653132     0.12063774    -2.8162138      4.471817      -0.35249588   -14.35635       -4.5517406      0.36203933    -8.030429   -15.201206      -6.287722       0.23622651   -19.051283   -11.752037     -14.967655     -13.759575      -3.3683908    10.117752    ]\r\n  [ -7.079468      -5.265597      -1.5848751      2.253567    -6.46945       -8.430444      -3.5897791     -7.8991075    -5.6643214      2.3023212     -7.853886      -3.5041678   -12.051305     -10.392461       2.6387234     -5.6902485    -0.25494644    -7.3672867     -6.844403      -7.0112057     0.7349677     -3.6644032     -5.8336225     -6.475676    -8.460027      -3.5292912     13.694934       2.082901    -2.1121929      2.6222725     -6.9787374     -1.9552851    -2.3152606      1.9507776      5.016398      -0.16578133   -10.859069      -5.3187003      0.8972644     -5.7399955   -14.465164      -3.2254312      1.2975389    -15.831334   -14.408114     -14.99931      -14.778538      -1.4908476    11.922107    ]\r\n  [ -1.9436159     -4.575247      -6.390955      -4.9047637    -3.2908394     -3.1129427     -2.3592062     -1.394528    -2.3547149     -1.6598221     -2.1417007     -3.1081944    -2.5159578     -7.691215      -4.6543746     -4.771032    -2.7071314     -6.7351565     -4.7741923     -3.364289    -7.471546      -2.3670242     -7.543904      -5.047063    -4.729469      -5.9341073      0.3747803     -1.0018672     0.9036009      1.5984081     -1.3429843     -0.2840887     1.367341       1.3728013     -1.4689537     -1.5738052    -9.112517      -9.113147      -3.9519968     -4.039954    -4.3511705     -7.818344       0.18420011   -10.240898    -8.708661      -9.73697       -9.248626      -3.1827362    14.422031    ]\r\n  [ -6.3566413     -8.544102     -10.048785      -5.773206    -3.3318303     -2.4811735     -3.6157846     -4.06117   -11.2877655     -3.158837      -7.167846      -9.7644825   -10.089507     -10.784669      -9.1176    -7.5642204    -4.9335723     -7.0899386     -6.513907      -1.8153943    -7.955276      -3.2648313     -9.505028      -9.440993    -4.5477047    -10.648444      -0.6833103     -1.6586121    -3.2512772     -2.2827086     -4.558927       0.26295188    -0.07339359     1.532421      -0.18356426    -4.358731   -11.891158      -8.6237    -7.578995      -6.1212106   -13.254722     -15.914986     -11.113473     -18.317835   -15.800079     -17.134977     -19.690008      -7.6697803    13.230232    ]\r\n]]\r\n```\r\nOutput: `[[  20   11   47   12   47   27   26   26   26]]`\r\n## Java (tested on Android, compiled TF2.3, see above)\r\nThe same tensor used in java (`shape=(1, 24, 49), dtype=float32`) against the `decoder.tflite` model:\r\n```java     \r\nfinal float[][][] logits_test= {{\r\n        { -1.1646128f,-3.7951221f,-1.0731119f,-5.230322f,-4.3759046f,-1.1051129f,-5.9621816f,-2.1139586f,-1.6347717f,-3.0709565f,0.25088528f,-4.5464454f,0.9719744f,-2.6028876f,-5.2125697f,-4.693067f,-3.4992895f,-6.196344f,-1.9408542f,-1.8609589f,-5.123465f,-2.1778982f,-0.3016574f,-1.4162496f,-0.05596298f,-3.3783443f,-4.0272593f,-3.7445364f,-0.0028205316f, -0.8726251f,-2.6476762f,-2.7670584f,-3.531002f,-2.86586f, -3.0021574f,1.9833964f,-6.5366592f,-4.3795114f,-9.089881f,-2.9438267f,-2.824938f,-8.183831f,-5.9174848f,-15.839222f,-8.734572f,-10.726117f,-9.316691f, 4.295628f,18.199356f},\r\n        { -0.031096319f,-3.871109f,-1.1181607f,-2.0011232f,-4.7141924f,-1.2818102f,-4.978454f, 2.0263755f,-0.21877344f,-0.46911952f,1.3215663f,-2.7769063f,3.4695244f,-1.7167692f,-4.468163f,-3.00177f,-3.4645154f,-4.2073197f,-5.397524f,-0.36775938f,-4.821177f,-1.638662f, 0.23415852f,-1.3846282f,-2.6762388f,-4.1896615f,-1.411914f,-2.8303812f,1.251152f, 0.5686881f,-0.6898544f,-2.794163f,-4.0943656f,-2.028222f,-4.0183196f,2.4679868f,-2.5596936f,-1.7351736f,-5.4891076f,-1.3824191f,-2.1094246f,-2.8548317f,-3.896057f,-13.179549f,-8.732274f,-8.050248f,-7.7822146f,1.5458045f,20.359257f},\r\n        {  1.2163008f,-2.6954525f,-0.86427027f,0.6024355f,-6.1996074f,-3.0337527f,-3.3046398f,2.9629385f,3.3836906f,0.6956072f,-0.5831372f,0.26549777f,2.1988766f,0.034438375f,-1.2926706f,-3.415465f,-1.8027607f,-4.706817f,-2.7042725f,1.7454389f,-1.63372f, -2.0432458f,0.5463361f,-2.4805515f,-3.2112253f,-3.8260605f,-2.330295f, 0.48245263f,-2.5352879f,-1.9981833f,-3.6866705f,-4.316192f,-4.7724094f,-3.8186944f,-6.9987373f,0.83749133f,-5.358316f,-3.382599f,-3.0282784f,-1.8950666f,-4.312699f,-0.3736072f,-5.263495f,-11.096023f,-10.782363f,-8.377029f,-9.81446f, -0.5631078f,18.654766f},\r\n        {  1.6706834f,-2.135751f,-2.6569047f,1.4258738f,-3.101607f,-7.347342f,-6.336166f,-0.92469734f,3.8745248f,-0.68873376f,-2.5481994f,3.0339756f,0.019852579f,2.4946487f,-3.264964f,-9.122855f,-0.5293837f,-3.387656f,-6.479887f,-0.41217116f,2.1036355f,-5.3812556f,-4.8552337f,-4.6979804f,-6.1752295f,-3.2327073f,-3.25391f, -1.0128698f,-4.135406f,-8.941207f,-3.8629827f,-5.694706f,-7.9600415f,-6.6381497f,-7.7798233f,-4.398782f,-5.243107f,-6.0830593f,3.9232616f,-4.3167377f,-3.767523f,-3.2243702f,-5.2422233f,-9.081962f,-9.653151f,-6.1841607f,-6.414804f, 3.442686f,16.106243f},\r\n        {  2.5365763f,-0.3145837f,6.921036f, 5.1193705f,1.4797157f,-8.922007f, 2.5846255f,1.3787552f,3.09406f,  9.555295f,-2.7645082f,15.209727f,-2.015432f,-3.7025976f,4.959807f,-5.207284f,0.76611304f,-3.6603796f,3.0620315f,-6.1943145f,18.713396f,-1.9181551f,-0.14312221f,-3.73462f,-5.586571f, 4.035087f, 6.6243644f,-4.5170846f,-2.1310537f,-4.0330253f,3.1677713f,-0.8995264f,0.41301596f,-2.8552415f,-1.0611678f,-2.8348706f,-7.8290887f,-1.8352652f,7.9356976f,-6.084311f,-6.470459f,-0.59961814f,-2.9941504f,-5.9037833f,-8.757953f,-8.05204f, -7.22634f, -2.3976681f,5.6136956f},\r\n        {  0.86832124f,-4.314832f,-1.1376096f,-1.2795423f,0.89594346f,-3.2714832f,-4.813009f,-3.6954434f,6.292471f, 1.9540067f,-1.269824f, 3.7818973f,1.2630774f,2.311032f,-3.6972969f,-8.757897f,-1.1408868f,-6.4515824f,-5.1825223f,0.3880705f,2.5104828f,-1.8647186f,1.7398003f,-0.53895247f,-3.8964303f,-4.1509137f,-3.1393435f,-2.174281f,-6.1708093f,-5.2546153f,-2.1971817f,-4.55695f,-4.3245444f,-1.6113325f,-4.6183186f,-2.4085505f,-3.1237144f,-8.3430395f,1.2164363f,-3.4950488f,-2.4105551f,-3.26954f, -3.483475f,-3.6448991f,-6.45342f, -4.3655534f,-3.9982698f,2.0058196f,13.91262f},\r\n        {  2.5283318f,-1.4483672f,0.9969271f,0.7170124f,4.1993585f,-5.9225745f,-1.3270315f,1.2754217f,0.6596287f,1.3184649f,-3.2381012f,16.223343f,-2.6598966f,-1.6049436f,-5.1619163f,-5.2185836f,-3.4646883f,0.37716112f,-1.4162335f,-3.7551687f,6.011435f,-6.2358327f,-1.0053082f,-5.046207f,-4.0837035f,0.4770194f,1.1101766f,-0.33754197f,1.6581774f,-4.2621255f,1.5940342f,1.3807868f,-2.2744715f,-0.29614684f,-2.8848875f,-2.2694578f,-8.568389f,-10.925359f,-0.16693757f,-1.9699303f,-5.057368f,-3.8518627f,-1.813983f,-14.393661f,-8.807828f,-10.923024f,-8.132841f, 6.037188f,6.71938f},\r\n        { -1.0551867f,-1.7578293f,1.9037585f,3.2873793f,2.0240114f,-2.354026f, 0.60487497f,-0.18619181f,-2.147204f, 0.6680657f,-5.2943177f,10.29514f,-3.7262926f,-2.830817f,-2.7276952f,-5.4776435f,-2.9794304f,-3.8625076f,-1.3735079f,-2.5151794f,2.8896475f,-4.4512296f,-2.0915463f,-1.2939028f,-3.104559f,-2.2537181f,-4.120955f,-4.342249f,-1.8565747f,-6.2458816f,-2.3996937f,-1.9764471f,-6.0649962f,-2.6825225f,-5.5547967f,-4.4913697f,-5.119028f,-4.1911855f,2.1648426f,0.5508409f,5.076616f, 1.6188253f,-3.582057f,-7.621736f,-1.5033997f,-2.183586f, 1.0535034f,9.071255f,11.899856f},\r\n        { -5.2529254f,-6.6716137f,-3.1146243f,-6.264843f,-2.824476f, 5.0508304f,-1.5491694f,-1.2883891f,0.74578935f,-7.33851f, -3.1347349f,-2.9942334f,0.5507162f,-1.6148804f,-4.938992f, 0.6466265f,-2.2037792f,-1.9882045f,-6.203813f, 4.076163f,-4.691016f, 1.2091874f,-1.0557848f,-4.588415f,-1.2097976f,-3.7823641f,-4.6124034f,-2.0866807f,-3.623191f,-5.36186f, -4.4979167f,-4.4606586f,-5.770378f,-1.2358248f,-6.613919f,-3.8514888f,-6.235285f,-5.905029f,-1.0715307f,2.8696413f,-0.47416314f,2.7852128f,-4.025336f,-4.1802793f,0.25927126f,-1.3094192f,3.6837552f,13.21021f,11.347459f},\r\n        { -5.057721f,-5.343788f,-1.7458328f,-6.3566365f,-2.9424264f,2.2776768f,-3.4749358f,-5.947929f,3.3519404f,-3.874953f,-6.0880322f,-6.619028f,-0.37194625f,-1.7473186f,-5.295511f,-3.2047918f,-1.077995f,-7.218947f,-4.7407937f,3.0634851f,-6.251387f, 1.4197563f,0.4252913f,-5.691605f,-2.564915f,-6.166654f,-5.7835813f,-1.8982929f,-4.4162564f,-3.5899343f,-4.082336f,-4.0357556f,-6.603014f,-3.4210358f,-7.8280125f,-1.4068938f,-5.7733307f,-8.0943f,  -2.8861597f,-2.887181f,-3.1830013f,-1.4008173f,-2.458579f,-2.2794507f,-2.2364883f,-2.1403112f,2.0037267f,9.625611f,12.308739f},\r\n        { -4.736945f,-10.628989f,-4.470187f,-5.965631f,-4.763827f,-0.7323f, -12.857819f, 0.12708224f,4.8935037f,-6.2190022f,0.14463969f,-3.6041832f,6.298675f, 7.4380403f,-8.638773f,-5.8449683f,-2.175786f,-2.8586502f,-7.0307355f,3.7856443f,-7.0863433f,-3.1873302f,3.9701254f,0.7573487f,1.4138317f,-9.364082f,-10.078433f,-5.241612f,-5.444578f,-6.0157566f,-4.574983f,-9.558782f,-12.992415f,-2.7820683f,-9.652053f,-2.7950761f,-5.322698f,-10.596647f,-0.91065645f,-5.9889627f,-0.9559841f,-6.9453692f,4.51732f, -6.399874f,-8.571931f,-5.1669536f,-4.506823f, 3.0171888f,22.158484f},\r\n        { -3.068115f,-4.752106f,-3.8195612f,-9.741685f,-8.325681f, 8.063115f,-7.9095597f,13.477923f,3.2089014f,-10.606659f, 3.0653007f,-2.3270218f,21.029633f, 4.2431703f,-5.482428f, 7.7515645f,5.5879655f,2.1668143f,-5.791489f,-0.39452899f,-9.12094f,  6.6312127f,12.389896f, 5.121841f,11.055828f,-4.6672044f,-3.212209f, 2.9474885f,-1.384693f,-1.3363131f,-1.3095058f,-2.4004366f,-4.466605f, 4.201877f,-2.345684f, 0.7684905f,-1.7537786f,-8.601547f,-2.332861f,-2.8643725f,-5.8592925f,4.338715f,-3.1820097f,-2.8485777f,-7.5072083f,-4.256681f,-2.5743508f,0.94620275f,7.3088403f},\r\n        { -5.452369f,-9.016264f,-5.144456f,-4.3855796f,-7.7754283f,1.135269f,-6.0672054f,-0.46107984f,-1.5757593f,-10.944435f,-5.541817f,-3.7400455f,1.863794f,-4.2366767f,-6.463822f,-1.712952f,-0.47423482f,-5.9268956f,-11.652019f,-1.4437933f,-12.260867f, 0.07316127f,0.79780954f,-7.9282737f,0.9615293f,-5.531859f,-8.971187f,-6.121879f,-5.8926487f,-8.665367f,-7.853302f,-6.535997f,-8.472007f,-1.7106935f,-9.621098f,-3.748615f,-9.184453f,-11.467549f,-1.8488725f,2.7637093f,0.13214706f,12.264789f,-1.925021f,-4.1397038f,0.675782f, 0.8341355f,4.790847f,15.223495f,16.989325f},\r\n        { -3.9985054f,-9.17712f, -9.745225f,-9.341779f,-4.371359f,-3.9573042f,-5.219108f,-10.031667f,-0.4832547f,-7.221607f,-9.378301f,-3.553962f,-4.277628f,-8.165889f,-6.766566f,-7.317974f,-6.7054176f,-8.275714f,-8.473683f,-2.6473777f,-9.137348f,-4.5135856f,-7.3449397f,-4.220116f,-4.296596f,-0.122100346f,-8.692269f, 9.825402f,0.2553134f,-1.4249631f,1.4904976f,-3.87309f,-6.7046905f,4.780445f,-8.814603f,-3.5223646f,-8.859568f,-13.593255f,-7.687883f, 1.6292764f,2.5215695f,10.300719f,-0.8014014f,-8.760519f,0.9533107f,-2.471599f,-0.55962145f,15.309047f,8.776088f},\r\n        {  2.1049914f,-5.7175593f,-0.6238275f,-1.5772029f,-2.3334796f,-7.376515f,-5.821173f,-4.5206504f,5.1160502f,0.7101815f,-2.4477665f,1.0354464f,-3.0778773f,-5.6364365f,-3.1852434f,-6.0968738f,-0.25753418f,-4.420667f,-5.6371856f,2.2084491f,-6.0431843f,-1.6050267f,-3.8200142f,-0.9649778f,-1.0719372f,3.1703932f,0.28604648f,19.212442f,5.1282506f,1.6608907f,3.9577866f,-1.4500607f,-7.6976585f,6.7093034f,-5.434231f,-0.1969173f,-1.6897439f,-11.350258f,-5.0953856f,-2.5845845f,-8.346431f,-5.030808f,-7.1316266f,-10.417641f,-9.598651f,-12.076622f,-15.840962f,-2.767027f,5.2507353f},\r\n        { -1.6916462f,-6.835287f,-1.2959373f,-1.9050026f,-6.942196f,-3.4665506f,-4.852651f,-3.9581516f,1.3320965f,-3.2232249f,0.0419402f,-0.6517505f,-3.0223713f,0.40320182f,-3.8511922f,-5.849378f,-3.6755798f,-4.371869f,-4.9620113f,-1.1411376f,-1.0673978f,-3.7844536f,-1.251507f,-5.0279365f,-2.329897f,-5.9839554f,2.9948866f,1.7984604f,-2.9251158f,-2.347848f,-0.07170879f,-2.4061537f,-1.4042406f,-2.319747f,-4.6162496f,-1.4313077f,-4.4996104f,-6.91234f, -0.17195721f,-6.1135383f,-4.90153f, -7.101444f,-5.3525157f,-5.6080756f,-8.066487f,-7.533741f,-8.561157f,-0.61673695f,11.969102f},\r\n        { -5.2910733f,-4.0735116f,3.8942814f,4.9986653f,-8.251605f,-5.863548f, 0.52648324f,-2.2165954f,-6.2707744f,4.5292373f,-7.903667f,-1.6777164f,-7.690946f,-3.0448997f,9.52889f, -3.925293f,3.7865422f,-6.1437564f,1.8726383f,-5.674963f,3.302004f,-3.169571f,-5.190231f,-7.159308f,-5.56988f, -6.0096536f,14.196355f,-2.219622f,-3.208381f, 2.9780896f,0.40442243f,-1.306855f,-1.3609582f,-1.6217264f,1.4096924f,2.9067333f,-10.75253f, -1.7663544f,3.9251118f,-6.1464143f,-13.524259f,-3.199681f, 0.40105966f,-12.946118f,-11.353162f,-12.651736f,-14.381966f,-2.9572937f,6.5882106f},\r\n        { -1.9417683f,-5.0573096f,-2.8481205f,-2.6008158f,-3.7663803f,-5.171085f,-4.253186f,-6.007723f,-0.06816677f,2.1923892f,-4.1221375f,-2.1063364f,-4.822325f,-1.5166817f,0.07642153f,-5.5134835f,0.9086635f,-5.2698345f,-3.3004758f,-1.0602385f,-1.2910267f,-1.3988553f,-1.7253238f,-3.6106098f,-3.3936682f,-3.8622196f,3.0371997f,-0.35850498f,-5.599482f,-3.3486063f,-2.944565f,-3.8021555f,-3.807739f,-1.49082f, -2.3763907f,-2.1279447f,-5.170482f,-5.828091f,-0.5425779f,-3.1264536f,-4.5475307f,-1.6777412f,-4.526622f,-5.372235f,-7.2601147f,-5.48196f, -6.54913f, -1.120478f,12.390136f},\r\n        { -3.461581f,-1.4558958f,6.1289496f,2.9735203f,-1.2450281f,-4.602448f,-1.9891156f,-6.117136f,-5.5234065f,-2.696553f,-3.586568f, 2.8084233f,-7.3573537f,-4.507172f, 5.5286765f,-2.4690719f,2.523189f,-2.8085434f,-1.6559683f,-1.8171147f,2.1076992f,-5.626428f,-7.231596f,-6.0143523f,-6.9726806f,-0.8886002f,13.542512f,-1.3580791f,0.6832211f,2.2281325f,0.20517886f,0.4207892f,0.1542706f,0.22497724f,2.903372f,-0.43212676f,-9.586123f,-6.539781f, 0.26695496f,-8.207171f,-15.066978f,-5.713083f,-2.879772f,-16.250275f,-12.022546f,-15.688462f,-14.957932f,-2.622998f,5.1257606f},\r\n        { -5.2359405f,-6.4311886f,3.7954795f,-0.6747386f,-3.9157393f,-4.123837f,-3.995203f,-6.349108f,-3.793074f,-0.80095726f,-1.330827f,-2.0078313f,-5.1554894f,-2.8631523f,-0.881418f,-5.6142144f,-1.8561249f,-6.5457616f,-4.5799966f,-2.044014f,0.47274694f,-1.1704346f,-3.5675278f,-4.5367837f,-5.979168f,-5.4071426f,6.3918777f,-1.1991844f,-2.0291529f,-0.08346957f,-1.2887247f,-3.2507048f,-1.9896507f,-0.8941142f,-0.2793278f,-3.2223158f,-5.5826225f,-4.921161f, 0.6414426f,-7.240561f,-9.987512f,-5.0585003f,-6.553722f,-7.946823f,-8.550385f,-7.669768f,-9.521777f,-3.718647f,13.088218f},\r\n        { -6.0787616f,-2.760232f,-1.467815f, 2.1198342f,-8.399127f,-8.965743f,-1.4192119f,-4.3845778f,-8.508703f,-0.2962972f,-7.1257405f,-3.9597538f,-7.555639f,-3.2412634f,4.911804f,-7.2009654f,1.1253124f,-6.595501f,-3.956956f,-5.8060346f,2.4504921f,-3.4591324f,-5.757239f,-10.887218f,-9.258279f,-9.541945f,12.070434f,-1.5752869f,-6.057066f, 2.3558514f,-3.168074f,-2.4653132f,0.12063774f,-2.8162138f,4.471817f,-0.35249588f,-14.35635f, -4.5517406f,0.36203933f,-8.030429f,-15.201206f,-6.287722f, 0.23622651f,-19.051283f,-11.752037f,-14.967655f,-13.759575f,-3.3683908f,10.117752f},\r\n        { -7.079468f,-5.265597f,-1.5848751f,2.253567f,-6.46945f, -8.430444f,-3.5897791f,-7.8991075f,-5.6643214f,2.3023212f,-7.853886f,-3.5041678f,-12.051305f,-10.392461f, 2.6387234f,-5.6902485f,-0.25494644f,-7.3672867f,-6.844403f,-7.0112057f,0.7349677f,-3.6644032f,-5.8336225f,-6.475676f,-8.460027f,-3.5292912f,13.694934f, 2.082901f,-2.1121929f,2.6222725f,-6.9787374f,-1.9552851f,-2.3152606f,1.9507776f,5.016398f,-0.16578133f,-10.859069f,-5.3187003f,0.8972644f,-5.7399955f,-14.465164f,-3.2254312f,1.2975389f,-15.831334f,-14.408114f,-14.99931f,-14.778538f,-1.4908476f,11.922107f},\r\n        { -1.9436159f,-4.575247f,-6.390955f,-4.9047637f,-3.2908394f,-3.1129427f,-2.3592062f,-1.394528f,-2.3547149f,-1.6598221f,-2.1417007f,-3.1081944f,-2.5159578f,-7.691215f,-4.6543746f,-4.771032f,-2.7071314f,-6.7351565f,-4.7741923f,-3.364289f,-7.471546f,-2.3670242f,-7.543904f,-5.047063f,-4.729469f,-5.9341073f,0.3747803f,-1.0018672f,0.9036009f,1.5984081f,-1.3429843f,-0.2840887f,1.367341f, 1.3728013f,-1.4689537f,-1.5738052f,-9.112517f,-9.113147f,-3.9519968f,-4.039954f,-4.3511705f,-7.818344f, 0.18420011f,-10.240898f,-8.708661f,-9.73697f, -9.248626f,-3.1827362f,14.422031f},\r\n        { -6.3566413f,-8.544102f,-10.048785f,-5.773206f,-3.3318303f,-2.4811735f,-3.6157846f,-4.06117f,-11.2877655f,-3.158837f,-7.167846f,-9.7644825f,-10.089507f,-10.784669f,-9.1176f,  -7.5642204f,-4.9335723f,-7.0899386f,-6.513907f,-1.8153943f,-7.955276f,-3.2648313f,-9.505028f,-9.440993f,-4.5477047f,-10.648444f,-0.6833103f,-1.6586121f,-3.2512772f,-2.2827086f,-4.558927f, 0.26295188f,-0.07339359f,1.532421f,-0.18356426f,-4.358731f,-11.891158f,-8.6237f,  -7.578995f,-6.1212106f,-13.254722f,-15.914986f,-11.113473f,-18.317835f,-15.800079f,-17.134977f,-19.690008f,-7.6697803f,13.230232f}\r\n}};\r\n```\r\n\r\nMethod in java (e.g. length = 20, has to be larger than expected dynamic results):\r\n```java\r\npublic static IntBuffer[] run(Interpreter interpreter, int length, float[][][] outputLogits){\r\n    try {\r\n        Object[] inputs = {outputLogits, new int[]{1}}; // batch size 1\r\n        Map<Integer, Object> outputs = new HashMap<>();\r\n        IntBuffer[] results = new IntBuffer[3];\r\n\r\n        for (int i = 0; i < 3; i++){\r\n            ByteBuffer buffer = ByteBuffer.allocateDirect(length * 4);//int32\r\n            buffer.order(ByteOrder.nativeOrder());\r\n            IntBuffer output = buffer.asIntBuffer();\r\n\r\n            results[i] = output;\r\n            outputs.put(i, output);\r\n        }\r\n        interpreter.runForMultipleInputsOutputs(inputs, outputs);\r\n        return results;\r\n    } catch (Exception e) {\r\n        e.printStackTrace();\r\n    }\r\n    return null;\r\n}\r\n\r\n```\r\n\r\nOutput: `[20, 11, 47, 12, 47, 27, 26, 26, 26, 0]`", "Thanks a lot for the debug!\r\n\r\nWonder:\r\n\r\nwhy there are three outputs in java? shouldn't it be one output?", "You're welcome, yeah top_paths is set to 3 and originally I returned all 3, just get the first one:\r\n\r\n```java\r\npublic static IntBuffer[] run(Interpreter interpreter, int length, float[][][] outputLogits){\r\n    try {\r\n        Object[] inputs = {outputLogits, new int[]{1}}; // batch size 1\r\n        Map<Integer, Object> outputs = new HashMap<>();\r\n\r\n        ByteBuffer buffer = ByteBuffer.allocateDirect(length * 4);//int32\r\n        buffer.order(ByteOrder.nativeOrder());\r\n        IntBuffer output = buffer.asIntBuffer();\r\n        outputs.put(0, output);\r\n\r\n        interpreter.runForMultipleInputsOutputs(inputs, outputs);\r\n        return output;\r\n    } catch (Exception e) {\r\n        e.printStackTrace();\r\n    }\r\n    return null;\r\n}\r\n\r\n```", "Hi Cefaci,\r\n\r\nLooking at the results again, I think I'm a little bit confused.\r\n\r\nThe python outputs a `2-d` tensor, shaped `[1, 9]`, and the result is \r\n\r\n```\r\n[[ 20 11 47 12 47 27 26 26 26]]\r\n```\r\n\r\nwhile the java outputs a `1-d` tensor, shaped [10], and the result is \r\n\r\n```\r\n[20, 11, 47, 12, 47, 27, 26, 26, 26, 0]\r\n```\r\n?\r\n\r\nAnd we should expect a `2-d` tensor right?\r\n\r\n\"top_paths is set to 3\", but we still only have 1 output right (while the output contains all the pats)?\r\n", "No it is good, it is how you retrieve it in java and the batchsize is here set to 1. The last zero is wrong and differs from the same python output, the length should be both 9 as well and not an extra added zero in the end:\r\n```\r\n[ 20 11 47 12 47 27 26 26 26] != \r\n[20, 11, 47, 12, 47, 27, 26, 26, 26, 0]\r\n```\r\nActually even the buffers extends by one the default value should be `-1` the one set in the tf.sparse.to_dense\r\n```\r\ndense_decoded = tf.sparse.to_dense(decoded[0], default_value=-1)\r\n```\r\nIt is `0` and this is an index part of my decoded characters", "I mean why python gives you a 2-d tensor while the java gives you a 1-d tensor?", "- \"I mean why python gives you a 2-d tensor while the java gives you a 1-d tensor?\":\r\nIf I increase the batch size the next result will be just appended to the IntBuffer:\r\n[20, 11, 47, 12, 47, 27, 26, 26, 26, 20, 11, 47, 12, 47, 27, 26, 26, 26, 0, 0, 0, 0]\r\n- The last zero is my fault, I checked the complete returned result, e.g.\r\nArrays.copyOfRange(b.array(), b.arrayOffset(), b.array().length);\r\n\r\nMany thanks for your help and sorry!!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42886\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42886\">No</a>\n"]}, {"number": 42885, "title": "Unexpected behaviour of tf.keras.activations.relu", "body": "**System information**\r\nTensorFlow version: v2.3.0-0-gb36436b087 2.3.0\r\n\r\n**Describe the current behavior**\r\nWhen passing `np.nan` to `tf.keras.activations.relu`, it returns 0.0. \r\nThis only happens when using GPU. The relu activation behaves as expected under CPU (it returns `nan`).\r\n\r\n**Describe the expected behavior**\r\nWhen passing `np.nan` to `tf.keras.activations.relu`, it should return `nan`.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1FhsgStZ_3tIIC3WJrCBl3TOjtgWyn2_2?usp=sharing\r\n\r\n![image](https://user-images.githubusercontent.com/47107572/91958933-4583ac80-ed08-11ea-95d2-854e631c114e.png)\r\n", "comments": ["@alvaro-garcia-carrasco \r\nI ran the code shared on tf-nightly and face a different error, please find the [gist shared](https://colab.research.google.com/gist/Saduf2019/341f62d0d325f5863ac2e545f1177504/untitled401.ipynb).", "@Saduf2019 \r\nThe bug seems to be fixed in the tf-nightly version. \r\n\r\nI don't know how to check that I am actually running on GPU in the tf-nightly version as the code I used to check for that in tf-2.3.0 now raises an error: \r\n```assert len(tf.config.list_physical_devices('GPU')) > 0``` \r\n\r\nBut the important thing is that in the nightly version the relu activation returns a `nan` when passed a `nan`.", "I am not sure that It Is solved in nightly:\r\nSee https://github.com/tensorflow/tensorflow/issues/42957", "Moreover I think that this could be closed as a duplicate of https://github.com/tensorflow/tensorflow/issues/40072", "@Saduf2019 It seems that you was the triager also on June mentioned ticket.", "To confirm https://github.com/tensorflow/tensorflow/issues/42957 claim it is not solved. \r\n\r\n@Saduf2019 if you check it is just a false negative:\r\n`pip install tf-nightly==2.4.0.dev20200818`\r\n`pip install tf-nightly==2.4.0.dev20200819`\r\n", "This issue is about the unexpected behaviour of tf.keras.activations.relu. This is fixed in the nightly version. \r\n\r\nUnfortunately, the code I provided to test it contained another issue which is addressed elsewhere ( #42957). I would consider this issue closed.", "@alvaro-garcia-carrasco I don't think so. We can close this just cause it is a duplicated of #40072 already triaged by @Saduf2019 in June.\r\n\r\nI don't know if it is limited to Colab instances but you can double test yourself in your machine:\r\nwith \r\n` pip install tf-nightly==2.4.0.dev20200818`\r\nand\r\n`  pip install tf-nightly==2.4.0.dev20200819` or newer\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.debugging.set_log_device_placement(True)\r\nsample = np.array([[np.nan]], dtype=np.float32)\r\nx=tf.keras.activations.relu(sample).numpy()\r\nprint(x.item())\r\n```", "> This issue is about the unexpected behaviour of tf.keras.activations.relu. This is fixed in the nightly version.\r\n> \r\n> Unfortunately, the code I provided to test it contained another issue which is addressed elsewhere ( #42957). I would consider this issue closed.\r\n\r\nThank you for your update, Glad the issue is resolved in tf-nightly. In case of any new error please feel free to create a new issue.\r\nMoving this issue to closed status with confirmation.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42885\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42885\">No</a>\n", "@Saduf2019 It Is not solved"]}, {"number": 42884, "title": "imdb.load_data() is not returning meaningful sentences as I have given print like x_train[0], x_train[1] after loading the data! ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nkeras imdb.load_data() method is not returning meaningful sentence as a row after loading in x_train, y_train, x_test, y_test.\r\n\r\n**Describe the expected behavior**\r\nIt should have an actual meaningful review sentence from the dataset.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n`from keras.datasets import imdb\r\nrow = ''\r\n(x_train, y_train), (x_test, y_test) = imdb.load_data()\r\nword_to_ix = imdb.get_word_index()\r\n#print(word_to_ix['hello'])\r\nix_to_word = dict( (value, key) for key, value in word_to_ix.items() )\r\n#print(ix_to_word[4822])\r\n\r\nfor i in range(len(x_train[0])):\r\n  row += ix_to_word[x_train[0][i]] + ' '\r\nprint(row)`\r\n[https://colab.research.google.com/gist/tawsifsazid/ef2b113e487c233f5ebd8cbddc95ef75/bug_maybe.ipynb?authuser=1](url)\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["https://colab.research.google.com/gist/tawsifsazid/ef2b113e487c233f5ebd8cbddc95ef75/bug_maybe.ipynb?authuser=1", "Hi @tawsifsazid, the indicies are offset by three to account for \"padding\", \"start of sequence\" and \"unknown\". Please take a look at [this StackOverflow thread](https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset) for more information. You should be able to use the following line of code to reconstruct the sentences `' '.join([ix_to_word.get(i - 3, 'UNK') for i in x_train[0]])` where 'UNK' indicates an out of vocabulary word.\r\n\r\nClosing this issue now since there is no bug."]}, {"number": 42883, "title": "Uninitialized memory access of per-channel params", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 5a16264ba6f12883726d12d484d4cd61405ddab7\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Host\r\n\r\n**Describe the problem**\r\nFunction in question:  tflite::PopulateConvolutionQuantizationParams() in tensorflow/lite/kernels/kernel_util.cc (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/kernel_util.cc#L138)\r\nOperators in question: conv and depthwise_conv\r\n\r\nPointer arguments 'per_channel_multiplier' and 'per_channel_shift' are accessed and written to in all cases. \r\nIn the non-int<8,16> case, these arguments can be NULL pointers or uninitialized pointers. The reason it doesn't\r\ncrash now for reference kernels is because memory is allocated for per-channel quant parameters irrespective\r\nof the quantization type. This ticket is for protecting accesses of per-channel params in PopulateConvolutionQuantizationParams().\r\n\r\nOnce that is done, memory usage for non per-channel cases can be reduced for TFLu(and TFL) as an improvement.\r\n\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n_Simple Step:_\r\nSimplest way is to run the unit test for conv or depthwise_conv and see that per-channel arguments are accessed and \r\nupdated in the non-per channel case. \r\n\r\n./tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/kernel_depthwise_conv_test\r\n\r\n_How it was discovered:_\r\nSince it is now possible to dynamically allocate per-channel params in cmsis-nn/<op>.cc (Thanks to https://github.com/tensorflow/tensorflow/commit/59d177d9acabe8e70bc33e554a364d2620bc6999)\r\nthe conv.cc and depthwise_conv.cc in cmsis-nn folder was updated based on PR https://github.com/tensorflow/tensorflow/pull/42770 with some additional\r\ncorrection to not allocate per-channel params for uint8 operators. This led to a crash.\r\n", "comments": ["Affects PR https://github.com/tensorflow/tensorflow/pull/43486", "With changes coming in, I realize linking the line number for the function start wasn't a smart idea!\r\nIt is PopulateConvolutionQuantizationParams()\r\n\r\n// Per-axis & per-tensor\r\nTfLiteStatus PopulateConvolutionQuantizationParams(\r\n    TfLiteContext* context, const TfLiteTensor* input,\r\n    const TfLiteTensor* filter, const TfLiteTensor* bias, TfLiteTensor* output,\r\n    const TfLiteFusedActivation& activation, int32_t* multiplier, int* shift,\r\n    int32_t* output_activation_min, int32_t* output_activation_max,\r\n    int32_t* per_channel_multiplier, int* per_channel_shift, int num_channels) {", "Adding a permalink to the function:\r\nhttps://github.com/tensorflow/tensorflow/blob/195369c5a0c63fb51f1deea1e05bd78e23e90cc2/tensorflow/lite/kernels/kernel_util.cc#L201-L217\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42883\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42883\">No</a>\n"]}]